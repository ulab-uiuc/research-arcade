{
    "Title": [
        "EMERGENT SYMBOL-LIKE NUMBER VARIABLES IN ARTIFICIAL NEURAL NETWORKS"
    ],
    "Abstract": [
        "There is an open question of what types of numeric representations can emergein neural systems. To what degree do neural networks induce abstract, mutable,slot-like numeric variables, and in what situations do these representations emerge?How do these representations change over the course of learning, and how can weunderstand the neural implementations in ways that are unified across differentmodels\u2019 implementations? In this work, we approach these questions by firsttraining sequence based neural systems using Next Token Prediction (NTP) objectives on numeric tasks. We then seek to understand the neural solutions throughthe lens of causal abstractions or symbolic programs. We use a combination ofcausal interventions and visualization methods to find that artificial neural modelsdo indeed develop strong analogs of interchangeable, mutable number variablespurely from the NTP objective. We then ask how variations on the tasks and modelarchitectures affect the models\u2019 learned solutions to find that these symbol-likenumeric representations do not form for every variant of the task, and transformerssolve the problem in a notably different way than their recurrent counterparts. Wethen show how the symbol-like variables change over the course of training to finda strong correlation between the models\u2019 task performance and the alignment oftheir symbol-like representations. Lastly, we show that in all cases, some degreeof gradience exists in these neural symbols, highlighting the difficulty of findingsimple, interpretable symbolic stories of how neural networks perform numerictasks. Taken together, our results are consistent with the view that neural networkscan approximate interpretable symbolic programs of number cognition, but theparticular program they approximate and the extent to which they approximate itcan vary widely, depending on the network architecture, training data, extent oftraining, and network size."
    ],
    "1 INTRODUCTION": [
        "Both biological and artificial Neural Networks (NNs) have powerful modeling abilities. We can seean example of this in biological NNs (BNNs) from the impressive capabilities of human cognition,and we can see this in artificial NNs (ANNs) where recent advances have had such great success thatANNs have been crowned the \u201cgold standard\u201d in many machine learning communities (Alzubaidiet al., 2021). The inner workings of NNs, however, are still often opaque. This is, in part, due to theirrepresentations being highly distributed. Individual neurons can play multiple roles within a network(Rumelhart et al., 1986; McClelland et al., 1986; Smolensky, 1988; Olah et al., 2017; 2020; Elhageet al., 2022; Scherlis et al., 2023; Olah, 2023).",
        "Symbolic algorithms/programs, in contrast, defined as processes that manipulate distinct, typedentities according to explicit rules and relations, can have the benefit of consistency, transparency,and generalization when compared to their neural counterparts. A concrete example of a symbolicalgorithm is a computer program, where the variables are abstract, mutable entities, able to representmany different values, and these variables are processed by well defined functions. Human designedsymbolic systems, however, can lack the expressivity and performance of NNs. This is apparent inthe field of natural language processing where neural architectures trained on vast amounts of data(Vaswani et al., 2017; Brown et al., 2020; Kaplan et al., 2020) have swept the field, surpassing the preexisting symbolic approaches. Furthermore, there are many existing theories that posit the necessityof algorithmic, symbolic, processing for higher level cognition (Do & Hasselmo, 2021; Fodor & Pylyshyn, 1988; Fodor, 1975; 1987; Newell, 1980; 1982; Pylyshyn, 1980; Marcus, 2018; Lake et al.,2017). While the aforementioned successes of neural systems may call such cognitive claims intoquestion, it might be argued that neural systems actually implement such symbolic algorithms; or,they may approximate them well enough that seeking to find the most aligned symbolic algorithmwould be a powerful step toward an accessible, unified understanding of the complex neural behavior.This approach of seeking to characterize the mechanisms of a NN-based system in terms of the mostaligned symbolic algorithm is, in some sense, the goal of most cognitive science, neuroscience, andmechanistic interpretability.",
        "In this work, we narrow our focus to numeric cognition and ask, how we can understand neuralimplementations of numeric concepts at the level of symbolic algorithms? Numeric reasoning has theadvantage of being well studied in humans of different ages and experience levels, which provides apowerful domain for comparisons between BNNs and ANNs (Di Nuovo & Jay, 2019). We focus on anumeric equivalence task that was used to test the numeric abilities of humans whose language lacksexplicit number words (Gordon, 2004). The task is formulated as a sequence of tokens, requiring thesubject to produce the same number of response tokens as a quantity of demonstration tokens initiallyobserved at the beginning of the task. This task is interesting for computational settings because thetraining labels vary in both type and length, and the numeric structures of interest are never explicitlylabeled. Similar versions of this task have also been used in previous theoretical and computationalwork (El-Naggar et al., 2023; Weiss et al., 2018; Behrens et al., 2024), which provides a platform toexpand upon in an effort to understand these seemingly disparate systems in unified ways.",
        "Figure 1: Visual depiction of different architecture\u2019s solutions achieving the same accuracy on thesame numeric equivalence task. The rectangles represent token types for a task in which the modelmust produce the same number of R tokens followed by the EOS token as it observed D tokens beforethe occurence of the T token (see Methods 3.1 for more details). The thought bubbles representcausally discovered, neural variables encoded within subspaces of the models\u2019 representations. Therecurrent models encode a representation of the count of the sequence that increments up before theT token and then back down after the T token to indicate the end of the task. Transformers learn asolution in which they recompute the task relevant information from their context at each step inthe sequence. All NoPE transformers align with the displayed solution, where they assign oppositenumeric values to the D and R tokens and then recompute their sum at each step in the sequence,knowing to stop when the difference equals 0. RoPE transformers can partially rely on positionalinformation unless they are trained on a variant of the task that breaks number-positional correlations.In both cases, the transformers avoid using a cumulative representation of the count that is transmittedto the next step and incremented.",
        "What sorts of representations do ANNs use to solve such a task and how do they arrive at theserepresentations? Do the networks represent numbers in a single number system? Do they use differentsolutions for different situations? Do the answers to these questions change over the course of training,and do the answers vary based on task and architectural details? How can we unify these solutionsin satisfying ways for cognitive scientists, neuroscientists, and computer scientists alike? We wishto understand the degree to which a neural system might implement a mutable, abstract numericvariable, similar to the kind we might assign to an allocated storage location in a computer program. 1. We find causal alignments between neural variables (subspaces of the activations) andsymbolic/causal variables from a counting program that increments and decrements a countvariable. 2. We show that transformer architectures solve the task by referencing and recomputinginformation from the context at each step in the sequence, contrasted against the recurrentsolution of storing a cumulative, Markovian state. 3. We show the importance of using causal interventions to substantiate claims about neuralsolutions, and we show the importance of finding aligned neural subspaces for the causalinterventions, rather than operating directly on raw activations. 4. We show that the recurrent models\u2019 alignment to the counting program can be stronglyinfluenced by task details that are seemingly unrelated to the underlying numeric principles.5. We show that the symbol-like neural variables are graded, with inferior interchangeabilitybetween larger numbers and between numbers that have a greater difference in magnitude.6. We examine the neural variables over the course of training to find a correlation between task accuracy and strength of the alignment. 7. Lastly, we show an effect of model size, where models of minimal size have a greater degreeof gradience in their alignment, while larger models have more precise neural variables.",
        "In this work, we pursue these questions by first training recurrent and attention based ANNs onnumber related Next Token Prediction (NTP) tasks. We then perform both causal and correlativeanalyses to understand their neural representations and solutions. Our contributions are as follows:"
    ],
    "2 RELATED WORK": [
        "We wish to highlight the importance of using causal manipulations for interpreting neural functions inthis work. Causal inference broadly refers to methods that isolate the particular effects of individualcomponents within a larger system (Pearl, 2010). An abundance of causal interpretability variantshave been used to determine what functions are being performed by the models\u2019 activations (orcircuits) (Olah et al., 2018; 2020; Wang et al., 2022; Geva et al., 2023; Merrill et al., 2023; Bhaskaret al., 2024; Wu et al., 2024). Vig et al. (2020) provides an integrative review of the rationale for andutility of causal mediation in neural model analyses. We rely heavily on DAS for our analyses. Thismethod can be thought of as a specific type of activation patching (also referred to as causal tracing)(Meng et al., 2023; Vig et al., 2020).",
        "Many publications explore ANNs\u2019 abilities to perform counting tasks (Di Nuovo & McClelland,2019; Fang et al., 2018; Sabathiel et al., 2020; Kondapaneni & Perona, 2020; Nasr et al., 2019;Zhang et al., 2018; Trott et al., 2018). Our tasks and modeling paradigms differ from many of thesepublications in that numbers are only latent in the structure of our tasks without explicit teaching ofdistinct symbols for distinct numeric values. El-Naggar et al. (2023) provided a theoretical treatmentof Recurrent Neural Network (RNN) solutions to a parentheses closing task, and Weiss et al. (2018)explored Long Short-Term Memory RNNs (LSTMs) (Hochreiter & Schmidhuber, 1997) and GatedRecurrent Units (GRUs) (Cho et al., 2014) in a similar numeric equivalence task looking at theactivations. These works showed correlates of a magnitude scaling solution in both theoretical andpractically trained ANNs. Our work builds on their findings by using causal methods for our analyses,and by expanding the models considered. Lastly, we mention Behrens et al. (2024), who exploredtransformer counting solutions in a task similar to ours. Our work builds upon their findings byincluding positional encodings in our transformers, avoiding explicit labels of the numeric concepts,and providing causal analyses."
    ],
    "3 METHODS": [
        "In this work, we train models on numeric equivalence tasks and then use interpretability methodssuch as Distributed Alignment Search (DAS) (Geiger et al., 2021; 2023) to understand the manner inwhich the models solve the task."
    ],
    "3.1 NUMERIC EQUIVALENCE TASKS": [
        "Each task we consider is defined by varying length sequences of tokens. Each sequence starts with aBeginning of Sequence (BOS) token and ends with an End of Sequence (EOS) token. Each sequenceis defined by a uniformly sampled object quantity from the inclusive range of 1 to 20. The sequenceis constructed as the combination of two phases. The first phase, called the demonstration phase(demo phase), starts with the BOS token and continues with a series of demo tokens equal in quantityto the sampled object quantity. Following the demo tokens is the Trigger token (T), indicating theend of the demo phase and the beginning of the response phase (resp phase). The resp phase consistsof a series of resp tokens equal in number to object quantity. The EOS token follows the resp tokens,denoting the end of the sequence.",
        "During the initial model training, we include all tokens in the autoregressive loss. During modelevaluation and DAS trainings, we only consider tokens in the resp phase\u2014which are fully determinedby the demo phase. During model trainings, we hold out the object quantities 4, 9, 14, and 17. A trialis considered correct when all resp tokens and the EOS token are correctly predicted by the modelafter the trigger. We include three variants of this task differing only in their demo and resp tokentypes.",
        "Figure 2: The activation values for each neuron (denoted by color) at each step in the trial with aobject quantity of 15. Values are averaged over 15 trials. In the rightmost panel, we label the specificneurons used in a one-off causal intervention described in Sections 3.5 and 4.1.",
        "Multi-Object Task: there are 3 demo token types {D1, D2, D3} with a single response token type,R. The demo tokens are uniformly sampled from the 3 possible token types. An examplesequence with a object quantity of 2 could be: \"BOS D3 D1 T R R EOS\" example with a object quantity of 2 is: \"BOS D D T R R EOS\" Same-Object Task: there is a single token type, C, used by both the demo and resp phases. An example with a object quantity of 2 would be: \"BOS C C T C C EOS\".",
        "Single-Object Task: there is a single demo token type, D, and a single response token type, R. An",
        "For some transformer trainings, we include Variable-Length (VL) variants of each task to breakcount-position correlations. In these variants, each token in the demo phase has a 0.2 probability ofbeing sampled as a unique \"void\" token type, V, that should be ignored when determining the objectquantity of the sequence. The number of demo tokens will still be equal to the object quantity whenthe trigger token is presented. As an example, consider the possible sequence with a object quantityof 2: \"BOS V D V V D T R R EOS\"."
    ],
    "3.2 MODEL ARCHITECTURES": [
        "The recurrent models in this paper consist of Gated Recurrent Units (GRUs) (Cho et al., 2014),and Long Short-Term Memory networks (LSTMs) (Hochreiter & Schmidhuber, 1997). Thesearchitectures both have a Markovian, hidden state vector that bottlenecks all predictive computationsfollowing the structure: ht+1 = f (ht, xt)(1)\u02c6xt+1 = g(ht+1)(2)Where ht is the hidden state vector at step t, xt is the input token at step t, f is the recurrent function(either a GRU or LSTM cell), and g is a multi-layer perceptron (MLP) used to make a prediction,denoted \u02c6xt+1, of the token at step t + 1. We contrast the recurrent architectures against transformerarchitectures (Vaswani et al., 2017; Touvron et al., 2023; Su et al., 2023) in that the transformers usea history of input tokens, Xt = [x1, x2, ..., xt], at each time step, t, to make a prediction: (3) Where f now represents the transformer architecture. We show results from 2 layer, single attentionhead transformers that use RoPE positional encodings (Su et al., 2023). Refer to Supplement A.4and Figure 6 for more model and architectural details. We consider transformers with No Positional Encodings (NoPE) in Supplemental section A.4. Except for in the training curves in Figure 5, wefirst train the models to >99.99% accuracy on their respective tasks before performing analyses.The models are evaluated on 15 sampled sequences of each of the 16 trained and 4 held out objectquantities. We train 6 model seeds for each training condition. Model seeds that failed to achievethis standard were dropped from the analyses, including 3 model seeds from the LSTM models inthe Same-Object task and one seed from the transformer models in each of the Single-Object andSame-Object tasks."
    ],
    "3.3 SYMBOLIC PROGRAMS": [
        "In this work, we examine the alignment of 3 different symbolic programs to the models\u2019 distributedrepresentations. 1. Up-Down Program: uses a single numeric variable, called the Count, to track the differencebetween the number of demo tokens and resp tokens at each step in the sequence. It alsocontains a Phase variable to determine whether it is in the demo or resp phase. The programends when the Count is equal to 0 during the resp phase. 2. Up-Up Program: uses two numeric variables\u2014the Demo Count and Resp Count\u2014inaddition to a Phase variable to track quantity at each step in the sequence. This programincrements the Demo Count during the demo phase and increments the Resp Count duringthe resp phase. It ends when the Demo Count is equal to the Resp Count during the respphase. 3. Context Distributed (Ctx-Distr) Program: queries a history of inputs at each step in thesequence to determine when to stop rather than encoding a cumulative quantity variable.A more specific version of this program (that appears to emerge under some conditions)is is one in which the program assigns a value of 1 to each demo token and a -1 to eachresp token (or visa-versa) and computes their combined sum at each step in the sequence todetermine the count. This program outputs the EOS token when the sum is 0.",
        "We include Algorithms 1, 2, and 3 in the supplement which show the pseudocode used to implementthe Up-Down, Up-Up, and Ctx-Distr programs in simulations. Refer to Figure 1 for an illustration ofthe Up-Down strategy and the more specific version of the Ctx-Distr strategy that is only observed insome transformers.",
        "It is important to note that there are an infinite number of causally equivalent implementations ofthese programs. For example, the Up-Down program could immediately add and subtract 1 fromthe Count at every step of the task in addition to carrying out the rest of the program as previouslydescribed. We do not discriminate between programs that are causally indistinct from one another inthis work."
    ],
    "3.4 DISTRIBUTED ALIGNMENT SEARCH (DAS)": [
        "DAS is a hypothesis testing framework for finding alignments between distributed systems andsymbolic programs/algorithms (also referred to as causal abstractions) by performing interchangeinterventions (equivalently referred to as causal interventions, patches, or substitutions) (Geiger et al.,2021; 2023). For all DAS experiments, we freeze the model weights before performing the analysis.",
        "In general, DAS measures the degree of alignment between the best subspace of a distributed model\u2019srepresentations with the variables from a specified symbolic program. The method uses causalinterventions to both train the alignment and to make claims about the degree of alignment. For agiven variable from the symbolic program, DAS learns an orthogonal rotation matrix, R \u2208 Rm\u00d7m,that orients a subspace of the distributed representations along a subset of the dimensions in therepresentation, allowing the subspace to be freely interchanged between representations. The methodrelies on the notion of counterfactual behavior to train the rotation matrix. For a given symbolicprogram, we know what the program\u2019s behavior should be after performing a causal intervention.This counterfactual behavior can be used as the training signal for the rotation matrices. The matricesare trained to convergence and are then validated on unseen causal interventions to determine thesuccess of the alignment.",
        "Concretely, we uniformly sample a time point from two separate sequences respectively. These timepoints are t for what we will call the target sequence and u for the source sequence, where targetrefers to the sequence and representations that will be intervened upon, and source refers to thesequence and representations that will be harvested from for the intervention. We run the model oneach sequence until time point t and u respectively. We then take the latent representations froma prespecified layer in the model at these points t and u. We refer to these representations as thetarget and source vectors, htrgu \u2208 Rm, where m is the number of neurons in eacht \u2208 Rm and hsrcdistributed representation. We then rotate htrgand hsrcu , and thenwe replace a pre-specified number of dimensions in rtrgu . Lastlywe apply the inverse of the rotation to rtrgt . This can be writtenformally as:",
        "Figure 3: (a) and (b) Theoretical neural solutions to the numeric tasks. The purple arrows representincoming demo tokens, the darker arrows indicate the trigger token, the lighter colored arrows indicateincrements to the response tokens, the green dot indicates the starting point. (d) and (e) show thefirst two principal components of a Same-Object and Multi-Object GRUs. Multiple trajectories areshown, each point is a projected latent state in a trajectory. The lines trace individual trajectories.(See Appendix 17 and 15 for details.) (c) IIA for the full hidden state substitutions described forthe Ctx-Distr program, and the DAS IIA for the Last Value alignment (see Figure 9 for expandeddetails). VL stands for the Variable-Length variants of the task in the x-labels. (f) IIA for the attentioninterventions. Results from the two layers in each model seed are sorted based on superior IIA andthen averaged over seeds.",
        "u using R resulting in rtrgt with the same dimensions from rsrc and rsrc t + DRhsrcu ) t = R\u22121((1 \u2212 D)Rhtrghv (4)Where D \u2208 Rm\u00d7m is a diagonal, binary matrix used to isolate the desired set of dimensions toreplace. In this work, we pre-specify the number of non-zero entries in D to be half of m. Theindices of these non-zero dimensions in D are unimportant as the orthogonal matrix can equivalentlylearn each basis in any row order. Finally, we discard hsrcand allow the model to continue makingtoken predictions from point t in the target sequence using hvt . We use the counterfactual behavior(tokens) of the symbolic program as the training sequence in the autoregressive loss to train therotation matrix.",
        "resulting in a new vector, denoted hv u t t Once our rotation matrix has converged, we can evaluate the quality of the alignment using theaccuracy of the model\u2019s predictions on the counterfactual outputs in held out causal interventions.This accuracy has been referred to as the Interchange Intervention Accuracy (IIA) in previous work(Geiger et al., 2023).",
        "u",
        "For the LSTM architecture, we perform DAS on a concatenation of the h and c recurrent state vectors.In the GRUs, we operate on the recurrent hidden state. In the transformers, we operate on the hiddenstate following the first transformer layer (see Figure 6). Unless otherwise stated, we use 10000intervention samples for training and 1000 samples for validation and testing. We uniformly sampleobject quantities and intervention time points, t and u, for both the original and source sequencesin the training, validation, and testing sets. We orthogonalize the rotation matrix using PyTorch\u2019sorthogonal parameterization with default settings. We train the rotation matrix for 1000, with a batchsize of 512, selecting the checkpoint with the best validation performance for analysis. We use alearning rate of 0.003 and an Adam optimizer."
    ],
    "3.5 ADDITIONAL INTERVENTIONS": [
        "A sufficient experiment to demonstrate the lack of use of a cumulative count variable is to lookfor unchanged behavior after performing a full activation vector substitution on relevant hiddenrepresentations. Concretely, our main test for the Ctx-Distr strategy is to replace a full hidden state attime step t with the full hidden state at time step u from a different set of inputs. We provide furtherdetail in Supplement A.5 as to why this experiment is sufficient for the claim of a time-distributedsolution. We trivially apply these interventions on the recurrent hidden states in the RNNs, and weapply these interventions to the hidden states from Layer 1 in the transformer architectures. Resultsare displayed as Ctx-Distr in Figure 4. If the model is using the Ctx-Distr program, we would expectthe models\u2019 subsequent token predictions to be unaffected by this intervention. We include a furtherDAS analysis to align the Last Value variable in the Ctx-Distr program (representing the incrementvalue of the previous input token). These alignments are applied to the embeddings in the GRUsand to the embeddings that are projected into the k and v vectors in the Transformers. We leave thepre-query embeddings unperturbed, further demonstrating the anti-Markovian hidden states.",
        "In an attempt to localize the transformers\u2019 computations to a single attention layer, we includeattention interventions that directly substitute the outputs of the self-attention module from time u totime t. We perform two intervention variants and report the average of their results in Figure 3(f).Intervention 1: Replace the attention output at a non-terminal step in the resp phase with the attentionoutput taken from a terminal step. The expected counterfactual output is the EOS token. Intervention2: Replace the attention output at an EOS step with the output from a non-terminal step in the respphase. The expected prediction is a resp token.",
        "Figure 4: Interchange intervention accuracy (IIA) on variables from different symbolic programs fordifferent tasks faceted by architecture type. The displayed IIA in the Up-Down program is takenfrom the Count variable. The IIA in the Up-Up program is taken as the better performing of the twopossible count variables for each model type respectively. All IIA measurements show the proportionof trials in which the model successfully predicts all counterfactual R and EOS tokens following acausal intervention.",
        "We also explore a direct substitution of individual artificial neuron activations in the Multi-Objecttrained models. In these experiments, we directly substitute the activation value of a specific neuronat time step t with the value of the same neuron at time step u from a different sequence. Weinclude an additional, single model activation intervention on the activations of neurons 12 and 18from the LSTM shown in Figure 2, where we substitute both values in the interventions. In alldirect interventions detailed in this section, we evaluate the model\u2019s IIA on counterfactual behaviorassuming a transfer of the Count."
    ],
    "4 RESULTS": [],
    "4.1 CAUSAL ABSTRACTIONS": [
        "We first turn our attention to Figure 4 where we can see DAS performance as a function of the causalabstraction used in the alignment. In the recurrent models (GRUs and LSTMs), we see that the mostaligned causal abstraction is the Up-Down program. The results are compared against the Up-Upprogram and the Ctx-Distr program which have significantly lower, albeit non-zero IIAs. We use this as evidence in favor of the interpretation that the recurrent models develop a count up, count downstrategy to track quantities within the task.",
        "To determine how the transformer architectures perform the task, we first look at the attention weightsfor both of the two transformer layers (see Figure 10). The RoPE transformers resp and EOS queriesgive surprisingly little attention to the resp tokens. We then perform substitutions of non-terminalhidden states in the response phase to find that the model\u2019s predictions are largely unaffected. Theresults of these interventions are the Ctx-Distr bars in Figure 4. We include an additional DASanalysis on the Last Value variable from the specific form of the Ctx-Distr program in the GRUs andRoPE Transformers. The resulting IIA for these Multi-Object transformers was a value of 0.827. Wealso examine a set of transformers trained on the Variable-Length variant of the Multi-Object task tobreak count-position correlations. These Variable-Length transformers achieved an IIA of 0.960 forthe same DAS analysis (see Figure 9). The lower IIA of the Multi-Object transformers is consistentwith the notion that they rely, in part, on a positional readout to solve the task. In an attempt toelucidate the processing layer in which the distributed counting operation occurs in the transformers,we included direct attention interventions, the results of which are displayed in Figure 3(f). Theseinterventions show degree to which the EOS decision can be localized to a single attention head.The lower IIAs for the Variable-Length transformers is consistent with the interpretation that theyhave a stronger tendency to spread their EOS decision across both layers. We provide an additionaltheoretical analysis with simulations of 1 layer No Positional Encoding (NoPE) transformers inSupplement A.4 where we show that we can add and subtract from the transformer\u2019s predicted countusing the strength-value of the demo tokens to add and the resp tokens to subtract.",
        "Figure 5: In all panels, the IIA comes from DAS using the Count variable in the Up-Down programon held out data. The models are all Multi-Object GRUs. (a) Both task accuracy and IIA over thecourse of training for different sizes of the recurrent state. (b) Converged IIA for the GRUs as afunction of increasing hidden state sizes. (c) The DAS IIA where the x-axis shows the target count(the count before the intervention) and the colors denote the source count (the count that is transferredinto the representation during the intervention). The curves are averaged over all models consideredin panel (b). The cyan, dashed line represents the mean IIA over all interventions for a given targetcount\u2014highlighting the greater number of samples for interventions on smaller numbers. (d) DASIIA as a function of the absolute difference between the target and source counts. The line stylesindicate different model sizes. Both panels (c) and (d) show that the contents of the interventionsaffect the IIA in a relatively smooth fashion.",
        "We include an analysis involving the direct substitution of activation values in the models\u2019 representations. Of all the neurons and models we analyzed, the best IIA was 0.399. This IIA was achieved inthe LSTM model intervention where we intervened on both the activations for neurons 12 and 18shown in Figure 2. We use Figure 2 to highlight the difficulty of directly analyzing neural activations,and the importance of learning the rotation in DAS. Interpreting and intervening on the activationsdirectly is a difficult task that can be misleading."
    ],
    "4.2 MODEL DIMENSIONALITY AND LEARNING TRAJECTORIES": [
        "We can see from Figure 5 that although many model sizes can solve the Multi-Object task, increasingthe number of dimensions in the hidden states of the GRUs improves IIA in alignments with theUp-Down program. We can also see in Figure 5 that the larger models tend to have less gradedalignments. We examine the symbolic alignments over the course of training in Figure 5. Of noteis the correlation between alignment and performance. This is especially pronounced in the largermodels. And we note the relatively flat curves of the alignment trajectories after the models solve thetask."
    ],
    "4.3 TASKS": [
        "An interesting result is the impact of demonstration token type on the resulting alignment of therecurrent models with the Up-Down program. We can see from Figure 4 that recurrent models trainedon the Same-Object task\u2014in which the demo tokens are the same type as the resp tokens\u2014havepoor alignment with any of the proposed symbolic programs. We use this result to highlight thesignificance of the unified, interchangeable numeric representations found in the Multi-Object andSingle-Object tasks.",
        "We present a number of theoretical neural solutions to the counting task in Figure 3 as examples ofpossible neural solutions to each of the tasks. The Overlap Solution, shown in blue in Panel 3(a), is anexample of how some solutions may fail to align with the Up-Down solution. In the Overlap Solution,we see that the Count is entangled with the phase of the trial due to the overlap of the trajectory onthe vertical axis. In this model, we would be unable to distinguish between a count of n in the demophase and a count of n + 1 in the response phase at the overlapping points in the trajectories. We donot make claims that this is how the Same-Object models are solving the task, but merely provide thetheoretical models as ways that it could solve the task."
    ],
    "4.4 SYMBOLIC GRADIENCE": [
        "We now shift towards a more nuanced perspective of symbolic alignments with neural systems, wherewe highlight the graded nature of the neural symbols. We can see from Figure 5 that the GRU modelstrained on the Multi-Object task have worse IIA when the quantities involved in the interventionare larger, and when the intervention quantities have a greater absolute difference. We point outthat the task training data forces the models to have more experience with smaller numbers, as theynecessarily interact with smaller numbers every time they interact with larger numbers. This isperhaps a causal factor for the more graded representations at larger numbers. The DAS training datasuffers from a similar issue, where we use a uniform sampling of the object quantities that define thetraining sequences and then we uniformly sample the intervention indices from these sequences. Thisresults in a disproportionately large number of training interventions containing smaller values."
    ],
    "5 DISCUSSION/CONCLUSION": [
        "In this work we used causal methods to demonstrate the existence of symbol-like variables within NNsolutions to numeric equivalence tasks. We showed that these numeric neural variables emerge purelyfrom an NTP objective and represent abstract information that is only latent in the task structure.These findings are a proof of principle that neural systems do not need explicit exposure to discretenumeric symbols for symbol-like representations of number to emerge. Nor do neural systems needbuilt-in counting principles to inform their numeric learning.",
        "We also demonstrated differences in the high-level solutions used by different model architecturesin different tasks. Namely, we showed that increasing the dimensionality of the recurrent modelsimproved their symbolic alignment, we showed that transformers solved the tasks by recomputing therelevant information at each step in the sequence\u2014contrasted against the cumulative count variablesdiscovered in the recurrent models\u2014and we showed that different solutions arise in the Same-ObjectTask compared to the Multi-Object and Single-Object variants. An interesting phenomenon in theLLM literature is the effect of model scale on performance (Brown et al., 2020; Kaplan et al., 2020).Although our scaling results are for GRUs on toy tasks, they are provocative for understanding whysize might improve autoregressive results. Perhaps increased dimensionality allows the models to findmore symbol-like, disentangled solutions when solving their NTP objectives. This is consistent withthe early learning and strong correlation between performance and symbolic alignment demonstratedin larger models in Figure 5. We conjecture the possibility that this result can be explained by thelottery ticket hypothesis (Frankle & Carbin, 2019) combined with lazy learning dynamics (Jacot et al.,2020). Perhaps the majority of what these models learn are linear functions of their initial features,and increasing the dimensionality of the model increases the number of potential pathways/featuresthat the model can use to solve the task.",
        "We are unsure if the \"stateless\", time-distributed solution exhibited by the transformers generalizesbeyond the counting tasks presented in this work. It is possible that this finding is representative ofa more general principle\u2014that transformers avoid solutions that use cumulative, Markovian state variables. We provide an analysis in Supplement A.4 of a one-layer transformer without positionalencodings trained on a variant of the Single-Object task without a BOS token, and without a T token.We experimentally and mathematically support the idea that this minimal model solves the task byassigning opposite numeric values to the demo and resp tokens and averaging their values at eachstep in the sequence. Although it seems as though the RoPE transformers presented in Figure 4 mightrely in part on a positional readout from the relatively low alignment with the Last Value variablein Figure 3(c), we managed to get a much higher alignment when using transformers trained on avariant of the task that breaks correlations between the position and count of the sequence. We findit worth noting that the Ctx-Distr solution exhibited by the transformers lends itself to the type ofsolutions that might be predicted by RASP-L (Zhou et al., 2023).",
        "GRUs and LSTMs trained on the Same-Object Task failed to align with any of the symbolic algorithmsthat we presented in this paper. To address this, we included Figure 3 showing the first two principalcomponents of a Same-Object GRU model over different trial trajectories. We also included a numberof theoretical models to assist conceptualization of why some neural solutions might align with somesymbolic algorithms whereas others would not. We note that there are symbolic programs that usememorization that could trivially align with each of the recurrent models. One such solution mightinvolve a single variable that encodes a tuple of each possible Count-Phase combination. In thiscase, the alignment would simply learn to transfer the complete state at each causal intervention. Asmentioned earlier in this work, we are only concerned with solutions that are causally distinct fromone another. We leave a more thorough, causal analysis of the Same-Object models to future work.",
        "An important aspect of our work is demonstrating the potential for misleading conclusions in theabsence of causal analysis methods. We can see this in Figure 2 where the activations for the LSTMmight be mistaken for being sufficient to change the model\u2019s count. Similarly, the PCA projectionsin Figure 3 might fail to provide predictions of neural variable interchangeability, and the attentionweights shown in Figures 10- 13 might mislead on token value interchangeability. We wish to beclear, however, that these non-causal techniques are still fruitful as tools for scientific exploration andconceptualization, complementing causal methods.",
        "We now expand upon the learning trajectories displayed in Figure 5. We can see from the performancecurves that both the models\u2019 task performance and alignment performance begin a transition awayfrom 0% at similar epochs and plateau at similar epochs. This result can be contrasted with analternative result in which the alignment curves significantly lag behind the task performance ofthe models. Alternatively, there could have been a stronger upward slope of the IIA following theinitial performance jump and plateau. In these hypothetical cases, a possible interpretation could havebeen that the network first develops more complex solutions or unique solutions for many differentinput-output pairs and subsequently unifies them over training. The pattern we observe instead isconsistent with the idea that the networks are biased towards the simplest, unified strategies from thebeginning of training. Perhaps our result is to be expected in light of works like Saxe et al. (2019)and Saxe et al. (2022) which show an inherent tendency for NNs trained via gradient descent to findsolutions that share network pathways. This would explain the driving force towards the demo andresponse phases sharing the same representation of a Count variable.",
        "Lastly, we demonstrated that the symbol-like, neural subspaces illuminated by DAS are not alwaysperfectly symbolic, often exhibiting a smooth, graded influence from the content of the variablesbeing intervened upon. We interpret these results as a reminder that representations in distributedsystems exist on a continuum despite seemingly discrete, symbolic performance on tasks. Theseresults have an analogy to children\u2019s number cognition in which children may appear to possess asymbol-like understanding of exact numbers and their associated principles, but when probed deeper,the symbol-like picture falls apart (Wynn, 1992; Davidson et al., 2012). Perhaps the graded nature ofthe neural representations reinforces the utility of thinking about network solutions as trajectoriesin a dynamical system, where the values along a set of dimensions are analogous to the values ofhigh-level, causal variables. We use our findings about symbolic gradience as a reminder that althoughNNs may discover approximations to interpretable, symbol-like solutions, their representations arestill ultimately graded\u2014adding nuance to the effort to find in them exact implementations of anysymbolic computer program."
    ]
}