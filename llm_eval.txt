generate_answer: The \textit{Sel3DCraft} pipeline begins by expanding user text input into multiple candidates via LLM. A dual-branch pipeline then generates/retrieves corresponding multi-view images, displayed in an interactive satellite view fo

The generated results at each timestep of the diffusion sampling process from $T$ to $1$. For example, given one prompt case "A man with a beard wearing glasses in blue shirt", the noise in the image is gradually reduced from tim

Llama2-7b

Simply supported beam with distributed loading.

Additional qualitative results for Edges$\rightarrow$Shoes$\times64$, where each pair of consecutive rows displaying the input image in the "Edges" domain and its translation in the "Shoes" domain, respectively.

Performance on a commonsense benchmark (HellaSwag), a linguistic understanding benchmark (SuperGLUE), and the massive multitask test. On previous benchmarks, smaller models start well above random chance levels and exhibit more con

Statistics over locations

The encoder E compresses continuous vectors into a latent embedding \(\bm{z} \in \mathbb{R}^{d_z}\) through \(\bm{z} = \mathcal{E}(\bm{u};\theta_E)\). Quantization then occurs through $K$ residual steps, where each step $k$ selects the closest codebook vector to the current residual as \(\bm{q}^{(k)} = \underset{\bm{c} \in \mathcal{C}^{(k)}}{\arg\min} \|\bm{r}^{(k)} - \bm{c}\|_2\), where \(\bm{r}^{(0)} = \bm{z}\). The Group CSID combines quantization indices into $CSID(\bm{u}) = (i^{(1)}, i^{(2)}, ..., i^{(K)})$, with $i^{(k)}$ denoting the codebook index of \(\bm{q}^{(k)}\). Reconstruction uses decoder D to map the quantized embedding $\bm{\hat{z}} = \sum_{i = 1}^{K} \bm{q}^{(i)}$ back to the input space as \(\hat{\bm{u}} = \mathcal{D}(\bm{\hat{z}};\theta_D)\).

The \textit{Sel3DCraft} pipeline begins by expanding user text input into multiple candidates via LLM. A dual-branch pipeline then generates/retrieves corresponding multi-view images, displayed in an interactive satellite view fo

The generated results at each timestep of the diffusion sampling process from $T$ to $1$. For example, given one prompt case "A man with a beard wearing glasses in blue shirt", the noise in the image is gradually reduced from tim

Llama2-7b

Simply supported beam with distributed loading.

Additional qualitative results for Edges$\rightarrow$Shoes$\times64$, where each pair of consecutive rows displaying the input image in the "Edges" domain and its translation in the "Shoes" domain, respectively.

Performance on a commonsense benchmark (HellaSwag), a linguistic understanding benchmark (SuperGLUE), and the massive multitask test. On previous benchmarks, smaller models start well above random chance levels and exhibit more con

Statistics over locations

The encoder E compresses continuous vectors into a latent embedding \(\bm{z} \in \mathbb{R}^{d_z}\) through \(\bm{z} = \mathcal{E}(\bm{u};\theta_E)\). Quantization then occurs through $K$ residual steps, where each step $k$ selects the closest codebook vector to the current residual as \(\bm{q}^{(k)} = \underset{\bm{c} \in \mathcal{C}^{(k)}}{\arg\min} \|\bm{r}^{(k)} - \bm{c}\|_2\), where \(\bm{r}^{(0)} = \bm{z}\). The Group CSID combines quantization indices into $CSID(\bm{u}) = (i^{(1)}, i^{(2)}, ..., i^{(K)})$, with $i^{(k)}$ denoting the codebook index of \(\bm{q}^{(k)}\). Reconstruction uses decoder D to map the quantized embedding $\bm{\hat{z}} = \sum_{i = 1}^{K} \bm{q}^{(i)}$ back to the input space as \(\hat{\bm{u}} = \mathcal{D}(\bm{\hat{z}};\theta_D)\).
original_answer: where the scaled dot product of query and key matrices is computed by dividing their product by \(\sqrt{d_k}\)\sqrt{d_k}. A causal mask \(\bm{M}\)\bm{M}M — with \(\bm{M}_{ij} = 0\)\bm{M}M_{ij}ij = 0 if \(i \geq j\)i \geq j and \(\bm{M}_{ij} = -\infty\)\bm{M}M_{ij}ij = -\infty if \(i < j\)i < j — is added to enforce autoregressive properties. Softmax is applied to this result, which is then multiplied by the value matrix to generate attention output \(\bm{A}\)\bm{A}A.
This mask prevents future information leakage, ensuring temporal consistency across \(L\)L layers. Each layer’s output is \(\bm{H}^{(l)} = \text{LayerNorm}\left( \bm{A} + \bm{H}^{(l-1)} \right)\)\bm{H}H^{(l)}(l) = \text{LayerNorm}\left( \bm{A}A + \bm{H}H^{(l-1)}(l-1) \right). For the final token at position \(T\)T, its layer-\(l\)l representation is \(\bm{H}^{(l)}_T\)\bm{H}H^{(l)}(l)_T; after \(L\)L layers, \(\bm{H}^{(L)}_T\)\bm{H}H^{(L)}(L)_T encodes the entire sequence’s hierarchical context.
\subsection{RQ-VAE for Group Clustering}
\label{subsec:rq_vae}
Residual-Quantized Variational AutoEncoder (RQ-VAE) \cite{rajput2023recommender} provides hierarchical vector quantization for efficient representation learning. Given user representations \(\bm{u} \in \mathbb{R}^d\)\bm{u}u \in \mathbb{R}^d encoded from long-term sequences, we map them to discrete Group Collaborative Semantic IDs (Group CSID) that capture stable group preferences.
generate_answer: The \textit{Sel3DCraft} pipeline begins by expanding user text input into multiple candidates via LLM. A dual-branch pipeline then generates/retrieves corresponding multi-view images, displayed in an interactive satellite view fo

The generated results at each timestep of the diffusion sampling process from $T$ to $1$. For example, given one prompt case "A man with a beard wearing glasses in blue shirt", the noise in the image is gradually reduced from tim

Llama2-7b

Simply supported beam with distributed loading.

Additional qualitative results for Edges$\rightarrow$Shoes$\times64$, where each pair of consecutive rows displaying the input image in the "Edges" domain and its translation in the "Shoes" domain, respectively.

Performance on a commonsense benchmark (HellaSwag), a linguistic understanding benchmark (SuperGLUE), and the massive multitask test. On previous benchmarks, smaller models start well above random chance levels and exhibit more con

Statistics over locations

The encoder E compresses continuous vectors into a latent embedding \(\bm{z} \in \mathbb{R}^{d_z}\) through \(\bm{z} = \mathcal{E}(\bm{u};\theta_E)\). Quantization then occurs through $K$ residual steps, where each step $k$ selects the closest codebook vector to the current residual as \(\bm{q}^{(k)} = \underset{\bm{c} \in \mathcal{C}^{(k)}}{\arg\min} \|\bm{r}^{(k)} - \bm{c}\|_2\), where \(\bm{r}^{(0)} = \bm{z}\). The Group CSID combines quantization indices into $CSID(\bm{u}) = (i^{(1)}, i^{(2)}, ..., i^{(K)})$, with $i^{(k)}$ denoting the codebook index of \(\bm{q}^{(k)}\). Reconstruction uses decoder D to map the quantized embedding $\bm{\hat{z}} = \sum_{i = 1}^{K} \bm{q}^{(i)}$ back to the input space as \(\hat{\bm{u}} = \mathcal{D}(\bm{\hat{z}};\theta_D)\).

The \textit{Sel3DCraft} pipeline begins by expanding user text input into multiple candidates via LLM. A dual-branch pipeline then generates/retrieves corresponding multi-view images, displayed in an interactive satellite view fo

The generated results at each timestep of the diffusion sampling process from $T$ to $1$. For example, given one prompt case "A man with a beard wearing glasses in blue shirt", the noise in the image is gradually reduced from tim

Llama2-7b

Simply supported beam with distributed loading.

Additional qualitative results for Edges$\rightarrow$Shoes$\times64$, where each pair of consecutive rows displaying the input image in the "Edges" domain and its translation in the "Shoes" domain, respectively.

Performance on a commonsense benchmark (HellaSwag), a linguistic understanding benchmark (SuperGLUE), and the massive multitask test. On previous benchmarks, smaller models start well above random chance levels and exhibit more con

Statistics over locations

The encoder E compresses continuous vectors into a latent embedding \(\bm{z} \in \mathbb{R}^{d_z}\) through \(\bm{z} = \mathcal{E}(\bm{u};\theta_E)\). Quantization then occurs through $K$ residual steps, where each step $k$ selects the closest codebook vector to the current residual as \(\bm{q}^{(k)} = \underset{\bm{c} \in \mathcal{C}^{(k)}}{\arg\min} \|\bm{r}^{(k)} - \bm{c}\|_2\), where \(\bm{r}^{(0)} = \bm{z}\). The Group CSID combines quantization indices into $CSID(\bm{u}) = (i^{(1)}, i^{(2)}, ..., i^{(K)})$, with $i^{(k)}$ denoting the codebook index of \(\bm{q}^{(k)}\). Reconstruction uses decoder D to map the quantized embedding $\bm{\hat{z}} = \sum_{i = 1}^{K} \bm{q}^{(i)}$ back to the input space as \(\hat{\bm{u}} = \mathcal{D}(\bm{\hat{z}};\theta_D)\).
original_answer: where the scaled dot product of query and key matrices is computed by dividing their product by \(\sqrt{d_k}\)\sqrt{d_k}. A causal mask \(\bm{M}\)\bm{M}M — with \(\bm{M}_{ij} = 0\)\bm{M}M_{ij}ij = 0 if \(i \geq j\)i \geq j and \(\bm{M}_{ij} = -\infty\)\bm{M}M_{ij}ij = -\infty if \(i < j\)i < j — is added to enforce autoregressive properties. Softmax is applied to this result, which is then multiplied by the value matrix to generate attention output \(\bm{A}\)\bm{A}A.
This mask prevents future information leakage, ensuring temporal consistency across \(L\)L layers. Each layer’s output is \(\bm{H}^{(l)} = \text{LayerNorm}\left( \bm{A} + \bm{H}^{(l-1)} \right)\)\bm{H}H^{(l)}(l) = \text{LayerNorm}\left( \bm{A}A + \bm{H}H^{(l-1)}(l-1) \right). For the final token at position \(T\)T, its layer-\(l\)l representation is \(\bm{H}^{(l)}_T\)\bm{H}H^{(l)}(l)_T; after \(L\)L layers, \(\bm{H}^{(L)}_T\)\bm{H}H^{(L)}(L)_T encodes the entire sequence’s hierarchical context.
\subsection{RQ-VAE for Group Clustering}
\label{subsec:rq_vae}
Residual-Quantized Variational AutoEncoder (RQ-VAE) \cite{rajput2023recommender} provides hierarchical vector quantization for efficient representation learning. Given user representations \(\bm{u} \in \mathbb{R}^d\)\bm{u}u \in \mathbb{R}^d encoded from long-term sequences, we map them to discrete Group Collaborative Semantic IDs (Group CSID) that capture stable group preferences.
generate_answer: The \textit{Sel3DCraft} pipeline begins by expanding user text input into multiple candidates via LLM. A dual-branch pipeline then generates/retrieves corresponding multi-view images, displayed in an interactive satellite view fo

The generated results at each timestep of the diffusion sampling process from $T$ to $1$. For example, given one prompt case "A man with a beard wearing glasses in blue shirt", the noise in the image is gradually reduced from tim

Llama2-7b

Simply supported beam with distributed loading.

Additional qualitative results for Edges$\rightarrow$Shoes$\times64$, where each pair of consecutive rows displaying the input image in the "Edges" domain and its translation in the "Shoes" domain, respectively.

Performance on a commonsense benchmark (HellaSwag), a linguistic understanding benchmark (SuperGLUE), and the massive multitask test. On previous benchmarks, smaller models start well above random chance levels and exhibit more con

Statistics over locations

The encoder E compresses continuous vectors into a latent embedding \(\bm{z} \in \mathbb{R}^{d_z}\) through \(\bm{z} = \mathcal{E}(\bm{u};\theta_E)\). Quantization then occurs through $K$ residual steps, where each step $k$ selects the closest codebook vector to the current residual as \(\bm{q}^{(k)} = \underset{\bm{c} \in \mathcal{C}^{(k)}}{\arg\min} \|\bm{r}^{(k)} - \bm{c}\|_2\), where \(\bm{r}^{(0)} = \bm{z}\). The Group CSID combines quantization indices into $CSID(\bm{u}) = (i^{(1)}, i^{(2)}, ..., i^{(K)})$, with $i^{(k)}$ denoting the codebook index of \(\bm{q}^{(k)}\). Reconstruction uses decoder D to map the quantized embedding $\bm{\hat{z}} = \sum_{i = 1}^{K} \bm{q}^{(i)}$ back to the input space as \(\hat{\bm{u}} = \mathcal{D}(\bm{\hat{z}};\theta_D)\).

The \textit{Sel3DCraft} pipeline begins by expanding user text input into multiple candidates via LLM. A dual-branch pipeline then generates/retrieves corresponding multi-view images, displayed in an interactive satellite view fo

The generated results at each timestep of the diffusion sampling process from $T$ to $1$. For example, given one prompt case "A man with a beard wearing glasses in blue shirt", the noise in the image is gradually reduced from tim

Llama2-7b

Simply supported beam with distributed loading.

Additional qualitative results for Edges$\rightarrow$Shoes$\times64$, where each pair of consecutive rows displaying the input image in the "Edges" domain and its translation in the "Shoes" domain, respectively.

Performance on a commonsense benchmark (HellaSwag), a linguistic understanding benchmark (SuperGLUE), and the massive multitask test. On previous benchmarks, smaller models start well above random chance levels and exhibit more con

Statistics over locations

The encoder E compresses continuous vectors into a latent embedding \(\bm{z} \in \mathbb{R}^{d_z}\) through \(\bm{z} = \mathcal{E}(\bm{u};\theta_E)\). Quantization then occurs through $K$ residual steps, where each step $k$ selects the closest codebook vector to the current residual as \(\bm{q}^{(k)} = \underset{\bm{c} \in \mathcal{C}^{(k)}}{\arg\min} \|\bm{r}^{(k)} - \bm{c}\|_2\), where \(\bm{r}^{(0)} = \bm{z}\). The Group CSID combines quantization indices into $CSID(\bm{u}) = (i^{(1)}, i^{(2)}, ..., i^{(K)})$, with $i^{(k)}$ denoting the codebook index of \(\bm{q}^{(k)}\). Reconstruction uses decoder D to map the quantized embedding $\bm{\hat{z}} = \sum_{i = 1}^{K} \bm{q}^{(i)}$ back to the input space as \(\hat{\bm{u}} = \mathcal{D}(\bm{\hat{z}};\theta_D)\).
original_answer: where the scaled dot product of query and key matrices is computed by dividing their product by \(\sqrt{d_k}\)\sqrt{d_k}. A causal mask \(\bm{M}\)\bm{M}M — with \(\bm{M}_{ij} = 0\)\bm{M}M_{ij}ij = 0 if \(i \geq j\)i \geq j and \(\bm{M}_{ij} = -\infty\)\bm{M}M_{ij}ij = -\infty if \(i < j\)i < j — is added to enforce autoregressive properties. Softmax is applied to this result, which is then multiplied by the value matrix to generate attention output \(\bm{A}\)\bm{A}A.
This mask prevents future information leakage, ensuring temporal consistency across \(L\)L layers. Each layer’s output is \(\bm{H}^{(l)} = \text{LayerNorm}\left( \bm{A} + \bm{H}^{(l-1)} \right)\)\bm{H}H^{(l)}(l) = \text{LayerNorm}\left( \bm{A}A + \bm{H}H^{(l-1)}(l-1) \right). For the final token at position \(T\)T, its layer-\(l\)l representation is \(\bm{H}^{(l)}_T\)\bm{H}H^{(l)}(l)_T; after \(L\)L layers, \(\bm{H}^{(L)}_T\)\bm{H}H^{(L)}(L)_T encodes the entire sequence’s hierarchical context.
\subsection{RQ-VAE for Group Clustering}
\label{subsec:rq_vae}
Residual-Quantized Variational AutoEncoder (RQ-VAE) \cite{rajput2023recommender} provides hierarchical vector quantization for efficient representation learning. Given user representations \(\bm{u} \in \mathbb{R}^d\)\bm{u}u \in \mathbb{R}^d encoded from long-term sequences, we map them to discrete Group Collaborative Semantic IDs (Group CSID) that capture stable group preferences.
generate_answer: The \textit{Sel3DCraft} pipeline begins by expanding user text input into multiple candidates via LLM. A dual-branch pipeline then generates/retrieves corresponding multi-view images, displayed in an interactive satellite view fo

The generated results at each timestep of the diffusion sampling process from $T$ to $1$. For example, given one prompt case "A man with a beard wearing glasses in blue shirt", the noise in the image is gradually reduced from tim

Llama2-7b

Simply supported beam with distributed loading.

Additional qualitative results for Edges$\rightarrow$Shoes$\times64$, where each pair of consecutive rows displaying the input image in the "Edges" domain and its translation in the "Shoes" domain, respectively.

Performance on a commonsense benchmark (HellaSwag), a linguistic understanding benchmark (SuperGLUE), and the massive multitask test. On previous benchmarks, smaller models start well above random chance levels and exhibit more con

Statistics over locations

The encoder E compresses continuous vectors into a latent embedding \(\bm{z} \in \mathbb{R}^{d_z}\) through \(\bm{z} = \mathcal{E}(\bm{u};\theta_E)\). Quantization then occurs through $K$ residual steps, where each step $k$ selects the closest codebook vector to the current residual as \(\bm{q}^{(k)} = \underset{\bm{c} \in \mathcal{C}^{(k)}}{\arg\min} \|\bm{r}^{(k)} - \bm{c}\|_2\), where \(\bm{r}^{(0)} = \bm{z}\). The Group CSID combines quantization indices into $CSID(\bm{u}) = (i^{(1)}, i^{(2)}, ..., i^{(K)})$, with $i^{(k)}$ denoting the codebook index of \(\bm{q}^{(k)}\). Reconstruction uses decoder D to map the quantized embedding $\bm{\hat{z}} = \sum_{i = 1}^{K} \bm{q}^{(i)}$ back to the input space as \(\hat{\bm{u}} = \mathcal{D}(\bm{\hat{z}};\theta_D)\).

The \textit{Sel3DCraft} pipeline begins by expanding user text input into multiple candidates via LLM. A dual-branch pipeline then generates/retrieves corresponding multi-view images, displayed in an interactive satellite view fo

The generated results at each timestep of the diffusion sampling process from $T$ to $1$. For example, given one prompt case "A man with a beard wearing glasses in blue shirt", the noise in the image is gradually reduced from tim

Llama2-7b

Simply supported beam with distributed loading.

Additional qualitative results for Edges$\rightarrow$Shoes$\times64$, where each pair of consecutive rows displaying the input image in the "Edges" domain and its translation in the "Shoes" domain, respectively.

Performance on a commonsense benchmark (HellaSwag), a linguistic understanding benchmark (SuperGLUE), and the massive multitask test. On previous benchmarks, smaller models start well above random chance levels and exhibit more con

Statistics over locations

The encoder E compresses continuous vectors into a latent embedding \(\bm{z} \in \mathbb{R}^{d_z}\) through \(\bm{z} = \mathcal{E}(\bm{u};\theta_E)\). Quantization then occurs through $K$ residual steps, where each step $k$ selects the closest codebook vector to the current residual as \(\bm{q}^{(k)} = \underset{\bm{c} \in \mathcal{C}^{(k)}}{\arg\min} \|\bm{r}^{(k)} - \bm{c}\|_2\), where \(\bm{r}^{(0)} = \bm{z}\). The Group CSID combines quantization indices into $CSID(\bm{u}) = (i^{(1)}, i^{(2)}, ..., i^{(K)})$, with $i^{(k)}$ denoting the codebook index of \(\bm{q}^{(k)}\). Reconstruction uses decoder D to map the quantized embedding $\bm{\hat{z}} = \sum_{i = 1}^{K} \bm{q}^{(i)}$ back to the input space as \(\hat{\bm{u}} = \mathcal{D}(\bm{\hat{z}};\theta_D)\).
original_answer: where the scaled dot product of query and key matrices is computed by dividing their product by \(\sqrt{d_k}\)\sqrt{d_k}. A causal mask \(\bm{M}\)\bm{M}M — with \(\bm{M}_{ij} = 0\)\bm{M}M_{ij}ij = 0 if \(i \geq j\)i \geq j and \(\bm{M}_{ij} = -\infty\)\bm{M}M_{ij}ij = -\infty if \(i < j\)i < j — is added to enforce autoregressive properties. Softmax is applied to this result, which is then multiplied by the value matrix to generate attention output \(\bm{A}\)\bm{A}A.
This mask prevents future information leakage, ensuring temporal consistency across \(L\)L layers. Each layer’s output is \(\bm{H}^{(l)} = \text{LayerNorm}\left( \bm{A} + \bm{H}^{(l-1)} \right)\)\bm{H}H^{(l)}(l) = \text{LayerNorm}\left( \bm{A}A + \bm{H}H^{(l-1)}(l-1) \right). For the final token at position \(T\)T, its layer-\(l\)l representation is \(\bm{H}^{(l)}_T\)\bm{H}H^{(l)}(l)_T; after \(L\)L layers, \(\bm{H}^{(L)}_T\)\bm{H}H^{(L)}(L)_T encodes the entire sequence’s hierarchical context.
\subsection{RQ-VAE for Group Clustering}
\label{subsec:rq_vae}
Residual-Quantized Variational AutoEncoder (RQ-VAE) \cite{rajput2023recommender} provides hierarchical vector quantization for efficient representation learning. Given user representations \(\bm{u} \in \mathbb{R}^d\)\bm{u}u \in \mathbb{R}^d encoded from long-term sequences, we map them to discrete Group Collaborative Semantic IDs (Group CSID) that capture stable group preferences.
generate_answer: The \textit{Sel3DCraft} pipeline begins by expanding user text input into multiple candidates via LLM. A dual-branch pipeline then generates/retrieves corresponding multi-view images, displayed in an interactive satellite view fo

The generated results at each timestep of the diffusion sampling process from $T$ to $1$. For example, given one prompt case "A man with a beard wearing glasses in blue shirt", the noise in the image is gradually reduced from tim

Llama2-7b

Simply supported beam with distributed loading.

Additional qualitative results for Edges$\rightarrow$Shoes$\times64$, where each pair of consecutive rows displaying the input image in the "Edges" domain and its translation in the "Shoes" domain, respectively.

Performance on a commonsense benchmark (HellaSwag), a linguistic understanding benchmark (SuperGLUE), and the massive multitask test. On previous benchmarks, smaller models start well above random chance levels and exhibit more con

Statistics over locations

The encoder E compresses continuous vectors into a latent embedding \(\bm{z} \in \mathbb{R}^{d_z}\) through \(\bm{z} = \mathcal{E}(\bm{u};\theta_E)\). Quantization then occurs through $K$ residual steps, where each step $k$ selects the closest codebook vector to the current residual as \(\bm{q}^{(k)} = \underset{\bm{c} \in \mathcal{C}^{(k)}}{\arg\min} \|\bm{r}^{(k)} - \bm{c}\|_2\), where \(\bm{r}^{(0)} = \bm{z}\). The Group CSID combines quantization indices into $CSID(\bm{u}) = (i^{(1)}, i^{(2)}, ..., i^{(K)})$, with $i^{(k)}$ denoting the codebook index of \(\bm{q}^{(k)}\). Reconstruction uses decoder D to map the quantized embedding $\bm{\hat{z}} = \sum_{i = 1}^{K} \bm{q}^{(i)}$ back to the input space as \(\hat{\bm{u}} = \mathcal{D}(\bm{\hat{z}};\theta_D)\).

The \textit{Sel3DCraft} pipeline begins by expanding user text input into multiple candidates via LLM. A dual-branch pipeline then generates/retrieves corresponding multi-view images, displayed in an interactive satellite view fo

The generated results at each timestep of the diffusion sampling process from $T$ to $1$. For example, given one prompt case "A man with a beard wearing glasses in blue shirt", the noise in the image is gradually reduced from tim

Llama2-7b

Simply supported beam with distributed loading.

Additional qualitative results for Edges$\rightarrow$Shoes$\times64$, where each pair of consecutive rows displaying the input image in the "Edges" domain and its translation in the "Shoes" domain, respectively.

Performance on a commonsense benchmark (HellaSwag), a linguistic understanding benchmark (SuperGLUE), and the massive multitask test. On previous benchmarks, smaller models start well above random chance levels and exhibit more con

Statistics over locations

The encoder E compresses continuous vectors into a latent embedding \(\bm{z} \in \mathbb{R}^{d_z}\) through \(\bm{z} = \mathcal{E}(\bm{u};\theta_E)\). Quantization then occurs through $K$ residual steps, where each step $k$ selects the closest codebook vector to the current residual as \(\bm{q}^{(k)} = \underset{\bm{c} \in \mathcal{C}^{(k)}}{\arg\min} \|\bm{r}^{(k)} - \bm{c}\|_2\), where \(\bm{r}^{(0)} = \bm{z}\). The Group CSID combines quantization indices into $CSID(\bm{u}) = (i^{(1)}, i^{(2)}, ..., i^{(K)})$, with $i^{(k)}$ denoting the codebook index of \(\bm{q}^{(k)}\). Reconstruction uses decoder D to map the quantized embedding $\bm{\hat{z}} = \sum_{i = 1}^{K} \bm{q}^{(i)}$ back to the input space as \(\hat{\bm{u}} = \mathcal{D}(\bm{\hat{z}};\theta_D)\).
original_answer: where the scaled dot product of query and key matrices is computed by dividing their product by \(\sqrt{d_k}\)\sqrt{d_k}. A causal mask \(\bm{M}\)\bm{M}M — with \(\bm{M}_{ij} = 0\)\bm{M}M_{ij}ij = 0 if \(i \geq j\)i \geq j and \(\bm{M}_{ij} = -\infty\)\bm{M}M_{ij}ij = -\infty if \(i < j\)i < j — is added to enforce autoregressive properties. Softmax is applied to this result, which is then multiplied by the value matrix to generate attention output \(\bm{A}\)\bm{A}A.
This mask prevents future information leakage, ensuring temporal consistency across \(L\)L layers. Each layer’s output is \(\bm{H}^{(l)} = \text{LayerNorm}\left( \bm{A} + \bm{H}^{(l-1)} \right)\)\bm{H}H^{(l)}(l) = \text{LayerNorm}\left( \bm{A}A + \bm{H}H^{(l-1)}(l-1) \right). For the final token at position \(T\)T, its layer-\(l\)l representation is \(\bm{H}^{(l)}_T\)\bm{H}H^{(l)}(l)_T; after \(L\)L layers, \(\bm{H}^{(L)}_T\)\bm{H}H^{(L)}(L)_T encodes the entire sequence’s hierarchical context.
\subsection{RQ-VAE for Group Clustering}
\label{subsec:rq_vae}
Residual-Quantized Variational AutoEncoder (RQ-VAE) \cite{rajput2023recommender} provides hierarchical vector quantization for efficient representation learning. Given user representations \(\bm{u} \in \mathbb{R}^d\)\bm{u}u \in \mathbb{R}^d encoded from long-term sequences, we map them to discrete Group Collaborative Semantic IDs (Group CSID) that capture stable group preferences.
generate_answer: The \textit{Sel3DCraft} pipeline begins by expanding user text input into multiple candidates via LLM. A dual-branch pipeline then generates/retrieves corresponding multi-view images, displayed in an interactive satellite view fo

The generated results at each timestep of the diffusion sampling process from $T$ to $1$. For example, given one prompt case "A man with a beard wearing glasses in blue shirt", the noise in the image is gradually reduced from tim

Llama2-7b

Simply supported beam with distributed loading.

Additional qualitative results for Edges$\rightarrow$Shoes$\times64$, where each pair of consecutive rows displaying the input image in the "Edges" domain and its translation in the "Shoes" domain, respectively.

Performance on a commonsense benchmark (HellaSwag), a linguistic understanding benchmark (SuperGLUE), and the massive multitask test. On previous benchmarks, smaller models start well above random chance levels and exhibit more con

Statistics over locations

The encoder E compresses continuous vectors into a latent embedding \(\bm{z} \in \mathbb{R}^{d_z}\) through \(\bm{z} = \mathcal{E}(\bm{u};\theta_E)\). Quantization then occurs through $K$ residual steps, where each step $k$ selects the closest codebook vector to the current residual as \(\bm{q}^{(k)} = \underset{\bm{c} \in \mathcal{C}^{(k)}}{\arg\min} \|\bm{r}^{(k)} - \bm{c}\|_2\), where \(\bm{r}^{(0)} = \bm{z}\). The Group CSID combines quantization indices into $CSID(\bm{u}) = (i^{(1)}, i^{(2)}, ..., i^{(K)})$, with $i^{(k)}$ denoting the codebook index of \(\bm{q}^{(k)}\). Reconstruction uses decoder D to map the quantized embedding $\bm{\hat{z}} = \sum_{i = 1}^{K} \bm{q}^{(i)}$ back to the input space as \(\hat{\bm{u}} = \mathcal{D}(\bm{\hat{z}};\theta_D)\).

The \textit{Sel3DCraft} pipeline begins by expanding user text input into multiple candidates via LLM. A dual-branch pipeline then generates/retrieves corresponding multi-view images, displayed in an interactive satellite view fo

The generated results at each timestep of the diffusion sampling process from $T$ to $1$. For example, given one prompt case "A man with a beard wearing glasses in blue shirt", the noise in the image is gradually reduced from tim

Llama2-7b

Simply supported beam with distributed loading.

Additional qualitative results for Edges$\rightarrow$Shoes$\times64$, where each pair of consecutive rows displaying the input image in the "Edges" domain and its translation in the "Shoes" domain, respectively.

Performance on a commonsense benchmark (HellaSwag), a linguistic understanding benchmark (SuperGLUE), and the massive multitask test. On previous benchmarks, smaller models start well above random chance levels and exhibit more con

Statistics over locations

The encoder E compresses continuous vectors into a latent embedding \(\bm{z} \in \mathbb{R}^{d_z}\) through \(\bm{z} = \mathcal{E}(\bm{u};\theta_E)\). Quantization then occurs through $K$ residual steps, where each step $k$ selects the closest codebook vector to the current residual as \(\bm{q}^{(k)} = \underset{\bm{c} \in \mathcal{C}^{(k)}}{\arg\min} \|\bm{r}^{(k)} - \bm{c}\|_2\), where \(\bm{r}^{(0)} = \bm{z}\). The Group CSID combines quantization indices into $CSID(\bm{u}) = (i^{(1)}, i^{(2)}, ..., i^{(K)})$, with $i^{(k)}$ denoting the codebook index of \(\bm{q}^{(k)}\). Reconstruction uses decoder D to map the quantized embedding $\bm{\hat{z}} = \sum_{i = 1}^{K} \bm{q}^{(i)}$ back to the input space as \(\hat{\bm{u}} = \mathcal{D}(\bm{\hat{z}};\theta_D)\).
original_answer: where the scaled dot product of query and key matrices is computed by dividing their product by \(\sqrt{d_k}\)\sqrt{d_k}. A causal mask \(\bm{M}\)\bm{M}M — with \(\bm{M}_{ij} = 0\)\bm{M}M_{ij}ij = 0 if \(i \geq j\)i \geq j and \(\bm{M}_{ij} = -\infty\)\bm{M}M_{ij}ij = -\infty if \(i < j\)i < j — is added to enforce autoregressive properties. Softmax is applied to this result, which is then multiplied by the value matrix to generate attention output \(\bm{A}\)\bm{A}A.
This mask prevents future information leakage, ensuring temporal consistency across \(L\)L layers. Each layer’s output is \(\bm{H}^{(l)} = \text{LayerNorm}\left( \bm{A} + \bm{H}^{(l-1)} \right)\)\bm{H}H^{(l)}(l) = \text{LayerNorm}\left( \bm{A}A + \bm{H}H^{(l-1)}(l-1) \right). For the final token at position \(T\)T, its layer-\(l\)l representation is \(\bm{H}^{(l)}_T\)\bm{H}H^{(l)}(l)_T; after \(L\)L layers, \(\bm{H}^{(L)}_T\)\bm{H}H^{(L)}(L)_T encodes the entire sequence’s hierarchical context.
\subsection{RQ-VAE for Group Clustering}
\label{subsec:rq_vae}
Residual-Quantized Variational AutoEncoder (RQ-VAE) \cite{rajput2023recommender} provides hierarchical vector quantization for efficient representation learning. Given user representations \(\bm{u} \in \mathbb{R}^d\)\bm{u}u \in \mathbb{R}^d encoded from long-term sequences, we map them to discrete Group Collaborative Semantic IDs (Group CSID) that capture stable group preferences.

Rouge Score: 0.1037037037037037
SBERT Score: 0.3773398697376251
GPT Evaluation Score: {'Adequacy': 1, 'Coverage': 1, 'Fluency': 3, 'Overall': 1, 'Notes': "Candidate completely diverges from the Reference, discussing unrelated topics (e.g., Sel3DCraft, image processing, benchmarks). Only a small, disconnected section (encoder, quantization, and decoder process) vaguely relates to the Reference's context (vector quantization for representation learning), but lacks coherence with the Reference's specific content."}
