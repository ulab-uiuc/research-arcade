\section{Evaluating \envname via Masked Node Prediction Task}
\label{evaluation}
Utilizing graph structures not only enables the design of the research simulation algorithm but also provides a natural way to evaluate it. As we show next, we propose to view research evaluation as a masked node prediction task, including evaluation for both paper writing and review writing.


\xhdr{Evaluation by masked node prediction} A masked node prediction task in the community graph $\mathcal{G}$ can be defined as first masking a specific node $v \in \mathcal{V}$ in the community graph by setting its hidden states $\mb{h}_v = \emptyset$, where the original hidden state is saved as $\mb{h}_v^*$; then an ideal model should be able to predict the hidden states $\mb{h}_v^*$ of the masked node from its neighborhood $\mathcal{N}(v)$. Concretely, in Equation \ref{paper_writing}, the output $\mb{h}_v$ can be regarded as the masked node prediction for evaluation of paper writing, suppose that the node $v$ is a masked version of a ground truth data node. Similarly, in Equation \ref{review_writing}, the output $\mb{r}_v$ can be regarded as the predicted node attributes for review writing, where the original review is represented as $\mb{r}_v^*$.
In general, we have:\\

\vspace{-8mm}
\begingroup
\small
\begin{equation}
\begin{split}
\mb{h}_v, \mb{r}_v &= \textsc{ResearchTown}\Big(
    \mathcal{G}(\mathcal{V}, \mathcal{E}); \{\mb{x}_u \mid u \in \mathcal{N}(v)\}; v
\Big)
\end{split}
\end{equation}
\endgroup
where $\mb{h}_v$ is the text-form hidden states of a masked node $v$ and  $\mb{r}_v$ is the text-form prediction output of a masked node $v$. Since we have real-world results for both paper writing and review, we treat them as ground truth even though they are not perfect because the goal of \envname is to simulate the human research community rather than to find optimal solutions for papers and reviews ($\mb{h}_v^*$ for paper ground-truth and $\mb{r}_v^*$ for review ground-truth) and we can systematically evaluate both processes to check the effectiveness of our simulation algorithm. More specifically, since we have access to ground-truth papers $\mb{h}_v^*$ when evaluating the review writing simulation, to avoid accumulated errors, we update Equation \ref{review_writing} during evaluation so that reviews $\mb{r}_v$ are generated based on $\mb{h}_v^*$, instead of $\mb{h}_v$:

\vspace{-6mm}
\begingroup
\small
\begin{equation}
\begin{split}
\mb{r}_{v} &= \textsc{AGG}\Big(
    \mb{h}_v^*, \big\{\mb{h}_d \mid (v,d) \in \mathcal{E}_{dd}\big\}
    , \big\{f_a(\cdot), \mb{h}_a \mid (v,a) \in \mathcal{E}_{ad}\big\}
\Big)
\end{split}
\end{equation}
\endgroup
\xhdr{Evaluation metric} We utilize state-of-the-art embedding models like text-embedding-large-3~\footnote{\url{https://openai.com/index/new-embedding-models-and-api-updates/}} to build distance function for $d_p(\mb{h}_v, \mb{h}_v^*)$ and $d_r(\mb{r}_v, \mb{r}_v^*)$. More details related to formal embedding-based metric definitions for paper writing and review writing tasks are available in Appendix~\S\ref{evaluation-details}.