\section{Experimental Settings}



\xhdr{\envname setting}
We utilize GPT-4o-mini~\footnote{We point to \texttt{GPT-4o-mini-2024-07-18} for use.} as the LLM backbone for implementing the agent functions, with the decoding temperature set to \( 0 \) to ensure reproducibility. To evaluate different aggregation strategies, we conduct experiments using specific types of nodes connected to the target node: (1) \textit{AGG-self}, where the aggregation relies solely on the target node; (2) \textit{AGG-agent}, which includes the target node and its neighboring agent nodes; (3) \textit{AGG-data}, which involves the target node and its neighboring data nodes; and (4) \textit{AGG-global}, which incorporates the target node and all its neighboring nodes, including agent and data nodes. We specifically refer to \textit{AGG-global} as our proposed \envname method for simulation, while the others serve as baselines. This experimental design enables a systematic comparison of the effects of different neighborhood information on the aggregation process. More details about different settings are available in Appendix~\S\ref{agg-setting-implementation}.
\label{researchtown-setting}

\xhdr{\benchname setting} To evaluate \envname for research simulation, we introduce \benchname, which consists of 1,000 paper writing tasks and 200 review writing tasks. All tasks are sourced from recent top-tier machine learning conferences such as NeurIPS 2024~\footnote{\url{https://neurips.cc/Conferences/2024}} and ICLR 2024~\footnote{\url{https://openreview.net/group?id=ICLR.cc/2024/Conference}}. Since most papers are released after the cutoff date of GPT-4o-mini, information leakage is not considered an issue. For paper writing tasks, we categorize them into three difficulty levels—\textit{hard} (333 tasks), \textit{medium} (334 tasks), and \textit{easy} (333 tasks)—based on the similarity results of data-only aggregation. Specifically, for review writing tasks, the reviewers prepared for each paper are selected from the top 5 researchers most related to the paper, as reviewer information is not publicly available in the real world. More details about the data collection and prevention of information leakage during simulation are in Appendix~\S\ref{research-bench-tech-details}.
\label{researchbench-setting}
