{"title": "A Bit of Freedom Goes a Long Way: Classical and Quantum Algorithms for Reinforcement Learning under a Generative Model", "author": "Andris Ambainis", "abstract": "\\begin{abstract}\nWe propose novel classical and quantum online algorithms for learning finite-horizon and infinite-horizon average-reward Markov Decision Processes (MDPs). Our algorithms are based on a hybrid exploration-generative reinforcement learning (RL) model wherein the agent can, from time to time, freely interact with the environment in a generative sampling fashion, i.e., by having access to a ``simulator''. By employing known classical and new quantum algorithms for approximating optimal policies under a generative model within our learning algorithms, we show that it is possible to avoid several paradigms from RL like ``optimism in the face of uncertainty'' and ``posterior sampling'' and instead compute and use optimal policies directly, which yields better regret bounds compared to previous works. For finite-horizon MDPs, our quantum algorithms obtain regret bounds which only depend logarithmically on the number of time steps $T$, thus breaking the $O(\\sqrt{T})$ classical barrier. This matches the time dependence of the prior quantum works of Ganguly et al.~(arXiv'23) and Zhong et al.~(ICML'24), but with improved dependence on other parameters like state space size $S$ and action space size $A$. For infinite-horizon MDPs, our classical and quantum bounds still maintain the $O(\\sqrt{T})$ dependence but with better $S$ and $A$ factors. Nonetheless, we propose a novel measure of regret for infinite-horizon MDPs with respect to which our quantum algorithms have $\\poly\\log{T}$ regret, exponentially better compared to classical algorithms. Finally, we generalise all of our results to compact state spaces.\n\\end{abstract}", "citations": {"sutton1998reinforcement": {"bib_key": "sutton1998reinforcement", "bib_title": "Reinforcement learning: An introduction.", "bib_author ": "Richard~S Sutton and Andrew~G Barto.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Reinforcement learning~\\cite{sutton1998reinforcement}", "next_context": "is a subfield of machine learning that studies how an agent can properly interact with a dynamical environment in order to maximise some type of reward."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The practical utility of MDPs lies in their ability to model decision-making in complex environments~\\cite{aastrom1965optimal, hu2007markov,sato2010markov,  bauerle2011markov, feinberg2012handbook, bennett2013artificial, chen2014distributed,steimle2017markov, natarajan2022planning}, allowing successful applications to several problems~\\cite{sutton1998reinforcement,Szepesvari2010algorithms,bertsekas2012dynamic,bertsekas2022abstract}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "The main interest is thus on the performance of the computed policy~\\cite{sutton1998reinforcement,Kearns1999finite,Kearns2002near}", "next_context": "."}], "importance_score": 1.5833333333333333}, "puterman2014markov": {"bib_key": "puterman2014markov", "bib_title": "Markov decision processes: discrete stochastic dynamic programming.", "bib_author ": "Martin~L Puterman.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Markov decision processes (MDPs)~\\cite{puterman2014markov}", "next_context": "serve as the most commonly used framework for modeling such agent-environment interactions."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "There are several types of MDPs in the literature~\\cite{puterman2014markov}", "next_context": ", the most common ones being\\emph{finite-horizon}MDPs,\\emph{infinite-horizon discounted}MDPs, and\\emph{infinite-horizon undiscounted}MDPs."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "One of the main problems associated with MDPs is that of computing\\varepsilon-optimal policies, for which several algorithms have been proposed~\\cite{ bellman1966dynamic,watkins1992q,meyn1997policy, kearns1998finite, sutton1999policy,lagoudakis2003least,bertsekas2011approximate,azar2012dynamic,puterman2014markov,silver2014deterministic, schulman2015trust,  jang2019q}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "The authors quantised the standard value iteration~\\cite{puterman2014markov}", "next_context": "and the modern value iteration algorithms from~\\cite{sidford2018near,li2020breaking}using quantum techniques like quantum minimum finding~\\cite{durr1996quantum}and quantum mean estimation~\\cite{brassard2002quantum,montanaro2015quant}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "Our quantum algorithms are quantised versions of the standard value iteration~\\cite{puterman2014markov}", "next_context": "and the modern value iteration algorithms from~\\cite{sidford2018near,li2020breaking}for finite-horizon MDPs, similarly in spirit to what Wang\\emph{et al.}~\\cite{wang2021quantum}did on infinite-horizon discounted MDPs."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "In order to present a quantum algorithm in this setting, we opted to adapt a simple classical value iteration~\\cite{puterman2014markov}", "next_context": "to the generative model with oracle\\mathcal{C}_pand assumed that the underlying infinite-horizon MDP has some contractive properties (see\\Cref{sec:value_iteration})."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "Our results apply to a broad class of infinite-horizon MDPs called\\emph{weakly communicating}(see\\Cref{sec:mdp}or~\\cite[Section~8.3.1]{puterman2014markov}", "next_context": ")."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Once again, the algorithm of Auer, Jaksch, and Ortner~\\cite{auer2008near}ranked among the main ones to be first proposed, although it applied to a smaller class of MDPs called\\emph{communicating}~\\cite{puterman2014markov}", "next_context": "."}, {"section": "Preliminaries", "subsection": "Background on Markov decision processes", "subsubsection": null, "prev_context": "In this work, we shall consider\\emph{weakly communicating}infinite-horizon MDPs~\\cite[Section~8.3]{puterman2014markov}", "next_context": "(there is no need to constrain the class of finite-horizon MDPs)."}, {"section": "Preliminaries", "subsection": "Background on Markov decision processes", "subsubsection": "Finite-horizon MDPs", "prev_context": "The operator\\mathcal{L}is monotonic, i.e.,u\\leqv\\implies\\mathcal{L}u\\leq\\mathcal{L}v, and is non-expansive, i.e.,\\operatornamesp(\\mathcal{L}u -\\mathcal{L}v)\\leq\\operatornamesp(u - v)and\\|\\mathcal{L}u -\\mathcal{L}v\\|_\\infty\\leq\\|u - v\\|_\\inftyfor allu,v\\in\\mathscr{B}(\\mathcal{X})~\\cite[Proposition~6.2.4]{puterman2014markov}", "next_context": "."}, {"section": "Preliminaries", "subsection": "Background on Markov decision processes", "subsubsection": "Finite-horizon MDPs", "prev_context": "~8.5.3]{puterman2014markov}", "next_context": "."}, {"section": "Preliminaries", "subsection": "Background on Markov decision processes", "subsubsection": "Finite-horizon MDPs", "prev_context": "It is well known~\\cite[Theorem~4.5.1]{puterman2014markov}", "next_context": "that any set of solutions\\{u_t\\}_t\\in[H]\\subset\\mathscr{B}(\\mathcal{X})to the optimality equations are such thatu_t(x) = V_t^\\ast(x)\\forallx\\in\\mathcal{X},t\\in[H]."}, {"section": "Our reinforcement learning model", "subsection": null, "subsubsection": null, "prev_context": "We note that, when the MDP is unichain (see~\\cite[Section~8.3]{puterman2014markov}", "next_context": "), the average reward of any stationary policy is constant, and therefore the minimum over\\mathcal{X}can be dropped from the definition of\\operatornameRegret_\\infty^\\mathbb{E}(T)."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Then one can prove thatu_t(x) = V_t^\\pi(x) = V_t^\\ast(x)for allx\\in\\mathcal{X}, meaning thatu_tis the optimal total expected reward from timetonward and\\piis an optimal policy~\\cite[Theorem~4.5.1]{puterman2014markov}", "next_context": "."}, {"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "It is known that when\\operatornamesp(u_t+1- u_t)\\leq\\varepsilon, the greedy policy with respect tou_tis\\varepsilon-optimal~\\cite[Theorem~9.4.5]{puterman2014markov}", "next_context": "."}, {"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "The next result generalises~\\cite[Theorem~8.5.6]{puterman2014markov}", "next_context": "."}], "importance_score": 15.083333333333334}, "hu2007markov": {"bib_key": "hu2007markov", "bib_title": "Markov decision processes with their applications, volume~14.", "bib_author ": "Qiying Hu and Wuyi Yue.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The practical utility of MDPs lies in their ability to model decision-making in complex environments~\\cite{aastrom1965optimal, hu2007markov,sato2010markov,  bauerle2011markov, feinberg2012handbook, bennett2013artificial, chen2014distributed,steimle2017markov, natarajan2022planning}", "next_context": ", allowing successful applications to several problems~\\cite{sutton1998reinforcement,Szepesvari2010algorithms,bertsekas2012dynamic,bertsekas2022abstract}."}], "importance_score": 0.1111111111111111}, "sato2010markov": {"bib_key": "sato2010markov", "bib_title": "\\'e", "bib_author ": "Renato~Cesar Sato and D", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The practical utility of MDPs lies in their ability to model decision-making in complex environments~\\cite{aastrom1965optimal, hu2007markov,sato2010markov,  bauerle2011markov, feinberg2012handbook, bennett2013artificial, chen2014distributed,steimle2017markov, natarajan2022planning}", "next_context": ", allowing successful applications to several problems~\\cite{sutton1998reinforcement,Szepesvari2010algorithms,bertsekas2012dynamic,bertsekas2022abstract}."}], "importance_score": 0.1111111111111111}, "bauerle2011markov": {"bib_key": "bauerle2011markov", "bib_title": "\\\"a", "bib_author ": "Nicole B", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The practical utility of MDPs lies in their ability to model decision-making in complex environments~\\cite{aastrom1965optimal, hu2007markov,sato2010markov,  bauerle2011markov, feinberg2012handbook, bennett2013artificial, chen2014distributed,steimle2017markov, natarajan2022planning}", "next_context": ", allowing successful applications to several problems~\\cite{sutton1998reinforcement,Szepesvari2010algorithms,bertsekas2012dynamic,bertsekas2022abstract}."}], "importance_score": 0.1111111111111111}, "feinberg2012handbook": {"bib_key": "feinberg2012handbook", "bib_title": "Handbook of Markov decision processes: methods and applications, volume~40.", "bib_author ": "Eugene~A Feinberg and Adam Shwartz.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The practical utility of MDPs lies in their ability to model decision-making in complex environments~\\cite{aastrom1965optimal, hu2007markov,sato2010markov,  bauerle2011markov, feinberg2012handbook, bennett2013artificial, chen2014distributed,steimle2017markov, natarajan2022planning}", "next_context": ", allowing successful applications to several problems~\\cite{sutton1998reinforcement,Szepesvari2010algorithms,bertsekas2012dynamic,bertsekas2022abstract}."}], "importance_score": 0.1111111111111111}, "bennett2013artificial": {"bib_key": "bennett2013artificial", "bib_title": "Artificial intelligence framework for simulating clinical decision-making: A Markov decision process approach.", "bib_author ": "Casey~C Bennett and Kris Hauser.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The practical utility of MDPs lies in their ability to model decision-making in complex environments~\\cite{aastrom1965optimal, hu2007markov,sato2010markov,  bauerle2011markov, feinberg2012handbook, bennett2013artificial, chen2014distributed,steimle2017markov, natarajan2022planning}", "next_context": ", allowing successful applications to several problems~\\cite{sutton1998reinforcement,Szepesvari2010algorithms,bertsekas2012dynamic,bertsekas2022abstract}."}], "importance_score": 0.1111111111111111}, "chen2014distributed": {"bib_key": "chen2014distributed", "bib_title": "Distributed autonomous virtual resource management in datacenters using finite-Markov decision process.", "bib_author ": "Liuhua Chen, Haiying Shen, and Karan Sapra.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The practical utility of MDPs lies in their ability to model decision-making in complex environments~\\cite{aastrom1965optimal, hu2007markov,sato2010markov,  bauerle2011markov, feinberg2012handbook, bennett2013artificial, chen2014distributed,steimle2017markov, natarajan2022planning}", "next_context": ", allowing successful applications to several problems~\\cite{sutton1998reinforcement,Szepesvari2010algorithms,bertsekas2012dynamic,bertsekas2022abstract}."}], "importance_score": 0.1111111111111111}, "steimle2017markov": {"bib_key": "steimle2017markov", "bib_title": "Markov decision processes for screening and treatment of chronic diseases.", "bib_author ": "Lauren~N Steimle and Brian~T Denton.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The practical utility of MDPs lies in their ability to model decision-making in complex environments~\\cite{aastrom1965optimal, hu2007markov,sato2010markov,  bauerle2011markov, feinberg2012handbook, bennett2013artificial, chen2014distributed,steimle2017markov, natarajan2022planning}", "next_context": ", allowing successful applications to several problems~\\cite{sutton1998reinforcement,Szepesvari2010algorithms,bertsekas2012dynamic,bertsekas2022abstract}."}], "importance_score": 0.1111111111111111}, "natarajan2022planning": {"bib_key": "natarajan2022planning", "bib_title": "Planning with Markov decision processes: An AI perspective.", "bib_author ": "Mausam Natarajan and Andrey Kolobov.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The practical utility of MDPs lies in their ability to model decision-making in complex environments~\\cite{aastrom1965optimal, hu2007markov,sato2010markov,  bauerle2011markov, feinberg2012handbook, bennett2013artificial, chen2014distributed,steimle2017markov, natarajan2022planning}", "next_context": ", allowing successful applications to several problems~\\cite{sutton1998reinforcement,Szepesvari2010algorithms,bertsekas2012dynamic,bertsekas2022abstract}."}], "importance_score": 0.1111111111111111}, "Szepesvari2010algorithms": {"bib_key": "Szepesvari2010algorithms", "bib_title": "Algorithms for Reinforcement Learning.", "bib_author ": "Csaba Szepesv\u00e1ri.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The practical utility of MDPs lies in their ability to model decision-making in complex environments~\\cite{aastrom1965optimal, hu2007markov,sato2010markov,  bauerle2011markov, feinberg2012handbook, bennett2013artificial, chen2014distributed,steimle2017markov, natarajan2022planning}, allowing successful applications to several problems~\\cite{sutton1998reinforcement,Szepesvari2010algorithms,bertsekas2012dynamic,bertsekas2022abstract}", "next_context": "."}], "importance_score": 0.25}, "bertsekas2012dynamic": {"bib_key": "bertsekas2012dynamic", "bib_title": "Dynamic programming and optimal control: Volume I, volume~4.", "bib_author ": "Dimitri Bertsekas.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The practical utility of MDPs lies in their ability to model decision-making in complex environments~\\cite{aastrom1965optimal, hu2007markov,sato2010markov,  bauerle2011markov, feinberg2012handbook, bennett2013artificial, chen2014distributed,steimle2017markov, natarajan2022planning}, allowing successful applications to several problems~\\cite{sutton1998reinforcement,Szepesvari2010algorithms,bertsekas2012dynamic,bertsekas2022abstract}", "next_context": "."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": "Classical approximate backward induction algorithm", "subsubsection": null, "prev_context": "In general, an\\epsilon-optimal value function yields an2\\epsilon/H-optimal greedy policy in the worst case~\\cite{Singh1994upper,bertsekas2012dynamic}", "next_context": "."}], "importance_score": 0.75}, "bertsekas2022abstract": {"bib_key": "bertsekas2022abstract", "bib_title": "Abstract dynamic programming.", "bib_author ": "Dimitri Bertsekas.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The practical utility of MDPs lies in their ability to model decision-making in complex environments~\\cite{aastrom1965optimal, hu2007markov,sato2010markov,  bauerle2011markov, feinberg2012handbook, bennett2013artificial, chen2014distributed,steimle2017markov, natarajan2022planning}, allowing successful applications to several problems~\\cite{sutton1998reinforcement,Szepesvari2010algorithms,bertsekas2012dynamic,bertsekas2022abstract}", "next_context": "."}], "importance_score": 0.25}, "Biamonte2017quantum": {"bib_key": "Biamonte2017quantum", "bib_title": "Quantum machine learning.", "bib_author ": "Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Parallel to reinforcement learning developments, quantum machine learning~\\cite{Biamonte2017quantum}", "next_context": ", a subfield of quantum computation~\\cite{nielsen2010quantum}, has been consolidated as an area of study that can provide quantum speed-ups to several traditional machine learning problems~\\cite{lloyd2013quantum,rebentrost2014quantum,kerenidis2017quantum,kerenidis2019qmeans,doriguello2023you}, including reinforcement learning~\\cite{dong2008quantum}."}], "importance_score": 1.0}, "nielsen2010quantum": {"bib_key": "nielsen2010quantum", "bib_title": "Quantum computation and quantum information.", "bib_author ": "Michael~A Nielsen and Isaac~L Chuang.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Parallel to reinforcement learning developments, quantum machine learning~\\cite{Biamonte2017quantum}, a subfield of quantum computation~\\cite{nielsen2010quantum}", "next_context": ", has been consolidated as an area of study that can provide quantum speed-ups to several traditional machine learning problems~\\cite{lloyd2013quantum,rebentrost2014quantum,kerenidis2017quantum,kerenidis2019qmeans,doriguello2023you}, including reinforcement learning~\\cite{dong2008quantum}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "This means that the agent is free to prepare any quantum state and have the environment apply\\mathcal{O}_p(or its inverse) onto such quantum state, plus any quantum gate from a universal gate set~\\cite{nielsen2010quantum}", "next_context": "."}, {"section": "Preliminaries", "subsection": "Background on quantum computation", "subsubsection": null, "prev_context": "Little background on quantum computation is needed for our paper and we refer the reader to~\\cite{nielsen2010quantum}", "next_context": "for more information."}], "importance_score": 3.0}, "lloyd2013quantum": {"bib_key": "lloyd2013quantum", "bib_title": "Quantum algorithms for supervised and unsupervised machine learning.", "bib_author ": "Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Parallel to reinforcement learning developments, quantum machine learning~\\cite{Biamonte2017quantum}, a subfield of quantum computation~\\cite{nielsen2010quantum}, has been consolidated as an area of study that can provide quantum speed-ups to several traditional machine learning problems~\\cite{lloyd2013quantum,rebentrost2014quantum,kerenidis2017quantum,kerenidis2019qmeans,doriguello2023you}", "next_context": ", including reinforcement learning~\\cite{dong2008quantum}."}], "importance_score": 0.2}, "rebentrost2014quantum": {"bib_key": "rebentrost2014quantum", "bib_title": "Quantum support vector machine for big data classification.", "bib_author ": "Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Parallel to reinforcement learning developments, quantum machine learning~\\cite{Biamonte2017quantum}, a subfield of quantum computation~\\cite{nielsen2010quantum}, has been consolidated as an area of study that can provide quantum speed-ups to several traditional machine learning problems~\\cite{lloyd2013quantum,rebentrost2014quantum,kerenidis2017quantum,kerenidis2019qmeans,doriguello2023you}", "next_context": ", including reinforcement learning~\\cite{dong2008quantum}."}], "importance_score": 0.2}, "kerenidis2017quantum": {"bib_key": "kerenidis2017quantum", "bib_title": "Quantum Recommendation Systems.", "bib_author ": "Iordanis Kerenidis and Anupam Prakash.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Parallel to reinforcement learning developments, quantum machine learning~\\cite{Biamonte2017quantum}, a subfield of quantum computation~\\cite{nielsen2010quantum}, has been consolidated as an area of study that can provide quantum speed-ups to several traditional machine learning problems~\\cite{lloyd2013quantum,rebentrost2014quantum,kerenidis2017quantum,kerenidis2019qmeans,doriguello2023you}", "next_context": ", including reinforcement learning~\\cite{dong2008quantum}."}], "importance_score": 0.2}, "kerenidis2019qmeans": {"bib_key": "kerenidis2019qmeans", "bib_title": "q-means: A quantum algorithm for unsupervised machine learning.", "bib_author ": "Iordanis Kerenidis, Jonas Landman, Alessandro Luongo, and Anupam Prakash.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Parallel to reinforcement learning developments, quantum machine learning~\\cite{Biamonte2017quantum}, a subfield of quantum computation~\\cite{nielsen2010quantum}, has been consolidated as an area of study that can provide quantum speed-ups to several traditional machine learning problems~\\cite{lloyd2013quantum,rebentrost2014quantum,kerenidis2017quantum,kerenidis2019qmeans,doriguello2023you}", "next_context": ", including reinforcement learning~\\cite{dong2008quantum}."}], "importance_score": 0.2}, "doriguello2023you": {"bib_key": "doriguello2023you", "bib_title": "\\~a", "bib_author ": "Jo", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Parallel to reinforcement learning developments, quantum machine learning~\\cite{Biamonte2017quantum}, a subfield of quantum computation~\\cite{nielsen2010quantum}, has been consolidated as an area of study that can provide quantum speed-ups to several traditional machine learning problems~\\cite{lloyd2013quantum,rebentrost2014quantum,kerenidis2017quantum,kerenidis2019qmeans,doriguello2023you}", "next_context": ", including reinforcement learning~\\cite{dong2008quantum}."}], "importance_score": 0.2}, "dong2008quantum": {"bib_key": "dong2008quantum", "bib_title": "Quantum reinforcement learning.", "bib_author ": "Daoyi Dong, Chunlin Chen, Hanxiong Li, and Tzyh-Jong Tarn.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Parallel to reinforcement learning developments, quantum machine learning~\\cite{Biamonte2017quantum}, a subfield of quantum computation~\\cite{nielsen2010quantum}, has been consolidated as an area of study that can provide quantum speed-ups to several traditional machine learning problems~\\cite{lloyd2013quantum,rebentrost2014quantum,kerenidis2017quantum,kerenidis2019qmeans,doriguello2023you}, including reinforcement learning~\\cite{dong2008quantum}", "next_context": "."}], "importance_score": 1.0}, "ganguly2023quantum": {"bib_key": "ganguly2023quantum", "bib_title": "Quantum computing provides exponential regret improvement in episodic reinforcement learning.", "bib_author ": "Bhargav Ganguly, Yulian Wu, Di~Wang, and Vaneet Aggarwal.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "It has been found that quantum algorithms are not only capable of achieving speedup in the time complexity of certain tasks, but also have the potential to be better``learners''than their classical counterparts in the online setting~\\cite{ganguly2023quantum,zhong2023provably}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": null, "prev_context": "For that, we propose a hybrid generative online-learning model in which classical and quantum computers are capable of offering an improved learning progress compared to several previous works~\\cite{auer2008near,bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient,ganguly2023quantum,zhong2023provably}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "We are actually only aware of the works of Zhong\\emph{et al.}~\\cite{zhong2023provably}and Ganguly\\emph{et al.}~\\cite{ganguly2023quantum}", "next_context": ", both for finite-horizon finite-state-space MDPs (there are further works on multi-armed bandits~\\cite{Lumbreras2022multiarmedquantum,dai2023quantum,wan2023quantum,su2025quantum})."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "~\\cite{zhong2023provably,ganguly2023quantum}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Several RL algorithms~\\cite{auer2008near,bartlett2009regal,ortner2012online,lakshmanan2015improved,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,zhong2023provably,ganguly2023quantum}", "next_context": ", implement the principle of \u201coptimism in the face of uncertainty\u201d~\\cite{brafman2002r}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{ganguly2023quantum}", "next_context": ""}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Several regret bounds from\\Cref{res:res3,res:res4}considerably improve upon prior works~\\cite{auer2008near,azar2017minimax,zanette2019tighter,ortner2012online,lakshmanan2015improved,zhong2023provably,ganguly2023quantum}", "next_context": "(see\\Cref{table:results_finite-horizon,table:results_infinite-horizon}for a clear comparison)."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "The quantum bound\\widetilde{O}(S\\min\\{HA,H^2\\sqrt{A}\\})is quadratically better inS,A,Hcompared to~\\cite{zhong2023provably,ganguly2023quantum}", "next_context": "and maintains the logarithmic dependence onT."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "A striking feature of\\Cref{res:res3,res:res4}is that, just like~\\cite{zhong2023provably,ganguly2023quantum}", "next_context": ", the quantum regret bounds for finite-horizon MDPs are exponentially better in the number of stepsT, but the same is not true for infinite-horizon MDPs, where the term\\widetilde{O}(\\Lambda\\sqrt{T})hinders such an exponential advantage."}], "importance_score": 4.385714285714286}, "zhong2023provably": {"bib_key": "zhong2023provably", "bib_title": "Provably efficient exploration in quantum reinforcement learning with logarithmic worst-case regret.", "bib_author ": "Han Zhong, Jiachen Hu, Yecheng Xue, Tongyang Li, and Liwei Wang.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "It has been found that quantum algorithms are not only capable of achieving speedup in the time complexity of certain tasks, but also have the potential to be better``learners''than their classical counterparts in the online setting~\\cite{ganguly2023quantum,zhong2023provably}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": null, "prev_context": "For that, we propose a hybrid generative online-learning model in which classical and quantum computers are capable of offering an improved learning progress compared to several previous works~\\cite{auer2008near,bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient,ganguly2023quantum,zhong2023provably}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "In the quantum setting, one has access to an oracle\\mathcal{O}_pcalled quantum accessible-environment~\\cite{wang2021quantum,wiedemann2022quantum, jerbi2022quantum, zhong2023provably}", "next_context": "which is a unitary operator that acts~as\\label{eq:quantum_sampling_oracle}\\mathcal O_p:\\ket{x}\\ket{a}\\ket{\\bar 0}\\rightarrow\\int_x'\\in\\mathcal X\\sqrt{p(\\rd x'\\vert x, a)}\\ket{x}\\ket{a}\\ket{x'}\\quad\\text{for all}\\quad(x,a)\\in\\mathcal{X}\\times\\mathcal{A}(we postpone a more formal introduction to quantum computation to\\Cref{sec:quantum_computation})."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "We are actually only aware of the works of Zhong\\emph{et al.}~\\cite{zhong2023provably}", "next_context": "and Ganguly\\emph{et al.}~\\cite{ganguly2023quantum}, both for finite-horizon finite-state-space MDPs (there are further works on multi-armed bandits~\\cite{Lumbreras2022multiarmedquantum,dai2023quantum,wan2023quantum,su2025quantum})."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "~\\cite{zhong2023provably,ganguly2023quantum}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Several RL algorithms~\\cite{auer2008near,bartlett2009regal,ortner2012online,lakshmanan2015improved,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,zhong2023provably,ganguly2023quantum}", "next_context": ", implement the principle of \u201coptimism in the face of uncertainty\u201d~\\cite{brafman2002r}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{zhong2023provably}", "next_context": ""}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Several regret bounds from\\Cref{res:res3,res:res4}considerably improve upon prior works~\\cite{auer2008near,azar2017minimax,zanette2019tighter,ortner2012online,lakshmanan2015improved,zhong2023provably,ganguly2023quantum}", "next_context": "(see\\Cref{table:results_finite-horizon,table:results_infinite-horizon}for a clear comparison)."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "The quantum bound\\widetilde{O}(S\\min\\{HA,H^2\\sqrt{A}\\})is quadratically better inS,A,Hcompared to~\\cite{zhong2023provably,ganguly2023quantum}", "next_context": "and maintains the logarithmic dependence onT."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "A striking feature of\\Cref{res:res3,res:res4}is that, just like~\\cite{zhong2023provably,ganguly2023quantum}", "next_context": ", the quantum regret bounds for finite-horizon MDPs are exponentially better in the number of stepsT, but the same is not true for infinite-horizon MDPs, where the term\\widetilde{O}(\\Lambda\\sqrt{T})hinders such an exponential advantage."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "~\\cite{zhong2023provably}", "next_context": "."}, {"section": "Our reinforcement learning model", "subsection": "Quantum exploration-generative learning model", "subsubsection": null, "prev_context": "Such interaction is done in a generative fashion~\\cite{kearns1998finite,kakade2003sample}via quantum-accessible environments~\\cite{wiedemann2022quantum, wang2021quantum, jerbi2022quantum, zhong2023provably}", "next_context": ", i.e., through a quantum sampling oracle for the transition probabilities and a quantum reward oracle defined below."}, {"section": "Our reinforcement learning model", "subsection": "The reinforcement learning model of Zhong et al.", "subsubsection": null, "prev_context": "In~\\cite{zhong2023provably}", "next_context": ", Zhong\\emph{et al.}\\proposed quantum RL algorithms for  finite-horizon finite-state-space MDPs."}], "importance_score": 6.885714285714286}, "auer2008near": {"bib_key": "auer2008near", "bib_title": "Near-optimal regret bounds for reinforcement learning.", "bib_author ": "Peter Auer, Thomas Jaksch, and Ronald Ortner.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": null, "prev_context": "For that, we propose a hybrid generative online-learning model in which classical and quantum computers are capable of offering an improved learning progress compared to several previous works~\\cite{auer2008near,bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient,ganguly2023quantum,zhong2023provably}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "The task of obtaining large rewards is usually reframed as minimising some measure of how far the agent is from being optimal~\\cite{valiant1984theory,Littlestone1988learning,li2008knows,auer2008near}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "For finite-horizon finite-state-space MDPs, one of the main early algorithms was proposed by Auer, Jaksch, and Ortner~\\cite{auer2008near}", "next_context": ", which achieved a regret of\\widetilde{O}(\\sqrt{H^2S^2 AT})."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\footnote{Although \\cite{auer2008near} originally worked in the infinite-horizon setting, it is not hard to adapt their algorithm to the finite-horizon case.}Auer, Jaksch, and Ortner~\\cite{auer2008near}", "next_context": "also proved the regret lower bound\\Omega(\\sqrt{HSAT})."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Once again, the algorithm of Auer, Jaksch, and Ortner~\\cite{auer2008near}", "next_context": "ranked among the main ones to be first proposed, although it applied to a smaller class of MDPs called\\emph{communicating}~\\cite{puterman2014markov}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Later, Ortner and Ryabko~\\cite{ortner2012online}and Lakshmanan, Ortner, and Ryabko~\\cite{lakshmanan2015improved}adapted the algorithm from~\\cite{auer2008near}", "next_context": "for the case when the state space is\\mathcal{X}=[0,1]^Dunder the assumption thatrandpare H\\\"older continuous."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "In all the classical works mentioned above~\\cite{auer2008near,azar2017minimax,zanette2019tighter,bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient}", "next_context": ", the way the agent interacts with the environment and accumulates regret is straightforward: once again, for fear of being repetitious, the agent chooses an actiona_tat some environment's statex_tand obtains a rewardr(x_t,a_t), after which the environment's state transitions tox_t+1."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Several RL algorithms~\\cite{auer2008near,bartlett2009regal,ortner2012online,lakshmanan2015improved,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,zhong2023provably,ganguly2023quantum}", "next_context": ", implement the principle of \u201coptimism in the face of uncertainty\u201d~\\cite{brafman2002r}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{auer2008near}", "next_context": ""}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Several regret bounds from\\Cref{res:res3,res:res4}considerably improve upon prior works~\\cite{auer2008near,azar2017minimax,zanette2019tighter,ortner2012online,lakshmanan2015improved,zhong2023provably,ganguly2023quantum}", "next_context": "(see\\Cref{table:results_finite-horizon,table:results_infinite-horizon}for a clear comparison)."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{auer2008near}", "next_context": ""}, {"section": "Our reinforcement learning model", "subsection": null, "subsubsection": null, "prev_context": "The infinite-horizon in-path regret\\operatornameRegret_\\infty^\\rmpath(T)is the standard choice for most prior works on RL~\\cite{auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,ortner2012online,fruit2018efficient}", "next_context": "and takes into account the actual path of observed state-action pairs(x_t,a_t)during learning, while the infinite-horizon expected regret\\operatornameRegret_\\infty^\\mathbb{E}(T)is a novel measure of regret that we introduce here."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Several RL algorithms~\\cite{auer2006logarithmic,bartlett2009regal,auer2008near,ortner2012online,lakshmanan2015improved,fruit2018efficient}", "next_context": "compute approximate optimal policies for the target unknown MDPMgiven empirical estimates of the true stochastic kernelspand rewardsr."}, {"section": "Algorithms for online learning of finite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Like several classical RL algorithms for finite-horizon MDPs~\\cite{Kearns2002near,brafman2002r,strehl2006pac,auer2008near,dann2017unifying}", "next_context": ", they proceed in episodes."}, {"section": "Algorithms for online learning of finite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Unlike prior RL algorithms for finite-horizon MDPs~\\cite{Kearns2002near,brafman2002r,strehl2006pac,auer2008near,dann2017unifying,wang2021quantum}", "next_context": ",\\cref{algo:quantum_UCCRL_finite-horizon}does not keep an approximation\\widetilde{p}for the true stochastic kernelpvia the state-action pairs(x_t,a_t)observed during exploration phases and therefore does not employ\\widetilde{p}to calculate approximate optimal policies."}, {"section": "Algorithms for online learning of infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Like several classical RL algorithms~\\cite{ortner2012online,auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,fruit2018efficient}", "next_context": ", they proceed in episodes, during each of which a chosen policy remains fixed."}, {"section": "Algorithms for online learning of infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Unlike prior classical RL algorithms~\\cite{ortner2012online,auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,fruit2018efficient}", "next_context": ",\\cref{algo:quantum_UCCRL}does not keep estimates of the true stochastic kernels given the state-action pairs(x_t,a_t)observed during exploration phases, and therefore, it does not adhere to the standard``optimism-in-the-face-of-uncertainty''principle of maintaining a set of plausible MDPs\\mathcal{M}that contains the true MDPMwith high probability."}], "importance_score": 7.8119047619047635}, "bartlett2009regal": {"bib_key": "bartlett2009regal", "bib_title": "REGAL: a regularization based algorithm for reinforcement learning in weakly communicating MDPs.", "bib_author ": "Peter Bartlett and Ambuj Tewari.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": null, "prev_context": "For that, we propose a hybrid generative online-learning model in which classical and quantum computers are capable of offering an improved learning progress compared to several previous works~\\cite{auer2008near,bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient,ganguly2023quantum,zhong2023provably}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "Regarding infinite-horizon undiscounted MDPs, the list of classical algorithms for computing optimal policies under a generative model is more recent~\\cite{wang2017primal,jin2020efficiently,jin2021towards,wang2022near,zhang2023sharper,li2024stochastic}, one of the best complexities being\\widetilde{O}\\big(\\frac{\\Lambda^2 SA}{\\varepsilon^2}\\big)due to Zhang and Xie~\\cite{zhang2023sharper}, where\\operatornamesp(h^\\ast) :=\\max_x\\in\\mathcal{X}h^\\ast(x) -\\min_x\\in\\mathcal{X}h^\\ast(x)\\leq\\Lambdais an upper-bound on the span of the optimal biash^\\ast(x), which is a mild assumption used in several RL works~\\cite{bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Bartlett and Tewari~\\cite{bartlett2009regal}", "next_context": "achieved the regret bound\\widetilde{O}(\\Lambda\\sqrt{S^2AT})for the broader class of weakly communicating MDPs, where\\operatornamesp(h^\\ast)\\leq\\Lambda."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "The algorithms of~\\cite{bartlett2009regal,ortner2012online,lakshmanan2015improved}", "next_context": "are, however, time inefficient, which was later fixed by Fruit\\emph{et al.}~\\cite{fruit2018efficient}, whose time-efficient algorithm still maintains the regret bound\\widetilde{O}(\\Lambda\\sqrt{S^2AT})for finite state spaces."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "In all the classical works mentioned above~\\cite{auer2008near,azar2017minimax,zanette2019tighter,bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient}", "next_context": ", the way the agent interacts with the environment and accumulates regret is straightforward: once again, for fear of being repetitious, the agent chooses an actiona_tat some environment's statex_tand obtains a rewardr(x_t,a_t), after which the environment's state transitions tox_t+1."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Several RL algorithms~\\cite{auer2008near,bartlett2009regal,ortner2012online,lakshmanan2015improved,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,zhong2023provably,ganguly2023quantum}", "next_context": ", implement the principle of \u201coptimism in the face of uncertainty\u201d~\\cite{brafman2002r}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{bartlett2009regal}", "next_context": ""}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Regarding infinite-horizon MDPs, our classical regret\\widetilde{O}(\\Lambda\\sqrt{SAT})improves upon the bound\\widetilde{O}(\\Lambda\\sqrt{S^2AT})of~\\cite{bartlett2009regal,fruit2018efficient}", "next_context": ", while for compact state spaces, the classical regret\\widetilde{O}(\\LambdaT^\\frac{D+\\alpha}{D+2\\alpha}\\sqrt{A})is superior to the bound\\widetilde{O}(\\LambdaT^\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha}\\sqrt{A})of~\\cite{lakshmanan2015improved}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{bartlett2009regal}", "next_context": "^\\dagger"}, {"section": "Our reinforcement learning model", "subsection": null, "subsubsection": null, "prev_context": "The infinite-horizon in-path regret\\operatornameRegret_\\infty^\\rmpath(T)is the standard choice for most prior works on RL~\\cite{auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,ortner2012online,fruit2018efficient}", "next_context": "and takes into account the actual path of observed state-action pairs(x_t,a_t)during learning, while the infinite-horizon expected regret\\operatornameRegret_\\infty^\\mathbb{E}(T)is a novel measure of regret that we introduce here."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Several RL algorithms~\\cite{auer2006logarithmic,bartlett2009regal,auer2008near,ortner2012online,lakshmanan2015improved,fruit2018efficient}", "next_context": "compute approximate optimal policies for the target unknown MDPMgiven empirical estimates of the true stochastic kernelspand rewardsr."}, {"section": "Algorithms for online learning of infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Like several classical RL algorithms~\\cite{ortner2012online,auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,fruit2018efficient}", "next_context": ", they proceed in episodes, during each of which a chosen policy remains fixed."}, {"section": "Algorithms for online learning of infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Unlike prior classical RL algorithms~\\cite{ortner2012online,auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,fruit2018efficient}", "next_context": ",\\cref{algo:quantum_UCCRL}does not keep estimates of the true stochastic kernels given the state-action pairs(x_t,a_t)observed during exploration phases, and therefore, it does not adhere to the standard``optimism-in-the-face-of-uncertainty''principle of maintaining a set of plausible MDPs\\mathcal{M}that contains the true MDPMwith high probability."}], "importance_score": 5.135714285714287}, "ortner2012online": {"bib_key": "ortner2012online", "bib_title": "Online regret bounds for undiscounted continuous reinforcement learning.", "bib_author ": "Ronald Ortner and Daniil Ryabko.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": null, "prev_context": "For that, we propose a hybrid generative online-learning model in which classical and quantum computers are capable of offering an improved learning progress compared to several previous works~\\cite{auer2008near,bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient,ganguly2023quantum,zhong2023provably}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "Regarding infinite-horizon undiscounted MDPs, the list of classical algorithms for computing optimal policies under a generative model is more recent~\\cite{wang2017primal,jin2020efficiently,jin2021towards,wang2022near,zhang2023sharper,li2024stochastic}, one of the best complexities being\\widetilde{O}\\big(\\frac{\\Lambda^2 SA}{\\varepsilon^2}\\big)due to Zhang and Xie~\\cite{zhang2023sharper}, where\\operatornamesp(h^\\ast) :=\\max_x\\in\\mathcal{X}h^\\ast(x) -\\min_x\\in\\mathcal{X}h^\\ast(x)\\leq\\Lambdais an upper-bound on the span of the optimal biash^\\ast(x), which is a mild assumption used in several RL works~\\cite{bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Later, Ortner and Ryabko~\\cite{ortner2012online}", "next_context": "and Lakshmanan, Ortner, and Ryabko~\\cite{lakshmanan2015improved}adapted the algorithm from~\\cite{auer2008near}for the case when the state space is\\mathcal{X}=[0,1]^Dunder the assumption thatrandpare H\\\"older continuous."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "The algorithms of~\\cite{bartlett2009regal,ortner2012online,lakshmanan2015improved}", "next_context": "are, however, time inefficient, which was later fixed by Fruit\\emph{et al.}~\\cite{fruit2018efficient}, whose time-efficient algorithm still maintains the regret bound\\widetilde{O}(\\Lambda\\sqrt{S^2AT})for finite state spaces."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "In all the classical works mentioned above~\\cite{auer2008near,azar2017minimax,zanette2019tighter,bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient}", "next_context": ", the way the agent interacts with the environment and accumulates regret is straightforward: once again, for fear of being repetitious, the agent chooses an actiona_tat some environment's statex_tand obtains a rewardr(x_t,a_t), after which the environment's state transitions tox_t+1."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Several RL algorithms~\\cite{auer2008near,bartlett2009regal,ortner2012online,lakshmanan2015improved,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,zhong2023provably,ganguly2023quantum}", "next_context": ", implement the principle of \u201coptimism in the face of uncertainty\u201d~\\cite{brafman2002r}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{ortner2012online}", "next_context": ""}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Several regret bounds from\\Cref{res:res3,res:res4}considerably improve upon prior works~\\cite{auer2008near,azar2017minimax,zanette2019tighter,ortner2012online,lakshmanan2015improved,zhong2023provably,ganguly2023quantum}", "next_context": "(see\\Cref{table:results_finite-horizon,table:results_infinite-horizon}for a clear comparison)."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{ortner2012online}", "next_context": "^\\dagger"}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "~\\cite{ortner2012online}", "next_context": "and for pointing out Ref."}, {"section": "Preliminaries", "subsection": "Discretization of state space", "subsubsection": null, "prev_context": "Similar assumptions have been considered in~\\cite{saldi2017asymptotic, ortner2012online, kara2023q}", "next_context": "."}, {"section": "Our reinforcement learning model", "subsection": null, "subsubsection": null, "prev_context": "The infinite-horizon in-path regret\\operatornameRegret_\\infty^\\rmpath(T)is the standard choice for most prior works on RL~\\cite{auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,ortner2012online,fruit2018efficient}", "next_context": "and takes into account the actual path of observed state-action pairs(x_t,a_t)during learning, while the infinite-horizon expected regret\\operatornameRegret_\\infty^\\mathbb{E}(T)is a novel measure of regret that we introduce here."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Several RL algorithms~\\cite{auer2006logarithmic,bartlett2009regal,auer2008near,ortner2012online,lakshmanan2015improved,fruit2018efficient}", "next_context": "compute approximate optimal policies for the target unknown MDPMgiven empirical estimates of the true stochastic kernelspand rewardsr."}, {"section": "Algorithms for online learning of infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Like several classical RL algorithms~\\cite{ortner2012online,auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,fruit2018efficient}", "next_context": ", they proceed in episodes, during each of which a chosen policy remains fixed."}, {"section": "Algorithms for online learning of infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Unlike prior classical RL algorithms~\\cite{ortner2012online,auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,fruit2018efficient}", "next_context": ",\\cref{algo:quantum_UCCRL}does not keep estimates of the true stochastic kernels given the state-action pairs(x_t,a_t)observed during exploration phases, and therefore, it does not adhere to the standard``optimism-in-the-face-of-uncertainty''principle of maintaining a set of plausible MDPs\\mathcal{M}that contains the true MDPMwith high probability."}], "importance_score": 6.111904761904763}, "lakshmanan2015improved": {"bib_key": "lakshmanan2015improved", "bib_title": "Improved regret bounds for undiscounted continuous reinforcement learning.", "bib_author ": "Kailasam Lakshmanan, Ronald Ortner, and Daniil Ryabko.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": null, "prev_context": "For that, we propose a hybrid generative online-learning model in which classical and quantum computers are capable of offering an improved learning progress compared to several previous works~\\cite{auer2008near,bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient,ganguly2023quantum,zhong2023provably}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "Regarding infinite-horizon undiscounted MDPs, the list of classical algorithms for computing optimal policies under a generative model is more recent~\\cite{wang2017primal,jin2020efficiently,jin2021towards,wang2022near,zhang2023sharper,li2024stochastic}, one of the best complexities being\\widetilde{O}\\big(\\frac{\\Lambda^2 SA}{\\varepsilon^2}\\big)due to Zhang and Xie~\\cite{zhang2023sharper}, where\\operatornamesp(h^\\ast) :=\\max_x\\in\\mathcal{X}h^\\ast(x) -\\min_x\\in\\mathcal{X}h^\\ast(x)\\leq\\Lambdais an upper-bound on the span of the optimal biash^\\ast(x), which is a mild assumption used in several RL works~\\cite{bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Later, Ortner and Ryabko~\\cite{ortner2012online}and Lakshmanan, Ortner, and Ryabko~\\cite{lakshmanan2015improved}", "next_context": "adapted the algorithm from~\\cite{auer2008near}for the case when the state space is\\mathcal{X}=[0,1]^Dunder the assumption thatrandpare H\\\"older continuous."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "The algorithms of~\\cite{bartlett2009regal,ortner2012online,lakshmanan2015improved}", "next_context": "are, however, time inefficient, which was later fixed by Fruit\\emph{et al.}~\\cite{fruit2018efficient}, whose time-efficient algorithm still maintains the regret bound\\widetilde{O}(\\Lambda\\sqrt{S^2AT})for finite state spaces."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "In all the classical works mentioned above~\\cite{auer2008near,azar2017minimax,zanette2019tighter,bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient}", "next_context": ", the way the agent interacts with the environment and accumulates regret is straightforward: once again, for fear of being repetitious, the agent chooses an actiona_tat some environment's statex_tand obtains a rewardr(x_t,a_t), after which the environment's state transitions tox_t+1."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Several RL algorithms~\\cite{auer2008near,bartlett2009regal,ortner2012online,lakshmanan2015improved,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,zhong2023provably,ganguly2023quantum}", "next_context": ", implement the principle of \u201coptimism in the face of uncertainty\u201d~\\cite{brafman2002r}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{lakshmanan2015improved}", "next_context": ""}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Several regret bounds from\\Cref{res:res3,res:res4}considerably improve upon prior works~\\cite{auer2008near,azar2017minimax,zanette2019tighter,ortner2012online,lakshmanan2015improved,zhong2023provably,ganguly2023quantum}", "next_context": "(see\\Cref{table:results_finite-horizon,table:results_infinite-horizon}for a clear comparison)."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Regarding infinite-horizon MDPs, our classical regret\\widetilde{O}(\\Lambda\\sqrt{SAT})improves upon the bound\\widetilde{O}(\\Lambda\\sqrt{S^2AT})of~\\cite{bartlett2009regal,fruit2018efficient}, while for compact state spaces, the classical regret\\widetilde{O}(\\LambdaT^\\frac{D+\\alpha}{D+2\\alpha}\\sqrt{A})is superior to the bound\\widetilde{O}(\\LambdaT^\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha}\\sqrt{A})of~\\cite{lakshmanan2015improved}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{lakshmanan2015improved}", "next_context": "^\\dagger"}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "~\\cite{lakshmanan2015improved}", "next_context": "."}, {"section": "Preliminaries", "subsection": null, "subsubsection": null, "prev_context": "[\\cite[Lemma~2]{lakshmanan2015improved}", "next_context": "]\\label{fact:useful_inequality}For any sequencez_1,\\dots, z_n\\in\\mathbb{R}with0\\leqz_k\\leqZ_k-1\\coloneqq\\max\\{1,\\sum_i=1^k-1z_i\\},\\sum_k=1^m\\frac{z_k}{Z_{k-1}^{1-\\gamma}}\\leq\\frac{Z_m^\\gamma}{2^\\gamma-1}\\quad\\text{for any}\\quad\\gamma\\in[0,1]."}, {"section": "Our reinforcement learning model", "subsection": null, "subsubsection": null, "prev_context": "The infinite-horizon in-path regret\\operatornameRegret_\\infty^\\rmpath(T)is the standard choice for most prior works on RL~\\cite{auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,ortner2012online,fruit2018efficient}", "next_context": "and takes into account the actual path of observed state-action pairs(x_t,a_t)during learning, while the infinite-horizon expected regret\\operatornameRegret_\\infty^\\mathbb{E}(T)is a novel measure of regret that we introduce here."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Several RL algorithms~\\cite{auer2006logarithmic,bartlett2009regal,auer2008near,ortner2012online,lakshmanan2015improved,fruit2018efficient}", "next_context": "compute approximate optimal policies for the target unknown MDPMgiven empirical estimates of the true stochastic kernelspand rewardsr."}, {"section": "Algorithms for online learning of infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Like several classical RL algorithms~\\cite{ortner2012online,auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,fruit2018efficient}", "next_context": ", they proceed in episodes, during each of which a chosen policy remains fixed."}, {"section": "Algorithms for online learning of infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Unlike prior classical RL algorithms~\\cite{ortner2012online,auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,fruit2018efficient}", "next_context": ",\\cref{algo:quantum_UCCRL}does not keep estimates of the true stochastic kernels given the state-action pairs(x_t,a_t)observed during exploration phases, and therefore, it does not adhere to the standard``optimism-in-the-face-of-uncertainty''principle of maintaining a set of plausible MDPs\\mathcal{M}that contains the true MDPMwith high probability."}], "importance_score": 7.77857142857143}, "fruit2018efficient": {"bib_key": "fruit2018efficient", "bib_title": "Efficient bias-span-constrained exploration-exploitation in reinforcement learning.", "bib_author ": "Ronan Fruit, Matteo Pirotta, Alessandro Lazaric, and Ronald Ortner.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": null, "prev_context": "For that, we propose a hybrid generative online-learning model in which classical and quantum computers are capable of offering an improved learning progress compared to several previous works~\\cite{auer2008near,bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient,ganguly2023quantum,zhong2023provably}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "Regarding infinite-horizon undiscounted MDPs, the list of classical algorithms for computing optimal policies under a generative model is more recent~\\cite{wang2017primal,jin2020efficiently,jin2021towards,wang2022near,zhang2023sharper,li2024stochastic}, one of the best complexities being\\widetilde{O}\\big(\\frac{\\Lambda^2 SA}{\\varepsilon^2}\\big)due to Zhang and Xie~\\cite{zhang2023sharper}, where\\operatornamesp(h^\\ast) :=\\max_x\\in\\mathcal{X}h^\\ast(x) -\\min_x\\in\\mathcal{X}h^\\ast(x)\\leq\\Lambdais an upper-bound on the span of the optimal biash^\\ast(x), which is a mild assumption used in several RL works~\\cite{bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "The algorithms of~\\cite{bartlett2009regal,ortner2012online,lakshmanan2015improved}are, however, time inefficient, which was later fixed by Fruit\\emph{et al.}~\\cite{fruit2018efficient}", "next_context": ", whose time-efficient algorithm still maintains the regret bound\\widetilde{O}(\\Lambda\\sqrt{S^2AT})for finite state spaces."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "In all the classical works mentioned above~\\cite{auer2008near,azar2017minimax,zanette2019tighter,bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient}", "next_context": ", the way the agent interacts with the environment and accumulates regret is straightforward: once again, for fear of being repetitious, the agent chooses an actiona_tat some environment's statex_tand obtains a rewardr(x_t,a_t), after which the environment's state transitions tox_t+1."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Regarding infinite-horizon MDPs, our classical regret\\widetilde{O}(\\Lambda\\sqrt{SAT})improves upon the bound\\widetilde{O}(\\Lambda\\sqrt{S^2AT})of~\\cite{bartlett2009regal,fruit2018efficient}", "next_context": ", while for compact state spaces, the classical regret\\widetilde{O}(\\LambdaT^\\frac{D+\\alpha}{D+2\\alpha}\\sqrt{A})is superior to the bound\\widetilde{O}(\\LambdaT^\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha}\\sqrt{A})of~\\cite{lakshmanan2015improved}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{fruit2018efficient}", "next_context": ""}, {"section": "Our reinforcement learning model", "subsection": null, "subsubsection": null, "prev_context": "The infinite-horizon in-path regret\\operatornameRegret_\\infty^\\rmpath(T)is the standard choice for most prior works on RL~\\cite{auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,ortner2012online,fruit2018efficient}", "next_context": "and takes into account the actual path of observed state-action pairs(x_t,a_t)during learning, while the infinite-horizon expected regret\\operatornameRegret_\\infty^\\mathbb{E}(T)is a novel measure of regret that we introduce here."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Several RL algorithms~\\cite{auer2006logarithmic,bartlett2009regal,auer2008near,ortner2012online,lakshmanan2015improved,fruit2018efficient}", "next_context": "compute approximate optimal policies for the target unknown MDPMgiven empirical estimates of the true stochastic kernelspand rewardsr."}, {"section": "Algorithms for online learning of infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Like several classical RL algorithms~\\cite{ortner2012online,auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,fruit2018efficient}", "next_context": ", they proceed in episodes, during each of which a chosen policy remains fixed."}, {"section": "Algorithms for online learning of infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Unlike prior classical RL algorithms~\\cite{ortner2012online,auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,fruit2018efficient}", "next_context": ",\\cref{algo:quantum_UCCRL}does not keep estimates of the true stochastic kernels given the state-action pairs(x_t,a_t)observed during exploration phases, and therefore, it does not adhere to the standard``optimism-in-the-face-of-uncertainty''principle of maintaining a set of plausible MDPs\\mathcal{M}that contains the true MDPMwith high probability."}], "importance_score": 3.7023809523809517}, "bellman1966dynamic": {"bib_key": "bellman1966dynamic", "bib_title": "Dynamic programming.", "bib_author ": "Richard Bellman.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "One of the main problems associated with MDPs is that of computing\\varepsilon-optimal policies, for which several algorithms have been proposed~\\cite{ bellman1966dynamic,watkins1992q,meyn1997policy, kearns1998finite, sutton1999policy,lagoudakis2003least,bertsekas2011approximate,azar2012dynamic,puterman2014markov,silver2014deterministic, schulman2015trust,  jang2019q}", "next_context": "."}], "importance_score": 0.08333333333333333}, "watkins1992q": {"bib_key": "watkins1992q", "bib_title": "Q-learning.", "bib_author ": "Christopher~JCH Watkins and Peter Dayan.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "One of the main problems associated with MDPs is that of computing\\varepsilon-optimal policies, for which several algorithms have been proposed~\\cite{ bellman1966dynamic,watkins1992q,meyn1997policy, kearns1998finite, sutton1999policy,lagoudakis2003least,bertsekas2011approximate,azar2012dynamic,puterman2014markov,silver2014deterministic, schulman2015trust,  jang2019q}", "next_context": "."}], "importance_score": 0.08333333333333333}, "meyn1997policy": {"bib_key": "meyn1997policy", "bib_title": "The policy iteration algorithm for average reward Markov decision processes with general state space.", "bib_author ": "Sean~P Meyn.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "One of the main problems associated with MDPs is that of computing\\varepsilon-optimal policies, for which several algorithms have been proposed~\\cite{ bellman1966dynamic,watkins1992q,meyn1997policy, kearns1998finite, sutton1999policy,lagoudakis2003least,bertsekas2011approximate,azar2012dynamic,puterman2014markov,silver2014deterministic, schulman2015trust,  jang2019q}", "next_context": "."}], "importance_score": 0.08333333333333333}, "kearns1998finite": {"bib_key": "kearns1998finite", "bib_title": "Finite-sample convergence rates for Q-learning and indirect algorithms.", "bib_author ": "Michael Kearns and Satinder Singh.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "One of the main problems associated with MDPs is that of computing\\varepsilon-optimal policies, for which several algorithms have been proposed~\\cite{ bellman1966dynamic,watkins1992q,meyn1997policy, kearns1998finite, sutton1999policy,lagoudakis2003least,bertsekas2011approximate,azar2012dynamic,puterman2014markov,silver2014deterministic, schulman2015trust,  jang2019q}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "\\emph{generative model}~\\cite{kearns1998finite,Kearns2002sparse,kakade2003sample}", "next_context": "where one has full knowledge of state and action spaces\\mathcal{X},\\mathcal{A}and of the reward functionr:\\mathcal{X}\\times\\mathcal{A}\\to[0,1], but the transition probabilitiesp(\\cdot|x,a)can only be accessed through an oracle."}, {"section": "Our reinforcement learning model", "subsection": "Quantum exploration-generative learning model", "subsubsection": null, "prev_context": "Such interaction is done in a generative fashion~\\cite{kearns1998finite,kakade2003sample}", "next_context": "via quantum-accessible environments~\\cite{wiedemann2022quantum, wang2021quantum, jerbi2022quantum, zhong2023provably}, i.e., through a quantum sampling oracle for the transition probabilities and a quantum reward oracle defined below."}, {"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Several classical algorithms have been proposed for such task, one of them main ones being value iteration~\\cite{kearns1998finite, lutter2021value, hartmanns2020optimistic, bertsekas1998new, quatmann2018sound, shani2007forward, weng2013interactive}", "next_context": ", which is quite similar to the backward induction algorithm covered in the last section: starting from some initial functionu_0\\in\\mathscr{B}(\\mathcal{X}), normallyu_0\\equiv0, generate a sequence of functions(u_t)_t\\in\\mathbb{N}according to the update ruleu_t+1=\\mathcal{L}u_t."}], "importance_score": 1.0595238095238095}, "sutton1999policy": {"bib_key": "sutton1999policy", "bib_title": "Policy gradient methods for reinforcement learning with function approximation.", "bib_author ": "Richard~S Sutton, David McAllester, Satinder Singh, and Yishay Mansour.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "One of the main problems associated with MDPs is that of computing\\varepsilon-optimal policies, for which several algorithms have been proposed~\\cite{ bellman1966dynamic,watkins1992q,meyn1997policy, kearns1998finite, sutton1999policy,lagoudakis2003least,bertsekas2011approximate,azar2012dynamic,puterman2014markov,silver2014deterministic, schulman2015trust,  jang2019q}", "next_context": "."}], "importance_score": 0.08333333333333333}, "lagoudakis2003least": {"bib_key": "lagoudakis2003least", "bib_title": "Least-squares policy iteration.", "bib_author ": "Michail~G Lagoudakis and Ronald Parr.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "One of the main problems associated with MDPs is that of computing\\varepsilon-optimal policies, for which several algorithms have been proposed~\\cite{ bellman1966dynamic,watkins1992q,meyn1997policy, kearns1998finite, sutton1999policy,lagoudakis2003least,bertsekas2011approximate,azar2012dynamic,puterman2014markov,silver2014deterministic, schulman2015trust,  jang2019q}", "next_context": "."}], "importance_score": 0.08333333333333333}, "bertsekas2011approximate": {"bib_key": "bertsekas2011approximate", "bib_title": "Approximate policy iteration: A survey and some new methods.", "bib_author ": "Dimitri~P Bertsekas.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "One of the main problems associated with MDPs is that of computing\\varepsilon-optimal policies, for which several algorithms have been proposed~\\cite{ bellman1966dynamic,watkins1992q,meyn1997policy, kearns1998finite, sutton1999policy,lagoudakis2003least,bertsekas2011approximate,azar2012dynamic,puterman2014markov,silver2014deterministic, schulman2015trust,  jang2019q}", "next_context": "."}], "importance_score": 0.08333333333333333}, "azar2012dynamic": {"bib_key": "azar2012dynamic", "bib_title": "c", "bib_author ": "Mohammad~Gheshlaghi Azar, Vicen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "One of the main problems associated with MDPs is that of computing\\varepsilon-optimal policies, for which several algorithms have been proposed~\\cite{ bellman1966dynamic,watkins1992q,meyn1997policy, kearns1998finite, sutton1999policy,lagoudakis2003least,bertsekas2011approximate,azar2012dynamic,puterman2014markov,silver2014deterministic, schulman2015trust,  jang2019q}", "next_context": "."}], "importance_score": 0.08333333333333333}, "silver2014deterministic": {"bib_key": "silver2014deterministic", "bib_title": "Deterministic policy gradient algorithms.", "bib_author ": "David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "One of the main problems associated with MDPs is that of computing\\varepsilon-optimal policies, for which several algorithms have been proposed~\\cite{ bellman1966dynamic,watkins1992q,meyn1997policy, kearns1998finite, sutton1999policy,lagoudakis2003least,bertsekas2011approximate,azar2012dynamic,puterman2014markov,silver2014deterministic, schulman2015trust,  jang2019q}", "next_context": "."}], "importance_score": 0.08333333333333333}, "schulman2015trust": {"bib_key": "schulman2015trust", "bib_title": "Trust region policy optimization.", "bib_author ": "John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "One of the main problems associated with MDPs is that of computing\\varepsilon-optimal policies, for which several algorithms have been proposed~\\cite{ bellman1966dynamic,watkins1992q,meyn1997policy, kearns1998finite, sutton1999policy,lagoudakis2003least,bertsekas2011approximate,azar2012dynamic,puterman2014markov,silver2014deterministic, schulman2015trust,  jang2019q}", "next_context": "."}], "importance_score": 0.08333333333333333}, "jang2019q": {"bib_key": "jang2019q", "bib_title": "Q-learning algorithms: A comprehensive classification and applications.", "bib_author ": "Beakcheol Jang, Myeonghwi Kim, Gaspard Harerimana, and Jong~Wook Kim.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "One of the main problems associated with MDPs is that of computing\\varepsilon-optimal policies, for which several algorithms have been proposed~\\cite{ bellman1966dynamic,watkins1992q,meyn1997policy, kearns1998finite, sutton1999policy,lagoudakis2003least,bertsekas2011approximate,azar2012dynamic,puterman2014markov,silver2014deterministic, schulman2015trust,  jang2019q}", "next_context": "."}], "importance_score": 0.08333333333333333}, "Kearns1999finite": {"bib_key": "Kearns1999finite", "bib_title": "Finite-sample convergence rates for Q-learning and indirect algorithms.", "bib_author ": "Michael Kearns and Satinder Singh.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "The main interest is thus on the performance of the computed policy~\\cite{sutton1998reinforcement,Kearns1999finite,Kearns2002near}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "There is a long list of works that studied the classical query complexity of obtaining optimal policies~\\cite{Kearns1999finite,Kearns2002sparse,GheshlaghiAzar2013,wang2017randomized,Sidford2018variance,sidford2018near,li2020breaking}", "next_context": "."}], "importance_score": 0.47619047619047616}, "Kearns2002near": {"bib_key": "Kearns2002near", "bib_title": "Near-optimal reinforcement learning in polynomial time.", "bib_author ": "Michael Kearns and Satinder Singh.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "The main interest is thus on the performance of the computed policy~\\cite{sutton1998reinforcement,Kearns1999finite,Kearns2002near}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\exploitation trade-off arises: should the agent explore poorly understood states and actions in order to improve its understanding of the MDP and improve its future performance via better policies, or exploit its current knowledge to optimise short-term rewards~\\cite{Kearns2002near}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "In order to deal with such a dilemma, a few general strategies have been proposed~\\cite{strens2000bayesian,brafman2002r,Kearns2002near}", "next_context": "."}, {"section": "Algorithms for online learning of finite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Like several classical RL algorithms for finite-horizon MDPs~\\cite{Kearns2002near,brafman2002r,strehl2006pac,auer2008near,dann2017unifying}", "next_context": ", they proceed in episodes."}, {"section": "Algorithms for online learning of finite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Unlike prior RL algorithms for finite-horizon MDPs~\\cite{Kearns2002near,brafman2002r,strehl2006pac,auer2008near,dann2017unifying,wang2021quantum}", "next_context": ",\\cref{algo:quantum_UCCRL_finite-horizon}does not keep an approximation\\widetilde{p}for the true stochastic kernelpvia the state-action pairs(x_t,a_t)observed during exploration phases and therefore does not employ\\widetilde{p}to calculate approximate optimal policies."}], "importance_score": 2.033333333333333}, "Kearns2002sparse": {"bib_key": "Kearns2002sparse", "bib_title": "A sparse sampling algorithm for near-optimal planning in large Markov decision processes.", "bib_author ": "Michael Kearns, Yishay Mansour, and Andrew~Y. Ng.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "\\emph{generative model}~\\cite{kearns1998finite,Kearns2002sparse,kakade2003sample}", "next_context": "where one has full knowledge of state and action spaces\\mathcal{X},\\mathcal{A}and of the reward functionr:\\mathcal{X}\\times\\mathcal{A}\\to[0,1], but the transition probabilitiesp(\\cdot|x,a)can only be accessed through an oracle."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "There is a long list of works that studied the classical query complexity of obtaining optimal policies~\\cite{Kearns1999finite,Kearns2002sparse,GheshlaghiAzar2013,wang2017randomized,Sidford2018variance,sidford2018near,li2020breaking}", "next_context": "."}], "importance_score": 0.47619047619047616}, "kakade2003sample": {"bib_key": "kakade2003sample", "bib_title": "On the sample complexity of reinforcement learning.", "bib_author ": "Sham~Machandranath Kakade.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "\\emph{generative model}~\\cite{kearns1998finite,Kearns2002sparse,kakade2003sample}", "next_context": "where one has full knowledge of state and action spaces\\mathcal{X},\\mathcal{A}and of the reward functionr:\\mathcal{X}\\times\\mathcal{A}\\to[0,1], but the transition probabilitiesp(\\cdot|x,a)can only be accessed through an oracle."}, {"section": "Our reinforcement learning model", "subsection": "Quantum exploration-generative learning model", "subsubsection": null, "prev_context": "Such interaction is done in a generative fashion~\\cite{kearns1998finite,kakade2003sample}", "next_context": "via quantum-accessible environments~\\cite{wiedemann2022quantum, wang2021quantum, jerbi2022quantum, zhong2023provably}, i.e., through a quantum sampling oracle for the transition probabilities and a quantum reward oracle defined below."}], "importance_score": 0.8333333333333333}, "wang2021quantum": {"bib_key": "wang2021quantum", "bib_title": "Quantum algorithms for reinforcement learning with a generative model.", "bib_author ": "Daochen Wang, Aarthi Sundaram, Robin Kothari, Ashish Kapoor, and Martin Roetteler.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "In the quantum setting, one has access to an oracle\\mathcal{O}_pcalled quantum accessible-environment~\\cite{wang2021quantum,wiedemann2022quantum, jerbi2022quantum, zhong2023provably}", "next_context": "which is a unitary operator that acts~as\\label{eq:quantum_sampling_oracle}\\mathcal O_p:\\ket{x}\\ket{a}\\ket{\\bar 0}\\rightarrow\\int_x'\\in\\mathcal X\\sqrt{p(\\rd x'\\vert x, a)}\\ket{x}\\ket{a}\\ket{x'}\\quad\\text{for all}\\quad(x,a)\\in\\mathcal{X}\\times\\mathcal{A}(we postpone a more formal introduction to quantum computation to\\Cref{sec:quantum_computation})."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "In the quantum setting, on the other hand, we are only aware of a few works related to the problem of computing optimal policies~\\cite{wiedemann2022quantum,wang2021quantum,jerbi2022quantum,cherrat2023quantum}", "next_context": ", all for infinite-horizon discounted MDPs."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "More closely related to our paper is the work of Wang\\emph{et al.}~\\cite{wang2021quantum}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "Our quantum algorithms are quantised versions of the standard value iteration~\\cite{puterman2014markov}and the modern value iteration algorithms from~\\cite{sidford2018near,li2020breaking}for finite-horizon MDPs, similarly in spirit to what Wang\\emph{et al.}~\\cite{wang2021quantum}", "next_context": "did on infinite-horizon discounted MDPs."}, {"section": "Our reinforcement learning model", "subsection": "Quantum exploration-generative learning model", "subsubsection": null, "prev_context": "Such interaction is done in a generative fashion~\\cite{kearns1998finite,kakade2003sample}via quantum-accessible environments~\\cite{wiedemann2022quantum, wang2021quantum, jerbi2022quantum, zhong2023provably}", "next_context": ", i.e., through a quantum sampling oracle for the transition probabilities and a quantum reward oracle defined below."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": "Quantum backward induction algorithm", "subsubsection": null, "prev_context": "Although we are not aware of any works on quantum algorithms for finite-horizon MDPs with a generative model, Wang\\emph{et al.}~\\cite{wang2021quantum}", "next_context": "considered the case of infinite-horizon\\emph{discounted}MDPs with a generative model, which shares several similarities with the finite-horizon case."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": "Quantum backward induction algorithm", "subsubsection": null, "prev_context": "The following analysis thus takes inspiration from the work of Wang\\emph{et al.}~\\cite{wang2021quantum}", "next_context": ", who also employed quantum mean estimation subroutines to speed-up the corresponding classical value iteration algorithm for infinite-horizon discounted MDPs from Sidford\\emph{et al.}~\\cite{sidford2018near}."}, {"section": "Algorithms for online learning of finite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Unlike prior RL algorithms for finite-horizon MDPs~\\cite{Kearns2002near,brafman2002r,strehl2006pac,auer2008near,dann2017unifying,wang2021quantum}", "next_context": ",\\cref{algo:quantum_UCCRL_finite-horizon}does not keep an approximation\\widetilde{p}for the true stochastic kernelpvia the state-action pairs(x_t,a_t)observed during exploration phases and therefore does not employ\\widetilde{p}to calculate approximate optimal policies."}], "importance_score": 4.916666666666667}, "wiedemann2022quantum": {"bib_key": "wiedemann2022quantum", "bib_title": "Quantum policy iteration via amplitude estimation and Grover search  towards quantum advantage for reinforcement learning.", "bib_author ": "Simon Wiedemann, Daniel Hein, Steffen Udluft, and Christian~B. Mendl.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "In the quantum setting, one has access to an oracle\\mathcal{O}_pcalled quantum accessible-environment~\\cite{wang2021quantum,wiedemann2022quantum, jerbi2022quantum, zhong2023provably}", "next_context": "which is a unitary operator that acts~as\\label{eq:quantum_sampling_oracle}\\mathcal O_p:\\ket{x}\\ket{a}\\ket{\\bar 0}\\rightarrow\\int_x'\\in\\mathcal X\\sqrt{p(\\rd x'\\vert x, a)}\\ket{x}\\ket{a}\\ket{x'}\\quad\\text{for all}\\quad(x,a)\\in\\mathcal{X}\\times\\mathcal{A}(we postpone a more formal introduction to quantum computation to\\Cref{sec:quantum_computation})."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "In the quantum setting, on the other hand, we are only aware of a few works related to the problem of computing optimal policies~\\cite{wiedemann2022quantum,wang2021quantum,jerbi2022quantum,cherrat2023quantum}", "next_context": ", all for infinite-horizon discounted MDPs."}, {"section": "Our reinforcement learning model", "subsection": "Quantum exploration-generative learning model", "subsubsection": null, "prev_context": "Such interaction is done in a generative fashion~\\cite{kearns1998finite,kakade2003sample}via quantum-accessible environments~\\cite{wiedemann2022quantum, wang2021quantum, jerbi2022quantum, zhong2023provably}", "next_context": ", i.e., through a quantum sampling oracle for the transition probabilities and a quantum reward oracle defined below."}], "importance_score": 0.75}, "jerbi2022quantum": {"bib_key": "jerbi2022quantum", "bib_title": "Quantum Policy Gradient Algorithms.", "bib_author ": "Sofiene Jerbi, Arjan Cornelissen, Maris Ozols, and Vedran Dunjko.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "In the quantum setting, one has access to an oracle\\mathcal{O}_pcalled quantum accessible-environment~\\cite{wang2021quantum,wiedemann2022quantum, jerbi2022quantum, zhong2023provably}", "next_context": "which is a unitary operator that acts~as\\label{eq:quantum_sampling_oracle}\\mathcal O_p:\\ket{x}\\ket{a}\\ket{\\bar 0}\\rightarrow\\int_x'\\in\\mathcal X\\sqrt{p(\\rd x'\\vert x, a)}\\ket{x}\\ket{a}\\ket{x'}\\quad\\text{for all}\\quad(x,a)\\in\\mathcal{X}\\times\\mathcal{A}(we postpone a more formal introduction to quantum computation to\\Cref{sec:quantum_computation})."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "In the quantum setting, on the other hand, we are only aware of a few works related to the problem of computing optimal policies~\\cite{wiedemann2022quantum,wang2021quantum,jerbi2022quantum,cherrat2023quantum}", "next_context": ", all for infinite-horizon discounted MDPs."}, {"section": "Our reinforcement learning model", "subsection": "Quantum exploration-generative learning model", "subsubsection": null, "prev_context": "Such interaction is done in a generative fashion~\\cite{kearns1998finite,kakade2003sample}via quantum-accessible environments~\\cite{wiedemann2022quantum, wang2021quantum, jerbi2022quantum, zhong2023provably}", "next_context": ", i.e., through a quantum sampling oracle for the transition probabilities and a quantum reward oracle defined below."}], "importance_score": 0.75}, "GheshlaghiAzar2013": {"bib_key": "GheshlaghiAzar2013", "bib_title": "\\'e", "bib_author ": "Mohammad Gheshlaghi~Azar, R", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "There is a long list of works that studied the classical query complexity of obtaining optimal policies~\\cite{Kearns1999finite,Kearns2002sparse,GheshlaghiAzar2013,wang2017randomized,Sidford2018variance,sidford2018near,li2020breaking}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "For finite-horizon and infinite-horizon\\emph{discounted}MDPs with\\emph{finite}state space, Sidford\\emph{et al.}~\\cite{sidford2018near}and Li\\emph{et al.}~\\cite{li2020breaking}obtained sample-optimal algorithms that output an\\varepsilon-optimal policy using\\widetilde{O}\\big(\\frac{H^3SA}{\\varepsilon^2}\\big)queries to\\mathcal{C}_p, which matches the sample lower bounds from~\\cite{GheshlaghiAzar2013,sidford2018near}", "next_context": "up to polylogarithmic factors."}], "importance_score": 0.6428571428571428}, "wang2017randomized": {"bib_key": "wang2017randomized", "bib_title": "Randomized linear programming solves the discounted Markov decision problem in nearly-linear (sometimes sublinear) running time.", "bib_author ": "Mengdi Wang.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "There is a long list of works that studied the classical query complexity of obtaining optimal policies~\\cite{Kearns1999finite,Kearns2002sparse,GheshlaghiAzar2013,wang2017randomized,Sidford2018variance,sidford2018near,li2020breaking}", "next_context": "."}], "importance_score": 0.14285714285714285}, "Sidford2018variance": {"bib_key": "Sidford2018variance", "bib_title": "Variance reduced value iteration and faster algorithms for solving Markov decision processes.", "bib_author ": "Aaron Sidford, Mengdi Wang, Xian Wu, and Yinyu Ye.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "There is a long list of works that studied the classical query complexity of obtaining optimal policies~\\cite{Kearns1999finite,Kearns2002sparse,GheshlaghiAzar2013,wang2017randomized,Sidford2018variance,sidford2018near,li2020breaking}", "next_context": "."}], "importance_score": 0.14285714285714285}, "sidford2018near": {"bib_key": "sidford2018near", "bib_title": "Near-optimal time and sample complexities for solving Markov decision processes with a generative model.", "bib_author ": "Aaron Sidford, Mengdi Wang, Xian Wu, Lin Yang, and Yinyu Ye.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "There is a long list of works that studied the classical query complexity of obtaining optimal policies~\\cite{Kearns1999finite,Kearns2002sparse,GheshlaghiAzar2013,wang2017randomized,Sidford2018variance,sidford2018near,li2020breaking}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "For finite-horizon and infinite-horizon\\emph{discounted}MDPs with\\emph{finite}state space, Sidford\\emph{et al.}~\\cite{sidford2018near}", "next_context": "and Li\\emph{et al.}~\\cite{li2020breaking}obtained sample-optimal algorithms that output an\\varepsilon-optimal policy using\\widetilde{O}\\big(\\frac{H^3SA}{\\varepsilon^2}\\big)queries to\\mathcal{C}_p, which matches the sample lower bounds from~\\cite{GheshlaghiAzar2013,sidford2018near}up to polylogarithmic factors."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "For finite-horizon and infinite-horizon\\emph{discounted}MDPs with\\emph{finite}state space, Sidford\\emph{et al.}~\\cite{sidford2018near}and Li\\emph{et al.}~\\cite{li2020breaking}obtained sample-optimal algorithms that output an\\varepsilon-optimal policy using\\widetilde{O}\\big(\\frac{H^3SA}{\\varepsilon^2}\\big)queries to\\mathcal{C}_p, which matches the sample lower bounds from~\\cite{GheshlaghiAzar2013,sidford2018near}", "next_context": "up to polylogarithmic factors."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "The authors quantised the standard value iteration~\\cite{puterman2014markov}and the modern value iteration algorithms from~\\cite{sidford2018near,li2020breaking}", "next_context": "using quantum techniques like quantum minimum finding~\\cite{durr1996quantum}and quantum mean estimation~\\cite{brassard2002quantum,montanaro2015quant}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "Their final result is a quantum algorithm that outputs an\\varepsilon-optimal policy with quantum query complexity\\widetilde{O}\\big(\\frac{S}{\\varepsilon}\\min\\{\\Gamma^3/2A,\\Gamma^3\\sqrt{A}\\}\\big), improving the classical query complexity\\widetilde{O}\\big(\\frac{\\Gamma^3 SA}{\\varepsilon^2}\\big)from~\\cite{sidford2018near,li2020breaking}", "next_context": "(\\Gammais the``effective horizon''in infinite-horizon discounted MDPs)."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "Our quantum algorithms are quantised versions of the standard value iteration~\\cite{puterman2014markov}and the modern value iteration algorithms from~\\cite{sidford2018near,li2020breaking}", "next_context": "for finite-horizon MDPs, similarly in spirit to what Wang\\emph{et al.}~\\cite{wang2021quantum}did on infinite-horizon discounted MDPs."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\Cref{sec:optimal_policies_finite-horizon}describes the classical algorithm of Sidford\\emph{et al.}~\\cite{sidford2018near}", "next_context": "for approximating optimal policies under a generative model for finite-horizon MDPs and our newly introduced quantum versions, while\\Cref{sec:value_iteration}describes generative classical and quantum algorithms for computing optimal policies for infinite-horizon MDPs."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": "Classical approximate backward induction algorithm", "subsubsection": null, "prev_context": "The classical complexity of computing\\varepsilon-approximate optimal policies for finite-horizon MDPs with generative model has been completely characterised by Sidford\\emph{et al.}~\\cite{sidford2018near}", "next_context": "."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": "Classical approximate backward induction algorithm", "subsubsection": null, "prev_context": "By leveraging several techniques, the authors proposed a modern version of the above backward induction algorithm with sample complexity\\widetilde{O}(H^3 SA/\\varepsilon^2), which is optimal up to logarithmic factors~\\cite[Corollary~F.7]{sidford2018near}", "next_context": "."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": "Classical approximate backward induction algorithm", "subsubsection": null, "prev_context": "\\emph{et al.}~\\cite{sidford2018near}", "next_context": ", which is described in\\Cref{algo:classical_backward_recursion}, already generalised to compact state spaces."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": "Classical approximate backward induction algorithm", "subsubsection": null, "prev_context": "The backward induction algorithm from Sidford\\emph{et al.}~\\cite{sidford2018near}", "next_context": "works in\\emph{epochs}."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": "Classical approximate backward induction algorithm", "subsubsection": null, "prev_context": "The algorithm from Sidford\\emph{et al.}~\\cite{sidford2018near}", "next_context": "uses three crucial techniques:\\emph{monotonicity},\\emph{variance reduction}, and\\emph{total-variance}techniques."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": "Classical approximate backward induction algorithm", "subsubsection": null, "prev_context": "It can be shown~\\cite{sidford2018near}", "next_context": "(see\\Cref{fact:upper_bound_variance}below) that the total accumulation error is\\sqrt{H^3/m}, thus pickingm = O(H^3/\\epsilon_k^2)samples is sufficient to obtain a total error equal to\\epsilon_k."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": "Classical approximate backward induction algorithm", "subsubsection": null, "prev_context": "By putting all the three aforementioned techniques together, one arrives at the result from Sidford\\emph{et al.}~\\cite{sidford2018near}", "next_context": "."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": "Classical approximate backward induction algorithm", "subsubsection": null, "prev_context": "[\\cite[Lemma~F.4]{sidford2018near}", "next_context": "]\\label{fact:upper_bound_variance}Let\\sigma_t^\\pi(x,a) :=\\int_\\mathcal{X}p(\\rdx'|x,a)V_t^\\pi(x')^2 -\\big(\\int_\\mathcal{X}p(\\rdx'|x,a)V^\\pi_t(x')\\big)^2for a given policy\\pi."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": "Quantum backward induction algorithm", "subsubsection": null, "prev_context": "The following analysis thus takes inspiration from the work of Wang\\emph{et al.}~\\cite{wang2021quantum}, who also employed quantum mean estimation subroutines to speed-up the corresponding classical value iteration algorithm for infinite-horizon discounted MDPs from Sidford\\emph{et al.}~\\cite{sidford2018near}", "next_context": "."}, {"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": "Classical value iteration algorithm under a generative model", "subsubsection": null, "prev_context": "~\\cite{sidford2018near,li2020breaking}", "next_context": "."}], "importance_score": 13.642857142857142}, "li2020breaking": {"bib_key": "li2020breaking", "bib_title": "Breaking the sample size barrier in model-based reinforcement learning with a generative model.", "bib_author ": "Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "There is a long list of works that studied the classical query complexity of obtaining optimal policies~\\cite{Kearns1999finite,Kearns2002sparse,GheshlaghiAzar2013,wang2017randomized,Sidford2018variance,sidford2018near,li2020breaking}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "For finite-horizon and infinite-horizon\\emph{discounted}MDPs with\\emph{finite}state space, Sidford\\emph{et al.}~\\cite{sidford2018near}and Li\\emph{et al.}~\\cite{li2020breaking}", "next_context": "obtained sample-optimal algorithms that output an\\varepsilon-optimal policy using\\widetilde{O}\\big(\\frac{H^3SA}{\\varepsilon^2}\\big)queries to\\mathcal{C}_p, which matches the sample lower bounds from~\\cite{GheshlaghiAzar2013,sidford2018near}up to polylogarithmic factors."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "The authors quantised the standard value iteration~\\cite{puterman2014markov}and the modern value iteration algorithms from~\\cite{sidford2018near,li2020breaking}", "next_context": "using quantum techniques like quantum minimum finding~\\cite{durr1996quantum}and quantum mean estimation~\\cite{brassard2002quantum,montanaro2015quant}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "Their final result is a quantum algorithm that outputs an\\varepsilon-optimal policy with quantum query complexity\\widetilde{O}\\big(\\frac{S}{\\varepsilon}\\min\\{\\Gamma^3/2A,\\Gamma^3\\sqrt{A}\\}\\big), improving the classical query complexity\\widetilde{O}\\big(\\frac{\\Gamma^3 SA}{\\varepsilon^2}\\big)from~\\cite{sidford2018near,li2020breaking}", "next_context": "(\\Gammais the``effective horizon''in infinite-horizon discounted MDPs)."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "Our quantum algorithms are quantised versions of the standard value iteration~\\cite{puterman2014markov}and the modern value iteration algorithms from~\\cite{sidford2018near,li2020breaking}", "next_context": "for finite-horizon MDPs, similarly in spirit to what Wang\\emph{et al.}~\\cite{wang2021quantum}did on infinite-horizon discounted MDPs."}, {"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": "Classical value iteration algorithm under a generative model", "subsubsection": null, "prev_context": "~\\cite{sidford2018near,li2020breaking}", "next_context": "."}], "importance_score": 3.142857142857143}, "cherrat2023quantum": {"bib_key": "cherrat2023quantum", "bib_title": "Quantum reinforcement learning via policy iteration.", "bib_author ": "El~Amine Cherrat, Iordanis Kerenidis, and Anupam Prakash.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "In the quantum setting, on the other hand, we are only aware of a few works related to the problem of computing optimal policies~\\cite{wiedemann2022quantum,wang2021quantum,jerbi2022quantum,cherrat2023quantum}", "next_context": ", all for infinite-horizon discounted MDPs."}], "importance_score": 0.25}, "durr1996quantum": {"bib_key": "durr1996quantum", "bib_title": "A quantum algorithm for finding the minimum.", "bib_author ": "Christoph Durr and Peter Hoyer.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "The authors quantised the standard value iteration~\\cite{puterman2014markov}and the modern value iteration algorithms from~\\cite{sidford2018near,li2020breaking}using quantum techniques like quantum minimum finding~\\cite{durr1996quantum}", "next_context": "and quantum mean estimation~\\cite{brassard2002quantum,montanaro2015quant}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "In order to achieve the stated query complexity, we employ standard quantum minimum finding~\\cite{durr1996quantum}", "next_context": ", the quantum mean estimation subroutine from Kothari and O'Donnell~\\cite{Kothari2023mean}, and its multivariate version due to Cornelissen, Hamoudi, and Jerbi~\\cite{cornelissen2022near}, and Tang~\\cite{tang2025more}."}, {"section": "Preliminaries", "subsection": "Classical and quantum subroutines", "subsubsection": null, "prev_context": "\\\"urr and H\\oyer~\\cite{durr1996quantum}", "next_context": "."}, {"section": "Preliminaries", "subsection": "Classical and quantum subroutines", "subsubsection": null, "prev_context": "[Quantum max-finding\\cite{durr1996quantum}", "next_context": "]\\label{fact:quantum_minimum_finding}Given quantum access tou\\in\\mathscr{B}(\\mathcal{X})via oracle\\mathcal{O}_u, one can find\\max_x\\in\\mathcal{X}u(x)and\\min_x\\in\\mathcal{X}u(x)with probability1-\\deltausingO(\\sqrt{|\\mathcal{X}|}\\log\\frac{1}{\\delta})queries to\\mathcal{O}_u."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": "Quantum backward induction algorithm", "subsubsection": null, "prev_context": "Although this will inevitably lead to a worse sample complexity on the horizonH, it is possible now to employ quantum minimum finding~\\cite{durr1996quantum}", "next_context": "together with quantum mean estimation, which brings down the sample complexity on the action space size fromO(A)down toO(\\sqrt{A})."}], "importance_score": 5.0}, "brassard2002quantum": {"bib_key": "brassard2002quantum", "bib_title": "Quantum amplitude amplification and estimation.", "bib_author ": "Gilles Brassard, Peter Hoyer, Michele Mosca, and Alain Tapp.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "The authors quantised the standard value iteration~\\cite{puterman2014markov}and the modern value iteration algorithms from~\\cite{sidford2018near,li2020breaking}using quantum techniques like quantum minimum finding~\\cite{durr1996quantum}and quantum mean estimation~\\cite{brassard2002quantum,montanaro2015quant}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "More critically, however, we would like to employ quantum subroutines~\\cite{brassard2002quantum,montanaro2015quant,cornelissen2022near,Kothari2023mean}", "next_context": "that make use of the inverse of\\mathcal{O}_p."}], "importance_score": 0.75}, "montanaro2015quant": {"bib_key": "montanaro2015quant", "bib_title": "Quantum speedup of Monte Carlo methods.", "bib_author ": "Ashley Montanaro.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "The authors quantised the standard value iteration~\\cite{puterman2014markov}and the modern value iteration algorithms from~\\cite{sidford2018near,li2020breaking}using quantum techniques like quantum minimum finding~\\cite{durr1996quantum}and quantum mean estimation~\\cite{brassard2002quantum,montanaro2015quant}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "More critically, however, we would like to employ quantum subroutines~\\cite{brassard2002quantum,montanaro2015quant,cornelissen2022near,Kothari2023mean}", "next_context": "that make use of the inverse of\\mathcal{O}_p."}, {"section": "Preliminaries", "subsection": "Classical and quantum subroutines", "subsubsection": null, "prev_context": "Several quantum mean estimation algorithms have been proposed~\\cite{montanaro2015quant,van2021quantum,hamoudi2021quantum,cornelissen2022near,Kothari2023mean}", "next_context": "."}], "importance_score": 0.95}, "Kothari2023mean": {"bib_key": "Kothari2023mean", "bib_title": "Donnell. Mean estimation when you have the source code; or, quantum Monte Carlo methods. In Proceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 1186--1215, 2023", "bib_author ": "Robin Kothari and Ryan O", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "In order to achieve the stated query complexity, we employ standard quantum minimum finding~\\cite{durr1996quantum}, the quantum mean estimation subroutine from Kothari and O'Donnell~\\cite{Kothari2023mean}", "next_context": ", and its multivariate version due to Cornelissen, Hamoudi, and Jerbi~\\cite{cornelissen2022near}, and Tang~\\cite{tang2025more}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "More critically, however, we would like to employ quantum subroutines~\\cite{brassard2002quantum,montanaro2015quant,cornelissen2022near,Kothari2023mean}", "next_context": "that make use of the inverse of\\mathcal{O}_p."}, {"section": "Preliminaries", "subsection": "Classical and quantum subroutines", "subsubsection": null, "prev_context": "Several quantum mean estimation algorithms have been proposed~\\cite{montanaro2015quant,van2021quantum,hamoudi2021quantum,cornelissen2022near,Kothari2023mean}", "next_context": "."}, {"section": "Preliminaries", "subsection": "Classical and quantum subroutines", "subsubsection": null, "prev_context": "Here we shall employ the univariate version due to Kothari and O'Donnell~\\cite{Kothari2023mean}", "next_context": "and the multivariate version due to Cornelissen, Hamoudi, and Jerbi~\\cite{cornelissen2022near}, or more specifically, the improved version due to Tang~\\cite{tang2025more}."}, {"section": "Preliminaries", "subsection": "Classical and quantum subroutines", "subsubsection": null, "prev_context": "[Quantum mean estimation with variance\\cite[Theorem~1.1]{Kothari2023mean}", "next_context": "]\\label{fact:quantum_mean_estimation_variance}Let\\epsilon>0and\\delta\\in(0, 1)."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": "Quantum backward induction algorithm", "subsubsection": null, "prev_context": "~\\cite{cornelissen2022near,Kothari2023mean}", "next_context": "in order to estimate the quantities\\int_\\mathcal{X}p(\\rdx'|x,a)u^(k-1)_t(x')and\\int_\\mathcal{X}p(\\rdx'|x,a)\\big(u^(k)_t(x') - u^(k-1)_t(x')\\big)."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": "Quantum backward induction algorithm", "subsubsection": null, "prev_context": "The remaining quantities\\int_\\mathcal{X}p(\\rdx'|x,a)\\big(u^(k)_t(x') - u^(k-1)_t(x')\\big)during the backward recursion are estimated using the quantum mean estimation subroutine from Kothari and O'Donnell~\\cite{Kothari2023mean}", "next_context": "."}], "importance_score": 4.95}, "cornelissen2022near": {"bib_key": "cornelissen2022near", "bib_title": "Near-optimal quantum algorithms for multivariate mean estimation.", "bib_author ": "Arjan Cornelissen, Yassine Hamoudi, and Sofiene Jerbi.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "In order to achieve the stated query complexity, we employ standard quantum minimum finding~\\cite{durr1996quantum}, the quantum mean estimation subroutine from Kothari and O'Donnell~\\cite{Kothari2023mean}, and its multivariate version due to Cornelissen, Hamoudi, and Jerbi~\\cite{cornelissen2022near}", "next_context": ", and Tang~\\cite{tang2025more}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "More critically, however, we would like to employ quantum subroutines~\\cite{brassard2002quantum,montanaro2015quant,cornelissen2022near,Kothari2023mean}", "next_context": "that make use of the inverse of\\mathcal{O}_p."}, {"section": "Preliminaries", "subsection": "Classical and quantum subroutines", "subsubsection": null, "prev_context": "Several quantum mean estimation algorithms have been proposed~\\cite{montanaro2015quant,van2021quantum,hamoudi2021quantum,cornelissen2022near,Kothari2023mean}", "next_context": "."}, {"section": "Preliminaries", "subsection": "Classical and quantum subroutines", "subsubsection": null, "prev_context": "Here we shall employ the univariate version due to Kothari and O'Donnell~\\cite{Kothari2023mean}and the multivariate version due to Cornelissen, Hamoudi, and Jerbi~\\cite{cornelissen2022near}", "next_context": ", or more specifically, the improved version due to Tang~\\cite{tang2025more}."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": "Quantum backward induction algorithm", "subsubsection": null, "prev_context": "~\\cite{cornelissen2022near,Kothari2023mean}", "next_context": "in order to estimate the quantities\\int_\\mathcal{X}p(\\rdx'|x,a)u^(k-1)_t(x')and\\int_\\mathcal{X}p(\\rdx'|x,a)\\big(u^(k)_t(x') - u^(k-1)_t(x')\\big)."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": "Quantum backward induction algorithm", "subsubsection": null, "prev_context": "To get around this issue, we employ the quantum multivariate mean estimation subroutine from Cornelissen, Hamoudi, and Jerbi~\\cite{cornelissen2022near}", "next_context": "to estimate theseHaverage quantities at once with only anO(\\sqrt{H})overhead."}], "importance_score": 3.95}, "tang2025more": {"bib_key": "tang2025more", "bib_title": "More-efficient quantum multivariate mean value estimator from generalized Grover gate.", "bib_author ": "Letian Tang.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "In order to achieve the stated query complexity, we employ standard quantum minimum finding~\\cite{durr1996quantum}, the quantum mean estimation subroutine from Kothari and O'Donnell~\\cite{Kothari2023mean}, and its multivariate version due to Cornelissen, Hamoudi, and Jerbi~\\cite{cornelissen2022near}, and Tang~\\cite{tang2025more}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "~\\cite{tang2025more}", "next_context": ", Alessandro Luongo and Miklos Santha for comments on the manuscript, and Yecheng Xue for clarifications on Ref."}, {"section": "Preliminaries", "subsection": "Classical and quantum subroutines", "subsubsection": null, "prev_context": "Here we shall employ the univariate version due to Kothari and O'Donnell~\\cite{Kothari2023mean}and the multivariate version due to Cornelissen, Hamoudi, and Jerbi~\\cite{cornelissen2022near}, or more specifically, the improved version due to Tang~\\cite{tang2025more}", "next_context": "."}, {"section": "Preliminaries", "subsection": "Classical and quantum subroutines", "subsubsection": null, "prev_context": "[Quantum multidimensional mean estimation with variance~\\cite[Theorem~I.2]{tang2025more}", "next_context": "]\\label{fact:quantum_multidimensional_mean_estimation_covariance_matrix}Let\\epsilon> 0and\\delta\\in(0, 1)."}], "importance_score": 4.0}, "wang2017primal": {"bib_key": "wang2017primal", "bib_title": "Primal-dual $$ learning: Sample complexity and sublinear run time for ergodic Markov decision problems.", "bib_author ": "Mengdi Wang.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "Regarding infinite-horizon undiscounted MDPs, the list of classical algorithms for computing optimal policies under a generative model is more recent~\\cite{wang2017primal,jin2020efficiently,jin2021towards,wang2022near,zhang2023sharper,li2024stochastic}", "next_context": ", one of the best complexities being\\widetilde{O}\\big(\\frac{\\Lambda^2 SA}{\\varepsilon^2}\\big)due to Zhang and Xie~\\cite{zhang2023sharper}, where\\operatornamesp(h^\\ast) :=\\max_x\\in\\mathcal{X}h^\\ast(x) -\\min_x\\in\\mathcal{X}h^\\ast(x)\\leq\\Lambdais an upper-bound on the span of the optimal biash^\\ast(x), which is a mild assumption used in several RL works~\\cite{bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient}."}, {"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": "Classical value iteration algorithm under a generative model", "subsubsection": null, "prev_context": "A few different classical algorithms for computing optimal policies under a generative model for infinite-horizon MDPs have been proposed~\\cite{wang2017primal,jin2020efficiently,jin2021towards,wang2022near,zhang2023sharper,li2024stochastic}", "next_context": ","}], "importance_score": 0.3333333333333333}, "jin2020efficiently": {"bib_key": "jin2020efficiently", "bib_title": "Efficiently solving MDPs with stochastic mirror descent.", "bib_author ": "Yujia Jin and Aaron Sidford.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "Regarding infinite-horizon undiscounted MDPs, the list of classical algorithms for computing optimal policies under a generative model is more recent~\\cite{wang2017primal,jin2020efficiently,jin2021towards,wang2022near,zhang2023sharper,li2024stochastic}", "next_context": ", one of the best complexities being\\widetilde{O}\\big(\\frac{\\Lambda^2 SA}{\\varepsilon^2}\\big)due to Zhang and Xie~\\cite{zhang2023sharper}, where\\operatornamesp(h^\\ast) :=\\max_x\\in\\mathcal{X}h^\\ast(x) -\\min_x\\in\\mathcal{X}h^\\ast(x)\\leq\\Lambdais an upper-bound on the span of the optimal biash^\\ast(x), which is a mild assumption used in several RL works~\\cite{bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient}."}, {"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": "Classical value iteration algorithm under a generative model", "subsubsection": null, "prev_context": "A few different classical algorithms for computing optimal policies under a generative model for infinite-horizon MDPs have been proposed~\\cite{wang2017primal,jin2020efficiently,jin2021towards,wang2022near,zhang2023sharper,li2024stochastic}", "next_context": ","}], "importance_score": 0.3333333333333333}, "jin2021towards": {"bib_key": "jin2021towards", "bib_title": "Towards tight bounds on the sample complexity of average-reward MDPs.", "bib_author ": "Yujia Jin and Aaron Sidford.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "Regarding infinite-horizon undiscounted MDPs, the list of classical algorithms for computing optimal policies under a generative model is more recent~\\cite{wang2017primal,jin2020efficiently,jin2021towards,wang2022near,zhang2023sharper,li2024stochastic}", "next_context": ", one of the best complexities being\\widetilde{O}\\big(\\frac{\\Lambda^2 SA}{\\varepsilon^2}\\big)due to Zhang and Xie~\\cite{zhang2023sharper}, where\\operatornamesp(h^\\ast) :=\\max_x\\in\\mathcal{X}h^\\ast(x) -\\min_x\\in\\mathcal{X}h^\\ast(x)\\leq\\Lambdais an upper-bound on the span of the optimal biash^\\ast(x), which is a mild assumption used in several RL works~\\cite{bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient}."}, {"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": "Classical value iteration algorithm under a generative model", "subsubsection": null, "prev_context": "A few different classical algorithms for computing optimal policies under a generative model for infinite-horizon MDPs have been proposed~\\cite{wang2017primal,jin2020efficiently,jin2021towards,wang2022near,zhang2023sharper,li2024stochastic}", "next_context": ","}], "importance_score": 0.3333333333333333}, "wang2022near": {"bib_key": "wang2022near", "bib_title": "Near sample-optimal reduction-based policy learning for average reward MDP.", "bib_author ": "Jinghan Wang, Mengdi Wang, and Lin~F Yang.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "Regarding infinite-horizon undiscounted MDPs, the list of classical algorithms for computing optimal policies under a generative model is more recent~\\cite{wang2017primal,jin2020efficiently,jin2021towards,wang2022near,zhang2023sharper,li2024stochastic}", "next_context": ", one of the best complexities being\\widetilde{O}\\big(\\frac{\\Lambda^2 SA}{\\varepsilon^2}\\big)due to Zhang and Xie~\\cite{zhang2023sharper}, where\\operatornamesp(h^\\ast) :=\\max_x\\in\\mathcal{X}h^\\ast(x) -\\min_x\\in\\mathcal{X}h^\\ast(x)\\leq\\Lambdais an upper-bound on the span of the optimal biash^\\ast(x), which is a mild assumption used in several RL works~\\cite{bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient}."}, {"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": "Classical value iteration algorithm under a generative model", "subsubsection": null, "prev_context": "A few different classical algorithms for computing optimal policies under a generative model for infinite-horizon MDPs have been proposed~\\cite{wang2017primal,jin2020efficiently,jin2021towards,wang2022near,zhang2023sharper,li2024stochastic}", "next_context": ","}], "importance_score": 0.3333333333333333}, "zhang2023sharper": {"bib_key": "zhang2023sharper", "bib_title": "Sharper model-free reinforcement learning for average-reward Markov decision processes.", "bib_author ": "Zihan Zhang and Qiaomin Xie.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "Regarding infinite-horizon undiscounted MDPs, the list of classical algorithms for computing optimal policies under a generative model is more recent~\\cite{wang2017primal,jin2020efficiently,jin2021towards,wang2022near,zhang2023sharper,li2024stochastic}", "next_context": ", one of the best complexities being\\widetilde{O}\\big(\\frac{\\Lambda^2 SA}{\\varepsilon^2}\\big)due to Zhang and Xie~\\cite{zhang2023sharper}, where\\operatornamesp(h^\\ast) :=\\max_x\\in\\mathcal{X}h^\\ast(x) -\\min_x\\in\\mathcal{X}h^\\ast(x)\\leq\\Lambdais an upper-bound on the span of the optimal biash^\\ast(x), which is a mild assumption used in several RL works~\\cite{bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "Regarding infinite-horizon undiscounted MDPs, the list of classical algorithms for computing optimal policies under a generative model is more recent~\\cite{wang2017primal,jin2020efficiently,jin2021towards,wang2022near,zhang2023sharper,li2024stochastic}, one of the best complexities being\\widetilde{O}\\big(\\frac{\\Lambda^2 SA}{\\varepsilon^2}\\big)due to Zhang and Xie~\\cite{zhang2023sharper}", "next_context": ", where\\operatornamesp(h^\\ast) :=\\max_x\\in\\mathcal{X}h^\\ast(x) -\\min_x\\in\\mathcal{X}h^\\ast(x)\\leq\\Lambdais an upper-bound on the span of the optimal biash^\\ast(x), which is a mild assumption used in several RL works~\\cite{bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{zhang2023sharper}", "next_context": ""}, {"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": "Classical value iteration algorithm under a generative model", "subsubsection": null, "prev_context": "A few different classical algorithms for computing optimal policies under a generative model for infinite-horizon MDPs have been proposed~\\cite{wang2017primal,jin2020efficiently,jin2021towards,wang2022near,zhang2023sharper,li2024stochastic}", "next_context": ","}, {"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": "Classical value iteration algorithm under a generative model", "subsubsection": null, "prev_context": "one of the best complexities being\\widetilde{O}\\big(\\frac{\\Lambda^2 SA}{\\varepsilon^2}\\big)due to Zhang and Xie~\\cite{zhang2023sharper}", "next_context": "."}], "importance_score": 3.3333333333333335}, "li2024stochastic": {"bib_key": "li2024stochastic", "bib_title": "Stochastic first-order methods for average-reward Markov decision processes.", "bib_author ": "Tianjiao Li, Feiyang Wu, and Guanghui Lan.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Computing approximate optimal policies under a generative model", "prev_context": "Regarding infinite-horizon undiscounted MDPs, the list of classical algorithms for computing optimal policies under a generative model is more recent~\\cite{wang2017primal,jin2020efficiently,jin2021towards,wang2022near,zhang2023sharper,li2024stochastic}", "next_context": ", one of the best complexities being\\widetilde{O}\\big(\\frac{\\Lambda^2 SA}{\\varepsilon^2}\\big)due to Zhang and Xie~\\cite{zhang2023sharper}, where\\operatornamesp(h^\\ast) :=\\max_x\\in\\mathcal{X}h^\\ast(x) -\\min_x\\in\\mathcal{X}h^\\ast(x)\\leq\\Lambdais an upper-bound on the span of the optimal biash^\\ast(x), which is a mild assumption used in several RL works~\\cite{bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient}."}, {"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": "Classical value iteration algorithm under a generative model", "subsubsection": null, "prev_context": "A few different classical algorithms for computing optimal policies under a generative model for infinite-horizon MDPs have been proposed~\\cite{wang2017primal,jin2020efficiently,jin2021towards,wang2022near,zhang2023sharper,li2024stochastic}", "next_context": ","}], "importance_score": 0.3333333333333333}, "crammer2003online": {"bib_key": "crammer2003online", "bib_title": "Online classification on a budget.", "bib_author ": "Koby Crammer, Jaz Kandola, and Yoram Singer.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Here one is interested in the performance of the learning algorithm during the learning process, an area of study that finds applications in several topics~\\cite{crammer2003online, ying2006online,liang2006fast,tekin2010online,li2014online, ouyang2017learning,aaronson2018online,lim2022quantum}", "next_context": "."}], "importance_score": 0.125}, "ying2006online": {"bib_key": "ying2006online", "bib_title": "Online regularized classification algorithms.", "bib_author ": "Yiming Ying and D-X Zhou.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Here one is interested in the performance of the learning algorithm during the learning process, an area of study that finds applications in several topics~\\cite{crammer2003online, ying2006online,liang2006fast,tekin2010online,li2014online, ouyang2017learning,aaronson2018online,lim2022quantum}", "next_context": "."}], "importance_score": 0.125}, "liang2006fast": {"bib_key": "liang2006fast", "bib_title": "A fast and accurate online sequential learning algorithm for feedforward networks.", "bib_author ": "Nan-Ying Liang, Guang-Bin Huang, Paramasivan Saratchandran, and Narasimhan Sundararajan.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Here one is interested in the performance of the learning algorithm during the learning process, an area of study that finds applications in several topics~\\cite{crammer2003online, ying2006online,liang2006fast,tekin2010online,li2014online, ouyang2017learning,aaronson2018online,lim2022quantum}", "next_context": "."}], "importance_score": 0.125}, "tekin2010online": {"bib_key": "tekin2010online", "bib_title": "Online algorithms for the multi-armed bandit problem with Markovian rewards.", "bib_author ": "Cem Tekin and Mingyan Liu.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Here one is interested in the performance of the learning algorithm during the learning process, an area of study that finds applications in several topics~\\cite{crammer2003online, ying2006online,liang2006fast,tekin2010online,li2014online, ouyang2017learning,aaronson2018online,lim2022quantum}", "next_context": "."}], "importance_score": 0.125}, "li2014online": {"bib_key": "li2014online", "bib_title": "Online portfolio selection: A survey.", "bib_author ": "Bin Li and Steven~CH Hoi.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Here one is interested in the performance of the learning algorithm during the learning process, an area of study that finds applications in several topics~\\cite{crammer2003online, ying2006online,liang2006fast,tekin2010online,li2014online, ouyang2017learning,aaronson2018online,lim2022quantum}", "next_context": "."}], "importance_score": 0.125}, "ouyang2017learning": {"bib_key": "ouyang2017learning", "bib_title": "Learning unknown Markov decision processes: A Thompson sampling approach.", "bib_author ": "Yi~Ouyang, Mukul Gagrani, Ashutosh Nayyar, and Rahul Jain.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Here one is interested in the performance of the learning algorithm during the learning process, an area of study that finds applications in several topics~\\cite{crammer2003online, ying2006online,liang2006fast,tekin2010online,li2014online, ouyang2017learning,aaronson2018online,lim2022quantum}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{ouyang2017learning}", "next_context": ""}], "importance_score": 1.125}, "aaronson2018online": {"bib_key": "aaronson2018online", "bib_title": "Online learning of quantum states.", "bib_author ": "Scott Aaronson, Xinyi Chen, Elad Hazan, Satyen Kale, and Ashwin Nayak.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Here one is interested in the performance of the learning algorithm during the learning process, an area of study that finds applications in several topics~\\cite{crammer2003online, ying2006online,liang2006fast,tekin2010online,li2014online, ouyang2017learning,aaronson2018online,lim2022quantum}", "next_context": "."}], "importance_score": 0.125}, "lim2022quantum": {"bib_key": "lim2022quantum", "bib_title": "A quantum online portfolio optimization algorithm.", "bib_author ": "Debbie Lim and Patrick Rebentrost.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Here one is interested in the performance of the learning algorithm during the learning process, an area of study that finds applications in several topics~\\cite{crammer2003online, ying2006online,liang2006fast,tekin2010online,li2014online, ouyang2017learning,aaronson2018online,lim2022quantum}", "next_context": "."}], "importance_score": 0.125}, "strens2000bayesian": {"bib_key": "strens2000bayesian", "bib_title": "A Bayesian framework for reinforcement learning.", "bib_author ": "Malcolm J.~A. Strens.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "In order to deal with such a dilemma, a few general strategies have been proposed~\\cite{strens2000bayesian,brafman2002r,Kearns2002near}", "next_context": "."}], "importance_score": 0.3333333333333333}, "brafman2002r": {"bib_key": "brafman2002r", "bib_title": "R-MAX - a general polynomial time algorithm for near-optimal reinforcement learning.", "bib_author ": "Ronen~I Brafman and Moshe Tennenholtz.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "In order to deal with such a dilemma, a few general strategies have been proposed~\\cite{strens2000bayesian,brafman2002r,Kearns2002near}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Several RL algorithms~\\cite{auer2008near,bartlett2009regal,ortner2012online,lakshmanan2015improved,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,zhong2023provably,ganguly2023quantum}, implement the principle of \u201coptimism in the face of uncertainty\u201d~\\cite{brafman2002r}", "next_context": "."}, {"section": "Algorithms for online learning of finite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Like several classical RL algorithms for finite-horizon MDPs~\\cite{Kearns2002near,brafman2002r,strehl2006pac,auer2008near,dann2017unifying}", "next_context": ", they proceed in episodes."}, {"section": "Algorithms for online learning of finite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Unlike prior RL algorithms for finite-horizon MDPs~\\cite{Kearns2002near,brafman2002r,strehl2006pac,auer2008near,dann2017unifying,wang2021quantum}", "next_context": ",\\cref{algo:quantum_UCCRL_finite-horizon}does not keep an approximation\\widetilde{p}for the true stochastic kernelpvia the state-action pairs(x_t,a_t)observed during exploration phases and therefore does not employ\\widetilde{p}to calculate approximate optimal policies."}], "importance_score": 1.7}, "valiant1984theory": {"bib_key": "valiant1984theory", "bib_title": "A theory of the learnable.", "bib_author ": "Leslie~G Valiant.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "The task of obtaining large rewards is usually reframed as minimising some measure of how far the agent is from being optimal~\\cite{valiant1984theory,Littlestone1988learning,li2008knows,auer2008near}", "next_context": "."}], "importance_score": 0.25}, "Littlestone1988learning": {"bib_key": "Littlestone1988learning", "bib_title": "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm.", "bib_author ": "Nick Littlestone.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "The task of obtaining large rewards is usually reframed as minimising some measure of how far the agent is from being optimal~\\cite{valiant1984theory,Littlestone1988learning,li2008knows,auer2008near}", "next_context": "."}], "importance_score": 0.25}, "li2008knows": {"bib_key": "li2008knows", "bib_title": "Knows what it knows: a framework for self-aware learning.", "bib_author ": "Lihong Li, Michael~L. Littman, and Thomas~J. Walsh.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "The task of obtaining large rewards is usually reframed as minimising some measure of how far the agent is from being optimal~\\cite{valiant1984theory,Littlestone1988learning,li2008knows,auer2008near}", "next_context": "."}], "importance_score": 0.25}, "osband2014model": {"bib_key": "osband2014model", "bib_title": "Model-based reinforcement learning and the eluder dimension.", "bib_author ": "Ian Osband and Benjamin~Van Roy.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Since then, several works provided better regret upper bounds~\\cite{osband2014model,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,jin2020provably,yang2020reinforcement}", "next_context": ", the most relevant to our paper being the regret bound of Azar, Osband, and Munos~\\cite{azar2017minimax}, which matches the lower bound for a certain range of parameters."}], "importance_score": 0.14285714285714285}, "azar2017minimax": {"bib_key": "azar2017minimax", "bib_title": "\\'e", "bib_author ": "Mohammad~Gheshlaghi Azar, Ian Osband, and R", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Since then, several works provided better regret upper bounds~\\cite{osband2014model,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,jin2020provably,yang2020reinforcement}", "next_context": ", the most relevant to our paper being the regret bound of Azar, Osband, and Munos~\\cite{azar2017minimax}, which matches the lower bound for a certain range of parameters."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Since then, several works provided better regret upper bounds~\\cite{osband2014model,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,jin2020provably,yang2020reinforcement}, the most relevant to our paper being the regret bound of Azar, Osband, and Munos~\\cite{azar2017minimax}", "next_context": ", which matches the lower bound for a certain range of parameters."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "In all the classical works mentioned above~\\cite{auer2008near,azar2017minimax,zanette2019tighter,bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient}", "next_context": ", the way the agent interacts with the environment and accumulates regret is straightforward: once again, for fear of being repetitious, the agent chooses an actiona_tat some environment's statex_tand obtains a rewardr(x_t,a_t), after which the environment's state transitions tox_t+1."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Several RL algorithms~\\cite{auer2008near,bartlett2009regal,ortner2012online,lakshmanan2015improved,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,zhong2023provably,ganguly2023quantum}", "next_context": ", implement the principle of \u201coptimism in the face of uncertainty\u201d~\\cite{brafman2002r}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{azar2017minimax}", "next_context": ""}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Several regret bounds from\\Cref{res:res3,res:res4}considerably improve upon prior works~\\cite{auer2008near,azar2017minimax,zanette2019tighter,ortner2012online,lakshmanan2015improved,zhong2023provably,ganguly2023quantum}", "next_context": "(see\\Cref{table:results_finite-horizon,table:results_infinite-horizon}for a clear comparison)."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "For finite-horizon MDPs, our classical bound\\widetilde{O}(\\sqrt{HSAT})matches the one from~\\cite{azar2017minimax,zanette2019tighter}", "next_context": "whenT\\geqH^3S^3AandSA\\geqH, and avoids the extra terms\\widetilde{O}(H^2S^2A + H\\sqrt{T})outside this parameter range."}], "importance_score": 3.0285714285714285}, "jin2018qlearning": {"bib_key": "jin2018qlearning", "bib_title": "Is Q-learning provably efficient?", "bib_author ": "Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Since then, several works provided better regret upper bounds~\\cite{osband2014model,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,jin2020provably,yang2020reinforcement}", "next_context": ", the most relevant to our paper being the regret bound of Azar, Osband, and Munos~\\cite{azar2017minimax}, which matches the lower bound for a certain range of parameters."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Several RL algorithms~\\cite{auer2008near,bartlett2009regal,ortner2012online,lakshmanan2015improved,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,zhong2023provably,ganguly2023quantum}", "next_context": ", implement the principle of \u201coptimism in the face of uncertainty\u201d~\\cite{brafman2002r}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\multirow2*\\cite{jin2018qlearning}", "next_context": ""}], "importance_score": 1.2428571428571429}, "zanette2019tighter": {"bib_key": "zanette2019tighter", "bib_title": "Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds.", "bib_author ": "Andrea Zanette and Emma Brunskill.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Since then, several works provided better regret upper bounds~\\cite{osband2014model,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,jin2020provably,yang2020reinforcement}", "next_context": ", the most relevant to our paper being the regret bound of Azar, Osband, and Munos~\\cite{azar2017minimax}, which matches the lower bound for a certain range of parameters."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "This range was later improved by Zanette and Brunskill~\\cite{zanette2019tighter}", "next_context": "."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "In all the classical works mentioned above~\\cite{auer2008near,azar2017minimax,zanette2019tighter,bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient}", "next_context": ", the way the agent interacts with the environment and accumulates regret is straightforward: once again, for fear of being repetitious, the agent chooses an actiona_tat some environment's statex_tand obtains a rewardr(x_t,a_t), after which the environment's state transitions tox_t+1."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Several RL algorithms~\\cite{auer2008near,bartlett2009regal,ortner2012online,lakshmanan2015improved,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,zhong2023provably,ganguly2023quantum}", "next_context": ", implement the principle of \u201coptimism in the face of uncertainty\u201d~\\cite{brafman2002r}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{zanette2019tighter}", "next_context": ""}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Several regret bounds from\\Cref{res:res3,res:res4}considerably improve upon prior works~\\cite{auer2008near,azar2017minimax,zanette2019tighter,ortner2012online,lakshmanan2015improved,zhong2023provably,ganguly2023quantum}", "next_context": "(see\\Cref{table:results_finite-horizon,table:results_infinite-horizon}for a clear comparison)."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "For finite-horizon MDPs, our classical bound\\widetilde{O}(\\sqrt{HSAT})matches the one from~\\cite{azar2017minimax,zanette2019tighter}", "next_context": "whenT\\geqH^3S^3AandSA\\geqH, and avoids the extra terms\\widetilde{O}(H^2S^2A + H\\sqrt{T})outside this parameter range."}], "importance_score": 3.0285714285714285}, "efroni2019tight": {"bib_key": "efroni2019tight", "bib_title": "Tight regret bounds for model-based reinforcement learning with greedy policies.", "bib_author ": "Yonathan Efroni, Nadav Merlis, Mohammad Ghavamzadeh, and Shie Mannor.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Since then, several works provided better regret upper bounds~\\cite{osband2014model,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,jin2020provably,yang2020reinforcement}", "next_context": ", the most relevant to our paper being the regret bound of Azar, Osband, and Munos~\\cite{azar2017minimax}, which matches the lower bound for a certain range of parameters."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Several RL algorithms~\\cite{auer2008near,bartlett2009regal,ortner2012online,lakshmanan2015improved,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,zhong2023provably,ganguly2023quantum}", "next_context": ", implement the principle of \u201coptimism in the face of uncertainty\u201d~\\cite{brafman2002r}."}, {"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{efroni2019tight}", "next_context": ""}], "importance_score": 1.2428571428571429}, "jin2020provably": {"bib_key": "jin2020provably", "bib_title": "Provably efficient reinforcement learning with linear function approximation.", "bib_author ": "Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Since then, several works provided better regret upper bounds~\\cite{osband2014model,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,jin2020provably,yang2020reinforcement}", "next_context": ", the most relevant to our paper being the regret bound of Azar, Osband, and Munos~\\cite{azar2017minimax}, which matches the lower bound for a certain range of parameters."}], "importance_score": 0.14285714285714285}, "yang2020reinforcement": {"bib_key": "yang2020reinforcement", "bib_title": "Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound.", "bib_author ": "Lin Yang and Mengdi Wang.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "Since then, several works provided better regret upper bounds~\\cite{osband2014model,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,jin2020provably,yang2020reinforcement}", "next_context": ", the most relevant to our paper being the regret bound of Azar, Osband, and Munos~\\cite{azar2017minimax}, which matches the lower bound for a certain range of parameters."}], "importance_score": 0.14285714285714285}, "Lumbreras2022multiarmedquantum": {"bib_key": "Lumbreras2022multiarmedquantum", "bib_title": "Multi-armed quantum bandits: Exploration versus exploitation when learning properties of quantum states.", "bib_author ": "Josep Lumbreras, Erkka Haapasalo, and Marco Tomamichel.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "We are actually only aware of the works of Zhong\\emph{et al.}~\\cite{zhong2023provably}and Ganguly\\emph{et al.}~\\cite{ganguly2023quantum}, both for finite-horizon finite-state-space MDPs (there are further works on multi-armed bandits~\\cite{Lumbreras2022multiarmedquantum,dai2023quantum,wan2023quantum,su2025quantum}", "next_context": ")."}], "importance_score": 0.25}, "dai2023quantum": {"bib_key": "dai2023quantum", "bib_title": "Quantum bayesian optimization.", "bib_author ": "Zhongxiang Dai, Gregory Kang~Ruey Lau, Arun Verma, YAO SHU, Bryan Kian~Hsiang Low, and Patrick Jaillet.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "We are actually only aware of the works of Zhong\\emph{et al.}~\\cite{zhong2023provably}and Ganguly\\emph{et al.}~\\cite{ganguly2023quantum}, both for finite-horizon finite-state-space MDPs (there are further works on multi-armed bandits~\\cite{Lumbreras2022multiarmedquantum,dai2023quantum,wan2023quantum,su2025quantum}", "next_context": ")."}], "importance_score": 0.25}, "wan2023quantum": {"bib_key": "wan2023quantum", "bib_title": "Quantum multi-armed bandits and stochastic linear bandits enjoy logarithmic regrets.", "bib_author ": "Zongqi Wan, Zhijie Zhang, Tongyang Li, Jialin Zhang, and Xiaoming Sun.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "We are actually only aware of the works of Zhong\\emph{et al.}~\\cite{zhong2023provably}and Ganguly\\emph{et al.}~\\cite{ganguly2023quantum}, both for finite-horizon finite-state-space MDPs (there are further works on multi-armed bandits~\\cite{Lumbreras2022multiarmedquantum,dai2023quantum,wan2023quantum,su2025quantum}", "next_context": ")."}], "importance_score": 0.25}, "su2025quantum": {"bib_key": "su2025quantum", "bib_title": "Quantum algorithms for bandits with knapsacks with improved regret and time complexities.", "bib_author ": "Yuexin Su, Ziyi Yang, Peiyuan Huang, Tongyang Li, and Yinyu Ye.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "We are actually only aware of the works of Zhong\\emph{et al.}~\\cite{zhong2023provably}and Ganguly\\emph{et al.}~\\cite{ganguly2023quantum}, both for finite-horizon finite-state-space MDPs (there are further works on multi-armed bandits~\\cite{Lumbreras2022multiarmedquantum,dai2023quantum,wan2023quantum,su2025quantum}", "next_context": ")."}], "importance_score": 0.25}, "bai2019provably": {"bib_key": "bai2019provably", "bib_title": "Provably efficient q-learning with low switching cost.", "bib_author ": "Yu~Bai, Tengyang Xie, Nan Jiang, and Yu-Xiang Wang.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{bai2019provably}", "next_context": ""}], "importance_score": 1.0}, "zhang2020almost": {"bib_key": "zhang2020almost", "bib_title": "Almost optimal model-free reinforcement learningvia reference-advantage decomposition.", "bib_author ": "Zihan Zhang, Yuan Zhou, and Xiangyang Ji.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{zhang2020almost}", "next_context": ""}], "importance_score": 1.0}, "menard2021ucb": {"bib_key": "menard2021ucb", "bib_title": "UCB momentum Q-learning: Correcting the bias without forgetting.", "bib_author ": "Pierre Menard, Omar~Darwiche Domingues, Xuedong Shang, and Michal Valko.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{menard2021ucb}", "next_context": ""}], "importance_score": 1.0}, "li2021breaking": {"bib_key": "li2021breaking", "bib_title": "Breaking the sample complexity barrier to regret-optimal model-free reinforcement learning.", "bib_author ": "Gen Li, Laixi Shi, Yuxin Chen, Yuantao Gu, and Yuejie Chi.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{li2021breaking}", "next_context": ""}], "importance_score": 1.0}, "agrawal2024optimistic": {"bib_key": "agrawal2024optimistic", "bib_title": "Optimistic Q-learning for average reward and episodic reinforcement learning extended abstract.", "bib_author ": "Priyank Agrawal and Shipra Agrawal.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{agrawal2024optimistic}", "next_context": ""}], "importance_score": 1.0}, "zhang2019regret": {"bib_key": "zhang2019regret", "bib_title": "Regret minimization for reinforcement learning by evaluating the optimal bias function.", "bib_author ": "Zihan Zhang and Xiangyang Ji.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{zhang2019regret}", "next_context": "^\\dagger"}], "importance_score": 1.0}, "fruit2020improved": {"bib_key": "fruit2020improved", "bib_title": "Improved analysis of UCRL2 with empirical Bernstein inequality.", "bib_author ": "Ronan Fruit, Matteo Pirotta, and Alessandro Lazaric.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{fruit2020improved}", "next_context": ""}], "importance_score": 1.0}, "ortner2020regret": {"bib_key": "ortner2020regret", "bib_title": "Regret bounds for reinforcement learning via Markov chain concentration.", "bib_author ": "Ronald Ortner.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{ortner2020regret}", "next_context": "^\\dagger"}], "importance_score": 1.0}, "wei2020model": {"bib_key": "wei2020model", "bib_title": "Model-free reinforcement learning in infinite-horizon average-reward Markov decision processes.", "bib_author ": "Chen-Yu Wei, Mehdi~Jafarnia Jahromi, Haipeng Luo, Hiteshi Sharma, and Rahul Jain.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\cite{wei2020model}", "next_context": ""}], "importance_score": 1.0}, "wei2021learning": {"bib_key": "wei2021learning", "bib_title": "Learning infinite-horizon average-reward MDPs with linear function approximation.", "bib_author ": "Chen-Yu Wei, Mehdi Jafarnia~Jahromi, Haipeng Luo, and Rahul Jain.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": "Our work", "subsubsection": "Online learning of MDPs", "prev_context": "\\hline\\multirow2*\\cite{wei2021learning}", "next_context": ""}], "importance_score": 1.0}, "giovannetti2008architectures": {"bib_key": "giovannetti2008architectures", "bib_title": "Architectures for a quantum random access memory.", "bib_author ": "Vittorio Giovannetti, Seth Lloyd, and Lorenzo Maccone.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Preliminaries", "subsection": "Background on quantum computation", "subsubsection": null, "prev_context": "Quantum access to a function is usually referred to as a quantum random access memory (QRAM)~\\cite{giovannetti2008architectures,giovannetti2008quantum,jaques2023qram,allcock2023constant}", "next_context": "."}], "importance_score": 0.25}, "giovannetti2008quantum": {"bib_key": "giovannetti2008quantum", "bib_title": "Quantum random access memory.", "bib_author ": "Vittorio Giovannetti, Seth Lloyd, and Lorenzo Maccone.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Preliminaries", "subsection": "Background on quantum computation", "subsubsection": null, "prev_context": "Quantum access to a function is usually referred to as a quantum random access memory (QRAM)~\\cite{giovannetti2008architectures,giovannetti2008quantum,jaques2023qram,allcock2023constant}", "next_context": "."}], "importance_score": 0.25}, "jaques2023qram": {"bib_key": "jaques2023qram", "bib_title": "Qram: A survey and critique.", "bib_author ": "Samuel Jaques and Arthur~G Rattew.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Preliminaries", "subsection": "Background on quantum computation", "subsubsection": null, "prev_context": "Quantum access to a function is usually referred to as a quantum random access memory (QRAM)~\\cite{giovannetti2008architectures,giovannetti2008quantum,jaques2023qram,allcock2023constant}", "next_context": "."}], "importance_score": 0.25}, "allcock2023constant": {"bib_key": "allcock2023constant", "bib_title": "Constant-depth circuits for Boolean functions and quantum memory devices using multi-qubit gates.", "bib_author ": "Jonathan Allcock, Jinge Bao, Joao~F. Doriguello, Alessandro Luongo, and Miklos Santha.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Preliminaries", "subsection": "Background on quantum computation", "subsubsection": null, "prev_context": "Quantum access to a function is usually referred to as a quantum random access memory (QRAM)~\\cite{giovannetti2008architectures,giovannetti2008quantum,jaques2023qram,allcock2023constant}", "next_context": "."}], "importance_score": 0.25}, "van2021quantum": {"bib_key": "van2021quantum", "bib_title": "Ap", "bib_author ": "Joran", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Preliminaries", "subsection": "Classical and quantum subroutines", "subsubsection": null, "prev_context": "Several quantum mean estimation algorithms have been proposed~\\cite{montanaro2015quant,van2021quantum,hamoudi2021quantum,cornelissen2022near,Kothari2023mean}", "next_context": "."}, {"section": "Our reinforcement learning model", "subsection": "The reinforcement learning model of Zhong et al.", "subsubsection": null, "prev_context": "This is implicit when they use quantum multi-dimensional amplitude estimation~\\cite{van2021quantum}", "next_context": "withO(n(x,a))iterations for each state-action pair(x,a)\\in\\mathcal{X}\\times\\mathcal{A}, wheren(x,a)is the number of times(x,a)has been observed in the last exploration phase."}], "importance_score": 1.2}, "hamoudi2021quantum": {"bib_key": "hamoudi2021quantum", "bib_title": "Quantum Sub-Gaussian Mean Estimator.", "bib_author ": "Yassine Hamoudi.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Preliminaries", "subsection": "Classical and quantum subroutines", "subsubsection": null, "prev_context": "Several quantum mean estimation algorithms have been proposed~\\cite{montanaro2015quant,van2021quantum,hamoudi2021quantum,cornelissen2022near,Kothari2023mean}", "next_context": "."}], "importance_score": 0.2}, "saldi2017asymptotic": {"bib_key": "saldi2017asymptotic", "bib_title": "\\\"u", "bib_author ": "Naci Saldi, Serdar Y", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Preliminaries", "subsection": "Background on Markov decision processes", "subsubsection": "Infinite-horizon MDPs", "prev_context": "We refer the reader to~\\cite[Theorem~2.5]{saldi2017asymptotic}", "next_context": ",\\cite[Theorem~5.5.4]{hernandez2012discrete},\\cite[Chapter~3]{hernandez2001adaptive},\\cite[Theorem~2.6 \\"}, {"section": "Preliminaries", "subsection": "Discretization of state space", "subsubsection": null, "prev_context": "See~\\cite{saldi2017asymptotic}", "next_context": "for more information."}, {"section": "Preliminaries", "subsection": "Discretization of state space", "subsubsection": null, "prev_context": "Similar assumptions have been considered in~\\cite{saldi2017asymptotic, ortner2012online, kara2023q}", "next_context": "."}, {"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "r_d_\\varepsilon(x')\\mu^d_\\varepsilon_x(\\text{d}x')~\\cite[Theorem~2.5]{saldi2017asymptotic}", "next_context": "."}], "importance_score": 3.3333333333333335}, "hernandez2012discrete": {"bib_key": "hernandez2012discrete", "bib_title": "\\'e", "bib_author ": "On", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Preliminaries", "subsection": "Background on Markov decision processes", "subsubsection": "Infinite-horizon MDPs", "prev_context": "We refer the reader to~\\cite[Theorem~2.5]{saldi2017asymptotic},\\cite[Theorem~5.5.4]{hernandez2012discrete}", "next_context": ",\\cite[Chapter~3]{hernandez2001adaptive},\\cite[Theorem~2.6 \\"}], "importance_score": 1.0}, "hernandez2001adaptive": {"bib_key": "hernandez2001adaptive", "bib_title": "Adaptive Markov Control Processes.", "bib_author ": "O.~Hernandez-Lerma.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Preliminaries", "subsection": "Background on Markov decision processes", "subsubsection": "Infinite-horizon MDPs", "prev_context": "We refer the reader to~\\cite[Theorem~2.5]{saldi2017asymptotic},\\cite[Theorem~5.5.4]{hernandez2012discrete},\\cite[Chapter~3]{hernandez2001adaptive}", "next_context": ",\\cite[Theorem~2.6 \\"}], "importance_score": 1.0}, "gordienko1995average": {"bib_key": "gordienko1995average", "bib_title": "Average cost Markov control processes with weighted norms: existence of canonical policies.", "bib_author ": "Evgueni Gordienko and On\u00e9simo Hern\u00e1ndez-Lerma.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Preliminaries", "subsection": "Background on Markov decision processes", "subsubsection": "Infinite-horizon MDPs", "prev_context": " Lemma~3.4]{gordienko1995average}", "next_context": ",\\cite[Theorem~3]{jaskiewicz2006optimality},\\cite[Theorem 3.3]{vega2003average}, and\\cite[Chapter~10]{hernandez1999further}for more information."}], "importance_score": 1.0}, "jaskiewicz2006optimality": {"bib_key": "jaskiewicz2006optimality", "bib_title": "On the optimality equation for average cost Markov control processes with Feller transition probabilities.", "bib_author ": "Anna Ja\u015bkiewicz and Andrzej~S. Nowak.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Preliminaries", "subsection": "Background on Markov decision processes", "subsubsection": "Infinite-horizon MDPs", "prev_context": " Lemma~3.4]{gordienko1995average},\\cite[Theorem~3]{jaskiewicz2006optimality}", "next_context": ",\\cite[Theorem 3.3]{vega2003average}, and\\cite[Chapter~10]{hernandez1999further}for more information."}], "importance_score": 1.0}, "vega2003average": {"bib_key": "vega2003average", "bib_title": "The average cost optimality equation: a fixed point approach.", "bib_author ": "Oscar Vega-Amaya.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Preliminaries", "subsection": "Background on Markov decision processes", "subsubsection": "Infinite-horizon MDPs", "prev_context": " Lemma~3.4]{gordienko1995average},\\cite[Theorem~3]{jaskiewicz2006optimality},\\cite[Theorem 3.3]{vega2003average}", "next_context": ", and\\cite[Chapter~10]{hernandez1999further}for more information."}], "importance_score": 1.0}, "hernandez1999further": {"bib_key": "hernandez1999further", "bib_title": "Further Topics on Discrete-Time Markov Control Processes.", "bib_author ": "O.~Hernandez-Lerma and J.B. Lasserre.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Preliminaries", "subsection": "Background on Markov decision processes", "subsubsection": "Infinite-horizon MDPs", "prev_context": " Lemma~3.4]{gordienko1995average},\\cite[Theorem~3]{jaskiewicz2006optimality},\\cite[Theorem 3.3]{vega2003average}, and\\cite[Chapter~10]{hernandez1999further}", "next_context": "for more information."}], "importance_score": 1.0}, "gray1998quantization": {"bib_key": "gray1998quantization", "bib_title": "Quantization.", "bib_author ": "Robert~M. Gray and David~L. Neuhoff.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Preliminaries", "subsection": "Discretization of state space", "subsubsection": null, "prev_context": "The mapQ_n,\\mathcal{X}is often called a nearest neighbour quantizer with respect to distortion measurem_\\mathcal X~\\cite{gray1998quantization}", "next_context": "."}], "importance_score": 1.0}, "kara2023q": {"bib_key": "kara2023q", "bib_title": "Q-learning for continuous state and action MDPs under average cost criteria.", "bib_author ": "Ali~Devran Kara and Serdar Yuksel.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Preliminaries", "subsection": "Discretization of state space", "subsubsection": null, "prev_context": "Similar assumptions have been considered in~\\cite{saldi2017asymptotic, ortner2012online, kara2023q}", "next_context": "."}], "importance_score": 0.3333333333333333}, "auer2006logarithmic": {"bib_key": "auer2006logarithmic", "bib_title": "Logarithmic online regret bounds for undiscounted reinforcement learning.", "bib_author ": "Peter Auer and Ronald Ortner.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Our reinforcement learning model", "subsection": null, "subsubsection": null, "prev_context": "The infinite-horizon in-path regret\\operatornameRegret_\\infty^\\rmpath(T)is the standard choice for most prior works on RL~\\cite{auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,ortner2012online,fruit2018efficient}", "next_context": "and takes into account the actual path of observed state-action pairs(x_t,a_t)during learning, while the infinite-horizon expected regret\\operatornameRegret_\\infty^\\mathbb{E}(T)is a novel measure of regret that we introduce here."}, {"section": "Computing optimal policies for finite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Several RL algorithms~\\cite{auer2006logarithmic,bartlett2009regal,auer2008near,ortner2012online,lakshmanan2015improved,fruit2018efficient}", "next_context": "compute approximate optimal policies for the target unknown MDPMgiven empirical estimates of the true stochastic kernelspand rewardsr."}, {"section": "Algorithms for online learning of infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Like several classical RL algorithms~\\cite{ortner2012online,auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,fruit2018efficient}", "next_context": ", they proceed in episodes, during each of which a chosen policy remains fixed."}, {"section": "Algorithms for online learning of infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Unlike prior classical RL algorithms~\\cite{ortner2012online,auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,fruit2018efficient}", "next_context": ",\\cref{algo:quantum_UCCRL}does not keep estimates of the true stochastic kernels given the state-action pairs(x_t,a_t)observed during exploration phases, and therefore, it does not adhere to the standard``optimism-in-the-face-of-uncertainty''principle of maintaining a set of plausible MDPs\\mathcal{M}that contains the true MDPMwith high probability."}], "importance_score": 0.6666666666666666}, "Singh1994upper": {"bib_key": "Singh1994upper", "bib_title": "An upper bound on the loss from approximate optimal-value functions.", "bib_author ": "Satinder~P. Singh and Richard~C. Yee.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Computing optimal policies for finite-horizon MDPs", "subsection": "Classical approximate backward induction algorithm", "subsubsection": null, "prev_context": "In general, an\\epsilon-optimal value function yields an2\\epsilon/H-optimal greedy policy in the worst case~\\cite{Singh1994upper,bertsekas2012dynamic}", "next_context": "."}], "importance_score": 0.5}, "chen2023quantum": {"bib_key": "chen2023quantum", "bib_title": "Quantum Algorithms and Lower Bounds for Linear Regression with Norm Constraints.", "bib_author ": "Yanlin Chen and Ronald de~Wolf.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Computing optimal policies for finite-horizon MDPs", "subsection": "Quantum backward induction algorithm", "subsubsection": null, "prev_context": "To do so, we must analyse how quantum oracles fail (see~\\cite[Appendix~A]{chen2023quantum}", "next_context": "for a similar argument as follows)."}], "importance_score": 1.0}, "lutter2021value": {"bib_key": "lutter2021value", "bib_title": "Value iteration in continuous actions, states and time.", "bib_author ": "Michael Lutter, Shie Mannor, Jan Peters, Dieter Fox, and Animesh Garg.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Several classical algorithms have been proposed for such task, one of them main ones being value iteration~\\cite{kearns1998finite, lutter2021value, hartmanns2020optimistic, bertsekas1998new, quatmann2018sound, shani2007forward, weng2013interactive}", "next_context": ", which is quite similar to the backward induction algorithm covered in the last section: starting from some initial functionu_0\\in\\mathscr{B}(\\mathcal{X}), normallyu_0\\equiv0, generate a sequence of functions(u_t)_t\\in\\mathbb{N}according to the update ruleu_t+1=\\mathcal{L}u_t."}], "importance_score": 0.14285714285714285}, "hartmanns2020optimistic": {"bib_key": "hartmanns2020optimistic", "bib_title": "Optimistic value iteration.", "bib_author ": "Arnd Hartmanns and Benjamin~Lucien Kaminski.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Several classical algorithms have been proposed for such task, one of them main ones being value iteration~\\cite{kearns1998finite, lutter2021value, hartmanns2020optimistic, bertsekas1998new, quatmann2018sound, shani2007forward, weng2013interactive}", "next_context": ", which is quite similar to the backward induction algorithm covered in the last section: starting from some initial functionu_0\\in\\mathscr{B}(\\mathcal{X}), normallyu_0\\equiv0, generate a sequence of functions(u_t)_t\\in\\mathbb{N}according to the update ruleu_t+1=\\mathcal{L}u_t."}], "importance_score": 0.14285714285714285}, "bertsekas1998new": {"bib_key": "bertsekas1998new", "bib_title": "A new value iteration method for the average cost dynamic programming problem.", "bib_author ": "Dimitri~P Bertsekas.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Several classical algorithms have been proposed for such task, one of them main ones being value iteration~\\cite{kearns1998finite, lutter2021value, hartmanns2020optimistic, bertsekas1998new, quatmann2018sound, shani2007forward, weng2013interactive}", "next_context": ", which is quite similar to the backward induction algorithm covered in the last section: starting from some initial functionu_0\\in\\mathscr{B}(\\mathcal{X}), normallyu_0\\equiv0, generate a sequence of functions(u_t)_t\\in\\mathbb{N}according to the update ruleu_t+1=\\mathcal{L}u_t."}], "importance_score": 0.14285714285714285}, "quatmann2018sound": {"bib_key": "quatmann2018sound", "bib_title": "Sound value iteration.", "bib_author ": "Tim Quatmann and Joost-Pieter Katoen.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Several classical algorithms have been proposed for such task, one of them main ones being value iteration~\\cite{kearns1998finite, lutter2021value, hartmanns2020optimistic, bertsekas1998new, quatmann2018sound, shani2007forward, weng2013interactive}", "next_context": ", which is quite similar to the backward induction algorithm covered in the last section: starting from some initial functionu_0\\in\\mathscr{B}(\\mathcal{X}), normallyu_0\\equiv0, generate a sequence of functions(u_t)_t\\in\\mathbb{N}according to the update ruleu_t+1=\\mathcal{L}u_t."}], "importance_score": 0.14285714285714285}, "shani2007forward": {"bib_key": "shani2007forward", "bib_title": "Forward search value iteration for POMDPs.", "bib_author ": "Guy Shani, Ronen~I Brafman, and Solomon~Eyal Shimony.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Several classical algorithms have been proposed for such task, one of them main ones being value iteration~\\cite{kearns1998finite, lutter2021value, hartmanns2020optimistic, bertsekas1998new, quatmann2018sound, shani2007forward, weng2013interactive}", "next_context": ", which is quite similar to the backward induction algorithm covered in the last section: starting from some initial functionu_0\\in\\mathscr{B}(\\mathcal{X}), normallyu_0\\equiv0, generate a sequence of functions(u_t)_t\\in\\mathbb{N}according to the update ruleu_t+1=\\mathcal{L}u_t."}], "importance_score": 0.14285714285714285}, "weng2013interactive": {"bib_key": "weng2013interactive", "bib_title": "Interactive value iteration for Markov decision processes with unknown rewards.", "bib_author ": "Paul Weng and Bruno Zanuttini.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Several classical algorithms have been proposed for such task, one of them main ones being value iteration~\\cite{kearns1998finite, lutter2021value, hartmanns2020optimistic, bertsekas1998new, quatmann2018sound, shani2007forward, weng2013interactive}", "next_context": ", which is quite similar to the backward induction algorithm covered in the last section: starting from some initial functionu_0\\in\\mathscr{B}(\\mathcal{X}), normallyu_0\\equiv0, generate a sequence of functions(u_t)_t\\in\\mathbb{N}according to the update ruleu_t+1=\\mathcal{L}u_t."}], "importance_score": 0.14285714285714285}, "farahmand2010error": {"bib_key": "farahmand2010error", "bib_title": "\\'a", "bib_author ": "Amir-massoud Farahmand, Csaba Szepesv", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Approximate versions of value iteration have been studied and are used in various settings~\\cite{farahmand2010error, mann2015approximate, ernst2005approximate, munos2007performance, de2000existence, van2006performance}", "next_context": ", and here we consider a robust analogue of value iteration which differs from its standard implementation by generating a sequence of functions(u_t)_t\\in\\mathbb{N}such that\\label{eq:approximate_value_iteration}\\|u_t+1-\\mathcal{L}u_t\\|_\\infty\\leq\\varepsilon_u\\quad\\text{for a given}~\\varepsilon_u\\geq0,\\quad\\text{where}~t\\in\\mathbb{N}."}], "importance_score": 0.16666666666666666}, "mann2015approximate": {"bib_key": "mann2015approximate", "bib_title": "Approximate value iteration with temporally extended actions.", "bib_author ": "Timothy~A Mann, Shie Mannor, and Doina Precup.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Approximate versions of value iteration have been studied and are used in various settings~\\cite{farahmand2010error, mann2015approximate, ernst2005approximate, munos2007performance, de2000existence, van2006performance}", "next_context": ", and here we consider a robust analogue of value iteration which differs from its standard implementation by generating a sequence of functions(u_t)_t\\in\\mathbb{N}such that\\label{eq:approximate_value_iteration}\\|u_t+1-\\mathcal{L}u_t\\|_\\infty\\leq\\varepsilon_u\\quad\\text{for a given}~\\varepsilon_u\\geq0,\\quad\\text{where}~t\\in\\mathbb{N}."}], "importance_score": 0.16666666666666666}, "ernst2005approximate": {"bib_key": "ernst2005approximate", "bib_title": "Approximate value iteration in the reinforcement learning context. application to electrical power system control.", "bib_author ": "Damien Ernst, Mevludin Glavic, Pierre Geurts, and Louis Wehenkel.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Approximate versions of value iteration have been studied and are used in various settings~\\cite{farahmand2010error, mann2015approximate, ernst2005approximate, munos2007performance, de2000existence, van2006performance}", "next_context": ", and here we consider a robust analogue of value iteration which differs from its standard implementation by generating a sequence of functions(u_t)_t\\in\\mathbb{N}such that\\label{eq:approximate_value_iteration}\\|u_t+1-\\mathcal{L}u_t\\|_\\infty\\leq\\varepsilon_u\\quad\\text{for a given}~\\varepsilon_u\\geq0,\\quad\\text{where}~t\\in\\mathbb{N}."}], "importance_score": 0.16666666666666666}, "munos2007performance": {"bib_key": "munos2007performance", "bib_title": "emi Munos. Performance bounds in $L_p$\u2010norm for approximate value iteration. SIAM Journal on Control and Optimization, 46(2):541--561, 2007", "bib_author ": "R\\", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Approximate versions of value iteration have been studied and are used in various settings~\\cite{farahmand2010error, mann2015approximate, ernst2005approximate, munos2007performance, de2000existence, van2006performance}", "next_context": ", and here we consider a robust analogue of value iteration which differs from its standard implementation by generating a sequence of functions(u_t)_t\\in\\mathbb{N}such that\\label{eq:approximate_value_iteration}\\|u_t+1-\\mathcal{L}u_t\\|_\\infty\\leq\\varepsilon_u\\quad\\text{for a given}~\\varepsilon_u\\geq0,\\quad\\text{where}~t\\in\\mathbb{N}."}], "importance_score": 0.16666666666666666}, "de2000existence": {"bib_key": "de2000existence", "bib_title": "On the existence of fixed points for approximate value iteration and temporal-difference learning.", "bib_author ": "Daniela~Pucci De~Farias and Benjamin Van~Roy.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Approximate versions of value iteration have been studied and are used in various settings~\\cite{farahmand2010error, mann2015approximate, ernst2005approximate, munos2007performance, de2000existence, van2006performance}", "next_context": ", and here we consider a robust analogue of value iteration which differs from its standard implementation by generating a sequence of functions(u_t)_t\\in\\mathbb{N}such that\\label{eq:approximate_value_iteration}\\|u_t+1-\\mathcal{L}u_t\\|_\\infty\\leq\\varepsilon_u\\quad\\text{for a given}~\\varepsilon_u\\geq0,\\quad\\text{where}~t\\in\\mathbb{N}."}], "importance_score": 0.16666666666666666}, "van2006performance": {"bib_key": "van2006performance", "bib_title": "Performance loss bounds for approximate value iteration with state aggregation.", "bib_author ": "Benjamin Van~Roy.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Computing optimal policies for infinite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Approximate versions of value iteration have been studied and are used in various settings~\\cite{farahmand2010error, mann2015approximate, ernst2005approximate, munos2007performance, de2000existence, van2006performance}", "next_context": ", and here we consider a robust analogue of value iteration which differs from its standard implementation by generating a sequence of functions(u_t)_t\\in\\mathbb{N}such that\\label{eq:approximate_value_iteration}\\|u_t+1-\\mathcal{L}u_t\\|_\\infty\\leq\\varepsilon_u\\quad\\text{for a given}~\\varepsilon_u\\geq0,\\quad\\text{where}~t\\in\\mathbb{N}."}], "importance_score": 0.16666666666666666}, "strehl2006pac": {"bib_key": "strehl2006pac", "bib_title": "PAC model-free reinforcement learning.", "bib_author ": "Alexander~L. Strehl, Lihong Li, Eric Wiewiora, John Langford, and Michael~L. Littman.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Algorithms for online learning of finite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Like several classical RL algorithms for finite-horizon MDPs~\\cite{Kearns2002near,brafman2002r,strehl2006pac,auer2008near,dann2017unifying}", "next_context": ", they proceed in episodes."}, {"section": "Algorithms for online learning of finite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Unlike prior RL algorithms for finite-horizon MDPs~\\cite{Kearns2002near,brafman2002r,strehl2006pac,auer2008near,dann2017unifying,wang2021quantum}", "next_context": ",\\cref{algo:quantum_UCCRL_finite-horizon}does not keep an approximation\\widetilde{p}for the true stochastic kernelpvia the state-action pairs(x_t,a_t)observed during exploration phases and therefore does not employ\\widetilde{p}to calculate approximate optimal policies."}], "importance_score": 0.3666666666666667}, "dann2017unifying": {"bib_key": "dann2017unifying", "bib_title": "Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning.", "bib_author ": "Christoph Dann, Tor Lattimore, and Emma Brunskill.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Algorithms for online learning of finite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Like several classical RL algorithms for finite-horizon MDPs~\\cite{Kearns2002near,brafman2002r,strehl2006pac,auer2008near,dann2017unifying}", "next_context": ", they proceed in episodes."}, {"section": "Algorithms for online learning of finite-horizon MDPs", "subsection": null, "subsubsection": null, "prev_context": "Unlike prior RL algorithms for finite-horizon MDPs~\\cite{Kearns2002near,brafman2002r,strehl2006pac,auer2008near,dann2017unifying,wang2021quantum}", "next_context": ",\\cref{algo:quantum_UCCRL_finite-horizon}does not keep an approximation\\widetilde{p}for the true stochastic kernelpvia the state-action pairs(x_t,a_t)observed during exploration phases and therefore does not employ\\widetilde{p}to calculate approximate optimal policies."}], "importance_score": 0.3666666666666667}}, "refs": [], "table": [{"original": "\\begin{table}[t]\n\\centering\n\\def\\arraystretch{1.55}\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{|c|cc|cc|}\n\\hline\n\nWork /  & \\multicolumn{2}{c|}{Finite state space $\\mathcal{X}$ with size $S$} & \\multicolumn{2}{c|}{Compact state space $\\mathcal{X}=[0,1]^D$} \\\\ \n\\cline{2-5} \n$\\operatorname{Regret}_H(T)$ & \\multicolumn{1}{c|}{Regret (classical)} & Regret (quantum) & \\multicolumn{1}{c|}{Regret (classical)} & Regret (quantum) \\\\ \\hline \n\n\\cite{auer2008near} & \\multicolumn{1}{c|}{$\\sqrt{H^2S^2AT} + HSA$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{bartlett2009regal} & \\multicolumn{1}{c|}{$\\sqrt{H^2S^2AT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{ortner2012online} & \\multicolumn{1}{c|}{-} & - & \\multicolumn{1}{c|}{$\\sqrt{H^2 A}T^{\\frac{2D+\\alpha}{2D+2\\alpha}}$} & - \\\\ \\hline\n\n\\cite{lakshmanan2015improved} & \\multicolumn{1}{c|}{-} & - & \\multicolumn{1}{c|}{$\\sqrt{H^2 A}T^{\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha}}$} & - \\\\ \\hline\n\n\\cite{azar2017minimax} & \\multicolumn{1}{c|}{$\\sqrt{HSAT} + H^2S^2A + \\sqrt{H^2T}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n%\\cite{agrawal2017optimistic} & \\multicolumn{1}{c|}{$\\sqrt{H^2SAT} + H(S^7A^3T)^{\\frac{1}{4}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\multirow{2}{*}{\\cite{jin2018qlearning}} & \\multicolumn{1}{c|}{$\\sqrt{H^4SAT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \n\n& \\multicolumn{1}{c|}{$\\sqrt{H^3SAT} + \\sqrt{H^9S^3A^3}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{zanette2019tighter} & \\multicolumn{1}{c|}{$\\sqrt{HSAT} + H^2 S^{\\frac{3}{2}} A(\\sqrt{S} + \\sqrt{H})$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{efroni2019tight} & \\multicolumn{1}{c|}{$\\sqrt{HSAT} + H^2 S^{\\frac{3}{2}} A(\\sqrt{S} + \\sqrt{H})$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{bai2019provably} & \\multicolumn{1}{c|}{$\\sqrt{H^3SAT} + \\sqrt{H^9S^3A^3}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{zhang2020almost} & \\multicolumn{1}{c|}{$\\sqrt{H^2SAT} + H^8 S^2 A^{\\frac{3}{2}} T^{\\frac{1}{4}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{menard2021ucb} & \\multicolumn{1}{c|}{$\\sqrt{H^2SAT} + H^4 S A$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{li2021breaking} & \\multicolumn{1}{c|}{$\\sqrt{H^2SAT} + H^6 S A$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{zhong2023provably} & \\multicolumn{1}{c|}{-} & $H^3S^2A$ & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{ganguly2023quantum} & \\multicolumn{1}{c|}{-} & $H^2S^2A$ & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{agrawal2024optimistic} & \\multicolumn{1}{c|}{$\\sqrt{H^{12}S^2 AT} + H^9 S^2 A$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\rowcolor{Gray}\nThis work & \\multicolumn{1}{c|}{$\\sqrt{HSAT}$} & $\\min\\{HSA,H^2S\\sqrt{A}\\}$ & \\multicolumn{1}{c|}{$(H+\\sqrt{HA})T^{\\frac{D+\\alpha}{D+2\\alpha}}$} & $\\min\\{HA,H^2\\sqrt{A}\\}T^{\\frac{D}{D+\\alpha}}$ \\\\ \\hline\n\n\\end{tabular}}\n\\caption{Summary of several known upper bounds for the regret $\\operatorname{Regret}_H(T)$ of \\emph{finite-horizon} MDPs $\\langle \\mathcal{X},\\mathcal{A},H,p,r\\rangle$ with state space $\\mathcal{X}$, action space $\\mathcal{A}$ with size $A$, horizon $H$, and number of time steps $T$. The state space $\\mathcal{X}$ can be finite with size $S$ or $\\mathcal{X} = [0,1]^D$, in which case $p$ and $r$ are assumed to be H\\\"older continuous with exponent $\\alpha \\geq 0$. All bounds are up to $\\poly\\log$ factors in $A$, $H$, $T$, and $S$ (for $\\mathcal{X}$ finite). The reader should keep in mind that not all works assume the same RL model: there are model-based~\\cite{auer2008near} and model-free~\\cite{jin2018qlearning} approaches, while~\\cite{zhong2023provably,ganguly2023quantum} and our work assume a hybrid generative model. Some works focus on optimising other parameters besides regret, e.g., space and/or time.}\n\\label{table:results_finite-horizon}\n\\end{table}", "caption": "\\caption{Summary of several known upper bounds for the regret $\\operatorname{Regret}_H(T)$ of \\emph{finite-horizon} MDPs $\\langle \\mathcal{X},\\mathcal{A},H,p,r\\rangle$ with state space $\\mathcal{X}$, action space $\\mathcal{A}$ with size $A$, horizon $H$, and number of time steps $T$. The state space $\\mathcal{X}$ can be finite with size $S$ or $\\mathcal{X} = [0,1]^D$, in which case $p$ and $r$ are assumed to be H\\\"older continuous with exponent $\\alpha \\geq 0$. All bounds are up to $\\poly\\log$ factors in $A$, $H$, $T$, and $S$ (for $\\mathcal{X}$ finite). The reader should keep in mind that not all works assume the same RL model: there are model-based~\\cite{auer2008near} and model-free~\\cite{jin2018qlearning} approaches, while~\\cite{zhong2023provably,ganguly2023quantum} and our work assume a hybrid generative model. Some works focus on optimising other parameters besides regret, e.g., space and/or time.}", "label": "\\label{table:results_finite-horizon}", "tabular": "\\begin{tabular}{|c|cc|cc|}\n\\hline\n\nWork /  & \\multicolumn{2}{c|}{Finite state space $\\mathcal{X}$ with size $S$} & \\multicolumn{2}{c|}{Compact state space $\\mathcal{X}=[0,1]^D$} \\\\ \n\\cline{2-5} \n$\\operatorname{Regret}_H(T)$ & \\multicolumn{1}{c|}{Regret (classical)} & Regret (quantum) & \\multicolumn{1}{c|}{Regret (classical)} & Regret (quantum) \\\\ \\hline \n\n\\cite{auer2008near} & \\multicolumn{1}{c|}{$\\sqrt{H^2S^2AT} + HSA$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{bartlett2009regal} & \\multicolumn{1}{c|}{$\\sqrt{H^2S^2AT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{ortner2012online} & \\multicolumn{1}{c|}{-} & - & \\multicolumn{1}{c|}{$\\sqrt{H^2 A}T^{\\frac{2D+\\alpha}{2D+2\\alpha}}$} & - \\\\ \\hline\n\n\\cite{lakshmanan2015improved} & \\multicolumn{1}{c|}{-} & - & \\multicolumn{1}{c|}{$\\sqrt{H^2 A}T^{\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha}}$} & - \\\\ \\hline\n\n\\cite{azar2017minimax} & \\multicolumn{1}{c|}{$\\sqrt{HSAT} + H^2S^2A + \\sqrt{H^2T}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n%\\cite{agrawal2017optimistic} & \\multicolumn{1}{c|}{$\\sqrt{H^2SAT} + H(S^7A^3T)^{\\frac{1}{4}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\multirow{2}{*}{\\cite{jin2018qlearning}} & \\multicolumn{1}{c|}{$\\sqrt{H^4SAT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \n\n& \\multicolumn{1}{c|}{$\\sqrt{H^3SAT} + \\sqrt{H^9S^3A^3}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{zanette2019tighter} & \\multicolumn{1}{c|}{$\\sqrt{HSAT} + H^2 S^{\\frac{3}{2}} A(\\sqrt{S} + \\sqrt{H})$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{efroni2019tight} & \\multicolumn{1}{c|}{$\\sqrt{HSAT} + H^2 S^{\\frac{3}{2}} A(\\sqrt{S} + \\sqrt{H})$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{bai2019provably} & \\multicolumn{1}{c|}{$\\sqrt{H^3SAT} + \\sqrt{H^9S^3A^3}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{zhang2020almost} & \\multicolumn{1}{c|}{$\\sqrt{H^2SAT} + H^8 S^2 A^{\\frac{3}{2}} T^{\\frac{1}{4}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{menard2021ucb} & \\multicolumn{1}{c|}{$\\sqrt{H^2SAT} + H^4 S A$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{li2021breaking} & \\multicolumn{1}{c|}{$\\sqrt{H^2SAT} + H^6 S A$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{zhong2023provably} & \\multicolumn{1}{c|}{-} & $H^3S^2A$ & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{ganguly2023quantum} & \\multicolumn{1}{c|}{-} & $H^2S^2A$ & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{agrawal2024optimistic} & \\multicolumn{1}{c|}{$\\sqrt{H^{12}S^2 AT} + H^9 S^2 A$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\rowcolor{Gray}\nThis work & \\multicolumn{1}{c|}{$\\sqrt{HSAT}$} & $\\min\\{HSA,H^2S\\sqrt{A}\\}$ & \\multicolumn{1}{c|}{$(H+\\sqrt{HA})T^{\\frac{D+\\alpha}{D+2\\alpha}}$} & $\\min\\{HA,H^2\\sqrt{A}\\}T^{\\frac{D}{D+\\alpha}}$ \\\\ \\hline\n\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[t]\n\\centering\n\\def\\arraystretch{1.5}\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{|c|cc|cc|}\n\\hline\n\nWork /  & \\multicolumn{2}{c|}{Finite state space $\\mathcal{X}$ with size $S$} & \\multicolumn{2}{c|}{Compact state space $\\mathcal{X}=[0,1]^D$} \\\\ \n\\cline{2-5} \n$\\operatorname{Regret}_\\infty^{\\rm path}(T)$ & \\multicolumn{1}{c|}{Regret (classical)} & Regret (quantum) & \\multicolumn{1}{c|}{Regret (classical)} & Regret (quantum) \\\\ \\hline \n\n\\cite{auer2008near} & \\multicolumn{1}{c|}{$\\sqrt{\\Delta^2S^2AT} + \\Delta SA$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{bartlett2009regal}$^\\dagger$ & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2S^2AT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{ortner2012online}$^\\dagger$ & \\multicolumn{1}{c|}{-} & - & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 A}T^{\\frac{2D+\\alpha}{2D+2\\alpha}}$} & - \\\\ \\hline\n\n\\cite{lakshmanan2015improved}$^\\dagger$ & \\multicolumn{1}{c|}{-} & - & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 A}T^{\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha}}$} & - \\\\ \\hline\n\n%\\cite{agrawal2017optimistic} & \\multicolumn{1}{c|}{$\\sqrt{\\Delta^2SAT} + \\Delta(S^7A^3T)^{\\frac{1}{4}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{ouyang2017learning} & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 S^2 AT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{fruit2018efficient} & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 S^2 AT} + \\Lambda S^2A$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{zhang2019regret}$^\\dagger$ & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda S AT} + \\Lambda(S^{10} A T)^{\\frac{1}{4}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{fruit2020improved} & \\multicolumn{1}{c|}{$\\sqrt{\\Delta S^2 AT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{ortner2020regret}$^\\dagger$ & \\multicolumn{1}{c|}{$\\sqrt{t_{\\rm mix} S AT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{wei2020model} & \\multicolumn{1}{c|}{$\\Lambda (S AT^2)^{\\frac{1}{3}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\multirow{2}{*}{\\cite{wei2021learning}} & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 S^3 A^3 T}$} & - & \\multicolumn{1}{c|}{-} & - \\\\\n\n& \\multicolumn{1}{c|}{$\\sqrt{\\Lambda} (S A T)^{\\frac{3}{4}} + (\\Lambda S A T)^{\\frac{2}{3}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{zhang2023sharper} & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 S^{10} A^4 T}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\n\\rowcolor{Gray}\nThis work & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 SAT}$} & $\\sqrt{\\Lambda^2 T} + \\sqrt{\\Lambda^2 S^2 A}$ & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 A}T^{\\frac{D+\\alpha}{D+2\\alpha}}$} & $\\sqrt{\\Lambda^2 T} + \\sqrt{\\Lambda^2 A}T^{\\frac{D}{D+\\alpha}}$ \\\\ \\hline\\hline\n\n\\rowcolor{Gray}\n$\\operatorname{Regret}_\\infty^{\\mathbb{E}}(T)$ & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 SAT}$} & $\\sqrt{\\Lambda^2 S^2 A}$ & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 A}T^{\\frac{D+\\alpha}{D+2\\alpha}}$} & $\\sqrt{\\Lambda^2 A}T^{\\frac{D}{D+\\alpha}}$ \\\\ \\hline\n\n\\end{tabular}}\n\\caption{Summary of several known upper bounds for the \\emph{in-path} regret $\\operatorname{Regret}_\\infty^{\\rm path}(T)$ of \\emph{infinite-horizon} average-reward MDPs $\\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ with state space $\\mathcal{X}$, action space $\\mathcal{A}$ with size $A$, $\\operatorname{sp}(h^\\ast) \\leq \\Lambda$, and number of time steps $T$. The state space $\\mathcal{X}$ can be finite with size $S$ or $\\mathcal{X} = [0,1]^D$, in which case $p$ and $r$ are assumed to be H\\\"older continuous with exponent $\\alpha \\geq 0$. Here $\\Delta \\geq \\Lambda$ and $t_{\\rm mix}$ are the MDP's diameter and mixing time, respectively. In the last row, we show our bounds for the \\emph{expected} regret $\\operatorname{Regret}_\\infty^{\\mathbb{E}}(T)$. All bounds are up to $\\poly\\log$ factors in $A$, $\\Lambda$, $T$, and $S$ (for $\\mathcal{X}$ finite). Refs.\\ marked with $\\dagger$ have no efficient implementation. The reader should keep in mind that not all works assume the same RL model.}\n\\label{table:results_infinite-horizon}\n\\end{table}", "caption": "\\caption{Summary of several known upper bounds for the \\emph{in-path} regret $\\operatorname{Regret}_\\infty^{\\rm path}(T)$ of \\emph{infinite-horizon} average-reward MDPs $\\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ with state space $\\mathcal{X}$, action space $\\mathcal{A}$ with size $A$, $\\operatorname{sp}(h^\\ast) \\leq \\Lambda$, and number of time steps $T$. The state space $\\mathcal{X}$ can be finite with size $S$ or $\\mathcal{X} = [0,1]^D$, in which case $p$ and $r$ are assumed to be H\\\"older continuous with exponent $\\alpha \\geq 0$. Here $\\Delta \\geq \\Lambda$ and $t_{\\rm mix}$ are the MDP's diameter and mixing time, respectively. In the last row, we show our bounds for the \\emph{expected} regret $\\operatorname{Regret}_\\infty^{\\mathbb{E}}(T)$. All bounds are up to $\\poly\\log$ factors in $A$, $\\Lambda$, $T$, and $S$ (for $\\mathcal{X}$ finite). Refs.\\ marked with $\\dagger$ have no efficient implementation. The reader should keep in mind that not all works assume the same RL model.}", "label": "\\label{table:results_infinite-horizon}", "tabular": "\\begin{tabular}{|c|cc|cc|}\n\\hline\n\nWork /  & \\multicolumn{2}{c|}{Finite state space $\\mathcal{X}$ with size $S$} & \\multicolumn{2}{c|}{Compact state space $\\mathcal{X}=[0,1]^D$} \\\\ \n\\cline{2-5} \n$\\operatorname{Regret}_\\infty^{\\rm path}(T)$ & \\multicolumn{1}{c|}{Regret (classical)} & Regret (quantum) & \\multicolumn{1}{c|}{Regret (classical)} & Regret (quantum) \\\\ \\hline \n\n\\cite{auer2008near} & \\multicolumn{1}{c|}{$\\sqrt{\\Delta^2S^2AT} + \\Delta SA$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{bartlett2009regal}$^\\dagger$ & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2S^2AT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{ortner2012online}$^\\dagger$ & \\multicolumn{1}{c|}{-} & - & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 A}T^{\\frac{2D+\\alpha}{2D+2\\alpha}}$} & - \\\\ \\hline\n\n\\cite{lakshmanan2015improved}$^\\dagger$ & \\multicolumn{1}{c|}{-} & - & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 A}T^{\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha}}$} & - \\\\ \\hline\n\n%\\cite{agrawal2017optimistic} & \\multicolumn{1}{c|}{$\\sqrt{\\Delta^2SAT} + \\Delta(S^7A^3T)^{\\frac{1}{4}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{ouyang2017learning} & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 S^2 AT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{fruit2018efficient} & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 S^2 AT} + \\Lambda S^2A$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{zhang2019regret}$^\\dagger$ & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda S AT} + \\Lambda(S^{10} A T)^{\\frac{1}{4}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{fruit2020improved} & \\multicolumn{1}{c|}{$\\sqrt{\\Delta S^2 AT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{ortner2020regret}$^\\dagger$ & \\multicolumn{1}{c|}{$\\sqrt{t_{\\rm mix} S AT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{wei2020model} & \\multicolumn{1}{c|}{$\\Lambda (S AT^2)^{\\frac{1}{3}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\multirow{2}{*}{\\cite{wei2021learning}} & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 S^3 A^3 T}$} & - & \\multicolumn{1}{c|}{-} & - \\\\\n\n& \\multicolumn{1}{c|}{$\\sqrt{\\Lambda} (S A T)^{\\frac{3}{4}} + (\\Lambda S A T)^{\\frac{2}{3}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{zhang2023sharper} & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 S^{10} A^4 T}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\n\\rowcolor{Gray}\nThis work & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 SAT}$} & $\\sqrt{\\Lambda^2 T} + \\sqrt{\\Lambda^2 S^2 A}$ & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 A}T^{\\frac{D+\\alpha}{D+2\\alpha}}$} & $\\sqrt{\\Lambda^2 T} + \\sqrt{\\Lambda^2 A}T^{\\frac{D}{D+\\alpha}}$ \\\\ \\hline\\hline\n\n\\rowcolor{Gray}\n$\\operatorname{Regret}_\\infty^{\\mathbb{E}}(T)$ & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 SAT}$} & $\\sqrt{\\Lambda^2 S^2 A}$ & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 A}T^{\\frac{D+\\alpha}{D+2\\alpha}}$} & $\\sqrt{\\Lambda^2 A}T^{\\frac{D}{D+\\alpha}}$ \\\\ \\hline\n\n\\end{tabular}", "subtables": []}], "figure": [], "equations": ["\\begin{align*}\n    V_1^\\pi(x) := \\mathbb{E}_{x_1=x}^\\pi\\left[\\sum_{t=1}^H r(x_t,a_t)\\right].\n\\end{align*}", "\\begin{align*}\n    g^\\pi(x) := \\lim_{T\\to\\infty}\\mathbb{E}_{x_1=x}^\\pi\\left[\\frac{1}{T}\\sum_{t=1}^T r(x_t,a_t) \\right].\n\\end{align*}", "\\begin{align}\\label{eq:quantum_sampling_oracle}\n    \\mathcal O_{p}: \\ket{x}\\ket{a}\\ket{\\bar 0}\\rightarrow \\int_{x'\\in\\mathcal X} \\sqrt{p(\\rd x'\\vert x, a)} \\ket{x}\\ket{a}\\ket{x'} \\quad\\text{for all}\\quad (x,a)\\in\\mathcal{X}\\times\\mathcal{A}\n\\end{align}", "\\begin{align*}\n    \\operatorname{Regret}_{H}(T) := \\sum_{k=1}^K \\big(V_1^\\ast(x^{(k)}_1) - V_1^{\\pi^{(k)}}(x^{(k)}_1) \\big),\n\\end{align*}", "\\begin{align*}\n    \\operatorname{Regret}_{\\infty}^{\\rm path}(T) := Tg^\\ast - \\sum_{t=1}^T r(x_t,a_t).\n\\end{align*}", "\\begin{align*}\n        \\textbf{Classical:}&~ \\widetilde{O}\\big(\\sqrt{HSAT\\log(HSA)} \\big),\\\\\n        \\textbf{Quantum:}&~ \\widetilde{O}\\big(S\\min\\{HA,H^2\\sqrt{A}\\log(HS)\\}\\log(T/H)\\log(HSA)\\big),\n    \\end{align*}", "\\begin{align*}\n        \\textbf{Classical:}&~ \\widetilde{O}\\big(T^{\\frac{D+\\alpha}{D+2\\alpha}} ( H + \\sqrt{HA\\log(HAT)} \\big),\\\\\n        \\textbf{Quantum:}&~ \\widetilde{O}\\big(\\min\\{HA,H^2\\sqrt{A}\\log(HT)\\}T^{\\frac{D}{D+\\alpha}} \\log(T/H) \\log(HAT) \\big).\n    \\end{align*}", "\\begin{align*}\n        \\textbf{Classical:}&~ \\widetilde{O}\\big(\\Lambda\\sqrt{SAT\\log{T}\\log(SAT)} \\big), \\\\\n        \\textbf{Quantum:}&~ \\widetilde{O}\\big(\\Lambda\\sqrt{T\\log{T}} + \\Lambda S\\sqrt{A}\\log^2{T}\\log(SAT)\\log(ST)\\big),\n    \\end{align*}", "\\begin{align*}\n        \\textbf{Classical:}&~ \\widetilde{O}\\big(\\Lambda T^{\\frac{D+\\alpha}{D+2\\alpha}} \\sqrt{A\\log{T}\\log(AT)} \\big),\\\\\n        \\textbf{Quantum:}&~ \\widetilde{O}\\big(\\Lambda\\sqrt{T\\log{T}} + \\Lambda \\sqrt{A} T^{\\frac{D}{D+\\alpha}}\\log^3{T}\\log(AT) \\big).\n    \\end{align*}", "\\begin{align*}\n    \\operatorname{Regret}_\\infty^{\\mathbb{E}}(T) := \\sum_{t=1}^T \\left(g^\\ast - \\min_{x\\in\\mathcal{X}}g^{\\pi_t^\\infty}(x) \\right),\n\\end{align*}", "\\begin{align*}\n        \\widetilde{O}\\big(\\Lambda S\\sqrt{A}\\log^2{T}\\log(SAT)\\log(ST)\\big),\n    \\end{align*}", "\\begin{align*}\n        \\widetilde{O}\\big(\\Lambda\\sqrt{A} T^{\\frac{D}{D+\\alpha}}\\log^3{T}\\log(AT) \\big).\n    \\end{align*}", "\\begin{align*}\n        \\sum_{k=1}^m \\frac{z_k}{Z_{k-1}^{1-\\gamma}} \\leq \\frac{Z_m^\\gamma}{2^\\gamma-1} \\quad \\text{for any}\\quad \\gamma\\in[0,1].\n    \\end{align*}", "\\begin{align*}\n        \\sum_{k=1}^n \\frac{z_k}{Z_{k-1}} \\leq 4\\log_2(Z_n/2).\n    \\end{align*}", "\\begin{align*}\n        \\sum_{k=1}^n \\frac{z_k}{Z_{k-1}} \\leq \\sum_{k=1}^n \\frac{z_k}{Z_{k-1}^{1 - \\gamma}} \\leq \\frac{Z_n^\\gamma}{2^\\gamma-1} \\quad\\text{for any}\\quad \\gamma\\in [0, 1].\n    \\end{align*}", "\\begin{align*}\n        \\sum_{k=1}^n \\frac{z_k}{Z_{k-1}} \n        &\\leq \\frac{Z_n^{\\log_2\\left(\\frac{\\ln Z_n}{\\ln(Z_n/2)}\\right)}}{\\frac{\\ln Z_n}{\\ln(Z_n/2)} - 1} = \\frac{\\ln(Z_n/2)}{\\ln 2}\\left(1 + \\frac{\\ln 2}{\\ln(Z_n/2)}\\right)^{\\log_2 Z_n} \\leq \\frac{4}{\\ln 2}\\ln(Z_n/2),\n    \\end{align*}", "\\begin{align*}\n        \\operatorname{Pr}\\left[\\left|\\frac{1}{m}\\sum_{i=1}^m u(x_i) - \\int_{\\mathcal{X}} p(\\rd x) u(x) \\right| \\leq \\|u\\|_\\infty\\sqrt{\\frac{2}{m}\\ln\\frac{2}{\\delta}} \\right] \\geq 1-\\delta.\n    \\end{align*}", "\\begin{align*}\n        \\operatorname{Pr}\\left[\\left|\\frac{1}{m}\\sum_{i=1}^m u(x_i) - \\int_{\\mathcal{X}} p(\\rd x) u(x) \\right| \\leq \\sqrt{\\frac{2\\sigma}{m}\\ln\\frac{2}{\\delta}} + \\frac{2\\|u\\|_\\infty}{3m}\\ln\\frac{2}{\\delta} \\right] \\geq 1-\\delta.\n    \\end{align*}", "\\begin{align*}\n    r_{d}(x) := \\operatorname*{\\mathbb{E}}_{a\\sim d(\\cdot|x)}[r(x,a)] \\qquad\\text{and} \\qquad p_{d}(x'|x) := \\operatorname*{\\mathbb{E}}_{a\\sim d(\\cdot|x)}[p(x'|x,a)].\n\\end{align*}", "\\begin{align*}\n    P_\\mu^\\pi(X_1 = x) &= \\mu(x), \\\\\n    P_\\mu^\\pi(A_t = a|X_t = x_t) &= \\pi_{t}(a|x_{t}),\\\\\n    P_\\mu^\\pi(X_{t+1} = x|X_{t} = x_{t}, A_{t} = a_{t}) &= p(x|x_{t},a_{t}).\n\\end{align*}", "\\begin{align*}\n    V_1^\\pi(x) := \\mathbb{E}_x^\\pi\\left[\\sum_{t=1}^{H} r(x_{t},a_{t})\\right].\n\\end{align*}", "\\begin{align*}\n    V_t^\\pi(x) := \\mathbb{E}^\\pi_{x_{t}=x}\\left[\\sum_{t'=t}^{H} r(x_{t'},a_{t'}) \\right] \\qquad\\forall x\\in\\mathcal{X}.\n\\end{align*}", "\\begin{align*}\n    V^{\\pi_\\varepsilon}_1(x) \\geq V^\\pi_1(x) - \\varepsilon \\qquad\\forall x\\in\\mathcal{X}, \\pi\\in\\Pi^{\\rm R},\n\\end{align*}", "\\begin{align*}\n    V^\\ast_t(x) := \\sup_{\\pi\\in\\Pi^{\\rm R}} V^\\pi_t(x).\n\\end{align*}", "\\begin{align*}\n    \\forall u\\in\\mathscr{B}(\\mathcal{X}), x\\in\\mathcal{X} :\\quad (\\mathcal{L}_d u)(x) := r_d(x) + \\int_{\\mathcal{X}} \\! p_d(\\rd x'|x) u(x')  \\quad\\text{and}\\quad (\\mathcal{L}u)(x) := \\max_{d\\in\\mathcal{D}^{\\rm D}}\\{(\\mathcal{L}_d u)(x)\\}.\n\\end{align*}", "\\begin{align*}\n    u_{H+1} \\equiv 0 \\qquad\\text{and}\\qquad u_{t} = \\mathcal{L}u_{t+1}, \\quad t\\in[H], \n\\end{align*}", "\\begin{align*}\n    g^{\\pi}(x) := \\lim_{T\\to\\infty} \\mathbb{E}_x^\\pi\\left[\\frac{1}{T}\\sum_{t=1}^{T}r(x_t, a_t)\\right]. %= \\lim_{T\\to\\infty}\\frac{1}{T}\\sum_{t=0}^{T-1} \\int_{\\mathcal{X}} r_{\\pi^{(t)}}(y)p^{(t)}(\\rd y|x).\n\\end{align*}", "\\begin{align*}\n     g^{\\pi^\\varepsilon}(x) \\geq g^{\\pi}(x) - \\varepsilon \\quad\\forall x\\in\\mathcal{X}, \\pi\\in\\Pi^{\\rm R}.\n \\end{align*}", "\\begin{align*}\n     g^*(x) := \\sup_{\\pi\\in\\Pi^{\\rm R}} g^{\\pi}(x).\n\\end{align*}", "\\begin{align}\\label{eq:bias_expression}\n    h^{d^\\infty}(x) := \\lim_{T\\to \\infty} \\mathbb{E}_x^{d^\\infty}\\left[\\sum_{t=1}^{T} r_d(x_t) - g^{d^\\infty}(x_t)\\right] \\qquad\\forall x\\in\\mathcal{X}.\n\\end{align}", "\\begin{align*}\n    g^{d^\\infty}(x) = \\int_{\\mathcal{X}} p_d(\\rd y|x) g^{d^\\infty}(y)  \\qquad\\text{and}\\qquad h^{d^\\infty} = \\mathcal{L}_d h^{d^\\infty} - g^{d^\\infty}.\n\\end{align*}", "\\begin{align*}\n    h^\\ast = \\mathcal{L}h^\\ast - g^\\ast e.\n\\end{align*}", "\\begin{align*}\n    \\min_{i\\in[k_n]} m_{\\mathcal X}\\left(x, s_{n,i}\\right) < \\frac{1}{n}\\quad \\text{ for all }x\\in\\mathcal X. \n\\end{align*}", "\\begin{align*}\n    Q_{n, \\mathcal X}: \\mathcal X\\to \\mathcal{S}_n \\quad \\text{as}\\quad Q_{n, \\mathcal X}(x) = \\argmin_{s\\in\\mathcal{S}_n} m_{\\mathcal X}(x,s),\n\\end{align*}", "\\begin{align*}\n     \\mathcal X_{n, i} = \\{x\\in\\mathcal X: Q_{n,\\mathcal X}(x) = s_{n, i}\\},\n\\end{align*}", "\\begin{align*}\n    \\mathcal X_{n,0} = \\left[0, \\frac{1}{n}\\right] \\quad\\text{and}\\quad \\mathcal X_{n,i} = \\left(\\frac{i}{n}, \\frac{i+1}{n}\\right] ~\\text{for}~i = 1, \\dots, n-1. \n\\end{align*}", "\\begin{align*}\n        |  r(x, a) - r(x', a)| &\\leq L\\| x - x'\\|_2^\\alpha \\quad\\text{for all}\\quad(x,x',a)\\in\\mathcal{X}\\times\\mathcal{X}\\times\\mathcal{A},\\\\\n        \\Vert  p(\\cdot\\vert x, a) -  p(\\cdot\\vert x', a)\\Vert_{\\rm tvd} &\\leq L\\| x - x'\\|_2^\\alpha \\quad\\text{for all}\\quad(x,x',a)\\in\\mathcal{X}\\times\\mathcal{X}\\times\\mathcal{A}.\n    \\end{align*}", "\\begin{align*}\n    \\operatorname{Regret}_{H}(T) := \\sum_{k=1}^K \\big(V_1^\\ast(x^{(k)}_1) - V_1^{\\pi^{(k)}}(x^{(k)}_1) \\big).\n\\end{align*}", "\\begin{align*}\n    \\operatorname{Regret}_{\\infty}^{\\rm path}(T) := Tg^\\ast - \\sum_{t=1}^{T} r(x_{t},a_{t}),\n\\end{align*}", "\\begin{align*}\n    \\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T) := Tg^\\ast - \\sum_{t=1}^T\\min_{x\\in\\mathcal{X}}g^{d_t^\\infty}(x),\n\\end{align*}", "\\begin{align*}\n        \\mathcal O_{p}: \\ket{x}\\ket{a}\\ket{\\bar 0}\\rightarrow \\int_{x'\\in\\mathcal X} \\sqrt{p(\\rd x'\\vert x, a)} \\ket{x}\\ket{a}\\ket{x'} \\quad\\text{for all}\\quad (x,a)\\in\\mathcal{X}\\times\\mathcal{A}.\n    \\end{align*}", "\\begin{align*}\n        \\mathcal O_{p}: \\ket{s}\\ket{a}\\ket{\\bar 0}\\rightarrow \\sum_{s'\\in\\mathcal{S}} \\sqrt{p(s'\\vert s, a)} \\ket{s}\\ket{a}\\ket{s'} \\quad\\text{for all}\\quad (s,a)\\in\\mathcal{S}\\times\\mathcal{A}.\n    \\end{align*}", "\\begin{align*}\n        \\mathcal O_r:\\ket{x}\\ket{a}\\ket{\\bar 0}\\rightarrow \\ket{x}\\ket{a}\\ket{r(x, a)} \\quad\\text{for all}\\quad (x,a)\\in\\mathcal{X}\\times\\mathcal{A}.\n    \\end{align*}", "\\begin{align*}\n    \\mathcal{O}_{\\pi_t}:|x\\rangle|\\bar{0}\\rangle \\mapsto \\sum_{a\\in\\mathcal{A}} \\sqrt{\\pi_t(a|x)}|x\\rangle|a\\rangle \\qquad \\forall x\\in\\mathcal{X},\n\\end{align*}", "\\begin{align*}\n    \\forall x\\in\\mathcal{X}: \\qquad u_{t}(x) = (\\mathcal{L}u_{t+1})(x) \\qquad\\text{and}\\qquad \\pi_{t}(x) \\in \\argmax_{a\\in\\mathcal{A}}\\{(\\mathcal{L}_a u_{t+1})(x)\\}.\n\\end{align*}", "\\begin{align*}\n    u_{t}^{(k)}(x) \\gets \\max_{a\\in\\mathcal{A}}\\left\\{r(x,a) + \\int_{\\mathcal{X}}p(\\text{d}x'|x,a)(u^{(k)}_{t+1}(x') - u^{(k-1)}_{t+1}(x')) + \\int_{\\mathcal{X}}p(\\text{d}x'|x,a) u^{(k-1)}_{t+1}(x')  \\right\\}.\n\\end{align*}", "\\begin{align*}\n        \\forall (s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]:\\qquad\n        \\begin{aligned}\n            \\widetilde{\\mu}_t(s,a) &:= \\frac{1}{m}\\sum_{i=1}^m u_t(x_{s,a}^{(i)}),\\\\\n            \\widetilde{\\sigma}_t(s,a) &:= \\frac{1}{m}\\sum_{i=1}^{m} u_t(x^{(i)}_{s,a})^2 - \\left(\\frac{1}{m}\\sum_{i=1}^{m} u_t(x^{(i)}_{s,a}) \\right)^2.\n        \\end{aligned}\n    \\end{align*}", "\\begin{aligned}\n            \\widetilde{\\mu}_t(s,a) &:= \\frac{1}{m}\\sum_{i=1}^m u_t(x_{s,a}^{(i)}),\\\\\n            \\widetilde{\\sigma}_t(s,a) &:= \\frac{1}{m}\\sum_{i=1}^{m} u_t(x^{(i)}_{s,a})^2 - \\left(\\frac{1}{m}\\sum_{i=1}^{m} u_t(x^{(i)}_{s,a}) \\right)^2.\n        \\end{aligned}", "\\begin{align*}\n        |\\widetilde{\\mu}_t(s,a) - \\mu_t(s,a)| &\\leq \\sqrt{\\frac{2\\sigma_t(s,a)}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}} + \\frac{2\\|u_t\\|_\\infty}{3m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta},\\\\\n        |\\widetilde{\\sigma}_t(s,a) - \\sigma_t(s,a)| &\\leq 4\\sqrt{\\frac{2}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}} \\|u_t\\|_\\infty^2.\n    \\end{align*}", "\\begin{align*}\n        |\\widetilde{\\mu}_t(s,a) - \\mu_t(s,a)| \\leq \\sqrt{\\frac{2\\sigma_t(s,a)}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}} + \\frac{2\\|u_t\\|_\\infty}{3m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}, \n    \\end{align*}", "\\begin{align*}\n        |\\widetilde{\\mu}_t(s,a) - \\mu_t(s,a)| &\\leq \\sqrt{\\frac{2}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}} \\|u_t\\|_\\infty,\\\\\n        \\left|\\frac{1}{m}\\sum_{i=1}^m u_t(x^{(i)}_{s,a})^2 - \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t(x')^2\\right| &\\leq \\sqrt{\\frac{2}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}} \\|u_t\\|_\\infty^2.\n    \\end{align*}", "\\begin{align*}\n        |\\widetilde{\\mu}_t(s,a)^2 - \\mu_t(s,a)^2| &= |\\widetilde{\\mu}_t(s,a) + \\mu_t(s,a)||\\widetilde{\\mu}_t(s,a) - \\mu_t(s,a)|\\\\\n        &\\leq (2\\mu_t(s,a) + \\sqrt{2\\theta} \\|u_t\\|_\\infty )\\sqrt{2\\theta} \\|u_t\\|_\\infty\\\\\n        &\\leq (2\\sqrt{2\\theta} + 2\\theta)\\|u_t\\|_\\infty^2 \\tag{$\\mu_t(s,a) \\leq \\|u_t\\|_\\infty$}\\\\\n        &\\leq 3\\sqrt{2\\theta}\\|u_t\\|_\\infty^2,\n    \\end{align*}", "\\begin{align*}\n        |\\widetilde{\\sigma}_t(s,a) - \\sigma_t(s,a)| &\\leq |\\widetilde{\\mu}_t(s,a)^2 - \\mu_t(s,a)^2| + \\left|\\frac{1}{m}\\sum_{i=1}^m u_t(x^{(i)}_{s,a})^2 - \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_t(x')^2\\right|\\\\\n        &\\leq 4\\sqrt{\\frac{2}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}}\\|u_t\\|_\\infty^2,\n    \\end{align*}", "\\begin{align*}\n        |\\mu_t(x,a) - \\mu_t(s,a)| \\leq \\int_{\\mathcal{X}}|p(\\rd x'|x,a) -p(\\rd x'|s,a)| |u_t(x')| \\leq Ln^{-\\alpha}\\|u_t\\|_\\infty,\\\\\n        \\left| \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_t(x')^2 - \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t(x')^2\\right| \\leq Ln^{-\\alpha}\\|u_t\\|_\\infty^2,\n    \\end{align*}", "\\begin{align*}\n        \\forall s\\in\\mathcal{S}_n, x\\in\\mathcal{X}(s), a\\in\\mathcal{A}:\\qquad \\widehat{\\beta}(x,a) := \\widehat{\\beta}(s,a) := \\frac{1}{\\ell}\\sum_{i=1}^\\ell  u(x^{(i)}_{s,a}) - v(x^{(i)}_{s,a}) - \\frac{\\epsilon}{16H} - L n^{-\\alpha}\\epsilon.\n    \\end{align*}", "\\begin{align*}\n        \\int_{\\mathcal{X}}p(\\rd x'|x,a)(u(x') - v(x')) - \\frac{\\epsilon}{8H} - 2L n^{-\\alpha}\\epsilon \\leq \\widehat{\\beta}(x,a) \\leq \\int_{\\mathcal{X}}p(\\rd x'|x,a)(u(x') - v(x')).\n    \\end{align*}", "\\begin{align*}\n        \\left|\\frac{1}{\\ell}\\sum_{i=1}^\\ell u(x^{(i)}_{s,a}) - v(x^{(i)}_{s,a}) - \\int_{\\mathcal{X}}p(\\rd x'|x,a)(u(x') - v(x')) \\right| &\\leq \\left(\\sqrt{\\frac{2}{\\ell}\\ln\\frac{2H|\\mathcal{S}_n|A}{\\delta}} + Ln^{-\\alpha}\\right)\\|u-v\\|_\\infty\\\\\n        &\\leq \\frac{\\epsilon}{16H} + L n^{-\\alpha}\\epsilon.\n    \\end{align*}", "\\begin{align*}\n        \\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_t}(\\rd x_{t+1}|x) p_{\\pi_{t+1}}(\\rd x_{t+2}|x_{t+1})\\cdots p_{\\pi_{t'-1}}(\\rd x_{t'}|x_{t'-1})\\sqrt{\\sigma_{t'+1}^\\pi(x_{t'},\\pi_{t'}(x_{t'}))} \\leq H^{3/2},\n    \\end{align*}", "\\begin{align*}\n        V^\\ast_t(x) - \\epsilon_k - 12Ln^{-\\alpha}H^2 \\leq u^{(k)}_t(x) \\leq V^{\\pi^{(k)}}_t(x) \\leq  V^\\ast_t(x) \\qquad\\forall (x,t,k)\\in\\mathcal{X}\\times[H]\\times [K].\n    \\end{align*}", "\\begin{align*}\n        O\\left(\\frac{H^3|\\mathcal{S}_n|A}{\\varepsilon^2}\\log\\left(\\frac{H|\\mathcal{S}_n|A}{\\delta}\\log\\frac{H}{\\varepsilon} \\right) \\right).\n    \\end{align*}", "\\begin{align}\\label{eq:classical_backward_eq0}\n        V^\\ast_t(x) - \\epsilon_{k'} - 12Ln^{-\\alpha}H^2 \\leq u^{(k')}_{t}(x) \\leq (\\mathcal{L}_{\\pi^{(k')}_{t}}u^{(k')}_{t+1})(x) \\leq  V^\\ast_t(x) \\qquad \\forall k'\\in[k-1], t\\in[H],\n    \\end{align}", "\\begin{subequations}\n    \\begin{align}\n        |\\widetilde{\\mu}^{(k)}_t(s,a) - \\mu^{(k)}_t(s,a)| &\\leq \\sqrt{2\\theta_k\\sigma_t^{(k)}(s,a)} + \\frac{2}{3}\\theta_k H, \\label{eq:inequality_1}\\\\\n        |\\widetilde{\\sigma}_t^{(k)}(s,a) - \\sigma_t^{(k)}(s,a)| &\\leq 4\\sqrt{2\\theta_k} H^2, \\label{eq:inequality_2}\n    \\end{align}\n    \\end{subequations}", "\\begin{align}\n        |\\widetilde{\\mu}^{(k)}_t(s,a) - \\mu^{(k)}_t(s,a)| &\\leq \\sqrt{2\\theta_k\\sigma_t^{(k)}(s,a)} + \\frac{2}{3}\\theta_k H, \\label{eq:inequality_1}\\\\\n        |\\widetilde{\\sigma}_t^{(k)}(s,a) - \\sigma_t^{(k)}(s,a)| &\\leq 4\\sqrt{2\\theta_k} H^2, \\label{eq:inequality_2}\n    \\end{align}", "\\begin{align*}\n        |\\widetilde{\\mu}^{(k)}_t(s,a) - \\mu^{(k)}_t(s,a)| &\\leq \\sqrt{2\\theta_k\\widetilde{\\sigma}_t^{(k)}(s,a)} + \\left(\\frac{2}{3}\\theta_k +  2(2\\theta_k)^{3/4}\\right) H,\n    \\end{align*}", "\\begin{align*}\n        \\widehat{\\mu}^{(k)}_t(x,a) := \\widetilde{\\mu}^{(k)}_t(s,a) - \\sqrt{2\\theta_k\\widetilde{\\sigma}_t^{(k)}(s,a)} - \\left(\\frac{2}{3}\\theta_k +  2(2\\theta_k)^{3/4} + Ln^{-\\alpha}\\right) H.\n    \\end{align*}", "\\begin{align*}\n        \\mu_t^{(k)}(x,a) \\geq \\widehat{\\mu}^{(k)}_t(x,a) \\geq \\mu_t^{(k)}(x,a) - 2\\sqrt{2\\theta_k\\widetilde{\\sigma}_t^{(k)}(s,a)} - \\left(\\frac{4}{3}\\theta_k +  4(2\\theta_k)^{3/4} + 2Ln^{-\\alpha}\\right) H.\n    \\end{align*}", "\\begin{align*}\n        \\sqrt{\\widetilde{\\sigma}_t^{(k)}(s,a)} &\\leq \\sqrt{\\sigma_t^{(k)}(s,a)} + 2(2\\theta_k)^{1/4} H \\tag{by \\Cref{eq:inequality_2}}\\\\\n        &\\leq \\sqrt{\\sigma_t^{(k)}(x,a)} + 2(\\sqrt{Ln^{-\\alpha}} + (2\\theta_k)^{1/4}) H \\tag{by $|\\sigma_t^{(k)}(x,a) - \\sigma_t^{(k)}(s,a)| \\leq 4Ln^{-\\alpha} H^2$}\\\\\n        &\\leq \\sqrt{\\sigma_t^{\\ast}(x,a)} + 2\\epsilon_{k} + 12Ln^{-\\alpha}H^2 + 2(\\sqrt{Ln^{-\\alpha}} + (2\\theta_k)^{1/4}) H, \n    \\end{align*}", "\\begin{subequations}\\label{eq:classical_backward_eq1}\n    \\begin{align}\n        \\widehat{\\mu}^{(k)}_t \\!(x,a) \\!&\\leq\\! \\mu_t^{(k)} \\!(x,a), \\label{eq:classical_backward_eq1a} \\\\\n        \\widehat{\\mu}^{(k)}_t \\!(x,a) \\!&\\geq \\! \\mu_t^{(k)} \\!(x,a) \\!-\\! \\sqrt{8\\theta_k\\sigma_t^\\ast(x,a)} \\!-\\! \\sqrt{8\\theta_k}(2\\epsilon_{k} +12Ln^{-\\alpha}H^2) \\!-\\! \\left(\\!\\frac{16}{3}\\theta_k \\!+\\! 8(2\\theta_k)^{3/4} \\!+\\! 4Ln^{-\\alpha}\\!\\right)\\! H.\\label{eq:classical_backward_eq1b}\n    \\end{align}\n    \\end{subequations}", "\\begin{align}\n        \\widehat{\\mu}^{(k)}_t \\!(x,a) \\!&\\leq\\! \\mu_t^{(k)} \\!(x,a), \\label{eq:classical_backward_eq1a} \\\\\n        \\widehat{\\mu}^{(k)}_t \\!(x,a) \\!&\\geq \\! \\mu_t^{(k)} \\!(x,a) \\!-\\! \\sqrt{8\\theta_k\\sigma_t^\\ast(x,a)} \\!-\\! \\sqrt{8\\theta_k}(2\\epsilon_{k} +12Ln^{-\\alpha}H^2) \\!-\\! \\left(\\!\\frac{16}{3}\\theta_k \\!+\\! 8(2\\theta_k)^{3/4} \\!+\\! 4Ln^{-\\alpha}\\!\\right)\\! H.\\label{eq:classical_backward_eq1b}\n    \\end{align}", "\\begin{align*}\n        u_{t'}^{(k)}(x) \\leq (\\mathcal{L}_{\\pi^{(k)}_{t'}}u^{(k)}_{t'+1})(x) \\leq V_{t'}^{\\pi^{(k)}}(x) \\leq V^\\ast_{t'}(x) \\qquad\\forall x\\in\\mathcal{X}, t'=t+1,t+2,\\dots,H+1.\n    \\end{align*}", "\\begin{align*}\n        \\widehat{\\beta}_{t+1}^{(k)} (x,a) := \\frac{1}{\\ell_k}\\sum_{i=1}^{\\ell_k} \\big( u^{(k)}_{t+1}(\\bar{x}^{(i)}_{s,a}) - u^{(k-1)}_{t+1}(\\bar{x}^{(i)}_{s,a}) \\big) - \\frac{\\epsilon_{k}}{4H} - \\frac{3}{2}Ln^{-\\alpha}H.\n    \\end{align*}", "\\begin{subequations}\\label{eq:classical_backward_eq2}\n    \\begin{align}\n        \\widehat{\\beta}^{(k)}_{t+1}(x,a) &\\leq \\int_{\\mathcal{X}}  p(\\rd x'|x,a)\\big(u^{(k)}_{t}(x') - u^{(k-1)}_{t+1}(x') \\big), \\label{eq:classical_backward_eq2a}\\\\\n        \\widehat{\\beta}^{(k)}_{t+1}(x,a) &\\geq \\int_{\\mathcal{X}}  p(\\rd x'|x,a)\\big(u^{(k)}_{t+1}(x') - u^{(k-1)}_{t+1}(x') \\big) - \\frac{\\epsilon_{k}}{2H}- 3Ln^{-\\alpha}H. \\label{eq:classical_backward_eq2b}\n    \\end{align}\n    \\end{subequations}", "\\begin{align}\n        \\widehat{\\beta}^{(k)}_{t+1}(x,a) &\\leq \\int_{\\mathcal{X}}  p(\\rd x'|x,a)\\big(u^{(k)}_{t}(x') - u^{(k-1)}_{t+1}(x') \\big), \\label{eq:classical_backward_eq2a}\\\\\n        \\widehat{\\beta}^{(k)}_{t+1}(x,a) &\\geq \\int_{\\mathcal{X}}  p(\\rd x'|x,a)\\big(u^{(k)}_{t+1}(x') - u^{(k-1)}_{t+1}(x') \\big) - \\frac{\\epsilon_{k}}{2H}- 3Ln^{-\\alpha}H. \\label{eq:classical_backward_eq2b}\n    \\end{align}", "\\begin{align*}\n        u_t^{(k)}(x) := u_t^{(k)}(s) &= r(s,\\pi^{(k)}_t(s)) - Ln^{-\\alpha} + \\widehat{\\mu}_{t+1}^{(k)}(s,\\pi^{(k)}_t(s)) + \\widehat{\\beta}_{t+1}^{(k)}(s,\\pi^{(k)}_t(s)) \\\\\n        &\\leq r(x,\\pi^{(k)}_t(x)) + \\widehat{\\mu}_{t+1}^{(k)}(x,\\pi^{(k)}_t(x)) + \\widehat{\\beta}_{t+1}^{(k)}(x,\\pi^{(k)}_t(x))\\\\\n        &\\leq r(x,\\pi^{(k)}_t(x)) + \\int_{\\mathcal{X}}p(\\text{d}x'|x,\\pi^{(k)}_t(x)) u^{(k)}_{t+1}(x') \\tag{by \\Cref{eq:classical_backward_eq1a,eq:classical_backward_eq2a}}\\\\\n        &= (\\mathcal{L}_{\\pi^{(k)}_t}u^{(k)}_{t+1})(x).\n    \\end{align*}", "\\begin{align*}\n        u^{(k)}_t(x) = u^{(k-1)}_{t}(x) \\leq (\\mathcal{L}_{\\pi^{(k-1)}_{t}}u^{(k-1)}_{t+1})(x) \\leq (\\mathcal{L}_{\\pi^{(k-1)}_{t}}u^{(k)}_{t+1})(x) = (\\mathcal{L}_{\\pi^{(k)}_{t}}u^{(k)}_{t+1})(x),\n    \\end{align*}", "\\begin{align*}\n        V^\\ast_t(x) - u^{(k)}_t(x) &= (\\mathcal{L}V^\\ast_{t+1})(x) - u^{(k)}_t(x)\\\\\n        &\\leq \\max_{a\\in\\mathcal{A}}\\left\\{r(x,a) + \\int_{\\mathcal{X}}p(\\rd x'|x,a)V_{t+1}^\\ast(x') \\right\\} \\\\\n        &~- \\max_{a\\in\\mathcal{A}}\\left\\{r(x,a) - 2Ln^{-\\alpha} + \\widehat{\\mu}_{t+1}^{(k)}(x,a) + \\widehat{\\beta}_{t+1}^{(k)}(x,a) \\right\\} \\tag{$u_t^{(k)}(x) \\!\\geq\\! u_t^{(k)}(s) \\!-\\! Ln^{-\\alpha}$}\\\\\n        &\\leq \\int_{\\mathcal{X}} p(\\rd x'|x,\\pi^{\\ast}_{t}(x))\\big(V_{t+1}^\\ast(x') - u^{(k)}_{t+1}(x')\\big) + \\xi_{t+1}^{(k)}(x), \\tag{by \\Cref{eq:classical_backward_eq1b,eq:classical_backward_eq2b}}\n    \\end{align*}", "\\begin{align*}\n        \\xi_{t+1}^{(k)}(x) \\!:=\\! \\sqrt{8\\theta_k\\sigma^\\ast_{t+1}(x,\\pi^\\ast_{t}(x))} + \\frac{\\epsilon_{k}}{2H} + \\sqrt{8\\theta_k}(2\\epsilon_{k} +  12Ln^{-\\alpha}H^2)\n        + \\left(\\!\\frac{16}{3}\\theta_k + 8(2\\theta_k)^{3/4} + 9 Ln^{-\\alpha}\\!\\right)\\! H.\n    \\end{align*}", "\\begin{align*}\n        V^\\ast_t(x) - u^{(k)}_t(x) \\leq \\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x) p_{\\pi_{t+1}^\\ast}(\\rd x_{t+2}|x_{t+1})\\cdots p_{\\pi_{t'-1}^\\ast}(\\rd x_{t'}|x_{t'-1}) \\xi_{t'+1}^{(k)}(x_{t'}).\n    \\end{align*}", "\\begin{align*}\n        \\left\\|\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x)\\cdots p_{\\pi_{t'-1}^\\ast}(\\rd x_{t'}|x_{t'-1})\\sqrt{\\sigma_{t'+1}^\\ast(x_{t'},\\pi^\\ast_{t'}(x_{t'}))}\\right\\|_\\infty \\leq H^{3/2}, \\tag{\\Cref{fact:upper_bound_variance}}\n    \\end{align*}", "\\begin{align*}\n        V^\\ast_t(x) - u^{(k)}_t(x) &\\leq \\frac{\\epsilon_{k}}{2} + 2\\sqrt{2\\theta_k}H^{3/2} + 4\\sqrt{2\\theta_k}\\epsilon_{k}H +  \\left(\\frac{16}{3}\\theta_k + 8(2\\theta_k)^{3/4}\\right)H^2\\\\\n        &~~~~~+ 24\\sqrt{2\\theta_k}Ln^{-\\alpha}H^3 + 9Ln^{-\\alpha}H^2 \\\\\n        &\\leq \\epsilon_{k}\\left(\\frac{1}{2} + \\frac{2\\sqrt{2}}{\\sqrt{128}} + \\frac{4\\sqrt{2}}{\\sqrt{128}} + \\frac{16}{3\\cdot 128} + \\frac{8\\cdot 2^{3/4}}{128^{3/4}}\\right)\\\\\n        &~~~~~+ \\frac{24\\sqrt{2}}{\\sqrt{128}}Ln^{-\\alpha}H^{3/2} + 9Ln^{-\\alpha} H^2 \\tag{$\\theta_k\\leq \\frac{1}{128 H^3}\\min\\{\\epsilon_{k}^2, 1\\}$}\\\\\n        &< \\epsilon_k + 12Ln^{-\\alpha} H^2,\n    \\end{align*}", "\\begin{align*}\n        \\sum_{k=1}^H\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_t}(\\rd x_{t+1}|x) p_{\\pi_{t+1}}(\\rd x_{t+2}|x_{t+1})\\cdots p_{\\pi_{t'-1}}(\\rd x_{t'}|x_{t'-1})\\sigma_{k}^\\pi(x_{t'},\\pi_{t'}(x_{t'})) \\leq 4H^{3},\n    \\end{align*}", "\\begin{align*}\n        \\operatorname{Var}_{P_t^\\pi}(V^\\pi_{k+1}) &= P_t^\\pi(V_{k+1}^\\pi \\circ V_{k+1}^\\pi) - (P_t^\\pi V^\\pi_{k+1})\\circ (P_t^\\pi V^\\pi_{k+1})\\\\\n        &= P_t^\\pi(V_{k+1}^\\pi \\circ V_{k+1}^\\pi) - (V^\\pi_k - r^\\pi_k)\\circ (V^\\pi_k - r^\\pi_k) \\\\\n        &= P_t^\\pi(V_{k+1}^\\pi \\circ V_{k+1}^\\pi)  - V^\\pi_k\\circ V^\\pi_k + 2V^\\pi_k\\circ r_k^\\pi - r^\\pi_k\\circ r^\\pi_k \\\\\n        &\\leq P_t^\\pi(V_{k+1}^\\pi \\circ V_{k+1}^\\pi)  - V^\\pi_k\\circ V^\\pi_k + 2V^\\pi_k\\circ r_k^\\pi.\n    \\end{align*}", "\\begin{align*}\n        &\\sum_{k=1}^H \\sum_{j=t}^{H-1} \\prod_{i=t}^{j-1}P^\\pi_i \\operatorname{Var}_{P_{j}^\\pi}(V_k^\\pi) \\\\\n        &\\leq \\sum_{j=t}^{H-1} \\prod_{i=t}^{j-1}P^\\pi_i \\operatorname{Var}_{P_{j}^\\pi}(V_1^\\pi) + \\sum_{k=1}^{H-1} \\sum_{j=t}^{H-1} \\prod_{i=t}^{j-1}P^\\pi_i\\big( P_j^\\pi (V_{k+1}^\\pi \\circ V_{k+1}^\\pi)  - V^\\pi_k\\circ V^\\pi_k + 2V^\\pi_k\\circ r_{k}^\\pi\\big)\\\\\n        &\\leq 3H^3 + \\sum_{k=1}^{H-1} \\sum_{j=t}^{H-1} \\left(\\prod_{i=t}^j P_i^\\pi (V_{k+1}^\\pi \\circ V_{k+1}^\\pi) - \\prod_{i=t}^{j-1}P_i^\\pi (V_{k}^\\pi \\circ V_{k}^\\pi) \\right) \\tag{since $\\|\\operatorname{Var}_{P_{j}^\\pi}(V_1^\\pi)\\|_\\infty \\leq H^2$, $\\|V_k^\\pi\\|_\\infty \\leq H$, $\\|\\sum_{k=1}^{H-1}r_k^\\pi\\|_\\infty \\leq H$}\\\\\n        &= 3H^3 + \\sum_{k=2}^{H} \\sum_{j=t}^{H-1} \\prod_{i=t}^j P_i^\\pi (V_{k}^\\pi \\circ V_{k}^\\pi) - \\sum_{k=1}^{H-1} \\sum_{j=t}^{H-2} \\prod_{i=t}^{j}P_i^\\pi (V_{k}^\\pi \\circ V_{k}^\\pi) - \\sum_{k=1}^{H-1} (V_{k}^\\pi \\circ V_{k}^\\pi) \\\\\n        &= 3H^3 + \\sum_{k=2}^H \\prod_{i=t}^{H-1} P_i^\\pi (V^\\pi_k\\circ V^\\pi_k) + \\sum_{j=t}^{H-2} \\prod_{i=t}^j P_i^\\pi (V^\\pi_H \\circ V^\\pi_H) - \\sum_{j=t}^{H-2} \\prod_{i=t}^j P_i^\\pi (V^\\pi_1 \\circ V^\\pi_1) - \\sum_{k=1}^{H-1} (V_{k}^\\pi \\circ V_{k}^\\pi) \\\\\n        &\\leq 3H^3 + \\sum_{j=t}^{H-1} \\prod_{i=t}^j P_i^\\pi (V^\\pi_H \\circ V^\\pi_H) - \\sum_{j=t}^{H-2} \\prod_{i=t}^j P_i^\\pi (V^\\pi_1 \\circ V^\\pi_1) \\tag{$\\|\\prod_{i=t}^{H-1} P_i^\\pi\\|_1 \\leq 1$} \\\\\n        &\\leq 4H^3. \\tag*{($\\|V^\\pi_H\\|_\\infty \\leq 1$) \\qquad\\qedhere}\n    \\end{align*}", "\\begin{align*}\n        V^\\ast_t(x) - \\epsilon_k - 8 Ln^{-\\alpha}H^2 \\leq u^{(k)}_t(x) \\leq V^{\\pi^{(k)}}_t(x) \\leq  V^\\ast_t(x) \\qquad\\forall (x,t,k)\\in\\mathcal{X}\\times[H]\\times[K].\n    \\end{align*}", "\\begin{align*}\n        O\\left(\\frac{H^{2}|\\mathcal{S}_n|A}{\\varepsilon}\\log\\left(\\frac{H|\\mathcal{S}_n|A}{\\delta}\\log\\frac{H}{\\varepsilon} \\right) \\right).\n    \\end{align*}", "\\begin{align}\\label{eq:quantum_backward_eq0}\n        V^\\ast_t(x) - \\epsilon_{k'} - 8Ln^{-\\alpha}H^b \\leq u^{(k')}_{t}(x) \\leq (\\mathcal{L}_{\\pi^{(k')}_{t}}u^{(k')}_{t+1})(x) \\leq  V^\\ast_t(x) \\qquad \\forall k'\\in[k-1], t\\in[H],\n    \\end{align}", "\\begin{align*}\n        m_k := O\\left(\\frac{\\sqrt{H}}{\\theta_k}\\log\\frac{H|\\mathcal{S}_n|AK}{\\delta}\\right) = O\\left(\\frac{H^{2}}{\\min\\{\\epsilon_k,1\\}}\\log\\frac{H|\\mathcal{S}_n|AK}{\\delta}\\right)\n    \\end{align*}", "\\begin{align*}\n        n_k := O\\left(\\frac{\\sqrt{H}}{\\theta_k}\\log\\frac{H|\\mathcal{S}_n|AK}{\\delta}\\right) = O\\left(\\frac{H^{2}}{\\min\\{\\epsilon_k,1\\}}\\log\\frac{H|\\mathcal{S}_n|AK}{\\delta}\\right)\n    \\end{align*}", "\\begin{subequations}\n    \\begin{align}\n        |\\widetilde{\\mu}^{(k)}_t(s,a) - \\mu^{(k)}_t(s,a)| &\\leq \\theta_k\\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\sigma_{t'}^{(k)}(s,a)}, \\label{eq:quantum_inequality_1}\\\\\n        |\\widetilde{\\sigma}_t^{(k)}(s,a) - \\sigma_t^{(k)}(s,a)| &\\leq \\theta_k H^2, \\label{eq:quantum_inequality_2}\n    \\end{align}\n    \\end{subequations}", "\\begin{align}\n        |\\widetilde{\\mu}^{(k)}_t(s,a) - \\mu^{(k)}_t(s,a)| &\\leq \\theta_k\\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\sigma_{t'}^{(k)}(s,a)}, \\label{eq:quantum_inequality_1}\\\\\n        |\\widetilde{\\sigma}_t^{(k)}(s,a) - \\sigma_t^{(k)}(s,a)| &\\leq \\theta_k H^2, \\label{eq:quantum_inequality_2}\n    \\end{align}", "\\begin{align*}\n        |\\widetilde{\\mu}^{(k)}_t(s,a) - \\mu^{(k)}_t(s,a)| \\leq \\theta_k \\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\widetilde{\\sigma}_{t'}^{(k)}(s,a)} + \\theta_k^{3/2} H,\n    \\end{align*}", "\\begin{align*}\n        \\widehat{\\mu}^{(k)}_t(x,a) := \\widetilde{\\mu}^{(k)}_t(s,a) - \\theta_k\\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\widetilde{\\sigma}_{t'}^{(k)}(s,a)} - \\theta_k^{3/2} H - Ln^{-\\alpha}H,\n    \\end{align*}", "\\begin{align*}\n        \\sqrt{\\widetilde{\\sigma}_t^{(k)}(s,a)} &\\leq \\sqrt{\\sigma_t^{(k)}(s,a)} + \\sqrt{\\theta_k}H \\tag{by \\Cref{eq:quantum_inequality_2}}\\\\\n        &\\leq \\sqrt{\\sigma_t^{(k)}(x,a)} + \\sqrt{\\theta_k} H + 2\\sqrt{Ln^{-\\alpha}} H \\tag{by $|\\sigma_t^{(k)}(x,a) - \\sigma_t^{(k)}(s,a)| \\leq 4Ln^{-\\alpha} H^2$}\\\\\n        &\\leq \\sqrt{\\sigma^\\ast_t(x,a)} + 2\\epsilon_k + 8Ln^{-\\alpha}H^2 + \\sqrt{\\theta_k}H + 2\\sqrt{Ln^{-\\alpha}} H\n    \\end{align*}", "\\begin{subequations}\\label{eq:quantum_backward_eq1}\n    \\begin{align}\n        \\widehat{\\mu}^{(k)}_t(x,a) &\\!\\leq\\! \\mu_t^{(k)}(x,a), \\label{eq:quantum_backward_eq1a} \\\\\n            \\widehat{\\mu}^{(k)}_t(x,a) &\\!\\geq\\! \\mu_t^{(k)}(x,a) - 2\\theta_k\\sqrt{\\!\\frac{1}{H}\\!\\sum_{t'=1}^H \\!\\sigma_{t'}^\\ast(x,a)} - 2\\theta_k(2\\epsilon_{k} + 8Ln^{-\\alpha}H^2) - 6\\theta_k^{3/2} H - 4Ln^{-\\alpha}H . \\label{eq:quantum_backward_eq1b}\n    \\end{align}\n    \\end{subequations}", "\\begin{align}\n        \\widehat{\\mu}^{(k)}_t(x,a) &\\!\\leq\\! \\mu_t^{(k)}(x,a), \\label{eq:quantum_backward_eq1a} \\\\\n            \\widehat{\\mu}^{(k)}_t(x,a) &\\!\\geq\\! \\mu_t^{(k)}(x,a) - 2\\theta_k\\sqrt{\\!\\frac{1}{H}\\!\\sum_{t'=1}^H \\!\\sigma_{t'}^\\ast(x,a)} - 2\\theta_k(2\\epsilon_{k} + 8Ln^{-\\alpha}H^2) - 6\\theta_k^{3/2} H - 4Ln^{-\\alpha}H . \\label{eq:quantum_backward_eq1b}\n    \\end{align}", "\\begin{align*}\n        u_{t'}^{(k)}(x) \\leq (\\mathcal{L}_{\\pi^{(k)}_{t'}}u^{(k)}_{t'+1})(x) \\leq V_{t'}^{\\pi^{(k)}}(x) \\leq V^\\ast_{t'}(x) \\qquad\\forall x\\in\\mathcal{X}, t'=t+1,t+2,\\dots,H+1.\n    \\end{align*}", "\\begin{align*}\n       \\left| \\widetilde{\\beta}_{t+1}^{(k)}(s,a) - \\int_{\\mathcal{X}}p(\\rd x'|x,a)\\big(u^{(k)}_{t+1}(x') - u^{(k-1)}_{t+1}(x')\\big) \\right| \\leq \\left(\\frac{1}{16H} + Ln^{-\\alpha}\\right)(2\\epsilon_k + 8Ln^{-\\alpha}H^2)\n    \\end{align*}", "\\begin{align*}\n        \\widehat{\\beta}^{(k)}_{t+1}(x,a) := \\widetilde{\\beta}_{t+1}^{(k)}(s,a) - \\frac{\\epsilon_{k}}{4H} - Ln^{-\\alpha}H,\n    \\end{align*}", "\\begin{subequations}\\label{eq:quantum_backward_eq2}\n    \\begin{align}\n        \\widehat{\\beta}^{(k)}_{t+1}(x,a) &\\leq \\int_{\\mathcal{X}}  p(\\rd x'|x,a)\\big(u^{(k)}_{t+1}(x') - u^{(k-1)}_{t+1}(x') \\big), \\label{eq:quantum_backward_eq2a}\\\\\n        \\widehat{\\beta}^{(k)}_{t+1}(x,a) &\\geq \\int_{\\mathcal{X}}  p(\\rd x'|x,a)\\big(u^{(k)}_{t+1}(x') - u^{(k-1)}_{t+1}(x') \\big) - \\frac{\\epsilon_{k}}{2H}- 2Ln^{-\\alpha}H. \\label{eq:quantum_backward_eq2b}\n    \\end{align}\n    \\end{subequations}", "\\begin{align}\n        \\widehat{\\beta}^{(k)}_{t+1}(x,a) &\\leq \\int_{\\mathcal{X}}  p(\\rd x'|x,a)\\big(u^{(k)}_{t+1}(x') - u^{(k-1)}_{t+1}(x') \\big), \\label{eq:quantum_backward_eq2a}\\\\\n        \\widehat{\\beta}^{(k)}_{t+1}(x,a) &\\geq \\int_{\\mathcal{X}}  p(\\rd x'|x,a)\\big(u^{(k)}_{t+1}(x') - u^{(k-1)}_{t+1}(x') \\big) - \\frac{\\epsilon_{k}}{2H}- 2Ln^{-\\alpha}H. \\label{eq:quantum_backward_eq2b}\n    \\end{align}", "\\begin{align*}\n        V^\\ast_t(x) - u^{(k)}_t(x) &\\leq \\max_{a\\in\\mathcal{A}}\\left\\{r(x,a) + \\int_{\\mathcal{X}}p(\\rd x'|x,a)V_{t+1}^\\ast(x') \\right\\} \\\\\n        &~- \\max_{a\\in\\mathcal{A}}\\left\\{r(x,a) - 2Ln^{-\\alpha} + \\widehat{\\mu}_{t+1}^{(k)}(x,a) + \\widehat{\\beta}_{t+1}^{(k)}(x,a) \\right\\} \\tag{$u_t^{(k)}(x) \\!\\geq\\! u_t^{(k)}(s) \\!-\\! Ln^{-\\alpha}$}\\\\\n        &\\leq \\int_{\\mathcal{X}} p(\\rd x'|x,\\pi^{\\ast}_{t}(x))\\big(V_{t+1}^\\ast(x') - u^{(k)}_{t+1}(x')\\big) + \\xi_{t+1}^{(k)}(x), \\tag{by \\Cref{eq:quantum_backward_eq1b,eq:quantum_backward_eq2b}}\n    \\end{align*}", "\\begin{align*}\n        \\xi_{t+1}^{(k)}(x) := 2\\theta_k\\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\sigma^\\ast_{t'}(x,\\pi^\\ast_{t}(x))} + \\frac{\\epsilon_{k}}{2H} + 2\\theta_k(2\\epsilon_{k} +  8Ln^{-\\alpha}H^2) + 6\\theta_k^{3/2}H + 6Ln^{-\\alpha}H .\n    \\end{align*}", "\\begin{align*}\n        &\\left\\|\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x)\\cdots p_{\\pi_{t'}^\\ast}(\\rd x_{t'-1}|x_{t'-1}) \\sqrt{\\frac{1}{H}\\sum_{t''=1}^H \\sigma_{t''}^\\ast(x_{t'},\\pi^\\ast_{t'}(x_{t'}))} \\right\\|_\\infty \\\\\n        &\\leq \\left\\| \\sqrt{\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x)\\cdots p_{\\pi_{t'-1}^\\ast}(\\rd x_{t'}|x_{t'-1})\\sum_{t''=1}^H \\sigma_{t''}^\\ast(x_{t'},\\pi^\\ast_{t'}(x_{t'}))} \\right\\|_\\infty \\leq 2H^{3/2}, \\tag{by Cauchy\u2013Schwarz inequality and \\Cref{fact:upper_bound_variance2}}\n    \\end{align*}", "\\begin{align*}\n        V^\\ast_t(x) - u^{(k)}_t(x) &\\leq 4\\theta_k H^{3/2} + \\frac{\\epsilon_{k}}{2} + 2\\theta_k(2\\epsilon_{k} +  8Ln^{-\\alpha}H^2)H + 6\\theta_k^{3/2}H^2 + 6 Ln^{-\\alpha}H^2  \\\\\n        &\\leq \\epsilon_k\\left(\\frac{1}{2} + \\frac{4}{20} + \\frac{4}{20 H^{3/2}} + \\frac{6}{20^{3/2}H^{1/4}} \\right) + \\left(\\frac{2\\cdot 8}{20\\sqrt{H}} + 6\\right)Ln^{-\\alpha}H^2 \\tag{$\\theta_k = \\frac{1}{20 H^{3/2}}\\min\\{\\epsilon_k, 1\\}$}\\\\\n        &\\leq \\epsilon_k + 8Ln^{-\\alpha}H^2,\n    \\end{align*}", "\\begin{align*}\n\t\t\t\t\\widehat{u}_{t}(s) = \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\} \\quad\\text{and}\\quad\n\t\t\t\t \\pi_t(s) = \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}\n\t\t\t\\end{align*}", "\\begin{align*}\n        V^\\ast_t(x) - \\varepsilon - 2(1+H)HLn^{-\\alpha} \\leq u_t(x) \\leq V^{\\pi}_t(x) \\leq  V^\\ast_t(x) \\qquad\\forall (x,t)\\in\\mathcal{X}\\times[H].\n    \\end{align*}", "\\begin{align*}\n        O\\bigg(\\frac{H^{3}|\\mathcal{S}_n|\\sqrt{A}}{\\varepsilon}\\log\\left(\\frac{H|\\mathcal{S}_n|A}{\\delta} \\right)\\log\\left(\\frac{H|\\mathcal{S}_n|}{\\delta} \\right) \\bigg).\n    \\end{align*}", "\\begin{align*}\n        |(\\mathcal{L}u_{t+1})(s) - (\\mathcal{L}u_{t+1})(x)| &\\leq |r(s,a) - r(x,a)| + \\left|\\int_{\\mathcal{X}}(p(\\rd x'|s,a) - p(\\rd x'|x,a))u_{t+1}(x') \\right| \\\\\n        &\\leq (1+H)Ln^{-\\alpha}, \\tag{$\\|u_{t+1}\\|_\\infty \\leq H$ by induction}\n    \\end{align*}", "\\begin{align}\\label{eq:simple_quantum_eq1}\n        u_t(x) \\leq (\\mathcal{L}u_{t+1})(x) \\quad\\text{and}\\quad\n        u_t(x) \\geq (\\mathcal{L}u_{t+1})(x) - \\frac{\\varepsilon}{H} - 2(1+H)Ln^{-\\alpha}.\n    \\end{align}", "\\begin{align*}\n        V_t^\\ast(x) - u_t(x) &\\leq (\\mathcal{L}V^\\ast_{t+1})(x) - (\\mathcal{L}u_{t+1})(x) + \\frac{\\varepsilon}{H} + 2(1+H)Ln^{-\\alpha}\\\\\n        &\\leq \\int_{\\mathcal{X}} p(\\rd x'|x, \\pi_t^\\ast(x))\\big(V^\\ast_{t+1}(x') - u_{t+1}(x')\\big) + \\frac{\\varepsilon}{H} + 2(1+H)Ln^{-\\alpha}.\n    \\end{align*}", "\\begin{align*}\n        V_t^\\ast(x) - u_t(x) \\leq \\varepsilon + 2(1+H)HLn^{-\\alpha},\n    \\end{align*}", "\\begin{align*}\n        \\forall a\\in\\mathcal{A}: \\qquad \\|(\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)} - \\mathcal{U}_{s}^{(t+1)})|a\\rangle|\\bar{0}\\rangle\\| = \\sqrt{(1 - \\sqrt{1-\\delta_2})^2 + \\delta_2} = \\sqrt{2-2\\sqrt{1-\\delta_2}} \\leq \\sqrt{2\\delta_2},\n    \\end{align*}", "\\begin{align}\\label{eq:approximate_value_iteration}\n    \\|{{u}}_{t+1} - \\mathcal{L}{{u}}_{t}\\|_\\infty \\leq \\varepsilon_u  \\quad\\text{for a given}~\\varepsilon_u \\geq 0, \\quad\\text{where}~t\\in\\mathbb{N}.\n\\end{align}", "\\begin{align*}\n        \\operatorname{sp}({u}_{t+1} -  {u}_{t}) \\leq \\nu^t(\\operatorname{sp}(\\mathcal{N}{u}_{0} - {u}_{0}) + 2\\epsilon) + 4\\epsilon \\frac{1-\\nu^t}{1-\\nu} \\qquad\\text{for all}~ t\\in\\mathbb{N}.\n    \\end{align*}", "\\begin{align*}\n        \\operatorname{sp}({{u}}_{t+1} -  {{u}}_{t}) &\\leq \\operatorname{sp}({{u}}_{t+1} - \\mathcal{N} {{u}}_{t}) + \\operatorname{sp}( {{u}}_{t} - \\mathcal{N}{{u}}_{t-1}) + \\operatorname{sp}(\\mathcal{N} {{u}}_{t} - \\mathcal{N}{{u}}_{t-1})  \\tag{triangle inequality}\\\\\n        &\\leq 4\\epsilon + \\nu \\operatorname{sp}({{u}}_{t} -  {{u}}_{t-1}) \\tag{$1$-stage $\\nu$-span contraction and $\\|{u}_{t+1} - \\mathcal{N} {u}_{t}\\|_\\infty \\leq \\epsilon$}\\\\\n        &\\leq 4\\epsilon + \\nu\\left(\\nu^{t-1}(\\operatorname{sp}(\\mathcal{N}{{u}}_{0} - {{u}}_{0})+2\\epsilon) + 4\\epsilon \\frac{1-\\nu^{t-1}}{1-\\nu}\\right)\\tag{induction hypothesis}\\\\\n        &= \\nu^t(\\operatorname{sp}(\\mathcal{N}{{u}}_{0} - {{u}}_{0})+2\\epsilon) + 4\\epsilon\\frac{1-\\nu^t}{1-\\nu}. \\tag*{\\qedhere}\n    \\end{align*}", "\\begin{align*}\n    \\operatorname{sp}({{u}}_{t+1} - {{u}}_{t}) \\leq \\varepsilon_{s} \\quad\\text{for some}~\\varepsilon_{s} >0\n\\end{align*}", "\\begin{align*}\n        g^{\\varepsilon} := \\frac{1}{2}\\left(\\max_{x\\in\\mathcal{X}}\\{{u}_{t_{\\varepsilon}+1}(x) - {u}_{t_{\\varepsilon}}(x)\\} + \\min_{x\\in\\mathcal{X}}\\{{u}_{t_{\\varepsilon}+1}(x) - {u}_{t_{\\varepsilon}}(x)\\} \\right).\n    \\end{align*}", "\\begin{align*}\n        {g}^{d_\\varepsilon^\\infty}(x) &= \\int_{\\mathcal{X}} \\mu_x^{d_\\varepsilon}(\\text{d}x') \\left(r_{d_\\varepsilon}(x') + \\int_{\\mathcal{X}} p_{d_\\varepsilon}(\\text{d}x''|x') u_{t_\\varepsilon}(x'') - {{u}}_{t_\\varepsilon}(x')\\right) \\\\\n        &= \\int_{\\mathcal{X}} \\mu_x^{d_\\varepsilon}(\\text{d}x')\\big( (\\mathcal{L}_{d_\\varepsilon}{{u}}_{t_\\varepsilon})(x') - {{u}}_{t_\\varepsilon}(x')\\big).\n    \\end{align*}", "\\begin{align*}\n        \\inf_{x'\\in\\mathcal{X}}\\{{u}_{t_\\varepsilon+1}(x') - {u}_{t_\\varepsilon}(x')\\} \\leq \\int_{\\mathcal{X}} \\!\\mu_x^{d_\\varepsilon}(\\text{d}x')\\big( {{u}}_{t_\\varepsilon+1}(x') - {{u}}_{t_\\varepsilon}(x')\\big) \\leq \\sup_{x'\\in\\mathcal{X}}\\{{u}_{t_\\varepsilon+1}(x') - {u}_{t_\\varepsilon+1}(x')\\},\n    \\end{align*}", "\\begin{align*}\n        \\left| \\int_{\\mathcal{X}} \\!\\mu_x^{d_\\varepsilon}(\\text{d}x')\\big( {{u}}_{t_\\varepsilon+1}(x') - {{u}}_{t_\\varepsilon}(x')\\big) - g^\\varepsilon \\right| \\leq \\frac{1}{2} \\operatorname{sp}({{u}}_{t_\\varepsilon+1} \\!-\\! {{u}}_{t_\\varepsilon}) \\leq \\frac{\\varepsilon_s}{2}\n        \\!\\implies\\! \\|{g}^{d_\\varepsilon^\\infty} - g^\\varepsilon {e}\\|_\\infty \\leq \\varepsilon_u + \\frac{\\varepsilon_s}{2},\n    \\end{align*}", "\\begin{align*}\n        u_{t+1}(x) \\gets \\begin{dcases}\n            \\max\\left\\{\\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~\\widetilde{u}_{t+1}(s) \\geq \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\left\\{\\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~ \\widetilde{u}_{t+1}(s) \\leq \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}(s) &\\text{otherwise}.\n        \\end{dcases}\n    \\end{align*}", "\\begin{align*}\n        \\widetilde{O}\\left(\\min\\left\\{(1+\\Lambda)^2,\\frac{\\Lambda^2}{(1-\\nu)^2}\\right\\}\\frac{|\\mathcal{S}_n|A}{(1-\\nu)^2\\varepsilon^2}\\frac{\\log\\frac{1}{\\varepsilon}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|A}{\\delta}\\right).\n    \\end{align*}", "\\begin{align*}\n        \\operatorname{sp}(u_{t+1}) \\leq \\operatorname{sp}(\\mathcal{L}u_t) \\leq \\operatorname{sp}(h^\\ast) + \\operatorname{sp}(\\mathcal{L}u_t - \\mathcal{L}h^\\ast) \\leq (1+\\nu^{t+1})\\Lambda + \\frac{2\\nu\\bar{\\varepsilon}_u}{1-\\nu}  \\leq 4\\Lambda + 3,\n    \\end{align*}", "\\begin{align*}\n        \\operatorname{sp}(u_1 - h^\\ast) &\\leq \\operatorname{sp}(\\mathcal{L}0 - \\mathcal{L}h^\\ast) + \\operatorname{sp}(u_1 - \\mathcal{L}0) \\leq \\nu \\Lambda + 2Ln^{-\\alpha} \\leq \\nu\\Lambda + \\frac{2\\bar{\\varepsilon}_u}{1-\\nu},\\\\\n        \\operatorname{sp}(u_1) &\\leq \\operatorname{sp}(\\mathcal{L}0) \\leq \\operatorname{sp}(h^\\ast) + \\operatorname{sp}(\\mathcal{L}0 - \\mathcal{L}h^\\ast) \\leq (1+\\nu)\\Lambda \\leq \\frac{2\\Lambda}{1-\\nu},\n    \\end{align*}", "\\begin{align*}\n        \\left|\\mu_{t+1}(s,a) - \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t(x') \\right| \\leq \\sqrt{\\frac{2}{m_t}\\ln\\frac{\\pi^2t^2|\\mathcal{S}_n|A}{6\\delta}} \\operatorname{sp}(u_t) \\leq \\frac{\\varepsilon_u}{2},\n    \\end{align*}", "\\begin{align*}\n        u_{t+1}(s) \\!=\\! \\begin{cases}\n            \\max\\{\\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\} &\\text{if}~\\widetilde{u}_{t+1}(s) \\geq \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\{\\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\} &\\text{if}~ \\widetilde{u}_{t+1}(s) \\leq \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}(s) &\\text{otherwise}.\n        \\end{cases}\n    \\end{align*}", "\\begin{align*}\n        |(\\mathcal{L}u_{t})(s) - (\\mathcal{L}u_{t})(x)| &\\leq |r(s,a) - r(x,a)| + \\left|\\int_{\\mathcal{X}} \\!(p(\\rd x'|s,a) - p(\\rd x'|x,a)) \\!\\left(\\! u_{t}(x') - \\min_{x''\\in\\mathcal{X}}u_t(x'') \\!\\right) \\right| \\\\\n        &\\leq 4(1+\\Lambda)Ln^{-\\alpha}, \\tag{$\\operatorname{sp}(u_t) \\leq 4\\Lambda + 3$ by induction}\n    \\end{align*}", "\\begin{align*}\n        \\operatorname{sp}(u_{t+1} - h^\\ast) &\\leq \\operatorname{sp}(u_{t+1} - \\bar{\\mathcal{L}}u_t) + \\operatorname{sp}(\\bar{\\mathcal{L}}u_t - \\bar{\\mathcal{L}}h^\\ast) + \\operatorname{sp}(\\bar{\\mathcal{L}}h^\\ast - \\mathcal{L}h^\\ast)  \\tag{triangle inequality}\\\\\n        &\\leq 2\\varepsilon_u + \\nu\\operatorname{sp}(u_t - h^\\ast) + 2(1+\\Lambda)Ln^{-\\alpha}  \\tag{$\\operatorname{sp}(v) \\leq 2\\|v\\|_\\infty$ and $\\mathcal{L}$ contraction}\\\\\n        &\\leq 2\\bar{\\varepsilon}_u + \\nu^{t+1}\\Lambda + \\frac{2\\nu}{1-\\nu}\\bar{\\varepsilon}_u \\tag{induction hypothesis}\\\\\n        &= \\nu^{t+1}\\Lambda + \\frac{2\\bar{\\varepsilon}_u}{1-\\nu}.\n    \\end{align*}", "\\begin{align*}\n        \\operatorname{sp}(u_{t+1}) \\leq \\operatorname{sp}(\\mathcal{L}u_t) \\leq \\operatorname{sp}(h^\\ast) + \\operatorname{sp}(\\mathcal{L}u_t - \\mathcal{L}h^\\ast) \\leq \\Lambda + \\nu\\frac{2\\Lambda}{1-\\nu} \\leq \\frac{2\\Lambda}{1-\\nu}.\n    \\end{align*}", "\\begin{align*}\n        \\operatorname{sp}(u_{t+1} - u_t) \\leq \\nu^t(1 + 2\\varepsilon_u + 8(1+\\Lambda)Ln^{-\\alpha}) + \\frac{4\\varepsilon_u + 16(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\qquad\\forall t\\in\\mathbb{N},\n    \\end{align*}", "\\begin{align*}\n        t\\geq t^\\ast := \\left\\lceil \\frac{\\log\\frac{(1-\\nu)(1+2\\varepsilon_u + 8(1+\\Lambda)Ln^{-\\alpha})}{2\\varepsilon_u + 2(1+\\Lambda)Ln^{-\\alpha}}}{\\log\\frac{1}{\\nu}} \\right\\rceil = O\\left(\\frac{\\log\\frac{1}{\\varepsilon}}{\\log\\frac{1}{\\nu}} \\right).\n    \\end{align*}", "\\begin{align*}\n        g^\\varepsilon = \\frac{1}{2}\\left(\\max_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast + 1}(x) - {u}_{t^\\ast}(x)\\} + \\min_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast + 1}(x) - {u}_{t^\\ast}(x)\\}\\right)\n    \\end{align*}", "\\begin{align*}\n            \\widetilde{u}_{t+1}(s) = \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\} \\quad\\text{and}\\quad a_{t+1}(s) = \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}\n        \\end{align*}", "\\begin{align*}\n        u_{t+1}(x) \\gets \\begin{dcases}\n            \\max\\left\\{\\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~\\widetilde{u}_{t+1}(s) \\geq \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\left\\{\\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~ \\widetilde{u}_{t+1}(s) \\leq \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}(s) &\\text{otherwise}.\n        \\end{dcases}\n    \\end{align*}", "\\begin{align*}\n        \\widetilde{O}\\left(\\min\\left\\{1+\\Lambda,\\frac{\\Lambda}{1-\\nu}\\right\\}\\frac{|\\mathcal{S}_n|\\sqrt{A}}{(1-\\nu)\\varepsilon}\\frac{\\log\\frac{1}{\\varepsilon}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|A}{\\delta}\\log\\frac{|\\mathcal{S}_n|}{\\delta}\\right).\n    \\end{align*}", "\\begin{align*}\n        \\sum_{a\\in\\mathcal{A}}\\alpha_a |s,a\\rangle|\\bar{0}\\rangle \\mapsto \\sum_{a\\in\\mathcal{A}}\\alpha_a |s,a\\rangle|\\mu_{t+1}(s,a)\\rangle|\\operatorname{garbage}(a)\\rangle,\n    \\end{align*}", "\\begin{align*}\n        \\left| {\\mu}_{t+1}(s, a) -\\int_{\\mathcal{X}} p(\\rd x'|s,a) {u}_{t}(x') \\right| \\leq \\frac{\\varepsilon_u}{2} \\qquad \\forall(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}.\n    \\end{align*}", "\\begin{align*}\n        u_{t+1}(x) \\!=\\! \\begin{cases}\n            \\max\\{\\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\} &\\text{if}~\\widetilde{u}_{t+1}(s) \\geq \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\{\\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\} &\\text{if}~ \\widetilde{u}_{t+1}(s) \\leq \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}(s) &\\text{otherwise}.\n        \\end{cases}\n    \\end{align*}", "\\begin{align*}\n        g^\\varepsilon = \\frac{1}{2}\\left(\\max_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast + 1}(x) - {u}_{t^\\ast}(x)\\} + \\min_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast + 1}(x) - {u}_{t^\\ast}(x)\\}\\right)\n    \\end{align*}", "\\begin{align*}\n                \\|V_1^{\\pi^{(k)}} - V_1^\\ast\\|_\\infty \\leq  c\\sqrt{\\frac{H^3|\\mathcal{S}_n|A}{k}\\log\\left(\\frac{H|\\mathcal{S}_n|A\\log{K}}{\\delta}\\right)} - cLn^{-\\alpha}H^2 \n            \\end{align*}", "\\begin{align*}\n                \\|V_1^{\\pi^{(k)}} - V_1^\\ast\\|_\\infty \\leq c \\frac{|\\mathcal{S}_n|}{k}\\min\\{HA,H^2\\sqrt{A}\\}\\log^2\\left(\\frac{H|\\mathcal{S}_n|A\\log{K}}{\\delta}\\right) - cLn^{-\\alpha}H^2\n            \\end{align*}", "\\begin{align*}\n        \\widetilde{O}\\left(\\sqrt{H|\\mathcal{S}_n|AT\\log\\frac{H|\\mathcal{S}_n|A}{\\delta}} + HTLn^{-\\alpha} \\right),\n    \\end{align*}", "\\begin{align*}\n    \t\\widetilde{O}\\left(T^{\\frac{D+\\alpha}{D+2\\alpha}}\\left(H + \\sqrt{HA\\log\\frac{HAT}{\\delta}}\\right) \\right).\n    \\end{align*}", "\\begin{align*}\n    \t\\widetilde{O}\\left(\\sqrt{HSAT\\log\\frac{HSA}{\\delta}} \\right). \n    \\end{align*}", "\\begin{align*}\n        \\operatorname{Regret}_{H}(T) := \\sum_{k=1}^K\\big( V_1^\\ast(x_1^{(k)}) - V_1^{\\pi^{(k)}}(x_1^{(k)})\\big).\n    \\end{align*}", "\\begin{align*}\n        V_1^\\ast(x) - \\frac{C}{\\sqrt{k}} - 12H^2Ln^{-\\alpha} \\leq V_1^{\\pi^{(k)}}(x) \\leq V_1^\\ast(x), \\quad\\text{where}~ C = \\widetilde{O}\\left(\\sqrt{H^2|\\mathcal{S}_n|A \\log\\frac{H|\\mathcal{S}_n|A}{\\delta} } \\right)\n    \\end{align*}", "\\begin{align*}\n        \\operatorname{Regret}_{H}(T) \\leq \\sum_{k=1}^K \\!\\left(\\!\\frac{C}{\\sqrt{k}} + 12H^2Ln^{-\\alpha}\\!\\right)  \\leq \\frac{C\\sqrt{K}}{\\sqrt{2}-1} + 12H^2KLn^{-\\alpha} = \\frac{C}{\\sqrt{2}-1}\\sqrt{\\frac{T}{H}} + 12HTLn^{-\\alpha},\n    \\end{align*}", "\\begin{align*}\n        \\widetilde{O}\\left(|\\mathcal{S}_n|\\min\\left\\{HA, H^2\\sqrt{A}\\log\\frac{H|\\mathcal{S}_n|}{\\delta}\\right\\}\\log\\frac{T}{H}\\log\\frac{H|\\mathcal{S}_n|A}{\\delta} + HTLn^{-\\alpha} \\right),\n    \\end{align*}", "\\begin{align*}\n    \t\\widetilde{O}\\left(T^{\\frac{D}{D+\\alpha}}\\min\\left\\{HA,H^2\\sqrt{A}\\log\\frac{HT}{\\delta}\\right\\}\\log\\frac{T}{H}\\log\\frac{HTA}{\\delta} \\right).\n    \\end{align*}", "\\begin{align*}\n    \t\\widetilde{O}\\left(S\\min\\left\\{HA,H^2\\sqrt{A}\\log\\frac{HS}{\\delta}\\right\\}\\log\\frac{T}{H}\\log\\frac{HSA}{\\delta} \\right). \n    \\end{align*}", "\\begin{align*}\n        V_1^\\ast(x) &- \\frac{C}{k} - 8H^2Ln^{-\\alpha} \\leq V_1^{\\pi^{(k)}}(x) \\leq V_1^\\ast(x), \\\\\n        \\text{where}~& C = \\widetilde{O}\\left(|\\mathcal{S}_n|\\min\\left\\{HA,H^2\\sqrt{A}\\log\\frac{H|\\mathcal{S}_n|}{\\delta}\\right\\} \\log\\frac{H|\\mathcal{S}_n|A}{\\delta} \\right)\n    \\end{align*}", "\\begin{align*}\n            g^{\\widetilde{d}_k^\\infty}(x) \\geq  g^* - c\\Lambda\\sqrt{\\frac{|\\mathcal{S}_n|A\\log{T}}{\\tau_k}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}} - c(1+\\Lambda)Ln^{-\\alpha} \n        \\end{align*}", "\\begin{align*}\n            g^{\\widetilde{d}_k^\\infty}(x) \\geq  g^* - c\\Lambda \\frac{|\\mathcal{S}_n|\\sqrt{A}\\log{T}}{\\tau_k}\\log^2\\frac{|\\mathcal{S}_n|AT}{\\delta} - c(1+\\Lambda)Ln^{-\\alpha} \n        \\end{align*}", "\\begin{align*}\n        \\widetilde{O}\\left(\\min\\left\\{\\Lambda+1,\\frac{\\Lambda}{1-\\nu}\\right\\} \\sqrt{\\frac{|\\mathcal{S}_n| A}{(1-\\nu)^2}\\frac{T\\log{T}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}} + \\frac{T(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\right),\n    \\end{align*}", "\\begin{align*}\n        \\widetilde{O}\\left(\\Lambda T^{\\frac{D+\\alpha}{D+2\\alpha}}\\sqrt{A\\log{T}\\log\\frac{AT}{\\delta}}  \\right).\n    \\end{align*}", "\\begin{align*}\n        \\widetilde{O}\\left(\\Lambda\\sqrt{SAT\\log{T}\\log\\frac{SAT}{\\delta}}\\right).\n    \\end{align*}", "\\begin{align*}\n        \\Delta_k := \\int_{\\mathcal{X}} n_k(\\rd x) \\big(g^* - r_{\\widetilde{d}_k}(x)\\big).\n    \\end{align*}", "\\begin{align*}\n        \\widetilde{g}_k &\\geq g^{\\ast} - C\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}} - \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\\\\n        \\text{where}~ C &= O\\left(\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu}\\right\\}\\sqrt{\\frac{|\\mathcal{S}_n|A}{(1-\\nu)^2}\\frac{1}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}}\\right),\n    \\end{align*}", "\\begin{align*}\n        \\Delta_k &\\leq \\int_{\\mathcal{X}} n_k(\\rd x) \\big(\\widetilde{g}_k -  r_{\\widetilde{d}_k}(x) \\big) + \\sum_{s\\in\\mathcal{S}_n} n_k(s)\\left(\\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} + C\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}} \\right)\\\\\n        &= \\int_{\\mathcal{X}} n_k(\\rd x) \\big(\\widetilde{g}_k -  r_{\\widetilde{d}_k}(x) \\big)  + \\tau_k \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} + C\\sum_{s\\in\\mathcal{S}_n} n_k(s)\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}}.\n    \\end{align*}", "\\begin{align*}\n        |u_{t^\\ast+1}(x) - u_{t^\\ast}(x) - \\widetilde{g}_k| \\leq \\frac{3C}{2}\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\qquad\\forall x\\in\\mathcal{X}.\n    \\end{align*}", "\\begin{align*}\n        (\\mathcal{L} {u}_{t^\\ast})(x) = r_{\\widetilde{d}_k}(x) + \\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x'|x) u_{t^\\ast}(x').\n    \\end{align*}", "\\begin{align*}\n        \\left|\\big(\\widetilde{g}_k - r_{\\widetilde{d}_k}(x) \\big) - \\left(\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x'|x) u_{t^\\ast}(x') - u_{t^\\ast}(x) \\right) \\right| \\leq 2C \\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}} + \\frac{22(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu},\n    \\end{align*}", "\\begin{align}\n        \\Delta_k &\\leq \\int_{\\mathcal{X}} n_k(\\rd x) \\left(\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x'|x) u_{t^\\ast}(x') - u_{t^\\ast}(x) \\right) + \\tau_k \\frac{35(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} + 3C\\sum_{s\\in\\mathcal{S}_n}  n_k(s)\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}}. \\label{eq:regret_infinite_horizon_eq1}\n    \\end{align}", "\\begin{align*}\n        w_k(x) := u_{t^\\ast}(x) - \\frac{1}{2}\\left(\\max_{x'\\in\\mathcal{X}}\\{u_{t^\\ast}(x')\\} + \\min_{x'\\in\\mathcal{X}}\\{u_{t^\\ast}(x')\\}\\right).\n    \\end{align*}", "\\begin{align*}\n        \\int_{\\mathcal{X}} n_k(\\rd x) &\\left(\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x'|x) w_k(x') - w_k(x) \\right)\\\\\n        &= \\sum_{t=t_k}^{t_{k+1}-1}\\left(\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\rd x'|x_t) w_k(x') - w_k(x_t)\\right)\\\\\n        &= w_k(x_{t_{k+1}}) - w_k(x_{t_{k}}) + \\sum_{t=t_k}^{t_{k+1}-1}\\left(\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\rd x'|x_t) w_k(x') - w_k(x_{t+1}) \\right) \\\\\n        &= w_k(x_{t_{k+1}}) - w_k(x_{t_{k}}) + \\sum_{t=t_k}^{t_{k+1}-1}X_t.\n    \\end{align*}", "\\begin{align*}\n        \\operatorname{Pr}\\left[\\sum_{t=1}^T X_t \\geq \\min\\left\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\right\\}\\sqrt{2T\\ln\\frac{8T^{5/4}}{\\delta}}\\right] \\leq \\frac{\\delta}{8T^{5/4}}.\n    \\end{align*}", "\\begin{align*}\n        \\sum_{k=1}^m \\int_{\\mathcal{X}} \\! n_k(\\rd x) \\!\\left(\\int_{\\mathcal{X}} \\! p_{\\widetilde{d}_k}(\\text{d}x'|x) w_k(x') - w_k(x) \\right) \\leq \\min\\!\\left\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\right\\}\\!\\left(\\!\\lceil\\log_2{T}\\rceil + \\sqrt{2T\\ln\\frac{8T^{5/4}}{\\delta}}\\right)\n    \\end{align*}", "\\begin{align*}\n        \\sum_{k=1}^m \\sum_{s\\in\\mathcal{S}_n}\\frac{n_k(s)}{\\sqrt{t_k}} = \\sum_{k=1}^m \\frac{\\sum_{s\\in\\mathcal{S}_n} n_k(s)}{\\sqrt{\\sum_{i=1}^{k-1} \\sum_{s\\in\\mathcal{S}} n_k(s)}} \\leq \\frac{\\sqrt{T}}{\\sqrt{2}-1}.\n    \\end{align*}", "\\begin{align*}\n        \\operatorname{Regret}_{\\infty}^{\\rm path}(T) \\leq 2\\min\\left\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\right\\}\\sqrt{2T\\ln \\frac{8T^{5/4}}{\\delta}} + \\frac{3\\sqrt{2}C}{\\sqrt{2}-1}\\sqrt{T\\lceil\\log_2{T}\\rceil} + T\\frac{35(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}.\n    \\end{align*}", "\\begin{align*}\n        \\widetilde{O}\\left(\\!{\\min}\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu} \\right\\}\\!\\left(\\!\\sqrt{T\\log\\frac{T}{\\delta}} + \\frac{|\\mathcal{S}_n| \\sqrt{A}}{1-\\nu}\\frac{\\log^2{T}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}\\log\\frac{|\\mathcal{S}_n|T}{\\delta}\\!\\right) + \\frac{T(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\right), \n    \\end{align*}", "\\begin{align*}\n        \\widetilde{O}\\left(\\Lambda\\sqrt{T\\log\\frac{T}{\\delta}} + \\Lambda T^{\\frac{D}{D+\\alpha}}\\sqrt{A} \\log^2{T}\\log\\frac{AT}{\\delta}\\log\\frac{T}{\\delta} \\right).\n    \\end{align*}", "\\begin{align*}\n        \\widetilde{O}\\left(\\Lambda\\sqrt{T\\log\\frac{T}{\\delta}} + \\Lambda S\\sqrt{A} \\log^2{T}\\log\\frac{SAT}{\\delta}\\log\\frac{ST}{\\delta}\\right).\n    \\end{align*}", "\\begin{align*}\n        \\widetilde{g}_k &\\geq g^{\\ast} - C\\frac{\\log_2{\\tau_k}}{\\tau_k} - \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\\\\n        \\text{where}~ C &= O\\left(\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu}\\right\\}\\frac{|\\mathcal{S}_n|\\sqrt{A}}{1-\\nu}\\frac{1}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}\\log\\frac{|\\mathcal{S}_n|T}{\\delta}\\right),\n    \\end{align*}", "\\begin{align*}\n        \\Delta_k \\leq \\int_{\\mathcal{X}} n_k(\\rd x) \\big(\\widetilde{g}_k -  r_{\\widetilde{d}_k}(x) \\big)  + \\tau_k \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} + C\\sum_{s\\in\\mathcal{S}_n} n_k(s)\\frac{\\log_2{\\tau_k}}{\\tau_k}.\n    \\end{align*}", "\\begin{align}\n        \\Delta_k &\\leq \\int_{\\mathcal{X}} n_k(\\rd x) \\left(\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x'|x) u_{t^\\ast}(x') - u_{t^\\ast}(x) \\right) + \\tau_k \\frac{35(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} + 3C\\sum_{s\\in\\mathcal{S}_n}  n_k(s)\\frac{\\log_2{\\tau_k}}{\\tau_k}. \\label{eq:regret_infinite_horizon_eq1_classical}\n    \\end{align}", "\\begin{align*}\n        \\sum_{k=1}^m \\int_{\\mathcal{X}} \\!\\! n_k(\\rd x) \\!\\left(\\int_{\\mathcal{X}} \\! p_{\\widetilde{d}_k}(\\text{d}x'|x) w_k(x') - w_k(x) \\!\\right) \\leq \\min\\!\\left\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\right\\}\\!\\left(\\!\\lceil\\log_2{T}\\rceil + \\sqrt{2T\\ln\\!\\frac{8T^{5/4}}{\\delta}}\\right).\n    \\end{align*}", "\\begin{align*}\n        \\sum_{k=1}^m \\sum_{s\\in\\mathcal{S}}\\frac{n_k(s)}{t_k} = \\sum_{k=1}^m \\frac{\\sum_{s\\in\\mathcal{S}} n_k(s)}{\\sum_{i=1}^{k-1} \\sum_{s\\in\\mathcal{S}} n_k(s)} \\leq 4 \\log_2{T}.\n    \\end{align*}", "\\begin{align*}\n        \\operatorname{Regret}_{\\infty}^{\\rm path}(T) \\leq 2\\min\\left\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\right\\}\\sqrt{2T\\ln \\frac{8T^{5/4}}{\\delta}} + 24 C\\lceil\\log_2{T}\\rceil^2 + T\\frac{35(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}.\n    \\end{align*}", "\\begin{align*}\n        \\widetilde{O}\\left(\\min\\left\\{\\Lambda+1,\\frac{\\Lambda}{1-\\nu}\\right\\} \\sqrt{\\frac{|\\mathcal{S}_n| A}{(1-\\nu)^2}\\frac{T\\log{T}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}} + \\frac{T(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\right),\n    \\end{align*}", "\\begin{align*}\n        \\widetilde{O}\\left(\\Lambda T^{\\frac{D+\\alpha}{D+2\\alpha}}\\sqrt{A\\log{T}\\log\\frac{AT}{\\delta}}  \\right).\n    \\end{align*}", "\\begin{align*}\n        \\widetilde{O}\\left(\\Lambda\\sqrt{SAT\\log{T}\\log\\frac{SAT}{\\delta}}\\right).\n    \\end{align*}", "\\begin{align*}\n        \\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T) = \\sum_{k=1}^m \\tau_k\\left(g^\\ast - \\min_{x\\in\\mathcal{X}} g^{\\widetilde{d}_k^\\infty}(x) \\right),\n    \\end{align*}", "\\begin{align*}\n        g^{\\widetilde{d}_k^\\infty}(x) &\\geq g^\\ast - C\\sqrt{\\frac{\\log_2\\tau_k}{\\tau_k}} - \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu},\\\\\n        \\text{where}~ C &= O\\left(\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu}\\right\\}\\sqrt{\\frac{|\\mathcal{S}_n|A}{(1-\\nu)^2}\\frac{1}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}}\\right),\n    \\end{align*}", "\\begin{align*}\n        \\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T) \\!\\leq\\! \\sum_{k=1}^m \\!\\left(\\! C\\sqrt{\\tau_k\\log_2\\tau_k} + \\tau_k \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\right) \\!\\leq\\! \\frac{\\sqrt{2}C}{\\sqrt{2}-1}\\sqrt{T\\lceil\\log_2 T\\rceil} + T\\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu},\n    \\end{align*}", "\\begin{align*}\n        \\widetilde{O}\\left(\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu} \\right\\}\\frac{|\\mathcal{S}_n| \\sqrt{A}}{1-\\nu}\\frac{\\log^2{T}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}\\log\\frac{|\\mathcal{S}_n|T}{\\delta} + \\frac{T(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\right), \n    \\end{align*}", "\\begin{align*}\n        \\widetilde{O}\\left(\\Lambda T^{\\frac{D}{D+\\alpha}}\\sqrt{A} \\log^2{T}\\log\\frac{AT}{\\delta}\\log\\frac{T}{\\delta} \\right).\n    \\end{align*}", "\\begin{align*}\n        \\widetilde{O}\\left(\\Lambda S\\sqrt{A} \\log^2{T}\\log\\frac{SAT}{\\delta}\\log\\frac{ST}{\\delta}\\right).\n    \\end{align*}", "\\begin{align*}\n        g^{\\widetilde{d}_k^\\infty}(x) &\\geq g^\\ast - C\\frac{\\log_2\\tau_k}{\\tau_k} - \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu},\\\\\n        \\text{where}~ C &= O\\left(\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu}\\right\\}\\frac{|\\mathcal{S}_n|\\sqrt{A}}{1-\\nu}\\frac{1}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}\\log\\frac{|\\mathcal{S}_n|T}{\\delta}\\right),\n    \\end{align*}", "\\begin{align*}\n        \\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T) \\leq \\sum_{k=1}^m \\left( C\\log_2\\tau_k + \\tau_k \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\right) \\leq C\\lceil\\log_2 T\\rceil^2 + T\\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}.\n    \\end{align*}"], "algorithm": ["\\begin{algorithm}[t!]\n    \\caption{Classical approximate backward induction algorithm}\n    \\label{algo:classical_backward_recursion}\n    \\begin{algorithmic}[1]  \n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$, finite action space $\\mathcal{A}$, horizon $H$, classical sampling access to probability kernels $p$, failure probability $\\delta\\in(0, 1)$, error $\\varepsilon > 0$.\n\n    \\Ensure $(\\varepsilon + 12Ln^{-\\alpha}H^2)$-optimal policy $\\pi^{(K)} \\in \\Pi^{\\rm D}$. \n\n    \\State Initialise $u^{(0)}_t \\equiv 0$ for all $t\\in[H]$ and $\\pi^{(0)}\\in\\Pi^{\\rm D}$ arbitrary\n\n    \\For{$k=1$ to $K:=\\lceil\\log_2(H/\\varepsilon)\\rceil$}\n\n        \\State $\\epsilon_k \\gets H/2^k$\n\n        \\State $m_k \\gets \\big\\lceil\\frac{128H^3}{\\min\\{\\epsilon^2_{k},1\\}}\\ln\\frac{16H|\\mathcal{S}_n|AK}{\\delta}\\big\\rceil$, $\\ell_k \\gets \\big\\lceil 512 H^2\\ln\\frac{4H|\\mathcal{S}_n|AK}{\\delta}\\big\\rceil$, $\\theta_k \\gets \\frac{1}{m_k}\\ln\\frac{16H|\\mathcal{S}_n|AK}{\\delta}$\n    \n        \\State For each $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, sample $x^{(1)}_{s,a},x^{(2)}_{s,a},\\dots,x^{(m_k)}_{s,a}\\in\\mathcal{X}$\n    \n        \\For{$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$}\n\n            \\State $\\widetilde{\\sigma}^{(k)}_t(s,a) \\gets \\frac{1}{m_k}\\sum_{i=1}^{m_k}(u^{(k-1)}_{t}(x^{(i)}_{s,a}))^2 - \\big(\\frac{1}{m_k}\\sum_{i=1}^{m_k}u^{(k-1)}_{t}(x^{(i)}_{s,a}) \\big)^2$ \\label{line:classical_backward_line1}\n    \n            \\State $\\widehat{\\mu}^{(k)}_t(s,a) \\gets \\frac{1}{m_k}\\sum_{i=1}^{m_k}u^{(k-1)}_{t}(x^{(i)}_{s,a}) - \\sqrt{2\\theta_k\\widetilde{\\sigma}_t^{(k)}(s,a)} - \\left(\\frac{2}{3}\\theta_k +  2(2\\theta_k)^{3/4} + Ln^{-\\alpha}\\right)H$ \\label{line:classical_backward_line2}\n    \n        \\EndFor\n\n        \\State $\\widehat{\\mu}_t^{(k)}(x,a) \\gets \\widehat{\\mu}_t^{(k)}(s,a)$ for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, $a\\in\\mathcal{A}$, and $t\\in[H]$\n\n        \\State $u^{(k)}_H(x) \\gets u^{(k-1)}_{H}(x)$ for all $x\\in\\mathcal{X}$ and $\\pi_H^{(k)}\\gets \\pi_{H}^{(k-1)}$\n\n        \\For{$t=H-1,H-2,\\dots,1$}\n\n            \\For{$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$}\n\n            \\State Sample $\\bar{x}^{(1)}_{s,a},\\bar{x}^{(2)}_{s,a},\\dots,\\bar{x}^{(\\ell_k)}_{s,a}\\in\\mathcal{X}$\n\n            \\State $\\widehat{\\beta}_{t+1}^{(k)}(s,a) \\gets \\frac{1}{\\ell_k}\\sum_{i=1}^{\\ell_k}\\big(u^{(k)}_{t+1}(\\bar{x}^{(i)}_{s,a}) -  u^{(k-1)}_{t+1}(\\bar{x}^{(i)}_{s,a})\\big) - \\frac{\\epsilon_{k}}{4H} - \\frac{3}{2}Ln^{-\\alpha}H$ \\label{line:classical_backward_line3}\n\n            \\EndFor\n\n            \\For{$s\\in\\mathcal{S}_n$}\n\n            \\State $u^{(k)}_t(s) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\widehat{\\mu}_{t+1}^{(k)}(s,a) + \\widehat{\\beta}_{t+1}^{(k)}(s,a)\\} - Ln^{-\\alpha}$\n            \n            \\State $\\pi_t^{(k)}(s) \\gets \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\widehat{\\mu}_{t+1}^{(k)}(s,a) + \\widehat{\\beta}_{t+1}^{(k)}(s,a)\\}$\n\n            \\State If $u^{(k)}_t(s) \\leq u^{(k-1)}_{t}(s)$, then $u^{(k)}_t(s) \\!\\gets\\! u^{(k-1)}_{t}(s)$ and $\\pi_t^{(k)}(s) \\!\\gets\\! \\pi_{t}^{(k-1)}(s)$ $\\forall x\\in\\mathcal{X}(s)$ \\label{line:classical_backward_line4}\n\n            %\\State $u_t^{(k)}(x) \\gets u_t^{(k)}(s)$ and $\\pi_t^{(k)}(x) \\gets \\pi_t^{(k)}(s)$ for all $x\\in\\mathcal{X}(s)$\n\n            \\EndFor\n\n        \\EndFor\n\n    \\EndFor\n    \n    \\State \\Return $\\pi^{(K)}\\in\\Pi^{\\rm D}$\n\\end{algorithmic}\n\\end{algorithm}", "\\begin{algorithmic}[1]  \n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$, finite action space $\\mathcal{A}$, horizon $H$, classical sampling access to probability kernels $p$, failure probability $\\delta\\in(0, 1)$, error $\\varepsilon > 0$.\n\n    \\Ensure $(\\varepsilon + 12Ln^{-\\alpha}H^2)$-optimal policy $\\pi^{(K)} \\in \\Pi^{\\rm D}$. \n\n    \\State Initialise $u^{(0)}_t \\equiv 0$ for all $t\\in[H]$ and $\\pi^{(0)}\\in\\Pi^{\\rm D}$ arbitrary\n\n    \\For{$k=1$ to $K:=\\lceil\\log_2(H/\\varepsilon)\\rceil$}\n\n        \\State $\\epsilon_k \\gets H/2^k$\n\n        \\State $m_k \\gets \\big\\lceil\\frac{128H^3}{\\min\\{\\epsilon^2_{k},1\\}}\\ln\\frac{16H|\\mathcal{S}_n|AK}{\\delta}\\big\\rceil$, $\\ell_k \\gets \\big\\lceil 512 H^2\\ln\\frac{4H|\\mathcal{S}_n|AK}{\\delta}\\big\\rceil$, $\\theta_k \\gets \\frac{1}{m_k}\\ln\\frac{16H|\\mathcal{S}_n|AK}{\\delta}$\n    \n        \\State For each $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, sample $x^{(1)}_{s,a},x^{(2)}_{s,a},\\dots,x^{(m_k)}_{s,a}\\in\\mathcal{X}$\n    \n        \\For{$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$}\n\n            \\State $\\widetilde{\\sigma}^{(k)}_t(s,a) \\gets \\frac{1}{m_k}\\sum_{i=1}^{m_k}(u^{(k-1)}_{t}(x^{(i)}_{s,a}))^2 - \\big(\\frac{1}{m_k}\\sum_{i=1}^{m_k}u^{(k-1)}_{t}(x^{(i)}_{s,a}) \\big)^2$ \\label{line:classical_backward_line1}\n    \n            \\State $\\widehat{\\mu}^{(k)}_t(s,a) \\gets \\frac{1}{m_k}\\sum_{i=1}^{m_k}u^{(k-1)}_{t}(x^{(i)}_{s,a}) - \\sqrt{2\\theta_k\\widetilde{\\sigma}_t^{(k)}(s,a)} - \\left(\\frac{2}{3}\\theta_k +  2(2\\theta_k)^{3/4} + Ln^{-\\alpha}\\right)H$ \\label{line:classical_backward_line2}\n    \n        \\EndFor\n\n        \\State $\\widehat{\\mu}_t^{(k)}(x,a) \\gets \\widehat{\\mu}_t^{(k)}(s,a)$ for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, $a\\in\\mathcal{A}$, and $t\\in[H]$\n\n        \\State $u^{(k)}_H(x) \\gets u^{(k-1)}_{H}(x)$ for all $x\\in\\mathcal{X}$ and $\\pi_H^{(k)}\\gets \\pi_{H}^{(k-1)}$\n\n        \\For{$t=H-1,H-2,\\dots,1$}\n\n            \\For{$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$}\n\n            \\State Sample $\\bar{x}^{(1)}_{s,a},\\bar{x}^{(2)}_{s,a},\\dots,\\bar{x}^{(\\ell_k)}_{s,a}\\in\\mathcal{X}$\n\n            \\State $\\widehat{\\beta}_{t+1}^{(k)}(s,a) \\gets \\frac{1}{\\ell_k}\\sum_{i=1}^{\\ell_k}\\big(u^{(k)}_{t+1}(\\bar{x}^{(i)}_{s,a}) -  u^{(k-1)}_{t+1}(\\bar{x}^{(i)}_{s,a})\\big) - \\frac{\\epsilon_{k}}{4H} - \\frac{3}{2}Ln^{-\\alpha}H$ \\label{line:classical_backward_line3}\n\n            \\EndFor\n\n            \\For{$s\\in\\mathcal{S}_n$}\n\n            \\State $u^{(k)}_t(s) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\widehat{\\mu}_{t+1}^{(k)}(s,a) + \\widehat{\\beta}_{t+1}^{(k)}(s,a)\\} - Ln^{-\\alpha}$\n            \n            \\State $\\pi_t^{(k)}(s) \\gets \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\widehat{\\mu}_{t+1}^{(k)}(s,a) + \\widehat{\\beta}_{t+1}^{(k)}(s,a)\\}$\n\n            \\State If $u^{(k)}_t(s) \\leq u^{(k-1)}_{t}(s)$, then $u^{(k)}_t(s) \\!\\gets\\! u^{(k-1)}_{t}(s)$ and $\\pi_t^{(k)}(s) \\!\\gets\\! \\pi_{t}^{(k-1)}(s)$ $\\forall x\\in\\mathcal{X}(s)$ \\label{line:classical_backward_line4}\n\n            %\\State $u_t^{(k)}(x) \\gets u_t^{(k)}(s)$ and $\\pi_t^{(k)}(x) \\gets \\pi_t^{(k)}(s)$ for all $x\\in\\mathcal{X}(s)$\n\n            \\EndFor\n\n        \\EndFor\n\n    \\EndFor\n    \n    \\State \\Return $\\pi^{(K)}\\in\\Pi^{\\rm D}$\n\\end{algorithmic}", "\\begin{algorithm}[t!]\n    \\caption{Modern quantum backward induction algorithm}\n    \\label{algo:quantum_backward_recursion}\n    \\begin{algorithmic}[1]  \n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$,  finite action space $\\mathcal{A}$, horizon $H$, quantum sampling access to probability kernels $p$, failure probability $\\delta\\in(0, 1)$, error $\\varepsilon > 0$.\n\n    \\Ensure $(\\varepsilon + 8Ln^{-\\alpha}H^2)$-optimal policy $\\pi^{(K)} \\in \\Pi^{\\rm D}$. \n\n    \\State Initialise $u^{(0)}_t \\equiv 0$ for all $t\\in[H]$ and $\\pi^{(0)}\\in\\Pi^{\\rm D}$ arbitrary\n\n    \\For{$k=1$ to $K:=\\lceil\\log_2(H/\\varepsilon)\\rceil$}\n\n        \\State $\\epsilon_k \\gets H/2^k$ and $\\theta_k \\gets \\frac{\\min\\{\\epsilon_k,1\\}}{20 H^{3/2}}$\n    \n%        \\State For each $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, sample $x^{(1)}_{s,a},x^{(2)}_{s,a},\\dots,x^{(m_k)}_{s,a}\\in\\mathcal{X}$\n    \n        \\For{$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$}\n\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use \\Cref{fact:quantum_multidimensional_mean_estimation_covariance_matrix} to get $\\widetilde{\\sigma}^{(k)}_t(s,a)$ such that $\\big|\\widetilde{\\sigma}^{(k)}_t(s,a) - \\sigma^{(k)}_t(s,a)\\big| \\leq \\theta_kH^2$, where $\\sigma^{(k)}_t(s,a) := \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t^{(k-1)}(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t^{(k-1)}(x')\\big)^2$ \\label{line:quantum_backward_line1}}\n    \n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use \\Cref{fact:quantum_multidimensional_mean_estimation_covariance_matrix} to get $\\widetilde{\\mu}^{(k)}_t(s,a)$ such that $\\big| \\widetilde{\\mu}^{(k)}_t(s,a) - \\mu_t^{(k)}(s,a)\\big| \\leq \\theta_k \\sqrt{\\frac{1}{H}\\sum_{t'=1}^H \\sigma_{t'}^{(k)}(s,a)}$, where $\\mu_t^{(k)}(s,a) := \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t^{(k-1)}(x')$ \\label{line:quantum_backward_line2}}\n    \n        \\EndFor\n\n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent}{$\\widehat{\\mu}_t^{(k)}(x,a) \\gets \\widetilde{\\mu}_t^{(k)}(s,a) - \\theta_k\\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\widetilde{\\sigma}_{t'}^{(k)}(s,a)} - (\\theta_k^{3/2} + Ln^{-\\alpha})H$ for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$, $x\\in\\mathcal{X}(s)$}\n\n        \\State $u^{(k)}_H(x) \\gets u^{(k-1)}_{H}(x)$ for all $x\\in\\mathcal{X}$ and $\\pi_H^{(k)}\\gets \\pi_{H}^{(k-1)}$\n\n        \\For{$t=H-1,H-2,\\dots,1$}\n\n            \\For{$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$}\n\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent-\\algorithmicindent}{Use \\Cref{lem:quantum_mean_estimation} to get $\\widetilde{\\beta}_{t+1}^{(k)}\\!(s,a)$ such that $\\big|\\widetilde{\\beta}_{t+1}^{(k)}\\!(s,a) - \\beta^{(k)}_{t+1}\\!(s,a) \\big| \\!\\leq\\! \\frac{\\epsilon_k}{4H} - Ln^{-\\alpha}H$, where $\\beta^{(k)}_{t+1}(s,a) := \\int_{\\mathcal{X}}p(\\rd x'|s,a)\\big(u^{(k)}_{t+1}(x') -  u^{(k-1)}_{t+1}(x')\\big)$ \\label{line:quantum_backward_line3}}\n\n            \\State $\\widehat{\\beta}^{(k)}_{t+1}(s,a) \\gets \\widetilde{\\beta}_{t+1}^{(k)}(s,a) - \\frac{\\epsilon_k}{4H} - Ln^{-\\alpha}H$\n\n            \\EndFor\n\n            \\For{$s\\in\\mathcal{S}_n$}\n\n            \\State $u^{(k)}_t(s) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\widehat{\\mu}_{t+1}^{(k)}(s,a) + \\widehat{\\beta}_{t+1}^{(k)}(s,a)\\} - Ln^{-\\alpha}$\n            \n            \\State $\\pi_t^{(k)}(s) \\gets \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\widehat{\\mu}_{t+1}^{(k)}(s,a) + \\widehat{\\beta}_{t+1}^{(k)}(s,a)\\}$\n\n            \\State If $u^{(k)}_t(s) \\leq u^{(k-1)}_{t}(s)$, then $u^{(k)}_t(x) \\!\\gets\\! u^{(k-1)}_{t}(x)$ and $\\pi_t^{(k)}(x) \\!\\gets\\! \\pi_{t}^{(k-1)}(x)$  $\\forall x\\in\\mathcal{X}(s)$\n            \n            \\EndFor\n\n        \\EndFor\n\n    \\EndFor\n    \n    \\State \\Return $\\pi^{(K)}\\in\\Pi^{\\rm D}$\n\\end{algorithmic}\n\\end{algorithm}", "\\begin{algorithmic}[1]  \n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$,  finite action space $\\mathcal{A}$, horizon $H$, quantum sampling access to probability kernels $p$, failure probability $\\delta\\in(0, 1)$, error $\\varepsilon > 0$.\n\n    \\Ensure $(\\varepsilon + 8Ln^{-\\alpha}H^2)$-optimal policy $\\pi^{(K)} \\in \\Pi^{\\rm D}$. \n\n    \\State Initialise $u^{(0)}_t \\equiv 0$ for all $t\\in[H]$ and $\\pi^{(0)}\\in\\Pi^{\\rm D}$ arbitrary\n\n    \\For{$k=1$ to $K:=\\lceil\\log_2(H/\\varepsilon)\\rceil$}\n\n        \\State $\\epsilon_k \\gets H/2^k$ and $\\theta_k \\gets \\frac{\\min\\{\\epsilon_k,1\\}}{20 H^{3/2}}$\n    \n%        \\State For each $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, sample $x^{(1)}_{s,a},x^{(2)}_{s,a},\\dots,x^{(m_k)}_{s,a}\\in\\mathcal{X}$\n    \n        \\For{$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$}\n\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use \\Cref{fact:quantum_multidimensional_mean_estimation_covariance_matrix} to get $\\widetilde{\\sigma}^{(k)}_t(s,a)$ such that $\\big|\\widetilde{\\sigma}^{(k)}_t(s,a) - \\sigma^{(k)}_t(s,a)\\big| \\leq \\theta_kH^2$, where $\\sigma^{(k)}_t(s,a) := \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t^{(k-1)}(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t^{(k-1)}(x')\\big)^2$ \\label{line:quantum_backward_line1}}\n    \n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use \\Cref{fact:quantum_multidimensional_mean_estimation_covariance_matrix} to get $\\widetilde{\\mu}^{(k)}_t(s,a)$ such that $\\big| \\widetilde{\\mu}^{(k)}_t(s,a) - \\mu_t^{(k)}(s,a)\\big| \\leq \\theta_k \\sqrt{\\frac{1}{H}\\sum_{t'=1}^H \\sigma_{t'}^{(k)}(s,a)}$, where $\\mu_t^{(k)}(s,a) := \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t^{(k-1)}(x')$ \\label{line:quantum_backward_line2}}\n    \n        \\EndFor\n\n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent}{$\\widehat{\\mu}_t^{(k)}(x,a) \\gets \\widetilde{\\mu}_t^{(k)}(s,a) - \\theta_k\\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\widetilde{\\sigma}_{t'}^{(k)}(s,a)} - (\\theta_k^{3/2} + Ln^{-\\alpha})H$ for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$, $x\\in\\mathcal{X}(s)$}\n\n        \\State $u^{(k)}_H(x) \\gets u^{(k-1)}_{H}(x)$ for all $x\\in\\mathcal{X}$ and $\\pi_H^{(k)}\\gets \\pi_{H}^{(k-1)}$\n\n        \\For{$t=H-1,H-2,\\dots,1$}\n\n            \\For{$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$}\n\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent-\\algorithmicindent}{Use \\Cref{lem:quantum_mean_estimation} to get $\\widetilde{\\beta}_{t+1}^{(k)}\\!(s,a)$ such that $\\big|\\widetilde{\\beta}_{t+1}^{(k)}\\!(s,a) - \\beta^{(k)}_{t+1}\\!(s,a) \\big| \\!\\leq\\! \\frac{\\epsilon_k}{4H} - Ln^{-\\alpha}H$, where $\\beta^{(k)}_{t+1}(s,a) := \\int_{\\mathcal{X}}p(\\rd x'|s,a)\\big(u^{(k)}_{t+1}(x') -  u^{(k-1)}_{t+1}(x')\\big)$ \\label{line:quantum_backward_line3}}\n\n            \\State $\\widehat{\\beta}^{(k)}_{t+1}(s,a) \\gets \\widetilde{\\beta}_{t+1}^{(k)}(s,a) - \\frac{\\epsilon_k}{4H} - Ln^{-\\alpha}H$\n\n            \\EndFor\n\n            \\For{$s\\in\\mathcal{S}_n$}\n\n            \\State $u^{(k)}_t(s) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\widehat{\\mu}_{t+1}^{(k)}(s,a) + \\widehat{\\beta}_{t+1}^{(k)}(s,a)\\} - Ln^{-\\alpha}$\n            \n            \\State $\\pi_t^{(k)}(s) \\gets \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\widehat{\\mu}_{t+1}^{(k)}(s,a) + \\widehat{\\beta}_{t+1}^{(k)}(s,a)\\}$\n\n            \\State If $u^{(k)}_t(s) \\leq u^{(k-1)}_{t}(s)$, then $u^{(k)}_t(x) \\!\\gets\\! u^{(k-1)}_{t}(x)$ and $\\pi_t^{(k)}(x) \\!\\gets\\! \\pi_{t}^{(k-1)}(x)$  $\\forall x\\in\\mathcal{X}(s)$\n            \n            \\EndFor\n\n        \\EndFor\n\n    \\EndFor\n    \n    \\State \\Return $\\pi^{(K)}\\in\\Pi^{\\rm D}$\n\\end{algorithmic}", "\\begin{algorithm}[t!]\n    \\caption{Simple quantum backward induction algorithm}\n    \\label{algo:quantum_backward_recursion2}\n    \\begin{algorithmic}[1]  \n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$,  finite action space $\\mathcal{A}$, horizon $H$, quantum sampling access to probability kernels $p$, failure probability $\\delta\\in(0, 1)$, error $\\varepsilon > 0$.\n\n    \\Ensure $(\\varepsilon + (1+H)HLn^{-\\alpha})$-optimal policy $\\pi \\in \\Pi^{\\rm D}$. \n\n\n    \\State $\\widehat{u}_H(s) \\gets \\min_{a\\in\\mathcal{A}}\\{r(s,a)\\}$ and $\\pi_H(s) \\gets \\argmin_{a\\in\\mathcal{A}}\\{r(s,a)\\}$ with probability $1-\\frac{\\delta}{H|\\mathcal{S}_n|}$ for all $s\\in\\mathcal{S}_n$ (\\Cref{fact:quantum_minimum_finding})\n    \n    \\State $u_H(x) \\gets \\widehat{u}_H(s) - Ln^{-\\alpha}$ and $\\pi_H(x) \\gets \\pi_H(s)$ for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$\n    \n        \\For{$t=H-1,H-2,\\dots,1$}\n\n            \\For{$s\\in\\mathcal{S}_n$}\n\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use \\Cref{lem:quantum_mean_estimation} to obtain a unitary $\\mathcal{U}_s^{(t+1)}:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|\\mu_{t+1}(s,a)\\rangle$ for all $a\\in\\mathcal{A}$, where $|\\mu_{t+1}(s,a) - \\int_{\\mathcal{X}}p(\\rd x'|s,a) u_{t+1}(x')| \\leq \\frac{\\varepsilon}{2H}$ with high probability}\n\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use quantum maximum finding with unitary $\\mathcal{U}_s^{(t+1)}$ (\\Cref{fact:quantum_minimum_finding} with $\\mathcal{U}_s^{(t+1)}$) to obtain $\\widehat{u}_t(s)$ and $a_t(s)$ such that, with probability $1-\\frac{\\delta}{H|\\mathcal{S}_n|}$,\n\t\t\t%\n\t\t\t\\begin{align*}\n\t\t\t\t\\widehat{u}_{t}(s) = \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\} \\quad\\text{and}\\quad\n\t\t\t\t \\pi_t(s) = \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}\n\t\t\t\\end{align*}\\label{line:quantum_simple}}\n\n\t\t\t\\State $u_t(x) \\gets \\widehat{u}_t(s) - \\frac{\\varepsilon}{2H} - (1+H)Ln^{-\\alpha}$ and $\\pi_t(x) \\gets \\pi_t(s)$ for all $x\\in\\mathcal{X}$\n\n\t\t\t\\State If $u_t(s) \\leq u_{t+1}(s)$, then $u_t(x) \\gets u_{t+1}(x)$ and $\\pi_t(x) \\gets \\pi_{t+1}(x)$ for all $x\\in\\mathcal{X}(s)$ \\label{line:simple_quantum_enforcement}\n\t\t\n            \\EndFor\n\n            \n        \\EndFor\n    \n    \\State \\Return $\\pi = (\\pi_1,\\dots,\\pi_H)\\in\\Pi^{\\rm D}$\n\\end{algorithmic}\n\\end{algorithm}", "\\begin{algorithmic}[1]  \n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$,  finite action space $\\mathcal{A}$, horizon $H$, quantum sampling access to probability kernels $p$, failure probability $\\delta\\in(0, 1)$, error $\\varepsilon > 0$.\n\n    \\Ensure $(\\varepsilon + (1+H)HLn^{-\\alpha})$-optimal policy $\\pi \\in \\Pi^{\\rm D}$. \n\n\n    \\State $\\widehat{u}_H(s) \\gets \\min_{a\\in\\mathcal{A}}\\{r(s,a)\\}$ and $\\pi_H(s) \\gets \\argmin_{a\\in\\mathcal{A}}\\{r(s,a)\\}$ with probability $1-\\frac{\\delta}{H|\\mathcal{S}_n|}$ for all $s\\in\\mathcal{S}_n$ (\\Cref{fact:quantum_minimum_finding})\n    \n    \\State $u_H(x) \\gets \\widehat{u}_H(s) - Ln^{-\\alpha}$ and $\\pi_H(x) \\gets \\pi_H(s)$ for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$\n    \n        \\For{$t=H-1,H-2,\\dots,1$}\n\n            \\For{$s\\in\\mathcal{S}_n$}\n\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use \\Cref{lem:quantum_mean_estimation} to obtain a unitary $\\mathcal{U}_s^{(t+1)}:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|\\mu_{t+1}(s,a)\\rangle$ for all $a\\in\\mathcal{A}$, where $|\\mu_{t+1}(s,a) - \\int_{\\mathcal{X}}p(\\rd x'|s,a) u_{t+1}(x')| \\leq \\frac{\\varepsilon}{2H}$ with high probability}\n\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use quantum maximum finding with unitary $\\mathcal{U}_s^{(t+1)}$ (\\Cref{fact:quantum_minimum_finding} with $\\mathcal{U}_s^{(t+1)}$) to obtain $\\widehat{u}_t(s)$ and $a_t(s)$ such that, with probability $1-\\frac{\\delta}{H|\\mathcal{S}_n|}$,\n\t\t\t%\n\t\t\t\\begin{align*}\n\t\t\t\t\\widehat{u}_{t}(s) = \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\} \\quad\\text{and}\\quad\n\t\t\t\t \\pi_t(s) = \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}\n\t\t\t\\end{align*}\\label{line:quantum_simple}}\n\n\t\t\t\\State $u_t(x) \\gets \\widehat{u}_t(s) - \\frac{\\varepsilon}{2H} - (1+H)Ln^{-\\alpha}$ and $\\pi_t(x) \\gets \\pi_t(s)$ for all $x\\in\\mathcal{X}$\n\n\t\t\t\\State If $u_t(s) \\leq u_{t+1}(s)$, then $u_t(x) \\gets u_{t+1}(x)$ and $\\pi_t(x) \\gets \\pi_{t+1}(x)$ for all $x\\in\\mathcal{X}(s)$ \\label{line:simple_quantum_enforcement}\n\t\t\n            \\EndFor\n\n            \n        \\EndFor\n    \n    \\State \\Return $\\pi = (\\pi_1,\\dots,\\pi_H)\\in\\Pi^{\\rm D}$\n\\end{algorithmic}", "\\begin{algorithm}[t!]\n    \\caption{Classical approximate value iteration algorithm}\n    \\label{algo:classical_extended_value_iteration}\n    \\begin{algorithmic}[1]  \n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$, finite action space $\\mathcal{A}$, classical sampling access to probability kernels $p$, failure probability $\\delta\\in(0, 1)$, parameters $\\Lambda > 0$, $\\nu\\in[0,1)$, $L,\\alpha \\geq 0$, errors $\\varepsilon, \\varepsilon_u > 0$ such that $\\varepsilon_u = \\frac{1}{4}(1-\\nu)\\varepsilon$.\n\n    \\Ensure $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$-optimal stationary policy $\\widetilde{d}^\\infty$.\n\n    \\State $t\\gets 1$ and initialise ${u}_{0} \\gets {0}$ and $u_1(x) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a)\\}$ for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$\n\n    \\While {$\\operatorname{sp}({{u}}_{t} - {{u}}_{t-1}) > \\frac{3\\varepsilon}{2} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$}\n    \n    \\State $m_t \\gets \\big\\lceil \\frac{512}{(1-\\nu)^2 \\varepsilon^2}{\\min}\\big\\{ 4(1+\\Lambda)^2, \\frac{\\Lambda^2}{(1-\\nu)^2}\\big\\}\\ln\\frac{\\pi^2 t^2 |\\mathcal{S}_n|A}{6\\delta} \\big\\rceil$\n    \n    \\State For $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, sample $x^{(1)}_{s,a},x^{(2)}_{s,a},\\dots,x^{(m_t)}_{s,a} \\in\\mathcal{X}$\n        \n\n    \\State  \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent}{For all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$, $a_{t+1}(x) \\gets \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\frac{1}{m_t}\\sum_{i=1}^{m_t} u_t(x^{(i)}_{s,a})\\}$, $\\widetilde{u}_{t+1}(s) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\frac{1}{m_t}\\sum_{i=1}^{m_t} u_t(x^{(i)}_{s,a})\\}$, and \n    %\n    \\begin{align*}\n        u_{t+1}(x) \\gets \\begin{dcases}\n            \\max\\left\\{\\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~\\widetilde{u}_{t+1}(s) \\geq \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\left\\{\\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~ \\widetilde{u}_{t+1}(s) \\leq \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}(s) &\\text{otherwise}.\n        \\end{dcases}\n    \\end{align*}}\n\n\n    \\State $t \\gets t+1$\n\n    \\EndWhile\n    \n    \\State \\Return $\\widetilde{d}\\in\\mathcal{D}^{\\rm D}$ where $\\widetilde{d}(x) := a_t(x)$ for all $x\\in\\mathcal{X}$\n\\end{algorithmic}\n\\end{algorithm}", "\\begin{algorithmic}[1]  \n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$, finite action space $\\mathcal{A}$, classical sampling access to probability kernels $p$, failure probability $\\delta\\in(0, 1)$, parameters $\\Lambda > 0$, $\\nu\\in[0,1)$, $L,\\alpha \\geq 0$, errors $\\varepsilon, \\varepsilon_u > 0$ such that $\\varepsilon_u = \\frac{1}{4}(1-\\nu)\\varepsilon$.\n\n    \\Ensure $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$-optimal stationary policy $\\widetilde{d}^\\infty$.\n\n    \\State $t\\gets 1$ and initialise ${u}_{0} \\gets {0}$ and $u_1(x) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a)\\}$ for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$\n\n    \\While {$\\operatorname{sp}({{u}}_{t} - {{u}}_{t-1}) > \\frac{3\\varepsilon}{2} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$}\n    \n    \\State $m_t \\gets \\big\\lceil \\frac{512}{(1-\\nu)^2 \\varepsilon^2}{\\min}\\big\\{ 4(1+\\Lambda)^2, \\frac{\\Lambda^2}{(1-\\nu)^2}\\big\\}\\ln\\frac{\\pi^2 t^2 |\\mathcal{S}_n|A}{6\\delta} \\big\\rceil$\n    \n    \\State For $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, sample $x^{(1)}_{s,a},x^{(2)}_{s,a},\\dots,x^{(m_t)}_{s,a} \\in\\mathcal{X}$\n        \n\n    \\State  \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent}{For all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$, $a_{t+1}(x) \\gets \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\frac{1}{m_t}\\sum_{i=1}^{m_t} u_t(x^{(i)}_{s,a})\\}$, $\\widetilde{u}_{t+1}(s) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\frac{1}{m_t}\\sum_{i=1}^{m_t} u_t(x^{(i)}_{s,a})\\}$, and \n    %\n    \\begin{align*}\n        u_{t+1}(x) \\gets \\begin{dcases}\n            \\max\\left\\{\\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~\\widetilde{u}_{t+1}(s) \\geq \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\left\\{\\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~ \\widetilde{u}_{t+1}(s) \\leq \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}(s) &\\text{otherwise}.\n        \\end{dcases}\n    \\end{align*}}\n\n\n    \\State $t \\gets t+1$\n\n    \\EndWhile\n    \n    \\State \\Return $\\widetilde{d}\\in\\mathcal{D}^{\\rm D}$ where $\\widetilde{d}(x) := a_t(x)$ for all $x\\in\\mathcal{X}$\n\\end{algorithmic}", "\\begin{algorithm}[t!]\n    \\caption{Quantum value iteration algorithm}\n    \\label{algo:quantum_extended_value_iteration}\n    \\begin{algorithmic}[1]  \n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$, finite action space $\\mathcal{A}$, quantum sampling access to probability kernels $p$, failure probability $\\delta\\in(0, 1)$, parameters $\\Lambda>0$, $\\nu\\in[0,1)$, $L,\\alpha \\geq 0$, errors $\\varepsilon_u, \\varepsilon > 0$ such that $\\varepsilon_u = \\frac{1}{4}(1-\\nu)\\varepsilon$.\n\n    \\Ensure $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$-optimal stationary policy $\\widetilde{d}^\\infty$. \n\n    \\State $t\\gets 1$ and initialise ${u}_{0} \\gets {0}$ \n    \\State $u_{1}(x) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a)\\}$ for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$ with probability $1-\\frac{6\\delta}{\\pi^2 |\\mathcal{S}_n|}$ (\\Cref{fact:quantum_minimum_finding})\n\n    \\While {$\\operatorname{sp}({{u}}_{t} - {{u}}_{t-1}) > \\frac{3\\varepsilon}{2} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$}\n    \n    \\State Build quantum access to ${{u}}_{t}$\n    \\For{$s\\in\\mathcal{S}_n$}\n        %\\State $\\widetilde\\mu_{\\max}(s, a)\\gets$ Run Algorithm~\\ref{algo:inner_maximization} with additive error $\\frac{\\epsilon}{2}$ and success probability $1-\\frac{\\delta}{3}$ using Lemma~\\ref{lem:quantum_inner_maximization}. \\label{algo_line:quantum_inner_maximization}\n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use \\cref{lem:quantum_mean_estimation} to obtain a unitary $\\mathcal{U}^{(t)}_s:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|{\\mu}_{t}(s,a)\\rangle$ for all $a\\in\\mathcal{A}$, where $|{\\mu}_{t}(s,a) - \\int_{\\mathcal{X}} p(\\rd x'|s,a) {u}_{t}(x') | \\leq \\frac{\\varepsilon_u}{2}$ with high probability}\n        \n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use quantum maximum finding with unitary $\\mathcal{U}_s^{(t)}$ (\\cref{fact:quantum_minimum_finding} with $\\mathcal{U}^{(t)}_s$) to get $\\widetilde{u}_{t+1}(s)$ and $a_{t+1}(s)$ such that, with probability $1-\\frac{6\\delta}{\\pi^2 t^2 |\\mathcal{S}_n|}$,\n        %\n        \\begin{align*}\n            \\widetilde{u}_{t+1}(s) = \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\} \\quad\\text{and}\\quad a_{t+1}(s) = \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}\n        \\end{align*}}\n    \\EndFor\n\n    \\State For all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$, $a_{t+1}(x) \\gets a_{t+1}(s)$ and\n    %\n    \\begin{align*}\n        u_{t+1}(x) \\gets \\begin{dcases}\n            \\max\\left\\{\\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~\\widetilde{u}_{t+1}(s) \\geq \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\left\\{\\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~ \\widetilde{u}_{t+1}(s) \\leq \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}(s) &\\text{otherwise}.\n        \\end{dcases}\n    \\end{align*}\n\n\n    \\State $t \\gets t+1$\n\n    \\EndWhile\n    \n    \\State \\Return $\\widetilde{d}\\in\\mathcal{D}^{\\rm D}$ where $\\widetilde{d}(x) := a_t(x)$ for all $x\\in\\mathcal{X}$\n\\end{algorithmic}\n\\end{algorithm}", "\\begin{algorithmic}[1]  \n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$, finite action space $\\mathcal{A}$, quantum sampling access to probability kernels $p$, failure probability $\\delta\\in(0, 1)$, parameters $\\Lambda>0$, $\\nu\\in[0,1)$, $L,\\alpha \\geq 0$, errors $\\varepsilon_u, \\varepsilon > 0$ such that $\\varepsilon_u = \\frac{1}{4}(1-\\nu)\\varepsilon$.\n\n    \\Ensure $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$-optimal stationary policy $\\widetilde{d}^\\infty$. \n\n    \\State $t\\gets 1$ and initialise ${u}_{0} \\gets {0}$ \n    \\State $u_{1}(x) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a)\\}$ for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$ with probability $1-\\frac{6\\delta}{\\pi^2 |\\mathcal{S}_n|}$ (\\Cref{fact:quantum_minimum_finding})\n\n    \\While {$\\operatorname{sp}({{u}}_{t} - {{u}}_{t-1}) > \\frac{3\\varepsilon}{2} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$}\n    \n    \\State Build quantum access to ${{u}}_{t}$\n    \\For{$s\\in\\mathcal{S}_n$}\n        %\\State $\\widetilde\\mu_{\\max}(s, a)\\gets$ Run Algorithm~\\ref{algo:inner_maximization} with additive error $\\frac{\\epsilon}{2}$ and success probability $1-\\frac{\\delta}{3}$ using Lemma~\\ref{lem:quantum_inner_maximization}. \\label{algo_line:quantum_inner_maximization}\n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use \\cref{lem:quantum_mean_estimation} to obtain a unitary $\\mathcal{U}^{(t)}_s:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|{\\mu}_{t}(s,a)\\rangle$ for all $a\\in\\mathcal{A}$, where $|{\\mu}_{t}(s,a) - \\int_{\\mathcal{X}} p(\\rd x'|s,a) {u}_{t}(x') | \\leq \\frac{\\varepsilon_u}{2}$ with high probability}\n        \n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use quantum maximum finding with unitary $\\mathcal{U}_s^{(t)}$ (\\cref{fact:quantum_minimum_finding} with $\\mathcal{U}^{(t)}_s$) to get $\\widetilde{u}_{t+1}(s)$ and $a_{t+1}(s)$ such that, with probability $1-\\frac{6\\delta}{\\pi^2 t^2 |\\mathcal{S}_n|}$,\n        %\n        \\begin{align*}\n            \\widetilde{u}_{t+1}(s) = \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\} \\quad\\text{and}\\quad a_{t+1}(s) = \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}\n        \\end{align*}}\n    \\EndFor\n\n    \\State For all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$, $a_{t+1}(x) \\gets a_{t+1}(s)$ and\n    %\n    \\begin{align*}\n        u_{t+1}(x) \\gets \\begin{dcases}\n            \\max\\left\\{\\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~\\widetilde{u}_{t+1}(s) \\geq \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\left\\{\\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~ \\widetilde{u}_{t+1}(s) \\leq \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}(s) &\\text{otherwise}.\n        \\end{dcases}\n    \\end{align*}\n\n\n    \\State $t \\gets t+1$\n\n    \\EndWhile\n    \n    \\State \\Return $\\widetilde{d}\\in\\mathcal{D}^{\\rm D}$ where $\\widetilde{d}(x) := a_t(x)$ for all $x\\in\\mathcal{X}$\n\\end{algorithmic}", "\\begin{algorithm}[t!]\n    \\caption{Classical/quantum online-learning algorithm for finite-horizon MDPs}\n    \\label{algo:quantum_UCCRL_finite-horizon}\n    \\begin{algorithmic}[1]\n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$, finite action space $\\mathcal A$, horizon $H$, rewards $r:\\mathcal{X}\\times\\mathcal{A}\\to [0,1]$, failure probability $\\delta\\in(0,1)$, parameters $L,\\alpha\\geq 0$.\n\n    \\For{episodes $k\\in[K]$}\n        \n        \\phase{Generative phase}\n\n        \\If{$k=2^{\\lfloor \\log_2{k}\\rfloor}$}\n    \n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{\\textbf{Classical:} By using \\cref{algo:classical_backward_recursion}, choose a policy $\\pi^{(k)}\\in\\Pi^{\\rm D}$ such that, with failure probability at most $\\frac{\\delta}{\\lceil\\log_2{K}\\rceil}$ and for some constant $c>0$,\n            \\begin{align*}\n                \\|V_1^{\\pi^{(k)}} - V_1^\\ast\\|_\\infty \\leq  c\\sqrt{\\frac{H^3|\\mathcal{S}_n|A}{k}\\log\\left(\\frac{H|\\mathcal{S}_n|A\\log{K}}{\\delta}\\right)} - cLn^{-\\alpha}H^2 \n            \\end{align*}}\n\n            \\setcounter{ALG@line}{2}\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{\\textbf{Quantum:} By using \\cref{algo:quantum_backward_recursion} or \\Cref{algo:quantum_backward_recursion2}, choose a policy $\\pi^{(k)}\\in\\Pi^{\\rm D}$ such that, with failure probability at most $\\frac{\\delta}{\\lceil\\log_2{K}\\rceil}$ and for some constant $c>0$,\n            \\begin{align*}\n                \\|V_1^{\\pi^{(k)}} - V_1^\\ast\\|_\\infty \\leq c \\frac{|\\mathcal{S}_n|}{k}\\min\\{HA,H^2\\sqrt{A}\\}\\log^2\\left(\\frac{H|\\mathcal{S}_n|A\\log{K}}{\\delta}\\right) - cLn^{-\\alpha}H^2\n            \\end{align*}}\n        \n        \\Else\n            \\State $\\pi^{(k)} \\gets \\pi^{(k-1)}$\n        \\EndIf\n\n        \\phase{Exploration phase}\n        \\State Observe initial state $x^{(k)}_1$\n        \\For{$t\\in[H]$}\n            \\State Choose action $a^{(k)}_t = \\pi_t^{(k)}(x_t^{(k)})$ and obtain reward $r^{(k)}_t \\gets r(x^{(k)}_t,a^{(k)}_t)$\n            \\State Observe next state $x^{(k)}_{t+1} \\sim p(\\cdot|x^{(k)}_t,a_t^{(k)})$\n        \\EndFor\n        \n    \\EndFor\n    %\\Ensure \n\\end{algorithmic}\n\\end{algorithm}", "\\begin{algorithmic}[1]\n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$, finite action space $\\mathcal A$, horizon $H$, rewards $r:\\mathcal{X}\\times\\mathcal{A}\\to [0,1]$, failure probability $\\delta\\in(0,1)$, parameters $L,\\alpha\\geq 0$.\n\n    \\For{episodes $k\\in[K]$}\n        \n        \\phase{Generative phase}\n\n        \\If{$k=2^{\\lfloor \\log_2{k}\\rfloor}$}\n    \n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{\\textbf{Classical:} By using \\cref{algo:classical_backward_recursion}, choose a policy $\\pi^{(k)}\\in\\Pi^{\\rm D}$ such that, with failure probability at most $\\frac{\\delta}{\\lceil\\log_2{K}\\rceil}$ and for some constant $c>0$,\n            \\begin{align*}\n                \\|V_1^{\\pi^{(k)}} - V_1^\\ast\\|_\\infty \\leq  c\\sqrt{\\frac{H^3|\\mathcal{S}_n|A}{k}\\log\\left(\\frac{H|\\mathcal{S}_n|A\\log{K}}{\\delta}\\right)} - cLn^{-\\alpha}H^2 \n            \\end{align*}}\n\n            \\setcounter{ALG@line}{2}\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{\\textbf{Quantum:} By using \\cref{algo:quantum_backward_recursion} or \\Cref{algo:quantum_backward_recursion2}, choose a policy $\\pi^{(k)}\\in\\Pi^{\\rm D}$ such that, with failure probability at most $\\frac{\\delta}{\\lceil\\log_2{K}\\rceil}$ and for some constant $c>0$,\n            \\begin{align*}\n                \\|V_1^{\\pi^{(k)}} - V_1^\\ast\\|_\\infty \\leq c \\frac{|\\mathcal{S}_n|}{k}\\min\\{HA,H^2\\sqrt{A}\\}\\log^2\\left(\\frac{H|\\mathcal{S}_n|A\\log{K}}{\\delta}\\right) - cLn^{-\\alpha}H^2\n            \\end{align*}}\n        \n        \\Else\n            \\State $\\pi^{(k)} \\gets \\pi^{(k-1)}$\n        \\EndIf\n\n        \\phase{Exploration phase}\n        \\State Observe initial state $x^{(k)}_1$\n        \\For{$t\\in[H]$}\n            \\State Choose action $a^{(k)}_t = \\pi_t^{(k)}(x_t^{(k)})$ and obtain reward $r^{(k)}_t \\gets r(x^{(k)}_t,a^{(k)}_t)$\n            \\State Observe next state $x^{(k)}_{t+1} \\sim p(\\cdot|x^{(k)}_t,a_t^{(k)})$\n        \\EndFor\n        \n    \\EndFor\n    %\\Ensure \n\\end{algorithmic}", "\\begin{algorithm}[t!]\n    \\caption{Classical/quantum online-learning algorithm for infinite-horizon MDPs}\n    \\label{algo:quantum_UCCRL}\n    \\begin{algorithmic}[1]\n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$, finite action space $\\mathcal{A}$, upper bound $\\Lambda$ on optimal bias span, rewards $r:\\mathcal{X}\\times\\mathcal{A}\\to [0,1]$, failure probability $\\delta\\in(0,1)$, parameters $L,\\alpha \\geq 0$.\n    \n    \\State $t \\gets 1$ and $\\tau_1 \\gets 1$\n    \n    \\For {episodes $k=1, 2, \\dots$}\n        \n        \\phase{Generative phase}\n\n        \n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent}{\\textbf{Classical:} By using \\cref{algo:classical_extended_value_iteration}, choose decision rule $\\widetilde{d}_k\\in\\mathcal{D}^{\\rm D}$ such that, with failure probability at most $\\frac{\\delta}{8T^{5/4}}$ and for some constant $c>0$,\n        %\n        \\begin{align*}\n            g^{\\widetilde{d}_k^\\infty}(x) \\geq  g^* - c\\Lambda\\sqrt{\\frac{|\\mathcal{S}_n|A\\log{T}}{\\tau_k}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}} - c(1+\\Lambda)Ln^{-\\alpha} \n        \\end{align*}}\n\n        \\setcounter{ALG@line}{2}\n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent}{\\textbf{Quantum:} By using \\cref{algo:quantum_extended_value_iteration}, choose decision rule $\\widetilde{d}_k\\in\\mathcal{D}^{\\rm D}$ such that, with failure probability at most $\\frac{\\delta}{8T^{5/4}}$ and for some constant $c>0$,\n        %\n        \\begin{align*}\n            g^{\\widetilde{d}_k^\\infty}(x) \\geq  g^* - c\\Lambda \\frac{|\\mathcal{S}_n|\\sqrt{A}\\log{T}}{\\tau_k}\\log^2\\frac{|\\mathcal{S}_n|AT}{\\delta} - c(1+\\Lambda)Ln^{-\\alpha} \n        \\end{align*}}\n\n        \\phase{Exploration phase}\n        \\State $\\tau_{k+1} \\gets t$ and observe random initial state $x_{t}$\n        \\While {$t < 2\\tau_{k+1}$}\n            \\State Choose an action $a_{t}\\sim \\widetilde{d}_k(x_{t})$ and obtain reward $r_{t}\\gets r(x_{t},a_{t})$\n            \\State Observe next state $x_{t+1}\\sim p(\\cdot|x_{t},a_{t})$\n            \\State $t \\gets t + 1$\n        \\EndWhile\n        \n    \\EndFor\n    %\\Ensure \n\\end{algorithmic}\n\\end{algorithm}", "\\begin{algorithmic}[1]\n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$, finite action space $\\mathcal{A}$, upper bound $\\Lambda$ on optimal bias span, rewards $r:\\mathcal{X}\\times\\mathcal{A}\\to [0,1]$, failure probability $\\delta\\in(0,1)$, parameters $L,\\alpha \\geq 0$.\n    \n    \\State $t \\gets 1$ and $\\tau_1 \\gets 1$\n    \n    \\For {episodes $k=1, 2, \\dots$}\n        \n        \\phase{Generative phase}\n\n        \n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent}{\\textbf{Classical:} By using \\cref{algo:classical_extended_value_iteration}, choose decision rule $\\widetilde{d}_k\\in\\mathcal{D}^{\\rm D}$ such that, with failure probability at most $\\frac{\\delta}{8T^{5/4}}$ and for some constant $c>0$,\n        %\n        \\begin{align*}\n            g^{\\widetilde{d}_k^\\infty}(x) \\geq  g^* - c\\Lambda\\sqrt{\\frac{|\\mathcal{S}_n|A\\log{T}}{\\tau_k}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}} - c(1+\\Lambda)Ln^{-\\alpha} \n        \\end{align*}}\n\n        \\setcounter{ALG@line}{2}\n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent}{\\textbf{Quantum:} By using \\cref{algo:quantum_extended_value_iteration}, choose decision rule $\\widetilde{d}_k\\in\\mathcal{D}^{\\rm D}$ such that, with failure probability at most $\\frac{\\delta}{8T^{5/4}}$ and for some constant $c>0$,\n        %\n        \\begin{align*}\n            g^{\\widetilde{d}_k^\\infty}(x) \\geq  g^* - c\\Lambda \\frac{|\\mathcal{S}_n|\\sqrt{A}\\log{T}}{\\tau_k}\\log^2\\frac{|\\mathcal{S}_n|AT}{\\delta} - c(1+\\Lambda)Ln^{-\\alpha} \n        \\end{align*}}\n\n        \\phase{Exploration phase}\n        \\State $\\tau_{k+1} \\gets t$ and observe random initial state $x_{t}$\n        \\While {$t < 2\\tau_{k+1}$}\n            \\State Choose an action $a_{t}\\sim \\widetilde{d}_k(x_{t})$ and obtain reward $r_{t}\\gets r(x_{t},a_{t})$\n            \\State Observe next state $x_{t+1}\\sim p(\\cdot|x_{t},a_{t})$\n            \\State $t \\gets t + 1$\n        \\EndWhile\n        \n    \\EndFor\n    %\\Ensure \n\\end{algorithmic}"], "sections": {"Introduction": {"content": "\n\nReinforcement learning~\\cite{sutton1998reinforcement} is a subfield of machine learning that studies how an agent can properly interact with a dynamical environment in order to maximise some type of reward. Markov decision processes (MDPs)~\\cite{puterman2014markov} serve as the most commonly used framework for modeling such agent-environment interactions. In its most general form, a discrete-time MDP $M$M can be described by a tuple $\\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$\\langle \\mathcal{X},\\mathcal{A},p,r\\rangle, where the state space $\\mathcal{X}$\\mathcal{X} is the set of possible states the environment can assume, the action space $\\mathcal{A}$\\mathcal{A} is the set of possible actions the agent can choose, the reward function $r:\\mathcal{X}\\times\\mathcal{A}\\to[0,1]$r:\\mathcal{X}\\times\\mathcal{A}\\to[0,1] yields a reward to the agent when interacting with the environment, and the stochastic kernels $p(\\cdot|x,a)$p(\\cdot|x,a) denote the transition probability to another environment state given the current state-action pair $(x,a)$(x,a). At any (discrete) point in time, the environment is in some state $x_t\\in\\mathcal{X}$x_t\\in\\mathcal{X} and the agent must choose an action $a_t\\in\\mathcal{A}$a_t\\in\\mathcal{A}, after which they receive a reward $r(x_t,a_t)$r(x_t,a_t) and the environment randomly transitions to a new state $x_{t+1}$x_{t+1}t+1 according to $p(\\cdot|x_t,a_t)$p(\\cdot|x_t,a_t). The agent chooses an action through a \\emph{policy}  $\\pi = (\\pi_t)_t$\\pi = (\\pi_t)_t, which is a sequence of \\emph{decision rules} $\\pi_t$\\pi_t, which in turn  are probability distribution over actions $a\\in\\mathcal{A}$a\\in\\mathcal{A} given $x\\in\\mathcal{X}$x\\in\\mathcal{X}. The practical utility of MDPs lies in their ability to model decision-making in complex environments~\\cite{aastrom1965optimal, hu2007markov,sato2010markov,  bauerle2011markov, feinberg2012handbook, bennett2013artificial, chen2014distributed,steimle2017markov, natarajan2022planning}, allowing successful applications to several problems~\\cite{sutton1998reinforcement,Szepesvari2010algorithms,bertsekas2012dynamic,bertsekas2022abstract}.\n\nThere are several types of MDPs in the literature~\\cite{puterman2014markov}, the most common ones being \\emph{finite-horizon} MDPs, \\emph{infinite-horizon discounted} MDPs, and \\emph{infinite-horizon undiscounted} MDPs. In this paper, we shall focus on finite-horizon and infinite-horizon undiscounted MDPs. For the former (also known as tabular MDP), the agent interacts with the environment for a finite pre-determined number of time steps $H$H, which is called the horizon. The standard criteria to evaluate the performance of the agent is the \\emph{expected total reward} $V_1^\\pi(x)$V_1^\\pi(x) over the horizon $H$H when executing policy $\\pi$\\pi with initial state $x\\in\\mathcal{X}$x\\in\\mathcal{X},\n\\begin{align*}\n    V_1^\\pi(x) := \\mathbb{E}_{x_1=x}^\\pi\\left[\\sum_{t=1}^H r(x_t,a_t)\\right].\n\\end{align*}\\begin{align*}\n    V_1^\\pi(x) := \\mathbb{E}_{x_1=x}^\\pi\\left[\\sum_{t=1}^H r(x_t,a_t)\\right].\n\\end{align*}\n    V_1^\\pi(x) := \\mathbb{E}_{x_1=x}x_1=x^\\pi\\left[\\sum_{t=1}t=1^H r(x_t,a_t)\\right].\n\nA policy $\\pi^\\varepsilon$\\pi^\\varepsilon is $\\varepsilon$\\varepsilon-\\emph{optimal} if $V_1^{\\pi^\\varepsilon}(x) \\geq V_1^\\pi(x) - \\varepsilon$V_1^{\\pi^\\varepsilon}\\pi^\\varepsilon(x) \\geq V_1^\\pi(x) - \\varepsilon for all $\\pi$\\pi and $x\\in\\mathcal{X}$x\\in\\mathcal{X}, and the optimal total expected reward is $V_1^\\ast(x) = \\sup_\\pi V_1^\\pi(x)$V_1^\\ast(x) = \\sup_\\pi V_1^\\pi(x). In infinite-horizon (undiscounted) MDPs, the agent can interact with the environment for an infinite amount of time steps, meaning that $H=\\infty$H=\\infty. One of the standard criteria to evaluate the performance of the agent is the \\emph{average reward} (or gain) $g^\\pi(x)$g^\\pi(x) when executing policy $\\pi$\\pi with initial state $x\\in\\mathcal{X}$x\\in\\mathcal{X},\n\\begin{align*}\n    g^\\pi(x) := \\lim_{T\\to\\infty}\\mathbb{E}_{x_1=x}^\\pi\\left[\\frac{1}{T}\\sum_{t=1}^T r(x_t,a_t) \\right].\n\\end{align*}\\begin{align*}\n    g^\\pi(x) := \\lim_{T\\to\\infty}\\mathbb{E}_{x_1=x}^\\pi\\left[\\frac{1}{T}\\sum_{t=1}^T r(x_t,a_t) \\right].\n\\end{align*}\n    g^\\pi(x) := \\lim_{T\\to\\infty}T\\to\\infty\\mathbb{E}_{x_1=x}x_1=x^\\pi\\left[\\frac{1}{T}\\sum_{t=1}t=1^T r(x_t,a_t) \\right].\n\nA policy $\\pi^\\varepsilon$\\pi^\\varepsilon is $\\varepsilon$\\varepsilon-optimal if $g^{\\pi^\\varepsilon}(x) \\geq g^\\pi(x) - \\varepsilon$g^{\\pi^\\varepsilon}\\pi^\\varepsilon(x) \\geq g^\\pi(x) - \\varepsilon for all $\\pi$\\pi and $x\\in\\mathcal{X}$x\\in\\mathcal{X}, and the optimal average reward is $g^\\ast(x) = \\sup_\\pi g^\\pi(x)$g^\\ast(x) = \\sup_\\pi g^\\pi(x). \n\nParallel to reinforcement learning developments, quantum machine learning~\\cite{Biamonte2017quantum}, a subfield of quantum computation~\\cite{nielsen2010quantum}, has been consolidated as an area of study that can provide quantum speed-ups to several traditional machine learning problems~\\cite{lloyd2013quantum,rebentrost2014quantum,kerenidis2017quantum,kerenidis2019qmeans,doriguello2023you}, including reinforcement learning~\\cite{dong2008quantum}. It has been found that quantum algorithms are not only capable of achieving speedup in the time complexity of certain tasks, but also have the potential to be better ``learners'' than their classical counterparts in the online setting~\\cite{ganguly2023quantum,zhong2023provably}.\n\n\n\n\n\n\n\n\n\n\\subsection{Our work}\n\nIn this work we study two fundamental problems of MDPs: (i) approximating optimal policies under a generative model and (ii) online learning of MDPs, both in the classical and quantum settings. For that, we propose a hybrid generative online-learning model in which classical and quantum computers are capable of offering an improved learning progress compared to several previous works~\\cite{auer2008near,bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient,ganguly2023quantum,zhong2023provably}.\n\n\\subsubsection{Computing approximate optimal policies under a generative model}\n\nOne of the main problems associated with MDPs is that of computing $\\varepsilon$\\varepsilon-optimal policies, for which several algorithms have been proposed~\\cite{ bellman1966dynamic,watkins1992q,meyn1997policy, kearns1998finite, sutton1999policy,lagoudakis2003least,bertsekas2011approximate,azar2012dynamic,puterman2014markov,silver2014deterministic, schulman2015trust,  jang2019q}. The main interest is thus on the performance of the computed policy~\\cite{sutton1998reinforcement,Kearns1999finite,Kearns2002near}. Several input access models can be considered for computing optimal policies. In this paper, we consider the so-called \\emph{generative model}~\\cite{kearns1998finite,Kearns2002sparse,kakade2003sample} where one has full knowledge of state and action spaces $\\mathcal{X},\\mathcal{A}$\\mathcal{X},\\mathcal{A} and of the reward function $r:\\mathcal{X}\\times\\mathcal{A}\\to[0,1]$r:\\mathcal{X}\\times\\mathcal{A}\\to[0,1], but the transition probabilities $p(\\cdot|x,a)$p(\\cdot|x,a) can only be accessed through an oracle. In this scenario one is usually concerned with the sample complexity of employing such an oracle. In the classical setting, one has access to an oracle $\\mathcal{C}_p$\\mathcal{C}_p that, on input $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$(x,a)\\in\\mathcal{X}\\times\\mathcal{A}, returns $x'\\in\\mathcal{X}$x'\\in\\mathcal{X} with probability $p(x'|x,a)$p(x'|x,a). In the quantum setting, one has access to an oracle $\\mathcal{O}_p$\\mathcal{O}_p called quantum accessible-environment~\\cite{wang2021quantum,wiedemann2022quantum, jerbi2022quantum, zhong2023provably} which is a unitary operator that acts~as\n\\begin{align}\\label{eq:quantum_sampling_oracle}\n    \\mathcal O_{p}: \\ket{x}\\ket{a}\\ket{\\bar 0}\\rightarrow \\int_{x'\\in\\mathcal X} \\sqrt{p(\\rd x'\\vert x, a)} \\ket{x}\\ket{a}\\ket{x'} \\quad\\text{for all}\\quad (x,a)\\in\\mathcal{X}\\times\\mathcal{A}\n\\end{align}\\begin{align}\\label{eq:quantum_sampling_oracle}\n    \\mathcal O_{p}: \\ket{x}\\ket{a}\\ket{\\bar 0}\\rightarrow \\int_{x'\\in\\mathcal X} \\sqrt{p(\\rd x'\\vert x, a)} \\ket{x}\\ket{a}\\ket{x'} \\quad\\text{for all}\\quad (x,a)\\in\\mathcal{X}\\times\\mathcal{A}\n\\end{align}\\label{eq:quantum_sampling_oracle}\n    \\mathcal O_{p}p: \\ket{x}\\ket{a}\\ket{\\bar 0}\\rightarrow \\int_{x'\\in\\mathcal X}x'\\in\\mathcal X \\sqrt{p(\\rd x'\\vert x, a)} \\ket{x}\\ket{a}\\ket{x'} \\quad\\text{for all}\\quad (x,a)\\in\\mathcal{X}\\times\\mathcal{A}\n\n(we postpone a more formal introduction to quantum computation to \\Cref{sec:quantum_computation}). In this work, we assume $\\mathcal{X}$\\mathcal{X} is compact (without loss of generality in an Euclidean space) and $\\mathcal{A}$\\mathcal{A} is finite with size $A$A. When $\\mathcal{X}$\\mathcal{X} is finite we use $S$S for its size. For continuous state space $\\mathcal{X}$\\mathcal{X}, we will assume that $p$p and $r$r are H\\\"older continuous with parameters $L,\\alpha \\geq 0$L,\\alpha \\geq 0, i.e., $|r(x,a) - r(x',a)| \\leq L\\|x-x'\\|_2^{\\alpha}$|r(x,a) - r(x',a)| \\leq L\\|x-x'\\|_2^{\\alpha}\\alpha and $\\|p(\\cdot|x,a) - p(\\cdot|x',a)\\|_{\\rm tvd} \\leq L\\|x-x'\\|_2^{\\alpha}$\\|p(\\cdot|x,a) - p(\\cdot|x',a)\\|_{\\rm tvd}\\rm tvd \\leq L\\|x-x'\\|_2^{\\alpha}\\alpha for all $(x,x',a)\\in\\mathcal{X}\\times\\mathcal{X}\\times\\mathcal{A}$(x,x',a)\\in\\mathcal{X}\\times\\mathcal{X}\\times\\mathcal{A}. \n\n\\paragraph*{Finite-horizon MDPs.}Finite-horizon MDPs. There is a long list of works that studied the classical query complexity of obtaining optimal policies~\\cite{Kearns1999finite,Kearns2002sparse,GheshlaghiAzar2013,wang2017randomized,Sidford2018variance,sidford2018near,li2020breaking}. For finite-horizon and infinite-horizon \\emph{discounted} MDPs with \\emph{finite} state space, Sidford \\emph{et al.}~\\cite{sidford2018near} and Li \\emph{et al.}~\\cite{li2020breaking} obtained sample-optimal algorithms that output an $\\varepsilon$\\varepsilon-optimal policy using $\\widetilde{O}\\big(\\frac{H^3SA}{\\varepsilon^2} \\big)$\\widetilde{O}\\big(\\frac{H^3SA}{\\varepsilon^2} \\big) queries to $\\mathcal{C}_p$\\mathcal{C}_p, which matches the sample lower bounds from~\\cite{GheshlaghiAzar2013,sidford2018near} up to polylogarithmic factors. In the quantum setting, on the other hand, we are only aware of a few works related to the problem of computing optimal policies~\\cite{wiedemann2022quantum,wang2021quantum,jerbi2022quantum,cherrat2023quantum}, all for infinite-horizon discounted MDPs. More closely related to our paper is the work of Wang \\emph{et al.}~\\cite{wang2021quantum}. The authors quantised the standard value iteration~\\cite{puterman2014markov} and the modern value iteration algorithms from~\\cite{sidford2018near,li2020breaking} using quantum techniques like quantum minimum finding~\\cite{durr1996quantum} and quantum mean estimation~\\cite{brassard2002quantum,montanaro2015quant}. Their final result is a quantum algorithm that outputs an $\\varepsilon$\\varepsilon-optimal policy with quantum query complexity $\\widetilde{O}\\big(\\frac{S}{\\varepsilon}\\min\\{\\Gamma^{3/2}A,\\Gamma^{3}\\sqrt{A}\\} \\big)$\\widetilde{O}\\big(\\frac{S}{\\varepsilon}\\min\\{\\Gamma^{3/2}3/2A,\\Gamma^{3}3\\sqrt{A}\\} \\big), improving the classical query complexity $\\widetilde{O}\\big(\\frac{\\Gamma^3 SA}{\\varepsilon^2}\\big)$\\widetilde{O}\\big(\\frac{\\Gamma^3 SA}{\\varepsilon^2}\\big) from~\\cite{sidford2018near,li2020breaking} ($\\Gamma$\\Gamma is the ``effective horizon'' in infinite-horizon discounted MDPs).\n\nThe problem of computing optimal policies under a quantum generative model for finite-horizon MDPs is, as far as we are aware, still unexplored. As one of our main results, we propose quantum algorithms for computing optimal policies in this setting for both compact and finite state spaces. In the following, $\\mathcal{S}_n \\subset \\mathcal{X}$\\mathcal{S}_n \\subset \\mathcal{X} is a $\\frac{1}{n}$\\frac{1}{n}-net for $\\mathcal{X}$\\mathcal{X} if $\\min_{s\\in\\mathcal{S}_n} \\|x - s\\|_2 \\leq \\frac{1}{n}$\\min_{s\\in\\mathcal{S}_n}s\\in\\mathcal{S}_n \\|x - s\\|_2 \\leq \\frac{1}{n}  $\\forall x\\in\\mathcal{X}$\\forall x\\in\\mathcal{X}. \n\\begin{result}[Finite-horizon]\\label{res:res1}\n    Let $M = \\langle\\mathcal{X},\\mathcal{A},H,p,r\\rangle$ be a finite-horizon MDP and $\\mathcal{S}_n$ a $\\frac{1}{n}$-net for the compact state space $\\mathcal{X}$. Assume that $p$ and $r$ are H\\\"older continuous with parameters $L,\\alpha \\geq 0$. There are classical and quantum algorithms that output an $(\\varepsilon + cH^2Ln^{-\\alpha})$-optimal policy with high probability, where $c>0$ is some constant. The classical algorithm uses $\\widetilde{O}\\big(\\frac{H^3|\\mathcal{S}_n|A}{\\varepsilon^2} \\big)$ classical queries and the quantum algorithm uses $\\widetilde{O}\\big(\\frac{|\\mathcal{S}_n|}{\\varepsilon}\\min\\{HA,H^2\\sqrt{A} \\}\\big)$ quantum queries, where $\\widetilde{O}(\\cdot)$ hides $\\poly\\log$ factors. If $\\mathcal{X}$ is finite with size $S$, then $L=0$ and $|\\mathcal{S}_n| = S$.\n\\end{result}\\begin{result}[Finite-horizon]\\label{res:res1}\n    Let $M = \\langle\\mathcal{X},\\mathcal{A},H,p,r\\rangle$ be a finite-horizon MDP and $\\mathcal{S}_n$ a $\\frac{1}{n}$-net for the compact state space $\\mathcal{X}$. Assume that $p$ and $r$ are H\\\"older continuous with parameters $L,\\alpha \\geq 0$. There are classical and quantum algorithms that output an $(\\varepsilon + cH^2Ln^{-\\alpha})$-optimal policy with high probability, where $c>0$ is some constant. The classical algorithm uses $\\widetilde{O}\\big(\\frac{H^3|\\mathcal{S}_n|A}{\\varepsilon^2} \\big)$ classical queries and the quantum algorithm uses $\\widetilde{O}\\big(\\frac{|\\mathcal{S}_n|}{\\varepsilon}\\min\\{HA,H^2\\sqrt{A} \\}\\big)$ quantum queries, where $\\widetilde{O}(\\cdot)$ hides $\\poly\\log$ factors. If $\\mathcal{X}$ is finite with size $S$, then $L=0$ and $|\\mathcal{S}_n| = S$.\n\\end{result}[Finite-horizon]\\label{res:res1}\n    Let $M = \\langle\\mathcal{X},\\mathcal{A},H,p,r\\rangle$M = \\langle\\mathcal{X},\\mathcal{A},H,p,r\\rangle be a finite-horizon MDP and $\\mathcal{S}_n$\\mathcal{S}_n a $\\frac{1}{n}$\\frac{1}{n}-net for the compact state space $\\mathcal{X}$\\mathcal{X}. Assume that $p$p and $r$r are H\\\"older continuous with parameters $L,\\alpha \\geq 0$L,\\alpha \\geq 0. There are classical and quantum algorithms that output an $(\\varepsilon + cH^2Ln^{-\\alpha})$(\\varepsilon + cH^2Ln^{-\\alpha}-\\alpha)-optimal policy with high probability, where $c>0$c>0 is some constant. The classical algorithm uses $\\widetilde{O}\\big(\\frac{H^3|\\mathcal{S}_n|A}{\\varepsilon^2} \\big)$\\widetilde{O}\\big(\\frac{H^3|\\mathcal{S}_n|A}{\\varepsilon^2} \\big) classical queries and the quantum algorithm uses $\\widetilde{O}\\big(\\frac{|\\mathcal{S}_n|}{\\varepsilon}\\min\\{HA,H^2\\sqrt{A} \\}\\big)$\\widetilde{O}\\big(\\frac{|\\mathcal{S}_n|}{\\varepsilon}\\min\\{HA,H^2\\sqrt{A} \\}\\big) quantum queries, where $\\widetilde{O}(\\cdot)$\\widetilde{O}(\\cdot) hides $\\poly\\log$\\poly\\log factors. If $\\mathcal{X}$\\mathcal{X} is finite with size $S$S, then $L=0$L=0 and $|\\mathcal{S}_n| = S$|\\mathcal{S}_n| = S.\n\nOur quantum algorithms are quantised versions of the standard value iteration~\\cite{puterman2014markov} and the modern value iteration algorithms from~\\cite{sidford2018near,li2020breaking} for finite-horizon MDPs, similarly in spirit to what Wang \\emph{et al.}~\\cite{wang2021quantum} did on infinite-horizon discounted MDPs. In order to achieve the stated query complexity, we employ standard quantum minimum finding~\\cite{durr1996quantum}, the quantum mean estimation subroutine from Kothari and O'Donnell~\\cite{Kothari2023mean}, and its multivariate version due to Cornelissen, Hamoudi, and Jerbi~\\cite{cornelissen2022near}, and Tang~\\cite{tang2025more}.\n\n\\paragraph{Infinite-horizon MDPs.}Infinite-horizon MDPs. Regarding infinite-horizon undiscounted MDPs, the list of classical algorithms for computing optimal policies under a generative model is more recent~\\cite{wang2017primal,jin2020efficiently,jin2021towards,wang2022near,zhang2023sharper,li2024stochastic}, one of the best complexities being $\\widetilde{O}\\big(\\frac{\\Lambda^2 SA}{\\varepsilon^2}\\big)$\\widetilde{O}\\big(\\frac{\\Lambda^2 SA}{\\varepsilon^2}\\big) due to Zhang and Xie~\\cite{zhang2023sharper}, where $\\operatorname{sp}(h^\\ast) := \\max_{x\\in\\mathcal{X}}h^\\ast(x) - \\min_{x\\in\\mathcal{X}}h^\\ast(x) \\leq \\Lambda$\\operatorname{sp}sp(h^\\ast) := \\max_{x\\in\\mathcal{X}}x\\in\\mathcal{X}h^\\ast(x) - \\min_{x\\in\\mathcal{X}}x\\in\\mathcal{X}h^\\ast(x) \\leq \\Lambda is an upper-bound on the span of the optimal bias $h^\\ast(x)$h^\\ast(x), which is a mild assumption used in several RL works~\\cite{bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient}. Here $h^\\pi(x)$h^\\pi(x) is the \\emph{bias function} of a policy $\\pi$\\pi, which informally measures the difference in accumulated rewards when starting at $x\\in\\mathcal{X}$x\\in\\mathcal{X} compared to starting from the stationary distribution (see \\Cref{sec:mdp} for a formal definition). In order to present a quantum algorithm in this setting, we opted to adapt a simple classical value iteration~\\cite{puterman2014markov} to the generative model with oracle $\\mathcal{C}_p$\\mathcal{C}_p and assumed that the underlying infinite-horizon MDP has some contractive properties (see \\Cref{sec:value_iteration}). Our results apply to a broad class of infinite-horizon MDPs called \\emph{weakly communicating} (see \\Cref{sec:mdp} or~\\cite[Section~8.3.1]{puterman2014markov}).\n\\begin{result}[Infinite-horizon]\\label{res:res2}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ be an infinite-horizon (undiscounted) average-reward weakly communicating MDP with $\\operatorname{sp}(h^\\ast) \\leq \\Lambda$ and $\\mathcal{S}_n$ a $\\frac{1}{n}$-net for the compact state space $\\mathcal{X}$. Assume $p$ and $r$ are H\\\"older continuous with parameters $L,\\alpha \\geq 0$ and that $M$ has contractive properties. There are classical and quantum algorithms that output an $(\\varepsilon + c(1+\\Lambda)Ln^{-\\alpha})$-optimal stationary policy with high probability, where $c>0$ is some constant. The classical algorithm uses $\\widetilde{O}\\big(\\frac{\\Lambda^2|\\mathcal{S}_n|A}{\\varepsilon^2}\\big)$ classical queries and the quantum algorithm uses $\\widetilde{O}\\big(\\frac{\\Lambda|\\mathcal{S}_n|\\sqrt{A}}{\\varepsilon} \\big)$ quantum queries, where $\\widetilde{O}(\\cdot)$ hides $\\poly\\log$ factors. If $\\mathcal{X}$ is finite with size $S$, then $L=0$ and $|\\mathcal{S}_n| = S$. \n\\end{result}\\begin{result}[Infinite-horizon]\\label{res:res2}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ be an infinite-horizon (undiscounted) average-reward weakly communicating MDP with $\\operatorname{sp}(h^\\ast) \\leq \\Lambda$ and $\\mathcal{S}_n$ a $\\frac{1}{n}$-net for the compact state space $\\mathcal{X}$. Assume $p$ and $r$ are H\\\"older continuous with parameters $L,\\alpha \\geq 0$ and that $M$ has contractive properties. There are classical and quantum algorithms that output an $(\\varepsilon + c(1+\\Lambda)Ln^{-\\alpha})$-optimal stationary policy with high probability, where $c>0$ is some constant. The classical algorithm uses $\\widetilde{O}\\big(\\frac{\\Lambda^2|\\mathcal{S}_n|A}{\\varepsilon^2}\\big)$ classical queries and the quantum algorithm uses $\\widetilde{O}\\big(\\frac{\\Lambda|\\mathcal{S}_n|\\sqrt{A}}{\\varepsilon} \\big)$ quantum queries, where $\\widetilde{O}(\\cdot)$ hides $\\poly\\log$ factors. If $\\mathcal{X}$ is finite with size $S$, then $L=0$ and $|\\mathcal{S}_n| = S$. \n\\end{result}[Infinite-horizon]\\label{res:res2}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle be an infinite-horizon (undiscounted) average-reward weakly communicating MDP with $\\operatorname{sp}(h^\\ast) \\leq \\Lambda$\\operatorname{sp}sp(h^\\ast) \\leq \\Lambda and $\\mathcal{S}_n$\\mathcal{S}_n a $\\frac{1}{n}$\\frac{1}{n}-net for the compact state space $\\mathcal{X}$\\mathcal{X}. Assume $p$p and $r$r are H\\\"older continuous with parameters $L,\\alpha \\geq 0$L,\\alpha \\geq 0 and that $M$M has contractive properties. There are classical and quantum algorithms that output an $(\\varepsilon + c(1+\\Lambda)Ln^{-\\alpha})$(\\varepsilon + c(1+\\Lambda)Ln^{-\\alpha}-\\alpha)-optimal stationary policy with high probability, where $c>0$c>0 is some constant. The classical algorithm uses $\\widetilde{O}\\big(\\frac{\\Lambda^2|\\mathcal{S}_n|A}{\\varepsilon^2}\\big)$\\widetilde{O}\\big(\\frac{\\Lambda^2|\\mathcal{S}_n|A}{\\varepsilon^2}\\big) classical queries and the quantum algorithm uses $\\widetilde{O}\\big(\\frac{\\Lambda|\\mathcal{S}_n|\\sqrt{A}}{\\varepsilon} \\big)$\\widetilde{O}\\big(\\frac{\\Lambda|\\mathcal{S}_n|\\sqrt{A}}{\\varepsilon} \\big) quantum queries, where $\\widetilde{O}(\\cdot)$\\widetilde{O}(\\cdot) hides $\\poly\\log$\\poly\\log factors. If $\\mathcal{X}$\\mathcal{X} is finite with size $S$S, then $L=0$L=0 and $|\\mathcal{S}_n| = S$|\\mathcal{S}_n| = S. \n\n\n\n\\subsubsection{Online learning of MDPs}\n\nAs another main contribution, we propose improved classical and quantum algorithms for learning unknown finite-horizon and infinite-horizon MDPs in an online fashion. Here one is interested in the performance of the learning algorithm during the learning process, an area of study that finds applications in several topics~\\cite{crammer2003online, ying2006online,liang2006fast,tekin2010online,li2014online, ouyang2017learning,aaronson2018online,lim2022quantum}. Interacting with an unknown MDP can be naturally framed as an online learning problem: at each time step $t\\in\\mathbb{N}$t\\in\\mathbb{N}, the environment is at some state $x_t\\in\\mathcal{X}$x_t\\in\\mathcal{X} and the agent must choose an action $a_t\\sim \\pi_t(\\cdot|x_t)$a_t\\sim \\pi_t(\\cdot|x_t) according to some policy $\\pi = (\\pi_t)_t$\\pi = (\\pi_t)_t in order to receive as large a reward $r(x_t,a_t)$r(x_t,a_t) as possible, after which the environment transitions to a new state $x_{t+1}\\sim p(\\cdot|x_t,a_t)$x_{t+1}t+1\\sim p(\\cdot|x_t,a_t). However, since the agent does not know the underlying transition probabilities $p(\\cdot|x,a)$p(\\cdot|x,a), an exploration vs.\\ exploitation trade-off arises: should the agent explore poorly understood states and actions in order to improve its understanding of the MDP and improve its future performance via better policies, or exploit its current knowledge to optimise short-term rewards~\\cite{Kearns2002near}. In order to deal with such a dilemma, a few general strategies have been proposed~\\cite{strens2000bayesian,brafman2002r,Kearns2002near}. \n\n\n\nThe task of obtaining large rewards is usually reframed as minimising some measure of how far the agent is from being optimal~\\cite{valiant1984theory,Littlestone1988learning,li2008knows,auer2008near}. One of the most famous measures is that of \\emph{regret}, which is the difference between the agent's (expected) rewards compared to that of the optimal policy. For finite-horizon MDPs, the regret over $K$K episodes (or $T=KH$T=KH time steps) is defined as\n\\begin{align*}\n    \\operatorname{Regret}_{H}(T) := \\sum_{k=1}^K \\big(V_1^\\ast(x^{(k)}_1) - V_1^{\\pi^{(k)}}(x^{(k)}_1) \\big),\n\\end{align*}\\begin{align*}\n    \\operatorname{Regret}_{H}(T) := \\sum_{k=1}^K \\big(V_1^\\ast(x^{(k)}_1) - V_1^{\\pi^{(k)}}(x^{(k)}_1) \\big),\n\\end{align*}\n    \\operatorname{Regret}Regret_{H}H(T) := \\sum_{k=1}k=1^K \\big(V_1^\\ast(x^{(k)}(k)_1) - V_1^{\\pi^{(k)}}\\pi^{(k)}(k)(x^{(k)}(k)_1) \\big),\n\nwhere $\\pi^{(k)}$\\pi^{(k)}(k) is the policy used for the $k$k-th episode and $x_1^{(k)}$x_1^{(k)}(k) is the environment's initial state in the $k$k-th episode. For infinite-horizon MDPs, the (in-path) regret over $T$T time steps is defined~as\\footnote{Works like~\\cite{bartlett2009regal} employ instead the regret $Tg^\\ast - \\mathbb{E}[\\sum_{t=1}^T r(x_t,a_t)]$.}\n\\begin{align*}\n    \\operatorname{Regret}_{\\infty}^{\\rm path}(T) := Tg^\\ast - \\sum_{t=1}^T r(x_t,a_t).\n\\end{align*}\\begin{align*}\n    \\operatorname{Regret}_{\\infty}^{\\rm path}(T) := Tg^\\ast - \\sum_{t=1}^T r(x_t,a_t).\n\\end{align*}\n    \\operatorname{Regret}Regret_{\\infty}\\infty^{\\rm path}\\rm path(T) := Tg^\\ast - \\sum_{t=1}t=1^T r(x_t,a_t).\n\n\n\nIn the classical setting, there are several proposed algorithms that achieve regret bounds sublinear in the number of time steps $o(T)$o(T). For finite-horizon finite-state-space MDPs, one of the main early algorithms was proposed by Auer, Jaksch, and Ortner~\\cite{auer2008near}, which achieved a regret of $\\widetilde{O}(\\sqrt{H^2S^2 AT})$\\widetilde{O}(\\sqrt{H^2S^2 AT}).\\footnote{Although \\cite{auer2008near} originally worked in the infinite-horizon setting, it is not hard to adapt their algorithm to the finite-horizon case.} Auer, Jaksch, and Ortner~\\cite{auer2008near} also proved the regret lower bound $\\Omega(\\sqrt{HSAT})$\\Omega(\\sqrt{HSAT}). Since then, several works provided better regret upper bounds~\\cite{osband2014model,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,jin2020provably,yang2020reinforcement}, the most relevant to our paper being the regret bound of Azar, Osband, and Munos~\\cite{azar2017minimax}, which matches the lower bound for a certain range of parameters. This range was later improved by Zanette and Brunskill~\\cite{zanette2019tighter}. We summarise several results from the literature in \\Cref{table:results_finite-horizon}.\n\nFor infinite-horizon MDPs, the landscape is richer. Once again, the algorithm of Auer, Jaksch, and Ortner~\\cite{auer2008near} ranked among the main ones to be first proposed, although it applied to a smaller class of MDPs called  \\emph{communicating}~\\cite{puterman2014markov}. Various works followed since then. Bartlett and Tewari~\\cite{bartlett2009regal} achieved the regret bound $\\widetilde{O}(\\Lambda\\sqrt{S^2AT})$\\widetilde{O}(\\Lambda\\sqrt{S^2AT}) for the broader class of weakly communicating MDPs, where $\\operatorname{sp}(h^\\ast) \\leq \\Lambda$\\operatorname{sp}sp(h^\\ast) \\leq \\Lambda. Later, Ortner and Ryabko~\\cite{ortner2012online} and Lakshmanan, Ortner, and Ryabko~\\cite{lakshmanan2015improved} adapted the algorithm from~\\cite{auer2008near} for the case when the state space is $\\mathcal{X}=[0,1]^D$\\mathcal{X}=[0,1]^D under the assumption that $r$r and $p$p are H\\\"older continuous. The algorithms of~\\cite{bartlett2009regal,ortner2012online,lakshmanan2015improved} are, however, time inefficient, which was later fixed by Fruit \\emph{et al.}~\\cite{fruit2018efficient}, whose time-efficient algorithm still maintains the regret bound $\\widetilde{O}(\\Lambda\\sqrt{S^2AT})$\\widetilde{O}(\\Lambda\\sqrt{S^2AT}) for finite state spaces. Further results from the literature are summarised in \\Cref{table:results_infinite-horizon}.\n\nIn the quantum setting, the number of works on online learning of MDPs is much smaller. We are actually only aware of the works of Zhong \\emph{et al.}~\\cite{zhong2023provably} and Ganguly \\emph{et al.}~\\cite{ganguly2023quantum}, both for finite-horizon finite-state-space MDPs (there are further works on multi-armed bandits~\\cite{Lumbreras2022multiarmedquantum,dai2023quantum,wan2023quantum,su2025quantum}). By leveraging a modified environment-agent interaction model (described below), both works obtained $O(\\poly(S,A,H,\\log{T}))$O(\\poly(S,A,H,\\log{T}T)) regret, exponentially better in the number of steps $T$T. Their precise regret is summarised in \\Cref{table:results_finite-horizon}.\n\nAs our main contribution, we propose improved classical and quantum online learning algorithms for both finite and infinite-horizon MDPs with compact (which include finite) state space. This is achieved by employing a different environment-agent interaction model which borrows elements from~\\cite{zhong2023provably,ganguly2023quantum}. In all the classical works mentioned above~\\cite{auer2008near,azar2017minimax,zanette2019tighter,bartlett2009regal,ortner2012online,lakshmanan2015improved,fruit2018efficient}, the way the agent interacts with the environment and accumulates regret is straightforward: once again, for fear of being repetitious, the agent chooses an action $a_t$a_t at some environment's state $x_t$x_t and obtains a reward $r(x_t,a_t)$r(x_t,a_t), after which the environment's state transitions to $x_{t+1}$x_{t+1}t+1. Generalising such interaction to a quantum setting is not obvious, though. Ideally, we would like to employ the quantum oracle $\\mathcal{O}_p$\\mathcal{O}_p from \\Cref{eq:quantum_sampling_oracle} to explore the MDP in superposition, which inevitably leads to some apparent conundrums. For once, a repeated interaction between agent and environment through quantum oracles will lead to a superposition of different possible rewards. It is then not clear what the final regret is, especially if the agent performs non-trivial intermediary quantum gates and measurements. More critically, however, we would like to employ quantum subroutines~\\cite{brassard2002quantum,montanaro2015quant,cornelissen2022near,Kothari2023mean} that make use of the inverse of $\\mathcal{O}_p$\\mathcal{O}_p. There is absolutely no equivalent inverse operation in the standard classical model and, moreover, one would imagine that ``inverting'' quantum operations could potentially ``undo'' the accumulated regret.\n\n\\paragraph{Our online learning model.}Our online learning model. We solve these issues by virtually separating the exploration phase from the policy learning phase. In the standard classical model, these are perform simultaneously: while interacting with the environment, the agent keeps track of all state-action pairs $(x_t,a_t)$(x_t,a_t) observed so far to come up with an estimation $\\widetilde{p}$\\widetilde{p} of the true transition probability, which is then used to obtain an approximate optimal policy. In our model, however, such interaction is split into \\emph{two} types of phases: \\emph{classical exploration} phases and \\emph{quantum generative} phases.\n\\begin{enumerate}\n    \\item \\textbf{Exploration phase:} an exploration phase corresponds exactly to the standard classical agent-environment interaction, during which the agent accumulates regret. More specifically, the agent chooses action $a_t$ at state $x_t$, obtains reward $r(x_t,a_t)$, and observes the new state $x_{t+1}$. The interaction is completely classical and lasts as long as the agent desires.\n    \\item \\textbf{Generative phase:} during the generative phase, the agent is \\emph{free} to interact with the environment using quantum accessible-environments, more specifically $\\mathcal{O}_p$, \\emph{without} accumulating regret. This means that the agent is free to prepare any quantum state and have the environment apply $\\mathcal{O}_p$ (or its inverse) onto such quantum state, plus any quantum gate from a universal gate set~\\cite{nielsen2010quantum}. The transition from exploration to generative phase and vice versa can be done at any moment. During the generative phase, the agent can \\emph{only use the oracle $\\mathcal{O}_p$ at most $O(\\tau)$ times,} where $\\tau$ is length of the previous \\emph{exploration phase}.\n\\end{enumerate}\\begin{enumerate}\n    \\item \\textbf{Exploration phase:} an exploration phase corresponds exactly to the standard classical agent-environment interaction, during which the agent accumulates regret. More specifically, the agent chooses action $a_t$ at state $x_t$, obtains reward $r(x_t,a_t)$, and observes the new state $x_{t+1}$. The interaction is completely classical and lasts as long as the agent desires.\n    \\item \\textbf{Generative phase:} during the generative phase, the agent is \\emph{free} to interact with the environment using quantum accessible-environments, more specifically $\\mathcal{O}_p$, \\emph{without} accumulating regret. This means that the agent is free to prepare any quantum state and have the environment apply $\\mathcal{O}_p$ (or its inverse) onto such quantum state, plus any quantum gate from a universal gate set~\\cite{nielsen2010quantum}. The transition from exploration to generative phase and vice versa can be done at any moment. During the generative phase, the agent can \\emph{only use the oracle $\\mathcal{O}_p$ at most $O(\\tau)$ times,} where $\\tau$ is length of the previous \\emph{exploration phase}.\n\\end{enumerate}\n    \\item \\textbf{Exploration phase:} an exploration phase corresponds exactly to the standard classical agent-environment interaction, during which the agent accumulates regret. More specifically, the agent chooses action $a_t$a_t at state $x_t$x_t, obtains reward $r(x_t,a_t)$r(x_t,a_t), and observes the new state $x_{t+1}$x_{t+1}t+1. The interaction is completely classical and lasts as long as the agent desires.\n    \\item \\textbf{Generative phase:} during the generative phase, the agent is \\emph{free} to interact with the environment using quantum accessible-environments, more specifically $\\mathcal{O}_p$\\mathcal{O}_p, \\emph{without} accumulating regret. This means that the agent is free to prepare any quantum state and have the environment apply $\\mathcal{O}_p$\\mathcal{O}_p (or its inverse) onto such quantum state, plus any quantum gate from a universal gate set~\\cite{nielsen2010quantum}. The transition from exploration to generative phase and vice versa can be done at any moment. During the generative phase, the agent can \\emph{only use the oracle $\\mathcal{O}_p$ at most $O(\\tau)$ times,} where $\\tau$\\tau is length of the previous \\emph{exploration phase}.\n\n\nBy separating the accumulation of regret from the policy learning phase as above, all the problems previously highlighted are avoided. The result is an alternation between exploration and generative phases. We name the free interaction phase ``generative'' because the agent is free to interact with the environment in a generative fashion and, ultimately, we shall employ the algorithms from \\Cref{res:res1,res:res2} to compute an $\\varepsilon$\\varepsilon-optimal policy during this phase. The restriction on the number of applications of $\\mathcal{O}_p$\\mathcal{O}_p during the generative phase is vital to guarantee a meaningful online-learning model, otherwise the agent could obtain a policy as close to optimal as needed and thus incur a regret as small as desired. In order to freely use the probability oracle $\\mathcal{O}_p$\\mathcal{O}_p, the agent must first pay a price in committing to actions and incurring into regret. Finally, we also consider a classical version of the above model wherein, during generative phases, the agent has access to a sampling oracle $\\mathcal{C}_p$\\mathcal{C}_p that returns $x'\\sim p(\\cdot|x,a)$x'\\sim p(\\cdot|x,a) on input $(x,a)$(x,a) (see \\Cref{sec:RL_model} for a more detail explanation on our online learning model).\n\n\n\nSeveral RL algorithms~\\cite{auer2008near,bartlett2009regal,ortner2012online,lakshmanan2015improved,azar2017minimax,jin2018qlearning,zanette2019tighter,efroni2019tight,zhong2023provably,ganguly2023quantum}, implement the principle of \u201coptimism in the face of uncertainty\u201d~\\cite{brafman2002r}. In these algorithms, each state-action pair is given some ``optimism'' such that its imagined value is as high as statistically possible and the agent chooses a policy according to such optimistic values. In other words, the estimated transition probability $\\widetilde{p}$\\widetilde{p}, together with its uncertainty, define a set of plausible MDPs $\\mathcal{M}$\\mathcal{M} that contains the true MDP $M$M with high probability. The agent then chooses an optimal policy with respect to \\emph{all} MDPs in $\\mathcal{M}$\\mathcal{M} (optimism). Our classical and quantum algorithms, on the other hand, manage to avoid this principle altogether since, during the generative phases, we have access to the true MDP via the classical or quantum sampling oracles $\\mathcal{C}_p$\\mathcal{C}_p or $\\mathcal{O}_p$\\mathcal{O}_p. Ultimately, an optimal policy is what is needed to maintain a small regret. Therefore, there is no reason to approximate the transition probabilities $p$p and the agent instead directly computes an approximate optimal policy during the generative phase via the algorithms from \\Cref{res:res1,res:res2}. If the agent would like to eventually learn $p$p, he or she can do so later on when a policy close to optimal has already been obtained.\nAs a consequence, our algorithms achieve better regret bounds in most cases (see \\Cref{table:results_finite-horizon,table:results_infinite-horizon}).\n\\begin{result}[Finite-horizon regret]\\label{res:res3}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},H,p,r\\rangle$ be a finite-horizon MDP. If $\\mathcal{X}$ is finite with size $S$, there are classical and quantum algorithms with regret $\\operatorname{Regret}_H(T)$ upper-bounded after $T$ steps, with high probability, by\n    %\n    \\begin{align*}\n        \\textbf{Classical:}&~ \\widetilde{O}\\big(\\sqrt{HSAT\\log(HSA)} \\big),\\\\\n        \\textbf{Quantum:}&~ \\widetilde{O}\\big(S\\min\\{HA,H^2\\sqrt{A}\\log(HS)\\}\\log(T/H)\\log(HSA)\\big),\n    \\end{align*}\n    %\n    where $\\widetilde{O}(\\cdot)$ hides $\\poly\\log\\log$ terms. If $\\mathcal{X} = [0,1]^D$, assume that $p$ and $r$ are H\\\"older continuous with parameters $L,\\alpha \\geq 0$, $L$ constant. There are classical and quantum algorithms with regret $\\operatorname{Regret}_H(T)$ upper-bounded after $T$ steps, with high probability, by\n    %\n    \\vspace{-0.1cm}\n    \\begin{align*}\n        \\textbf{Classical:}&~ \\widetilde{O}\\big(T^{\\frac{D+\\alpha}{D+2\\alpha}} ( H + \\sqrt{HA\\log(HAT)} \\big),\\\\\n        \\textbf{Quantum:}&~ \\widetilde{O}\\big(\\min\\{HA,H^2\\sqrt{A}\\log(HT)\\}T^{\\frac{D}{D+\\alpha}} \\log(T/H) \\log(HAT) \\big).\n    \\end{align*}\n    %\n    %where $\\widetilde{O}(\\cdot)$ hides $\\poly\\log\\log$ terms.\n\\end{result}\\begin{result}[Finite-horizon regret]\\label{res:res3}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},H,p,r\\rangle$ be a finite-horizon MDP. If $\\mathcal{X}$ is finite with size $S$, there are classical and quantum algorithms with regret $\\operatorname{Regret}_H(T)$ upper-bounded after $T$ steps, with high probability, by\n    %\n    \\begin{align*}\n        \\textbf{Classical:}&~ \\widetilde{O}\\big(\\sqrt{HSAT\\log(HSA)} \\big),\\\\\n        \\textbf{Quantum:}&~ \\widetilde{O}\\big(S\\min\\{HA,H^2\\sqrt{A}\\log(HS)\\}\\log(T/H)\\log(HSA)\\big),\n    \\end{align*}\n    %\n    where $\\widetilde{O}(\\cdot)$ hides $\\poly\\log\\log$ terms. If $\\mathcal{X} = [0,1]^D$, assume that $p$ and $r$ are H\\\"older continuous with parameters $L,\\alpha \\geq 0$, $L$ constant. There are classical and quantum algorithms with regret $\\operatorname{Regret}_H(T)$ upper-bounded after $T$ steps, with high probability, by\n    %\n    \\vspace{-0.1cm}\n    \\begin{align*}\n        \\textbf{Classical:}&~ \\widetilde{O}\\big(T^{\\frac{D+\\alpha}{D+2\\alpha}} ( H + \\sqrt{HA\\log(HAT)} \\big),\\\\\n        \\textbf{Quantum:}&~ \\widetilde{O}\\big(\\min\\{HA,H^2\\sqrt{A}\\log(HT)\\}T^{\\frac{D}{D+\\alpha}} \\log(T/H) \\log(HAT) \\big).\n    \\end{align*}\n    %\n    %where $\\widetilde{O}(\\cdot)$ hides $\\poly\\log\\log$ terms.\n\\end{result}[Finite-horizon regret]\\label{res:res3}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},H,p,r\\rangle$M = \\langle \\mathcal{X},\\mathcal{A},H,p,r\\rangle be a finite-horizon MDP. If $\\mathcal{X}$\\mathcal{X} is finite with size $S$S, there are classical and quantum algorithms with regret $\\operatorname{Regret}_H(T)$\\operatorname{Regret}Regret_H(T) upper-bounded after $T$T steps, with high probability, by\n    \n        \\textbf{Classical:}&~ \\widetilde{O}\\big(\\sqrt{HSAT\\log(HSA)} \\big),\\\\\n        \\textbf{Quantum:}&~ \\widetilde{O}\\big(S\\min\\{HA,H^2\\sqrt{A}\\log(HS)\\}\\log(T/H)\\log(HSA)\\big),\n    \n    where $\\widetilde{O}(\\cdot)$\\widetilde{O}(\\cdot) hides $\\poly\\log\\log$\\poly\\log\\log terms. If $\\mathcal{X} = [0,1]^D$\\mathcal{X} = [0,1]^D, assume that $p$p and $r$r are H\\\"older continuous with parameters $L,\\alpha \\geq 0$L,\\alpha \\geq 0, $L$L constant. There are classical and quantum algorithms with regret $\\operatorname{Regret}_H(T)$\\operatorname{Regret}Regret_H(T) upper-bounded after $T$T steps, with high probability, by\n    \\vspace{-0.1cm}\n    \n        \\textbf{Classical:}&~ \\widetilde{O}\\big(T^{\\frac{D+\\alpha}{D+2\\alpha}}\\frac{D+\\alpha}{D+2\\alpha} ( H + \\sqrt{HA\\log(HAT)} \\big),\\\\\n        \\textbf{Quantum:}&~ \\widetilde{O}\\big(\\min\\{HA,H^2\\sqrt{A}\\log(HT)\\}T^{\\frac{D}{D+\\alpha}}\\frac{D}{D+\\alpha} \\log(T/H) \\log(HAT) \\big).\n    \n    \n\n\\begin{result}[Infinite-horizon in-path regret]\\label{res:res4}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ be an infinite-horizon (undiscounted) average-reward weakly communicating MDP with $\\operatorname{sp}(h^\\ast) \\leq \\Lambda$. If $\\mathcal{X}$ is finite with size $S$, there are classical and quantum algorithms with in-path regret $\\operatorname{Regret}_{\\infty}^{\\rm path}(T)$ upper-bounded after $T$ steps, with high probability, by\n    %\n    \\begin{align*}\n        \\textbf{Classical:}&~ \\widetilde{O}\\big(\\Lambda\\sqrt{SAT\\log{T}\\log(SAT)} \\big), \\\\\n        \\textbf{Quantum:}&~ \\widetilde{O}\\big(\\Lambda\\sqrt{T\\log{T}} + \\Lambda S\\sqrt{A}\\log^2{T}\\log(SAT)\\log(ST)\\big),\n    \\end{align*}\n    %\n    where $\\widetilde{O}(\\cdot)$ hides $\\poly\\log\\log$ terms. If $\\mathcal{X} = [0,1]^D$, assume that $p$ and $r$ are H\\\"older continuous with parameters $L,\\alpha \\geq 0$, $L$ constant. There are classical and quantum algorithms with in-path regret $\\operatorname{Regret}_{\\infty}^{\\rm path}(T)$ upper-bounded after $T$ steps, with high probability, by\n    %\n    \\vspace{-0.1cm}\n    \\begin{align*}\n        \\textbf{Classical:}&~ \\widetilde{O}\\big(\\Lambda T^{\\frac{D+\\alpha}{D+2\\alpha}} \\sqrt{A\\log{T}\\log(AT)} \\big),\\\\\n        \\textbf{Quantum:}&~ \\widetilde{O}\\big(\\Lambda\\sqrt{T\\log{T}} + \\Lambda \\sqrt{A} T^{\\frac{D}{D+\\alpha}}\\log^3{T}\\log(AT) \\big).\n    \\end{align*}\n    %\n   % where $\\widetilde{O}(\\cdot)$ hides $\\poly\\log\\log$ terms.\n\\end{result}\\begin{result}[Infinite-horizon in-path regret]\\label{res:res4}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ be an infinite-horizon (undiscounted) average-reward weakly communicating MDP with $\\operatorname{sp}(h^\\ast) \\leq \\Lambda$. If $\\mathcal{X}$ is finite with size $S$, there are classical and quantum algorithms with in-path regret $\\operatorname{Regret}_{\\infty}^{\\rm path}(T)$ upper-bounded after $T$ steps, with high probability, by\n    %\n    \\begin{align*}\n        \\textbf{Classical:}&~ \\widetilde{O}\\big(\\Lambda\\sqrt{SAT\\log{T}\\log(SAT)} \\big), \\\\\n        \\textbf{Quantum:}&~ \\widetilde{O}\\big(\\Lambda\\sqrt{T\\log{T}} + \\Lambda S\\sqrt{A}\\log^2{T}\\log(SAT)\\log(ST)\\big),\n    \\end{align*}\n    %\n    where $\\widetilde{O}(\\cdot)$ hides $\\poly\\log\\log$ terms. If $\\mathcal{X} = [0,1]^D$, assume that $p$ and $r$ are H\\\"older continuous with parameters $L,\\alpha \\geq 0$, $L$ constant. There are classical and quantum algorithms with in-path regret $\\operatorname{Regret}_{\\infty}^{\\rm path}(T)$ upper-bounded after $T$ steps, with high probability, by\n    %\n    \\vspace{-0.1cm}\n    \\begin{align*}\n        \\textbf{Classical:}&~ \\widetilde{O}\\big(\\Lambda T^{\\frac{D+\\alpha}{D+2\\alpha}} \\sqrt{A\\log{T}\\log(AT)} \\big),\\\\\n        \\textbf{Quantum:}&~ \\widetilde{O}\\big(\\Lambda\\sqrt{T\\log{T}} + \\Lambda \\sqrt{A} T^{\\frac{D}{D+\\alpha}}\\log^3{T}\\log(AT) \\big).\n    \\end{align*}\n    %\n   % where $\\widetilde{O}(\\cdot)$ hides $\\poly\\log\\log$ terms.\n\\end{result}[Infinite-horizon in-path regret]\\label{res:res4}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle be an infinite-horizon (undiscounted) average-reward weakly communicating MDP with $\\operatorname{sp}(h^\\ast) \\leq \\Lambda$\\operatorname{sp}sp(h^\\ast) \\leq \\Lambda. If $\\mathcal{X}$\\mathcal{X} is finite with size $S$S, there are classical and quantum algorithms with in-path regret $\\operatorname{Regret}_{\\infty}^{\\rm path}(T)$\\operatorname{Regret}Regret_{\\infty}\\infty^{\\rm path}\\rm path(T) upper-bounded after $T$T steps, with high probability, by\n    \n        \\textbf{Classical:}&~ \\widetilde{O}\\big(\\Lambda\\sqrt{SAT\\log{T}\\log(SAT)} \\big), \\\\\n        \\textbf{Quantum:}&~ \\widetilde{O}\\big(\\Lambda\\sqrt{T\\log{T}} + \\Lambda S\\sqrt{A}\\log^2{T}T\\log(SAT)\\log(ST)\\big),\n    \n    where $\\widetilde{O}(\\cdot)$\\widetilde{O}(\\cdot) hides $\\poly\\log\\log$\\poly\\log\\log terms. If $\\mathcal{X} = [0,1]^D$\\mathcal{X} = [0,1]^D, assume that $p$p and $r$r are H\\\"older continuous with parameters $L,\\alpha \\geq 0$L,\\alpha \\geq 0, $L$L constant. There are classical and quantum algorithms with in-path regret $\\operatorname{Regret}_{\\infty}^{\\rm path}(T)$\\operatorname{Regret}Regret_{\\infty}\\infty^{\\rm path}\\rm path(T) upper-bounded after $T$T steps, with high probability, by\n    \\vspace{-0.1cm}\n    \n        \\textbf{Classical:}&~ \\widetilde{O}\\big(\\Lambda T^{\\frac{D+\\alpha}{D+2\\alpha}}\\frac{D+\\alpha}{D+2\\alpha} \\sqrt{A\\log{T}\\log(AT)} \\big),\\\\\n        \\textbf{Quantum:}&~ \\widetilde{O}\\big(\\Lambda\\sqrt{T\\log{T}} + \\Lambda \\sqrt{A} T^{\\frac{D}{D+\\alpha}}\\frac{D}{D+\\alpha}\\log^3{T}T\\log(AT) \\big).\n    \n    \n\n\\begin{table}[t]\n\\centering\n\\def\\arraystretch{1.55}\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{|c|cc|cc|}\n\\hline\n\nWork /  & \\multicolumn{2}{c|}{Finite state space $\\mathcal{X}$ with size $S$} & \\multicolumn{2}{c|}{Compact state space $\\mathcal{X}=[0,1]^D$} \\\\ \n\\cline{2-5} \n$\\operatorname{Regret}_H(T)$ & \\multicolumn{1}{c|}{Regret (classical)} & Regret (quantum) & \\multicolumn{1}{c|}{Regret (classical)} & Regret (quantum) \\\\ \\hline \n\n\\cite{auer2008near} & \\multicolumn{1}{c|}{$\\sqrt{H^2S^2AT} + HSA$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{bartlett2009regal} & \\multicolumn{1}{c|}{$\\sqrt{H^2S^2AT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{ortner2012online} & \\multicolumn{1}{c|}{-} & - & \\multicolumn{1}{c|}{$\\sqrt{H^2 A}T^{\\frac{2D+\\alpha}{2D+2\\alpha}}$} & - \\\\ \\hline\n\n\\cite{lakshmanan2015improved} & \\multicolumn{1}{c|}{-} & - & \\multicolumn{1}{c|}{$\\sqrt{H^2 A}T^{\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha}}$} & - \\\\ \\hline\n\n\\cite{azar2017minimax} & \\multicolumn{1}{c|}{$\\sqrt{HSAT} + H^2S^2A + \\sqrt{H^2T}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n%\\cite{agrawal2017optimistic} & \\multicolumn{1}{c|}{$\\sqrt{H^2SAT} + H(S^7A^3T)^{\\frac{1}{4}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\multirow{2}{*}{\\cite{jin2018qlearning}} & \\multicolumn{1}{c|}{$\\sqrt{H^4SAT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \n\n& \\multicolumn{1}{c|}{$\\sqrt{H^3SAT} + \\sqrt{H^9S^3A^3}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{zanette2019tighter} & \\multicolumn{1}{c|}{$\\sqrt{HSAT} + H^2 S^{\\frac{3}{2}} A(\\sqrt{S} + \\sqrt{H})$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{efroni2019tight} & \\multicolumn{1}{c|}{$\\sqrt{HSAT} + H^2 S^{\\frac{3}{2}} A(\\sqrt{S} + \\sqrt{H})$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{bai2019provably} & \\multicolumn{1}{c|}{$\\sqrt{H^3SAT} + \\sqrt{H^9S^3A^3}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{zhang2020almost} & \\multicolumn{1}{c|}{$\\sqrt{H^2SAT} + H^8 S^2 A^{\\frac{3}{2}} T^{\\frac{1}{4}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{menard2021ucb} & \\multicolumn{1}{c|}{$\\sqrt{H^2SAT} + H^4 S A$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{li2021breaking} & \\multicolumn{1}{c|}{$\\sqrt{H^2SAT} + H^6 S A$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{zhong2023provably} & \\multicolumn{1}{c|}{-} & $H^3S^2A$ & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{ganguly2023quantum} & \\multicolumn{1}{c|}{-} & $H^2S^2A$ & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{agrawal2024optimistic} & \\multicolumn{1}{c|}{$\\sqrt{H^{12}S^2 AT} + H^9 S^2 A$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\rowcolor{Gray}\nThis work & \\multicolumn{1}{c|}{$\\sqrt{HSAT}$} & $\\min\\{HSA,H^2S\\sqrt{A}\\}$ & \\multicolumn{1}{c|}{$(H+\\sqrt{HA})T^{\\frac{D+\\alpha}{D+2\\alpha}}$} & $\\min\\{HA,H^2\\sqrt{A}\\}T^{\\frac{D}{D+\\alpha}}$ \\\\ \\hline\n\n\\end{tabular}}\n\\caption{Summary of several known upper bounds for the regret $\\operatorname{Regret}_H(T)$ of \\emph{finite-horizon} MDPs $\\langle \\mathcal{X},\\mathcal{A},H,p,r\\rangle$ with state space $\\mathcal{X}$, action space $\\mathcal{A}$ with size $A$, horizon $H$, and number of time steps $T$. The state space $\\mathcal{X}$ can be finite with size $S$ or $\\mathcal{X} = [0,1]^D$, in which case $p$ and $r$ are assumed to be H\\\"older continuous with exponent $\\alpha \\geq 0$. All bounds are up to $\\poly\\log$ factors in $A$, $H$, $T$, and $S$ (for $\\mathcal{X}$ finite). The reader should keep in mind that not all works assume the same RL model: there are model-based~\\cite{auer2008near} and model-free~\\cite{jin2018qlearning} approaches, while~\\cite{zhong2023provably,ganguly2023quantum} and our work assume a hybrid generative model. Some works focus on optimising other parameters besides regret, e.g., space and/or time.}\n\\label{table:results_finite-horizon}\n\\end{table}\n\\centering\n\\def\\arraystretch{1.55}1.55\n\\resizebox{\\linewidth}\\linewidth{!}!{\n\\begin{tabular}{|c|cc|cc|}\n\\hline\n\nWork /  & \\multicolumn{2}{c|}{Finite state space $\\mathcal{X}$ with size $S$} & \\multicolumn{2}{c|}{Compact state space $\\mathcal{X}=[0,1]^D$} \\\\ \n\\cline{2-5} \n$\\operatorname{Regret}_H(T)$ & \\multicolumn{1}{c|}{Regret (classical)} & Regret (quantum) & \\multicolumn{1}{c|}{Regret (classical)} & Regret (quantum) \\\\ \\hline \n\n\\cite{auer2008near} & \\multicolumn{1}{c|}{$\\sqrt{H^2S^2AT} + HSA$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{bartlett2009regal} & \\multicolumn{1}{c|}{$\\sqrt{H^2S^2AT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{ortner2012online} & \\multicolumn{1}{c|}{-} & - & \\multicolumn{1}{c|}{$\\sqrt{H^2 A}T^{\\frac{2D+\\alpha}{2D+2\\alpha}}$} & - \\\\ \\hline\n\n\\cite{lakshmanan2015improved} & \\multicolumn{1}{c|}{-} & - & \\multicolumn{1}{c|}{$\\sqrt{H^2 A}T^{\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha}}$} & - \\\\ \\hline\n\n\\cite{azar2017minimax} & \\multicolumn{1}{c|}{$\\sqrt{HSAT} + H^2S^2A + \\sqrt{H^2T}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n%\\cite{agrawal2017optimistic} & \\multicolumn{1}{c|}{$\\sqrt{H^2SAT} + H(S^7A^3T)^{\\frac{1}{4}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\multirow{2}{*}{\\cite{jin2018qlearning}} & \\multicolumn{1}{c|}{$\\sqrt{H^4SAT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \n\n& \\multicolumn{1}{c|}{$\\sqrt{H^3SAT} + \\sqrt{H^9S^3A^3}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{zanette2019tighter} & \\multicolumn{1}{c|}{$\\sqrt{HSAT} + H^2 S^{\\frac{3}{2}} A(\\sqrt{S} + \\sqrt{H})$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{efroni2019tight} & \\multicolumn{1}{c|}{$\\sqrt{HSAT} + H^2 S^{\\frac{3}{2}} A(\\sqrt{S} + \\sqrt{H})$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{bai2019provably} & \\multicolumn{1}{c|}{$\\sqrt{H^3SAT} + \\sqrt{H^9S^3A^3}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{zhang2020almost} & \\multicolumn{1}{c|}{$\\sqrt{H^2SAT} + H^8 S^2 A^{\\frac{3}{2}} T^{\\frac{1}{4}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{menard2021ucb} & \\multicolumn{1}{c|}{$\\sqrt{H^2SAT} + H^4 S A$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{li2021breaking} & \\multicolumn{1}{c|}{$\\sqrt{H^2SAT} + H^6 S A$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{zhong2023provably} & \\multicolumn{1}{c|}{-} & $H^3S^2A$ & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{ganguly2023quantum} & \\multicolumn{1}{c|}{-} & $H^2S^2A$ & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{agrawal2024optimistic} & \\multicolumn{1}{c|}{$\\sqrt{H^{12}S^2 AT} + H^9 S^2 A$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\rowcolor{Gray}\nThis work & \\multicolumn{1}{c|}{$\\sqrt{HSAT}$} & $\\min\\{HSA,H^2S\\sqrt{A}\\}$ & \\multicolumn{1}{c|}{$(H+\\sqrt{HA})T^{\\frac{D+\\alpha}{D+2\\alpha}}$} & $\\min\\{HA,H^2\\sqrt{A}\\}T^{\\frac{D}{D+\\alpha}}$ \\\\ \\hline\n\n\\end{tabular}}\n\n\\hline\n\nWork /  & \\multicolumn{2}2{c|}c|{Finite state space $\\mathcal{X}$ with size $S$}Finite state space $\\mathcal{X}$\\mathcal{X} with size $S$S & \\multicolumn{2}2{c|}c|{Compact state space $\\mathcal{X}=[0,1]^D$}Compact state space $\\mathcal{X}=[0,1]^D$\\mathcal{X}=[0,1]^D \\\\ \n\\cline{2-5}2-5 \n$\\operatorname{Regret}_H(T)$\\operatorname{Regret}Regret_H(T) & \\multicolumn{1}1{c|}c|{Regret (classical)}Regret (classical) & Regret (quantum) & \\multicolumn{1}1{c|}c|{Regret (classical)}Regret (classical) & Regret (quantum) \\\\ \\hline \n\n\\cite{auer2008near} & \\multicolumn{1}1{c|}c|{$\\sqrt{H^2S^2AT} + HSA$}$\\sqrt{H^2S^2AT} + HSA$\\sqrt{H^2S^2AT} + HSA & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\cite{bartlett2009regal} & \\multicolumn{1}1{c|}c|{$\\sqrt{H^2S^2AT}$}$\\sqrt{H^2S^2AT}$\\sqrt{H^2S^2AT} & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\cite{ortner2012online} & \\multicolumn{1}1{c|}c|{-}- & - & \\multicolumn{1}1{c|}c|{$\\sqrt{H^2 A}T^{\\frac{2D+\\alpha}{2D+2\\alpha}}$}$\\sqrt{H^2 A}T^{\\frac{2D+\\alpha}{2D+2\\alpha}}$\\sqrt{H^2 A}T^{\\frac{2D+\\alpha}{2D+2\\alpha}}\\frac{2D+\\alpha}{2D+2\\alpha} & - \\\\ \\hline\n\n\\cite{lakshmanan2015improved} & \\multicolumn{1}1{c|}c|{-}- & - & \\multicolumn{1}1{c|}c|{$\\sqrt{H^2 A}T^{\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha}}$}$\\sqrt{H^2 A}T^{\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha}}$\\sqrt{H^2 A}T^{\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha}}\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha} & - \\\\ \\hline\n\n\\cite{azar2017minimax} & \\multicolumn{1}1{c|}c|{$\\sqrt{HSAT} + H^2S^2A + \\sqrt{H^2T}$}$\\sqrt{HSAT} + H^2S^2A + \\sqrt{H^2T}$\\sqrt{HSAT} + H^2S^2A + \\sqrt{H^2T} & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\n\n\\multirow{2}2{*}*{\\cite{jin2018qlearning}}\\cite{jin2018qlearning} & \\multicolumn{1}1{c|}c|{$\\sqrt{H^4SAT}$}$\\sqrt{H^4SAT}$\\sqrt{H^4SAT} & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \n\n& \\multicolumn{1}1{c|}c|{$\\sqrt{H^3SAT} + \\sqrt{H^9S^3A^3}$}$\\sqrt{H^3SAT} + \\sqrt{H^9S^3A^3}$\\sqrt{H^3SAT} + \\sqrt{H^9S^3A^3} & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\cite{zanette2019tighter} & \\multicolumn{1}1{c|}c|{$\\sqrt{HSAT} + H^2 S^{\\frac{3}{2}} A(\\sqrt{S} + \\sqrt{H})$}$\\sqrt{HSAT} + H^2 S^{\\frac{3}{2}} A(\\sqrt{S} + \\sqrt{H})$\\sqrt{HSAT} + H^2 S^{\\frac{3}{2}}\\frac{3}{2} A(\\sqrt{S} + \\sqrt{H}) & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\cite{efroni2019tight} & \\multicolumn{1}1{c|}c|{$\\sqrt{HSAT} + H^2 S^{\\frac{3}{2}} A(\\sqrt{S} + \\sqrt{H})$}$\\sqrt{HSAT} + H^2 S^{\\frac{3}{2}} A(\\sqrt{S} + \\sqrt{H})$\\sqrt{HSAT} + H^2 S^{\\frac{3}{2}}\\frac{3}{2} A(\\sqrt{S} + \\sqrt{H}) & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\cite{bai2019provably} & \\multicolumn{1}1{c|}c|{$\\sqrt{H^3SAT} + \\sqrt{H^9S^3A^3}$}$\\sqrt{H^3SAT} + \\sqrt{H^9S^3A^3}$\\sqrt{H^3SAT} + \\sqrt{H^9S^3A^3} & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\cite{zhang2020almost} & \\multicolumn{1}1{c|}c|{$\\sqrt{H^2SAT} + H^8 S^2 A^{\\frac{3}{2}} T^{\\frac{1}{4}}$}$\\sqrt{H^2SAT} + H^8 S^2 A^{\\frac{3}{2}} T^{\\frac{1}{4}}$\\sqrt{H^2SAT} + H^8 S^2 A^{\\frac{3}{2}}\\frac{3}{2} T^{\\frac{1}{4}}\\frac{1}{4} & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\cite{menard2021ucb} & \\multicolumn{1}1{c|}c|{$\\sqrt{H^2SAT} + H^4 S A$}$\\sqrt{H^2SAT} + H^4 S A$\\sqrt{H^2SAT} + H^4 S A & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\cite{li2021breaking} & \\multicolumn{1}1{c|}c|{$\\sqrt{H^2SAT} + H^6 S A$}$\\sqrt{H^2SAT} + H^6 S A$\\sqrt{H^2SAT} + H^6 S A & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\cite{zhong2023provably} & \\multicolumn{1}1{c|}c|{-}- & $H^3S^2A$H^3S^2A & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\cite{ganguly2023quantum} & \\multicolumn{1}1{c|}c|{-}- & $H^2S^2A$H^2S^2A & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\cite{agrawal2024optimistic} & \\multicolumn{1}1{c|}c|{$\\sqrt{H^{12}S^2 AT} + H^9 S^2 A$}$\\sqrt{H^{12}S^2 AT} + H^9 S^2 A$\\sqrt{H^{12}S^2 AT} + H^9 S^2 A & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\rowcolor{Gray}Gray\nThis work & \\multicolumn{1}1{c|}c|{$\\sqrt{HSAT}$}$\\sqrt{HSAT}$\\sqrt{HSAT} & $\\min\\{HSA,H^2S\\sqrt{A}\\}$\\min\\{HSA,H^2S\\sqrt{A}\\} & \\multicolumn{1}1{c|}c|{$(H+\\sqrt{HA})T^{\\frac{D+\\alpha}{D+2\\alpha}}$}$(H+\\sqrt{HA})T^{\\frac{D+\\alpha}{D+2\\alpha}}$(H+\\sqrt{HA})T^{\\frac{D+\\alpha}{D+2\\alpha}}\\frac{D+\\alpha}{D+2\\alpha} & $\\min\\{HA,H^2\\sqrt{A}\\}T^{\\frac{D}{D+\\alpha}}$\\min\\{HA,H^2\\sqrt{A}\\}T^{\\frac{D}{D+\\alpha}}\\frac{D}{D+\\alpha} \\\\ \\hline\n\n\n\\caption{Summary of several known upper bounds for the regret $\\operatorname{Regret}_H(T)$ of \\emph{finite-horizon} MDPs $\\langle \\mathcal{X},\\mathcal{A},H,p,r\\rangle$ with state space $\\mathcal{X}$, action space $\\mathcal{A}$ with size $A$, horizon $H$, and number of time steps $T$. The state space $\\mathcal{X}$ can be finite with size $S$ or $\\mathcal{X} = [0,1]^D$, in which case $p$ and $r$ are assumed to be H\\\"older continuous with exponent $\\alpha \\geq 0$. All bounds are up to $\\poly\\log$ factors in $A$, $H$, $T$, and $S$ (for $\\mathcal{X}$ finite). The reader should keep in mind that not all works assume the same RL model: there are model-based~\\cite{auer2008near} and model-free~\\cite{jin2018qlearning} approaches, while~\\cite{zhong2023provably,ganguly2023quantum} and our work assume a hybrid generative model. Some works focus on optimising other parameters besides regret, e.g., space and/or time.}\n\\label{table:results_finite-horizon}\n\n\nSeveral regret bounds from \\Cref{res:res3,res:res4} considerably improve upon prior works~\\cite{auer2008near,azar2017minimax,zanette2019tighter,ortner2012online,lakshmanan2015improved,zhong2023provably,ganguly2023quantum} (see \\Cref{table:results_finite-horizon,table:results_infinite-horizon} for a clear comparison). For finite-horizon MDPs, our classical bound $\\widetilde{O}(\\sqrt{HSAT})$\\widetilde{O}(\\sqrt{HSAT}) matches the one from~\\cite{azar2017minimax,zanette2019tighter} when $T\\geq H^3S^3A$T\\geq H^3S^3A and $SA\\geq H$SA\\geq H, and avoids the extra terms $\\widetilde{O}(H^2S^2A + H\\sqrt{T})$\\widetilde{O}(H^2S^2A + H\\sqrt{T}) outside this parameter range. The quantum bound $\\widetilde{O}(S\\min\\{HA,H^2\\sqrt{A}\\})$\\widetilde{O}(S\\min\\{HA,H^2\\sqrt{A}\\}) is quadratically better in $S$S, $A$A, $H$H compared to~\\cite{zhong2023provably,ganguly2023quantum} and maintains the logarithmic dependence on $T$T. Regarding infinite-horizon MDPs, our classical regret $\\widetilde{O}(\\Lambda\\sqrt{SAT})$\\widetilde{O}(\\Lambda\\sqrt{SAT}) improves upon the bound $\\widetilde{O}(\\Lambda\\sqrt{S^2AT})$\\widetilde{O}(\\Lambda\\sqrt{S^2AT}) of~\\cite{bartlett2009regal,fruit2018efficient}, while for compact state spaces, the classical regret $\\widetilde{O}(\\Lambda T^{\\frac{D+\\alpha}{D+2\\alpha}}\\sqrt{A})$\\widetilde{O}(\\Lambda T^{\\frac{D+\\alpha}{D+2\\alpha}}\\frac{D+\\alpha}{D+2\\alpha}\\sqrt{A}) is superior to the bound $\\widetilde{O}(\\Lambda T^{\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha}} \\sqrt{A})$\\widetilde{O}(\\Lambda T^{\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha}}\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha} \\sqrt{A}) of~\\cite{lakshmanan2015improved}.\\footnote{For $D>1$,~\\cite{lakshmanan2015improved} ignores a term $\\widetilde{O}(\\Lambda A T^{\\frac{D}{1+(D+2)\\alpha}})$ which dominates the complexity for small $\\alpha$, so our regret bound is better for all values of $\\alpha$ and $D$, see~\\cite[Eq.~(29)]{lakshmanan2015improved}.} Our quantum regret bounds for infinite-horizon MDPs are completely novel and clearly improve upon their classical counterparts. One should keep in mind, however, that these comparisons are made between different RL models.\n\n\\begin{table}[t]\n\\centering\n\\def\\arraystretch{1.5}\n\\resizebox{\\linewidth}{!}{\n\\begin{tabular}{|c|cc|cc|}\n\\hline\n\nWork /  & \\multicolumn{2}{c|}{Finite state space $\\mathcal{X}$ with size $S$} & \\multicolumn{2}{c|}{Compact state space $\\mathcal{X}=[0,1]^D$} \\\\ \n\\cline{2-5} \n$\\operatorname{Regret}_\\infty^{\\rm path}(T)$ & \\multicolumn{1}{c|}{Regret (classical)} & Regret (quantum) & \\multicolumn{1}{c|}{Regret (classical)} & Regret (quantum) \\\\ \\hline \n\n\\cite{auer2008near} & \\multicolumn{1}{c|}{$\\sqrt{\\Delta^2S^2AT} + \\Delta SA$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{bartlett2009regal}$^\\dagger$ & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2S^2AT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{ortner2012online}$^\\dagger$ & \\multicolumn{1}{c|}{-} & - & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 A}T^{\\frac{2D+\\alpha}{2D+2\\alpha}}$} & - \\\\ \\hline\n\n\\cite{lakshmanan2015improved}$^\\dagger$ & \\multicolumn{1}{c|}{-} & - & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 A}T^{\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha}}$} & - \\\\ \\hline\n\n%\\cite{agrawal2017optimistic} & \\multicolumn{1}{c|}{$\\sqrt{\\Delta^2SAT} + \\Delta(S^7A^3T)^{\\frac{1}{4}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{ouyang2017learning} & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 S^2 AT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{fruit2018efficient} & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 S^2 AT} + \\Lambda S^2A$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{zhang2019regret}$^\\dagger$ & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda S AT} + \\Lambda(S^{10} A T)^{\\frac{1}{4}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{fruit2020improved} & \\multicolumn{1}{c|}{$\\sqrt{\\Delta S^2 AT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{ortner2020regret}$^\\dagger$ & \\multicolumn{1}{c|}{$\\sqrt{t_{\\rm mix} S AT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{wei2020model} & \\multicolumn{1}{c|}{$\\Lambda (S AT^2)^{\\frac{1}{3}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\multirow{2}{*}{\\cite{wei2021learning}} & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 S^3 A^3 T}$} & - & \\multicolumn{1}{c|}{-} & - \\\\\n\n& \\multicolumn{1}{c|}{$\\sqrt{\\Lambda} (S A T)^{\\frac{3}{4}} + (\\Lambda S A T)^{\\frac{2}{3}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{zhang2023sharper} & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 S^{10} A^4 T}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\n\\rowcolor{Gray}\nThis work & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 SAT}$} & $\\sqrt{\\Lambda^2 T} + \\sqrt{\\Lambda^2 S^2 A}$ & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 A}T^{\\frac{D+\\alpha}{D+2\\alpha}}$} & $\\sqrt{\\Lambda^2 T} + \\sqrt{\\Lambda^2 A}T^{\\frac{D}{D+\\alpha}}$ \\\\ \\hline\\hline\n\n\\rowcolor{Gray}\n$\\operatorname{Regret}_\\infty^{\\mathbb{E}}(T)$ & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 SAT}$} & $\\sqrt{\\Lambda^2 S^2 A}$ & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 A}T^{\\frac{D+\\alpha}{D+2\\alpha}}$} & $\\sqrt{\\Lambda^2 A}T^{\\frac{D}{D+\\alpha}}$ \\\\ \\hline\n\n\\end{tabular}}\n\\caption{Summary of several known upper bounds for the \\emph{in-path} regret $\\operatorname{Regret}_\\infty^{\\rm path}(T)$ of \\emph{infinite-horizon} average-reward MDPs $\\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ with state space $\\mathcal{X}$, action space $\\mathcal{A}$ with size $A$, $\\operatorname{sp}(h^\\ast) \\leq \\Lambda$, and number of time steps $T$. The state space $\\mathcal{X}$ can be finite with size $S$ or $\\mathcal{X} = [0,1]^D$, in which case $p$ and $r$ are assumed to be H\\\"older continuous with exponent $\\alpha \\geq 0$. Here $\\Delta \\geq \\Lambda$ and $t_{\\rm mix}$ are the MDP's diameter and mixing time, respectively. In the last row, we show our bounds for the \\emph{expected} regret $\\operatorname{Regret}_\\infty^{\\mathbb{E}}(T)$. All bounds are up to $\\poly\\log$ factors in $A$, $\\Lambda$, $T$, and $S$ (for $\\mathcal{X}$ finite). Refs.\\ marked with $\\dagger$ have no efficient implementation. The reader should keep in mind that not all works assume the same RL model.}\n\\label{table:results_infinite-horizon}\n\\end{table}\n\\centering\n\\def\\arraystretch{1.5}1.5\n\\resizebox{\\linewidth}\\linewidth{!}!{\n\\begin{tabular}{|c|cc|cc|}\n\\hline\n\nWork /  & \\multicolumn{2}{c|}{Finite state space $\\mathcal{X}$ with size $S$} & \\multicolumn{2}{c|}{Compact state space $\\mathcal{X}=[0,1]^D$} \\\\ \n\\cline{2-5} \n$\\operatorname{Regret}_\\infty^{\\rm path}(T)$ & \\multicolumn{1}{c|}{Regret (classical)} & Regret (quantum) & \\multicolumn{1}{c|}{Regret (classical)} & Regret (quantum) \\\\ \\hline \n\n\\cite{auer2008near} & \\multicolumn{1}{c|}{$\\sqrt{\\Delta^2S^2AT} + \\Delta SA$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{bartlett2009regal}$^\\dagger$ & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2S^2AT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{ortner2012online}$^\\dagger$ & \\multicolumn{1}{c|}{-} & - & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 A}T^{\\frac{2D+\\alpha}{2D+2\\alpha}}$} & - \\\\ \\hline\n\n\\cite{lakshmanan2015improved}$^\\dagger$ & \\multicolumn{1}{c|}{-} & - & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 A}T^{\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha}}$} & - \\\\ \\hline\n\n%\\cite{agrawal2017optimistic} & \\multicolumn{1}{c|}{$\\sqrt{\\Delta^2SAT} + \\Delta(S^7A^3T)^{\\frac{1}{4}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{ouyang2017learning} & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 S^2 AT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{fruit2018efficient} & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 S^2 AT} + \\Lambda S^2A$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{zhang2019regret}$^\\dagger$ & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda S AT} + \\Lambda(S^{10} A T)^{\\frac{1}{4}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{fruit2020improved} & \\multicolumn{1}{c|}{$\\sqrt{\\Delta S^2 AT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{ortner2020regret}$^\\dagger$ & \\multicolumn{1}{c|}{$\\sqrt{t_{\\rm mix} S AT}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{wei2020model} & \\multicolumn{1}{c|}{$\\Lambda (S AT^2)^{\\frac{1}{3}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\multirow{2}{*}{\\cite{wei2021learning}} & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 S^3 A^3 T}$} & - & \\multicolumn{1}{c|}{-} & - \\\\\n\n& \\multicolumn{1}{c|}{$\\sqrt{\\Lambda} (S A T)^{\\frac{3}{4}} + (\\Lambda S A T)^{\\frac{2}{3}}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\\cite{zhang2023sharper} & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 S^{10} A^4 T}$} & - & \\multicolumn{1}{c|}{-} & - \\\\ \\hline\n\n\n\\rowcolor{Gray}\nThis work & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 SAT}$} & $\\sqrt{\\Lambda^2 T} + \\sqrt{\\Lambda^2 S^2 A}$ & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 A}T^{\\frac{D+\\alpha}{D+2\\alpha}}$} & $\\sqrt{\\Lambda^2 T} + \\sqrt{\\Lambda^2 A}T^{\\frac{D}{D+\\alpha}}$ \\\\ \\hline\\hline\n\n\\rowcolor{Gray}\n$\\operatorname{Regret}_\\infty^{\\mathbb{E}}(T)$ & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 SAT}$} & $\\sqrt{\\Lambda^2 S^2 A}$ & \\multicolumn{1}{c|}{$\\sqrt{\\Lambda^2 A}T^{\\frac{D+\\alpha}{D+2\\alpha}}$} & $\\sqrt{\\Lambda^2 A}T^{\\frac{D}{D+\\alpha}}$ \\\\ \\hline\n\n\\end{tabular}}\n\n\\hline\n\nWork /  & \\multicolumn{2}2{c|}c|{Finite state space $\\mathcal{X}$ with size $S$}Finite state space $\\mathcal{X}$\\mathcal{X} with size $S$S & \\multicolumn{2}2{c|}c|{Compact state space $\\mathcal{X}=[0,1]^D$}Compact state space $\\mathcal{X}=[0,1]^D$\\mathcal{X}=[0,1]^D \\\\ \n\\cline{2-5}2-5 \n$\\operatorname{Regret}_\\infty^{\\rm path}(T)$\\operatorname{Regret}Regret_\\infty^{\\rm path}\\rm path(T) & \\multicolumn{1}1{c|}c|{Regret (classical)}Regret (classical) & Regret (quantum) & \\multicolumn{1}1{c|}c|{Regret (classical)}Regret (classical) & Regret (quantum) \\\\ \\hline \n\n\\cite{auer2008near} & \\multicolumn{1}1{c|}c|{$\\sqrt{\\Delta^2S^2AT} + \\Delta SA$}$\\sqrt{\\Delta^2S^2AT} + \\Delta SA$\\sqrt{\\Delta^2S^2AT} + \\Delta SA & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\cite{bartlett2009regal}$^\\dagger$^\\dagger & \\multicolumn{1}1{c|}c|{$\\sqrt{\\Lambda^2S^2AT}$}$\\sqrt{\\Lambda^2S^2AT}$\\sqrt{\\Lambda^2S^2AT} & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\cite{ortner2012online}$^\\dagger$^\\dagger & \\multicolumn{1}1{c|}c|{-}- & - & \\multicolumn{1}1{c|}c|{$\\sqrt{\\Lambda^2 A}T^{\\frac{2D+\\alpha}{2D+2\\alpha}}$}$\\sqrt{\\Lambda^2 A}T^{\\frac{2D+\\alpha}{2D+2\\alpha}}$\\sqrt{\\Lambda^2 A}T^{\\frac{2D+\\alpha}{2D+2\\alpha}}\\frac{2D+\\alpha}{2D+2\\alpha} & - \\\\ \\hline\n\n\\cite{lakshmanan2015improved}$^\\dagger$^\\dagger & \\multicolumn{1}1{c|}c|{-}- & - & \\multicolumn{1}1{c|}c|{$\\sqrt{\\Lambda^2 A}T^{\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha}}$}$\\sqrt{\\Lambda^2 A}T^{\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha}}$\\sqrt{\\Lambda^2 A}T^{\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha}}\\frac{1+(D+1)\\alpha}{1+(D+2)\\alpha} & - \\\\ \\hline\n\n\n\n\\cite{ouyang2017learning} & \\multicolumn{1}1{c|}c|{$\\sqrt{\\Lambda^2 S^2 AT}$}$\\sqrt{\\Lambda^2 S^2 AT}$\\sqrt{\\Lambda^2 S^2 AT} & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\cite{fruit2018efficient} & \\multicolumn{1}1{c|}c|{$\\sqrt{\\Lambda^2 S^2 AT} + \\Lambda S^2A$}$\\sqrt{\\Lambda^2 S^2 AT} + \\Lambda S^2A$\\sqrt{\\Lambda^2 S^2 AT} + \\Lambda S^2A & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\cite{zhang2019regret}$^\\dagger$^\\dagger & \\multicolumn{1}1{c|}c|{$\\sqrt{\\Lambda S AT} + \\Lambda(S^{10} A T)^{\\frac{1}{4}}$}$\\sqrt{\\Lambda S AT} + \\Lambda(S^{10} A T)^{\\frac{1}{4}}$\\sqrt{\\Lambda S AT} + \\Lambda(S^{10}10 A T)^{\\frac{1}{4}}\\frac{1}{4} & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\cite{fruit2020improved} & \\multicolumn{1}1{c|}c|{$\\sqrt{\\Delta S^2 AT}$}$\\sqrt{\\Delta S^2 AT}$\\sqrt{\\Delta S^2 AT} & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\cite{ortner2020regret}$^\\dagger$^\\dagger & \\multicolumn{1}1{c|}c|{$\\sqrt{t_{\\rm mix} S AT}$}$\\sqrt{t_{\\rm mix} S AT}$\\sqrt{t_{\\rm mix} S AT} & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\cite{wei2020model} & \\multicolumn{1}1{c|}c|{$\\Lambda (S AT^2)^{\\frac{1}{3}}$}$\\Lambda (S AT^2)^{\\frac{1}{3}}$\\Lambda (S AT^2)^{\\frac{1}{3}}\\frac{1}{3} & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\multirow{2}2{*}*{\\cite{wei2021learning}}\\cite{wei2021learning} & \\multicolumn{1}1{c|}c|{$\\sqrt{\\Lambda^2 S^3 A^3 T}$}$\\sqrt{\\Lambda^2 S^3 A^3 T}$\\sqrt{\\Lambda^2 S^3 A^3 T} & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\\n\n& \\multicolumn{1}1{c|}c|{$\\sqrt{\\Lambda} (S A T)^{\\frac{3}{4}} + (\\Lambda S A T)^{\\frac{2}{3}}$}$\\sqrt{\\Lambda} (S A T)^{\\frac{3}{4}} + (\\Lambda S A T)^{\\frac{2}{3}}$\\sqrt{\\Lambda} (S A T)^{\\frac{3}{4}}\\frac{3}{4} + (\\Lambda S A T)^{\\frac{2}{3}}\\frac{2}{3} & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\\cite{zhang2023sharper} & \\multicolumn{1}1{c|}c|{$\\sqrt{\\Lambda^2 S^{10} A^4 T}$}$\\sqrt{\\Lambda^2 S^{10} A^4 T}$\\sqrt{\\Lambda^2 S^{10} A^4 T} & - & \\multicolumn{1}1{c|}c|{-}- & - \\\\ \\hline\n\n\n\\rowcolor{Gray}Gray\nThis work & \\multicolumn{1}1{c|}c|{$\\sqrt{\\Lambda^2 SAT}$}$\\sqrt{\\Lambda^2 SAT}$\\sqrt{\\Lambda^2 SAT} & $\\sqrt{\\Lambda^2 T} + \\sqrt{\\Lambda^2 S^2 A}$\\sqrt{\\Lambda^2 T} + \\sqrt{\\Lambda^2 S^2 A} & \\multicolumn{1}1{c|}c|{$\\sqrt{\\Lambda^2 A}T^{\\frac{D+\\alpha}{D+2\\alpha}}$}$\\sqrt{\\Lambda^2 A}T^{\\frac{D+\\alpha}{D+2\\alpha}}$\\sqrt{\\Lambda^2 A}T^{\\frac{D+\\alpha}{D+2\\alpha}}\\frac{D+\\alpha}{D+2\\alpha} & $\\sqrt{\\Lambda^2 T} + \\sqrt{\\Lambda^2 A}T^{\\frac{D}{D+\\alpha}}$\\sqrt{\\Lambda^2 T} + \\sqrt{\\Lambda^2 A}T^{\\frac{D}{D+\\alpha}}\\frac{D}{D+\\alpha} \\\\ \\hline\\hline\n\n\\rowcolor{Gray}Gray\n$\\operatorname{Regret}_\\infty^{\\mathbb{E}}(T)$\\operatorname{Regret}Regret_\\infty^{\\mathbb{E}}\\mathbb{E}(T) & \\multicolumn{1}1{c|}c|{$\\sqrt{\\Lambda^2 SAT}$}$\\sqrt{\\Lambda^2 SAT}$\\sqrt{\\Lambda^2 SAT} & $\\sqrt{\\Lambda^2 S^2 A}$\\sqrt{\\Lambda^2 S^2 A} & \\multicolumn{1}1{c|}c|{$\\sqrt{\\Lambda^2 A}T^{\\frac{D+\\alpha}{D+2\\alpha}}$}$\\sqrt{\\Lambda^2 A}T^{\\frac{D+\\alpha}{D+2\\alpha}}$\\sqrt{\\Lambda^2 A}T^{\\frac{D+\\alpha}{D+2\\alpha}}\\frac{D+\\alpha}{D+2\\alpha} & $\\sqrt{\\Lambda^2 A}T^{\\frac{D}{D+\\alpha}}$\\sqrt{\\Lambda^2 A}T^{\\frac{D}{D+\\alpha}}\\frac{D}{D+\\alpha} \\\\ \\hline\n\n\n\\caption{Summary of several known upper bounds for the \\emph{in-path} regret $\\operatorname{Regret}_\\infty^{\\rm path}(T)$ of \\emph{infinite-horizon} average-reward MDPs $\\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ with state space $\\mathcal{X}$, action space $\\mathcal{A}$ with size $A$, $\\operatorname{sp}(h^\\ast) \\leq \\Lambda$, and number of time steps $T$. The state space $\\mathcal{X}$ can be finite with size $S$ or $\\mathcal{X} = [0,1]^D$, in which case $p$ and $r$ are assumed to be H\\\"older continuous with exponent $\\alpha \\geq 0$. Here $\\Delta \\geq \\Lambda$ and $t_{\\rm mix}$ are the MDP's diameter and mixing time, respectively. In the last row, we show our bounds for the \\emph{expected} regret $\\operatorname{Regret}_\\infty^{\\mathbb{E}}(T)$. All bounds are up to $\\poly\\log$ factors in $A$, $\\Lambda$, $T$, and $S$ (for $\\mathcal{X}$ finite). Refs.\\ marked with $\\dagger$ have no efficient implementation. The reader should keep in mind that not all works assume the same RL model.}\n\\label{table:results_infinite-horizon}\n\n\n\\paragraph{A new measure of regret.}A new measure of regret. A striking feature of \\Cref{res:res3,res:res4} is that, just like~\\cite{zhong2023provably,ganguly2023quantum}, the quantum regret bounds for finite-horizon MDPs are exponentially better in the number of steps $T$T, but the same is not true for infinite-horizon MDPs, where the term $\\widetilde{O}(\\Lambda\\sqrt{T})$\\widetilde{O}(\\Lambda\\sqrt{T}) hinders such an exponential advantage. A closer look at the proof reveals that $\\widetilde{O}(\\Lambda\\sqrt{T})$\\widetilde{O}(\\Lambda\\sqrt{T}) comes from a (Azuma-Hoeffding) concentration inequality, which hints as being an artifact of a deviation from some mean. A comparison between finite and infinite-horizon regrets $\\operatorname{Regret}_H(T)$\\operatorname{Regret}Regret_H(T) and $\\operatorname{Regret}_\\infty^{\\rm path}(T)$\\operatorname{Regret}Regret_\\infty^{\\rm path}\\rm path(T) sheds further light into this issue. $\\operatorname{Regret}_H(T)$\\operatorname{Regret}Regret_H(T) is defined as the difference between two \\emph{expected} quantities, more precisely, the difference of expected sum of rewards of the optimal policy and the actual policy. On the other hand, $\\operatorname{Regret}_\\infty^{\\rm path}(T)$\\operatorname{Regret}Regret_\\infty^{\\rm path}\\rm path(T) takes into account the path of observed state-action pairs and not the difference of some quantity averaged with respect to the optimal policy and the actual policy, e.g., the average reward $g^\\pi(x)$g^\\pi(x). For this reason, we introduce a novel measure of regret for infinite-horizon MDPs, $\\operatorname{Regret}_\\infty^{\\mathbb{E}}(T)$\\operatorname{Regret}Regret_\\infty^{\\mathbb{E}}\\mathbb{E}(T), which we call \\emph{expected} regret, defined as\n\\begin{align*}\n    \\operatorname{Regret}_\\infty^{\\mathbb{E}}(T) := \\sum_{t=1}^T \\left(g^\\ast - \\min_{x\\in\\mathcal{X}}g^{\\pi_t^\\infty}(x) \\right),\n\\end{align*}\\begin{align*}\n    \\operatorname{Regret}_\\infty^{\\mathbb{E}}(T) := \\sum_{t=1}^T \\left(g^\\ast - \\min_{x\\in\\mathcal{X}}g^{\\pi_t^\\infty}(x) \\right),\n\\end{align*}\n    \\operatorname{Regret}Regret_\\infty^{\\mathbb{E}}\\mathbb{E}(T) := \\sum_{t=1}t=1^T \\left(g^\\ast - \\min_{x\\in\\mathcal{X}}x\\in\\mathcal{X}g^{\\pi_t^\\infty}\\pi_t^\\infty(x) \\right),\n\nwhere $\\pi_t^\\infty = (\\pi_t,\\pi_t,\\dots)$\\pi_t^\\infty = (\\pi_t,\\pi_t,\\dots) is the stationary policy obtained from the current decision rule $\\pi_t$\\pi_t. In other words, $g^{\\pi_t^\\infty}(x)$g^{\\pi_t^\\infty}\\pi_t^\\infty(x) measures the average reward per step if the agent were to use his or her current decision rule $\\pi_t$\\pi_t for the rest of the interaction. Obviously, the agent can pick a different decision rule $\\pi_{t+1}$\\pi_{t+1}t+1 at the next time step and thus incur a different average regret $g^\\ast - \\min_{x\\in\\mathcal{X}} g^{\\pi_{t+1}^\\infty}(x)$g^\\ast - \\min_{x\\in\\mathcal{X}}x\\in\\mathcal{X} g^{\\pi_{t+1}^\\infty}\\pi_{t+1}t+1^\\infty(x) at that step. The choice for minimising over $x\\in\\mathcal{X}$x\\in\\mathcal{X} takes into account a worst-case situation and is arbitrary in principle. One could have picked the current environment state $x_t$x_t and defined the average regret per step as $g^\\ast - g^{\\pi_t^\\infty}(x_t)$g^\\ast - g^{\\pi_t^\\infty}\\pi_t^\\infty(x_t) instead.\\footnote{If the MDP is \\emph{unichain}~\\cite[Section~8.3]{puterman2014markov}, $g^{\\pi_t^\\infty}(x)$ is independent of $x\\in\\mathcal{X}$ for any stationary policy, and so $g^\\ast - g^{\\pi_t^\\infty}$ is the average regret per step.}\n\n\n\nWe show that, although our classical algorithm has the same in-path and expected regrets, our quantum algorithm can achieve an expected regret $\\operatorname{Regret}_\\infty^{\\mathbb{E}}(T)$\\operatorname{Regret}Regret_\\infty^{\\mathbb{E}}\\mathbb{E}(T) that is exponentially better in $T$T compared to their classical counterparts (see also \\Cref{table:results_infinite-horizon}).\n\\begin{result}[Infinite-horizon expected regret]\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ be an infinite-horizon (undiscounted) average-reward weakly communicating MDP with $\\operatorname{sp}(h^\\ast) \\leq \\Lambda$. If $\\mathcal{X}$ is finite with size $S$, there is a quantum algorithm with expected regret $\\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T)$ upper-bounded after $T$ steps, with high probability, by\n    %\n    \\begin{align*}\n        \\widetilde{O}\\big(\\Lambda S\\sqrt{A}\\log^2{T}\\log(SAT)\\log(ST)\\big),\n    \\end{align*}\n    %\n    where $\\widetilde{O}(\\cdot)$ hides $\\poly\\log\\log$ terms. If $\\mathcal{X} = [0,1]^D$, assume that $p$ and $r$ are H\\\"older continuous with parameters $L,\\alpha \\geq 0$, $L$ constant. There is a quantum algorithm with expected regret $\\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T)$ upper-bounded after $T$ steps, with high probability, by\n    %\n    \\begin{align*}\n        \\widetilde{O}\\big(\\Lambda\\sqrt{A} T^{\\frac{D}{D+\\alpha}}\\log^3{T}\\log(AT) \\big).\n    \\end{align*}\n    %\n    %where $\\widetilde{O}(\\cdot)$ hides $\\poly\\log\\log$ terms.\n\\end{result}\\begin{result}[Infinite-horizon expected regret]\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ be an infinite-horizon (undiscounted) average-reward weakly communicating MDP with $\\operatorname{sp}(h^\\ast) \\leq \\Lambda$. If $\\mathcal{X}$ is finite with size $S$, there is a quantum algorithm with expected regret $\\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T)$ upper-bounded after $T$ steps, with high probability, by\n    %\n    \\begin{align*}\n        \\widetilde{O}\\big(\\Lambda S\\sqrt{A}\\log^2{T}\\log(SAT)\\log(ST)\\big),\n    \\end{align*}\n    %\n    where $\\widetilde{O}(\\cdot)$ hides $\\poly\\log\\log$ terms. If $\\mathcal{X} = [0,1]^D$, assume that $p$ and $r$ are H\\\"older continuous with parameters $L,\\alpha \\geq 0$, $L$ constant. There is a quantum algorithm with expected regret $\\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T)$ upper-bounded after $T$ steps, with high probability, by\n    %\n    \\begin{align*}\n        \\widetilde{O}\\big(\\Lambda\\sqrt{A} T^{\\frac{D}{D+\\alpha}}\\log^3{T}\\log(AT) \\big).\n    \\end{align*}\n    %\n    %where $\\widetilde{O}(\\cdot)$ hides $\\poly\\log\\log$ terms.\n\\end{result}[Infinite-horizon expected regret]\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle be an infinite-horizon (undiscounted) average-reward weakly communicating MDP with $\\operatorname{sp}(h^\\ast) \\leq \\Lambda$\\operatorname{sp}sp(h^\\ast) \\leq \\Lambda. If $\\mathcal{X}$\\mathcal{X} is finite with size $S$S, there is a quantum algorithm with expected regret $\\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T)$\\operatorname{Regret}Regret_{\\infty}\\infty^{\\mathbb{E}}\\mathbb{E}(T) upper-bounded after $T$T steps, with high probability, by\n    \n        \\widetilde{O}\\big(\\Lambda S\\sqrt{A}\\log^2{T}T\\log(SAT)\\log(ST)\\big),\n    \n    where $\\widetilde{O}(\\cdot)$\\widetilde{O}(\\cdot) hides $\\poly\\log\\log$\\poly\\log\\log terms. If $\\mathcal{X} = [0,1]^D$\\mathcal{X} = [0,1]^D, assume that $p$p and $r$r are H\\\"older continuous with parameters $L,\\alpha \\geq 0$L,\\alpha \\geq 0, $L$L constant. There is a quantum algorithm with expected regret $\\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T)$\\operatorname{Regret}Regret_{\\infty}\\infty^{\\mathbb{E}}\\mathbb{E}(T) upper-bounded after $T$T steps, with high probability, by\n    \n        \\widetilde{O}\\big(\\Lambda\\sqrt{A} T^{\\frac{D}{D+\\alpha}}\\frac{D}{D+\\alpha}\\log^3{T}T\\log(AT) \\big).\n    \n    \n\n\nUnder the expected regret $\\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T)$\\operatorname{Regret}Regret_{\\infty}\\infty^{\\mathbb{E}}\\mathbb{E}(T), our quantum algorithm for infinite-horizon finite-state-space MDPs achieves an exponentially better bound with respect to $T$T compared to the classical counterpart $\\widetilde{O}(\\Lambda\\sqrt{SAT})$\\widetilde{O}(\\Lambda\\sqrt{SAT}). This highlights the importance of a proper measure of regret for quantum RL algorithms, one which can isolate the extension of a quantum advantage from random fluctuations.\n\n\nThe remaining of this paper is organized as follows: in \\cref{sec:preliminaries} we introduce useful notations, the computational model, the quantum subroutines that will be used in this paper, and a self-contained introduction to MDPs. \\Cref{sec:RL_model} explains our agent-environment interaction model in details. \\Cref{sec:optimal_policies_finite-horizon} describes the classical algorithm of Sidford \\emph{et al.}~\\cite{sidford2018near} for approximating optimal policies under a generative model for finite-horizon MDPs and our newly introduced quantum versions, while \\Cref{sec:value_iteration} describes generative classical and quantum algorithms for computing optimal policies for infinite-horizon MDPs. In \\Cref{sec:finite-horizon_MDPs} we explain our RL algorithms for finite-horizon MDPs, while the infinite-horizon case is explored in \\Cref{sec:Quantum_UCCRL}.\n\n\\paragraph{Acknowledgments.}Acknowledgments.\nDL thanks Patrick Rebentrost for helpful discussions and Ronald Ortner for clarifications on Ref.~\\cite{ortner2012online} and for pointing out Ref.~\\cite{lakshmanan2015improved}. JFD thanks Arjan Cornelissen for pointing out Ref.~\\cite{tang2025more}, Alessandro Luongo and Miklos Santha for comments on the manuscript, and Yecheng Xue for clarifications on Ref.~\\cite{zhong2023provably}. AA was supported by Latvian Quantum Initiative under European Union Recovery and Resilience Facility project No. 2.3.1.1.i.0/1/22/I/CFLA/001. DL acknowledges funding from QuantERA Project QOPT. JFD is supported by ERC grant No.\\ 810115-DYNASNET. This project was finalised while JFD was a visiting researcher at the Simons Institute for the Theory of Computing.\n\n\n\n\n\n\n\n\n\n\n", "appendix": false}, "Preliminaries": {"content": "\\label{sec:preliminaries}\n\nFor $n\\in\\mathbb{N} := \\{1,2,\\dots\\}$n\\in\\mathbb{N} := \\{1,2,\\dots\\}, we use $[n]$[n] to represent the set $\\{1, \\dots, n\\}$\\{1, \\dots, n\\} and denote the $i$i-th entry of a vector ${u}\\in\\mathbb R^n${u}u\\in\\mathbb R^n by $u(i)$u(i) for all $i\\in[n]$i\\in[n]. The $\\ell_1$\\ell_1 and $\\ell_\\infty$\\ell_\\infty-norm of a vector ${u}\\in\\mathbb{R}^n${u}u\\in\\mathbb{R}^n are $\\Vert {u}\\Vert_1 \\coloneqq \\sum_{i\\in[n]} \\vert u(i)\\vert$\\Vert {u}u\\Vert_1 \\coloneqq \\sum_{i\\in[n]}i\\in[n] \\vert u(i)\\vert and $\\Vert {u}\\Vert_\\infty \\coloneqq \\max_{i\\in[n]} \\vert u(i)\\vert$\\Vert {u}u\\Vert_\\infty \\coloneqq \\max_{i\\in[n]}i\\in[n] \\vert u(i)\\vert, respectively. Given a metric space $\\mathcal{X}$\\mathcal{X}, let $\\mathscr{M}(\\mathcal{X})$\\mathscr{M}(\\mathcal{X}) be the $\\sigma$\\sigma-algebra of (Borel) measurable subsets of $\\mathcal{X}$\\mathcal{X} and $\\mathscr{P}(\\mathcal{X})$\\mathscr{P}(\\mathcal{X}) the collection of probability distributions on $\\mathscr{M}(\\mathcal{X})$\\mathscr{M}(\\mathcal{X}). Let $\\mathscr{B}(\\mathcal{X})$\\mathscr{B}(\\mathcal{X}) be the space of all bounded Borel measurable real-valued functions on $\\mathcal{X}$\\mathcal{X}. Given $u\\in\\mathscr{B}(\\mathcal{X})$u\\in\\mathscr{B}(\\mathcal{X}), let $\\operatorname{sp}(u)$\\operatorname{sp}sp(u) be the span seminorm defined as $\\operatorname{sp}(u) := \\sup_{x\\in\\mathcal{X}}u(x) - \\inf_{x\\in\\mathcal{X}}u(x)$\\operatorname{sp}sp(u) := \\sup_{x\\in\\mathcal{X}}x\\in\\mathcal{X}u(x) - \\inf_{x\\in\\mathcal{X}}x\\in\\mathcal{X}u(x). When $\\mathcal{X}$\\mathcal{X} is finite, we shall interchangeably interpret $u\\in\\mathscr{B}(\\mathcal{X})$u\\in\\mathscr{B}(\\mathcal{X}) as a bounded real-valued function and a bounded real-valued vector ${u}\\in\\mathbb{R}^{|\\mathcal{X}|}${u}u\\in\\mathbb{R}^{|\\mathcal{X}|}|\\mathcal{X}|. We use $e\\in\\mathscr{B}(\\mathcal{X})$e\\in\\mathscr{B}(\\mathcal{X}) to denote the all-ones function, $e(x) = 1$e(x) = 1 for all $x\\in\\mathcal{X}$x\\in\\mathcal{X}. Unless mentioned otherwise, we use $\\widetilde O(\\cdot)$\\widetilde O(\\cdot) to hide polylogarithmic factors, i.e., $\\widetilde{O}(f(n)) = O(f(n)\\cdot \\operatorname{poly}\\log(f(n)))$\\widetilde{O}(f(n)) = O(f(n)\\cdot \\operatorname{poly}poly\\log(f(n))).\n\nWe shall need the following simple lemmas.\n\\begin{fact}[{\\cite[Lemma~2]{lakshmanan2015improved}}]\\label{fact:useful_inequality}\n    For any sequence $z_1, \\dots, z_n\\in\\mathbb{R}$ with $0\\leq z_k \\leq Z_{k-1}\\coloneqq \\max\\{1, \\sum_{i=1}^{k-1} z_i\\}$,\n    %\n    \\begin{align*}\n        \\sum_{k=1}^m \\frac{z_k}{Z_{k-1}^{1-\\gamma}} \\leq \\frac{Z_m^\\gamma}{2^\\gamma-1} \\quad \\text{for any}\\quad \\gamma\\in[0,1].\n    \\end{align*}\n\\end{fact}\\begin{fact}[{\\cite[Lemma~2]{lakshmanan2015improved}}]\\label{fact:useful_inequality}\n    For any sequence $z_1, \\dots, z_n\\in\\mathbb{R}$ with $0\\leq z_k \\leq Z_{k-1}\\coloneqq \\max\\{1, \\sum_{i=1}^{k-1} z_i\\}$,\n    %\n    \\begin{align*}\n        \\sum_{k=1}^m \\frac{z_k}{Z_{k-1}^{1-\\gamma}} \\leq \\frac{Z_m^\\gamma}{2^\\gamma-1} \\quad \\text{for any}\\quad \\gamma\\in[0,1].\n    \\end{align*}\n\\end{fact}[{\\cite[Lemma~2]{lakshmanan2015improved}}\\cite[Lemma~2]{lakshmanan2015improved}]\\label{fact:useful_inequality}\n    For any sequence $z_1, \\dots, z_n\\in\\mathbb{R}$z_1, \\dots, z_n\\in\\mathbb{R} with $0\\leq z_k \\leq Z_{k-1}\\coloneqq \\max\\{1, \\sum_{i=1}^{k-1} z_i\\}$0\\leq z_k \\leq Z_{k-1}k-1\\coloneqq \\max\\{1, \\sum_{i=1}i=1^{k-1}k-1 z_i\\},\n    \n        \\sum_{k=1}k=1^m \\frac{z_k}{Z_{k-1}^{1-\\gamma}} \\leq \\frac{Z_m^\\gamma}{2^\\gamma-1} \\quad \\text{for any}\\quad \\gamma\\in[0,1].\n    \n\n\n\\begin{lemma}\\label{lem:regret}\n    For any sequence $z_1, \\dots, z_n\\in\\mathbb{R}$ with $0\\leq z_k \\leq Z_{k-1}\\coloneqq \\max\\{1, \\sum_{i=1}^{k-1} z_i\\}$ and $Z_n \\geq 4$,\n    \\begin{align*}\n        \\sum_{k=1}^n \\frac{z_k}{Z_{k-1}} \\leq 4\\log_2(Z_n/2).\n    \\end{align*}\n\\end{lemma}\\begin{lemma}\\label{lem:regret}\n    For any sequence $z_1, \\dots, z_n\\in\\mathbb{R}$ with $0\\leq z_k \\leq Z_{k-1}\\coloneqq \\max\\{1, \\sum_{i=1}^{k-1} z_i\\}$ and $Z_n \\geq 4$,\n    \\begin{align*}\n        \\sum_{k=1}^n \\frac{z_k}{Z_{k-1}} \\leq 4\\log_2(Z_n/2).\n    \\end{align*}\n\\end{lemma}\\label{lem:regret}\n    For any sequence $z_1, \\dots, z_n\\in\\mathbb{R}$z_1, \\dots, z_n\\in\\mathbb{R} with $0\\leq z_k \\leq Z_{k-1}\\coloneqq \\max\\{1, \\sum_{i=1}^{k-1} z_i\\}$0\\leq z_k \\leq Z_{k-1}k-1\\coloneqq \\max\\{1, \\sum_{i=1}i=1^{k-1}k-1 z_i\\} and $Z_n \\geq 4$Z_n \\geq 4,\n    \n        \\sum_{k=1}k=1^n \\frac{z_k}{Z_{k-1}} \\leq 4\\log_2(Z_n/2).\n    \n\n\\begin{proof}\n    We employ \\Cref{fact:useful_inequality} to obtain\n    %\n    \\begin{align*}\n        \\sum_{k=1}^n \\frac{z_k}{Z_{k-1}} \\leq \\sum_{k=1}^n \\frac{z_k}{Z_{k-1}^{1 - \\gamma}} \\leq \\frac{Z_n^\\gamma}{2^\\gamma-1} \\quad\\text{for any}\\quad \\gamma\\in [0, 1].\n    \\end{align*}\n    %\n    A simple calculation yields that the value $\\gamma = \\log_2\\big(\\frac{\\ln Z_n}{\\ln(Z_n/2)}\\big)$ (which is $\\leq 1$ since $Z_n \\geq 4$) minimises the above right-hand side and thus leads to\n    % Taking the derivative of $\\frac{Z_n^\\alpha}{2^\\alpha-1}$ with respect to $\\alpha$ and letting it be 0, we get  \n    % \\begin{align*}\n    %     -Z_n^\\alpha\\left(-2^\\alpha\\log Z_n + \\log Z_n + 2^\\alpha \\log 2\\right) & = 0\\\\\n    %     2^\\alpha \\log Z_n -  2^\\alpha \\log 2 & = \\log Z_n\\\\\n    %     2^\\alpha\\left(\\log Z_n - \\log 2\\right) & = \\log Z_n\\\\\n    %     2^\\alpha & = \\frac{\\log Z_n}{\\log Z_n - \\log 2}\\\\\n    %     \\alpha & = \\log_2\\left(\\frac{\\log Z_n}{\\log Z_n - \\log 2}\\right). \n    % \\end{align*}\n    \\begin{align*}\n        \\sum_{k=1}^n \\frac{z_k}{Z_{k-1}} \n        &\\leq \\frac{Z_n^{\\log_2\\left(\\frac{\\ln Z_n}{\\ln(Z_n/2)}\\right)}}{\\frac{\\ln Z_n}{\\ln(Z_n/2)} - 1} = \\frac{\\ln(Z_n/2)}{\\ln 2}\\left(1 + \\frac{\\ln 2}{\\ln(Z_n/2)}\\right)^{\\log_2 Z_n} \\leq \\frac{4}{\\ln 2}\\ln(Z_n/2),\n    \\end{align*}\n    %\n    where the last inequality uses that $\\big(1+\\frac{\\ln{2}}{\\ln(x/2)}\\big)^{\\log_2{x}}$ is a decreasing function of $x$ and $Z_n \\geq 4$.\n\\end{proof}\\begin{proof}\n    We employ \\Cref{fact:useful_inequality} to obtain\n    %\n    \\begin{align*}\n        \\sum_{k=1}^n \\frac{z_k}{Z_{k-1}} \\leq \\sum_{k=1}^n \\frac{z_k}{Z_{k-1}^{1 - \\gamma}} \\leq \\frac{Z_n^\\gamma}{2^\\gamma-1} \\quad\\text{for any}\\quad \\gamma\\in [0, 1].\n    \\end{align*}\n    %\n    A simple calculation yields that the value $\\gamma = \\log_2\\big(\\frac{\\ln Z_n}{\\ln(Z_n/2)}\\big)$ (which is $\\leq 1$ since $Z_n \\geq 4$) minimises the above right-hand side and thus leads to\n    % Taking the derivative of $\\frac{Z_n^\\alpha}{2^\\alpha-1}$ with respect to $\\alpha$ and letting it be 0, we get  \n    % \\begin{align*}\n    %     -Z_n^\\alpha\\left(-2^\\alpha\\log Z_n + \\log Z_n + 2^\\alpha \\log 2\\right) & = 0\\\\\n    %     2^\\alpha \\log Z_n -  2^\\alpha \\log 2 & = \\log Z_n\\\\\n    %     2^\\alpha\\left(\\log Z_n - \\log 2\\right) & = \\log Z_n\\\\\n    %     2^\\alpha & = \\frac{\\log Z_n}{\\log Z_n - \\log 2}\\\\\n    %     \\alpha & = \\log_2\\left(\\frac{\\log Z_n}{\\log Z_n - \\log 2}\\right). \n    % \\end{align*}\n    \\begin{align*}\n        \\sum_{k=1}^n \\frac{z_k}{Z_{k-1}} \n        &\\leq \\frac{Z_n^{\\log_2\\left(\\frac{\\ln Z_n}{\\ln(Z_n/2)}\\right)}}{\\frac{\\ln Z_n}{\\ln(Z_n/2)} - 1} = \\frac{\\ln(Z_n/2)}{\\ln 2}\\left(1 + \\frac{\\ln 2}{\\ln(Z_n/2)}\\right)^{\\log_2 Z_n} \\leq \\frac{4}{\\ln 2}\\ln(Z_n/2),\n    \\end{align*}\n    %\n    where the last inequality uses that $\\big(1+\\frac{\\ln{2}}{\\ln(x/2)}\\big)^{\\log_2{x}}$ is a decreasing function of $x$ and $Z_n \\geq 4$.\n\\end{proof}\n    We employ \\Cref{fact:useful_inequality} to obtain\n    \n        \\sum_{k=1}k=1^n \\frac{z_k}{Z_{k-1}} \\leq \\sum_{k=1}k=1^n \\frac{z_k}{Z_{k-1}^{1 - \\gamma}} \\leq \\frac{Z_n^\\gamma}{2^\\gamma-1} \\quad\\text{for any}\\quad \\gamma\\in [0, 1].\n    \n    A simple calculation yields that the value $\\gamma = \\log_2\\big(\\frac{\\ln Z_n}{\\ln(Z_n/2)}\\big)$\\gamma = \\log_2\\big(\\frac{\\ln Z_n}{\\ln(Z_n/2)}\\big) (which is $\\leq 1$\\leq 1 since $Z_n \\geq 4$Z_n \\geq 4) minimises the above right-hand side and thus leads to\n    \n        \\sum_{k=1}k=1^n \\frac{z_k}{Z_{k-1}} \n        &\\leq \\frac{Z_n^{\\log_2\\left(\\frac{\\ln Z_n}{\\ln(Z_n/2)}\\right)}}{\\frac{\\ln Z_n}{\\ln(Z_n/2)} - 1} = \\frac{\\ln(Z_n/2)}{\\ln 2}\\left(1 + \\frac{\\ln 2}{\\ln(Z_n/2)}\\right)^{\\log_2 Z_n}\\log_2 Z_n \\leq \\frac{4}{\\ln 2}\\ln(Z_n/2),\n    \n    where the last inequality uses that $\\big(1+\\frac{\\ln{2}}{\\ln(x/2)}\\big)^{\\log_2{x}}$\\big(1+\\frac{\\ln{2}}{\\ln(x/2)}\\big)^{\\log_2{x}}\\log_2{x}x is a decreasing function of $x$x and $Z_n \\geq 4$Z_n \\geq 4.\n\n\n\n\\subsection{Background on quantum computation}\n\\label{sec:quantum_computation}\n\nLittle background on quantum computation is needed for our paper and we refer the reader to~\\cite{nielsen2010quantum} for more information. The quantum state of a quantum system is described by a vector from a Hilbert space. A qubit, the quantum equivalent of a bit, is a quantum system described by a vector in $\\mathbb{C}^2$\\mathbb{C}^2, while an $n$n-qubit system is described by a vector in $\\mathbb{C}^{2^n}$\\mathbb{C}^{2^n}2^n. The evolution of a quantum state $|\\psi\\rangle\\in\\mathbb{C}^{2^n}$|\\psi\\rangle\\in\\mathbb{C}^{2^n}2^n is described by a unitary operator $U\\in\\mathbb{C}^{2^n\\times 2^n}$U\\in\\mathbb{C}^{2^n\\times 2^n}2^n\\times 2^n, $UU^\\dagger = I$UU^\\dagger = I where $U^\\dagger$U^\\dagger is the Hermitian conjugate of $U$U. In order to extract classical information from a quantum system, a quantum measurement is usually performed, which is a set $\\{E_m\\}_m$\\{E_m\\}_m of positive operators $E_m \\succ 0$E_m \\succ 0 that sum to identity, $\\sum_m E_m = I$\\sum_m E_m = I. The probability of measuring $E_m$E_m on $|\\psi\\rangle$|\\psi\\rangle is $p_m = \\langle\\psi|E_m|\\psi\\rangle$p_m = \\langle\\psi|E_m|\\psi\\rangle. We use $\\bar 0$\\bar 0 to denote the all-zeros vector and $\\ket{\\bar 0}$\\ket{\\bar 0} to denote the state $\\ket{0}\\otimes\\cdots\\otimes\\ket{0}$\\ket{0}\\otimes\\cdots\\otimes\\ket{0} where the number of qubits is clear from the context.\n\nIn this paper, we shall employ quantum oracles for functions $u\\in\\mathscr{B}(\\mathcal{X})$u\\in\\mathscr{B}(\\mathcal{X}) and probability distributions $p\\in\\mathscr{P}(\\mathcal{X})$p\\in\\mathscr{P}(\\mathcal{X}). We say we have quantum access to $u\\in\\mathscr{B}(\\mathcal{X})$u\\in\\mathscr{B}(\\mathcal{X}) if we have access to the oracle $\\mathcal{O}_u:|x\\rangle|\\bar{0}\\rangle \\mapsto |x\\rangle|u(x)\\rangle$\\mathcal{O}_u:|x\\rangle|\\bar{0}\\rangle \\mapsto |x\\rangle|u(x)\\rangle and its inverse, and we say we have quantum sampling access to $p\\in\\mathscr{P}(\\mathcal{X})$p\\in\\mathscr{P}(\\mathcal{X}) if we have access to the oracle $\\mathcal{O}_p:|\\bar{0}\\rangle \\mapsto \\int_{\\mathcal{X}} \\sqrt{p(\\rd x)}|x\\rangle$\\mathcal{O}_p:|\\bar{0}\\rangle \\mapsto \\int_{\\mathcal{X}}\\mathcal{X} \\sqrt{p(\\rd x)}|x\\rangle and its inverse. If $\\mathcal{X} = \\mathcal{S}$\\mathcal{X} = \\mathcal{S} is finite, then $\\mathcal{O}_p:|\\bar{0}\\rangle \\mapsto \\sum_{s\\in\\mathcal{S}} \\sqrt{p(s)}|s\\rangle$\\mathcal{O}_p:|\\bar{0}\\rangle \\mapsto \\sum_{s\\in\\mathcal{S}}s\\in\\mathcal{S} \\sqrt{p(s)}|s\\rangle. Quantum access to a function is usually referred to as a quantum random access memory (QRAM)~\\cite{giovannetti2008architectures,giovannetti2008quantum,jaques2023qram,allcock2023constant}. It is possible to build quantum access to a given $u\\in\\mathscr{B}(\\mathcal{X})$u\\in\\mathscr{B}(\\mathcal{X}) in $O(|\\mathcal{X}|)$O(|\\mathcal{X}|) time.\n\n\n\n\n\n\\subsection{Classical and quantum subroutines}\n\nIn this section, we present the quantum subroutines that will be used for the rest of the paper, starting with the minimum/maximum finding algorithm by D\\\"urr and H\\o{}yer~\\cite{durr1996quantum}.\n\\begin{fact}[Quantum max-finding \\cite{durr1996quantum}]\\label{fact:quantum_minimum_finding}\n    Given quantum access to $u\\in\\mathscr{B}(\\mathcal{X})$ via oracle $\\mathcal{O}_u$, one can find $\\max_{x\\in\\mathcal{X}} u(x)$ and $\\min_{x\\in\\mathcal{X}} u(x)$ with probability $1-\\delta$ using $O(\\sqrt{|\\mathcal{X}|}\\log \\frac{1}{\\delta})$ queries to $\\mathcal{O}_u$.\n\\end{fact}\\begin{fact}[Quantum max-finding \\cite{durr1996quantum}]\\label{fact:quantum_minimum_finding}\n    Given quantum access to $u\\in\\mathscr{B}(\\mathcal{X})$ via oracle $\\mathcal{O}_u$, one can find $\\max_{x\\in\\mathcal{X}} u(x)$ and $\\min_{x\\in\\mathcal{X}} u(x)$ with probability $1-\\delta$ using $O(\\sqrt{|\\mathcal{X}|}\\log \\frac{1}{\\delta})$ queries to $\\mathcal{O}_u$.\n\\end{fact}[Quantum max-finding \\cite{durr1996quantum}]\\label{fact:quantum_minimum_finding}\n    Given quantum access to $u\\in\\mathscr{B}(\\mathcal{X})$u\\in\\mathscr{B}(\\mathcal{X}) via oracle $\\mathcal{O}_u$\\mathcal{O}_u, one can find $\\max_{x\\in\\mathcal{X}} u(x)$\\max_{x\\in\\mathcal{X}}x\\in\\mathcal{X} u(x) and $\\min_{x\\in\\mathcal{X}} u(x)$\\min_{x\\in\\mathcal{X}}x\\in\\mathcal{X} u(x) with probability $1-\\delta$1-\\delta using $O(\\sqrt{|\\mathcal{X}|}\\log \\frac{1}{\\delta})$O(\\sqrt{|\\mathcal{X}|}\\log \\frac{1}{\\delta}) queries to $\\mathcal{O}_u$\\mathcal{O}_u.\n\n\nAnother important quantum subroutine is approximating the mean of some random variable. Several quantum mean estimation algorithms have been proposed~\\cite{montanaro2015quant,van2021quantum,hamoudi2021quantum,cornelissen2022near,Kothari2023mean}. Here we shall employ the univariate version due to Kothari and O'Donnell~\\cite{Kothari2023mean} and the multivariate version due to Cornelissen, Hamoudi, and Jerbi~\\cite{cornelissen2022near}, or more specifically, the improved version due to Tang~\\cite{tang2025more}.\n\n\\begin{fact}[Quantum mean estimation with variance {\\cite[Theorem~1.1]{Kothari2023mean}}]\\label{fact:quantum_mean_estimation_variance}\n    Let $\\epsilon>0$ and $\\delta\\in (0, 1)$. Assume quantum access to function $u:\\mathcal{X}\\to \\mathbb{R}$ via oracle $\\mathcal{O}_u$ and quantum sampling access to probability distribution $p\\in\\mathscr{P}(\\mathcal{X})$ via oracle $\\mathcal{O}_p$. Let $\\sigma := \\int_{\\mathcal{X}} p(\\rd x) u(x)^2 - \\big(\\int_{\\mathcal{X}} p(\\rd x) u(x)\\big)^2$. There is a quantum algorithm that computes $\\widetilde \\mu\\in\\mathbb{R}$ such that $| \\widetilde\\mu - \\int_{\\mathcal{X}} p(\\rd x) u(x)|\\leq \\sqrt{\\sigma}\\epsilon$ with success probability $1-\\delta$ using $O\\big(\\frac{1}{\\epsilon}\\log\\frac{1}{\\delta}\\big)$ queries to $\\mathcal{O}_u,\\mathcal{O}_p$ and their inverses.%, plus $O\\big(\\frac{\\sqrt{\\sigma}}{\\epsilon}\\log\\frac{1}{\\delta}\\big)$ elementary quantum gates. \n\\end{fact}\\begin{fact}[Quantum mean estimation with variance {\\cite[Theorem~1.1]{Kothari2023mean}}]\\label{fact:quantum_mean_estimation_variance}\n    Let $\\epsilon>0$ and $\\delta\\in (0, 1)$. Assume quantum access to function $u:\\mathcal{X}\\to \\mathbb{R}$ via oracle $\\mathcal{O}_u$ and quantum sampling access to probability distribution $p\\in\\mathscr{P}(\\mathcal{X})$ via oracle $\\mathcal{O}_p$. Let $\\sigma := \\int_{\\mathcal{X}} p(\\rd x) u(x)^2 - \\big(\\int_{\\mathcal{X}} p(\\rd x) u(x)\\big)^2$. There is a quantum algorithm that computes $\\widetilde \\mu\\in\\mathbb{R}$ such that $| \\widetilde\\mu - \\int_{\\mathcal{X}} p(\\rd x) u(x)|\\leq \\sqrt{\\sigma}\\epsilon$ with success probability $1-\\delta$ using $O\\big(\\frac{1}{\\epsilon}\\log\\frac{1}{\\delta}\\big)$ queries to $\\mathcal{O}_u,\\mathcal{O}_p$ and their inverses.%, plus $O\\big(\\frac{\\sqrt{\\sigma}}{\\epsilon}\\log\\frac{1}{\\delta}\\big)$ elementary quantum gates. \n\\end{fact}[Quantum mean estimation with variance {\\cite[Theorem~1.1]{Kothari2023mean}}\\cite[Theorem~1.1]{Kothari2023mean}]\\label{fact:quantum_mean_estimation_variance}\n    Let $\\epsilon>0$\\epsilon>0 and $\\delta\\in (0, 1)$\\delta\\in (0, 1). Assume quantum access to function $u:\\mathcal{X}\\to \\mathbb{R}$u:\\mathcal{X}\\to \\mathbb{R} via oracle $\\mathcal{O}_u$\\mathcal{O}_u and quantum sampling access to probability distribution $p\\in\\mathscr{P}(\\mathcal{X})$p\\in\\mathscr{P}(\\mathcal{X}) via oracle $\\mathcal{O}_p$\\mathcal{O}_p. Let $\\sigma := \\int_{\\mathcal{X}} p(\\rd x) u(x)^2 - \\big(\\int_{\\mathcal{X}} p(\\rd x) u(x)\\big)^2$\\sigma := \\int_{\\mathcal{X}}\\mathcal{X} p(\\rd x) u(x)^2 - \\big(\\int_{\\mathcal{X}}\\mathcal{X} p(\\rd x) u(x)\\big)^2. There is a quantum algorithm that computes $\\widetilde \\mu\\in\\mathbb{R}$\\widetilde \\mu\\in\\mathbb{R} such that $| \\widetilde\\mu - \\int_{\\mathcal{X}} p(\\rd x) u(x)|\\leq \\sqrt{\\sigma}\\epsilon$| \\widetilde\\mu - \\int_{\\mathcal{X}}\\mathcal{X} p(\\rd x) u(x)|\\leq \\sqrt{\\sigma}\\epsilon with success probability $1-\\delta$1-\\delta using $O\\big(\\frac{1}{\\epsilon}\\log\\frac{1}{\\delta}\\big)$O\\big(\\frac{1}{\\epsilon}\\log\\frac{1}{\\delta}\\big) queries to $\\mathcal{O}_u,\\mathcal{O}_p$\\mathcal{O}_u,\\mathcal{O}_p and their inverses.\n\n\\begin{corollary}[Quantum mean estimation]\\label{lem:quantum_mean_estimation}\n    Let $\\epsilon>0$ and $\\delta\\in (0, 1)$. Assume quantum access to function $u:\\mathcal{X}\\to \\mathbb{R}_+$ with known $\\min_{x\\in\\mathcal{X}}u(x)$ and $\\max_{x\\in\\mathcal{X}}u(x)$ via oracle $\\mathcal{O}_u$ and quantum sampling access to probability distribution $p\\in\\mathscr{P}(\\mathcal{X})$ via oracle $\\mathcal{O}_p$. There is a quantum algorithm that computes $\\widetilde \\mu \\in\\mathbb{R}$ such that $| \\widetilde\\mu - \\int_{X} p(\\rd x) u(x)|\\leq \\operatorname{sp}(u)\\epsilon$ with success probability $1-\\delta$ using $O\\big(\\frac{1}{\\epsilon}\\log\\frac{1}{\\delta}\\big)$ queries to $\\mathcal{O}_u,\\mathcal{O}_p$ and their inverses.%, plus $O\\big(\\frac{\\operatorname{sp}(u)}{\\epsilon}\\log\\frac{1}{\\delta}\\big)$ elementary quantum gates. \n\\end{corollary}\\begin{corollary}[Quantum mean estimation]\\label{lem:quantum_mean_estimation}\n    Let $\\epsilon>0$ and $\\delta\\in (0, 1)$. Assume quantum access to function $u:\\mathcal{X}\\to \\mathbb{R}_+$ with known $\\min_{x\\in\\mathcal{X}}u(x)$ and $\\max_{x\\in\\mathcal{X}}u(x)$ via oracle $\\mathcal{O}_u$ and quantum sampling access to probability distribution $p\\in\\mathscr{P}(\\mathcal{X})$ via oracle $\\mathcal{O}_p$. There is a quantum algorithm that computes $\\widetilde \\mu \\in\\mathbb{R}$ such that $| \\widetilde\\mu - \\int_{X} p(\\rd x) u(x)|\\leq \\operatorname{sp}(u)\\epsilon$ with success probability $1-\\delta$ using $O\\big(\\frac{1}{\\epsilon}\\log\\frac{1}{\\delta}\\big)$ queries to $\\mathcal{O}_u,\\mathcal{O}_p$ and their inverses.%, plus $O\\big(\\frac{\\operatorname{sp}(u)}{\\epsilon}\\log\\frac{1}{\\delta}\\big)$ elementary quantum gates. \n\\end{corollary}\\label{lem:quantum_mean_estimation}\n    Let $\\epsilon>0$\\epsilon>0 and $\\delta\\in (0, 1)$\\delta\\in (0, 1). Assume quantum access to function $u:\\mathcal{X}\\to \\mathbb{R}_+$u:\\mathcal{X}\\to \\mathbb{R}_+ with known $\\min_{x\\in\\mathcal{X}}u(x)$\\min_{x\\in\\mathcal{X}}x\\in\\mathcal{X}u(x) and $\\max_{x\\in\\mathcal{X}}u(x)$\\max_{x\\in\\mathcal{X}}x\\in\\mathcal{X}u(x) via oracle $\\mathcal{O}_u$\\mathcal{O}_u and quantum sampling access to probability distribution $p\\in\\mathscr{P}(\\mathcal{X})$p\\in\\mathscr{P}(\\mathcal{X}) via oracle $\\mathcal{O}_p$\\mathcal{O}_p. There is a quantum algorithm that computes $\\widetilde \\mu \\in\\mathbb{R}$\\widetilde \\mu \\in\\mathbb{R} such that $| \\widetilde\\mu - \\int_{X} p(\\rd x) u(x)|\\leq \\operatorname{sp}(u)\\epsilon$| \\widetilde\\mu - \\int_{X}X p(\\rd x) u(x)|\\leq \\operatorname{sp}sp(u)\\epsilon with success probability $1-\\delta$1-\\delta using $O\\big(\\frac{1}{\\epsilon}\\log\\frac{1}{\\delta}\\big)$O\\big(\\frac{1}{\\epsilon}\\log\\frac{1}{\\delta}\\big) queries to $\\mathcal{O}_u,\\mathcal{O}_p$\\mathcal{O}_u,\\mathcal{O}_p and their inverses.\n\n\\begin{fact}[Quantum multidimensional mean estimation with variance~{\\cite[Theorem~I.2]{tang2025more}}]\\label{fact:quantum_multidimensional_mean_estimation_covariance_matrix}\n    Let $\\epsilon > 0$ and $\\delta\\in (0, 1)$. Assume quantum access to $n$-dimensional function $f:\\mathcal{X}\\to \\mathbb{R}^n$ via oracle $\\mathcal{O}_f$ and quantum sampling access to probability distribution $p\\in\\mathscr{P}(\\mathcal{X})$ via oracle $\\mathcal{O}_p$. Let $\\sigma := \\sum_{i\\in[n]}\\int_{\\mathcal{X}} p(\\rd x) f_i(x)^2 - \\big(\\int_{\\mathcal{X}} p(\\rd x) f_i(x)\\big)^2$. There is a quantum algorithm that computes $\\widetilde\\mu\\in\\mathbb{R}^n$ such that $\\|\\widetilde\\mu - \\int_{X} p(\\rd x) f(x)\\|_\\infty \\leq \\sqrt{\\sigma}\\epsilon$ with success probability $1 - \\delta$ using $O\\big(\\frac{1}{\\epsilon}\\log\\frac{n}{\\delta}\\big)$ quantum queries to $\\mathcal{O}_p, \\mathcal{O}_f$ and their inverses.\n\\end{fact}\\begin{fact}[Quantum multidimensional mean estimation with variance~{\\cite[Theorem~I.2]{tang2025more}}]\\label{fact:quantum_multidimensional_mean_estimation_covariance_matrix}\n    Let $\\epsilon > 0$ and $\\delta\\in (0, 1)$. Assume quantum access to $n$-dimensional function $f:\\mathcal{X}\\to \\mathbb{R}^n$ via oracle $\\mathcal{O}_f$ and quantum sampling access to probability distribution $p\\in\\mathscr{P}(\\mathcal{X})$ via oracle $\\mathcal{O}_p$. Let $\\sigma := \\sum_{i\\in[n]}\\int_{\\mathcal{X}} p(\\rd x) f_i(x)^2 - \\big(\\int_{\\mathcal{X}} p(\\rd x) f_i(x)\\big)^2$. There is a quantum algorithm that computes $\\widetilde\\mu\\in\\mathbb{R}^n$ such that $\\|\\widetilde\\mu - \\int_{X} p(\\rd x) f(x)\\|_\\infty \\leq \\sqrt{\\sigma}\\epsilon$ with success probability $1 - \\delta$ using $O\\big(\\frac{1}{\\epsilon}\\log\\frac{n}{\\delta}\\big)$ quantum queries to $\\mathcal{O}_p, \\mathcal{O}_f$ and their inverses.\n\\end{fact}[Quantum multidimensional mean estimation with variance~{\\cite[Theorem~I.2]{tang2025more}}\\cite[Theorem~I.2]{tang2025more}]\\label{fact:quantum_multidimensional_mean_estimation_covariance_matrix}\n    Let $\\epsilon > 0$\\epsilon > 0 and $\\delta\\in (0, 1)$\\delta\\in (0, 1). Assume quantum access to $n$n-dimensional function $f:\\mathcal{X}\\to \\mathbb{R}^n$f:\\mathcal{X}\\to \\mathbb{R}^n via oracle $\\mathcal{O}_f$\\mathcal{O}_f and quantum sampling access to probability distribution $p\\in\\mathscr{P}(\\mathcal{X})$p\\in\\mathscr{P}(\\mathcal{X}) via oracle $\\mathcal{O}_p$\\mathcal{O}_p. Let $\\sigma := \\sum_{i\\in[n]}\\int_{\\mathcal{X}} p(\\rd x) f_i(x)^2 - \\big(\\int_{\\mathcal{X}} p(\\rd x) f_i(x)\\big)^2$\\sigma := \\sum_{i\\in[n]}i\\in[n]\\int_{\\mathcal{X}}\\mathcal{X} p(\\rd x) f_i(x)^2 - \\big(\\int_{\\mathcal{X}}\\mathcal{X} p(\\rd x) f_i(x)\\big)^2. There is a quantum algorithm that computes $\\widetilde\\mu\\in\\mathbb{R}^n$\\widetilde\\mu\\in\\mathbb{R}^n such that $\\|\\widetilde\\mu - \\int_{X} p(\\rd x) f(x)\\|_\\infty \\leq \\sqrt{\\sigma}\\epsilon$\\|\\widetilde\\mu - \\int_{X}X p(\\rd x) f(x)\\|_\\infty \\leq \\sqrt{\\sigma}\\epsilon with success probability $1 - \\delta$1 - \\delta using $O\\big(\\frac{1}{\\epsilon}\\log\\frac{n}{\\delta}\\big)$O\\big(\\frac{1}{\\epsilon}\\log\\frac{n}{\\delta}\\big) quantum queries to $\\mathcal{O}_p, \\mathcal{O}_f$\\mathcal{O}_p, \\mathcal{O}_f and their inverses.\n\n\nFinally, we shall make use of Hoeffding and Bernstein inequalities.\n\n\\begin{fact}[Hoeffding inequality]\\label{fact:hoeffding}\n    Let $\\delta\\in(0,1)$, $u\\in\\mathscr{B}(\\mathcal{X})$, and $x_1,\\dots,x_m\\in\\mathcal{X}$ samples from the distribution $p\\in\\mathscr{P}(\\mathcal{X})$. Then\n    %\n    \\begin{align*}\n        \\operatorname{Pr}\\left[\\left|\\frac{1}{m}\\sum_{i=1}^m u(x_i) - \\int_{\\mathcal{X}} p(\\rd x) u(x) \\right| \\leq \\|u\\|_\\infty\\sqrt{\\frac{2}{m}\\ln\\frac{2}{\\delta}} \\right] \\geq 1-\\delta.\n    \\end{align*}\n\\end{fact}\\begin{fact}[Hoeffding inequality]\\label{fact:hoeffding}\n    Let $\\delta\\in(0,1)$, $u\\in\\mathscr{B}(\\mathcal{X})$, and $x_1,\\dots,x_m\\in\\mathcal{X}$ samples from the distribution $p\\in\\mathscr{P}(\\mathcal{X})$. Then\n    %\n    \\begin{align*}\n        \\operatorname{Pr}\\left[\\left|\\frac{1}{m}\\sum_{i=1}^m u(x_i) - \\int_{\\mathcal{X}} p(\\rd x) u(x) \\right| \\leq \\|u\\|_\\infty\\sqrt{\\frac{2}{m}\\ln\\frac{2}{\\delta}} \\right] \\geq 1-\\delta.\n    \\end{align*}\n\\end{fact}[Hoeffding inequality]\\label{fact:hoeffding}\n    Let $\\delta\\in(0,1)$\\delta\\in(0,1), $u\\in\\mathscr{B}(\\mathcal{X})$u\\in\\mathscr{B}(\\mathcal{X}), and $x_1,\\dots,x_m\\in\\mathcal{X}$x_1,\\dots,x_m\\in\\mathcal{X} samples from the distribution $p\\in\\mathscr{P}(\\mathcal{X})$p\\in\\mathscr{P}(\\mathcal{X}). Then\n    \n        \\operatorname{Pr}Pr\\left[\\left|\\frac{1}{m}\\sum_{i=1}i=1^m u(x_i) - \\int_{\\mathcal{X}}\\mathcal{X} p(\\rd x) u(x) \\right| \\leq \\|u\\|_\\infty\\sqrt{\\frac{2}{m}\\ln\\frac{2}{\\delta}} \\right] \\geq 1-\\delta.\n    \n\n\n\\begin{fact}[Bernstein inequality]\\label{fact:bernstein}\n    Let $\\delta\\in(0,1)$, $u\\in\\mathscr{B}(\\mathcal{X})$, and $x_1,\\dots,x_m\\in\\mathcal{X}$ samples from the distribution $p\\in\\mathscr{P}(\\mathcal{X})$. Let $\\sigma := \\int_{\\mathcal{X}} p(\\rd x) u(x)^2 - \\big(\\int_{\\mathcal{X}} p(\\rd x) u(x)\\big)^2$. Then\n    %\n    \\begin{align*}\n        \\operatorname{Pr}\\left[\\left|\\frac{1}{m}\\sum_{i=1}^m u(x_i) - \\int_{\\mathcal{X}} p(\\rd x) u(x) \\right| \\leq \\sqrt{\\frac{2\\sigma}{m}\\ln\\frac{2}{\\delta}} + \\frac{2\\|u\\|_\\infty}{3m}\\ln\\frac{2}{\\delta} \\right] \\geq 1-\\delta.\n    \\end{align*}\n\\end{fact}\\begin{fact}[Bernstein inequality]\\label{fact:bernstein}\n    Let $\\delta\\in(0,1)$, $u\\in\\mathscr{B}(\\mathcal{X})$, and $x_1,\\dots,x_m\\in\\mathcal{X}$ samples from the distribution $p\\in\\mathscr{P}(\\mathcal{X})$. Let $\\sigma := \\int_{\\mathcal{X}} p(\\rd x) u(x)^2 - \\big(\\int_{\\mathcal{X}} p(\\rd x) u(x)\\big)^2$. Then\n    %\n    \\begin{align*}\n        \\operatorname{Pr}\\left[\\left|\\frac{1}{m}\\sum_{i=1}^m u(x_i) - \\int_{\\mathcal{X}} p(\\rd x) u(x) \\right| \\leq \\sqrt{\\frac{2\\sigma}{m}\\ln\\frac{2}{\\delta}} + \\frac{2\\|u\\|_\\infty}{3m}\\ln\\frac{2}{\\delta} \\right] \\geq 1-\\delta.\n    \\end{align*}\n\\end{fact}[Bernstein inequality]\\label{fact:bernstein}\n    Let $\\delta\\in(0,1)$\\delta\\in(0,1), $u\\in\\mathscr{B}(\\mathcal{X})$u\\in\\mathscr{B}(\\mathcal{X}), and $x_1,\\dots,x_m\\in\\mathcal{X}$x_1,\\dots,x_m\\in\\mathcal{X} samples from the distribution $p\\in\\mathscr{P}(\\mathcal{X})$p\\in\\mathscr{P}(\\mathcal{X}). Let $\\sigma := \\int_{\\mathcal{X}} p(\\rd x) u(x)^2 - \\big(\\int_{\\mathcal{X}} p(\\rd x) u(x)\\big)^2$\\sigma := \\int_{\\mathcal{X}}\\mathcal{X} p(\\rd x) u(x)^2 - \\big(\\int_{\\mathcal{X}}\\mathcal{X} p(\\rd x) u(x)\\big)^2. Then\n    \n        \\operatorname{Pr}Pr\\left[\\left|\\frac{1}{m}\\sum_{i=1}i=1^m u(x_i) - \\int_{\\mathcal{X}}\\mathcal{X} p(\\rd x) u(x) \\right| \\leq \\sqrt{\\frac{2\\sigma}{m}\\ln\\frac{2}{\\delta}} + \\frac{2\\|u\\|_\\infty}{3m}\\ln\\frac{2}{\\delta} \\right] \\geq 1-\\delta.\n    \n\n\n\\subsection{Background on Markov decision processes}\n\\label{sec:mdp}\n\nIn this paper, we are concerned with two types of discrete-time Markov decision processes (MDPs): \\emph{finite-horizon} and \\emph{infinite-horizon} MDPs. A finite-horizon MDP $M$M is described by a five-tuple $\\langle\\mathcal{X}, \\mathcal{A}, H, p, r\\rangle$\\langle\\mathcal{X}, \\mathcal{A}, H, p, r\\rangle, while an infinite-horizon MDP $M$M is described by a four-tuple $\\langle\\mathcal{X}, \\mathcal{A}, p, r\\rangle$\\langle\\mathcal{X}, \\mathcal{A}, p, r\\rangle. The Borel spaces $\\mathcal X$\\mathcal X and $\\mathcal A$\\mathcal A denote the \\emph{state} and \\emph{action} spaces, respectively.\\footnote{Some authors also include the set $\\{\\mathcal A_x:x\\in\\mathcal X\\}$ of allowable actions in state $x\\in\\mathcal{X}$ in the definition of an MDP. Here we assume for simplicity that $\\mathcal{A}_x = \\mathcal{A}$ for all $x\\in\\mathcal{X}$.} We assume that $\\mathcal{X}$\\mathcal{X} is compact and $\\mathcal{A}$\\mathcal{A} is finite with size $A$A. The horizon $H$H denotes the length or number of steps of the MDP. For infinite-horizon MDPs, $H=\\infty$H=\\infty. The \\emph{reward function} $r:\\mathcal X\\times \\mathcal A\\rightarrow \\mathbb [0,1]$r:\\mathcal X\\times \\mathcal A\\rightarrow \\mathbb [0,1] is a measurable function and the \\emph{stochastic kernels} $p(\\cdot\\vert x, a)$p(\\cdot\\vert x, a) denote the \\emph{transition probabilities} to the next state given the current state-action pair $(x, a)$(x, a).\\footnote{For finite-horizon MDPs, the reward function and stochastic kernels can depend on the time step as $r:\\mathcal{X}\\times\\mathcal{A}\\times[H]\\to[0,1]$ and $p:\\mathcal{X}\\times\\mathcal{A}\\times[H]\\to\\mathscr{P}(\\mathcal{X})$. We assume for simplicity that their form is time independent.} For simplicity, we assume that $p(\\cdot\\vert x, a)$p(\\cdot\\vert x, a) is weakly continuous in $(x, a)\\in\\mathcal X\\times\\mathcal A$(x, a)\\in\\mathcal X\\times\\mathcal A. In this work, we shall make a distinction between finite and infinite state spaces. For such, we reserve the letter $\\mathcal{S}$\\mathcal{S} for a finite state space, while $\\mathcal{X}$\\mathcal{X} can be an arbitrary compact state space. \n\nAn MDP models the interaction between an agent and the reinforcement learning environment. At any time step $t\\in\\mathbb{N}$t\\in\\mathbb{N}, the agent in a particular state $x_t\\in\\mathcal{X}$x_t\\in\\mathcal{X} chooses an action $a_t\\in\\mathcal{A}$a_t\\in\\mathcal{A}, obtains a reward $r(x_t,a_t)$r(x_t,a_t), and moves to a new state $x_{t+1}\\in\\mathcal{X}$x_{t+1}t+1\\in\\mathcal{X} with probability $p(x_{t+1}|x_t,a_t)$p(x_{t+1}t+1|x_t,a_t). For finite-horizon MDPs, this interaction is performed $H$H times, while for infinite-horizon MDPs, this interaction can be performed indefinitely. The agent chooses an action $a_t\\in\\mathcal{A}$a_t\\in\\mathcal{A} according to a \\emph{randomised Markovian decision rule} $\\pi_t:\\mathcal{X}\\to\\mathscr{P}(\\mathcal{A})$\\pi_t:\\mathcal{X}\\to\\mathscr{P}(\\mathcal{A}), i.e., a stochastic kernel on $\\mathcal{A}$\\mathcal{A} given $\\mathcal{X}$\\mathcal{X}. Instead of writing $\\pi_t(x)(\\cdot)\\in\\mathscr{P}(\\mathcal{A})$\\pi_t(x)(\\cdot)\\in\\mathscr{P}(\\mathcal{A}) we employ the notation $\\pi_t(\\cdot|x)$\\pi_t(\\cdot|x). A \\emph{randomised Markovian policy} is a sequence $\\pi=(\\pi_t)_{t}$\\pi=(\\pi_t)_{t}t of randomised Markovian decision rules on $\\mathcal{A}$\\mathcal{A} given $\\mathcal{X}$\\mathcal{X}.\\footnote{It is possible to consider history-dependent policies. The choice for Markovian policies is without loss of generality since we can construct a randomised Markov policy from a history-dependent policy with the same probability distribution of states and actions~\\cite[Theorem~5.5.1]{puterman2014markov}.} We say that a policy is \\emph{deterministic} if it is a sequence of stochastic kernels $\\pi=(\\pi_t)_{t}$\\pi=(\\pi_t)_{t}t on $\\mathcal A$\\mathcal A given $\\mathcal X$\\mathcal X such that $\\pi_t(a\\vert x) = 1$\\pi_t(a\\vert x) = 1 for some $a\\in\\mathcal{A}$a\\in\\mathcal{A}. A policy is said to be \\emph{stationary} if it is a constant sequence $\\pi=(\\pi_t)_{t}$\\pi=(\\pi_t)_{t}t of decision rules on $\\mathcal A$\\mathcal A given $\\mathcal X$\\mathcal X such that $\\pi_t = \\pi_{t'}$\\pi_t = \\pi_{t'}t' for all $t,t'$t,t'. Given a decision rule $d$d, we shall employ the notation $d^\\infty$d^\\infty for the stationary policy $\\pi = (d,d,\\dots)$\\pi = (d,d,\\dots) in infinite-horizon MDPs. The set of all randomised and deterministic decision rules are denoted by $\\mathcal{D}^{\\rm R}$\\mathcal{D}^{\\rm R}\\rm R and $\\mathcal{D}^{\\rm D}$\\mathcal{D}^{\\rm D}\\rm D, and the set of all randomised and deterministic policies are denoted by $\\Pi^{\\rm R}$\\Pi^{\\rm R}\\rm R and $\\Pi^{\\rm D}$\\Pi^{\\rm D}\\rm D, respectively. Given $d\\in\\mathcal{D}^{\\rm R}$d\\in\\mathcal{D}^{\\rm R}\\rm R, define $r_{d}(x)$r_{d}d(x) and $p_{d}(x'|x)$p_{d}d(x'|x) by\n\\begin{align*}\n    r_{d}(x) := \\operatorname*{\\mathbb{E}}_{a\\sim d(\\cdot|x)}[r(x,a)] \\qquad\\text{and} \\qquad p_{d}(x'|x) := \\operatorname*{\\mathbb{E}}_{a\\sim d(\\cdot|x)}[p(x'|x,a)].\n\\end{align*}\\begin{align*}\n    r_{d}(x) := \\operatorname*{\\mathbb{E}}_{a\\sim d(\\cdot|x)}[r(x,a)] \\qquad\\text{and} \\qquad p_{d}(x'|x) := \\operatorname*{\\mathbb{E}}_{a\\sim d(\\cdot|x)}[p(x'|x,a)].\n\\end{align*}\n    r_{d}d(x) := \\operatorname*{\\mathbb{E}}\\mathbb{E}_{a\\sim d(\\cdot|x)}a\\sim d(\\cdot|x)[r(x,a)] \\qquad\\text{and} \\qquad p_{d}d(x'|x) := \\operatorname*{\\mathbb{E}}\\mathbb{E}_{a\\sim d(\\cdot|x)}a\\sim d(\\cdot|x)[p(x'|x,a)].\n\n\n\nIn this work, we shall consider \\emph{weakly communicating} infinite-horizon MDPs~\\cite[Section~8.3]{puterman2014markov} (there is no need to constrain the class of finite-horizon MDPs). We say that an infinite-horizon MDP is weakly communicating if there is a \\emph{closed} set of states, with each state in that set accessible from every other state in that set under some deterministic stationary policy, plus a possibly empty set of states which is transient under every policy. Here, a state $x\\in\\mathcal{X}$x\\in\\mathcal{X} is \\emph{transient} if $\\mathbb{E}[\\tau_x] < \\infty$\\mathbb{E}[\\tau_x] < \\infty, where the random variable $\\tau_x$\\tau_x represents the number of visits to state $x$x.\n\nA policy $\\pi = (\\pi_{t})_{t}$\\pi = (\\pi_{t}t)_{t}t together with an initial distribution $\\mu$\\mu of the system state induces a probability measure $P_\\mu^\\pi$P_\\mu^\\pi on $(\\Omega, \\mathscr{M}(\\Omega))$(\\Omega, \\mathscr{M}(\\Omega)), where $\\Omega = (\\mathcal X\\times\\mathcal A)^H$\\Omega = (\\mathcal X\\times\\mathcal A)^H (finite-horizon) or $\\Omega = (\\mathcal X\\times\\mathcal A)^\\infty$\\Omega = (\\mathcal X\\times\\mathcal A)^\\infty (infinite-horizon), through\n\\begin{align*}\n    P_\\mu^\\pi(X_1 = x) &= \\mu(x), \\\\\n    P_\\mu^\\pi(A_t = a|X_t = x_t) &= \\pi_{t}(a|x_{t}),\\\\\n    P_\\mu^\\pi(X_{t+1} = x|X_{t} = x_{t}, A_{t} = a_{t}) &= p(x|x_{t},a_{t}).\n\\end{align*}\\begin{align*}\n    P_\\mu^\\pi(X_1 = x) &= \\mu(x), \\\\\n    P_\\mu^\\pi(A_t = a|X_t = x_t) &= \\pi_{t}(a|x_{t}),\\\\\n    P_\\mu^\\pi(X_{t+1} = x|X_{t} = x_{t}, A_{t} = a_{t}) &= p(x|x_{t},a_{t}).\n\\end{align*}\n    P_\\mu^\\pi(X_1 = x) &= \\mu(x), \\\\\n    P_\\mu^\\pi(A_t = a|X_t = x_t) &= \\pi_{t}t(a|x_{t}t),\\\\\n    P_\\mu^\\pi(X_{t+1}t+1 = x|X_{t}t = x_{t}t, A_{t}t = a_{t}t) &= p(x|x_{t}t,a_{t}t).\n\nWe denote by $\\mathbb{E}_\\mu^\\pi$\\mathbb{E}_\\mu^\\pi the expectation with respect to $P_\\mu^\\pi$P_\\mu^\\pi. If $\\mu = \\delta_x$\\mu = \\delta_x, we write simply $P_x^\\pi$P_x^\\pi and $\\mathbb{E}_x^\\pi$\\mathbb{E}_x^\\pi.\n\n\n\\subsubsection{Finite-horizon MDPs}\n\nFor finite-horizon MPDs with horizon $H$H, the standard criterion to evaluate the performance of an agent in a RL environment is the \\emph{expected total reward} criterion. The expected total reward $V_1^\\pi(x)$V_1^\\pi(x) over the decision making horizon $H$H when executing policy $\\pi$\\pi with initial state $x\\in\\mathcal{X}$x\\in\\mathcal{X} is \n\\begin{align*}\n    V_1^\\pi(x) := \\mathbb{E}_x^\\pi\\left[\\sum_{t=1}^{H} r(x_{t},a_{t})\\right].\n\\end{align*}\\begin{align*}\n    V_1^\\pi(x) := \\mathbb{E}_x^\\pi\\left[\\sum_{t=1}^{H} r(x_{t},a_{t})\\right].\n\\end{align*}\n    V_1^\\pi(x) := \\mathbb{E}_x^\\pi\\left[\\sum_{t=1}t=1^{H}H r(x_{t}t,a_{t}t)\\right].\n\nMore generally, for $t\\in[H]$t\\in[H], we define the \\emph{value function} $V_t^\\pi:\\mathcal{X}\\to\\mathbb{R}$V_t^\\pi:\\mathcal{X}\\to\\mathbb{R} as the total expected reward obtained by using policy $\\pi\\in\\Pi^{\\rm R}$\\pi\\in\\Pi^{\\rm R}\\rm R at time steps $t,t+1,\\dots, H$t,t+1,\\dots, H, i.e.,\n\\begin{align*}\n    V_t^\\pi(x) := \\mathbb{E}^\\pi_{x_{t}=x}\\left[\\sum_{t'=t}^{H} r(x_{t'},a_{t'}) \\right] \\qquad\\forall x\\in\\mathcal{X}.\n\\end{align*}\\begin{align*}\n    V_t^\\pi(x) := \\mathbb{E}^\\pi_{x_{t}=x}\\left[\\sum_{t'=t}^{H} r(x_{t'},a_{t'}) \\right] \\qquad\\forall x\\in\\mathcal{X}.\n\\end{align*}\n    V_t^\\pi(x) := \\mathbb{E}^\\pi_{x_{t}=x}x_{t}t=x\\left[\\sum_{t'=t}t'=t^{H}H r(x_{t'}t',a_{t'}t') \\right] \\qquad\\forall x\\in\\mathcal{X}.\n\nWe say that a policy $\\pi_\\varepsilon\\in\\Pi^{\\rm R}$\\pi_\\varepsilon\\in\\Pi^{\\rm R}\\rm R is $\\varepsilon$\\varepsilon-optimal for $\\varepsilon\\geq 0$\\varepsilon\\geq 0 if\n\\begin{align*}\n    V^{\\pi_\\varepsilon}_1(x) \\geq V^\\pi_1(x) - \\varepsilon \\qquad\\forall x\\in\\mathcal{X}, \\pi\\in\\Pi^{\\rm R},\n\\end{align*}\\begin{align*}\n    V^{\\pi_\\varepsilon}_1(x) \\geq V^\\pi_1(x) - \\varepsilon \\qquad\\forall x\\in\\mathcal{X}, \\pi\\in\\Pi^{\\rm R},\n\\end{align*}\n    V^{\\pi_\\varepsilon}\\pi_\\varepsilon_1(x) \\geq V^\\pi_1(x) - \\varepsilon \\qquad\\forall x\\in\\mathcal{X}, \\pi\\in\\Pi^{\\rm R}\\rm R,\n\nand a policy $\\pi^\\ast\\in\\Pi^{\\rm R}$\\pi^\\ast\\in\\Pi^{\\rm R}\\rm R is \\emph{optimal} if $V^{\\pi^\\ast}_1(x) \\geq V^{\\pi}_1(x)$V^{\\pi^\\ast}\\pi^\\ast_1(x) \\geq V^{\\pi}\\pi_1(x) for all $x\\in\\mathcal{X},\\pi\\in\\Pi^{\\rm R}$x\\in\\mathcal{X},\\pi\\in\\Pi^{\\rm R}\\rm R. The optimal total expected reward (or optimal value) $V^\\ast_t(x)$V^\\ast_t(x), for $t\\in[H]$t\\in[H], on initial state $x\\in\\mathcal{X}$x\\in\\mathcal{X} is defined as\n\\begin{align*}\n    V^\\ast_t(x) := \\sup_{\\pi\\in\\Pi^{\\rm R}} V^\\pi_t(x).\n\\end{align*}\\begin{align*}\n    V^\\ast_t(x) := \\sup_{\\pi\\in\\Pi^{\\rm R}} V^\\pi_t(x).\n\\end{align*}\n    V^\\ast_t(x) := \\sup_{\\pi\\in\\Pi^{\\rm R}}\\pi\\in\\Pi^{\\rm R}\\rm R V^\\pi_t(x).\n\n\nWe let $\\mathcal{L}_d:\\mathscr{B}(\\mathcal{X})\\to \\mathscr{B}(\\mathcal{X})$\\mathcal{L}_d:\\mathscr{B}(\\mathcal{X})\\to \\mathscr{B}(\\mathcal{X}) be the Bellman operator associated with decision rule $d\\in\\mathcal{D}^{\\rm R}$d\\in\\mathcal{D}^{\\rm R}\\rm R and $\\mathcal{L}:\\mathscr{B}(\\mathcal{X})\\to \\mathscr{B}(\\mathcal{X})$\\mathcal{L}:\\mathscr{B}(\\mathcal{X})\\to \\mathscr{B}(\\mathcal{X}) be the optimal Bellman operator defined as\n\\begin{align*}\n    \\forall u\\in\\mathscr{B}(\\mathcal{X}), x\\in\\mathcal{X} :\\quad (\\mathcal{L}_d u)(x) := r_d(x) + \\int_{\\mathcal{X}} \\! p_d(\\rd x'|x) u(x')  \\quad\\text{and}\\quad (\\mathcal{L}u)(x) := \\max_{d\\in\\mathcal{D}^{\\rm D}}\\{(\\mathcal{L}_d u)(x)\\}.\n\\end{align*}\\begin{align*}\n    \\forall u\\in\\mathscr{B}(\\mathcal{X}), x\\in\\mathcal{X} :\\quad (\\mathcal{L}_d u)(x) := r_d(x) + \\int_{\\mathcal{X}} \\! p_d(\\rd x'|x) u(x')  \\quad\\text{and}\\quad (\\mathcal{L}u)(x) := \\max_{d\\in\\mathcal{D}^{\\rm D}}\\{(\\mathcal{L}_d u)(x)\\}.\n\\end{align*}\n    \\forall u\\in\\mathscr{B}(\\mathcal{X}), x\\in\\mathcal{X} :\\quad (\\mathcal{L}_d u)(x) := r_d(x) + \\int_{\\mathcal{X}}\\mathcal{X} \\! p_d(\\rd x'|x) u(x')  \\quad\\text{and}\\quad (\\mathcal{L}u)(x) := \\max_{d\\in\\mathcal{D}^{\\rm D}}d\\in\\mathcal{D}^{\\rm D}\\rm D\\{(\\mathcal{L}_d u)(x)\\}.\n\nWe also write $\\mathcal{L}_d = \\mathcal{L}_a$\\mathcal{L}_d = \\mathcal{L}_a for a decision rule $d(x) = a\\in\\mathcal{A}$d(x) = a\\in\\mathcal{A}. Note that $\\mathcal{L}u = \\max_{d\\in\\mathcal{D}^{\\rm D}}\\mathcal{L}_d u = \\max_{a\\in\\mathcal{A}}\\mathcal{L}_a u$\\mathcal{L}u = \\max_{d\\in\\mathcal{D}^{\\rm D}}d\\in\\mathcal{D}^{\\rm D}\\rm D\\mathcal{L}_d u = \\max_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\mathcal{L}_a u. The operator $\\mathcal{L}$\\mathcal{L} is monotonic, i.e., $u\\leq v \\implies \\mathcal{L}u \\leq \\mathcal{L}v$u\\leq v \\implies \\mathcal{L}u \\leq \\mathcal{L}v, and is non-expansive, i.e., $\\operatorname{sp}(\\mathcal{L}u - \\mathcal{L}v) \\leq \\operatorname{sp}(u - v)$\\operatorname{sp}sp(\\mathcal{L}u - \\mathcal{L}v) \\leq \\operatorname{sp}sp(u - v) and $\\|\\mathcal{L}u - \\mathcal{L}v\\|_\\infty \\leq \\|u - v\\|_\\infty$\\|\\mathcal{L}u - \\mathcal{L}v\\|_\\infty \\leq \\|u - v\\|_\\infty for all $u,v\\in\\mathscr{B}(\\mathcal{X})$u,v\\in\\mathscr{B}(\\mathcal{X})~\\cite[Proposition~6.2.4]{puterman2014markov}. In order to derive convergence results, one usually assumes that $\\mathcal{L}$\\mathcal{L} is also a span contraction.\n\\begin{definition}[$J$-stage $\\nu$-span contraction]\n    Given $J\\in\\mathbb{N}$ and $\\nu\\in[0,1)$, we say that an operator $\\mathcal N\\!:\\!\\mathscr{B}(\\mathcal{X})\\to \\mathscr{B}(\\mathcal{X})$ is a $J$-stage $\\nu$-span contraction if $\\operatorname{sp}(\\mathcal N^J u - \\mathcal N^J  v) \\leq  \\nu \\operatorname{sp}(u - v)$ $\\forall u,v\\in\\mathscr{B}(\\mathcal{X})$.\n\\end{definition}\\begin{definition}[$J$-stage $\\nu$-span contraction]\n    Given $J\\in\\mathbb{N}$ and $\\nu\\in[0,1)$, we say that an operator $\\mathcal N\\!:\\!\\mathscr{B}(\\mathcal{X})\\to \\mathscr{B}(\\mathcal{X})$ is a $J$-stage $\\nu$-span contraction if $\\operatorname{sp}(\\mathcal N^J u - \\mathcal N^J  v) \\leq  \\nu \\operatorname{sp}(u - v)$ $\\forall u,v\\in\\mathscr{B}(\\mathcal{X})$.\n\\end{definition}\n    Given $J\\in\\mathbb{N}$J\\in\\mathbb{N} and $\\nu\\in[0,1)$\\nu\\in[0,1), we say that an operator $\\mathcal N\\!:\\!\\mathscr{B}(\\mathcal{X})\\to \\mathscr{B}(\\mathcal{X})$\\mathcal N\\!:\\!\\mathscr{B}(\\mathcal{X})\\to \\mathscr{B}(\\mathcal{X}) is a $J$J-stage $\\nu$\\nu-span contraction if $\\operatorname{sp}(\\mathcal N^J u - \\mathcal N^J  v) \\leq  \\nu \\operatorname{sp}(u - v)$\\operatorname{sp}sp(\\mathcal N^J u - \\mathcal N^J  v) \\leq  \\nu \\operatorname{sp}sp(u - v) $\\forall u,v\\in\\mathscr{B}(\\mathcal{X})$\\forall u,v\\in\\mathscr{B}(\\mathcal{X}).\n\nUnder some conditions on the stochastic kernel $p$p, one can show that $\\mathcal{L}$\\mathcal{L} is a $J$J-stage span contraction, see, e.g.,~\\cite[Theorems~8.5.2 \\&~8.5.3]{puterman2014markov}. \n\nThe equations\n\\begin{align*}\n    u_{H+1} \\equiv 0 \\qquad\\text{and}\\qquad u_{t} = \\mathcal{L}u_{t+1}, \\quad t\\in[H], \n\\end{align*}\\begin{align*}\n    u_{H+1} \\equiv 0 \\qquad\\text{and}\\qquad u_{t} = \\mathcal{L}u_{t+1}, \\quad t\\in[H], \n\\end{align*}\n    u_{H+1}H+1 \\equiv 0 \\qquad\\text{and}\\qquad u_{t}t = \\mathcal{L}u_{t+1}t+1, \\quad t\\in[H], \n\nare called \\emph{optimality equations}. It is well known~\\cite[Theorem~4.5.1]{puterman2014markov} that any set of solutions $\\{u_{t}\\}_{t\\in[H]}\\subset\\mathscr{B}(\\mathcal{X})$\\{u_{t}t\\}_{t\\in[H]}t\\in[H]\\subset\\mathscr{B}(\\mathcal{X}) to the optimality equations are such that $u_{t}(x) = V_t^\\ast(x)$u_{t}t(x) = V_t^\\ast(x) $\\forall x\\in\\mathcal{X},t\\in[H]$\\forall x\\in\\mathcal{X},t\\in[H].\n\n\\subsubsection{Infinite-horizon MDPs}\n\nFor infinite-horizon MDPs, the chosen criterion in this paper to evaluate the performance of an agent in a RL environment is the \\emph{average reward} criterion. The average reward (or gain) $g^{\\pi}(x)$g^{\\pi}\\pi(x) when executing policy $\\pi$\\pi with initial state $x\\in\\mathcal{X}$x\\in\\mathcal{X} is defined by\\footnote{More generally, the average reward is defined as $g^{\\pi}(x) = \\limsup_{T\\to\\infty} \\frac{1}{T}\\mathbb{E}_x^\\pi\\big[\\sum_{t=1}^{T}r(x_t, a_t)\\big]$.}\n\\begin{align*}\n    g^{\\pi}(x) := \\lim_{T\\to\\infty} \\mathbb{E}_x^\\pi\\left[\\frac{1}{T}\\sum_{t=1}^{T}r(x_t, a_t)\\right]. %= \\lim_{T\\to\\infty}\\frac{1}{T}\\sum_{t=0}^{T-1} \\int_{\\mathcal{X}} r_{\\pi^{(t)}}(y)p^{(t)}(\\rd y|x).\n\\end{align*}\\begin{align*}\n    g^{\\pi}(x) := \\lim_{T\\to\\infty} \\mathbb{E}_x^\\pi\\left[\\frac{1}{T}\\sum_{t=1}^{T}r(x_t, a_t)\\right]. %= \\lim_{T\\to\\infty}\\frac{1}{T}\\sum_{t=0}^{T-1} \\int_{\\mathcal{X}} r_{\\pi^{(t)}}(y)p^{(t)}(\\rd y|x).\n\\end{align*}\n    g^{\\pi}\\pi(x) := \\lim_{T\\to\\infty}T\\to\\infty \\mathbb{E}_x^\\pi\\left[\\frac{1}{T}\\sum_{t=1}t=1^{T}Tr(x_t, a_t)\\right]. \nWe say that a policy $\\pi^\\varepsilon\\in\\Pi^{\\rm R}$\\pi^\\varepsilon\\in\\Pi^{\\rm R}\\rm R is $\\varepsilon$\\varepsilon-optimal for $\\varepsilon \\geq 0$\\varepsilon \\geq 0 if \n \\begin{align*}\n     g^{\\pi^\\varepsilon}(x) \\geq g^{\\pi}(x) - \\varepsilon \\quad\\forall x\\in\\mathcal{X}, \\pi\\in\\Pi^{\\rm R}.\n \\end{align*}\\begin{align*}\n     g^{\\pi^\\varepsilon}(x) \\geq g^{\\pi}(x) - \\varepsilon \\quad\\forall x\\in\\mathcal{X}, \\pi\\in\\Pi^{\\rm R}.\n \\end{align*}\n     g^{\\pi^\\varepsilon}\\pi^\\varepsilon(x) \\geq g^{\\pi}\\pi(x) - \\varepsilon \\quad\\forall x\\in\\mathcal{X}, \\pi\\in\\Pi^{\\rm R}\\rm R.\n \n If $\\varepsilon=0$\\varepsilon=0, the policy $\\pi^\\ast\\in\\Pi^{\\rm R}$\\pi^\\ast\\in\\Pi^{\\rm R}\\rm R is said to be \\emph{optimal} if $g^{\\pi^\\ast}(x) \\geq g^{\\pi}(x)$g^{\\pi^\\ast}\\pi^\\ast(x) \\geq g^{\\pi}\\pi(x) for all $x\\in\\mathcal{X}, \\pi\\in\\Pi^{\\rm R}$x\\in\\mathcal{X}, \\pi\\in\\Pi^{\\rm R}\\rm R. The \\emph{optimal average reward} $g^\\ast(x)$g^\\ast(x) on initial state $x\\in\\mathcal{X}$x\\in\\mathcal{X} is defined as\n\\begin{align*}\n     g^*(x) := \\sup_{\\pi\\in\\Pi^{\\rm R}} g^{\\pi}(x).\n\\end{align*}\\begin{align*}\n     g^*(x) := \\sup_{\\pi\\in\\Pi^{\\rm R}} g^{\\pi}(x).\n\\end{align*}\n     g^*(x) := \\sup_{\\pi\\in\\Pi^{\\rm R}}\\pi\\in\\Pi^{\\rm R}\\rm R g^{\\pi}\\pi(x).\n\nAn optimal policy $\\pi^*\\in\\Pi^{\\rm R}$\\pi^*\\in\\Pi^{\\rm R}\\rm R exists whenever $g^{\\pi^*}(x) = g^*(x)$g^{\\pi^*}\\pi^*(x) = g^*(x) for all $x\\in\\mathcal X$x\\in\\mathcal X. The \\emph{bias} $h^{d^\\infty}(x)$h^{d^\\infty}d^\\infty(x) of a stationary policy $d^\\infty$d^\\infty, $d\\in\\mathcal{D}^{\\rm R}$d\\in\\mathcal{D}^{\\rm R}\\rm R, is the expected total difference between the reward and the stationary average reward, i.e.,\\footnote{This is valid for \\emph{aperiodic} chains. For periodic chains, \\cref{eq:bias_expression} holds in the Cesaro limit sense.}\n\\begin{align}\\label{eq:bias_expression}\n    h^{d^\\infty}(x) := \\lim_{T\\to \\infty} \\mathbb{E}_x^{d^\\infty}\\left[\\sum_{t=1}^{T} r_d(x_t) - g^{d^\\infty}(x_t)\\right] \\qquad\\forall x\\in\\mathcal{X}.\n\\end{align}\\begin{align}\\label{eq:bias_expression}\n    h^{d^\\infty}(x) := \\lim_{T\\to \\infty} \\mathbb{E}_x^{d^\\infty}\\left[\\sum_{t=1}^{T} r_d(x_t) - g^{d^\\infty}(x_t)\\right] \\qquad\\forall x\\in\\mathcal{X}.\n\\end{align}\\label{eq:bias_expression}\n    h^{d^\\infty}d^\\infty(x) := \\lim_{T\\to \\infty}T\\to \\infty \\mathbb{E}_x^{d^\\infty}d^\\infty\\left[\\sum_{t=1}t=1^{T}T r_d(x_t) - g^{d^\\infty}d^\\infty(x_t)\\right] \\qquad\\forall x\\in\\mathcal{X}.\n\nThe Bellman operator $\\mathcal{L}_d:\\mathscr{B}(\\mathcal{X})\\to\\mathscr{B}(\\mathcal{X})$\\mathcal{L}_d:\\mathscr{B}(\\mathcal{X})\\to\\mathscr{B}(\\mathcal{X}) associated with decision rule $d\\in\\mathcal{D}^{\\rm R}$d\\in\\mathcal{D}^{\\rm R}\\rm R and the optimal Bellman operator $\\mathcal{L}:\\mathscr{B}(\\mathcal{X})\\to\\mathscr{B}(\\mathcal{X})$\\mathcal{L}:\\mathscr{B}(\\mathcal{X})\\to\\mathscr{B}(\\mathcal{X}) are similarly defined as for finite-horizon MDPs.\n\nUnder suitable conditions on the kernels $p(\\cdot|x,a)$p(\\cdot|x,a), the optimal gain $g^\\ast(x)$g^\\ast(x) is state independent, i.e., $g^\\ast(x) = g^\\ast$g^\\ast(x) = g^\\ast for all $x\\in\\mathcal{X}$x\\in\\mathcal{X}. Moreover, for any measurable stationary policy $d^\\infty$d^\\infty, $d\\in\\mathcal{D}^{\\rm R}$d\\in\\mathcal{D}^{\\rm R}\\rm R, the gain $g^{d^\\infty}$g^{d^\\infty}d^\\infty and bias $h^{d^\\infty}$h^{d^\\infty}d^\\infty satisfy the system of evaluation equations\n\\begin{align*}\n    g^{d^\\infty}(x) = \\int_{\\mathcal{X}} p_d(\\rd y|x) g^{d^\\infty}(y)  \\qquad\\text{and}\\qquad h^{d^\\infty} = \\mathcal{L}_d h^{d^\\infty} - g^{d^\\infty}.\n\\end{align*}\\begin{align*}\n    g^{d^\\infty}(x) = \\int_{\\mathcal{X}} p_d(\\rd y|x) g^{d^\\infty}(y)  \\qquad\\text{and}\\qquad h^{d^\\infty} = \\mathcal{L}_d h^{d^\\infty} - g^{d^\\infty}.\n\\end{align*}\n    g^{d^\\infty}d^\\infty(x) = \\int_{\\mathcal{X}}\\mathcal{X} p_d(\\rd y|x) g^{d^\\infty}d^\\infty(y)  \\qquad\\text{and}\\qquad h^{d^\\infty}d^\\infty = \\mathcal{L}_d h^{d^\\infty}d^\\infty - g^{d^\\infty}d^\\infty.\n\nFinally, there is an optimal stationary deterministic policy $(d^\\ast)^\\infty$(d^\\ast)^\\infty, $d\\in\\mathcal{D}^{\\rm D}$d\\in\\mathcal{D}^{\\rm D}\\rm D, for which $(g^\\ast, h^\\ast) = (g^{(d^\\ast)^\\infty}, h^{(d^\\ast)^\\infty})$(g^\\ast, h^\\ast) = (g^{(d^\\ast)^\\infty}(d^\\ast)^\\infty, h^{(d^\\ast)^\\infty}(d^\\ast)^\\infty) satisfy the \\emph{average cost optimality equation}\n\\begin{align*}\n    h^\\ast = \\mathcal{L}h^\\ast - g^\\ast e.\n\\end{align*}\\begin{align*}\n    h^\\ast = \\mathcal{L}h^\\ast - g^\\ast e.\n\\end{align*}\n    h^\\ast = \\mathcal{L}h^\\ast - g^\\ast e.\n\nIn the finite-state-space setting, weakly communicating MDPs naturally satisfy all of the above, plus the fact that any optimal policy $\\pi^\\ast\\in\\argmax_{\\pi\\in\\Pi^{\\rm R}}g^\\pi(s)$\\pi^\\ast\\in\\argmax_{\\pi\\in\\Pi^{\\rm R}}\\pi\\in\\Pi^{\\rm R}\\rm Rg^\\pi(s) has constant gain. In the continuous-state-space setting with compact $\\mathcal{X}$\\mathcal{X}, extra requirements are needed on the stochastic kernels $p(\\cdot|x,a)$p(\\cdot|x,a), e.g., geometric convergence to an invariant probability measure. We refer the reader to~\\cite[Theorem~2.5]{saldi2017asymptotic}, \\cite[Theorem~5.5.4]{hernandez2012discrete}, \\cite[Chapter~3]{hernandez2001adaptive}, \\cite[Theorem~2.6 \\& Lemma~3.4]{gordienko1995average}, \\cite[Theorem~3]{jaskiewicz2006optimality}, \\cite[Theorem 3.3]{vega2003average}, and \\cite[Chapter~10]{hernandez1999further} for more information. From now on, we shall not worry about such conditions and assume the above properties hold.\n\n\n\\subsection{Discretization of state space}\\label{sec:discretization}\n\nWe briefly review a procedure to obtain finite-state models which will be used on continuous state-space MDPs. See~\\cite{saldi2017asymptotic} for more information. Consider a continuous state space $\\mathcal X$\\mathcal X and a metric $m_{\\mathcal X}$m_{\\mathcal X}\\mathcal X on $\\mathcal X$\\mathcal X. By assumption, $\\mathcal X$\\mathcal X is compact and hence totally bounded. Therefore, there exists a sequence $(\\{s_{n, i}\\}_{i\\in[k_n]})_{n\\geq 1}$(\\{s_{n, i}n, i\\}_{i\\in[k_n]}i\\in[k_n])_{n\\geq 1}n\\geq 1 of finite sets such that, for all $n\\in\\mathbb{N}$n\\in\\mathbb{N}, \n\\begin{align*}\n    \\min_{i\\in[k_n]} m_{\\mathcal X}\\left(x, s_{n,i}\\right) < \\frac{1}{n}\\quad \\text{ for all }x\\in\\mathcal X. \n\\end{align*}\\begin{align*}\n    \\min_{i\\in[k_n]} m_{\\mathcal X}\\left(x, s_{n,i}\\right) < \\frac{1}{n}\\quad \\text{ for all }x\\in\\mathcal X. \n\\end{align*}\n    \\min_{i\\in[k_n]}i\\in[k_n] m_{\\mathcal X}\\mathcal X\\left(x, s_{n,i}n,i\\right) < \\frac{1}{n}\\quad \\text{ for all }x\\in\\mathcal X. \n\nThe finite grid $\\mathcal{S}_n := \\{s_{n,i}\\}_{i\\in[k_n]}$\\mathcal{S}_n := \\{s_{n,i}n,i\\}_{i\\in[k_n]}i\\in[k_n] is called a $\\frac{1}{n}$\\frac{1}{n}-net in $\\mathcal{X}$\\mathcal{X}. Define the function\n\\begin{align*}\n    Q_{n, \\mathcal X}: \\mathcal X\\to \\mathcal{S}_n \\quad \\text{as}\\quad Q_{n, \\mathcal X}(x) = \\argmin_{s\\in\\mathcal{S}_n} m_{\\mathcal X}(x,s),\n\\end{align*}\\begin{align*}\n    Q_{n, \\mathcal X}: \\mathcal X\\to \\mathcal{S}_n \\quad \\text{as}\\quad Q_{n, \\mathcal X}(x) = \\argmin_{s\\in\\mathcal{S}_n} m_{\\mathcal X}(x,s),\n\\end{align*}\n    Q_{n, \\mathcal X}n, \\mathcal X: \\mathcal X\\to \\mathcal{S}_n \\quad \\text{as}\\quad Q_{n, \\mathcal X}n, \\mathcal X(x) = \\argmin_{s\\in\\mathcal{S}_n}s\\in\\mathcal{S}_n m_{\\mathcal X}\\mathcal X(x,s),\n\nwhere ties are broken so that $Q_{n, \\mathcal X}$Q_{n, \\mathcal X}n, \\mathcal X is measurable. The map $Q_{n,\\mathcal{X}}$Q_{n,\\mathcal{X}}n,\\mathcal{X} is often called a nearest neighbour quantizer with respect to distortion measure $m_{\\mathcal X}$m_{\\mathcal X}\\mathcal X~\\cite{gray1998quantization}. For each $n$n, this mapping induces a partition $\\left\\{\\mathcal X_{n, i}\\right\\}_{i\\in[k_n]}$\\left\\{\\mathcal X_{n, i}n, i\\right\\}_{i\\in[k_n]}i\\in[k_n] of $\\mathcal X$\\mathcal X given by \n\\begin{align*}\n     \\mathcal X_{n, i} = \\{x\\in\\mathcal X: Q_{n,\\mathcal X}(x) = s_{n, i}\\},\n\\end{align*}\\begin{align*}\n     \\mathcal X_{n, i} = \\{x\\in\\mathcal X: Q_{n,\\mathcal X}(x) = s_{n, i}\\},\n\\end{align*}\n     \\mathcal X_{n, i}n, i = \\{x\\in\\mathcal X: Q_{n,\\mathcal X}n,\\mathcal X(x) = s_{n, i}n, i\\},\n\nwith diameter $\\operatorname{diam}(\\mathcal X_{n, i}) = \\sup_{x, x'\\in \\mathcal X_{n, i}} m_{\\mathcal X}(x, x') < \\frac{2}{n }$\\operatorname{diam}diam(\\mathcal X_{n, i}n, i) = \\sup_{x, x'\\in \\mathcal X_{n, i}}x, x'\\in \\mathcal X_{n, i}n, i m_{\\mathcal X}\\mathcal X(x, x') < \\frac{2}{n }. Given $s\\in\\mathcal{S}$s\\in\\mathcal{S}, we also use the notation $\\mathcal{X}_n(s)$\\mathcal{X}_n(s) to denote $\\{x\\in\\mathcal{X}: Q_{n,\\mathcal{X}}(x) = s\\}$\\{x\\in\\mathcal{X}: Q_{n,\\mathcal{X}}n,\\mathcal{X}(x) = s\\}. \n\nAs an example, consider the $1$1-dimensional setting where the $\\mathcal X = [0, 1]$\\mathcal X = [0, 1]. The $\\frac{1}{n}$\\frac{1}{n}-net $\\mathcal{S}_n$\\mathcal{S}_n partitions $\\mathcal X$\\mathcal X into $n$n intervals $\\mathcal X_{n,0}, \\dots, \\mathcal X_{n,n-1}$\\mathcal X_{n,0}n,0, \\dots, \\mathcal X_{n,n-1}n,n-1, where\n\\begin{align*}\n    \\mathcal X_{n,0} = \\left[0, \\frac{1}{n}\\right] \\quad\\text{and}\\quad \\mathcal X_{n,i} = \\left(\\frac{i}{n}, \\frac{i+1}{n}\\right] ~\\text{for}~i = 1, \\dots, n-1. \n\\end{align*}\\begin{align*}\n    \\mathcal X_{n,0} = \\left[0, \\frac{1}{n}\\right] \\quad\\text{and}\\quad \\mathcal X_{n,i} = \\left(\\frac{i}{n}, \\frac{i+1}{n}\\right] ~\\text{for}~i = 1, \\dots, n-1. \n\\end{align*}\n    \\mathcal X_{n,0}n,0 = \\left[0, \\frac{1}{n}\\right] \\quad\\text{and}\\quad \\mathcal X_{n,i}n,i = \\left(\\frac{i}{n}, \\frac{i+1}{n}\\right] ~\\text{for}~i = 1, \\dots, n-1. \n\nEach interval $\\mathcal X_{n,i}$\\mathcal X_{n,i}n,i is represented by a state $s_{n,i}\\in\\mathcal S_n$s_{n,i}n,i\\in\\mathcal S_n. For the rest of our paper, for simplicity we assume an underlying Euclidean topology for $\\mathcal{X}$\\mathcal{X}, in which case $m_{\\mathcal{X}}$m_{\\mathcal{X}}\\mathcal{X} is simply the $\\ell_2$\\ell_2-norm $\\|\\cdot\\|_2$\\|\\cdot\\|_2.\n\nWe introduce natural assumptions for the transition probabilities and rewards in close states, which will be used throughout the paper whenever we work with continuous-state-space MDPs. Similar assumptions have been considered in~\\cite{saldi2017asymptotic, ortner2012online, kara2023q}. \n\\begin{assumption}[H\\\"older continuity]\\label{ass:Lipschitz}\n    The reward function $r:\\mathcal{X}\\times\\mathcal{A}\\to[0,1]$ and stochastic kernel $p:\\mathcal{X}\\times\\mathcal{A}\\to\\mathscr{P}(\\mathcal{X})$ are H\\\"older continuous, i.e., there are constants $L, \\alpha \\geq 0$ such that \n    %\n    \\begin{align*}\n        |  r(x, a) - r(x', a)| &\\leq L\\| x - x'\\|_2^\\alpha \\quad\\text{for all}\\quad(x,x',a)\\in\\mathcal{X}\\times\\mathcal{X}\\times\\mathcal{A},\\\\\n        \\Vert  p(\\cdot\\vert x, a) -  p(\\cdot\\vert x', a)\\Vert_{\\rm tvd} &\\leq L\\| x - x'\\|_2^\\alpha \\quad\\text{for all}\\quad(x,x',a)\\in\\mathcal{X}\\times\\mathcal{X}\\times\\mathcal{A}.\n    \\end{align*}\n\\end{assumption}\\begin{assumption}[H\\\"older continuity]\\label{ass:Lipschitz}\n    The reward function $r:\\mathcal{X}\\times\\mathcal{A}\\to[0,1]$ and stochastic kernel $p:\\mathcal{X}\\times\\mathcal{A}\\to\\mathscr{P}(\\mathcal{X})$ are H\\\"older continuous, i.e., there are constants $L, \\alpha \\geq 0$ such that \n    %\n    \\begin{align*}\n        |  r(x, a) - r(x', a)| &\\leq L\\| x - x'\\|_2^\\alpha \\quad\\text{for all}\\quad(x,x',a)\\in\\mathcal{X}\\times\\mathcal{X}\\times\\mathcal{A},\\\\\n        \\Vert  p(\\cdot\\vert x, a) -  p(\\cdot\\vert x', a)\\Vert_{\\rm tvd} &\\leq L\\| x - x'\\|_2^\\alpha \\quad\\text{for all}\\quad(x,x',a)\\in\\mathcal{X}\\times\\mathcal{X}\\times\\mathcal{A}.\n    \\end{align*}\n\\end{assumption}[H\\\"older continuity]\\label{ass:Lipschitz}\n    The reward function $r:\\mathcal{X}\\times\\mathcal{A}\\to[0,1]$r:\\mathcal{X}\\times\\mathcal{A}\\to[0,1] and stochastic kernel $p:\\mathcal{X}\\times\\mathcal{A}\\to\\mathscr{P}(\\mathcal{X})$p:\\mathcal{X}\\times\\mathcal{A}\\to\\mathscr{P}(\\mathcal{X}) are H\\\"older continuous, i.e., there are constants $L, \\alpha \\geq 0$L, \\alpha \\geq 0 such that \n    \n        |  r(x, a) - r(x', a)| &\\leq L\\| x - x'\\|_2^\\alpha \\quad\\text{for all}\\quad(x,x',a)\\in\\mathcal{X}\\times\\mathcal{X}\\times\\mathcal{A},\\\\\n        \\Vert  p(\\cdot\\vert x, a) -  p(\\cdot\\vert x', a)\\Vert_{\\rm tvd}\\rm tvd &\\leq L\\| x - x'\\|_2^\\alpha \\quad\\text{for all}\\quad(x,x',a)\\in\\mathcal{X}\\times\\mathcal{X}\\times\\mathcal{A}.\n    \n\n\n\n", "appendix": false}, "Our reinforcement learning model": {"content": "\n\\label{sec:RL_model}\n\nIn the standard classical online-learning setting, an agent must learn an unknown  MDP. We assume for simplicity that the space of possible states $\\mathcal{X}$\\mathcal{X}, the space of possible actions $\\mathcal{A}$\\mathcal{A}, and the reward function $r:\\mathcal{X}\\times\\mathcal{A}\\to[0,1]$r:\\mathcal{X}\\times\\mathcal{A}\\to[0,1] are known by the agent, who must then learn the stochastic kernel $p$p by interacting with the environment in the following way. By starting in an initial random state $x_{1}$x_{1}1, the agent must perform, at each time step $t\\in\\mathbb{N}$t\\in\\mathbb{N}, an action $a_{t}\\sim \\pi_{t}(\\cdot|x_{t})$a_{t}t\\sim \\pi_{t}t(\\cdot|x_{t}t) according to some chosen decision rule $\\pi_{t}\\in\\mathcal{D}^{\\rm R}$\\pi_{t}t\\in\\mathcal{D}^{\\rm R}\\rm R. After that, the agent receives the reward $r(x_{t},a_{t})$r(x_{t}t,a_{t}t) and the system moves to a new state $x_{t+1}\\sim p(\\cdot|x_{t},a_{t})$x_{t+1}t+1\\sim p(\\cdot|x_{t}t,a_{t}t) based on the previous state $x_{t}$x_{t}t and the action taken $a_{t}$a_{t}t. The decision rule $\\pi_{t}\\in\\mathcal{D}^{\\rm R}$\\pi_{t}t\\in\\mathcal{D}^{\\rm R}\\rm R selected by the agent will depend on the trajectory $(x_{0}, a_{0}, \\dots, x_{t-1}, a_{t-1}, x_{t})$(x_{0}0, a_{0}0, \\dots, x_{t-1}t-1, a_{t-1}t-1, x_{t}t) of states and actions up to step $t$t, and after receiving the reward $r(x_{t},a_{t})$r(x_{t}t,a_{t}t) and observing the new state $x_{t+1}$x_{t+1}t+1, the agent normally updates his estimate $\\widehat{p}$\\widehat{p} on the true transition measure. \n\nFor finite-horizon MDPs, the interaction between agent and environment is done in \\emph{episodes}, meaning intervals of $H$H steps each: at the beginning of the $k$k-th episode, the environment starts in a random state $x^{(k)}_1$x^{(k)}(k)_1 and the agent chooses a policy $\\pi^{(k)}$\\pi^{(k)}(k). After $H$H interacting steps, the $k$k-th episode ends and the $(k+1)$(k+1)-th episode starts, with a new random state $x_{1}^{(k+1)}$x_{1}1^{(k+1)}(k+1), at which point the agent gets to choose a new policy $\\pi^{(k+1)}$\\pi^{(k+1)}(k+1). A perfect knowledge of the MDP $M$M would have allowed the agent to select an optimal policy with optimal total expected reward $V_1^\\ast(x_1^{(k)})$V_1^\\ast(x_1^{(k)}(k)). Therefore, a natural benchmark for learning a finite-horizon MDP $M$M is through the \\emph{finite-horizon regret} over $K$K episodes (or $T=KH$T=KH total steps) defined as\n\\begin{align*}\n    \\operatorname{Regret}_{H}(T) := \\sum_{k=1}^K \\big(V_1^\\ast(x^{(k)}_1) - V_1^{\\pi^{(k)}}(x^{(k)}_1) \\big).\n\\end{align*}\\begin{align*}\n    \\operatorname{Regret}_{H}(T) := \\sum_{k=1}^K \\big(V_1^\\ast(x^{(k)}_1) - V_1^{\\pi^{(k)}}(x^{(k)}_1) \\big).\n\\end{align*}\n    \\operatorname{Regret}Regret_{H}H(T) := \\sum_{k=1}k=1^K \\big(V_1^\\ast(x^{(k)}(k)_1) - V_1^{\\pi^{(k)}}\\pi^{(k)}(k)(x^{(k)}(k)_1) \\big).\n\n\nFor infinite-horizon MDPs, there are no formal episodes. Agent and environment interact indeterminately and the agent is free to choose a different policy at any point. Normally in most RL algorithms, a stationary policy is chosen for several time steps (since the optimal policy is stationary), and any change or update to this policy due to new data marks the end of an episode. After $T$T steps, the agent accumulates a total reward of $\\sum_{t=1}^{T} r(x_{t},a_{t})$\\sum_{t=1}t=1^{T}T r(x_{t}t,a_{t}t). In the limit of large $T$T, a perfect knowledge on the MDP $M$M would have allowed the agent to select an optimal stationary policy with optimal average reward $g^\\ast$g^\\ast per step. \nIn this paper, we shall consider \\emph{two} types of regret for learning infinite-horizon MDPs: the \\emph{infinite-horizon in-path regret} over $T$T steps defined as\n\\begin{align*}\n    \\operatorname{Regret}_{\\infty}^{\\rm path}(T) := Tg^\\ast - \\sum_{t=1}^{T} r(x_{t},a_{t}),\n\\end{align*}\\begin{align*}\n    \\operatorname{Regret}_{\\infty}^{\\rm path}(T) := Tg^\\ast - \\sum_{t=1}^{T} r(x_{t},a_{t}),\n\\end{align*}\n    \\operatorname{Regret}Regret_{\\infty}\\infty^{\\rm path}\\rm path(T) := Tg^\\ast - \\sum_{t=1}t=1^{T}T r(x_{t}t,a_{t}t),\n\nand the \\emph{infinite-horizon expected regret} over $T$T steps defined as\n\\begin{align*}\n    \\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T) := Tg^\\ast - \\sum_{t=1}^T\\min_{x\\in\\mathcal{X}}g^{d_t^\\infty}(x),\n\\end{align*}\\begin{align*}\n    \\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T) := Tg^\\ast - \\sum_{t=1}^T\\min_{x\\in\\mathcal{X}}g^{d_t^\\infty}(x),\n\\end{align*}\n    \\operatorname{Regret}Regret_{\\infty}\\infty^{\\mathbb{E}}\\mathbb{E}(T) := Tg^\\ast - \\sum_{t=1}t=1^T\\min_{x\\in\\mathcal{X}}x\\in\\mathcal{X}g^{d_t^\\infty}d_t^\\infty(x),\n\nwhere $d_t\\in\\mathcal{D}^{\\rm R}$d_t\\in\\mathcal{D}^{\\rm R}\\rm R is the decision rule employed at time step $t\\in\\mathbb{N}$t\\in\\mathbb{N}. The infinite-horizon in-path regret $\\operatorname{Regret}_{\\infty}^{\\rm path}(T)$\\operatorname{Regret}Regret_{\\infty}\\infty^{\\rm path}\\rm path(T) is the standard choice for most prior works on RL~\\cite{auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,ortner2012online,fruit2018efficient} and takes into account the actual path of observed state-action pairs $(x_t,a_t)$(x_t,a_t) during learning, while the infinite-horizon expected regret $\\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T)$\\operatorname{Regret}Regret_{\\infty}\\infty^{\\mathbb{E}}\\mathbb{E}(T) is a novel measure of regret that we introduce here. The motivation behind $\\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T)$\\operatorname{Regret}Regret_{\\infty}\\infty^{\\mathbb{E}}\\mathbb{E}(T) is to ignore the randomness coming from the environment when transitioning to a new state and focus instead on an average quantity over all possible paths, very similarly to the finite-horizon regret $\\operatorname{Regret}_{H}(T)$\\operatorname{Regret}Regret_{H}H(T), which also only considers average quantities and not the accumulated reward. We note that, when the MDP is unichain (see~\\cite[Section~8.3]{puterman2014markov}), the average reward of any stationary policy is constant, and therefore the minimum over $\\mathcal{X}$\\mathcal{X} can be dropped from the definition of $\\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T)$\\operatorname{Regret}Regret_{\\infty}\\infty^{\\mathbb{E}}\\mathbb{E}(T).\n\n\\subsection{Quantum exploration-generative learning model}\n\nWe would like to define a corresponding online-learning model in which the agent can interact with the environment in a quantum fashion. Such a model, however, is not straightforward to define without running into apparent contradictions, since a quantum interaction between agent and environment would inevitably lead to a superposition of trajectories of states and actions $(x_{1}, a_{1}, \\dots, x_{t-1}, a_{t-1}, x_{t})$(x_{1}1, a_{1}1, \\dots, x_{t-1}t-1, a_{t-1}t-1, x_{t}t), for which there would be no clear measure of regret. We solve this problem by splitting the interaction between agent and environment into an alternation of \\emph{two} types of phases: classical \\emph{exploration} phases and quantum \\emph{generative} phases.\n\n\\paragraph*{Exploration phase.}Exploration phase. An exploration phase corresponds exactly to a sequence of episodes in the standard classical online-learning setting. In the $k$k-th exploration phase, the system starts in some random classical state $x_1^{(k)}$x_1^{(k)}(k). At each step $t\\in\\mathbb{N}$t\\in\\mathbb{N}, the agent chooses an action $a_t^{(k)}\\sim \\pi^{(k)}_t(\\cdot|x^{(k)}_t)$a_t^{(k)}(k)\\sim \\pi^{(k)}(k)_t(\\cdot|x^{(k)}(k)_t) according to some decision rule $\\pi_t^{(k)}\\in\\mathcal{D}^{\\rm R}$\\pi_t^{(k)}(k)\\in\\mathcal{D}^{\\rm R}\\rm R, after which the environment immediately returns the reward $r(x^{(k)}_t,a_t^{(k)})$r(x^{(k)}(k)_t,a_t^{(k)}(k)) and moves to a new state $x^{(k)}_{t+1}\\sim p(\\cdot|x^{(k)}_t, a_t^{(k)})$x^{(k)}(k)_{t+1}t+1\\sim p(\\cdot|x^{(k)}(k)_t, a_t^{(k)}(k)). For finite-horizon MDPs, the exploration phase lasts as many episodes, each with $H$H time steps, as the agent desires, and after $\\tau_k$\\tau_k time steps (proportional to $H$H), the incurred regret is $\\frac{\\tau_k}{H} \\big(V_1^\\ast(x_1^{(k)}) - V_1^{\\pi^{(k)}}(x^{(k)}_1)\\big)$\\frac{\\tau_k}{H} \\big(V_1^\\ast(x_1^{(k)}(k)) - V_1^{\\pi^{(k)}}\\pi^{(k)}(k)(x^{(k)}(k)_1)\\big).\\footnote{We consider the same initial state $x_1^{(k)}$ for simplicity only.} For infinite-horizon MDPs, the exploration phase can last as long as the agent desires, and after $\\tau_k$\\tau_k steps, the incurred in-path regret is $\\tau_k g^\\ast - \\sum_{t=1}^{\\tau_k}r(x_{t}^{(k)},a_{t}^{(k)})$\\tau_k g^\\ast - \\sum_{t=1}t=1^{\\tau_k}\\tau_kr(x_{t}t^{(k)}(k),a_{t}t^{(k)}(k)) and the expected regret is $\\tau_k g^\\ast - \\sum_{t=1}^{\\tau_k} \\min_{x\\in\\mathcal{X}} g^{(\\pi_t^{(k)})^{\\infty}}(x)$\\tau_k g^\\ast - \\sum_{t=1}t=1^{\\tau_k}\\tau_k \\min_{x\\in\\mathcal{X}}x\\in\\mathcal{X} g^{(\\pi_t^{(k)})^{\\infty}}(\\pi_t^{(k)}(k))^{\\infty}\\infty(x).\n\n\n\\paragraph*{Generative phase.}Generative phase. Once the $k$k-th exploration phase ends, the $k$k-th generative phase starts. During a generative phase, the agent is \\emph{free} to interact with the environment \\emph{without} incurring regret. Such interaction is done in a generative fashion~\\cite{kearns1998finite,kakade2003sample} via quantum-accessible environments~\\cite{wiedemann2022quantum, wang2021quantum, jerbi2022quantum, zhong2023provably}, i.e., through a quantum sampling oracle for the transition probabilities and a quantum reward oracle defined below.\n\n\\begin{definition}[Quantum sampling oracle for transition probabilities]\\label{def:quantum_sampling^oracle}\n    A quantum sampling oracle $\\mathcal O_{p}$ for the stochastic kernel $p:\\mathcal{X}\\times\\mathcal{A}\\to \\mathscr{P}(\\mathcal{X})$ is the unitary\n    %\n    \\begin{align*}\n        \\mathcal O_{p}: \\ket{x}\\ket{a}\\ket{\\bar 0}\\rightarrow \\int_{x'\\in\\mathcal X} \\sqrt{p(\\rd x'\\vert x, a)} \\ket{x}\\ket{a}\\ket{x'} \\quad\\text{for all}\\quad (x,a)\\in\\mathcal{X}\\times\\mathcal{A}.\n    \\end{align*}\n    %\n    If $\\mathcal{X} = \\mathcal{S}$ is finite, then\n    %\n    \\begin{align*}\n        \\mathcal O_{p}: \\ket{s}\\ket{a}\\ket{\\bar 0}\\rightarrow \\sum_{s'\\in\\mathcal{S}} \\sqrt{p(s'\\vert s, a)} \\ket{s}\\ket{a}\\ket{s'} \\quad\\text{for all}\\quad (s,a)\\in\\mathcal{S}\\times\\mathcal{A}.\n    \\end{align*}\n    %\n    We say we have quantum sampling access to $p$ if we have access to $\\mathcal{O}_p$ and $\\mathcal{O}_p^{\\dagger}$. %When $\\mathcal{X}$ is clear from context or is generic, we write $\\mathcal{O}_p$.\n\\end{definition}\\begin{definition}[Quantum sampling oracle for transition probabilities]\\label{def:quantum_sampling^oracle}\n    A quantum sampling oracle $\\mathcal O_{p}$ for the stochastic kernel $p:\\mathcal{X}\\times\\mathcal{A}\\to \\mathscr{P}(\\mathcal{X})$ is the unitary\n    %\n    \\begin{align*}\n        \\mathcal O_{p}: \\ket{x}\\ket{a}\\ket{\\bar 0}\\rightarrow \\int_{x'\\in\\mathcal X} \\sqrt{p(\\rd x'\\vert x, a)} \\ket{x}\\ket{a}\\ket{x'} \\quad\\text{for all}\\quad (x,a)\\in\\mathcal{X}\\times\\mathcal{A}.\n    \\end{align*}\n    %\n    If $\\mathcal{X} = \\mathcal{S}$ is finite, then\n    %\n    \\begin{align*}\n        \\mathcal O_{p}: \\ket{s}\\ket{a}\\ket{\\bar 0}\\rightarrow \\sum_{s'\\in\\mathcal{S}} \\sqrt{p(s'\\vert s, a)} \\ket{s}\\ket{a}\\ket{s'} \\quad\\text{for all}\\quad (s,a)\\in\\mathcal{S}\\times\\mathcal{A}.\n    \\end{align*}\n    %\n    We say we have quantum sampling access to $p$ if we have access to $\\mathcal{O}_p$ and $\\mathcal{O}_p^{\\dagger}$. %When $\\mathcal{X}$ is clear from context or is generic, we write $\\mathcal{O}_p$.\n\\end{definition}\\label{def:quantum_sampling^oracle}\n    A quantum sampling oracle $\\mathcal O_{p}$\\mathcal O_{p}p for the stochastic kernel $p:\\mathcal{X}\\times\\mathcal{A}\\to \\mathscr{P}(\\mathcal{X})$p:\\mathcal{X}\\times\\mathcal{A}\\to \\mathscr{P}(\\mathcal{X}) is the unitary\n    \n        \\mathcal O_{p}p: \\ket{x}\\ket{a}\\ket{\\bar 0}\\rightarrow \\int_{x'\\in\\mathcal X}x'\\in\\mathcal X \\sqrt{p(\\rd x'\\vert x, a)} \\ket{x}\\ket{a}\\ket{x'} \\quad\\text{for all}\\quad (x,a)\\in\\mathcal{X}\\times\\mathcal{A}.\n    \n    If $\\mathcal{X} = \\mathcal{S}$\\mathcal{X} = \\mathcal{S} is finite, then\n    \n        \\mathcal O_{p}p: \\ket{s}\\ket{a}\\ket{\\bar 0}\\rightarrow \\sum_{s'\\in\\mathcal{S}}s'\\in\\mathcal{S} \\sqrt{p(s'\\vert s, a)} \\ket{s}\\ket{a}\\ket{s'} \\quad\\text{for all}\\quad (s,a)\\in\\mathcal{S}\\times\\mathcal{A}.\n    \n    We say we have quantum sampling access to $p$p if we have access to $\\mathcal{O}_p$\\mathcal{O}_p and $\\mathcal{O}_p^{\\dagger}$\\mathcal{O}_p^{\\dagger}\\dagger. \n\\begin{definition}[Quantum reward oracle]\\label{def:reward}\n    A quantum reward oracle $\\mathcal O_r$ for the reward $r:\\mathcal{X}\\times\\mathcal{A}\\to[0,1]$ is the unitary\n    \\begin{align*}\n        \\mathcal O_r:\\ket{x}\\ket{a}\\ket{\\bar 0}\\rightarrow \\ket{x}\\ket{a}\\ket{r(x, a)} \\quad\\text{for all}\\quad (x,a)\\in\\mathcal{X}\\times\\mathcal{A}.\n    \\end{align*}\n    %\n    We say we have quantum access to $r$ if we have access to $\\mathcal{O}_r$ and $\\mathcal{O}_r^{\\dagger}$.% When $\\mathcal{X}$ is clear from context or is generic, we write $\\mathcal{O}_r$.\n\\end{definition}\\begin{definition}[Quantum reward oracle]\\label{def:reward}\n    A quantum reward oracle $\\mathcal O_r$ for the reward $r:\\mathcal{X}\\times\\mathcal{A}\\to[0,1]$ is the unitary\n    \\begin{align*}\n        \\mathcal O_r:\\ket{x}\\ket{a}\\ket{\\bar 0}\\rightarrow \\ket{x}\\ket{a}\\ket{r(x, a)} \\quad\\text{for all}\\quad (x,a)\\in\\mathcal{X}\\times\\mathcal{A}.\n    \\end{align*}\n    %\n    We say we have quantum access to $r$ if we have access to $\\mathcal{O}_r$ and $\\mathcal{O}_r^{\\dagger}$.% When $\\mathcal{X}$ is clear from context or is generic, we write $\\mathcal{O}_r$.\n\\end{definition}\\label{def:reward}\n    A quantum reward oracle $\\mathcal O_r$\\mathcal O_r for the reward $r:\\mathcal{X}\\times\\mathcal{A}\\to[0,1]$r:\\mathcal{X}\\times\\mathcal{A}\\to[0,1] is the unitary\n    \n        \\mathcal O_r:\\ket{x}\\ket{a}\\ket{\\bar 0}\\rightarrow \\ket{x}\\ket{a}\\ket{r(x, a)} \\quad\\text{for all}\\quad (x,a)\\in\\mathcal{X}\\times\\mathcal{A}.\n    \n    We say we have quantum access to $r$r if we have access to $\\mathcal{O}_r$\\mathcal{O}_r and $\\mathcal{O}_r^{\\dagger}$\\mathcal{O}_r^{\\dagger}\\dagger.\n\n\nWe shall assume that, since the reward function $r:\\mathcal{X}\\times\\mathcal{A}\\to[0,1]$r:\\mathcal{X}\\times\\mathcal{A}\\to[0,1] is known to the agent, there is no limitation on the use of $\\mathcal{O}_r$\\mathcal{O}_r. On the other hand, however, during the $k$k-th generative phase, the agent is free to employ the oracle $\\mathcal{O}_p$\\mathcal{O}_p and its inverse on any initial state $|x\\rangle|\\bar{0}\\rangle$|x\\rangle|\\bar{0}\\rangle, $x\\in\\mathcal{X}$x\\in\\mathcal{X}, in any desired way to a \\emph{maximum} number of times $O(\\tau_k)$O(\\tau_k) that depends linearly on the length $\\tau_{k}$\\tau_{k}k of the previous \\emph{exploration} phase. Moreover, we assume that all quantum registers are thrown away and reset at the end of the $k$k-th exploration phase. \n\nSetting a maximum number of uses for oracle $\\mathcal{O}_p$\\mathcal{O}_p and its inverses during a generative phase is vital to guarantee a meaningful online-learning model, otherwise the agent could explore and learn the MDP $M$M as much as needed to obtain a policy arbitrary close to optimal and final regret as small as desired. In order to freely explore, the agent must pay a price in requiring to commit to actions, receive rewards, and possibly incur regret. This trade-off is obtained by the previous exploration period $\\tau_{k}$\\tau_{k}k. \n\n\n\\subsection{Classical exploration-generative learning model}\n\nIt is possible to define a classical counterpart of the above quantum exploration-generative learning model. The exploration phases do not change, since they are already classical. The generative phases, on the other hand, allow a similar free interaction between agent and environment without regret, but through classical versions of oracle $\\mathcal{O}_p$\\mathcal{O}_p instead. Since the rewards are known, a classical version of $\\mathcal{O}_r$\\mathcal{O}_r is trivial. Moreover, a classical version of $\\mathcal{O}_p$\\mathcal{O}_p simply models the response of the environment on input $(x,a)\\in\\mathcal{X}\\times \\mathcal{A}$(x,a)\\in\\mathcal{X}\\times \\mathcal{A}, the difference being that the agent gets to choose $(x,a)$(x,a). We formalise such interaction below out of completeness.\n\\begin{definition}[Classical sampling oracle for transition probabilities]\n    A classical sampling oracle $\\mathcal{C}_p$ for the stochastic kernel $p:\\mathcal{X}\\times\\mathcal{A}\\to\\mathscr{P}(\\mathcal{X})$ is an operation that, on input $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$, samples $x'\\in\\mathcal{X}$ from the distribution $p(\\cdot|x,a)$. We say we have classical sampling access to $p$ if we have access to $\\mathcal{C}_p$. %When $\\mathcal{X}$ is clear from context or is generic, we write $\\mathcal{C}_p$.\n\\end{definition}\\begin{definition}[Classical sampling oracle for transition probabilities]\n    A classical sampling oracle $\\mathcal{C}_p$ for the stochastic kernel $p:\\mathcal{X}\\times\\mathcal{A}\\to\\mathscr{P}(\\mathcal{X})$ is an operation that, on input $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$, samples $x'\\in\\mathcal{X}$ from the distribution $p(\\cdot|x,a)$. We say we have classical sampling access to $p$ if we have access to $\\mathcal{C}_p$. %When $\\mathcal{X}$ is clear from context or is generic, we write $\\mathcal{C}_p$.\n\\end{definition}\n    A classical sampling oracle $\\mathcal{C}_p$\\mathcal{C}_p for the stochastic kernel $p:\\mathcal{X}\\times\\mathcal{A}\\to\\mathscr{P}(\\mathcal{X})$p:\\mathcal{X}\\times\\mathcal{A}\\to\\mathscr{P}(\\mathcal{X}) is an operation that, on input $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$(x,a)\\in\\mathcal{X}\\times\\mathcal{A}, samples $x'\\in\\mathcal{X}$x'\\in\\mathcal{X} from the distribution $p(\\cdot|x,a)$p(\\cdot|x,a). We say we have classical sampling access to $p$p if we have access to $\\mathcal{C}_p$\\mathcal{C}_p. \nDuring the $k$k-th generative phase, the agent is free to employ the oracle $\\mathcal{C}_p$\\mathcal{C}_p on any initial state $(x,a)$(x,a) to a maximum number of times $O(\\tau_k)$O(\\tau_k), similarly to the quantum model.\n\n\n\\subsection{The reinforcement learning model of Zhong et al.}\n\nIn~\\cite{zhong2023provably}, Zhong \\emph{et al.}\\ proposed quantum RL algorithms for  finite-horizon finite-state-space MDPs. In order to do so, the authors employed an exploration-generative learning model that shares several similarities to ours. Although not explicitly stated in their paper, their RL model alternates between exploration phases that accumulate rewards and thus regret, and \\emph{free} generative phases where no rewards are obtained. Their exploration phase is virtually the same as ours, during which the environment's state $x_{t}$x_{t}t jumps to $x_{t+1}\\sim p(\\cdot|x_{t},a_{t})$x_{t+1}t+1\\sim p(\\cdot|x_{t}t,a_{t}t) after the agent chooses an action $a_{t}\\sim \\pi_t(\\cdot|x_{t})$a_{t}t\\sim \\pi_t(\\cdot|x_{t}t) and receives reward $r(x_{t},a_{t})$r(x_{t}t,a_{t}t). Even though Zhong \\emph{et al.}\\ phrased such interaction in a quantum fashion using the oracle $\\mathcal{O}_p$\\mathcal{O}_p plus an action oracle $\\mathcal{O}_{\\pi_t}$\\mathcal{O}_{\\pi_t}\\pi_t for the decision rule $\\pi_t\\in\\mathcal{D}^{\\rm R}$\\pi_t\\in\\mathcal{D}^{\\rm R}\\rm R defined as\n\\begin{align*}\n    \\mathcal{O}_{\\pi_t}:|x\\rangle|\\bar{0}\\rangle \\mapsto \\sum_{a\\in\\mathcal{A}} \\sqrt{\\pi_t(a|x)}|x\\rangle|a\\rangle \\qquad \\forall x\\in\\mathcal{X},\n\\end{align*}\\begin{align*}\n    \\mathcal{O}_{\\pi_t}:|x\\rangle|\\bar{0}\\rangle \\mapsto \\sum_{a\\in\\mathcal{A}} \\sqrt{\\pi_t(a|x)}|x\\rangle|a\\rangle \\qquad \\forall x\\in\\mathcal{X},\n\\end{align*}\n    \\mathcal{O}_{\\pi_t}\\pi_t:|x\\rangle|\\bar{0}\\rangle \\mapsto \\sum_{a\\in\\mathcal{A}}a\\in\\mathcal{A} \\sqrt{\\pi_t(a|x)}|x\\rangle|a\\rangle \\qquad \\forall x\\in\\mathcal{X},\n\nthere is absolutely no need for such and the agent-environment interaction can be done entirely classically. Indeed, Zhong \\emph{et al.}\\ model the agent-environment interaction during an exploration phase as repeated applications of $\\mathcal{O}_p\\mathcal{O}_{\\pi_t}$\\mathcal{O}_p\\mathcal{O}_{\\pi_t}\\pi_t followed by a measurement, which returns $x_{t+1}$x_{t+1}t+1 from the distribution $p(\\cdot|x_{t},a_{t})$p(\\cdot|x_{t}t,a_{t}t), just as in our case.\n\nRegarding their free generative steps, Zhong \\emph{et al.}\\ allow the application of $\\mathcal{O}_p$\\mathcal{O}_p or $\\mathcal{O}^{\\dagger}_p$\\mathcal{O}^{\\dagger}\\dagger_p to a register \\emph{belonging} to the environment which contains a quantum state $|x_{t}\\rangle_{\\mathcal{X}}$|x_{t}t\\rangle_{\\mathcal{X}}\\mathcal{X} corresponding to the state $x_t$x_t of the environment which has been visited in an exploration step. Then, quantum mean estimation is applied and, in their Algorithm 1, Zhong \\emph{et al.} indicate that it can be implemented using the stored states $\\mathcal{O}_p |x_{t}\\rangle_{\\mathcal{X}}$\\mathcal{O}_p |x_{t}t\\rangle_{\\mathcal{X}}\\mathcal{X} and $\\mathcal{O}^{\\dagger}_p |x_{t}\\rangle_{\\mathcal{X}}$\\mathcal{O}^{\\dagger}\\dagger_p |x_{t}t\\rangle_{\\mathcal{X}}\\mathcal{X}. This claim is not actually correct because quantum mean estimation requires being able to apply oracles to quantum states created during the run of the mean estimation. This issue can be fixed by allowing quantum mean estimation to query $\\mathcal{O}_p$\\mathcal{O}_p on superpositions of $|x_{t}\\rangle_{\\mathcal{X}}$|x_{t}t\\rangle_{\\mathcal{X}}\\mathcal{X}. This effectively results in a model similar to ours, with groups of exploration steps being exploration phases and runs of quantum mean estimation being generative phases. The main difference between both models, however, is the choice for the degree of control over the environment: either the agent can manipulate the environment's state during generative phases and apply $\\mathcal{O}_p$\\mathcal{O}_p and $\\mathcal{O}_p^\\dagger$\\mathcal{O}_p^\\dagger onto states that the agent prepares (our model), or the environment's state $|x_{t}\\rangle_{\\mathcal{X}}$|x_{t}t\\rangle_{\\mathcal{X}}\\mathcal{X} is fixed from the end of the previous exploration phase and applications of $\\mathcal{O}_p$\\mathcal{O}_p and $\\mathcal{O}_p^\\dagger$\\mathcal{O}_p^\\dagger are done on such state $|x_{t}\\rangle_{\\mathcal{X}}$|x_{t}t\\rangle_{\\mathcal{X}}\\mathcal{X} only. Finally, the length of Zhong \\emph{et al.}'s generative phases is also proportional to the previous exploration period $\\tau_k$\\tau_k. This is implicit when they use quantum multi-dimensional amplitude estimation~\\cite{van2021quantum} with $O(n(x,a))$O(n(x,a)) iterations for each state-action pair $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$(x,a)\\in\\mathcal{X}\\times\\mathcal{A}, where $n(x,a)$n(x,a) is the number of times $(x,a)$(x,a) has been observed in the last exploration phase.\n\n\n", "appendix": false}, "Computing optimal policies for finite-horizon MDPs": {"content": "\n\\label{sec:optimal_policies_finite-horizon}\n\nSeveral RL algorithms~\\cite{auer2006logarithmic,bartlett2009regal,auer2008near,ortner2012online,lakshmanan2015improved,fruit2018efficient} compute approximate optimal policies for the target unknown MDP $M$M given empirical estimates of the true stochastic kernels $p$p and rewards $r$r. As we shall see in \\Cref{sec:Quantum_UCCRL}, our RL algorithms also require approximate optimal policies for $M$M, therefore, before introducing our RL algorithms under the online-learning model from \\Cref{sec:RL_model}, we study the problem of finding approximate optimal policies for $M$M if one is given access to oracles $\\mathcal{O}_p$\\mathcal{O}_p and $\\mathcal{O}_r$\\mathcal{O}_r and their inverses. We start by considering finite-horizon MDPs $M = \\langle \\mathcal{X},\\mathcal{A},H,p,r\\rangle$M = \\langle \\mathcal{X},\\mathcal{A},H,p,r\\rangle. As briefly mentioned in \\Cref{sec:mdp}, a simple backward induction algorithm can be used to find $V^\\ast_t(x)$V^\\ast_t(x) for all $t\\in[H]$t\\in[H]: For $t=H,H-1,\\dots,1$t=H,H-1,\\dots,1, compute\n\\begin{align*}\n    \\forall x\\in\\mathcal{X}: \\qquad u_{t}(x) = (\\mathcal{L}u_{t+1})(x) \\qquad\\text{and}\\qquad \\pi_{t}(x) \\in \\argmax_{a\\in\\mathcal{A}}\\{(\\mathcal{L}_a u_{t+1})(x)\\}.\n\\end{align*}\\begin{align*}\n    \\forall x\\in\\mathcal{X}: \\qquad u_{t}(x) = (\\mathcal{L}u_{t+1})(x) \\qquad\\text{and}\\qquad \\pi_{t}(x) \\in \\argmax_{a\\in\\mathcal{A}}\\{(\\mathcal{L}_a u_{t+1})(x)\\}.\n\\end{align*}\n    \\forall x\\in\\mathcal{X}: \\qquad u_{t}t(x) = (\\mathcal{L}u_{t+1}t+1)(x) \\qquad\\text{and}\\qquad \\pi_{t}t(x) \\in \\argmax_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{(\\mathcal{L}_a u_{t+1}t+1)(x)\\}.\n\nConsider the policy $\\pi = (\\pi_t)_{t\\in[H]}\\in\\Pi^{\\rm D}$\\pi = (\\pi_t)_{t\\in[H]}t\\in[H]\\in\\Pi^{\\rm D}\\rm D. Then one can prove that $u_{t}(x) = V_t^{\\pi}(x) = V_t^\\ast(x)$u_{t}t(x) = V_t^{\\pi}\\pi(x) = V_t^\\ast(x) for all $x\\in\\mathcal{X}$x\\in\\mathcal{X}, meaning that $u_{t}$u_{t}t is the optimal total expected reward from time $t$t onward and $\\pi$\\pi is an optimal policy~\\cite[Theorem~4.5.1]{puterman2014markov}. If $\\mathcal{X}=\\mathcal{S}$\\mathcal{X}=\\mathcal{S} is finite with size $S$S, then this simple backward induction algorithm has time complexity $O(HS^2A)$O(HS^2A). However, it is possible to improve this complexity within a generative model with access to oracle $\\mathcal{C}_p$\\mathcal{C}_p, as we shall see next.\n\n\\subsection{Classical approximate backward induction algorithm}\n\\label{sec:modern_classical_value_iteration}\n\nThe classical complexity of computing $\\varepsilon$\\varepsilon-approximate optimal policies for finite-horizon MDPs with generative model has been completely characterised by Sidford \\emph{et al.}~\\cite{sidford2018near}. By leveraging several techniques, the authors proposed a modern version of the above backward induction algorithm with sample complexity $\\widetilde{O}(H^3 SA/\\varepsilon^2)$\\widetilde{O}(H^3 SA/\\varepsilon^2), which is optimal up to logarithmic factors~\\cite[Corollary~F.7]{sidford2018near}. In this section, we review the algorithm from Sidford \\emph{et al.}~\\cite{sidford2018near}, which is described in \\Cref{algo:classical_backward_recursion}, already generalised to compact state spaces.\n\nThe backward induction algorithm from Sidford \\emph{et al.}~\\cite{sidford2018near} works in \\emph{epochs}. By starting at the beginning of the $k$k-th epoch with some initial functions $u^{(k-1)}_{1},u^{(k-1)}_{2},\\dots,u^{(k-1)}_{H}:\\mathcal{X}\\to\\mathbb{R}$u^{(k-1)}(k-1)_{1}1,u^{(k-1)}(k-1)_{2}2,\\dots,u^{(k-1)}(k-1)_{H}H:\\mathcal{X}\\to\\mathbb{R} such that $0 \\leq V^\\ast_t(x) - u^{(k-1)}_{t}(x) \\leq 2\\epsilon_k$0 \\leq V^\\ast_t(x) - u^{(k-1)}(k-1)_{t}t(x) \\leq 2\\epsilon_k for all $x\\in\\mathcal{X}$x\\in\\mathcal{X} and $t\\in[H]$t\\in[H], by the end of the $k$k-th epoch (which is the beginning of the $(k+1)$(k+1)-th epoch), their algorithm produces $u^{(k)}_{1},u^{(k)}_{2},\\dots,u^{(k)}_{H}:\\mathcal{X}\\to\\mathbb{R}$u^{(k)}(k)_{1}1,u^{(k)}(k)_{2}2,\\dots,u^{(k)}(k)_{H}H:\\mathcal{X}\\to\\mathbb{R} such that $0 \\leq V_t^\\ast(x) - u^{(k)}_{t}(x) \\leq \\epsilon_k$0 \\leq V_t^\\ast(x) - u^{(k)}(k)_{t}t(x) \\leq \\epsilon_k, thus halving the initial error. By starting with zero functions $u^{(0)}_1, \\dots, u^{(0)}_H \\equiv 0$u^{(0)}(0)_1, \\dots, u^{(0)}(0)_H \\equiv 0 at the first epoch, and noticing that $V_t^\\ast(x) \\leq H$V_t^\\ast(x) \\leq H, then one only needs to iterate $O(\\log(H/\\varepsilon))$O(\\log(H/\\varepsilon)) times in order to obtain a final $\\varepsilon$\\varepsilon-approximation.\n\nNow let us focus on a single epoch. The algorithm from Sidford \\emph{et al.}~\\cite{sidford2018near} uses three crucial techniques: \\emph{monotonicity}, \\emph{variance reduction}, and \\emph{total-variance} techniques. The monotonicity technique means maintaining the monotonicity condition $u^{(k)}_t(x) \\leq (\\mathcal{L}_{\\pi^{(k)}_t}u^{(k)}_{t+1})(x)$u^{(k)}(k)_t(x) \\leq (\\mathcal{L}_{\\pi^{(k)}_t}\\pi^{(k)}(k)_tu^{(k)}(k)_{t+1}t+1)(x) throughout the epoch, which guarantees that $u^{(k)}_t(x) \\leq V_t^{\\pi^{(k)}}(x) \\leq V_t^\\ast(x)$u^{(k)}(k)_t(x) \\leq V_t^{\\pi^{(k)}}\\pi^{(k)}(k)(x) \\leq V_t^\\ast(x) by the monotonicity of the Bellman operator. Therefore, an $\\epsilon$\\epsilon-optimal value function $u^{(k)}_t(x)$u^{(k)}(k)_t(x) satisfying such monotonicity condition yields an $\\epsilon$\\epsilon-optimal policy, which is not true otherwise. In general, an $\\epsilon$\\epsilon-optimal value function yields an $2\\epsilon/H$2\\epsilon/H-optimal greedy policy in the worst case~\\cite{Singh1994upper,bertsekas2012dynamic}.\n\nNaively, at each time step $t\\in[H]$t\\in[H] of each epoch $k$k, we need to estimate the quantity $\\int_{\\mathcal{X}}p(\\text{d}x'|x,a)u^{(k)}_t(x')$\\int_{\\mathcal{X}}\\mathcal{X}p(\\text{d}x'|x,a)u^{(k)}(k)_t(x') up to additive error $\\epsilon_k/H$\\epsilon_k/H in order to achieve the new value functions $u_{1}^{(k)},\\dots,u_{H}^{(k)}$u_{1}1^{(k)}(k),\\dots,u_{H}H^{(k)}(k) with error $\\epsilon_k$\\epsilon_k. Since $\\|u^{(k)}_t\\|_\\infty \\leq H$\\|u^{(k)}(k)_t\\|_\\infty \\leq H, by a Hoeffding bound $\\widetilde{O}(H^4/\\epsilon_k^2)$\\widetilde{O}(H^4/\\epsilon_k^2) samples would suffice for each time step $t\\in[H]$t\\in[H], leading to a total of $\\widetilde{O}(H^5/\\epsilon_k^2)$\\widetilde{O}(H^5/\\epsilon_k^2) samples for all time steps. The variance reduction technique rewrites the standard backward induction iteration as\n\\begin{align*}\n    u_{t}^{(k)}(x) \\gets \\max_{a\\in\\mathcal{A}}\\left\\{r(x,a) + \\int_{\\mathcal{X}}p(\\text{d}x'|x,a)(u^{(k)}_{t+1}(x') - u^{(k-1)}_{t+1}(x')) + \\int_{\\mathcal{X}}p(\\text{d}x'|x,a) u^{(k-1)}_{t+1}(x')  \\right\\}.\n\\end{align*}\\begin{align*}\n    u_{t}^{(k)}(x) \\gets \\max_{a\\in\\mathcal{A}}\\left\\{r(x,a) + \\int_{\\mathcal{X}}p(\\text{d}x'|x,a)(u^{(k)}_{t+1}(x') - u^{(k-1)}_{t+1}(x')) + \\int_{\\mathcal{X}}p(\\text{d}x'|x,a) u^{(k-1)}_{t+1}(x')  \\right\\}.\n\\end{align*}\n    u_{t}t^{(k)}(k)(x) \\gets \\max_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\left\\{r(x,a) + \\int_{\\mathcal{X}}\\mathcal{X}p(\\text{d}x'|x,a)(u^{(k)}(k)_{t+1}t+1(x') - u^{(k-1)}(k-1)_{t+1}t+1(x')) + \\int_{\\mathcal{X}}\\mathcal{X}p(\\text{d}x'|x,a) u^{(k-1)}(k-1)_{t+1}t+1(x')  \\right\\}.\n\nThe main idea is that the quantities $\\int_{\\mathcal{X}}p(\\text{d}x'|x,a) u^{(k-1)}_{t}(x')$\\int_{\\mathcal{X}}\\mathcal{X}p(\\text{d}x'|x,a) u^{(k-1)}(k-1)_{t}t(x') for all $t\\in[H]$t\\in[H] can be computed at the beginning of the epoch using the same batch of $\\widetilde{O}(H^4/\\epsilon^2)$\\widetilde{O}(H^4/\\epsilon^2) samples, which saves a factor of $H$H. Regarding the other quantity $\\int_{\\mathcal{X}}p(\\text{d}x'|x,a)(u^{(k)}_{t+1}(x') - u^{(k-1)}_{t+1}(x'))$\\int_{\\mathcal{X}}\\mathcal{X}p(\\text{d}x'|x,a)(u^{(k)}(k)_{t+1}t+1(x') - u^{(k-1)}(k-1)_{t+1}t+1(x')), since $\\|u^{(k)}_{t+1} - u^{(k-1)}_{t+1}\\|_\\infty \\leq \\epsilon_k$\\|u^{(k)}(k)_{t+1}t+1 - u^{(k-1)}(k-1)_{t+1}t+1\\|_\\infty \\leq \\epsilon_k by the monotonicity condition, it can be approximated up to error $\\epsilon_k/2H$\\epsilon_k/2H using only $\\widetilde{O}(H^2)$\\widetilde{O}(H^2) samples for each time step $t\\in[H]$t\\in[H], leading to a total of $\\widetilde{O}(H^3)$\\widetilde{O}(H^3) samples over all time steps.\n\nFinally, in order to reduce the sample complexity dependence on $H$H from $O(H^4)$O(H^4) down to $O(H^3)$O(H^3), the final technique, total variance, is employed, the main idea being that the true error accumulates much less than the naive sum of estimation errors at each time step. This means that one does not require to set the update error to be $\\epsilon_k/H$\\epsilon_k/H at each time step. It can be shown~\\cite{sidford2018near} (see \\Cref{fact:upper_bound_variance} below) that the total accumulation error is $\\sqrt{H^3/m}$\\sqrt{H^3/m}, thus picking $m = O(H^3/\\epsilon_k^2)$m = O(H^3/\\epsilon_k^2) samples is sufficient to obtain a total error equal to $\\epsilon_k$\\epsilon_k.\n\nBy putting all the three aforementioned techniques together, one arrives at the result from Sidford \\emph{et al.}~\\cite{sidford2018near}. For completeness, we include a proof of correctness for \\Cref{algo:classical_backward_recursion}, already generalised to compact state spaces. We start with the empirical estimation error for the means and variances on \\Cref{line:classical_backward_line1,line:classical_backward_line2} in \\Cref{algo:classical_backward_recursion}.\n\\begin{lemma}\\label{lem:bernstein_inequality_classical_backward}\n    Let $\\mathcal{X}$ a compact set with $\\frac{1}{n}$-net $\\mathcal{S}_n$, $\\mathcal{A}$ a finite set, and functions $\\{u_t\\!\\in\\!\\mathscr{B}(\\mathcal{X})\\}_{t\\in[H]}$. For each $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, consider samples $x_{s,a}^{(1)},\\dots,x_{s,a}^{(m)}\\in\\mathcal{X}$ from $p(\\cdot|s,a)$. For $t\\in[H]$, let $\\mu_t(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_t(x')$ and $\\sigma_t(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_t(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)u_t(x')\\big)^2$. Let the empirical estimates be\n    %\n    \\begin{align*}\n        \\forall (s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]:\\qquad\n        \\begin{aligned}\n            \\widetilde{\\mu}_t(s,a) &:= \\frac{1}{m}\\sum_{i=1}^m u_t(x_{s,a}^{(i)}),\\\\\n            \\widetilde{\\sigma}_t(s,a) &:= \\frac{1}{m}\\sum_{i=1}^{m} u_t(x^{(i)}_{s,a})^2 - \\left(\\frac{1}{m}\\sum_{i=1}^{m} u_t(x^{(i)}_{s,a}) \\right)^2.\n        \\end{aligned}\n    \\end{align*}\n    %\n    Then, with probability at least $1-\\delta$, for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$,\n    %\n    \\begin{align*}\n        |\\widetilde{\\mu}_t(s,a) - \\mu_t(s,a)| &\\leq \\sqrt{\\frac{2\\sigma_t(s,a)}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}} + \\frac{2\\|u_t\\|_\\infty}{3m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta},\\\\\n        |\\widetilde{\\sigma}_t(s,a) - \\sigma_t(s,a)| &\\leq 4\\sqrt{\\frac{2}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}} \\|u_t\\|_\\infty^2.\n    \\end{align*}\n    %\n    Moreover, $|\\mu_t(x,a) - \\mu_t(s,a)| \\leq Ln^{-\\alpha}\\|u_t\\|_\\infty$ and $|\\sigma_t(x,a) - \\sigma_t(s,a)| \\leq 4Ln^{-\\alpha}\\|u_t\\|_\\infty^2$ under {\\rm \\Cref{ass:Lipschitz}} for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, $a\\in\\mathcal{A}$, and $t\\in[H]$.\n\\end{lemma}\\begin{lemma}\\label{lem:bernstein_inequality_classical_backward}\n    Let $\\mathcal{X}$ a compact set with $\\frac{1}{n}$-net $\\mathcal{S}_n$, $\\mathcal{A}$ a finite set, and functions $\\{u_t\\!\\in\\!\\mathscr{B}(\\mathcal{X})\\}_{t\\in[H]}$. For each $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, consider samples $x_{s,a}^{(1)},\\dots,x_{s,a}^{(m)}\\in\\mathcal{X}$ from $p(\\cdot|s,a)$. For $t\\in[H]$, let $\\mu_t(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_t(x')$ and $\\sigma_t(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_t(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)u_t(x')\\big)^2$. Let the empirical estimates be\n    %\n    \\begin{align*}\n        \\forall (s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]:\\qquad\n        \\begin{aligned}\n            \\widetilde{\\mu}_t(s,a) &:= \\frac{1}{m}\\sum_{i=1}^m u_t(x_{s,a}^{(i)}),\\\\\n            \\widetilde{\\sigma}_t(s,a) &:= \\frac{1}{m}\\sum_{i=1}^{m} u_t(x^{(i)}_{s,a})^2 - \\left(\\frac{1}{m}\\sum_{i=1}^{m} u_t(x^{(i)}_{s,a}) \\right)^2.\n        \\end{aligned}\n    \\end{align*}\n    %\n    Then, with probability at least $1-\\delta$, for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$,\n    %\n    \\begin{align*}\n        |\\widetilde{\\mu}_t(s,a) - \\mu_t(s,a)| &\\leq \\sqrt{\\frac{2\\sigma_t(s,a)}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}} + \\frac{2\\|u_t\\|_\\infty}{3m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta},\\\\\n        |\\widetilde{\\sigma}_t(s,a) - \\sigma_t(s,a)| &\\leq 4\\sqrt{\\frac{2}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}} \\|u_t\\|_\\infty^2.\n    \\end{align*}\n    %\n    Moreover, $|\\mu_t(x,a) - \\mu_t(s,a)| \\leq Ln^{-\\alpha}\\|u_t\\|_\\infty$ and $|\\sigma_t(x,a) - \\sigma_t(s,a)| \\leq 4Ln^{-\\alpha}\\|u_t\\|_\\infty^2$ under {\\rm \\Cref{ass:Lipschitz}} for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, $a\\in\\mathcal{A}$, and $t\\in[H]$.\n\\end{lemma}\\label{lem:bernstein_inequality_classical_backward}\n    Let $\\mathcal{X}$\\mathcal{X} a compact set with $\\frac{1}{n}$\\frac{1}{n}-net $\\mathcal{S}_n$\\mathcal{S}_n, $\\mathcal{A}$\\mathcal{A} a finite set, and functions $\\{u_t\\!\\in\\!\\mathscr{B}(\\mathcal{X})\\}_{t\\in[H]}$\\{u_t\\!\\in\\!\\mathscr{B}(\\mathcal{X})\\}_{t\\in[H]}t\\in[H]. For each $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}, consider samples $x_{s,a}^{(1)},\\dots,x_{s,a}^{(m)}\\in\\mathcal{X}$x_{s,a}s,a^{(1)}(1),\\dots,x_{s,a}s,a^{(m)}(m)\\in\\mathcal{X} from $p(\\cdot|s,a)$p(\\cdot|s,a). For $t\\in[H]$t\\in[H], let $\\mu_t(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_t(x')$\\mu_t(x,a) := \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)u_t(x') and $\\sigma_t(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_t(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)u_t(x')\\big)^2$\\sigma_t(x,a) := \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)u_t(x')^2 - \\big(\\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)u_t(x')\\big)^2. Let the empirical estimates be\n    \n        \\forall (s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]:\\qquad\n        \n            \\widetilde{\\mu}_t(s,a) &:= \\frac{1}{m}\\sum_{i=1}i=1^m u_t(x_{s,a}s,a^{(i)}(i)),\\\\\n            \\widetilde{\\sigma}_t(s,a) &:= \\frac{1}{m}\\sum_{i=1}i=1^{m}m u_t(x^{(i)}(i)_{s,a}s,a)^2 - \\left(\\frac{1}{m}\\sum_{i=1}i=1^{m}m u_t(x^{(i)}(i)_{s,a}s,a) \\right)^2.\n        \n    \n    Then, with probability at least $1-\\delta$1-\\delta, for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H],\n    \n        |\\widetilde{\\mu}_t(s,a) - \\mu_t(s,a)| &\\leq \\sqrt{\\frac{2\\sigma_t(s,a)}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}} + \\frac{2\\|u_t\\|_\\infty}{3m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta},\\\\\n        |\\widetilde{\\sigma}_t(s,a) - \\sigma_t(s,a)| &\\leq 4\\sqrt{\\frac{2}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}} \\|u_t\\|_\\infty^2.\n    \n    Moreover, $|\\mu_t(x,a) - \\mu_t(s,a)| \\leq Ln^{-\\alpha}\\|u_t\\|_\\infty$|\\mu_t(x,a) - \\mu_t(s,a)| \\leq Ln^{-\\alpha}-\\alpha\\|u_t\\|_\\infty and $|\\sigma_t(x,a) - \\sigma_t(s,a)| \\leq 4Ln^{-\\alpha}\\|u_t\\|_\\infty^2$|\\sigma_t(x,a) - \\sigma_t(s,a)| \\leq 4Ln^{-\\alpha}-\\alpha\\|u_t\\|_\\infty^2 under {\\rm \\Cref{ass:Lipschitz}}\\rm \\Cref{ass:Lipschitz} for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n, $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s), $a\\in\\mathcal{A}$a\\in\\mathcal{A}, and $t\\in[H]$t\\in[H].\n\n\\begin{proof}\n    By Bernstein's inequality (\\Cref{fact:bernstein}) and a union bound over $\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$, then, with probability at least $1-\\frac{\\delta}{4}$, for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$,\n    %\n    \\begin{align*}\n        |\\widetilde{\\mu}_t(s,a) - \\mu_t(s,a)| \\leq \\sqrt{\\frac{2\\sigma_t(s,a)}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}} + \\frac{2\\|u_t\\|_\\infty}{3m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}, \n    \\end{align*}\n    %\n    given the first inequality. Regarding the second inequality, by Hoeffding's inequality (\\Cref{fact:hoeffding}) and a union bound over $\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$, with probability at least $1-\\frac{\\delta}{2}$, for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$,\n    %\n    \\begin{align*}\n        |\\widetilde{\\mu}_t(s,a) - \\mu_t(s,a)| &\\leq \\sqrt{\\frac{2}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}} \\|u_t\\|_\\infty,\\\\\n        \\left|\\frac{1}{m}\\sum_{i=1}^m u_t(x^{(i)}_{s,a})^2 - \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t(x')^2\\right| &\\leq \\sqrt{\\frac{2}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}} \\|u_t\\|_\\infty^2.\n    \\end{align*}\n    %\n    Call $\\theta := \\frac{1}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}$. Now notice that, conditioned on the above,\n    %\n    \\begin{align*}\n        |\\widetilde{\\mu}_t(s,a)^2 - \\mu_t(s,a)^2| &= |\\widetilde{\\mu}_t(s,a) + \\mu_t(s,a)||\\widetilde{\\mu}_t(s,a) - \\mu_t(s,a)|\\\\\n        &\\leq (2\\mu_t(s,a) + \\sqrt{2\\theta} \\|u_t\\|_\\infty )\\sqrt{2\\theta} \\|u_t\\|_\\infty\\\\\n        &\\leq (2\\sqrt{2\\theta} + 2\\theta)\\|u_t\\|_\\infty^2 \\tag{$\\mu_t(s,a) \\leq \\|u_t\\|_\\infty$}\\\\\n        &\\leq 3\\sqrt{2\\theta}\\|u_t\\|_\\infty^2,\n    \\end{align*}\n    %\n    assuming that $m$ is large enough so that $2\\theta \\leq 1$. This means that, with probability at least $1-\\frac{\\delta}{2}$, for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$,\n    %\n    \\begin{align*}\n        |\\widetilde{\\sigma}_t(s,a) - \\sigma_t(s,a)| &\\leq |\\widetilde{\\mu}_t(s,a)^2 - \\mu_t(s,a)^2| + \\left|\\frac{1}{m}\\sum_{i=1}^m u_t(x^{(i)}_{s,a})^2 - \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_t(x')^2\\right|\\\\\n        &\\leq 4\\sqrt{\\frac{2}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}}\\|u_t\\|_\\infty^2,\n    \\end{align*}\n    %\n    as required. Finally, by \\Cref{ass:Lipschitz}, for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, $a\\in\\mathcal{A}$, and $t\\in[H]$,\n    %\n    \\begin{align*}\n        |\\mu_t(x,a) - \\mu_t(s,a)| \\leq \\int_{\\mathcal{X}}|p(\\rd x'|x,a) -p(\\rd x'|s,a)| |u_t(x')| \\leq Ln^{-\\alpha}\\|u_t\\|_\\infty,\\\\\n        \\left| \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_t(x')^2 - \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t(x')^2\\right| \\leq Ln^{-\\alpha}\\|u_t\\|_\\infty^2,\n    \\end{align*}\n    %\n    and following a similar argument as above, $|\\sigma_t(x,a) - \\sigma_t(s,a)| \\leq 4Ln^{-\\alpha}\\|u_t\\|_\\infty^2$.\n\\end{proof}\\begin{proof}\n    By Bernstein's inequality (\\Cref{fact:bernstein}) and a union bound over $\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$, then, with probability at least $1-\\frac{\\delta}{4}$, for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$,\n    %\n    \\begin{align*}\n        |\\widetilde{\\mu}_t(s,a) - \\mu_t(s,a)| \\leq \\sqrt{\\frac{2\\sigma_t(s,a)}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}} + \\frac{2\\|u_t\\|_\\infty}{3m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}, \n    \\end{align*}\n    %\n    given the first inequality. Regarding the second inequality, by Hoeffding's inequality (\\Cref{fact:hoeffding}) and a union bound over $\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$, with probability at least $1-\\frac{\\delta}{2}$, for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$,\n    %\n    \\begin{align*}\n        |\\widetilde{\\mu}_t(s,a) - \\mu_t(s,a)| &\\leq \\sqrt{\\frac{2}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}} \\|u_t\\|_\\infty,\\\\\n        \\left|\\frac{1}{m}\\sum_{i=1}^m u_t(x^{(i)}_{s,a})^2 - \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t(x')^2\\right| &\\leq \\sqrt{\\frac{2}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}} \\|u_t\\|_\\infty^2.\n    \\end{align*}\n    %\n    Call $\\theta := \\frac{1}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}$. Now notice that, conditioned on the above,\n    %\n    \\begin{align*}\n        |\\widetilde{\\mu}_t(s,a)^2 - \\mu_t(s,a)^2| &= |\\widetilde{\\mu}_t(s,a) + \\mu_t(s,a)||\\widetilde{\\mu}_t(s,a) - \\mu_t(s,a)|\\\\\n        &\\leq (2\\mu_t(s,a) + \\sqrt{2\\theta} \\|u_t\\|_\\infty )\\sqrt{2\\theta} \\|u_t\\|_\\infty\\\\\n        &\\leq (2\\sqrt{2\\theta} + 2\\theta)\\|u_t\\|_\\infty^2 \\tag{$\\mu_t(s,a) \\leq \\|u_t\\|_\\infty$}\\\\\n        &\\leq 3\\sqrt{2\\theta}\\|u_t\\|_\\infty^2,\n    \\end{align*}\n    %\n    assuming that $m$ is large enough so that $2\\theta \\leq 1$. This means that, with probability at least $1-\\frac{\\delta}{2}$, for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$,\n    %\n    \\begin{align*}\n        |\\widetilde{\\sigma}_t(s,a) - \\sigma_t(s,a)| &\\leq |\\widetilde{\\mu}_t(s,a)^2 - \\mu_t(s,a)^2| + \\left|\\frac{1}{m}\\sum_{i=1}^m u_t(x^{(i)}_{s,a})^2 - \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_t(x')^2\\right|\\\\\n        &\\leq 4\\sqrt{\\frac{2}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}}\\|u_t\\|_\\infty^2,\n    \\end{align*}\n    %\n    as required. Finally, by \\Cref{ass:Lipschitz}, for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, $a\\in\\mathcal{A}$, and $t\\in[H]$,\n    %\n    \\begin{align*}\n        |\\mu_t(x,a) - \\mu_t(s,a)| \\leq \\int_{\\mathcal{X}}|p(\\rd x'|x,a) -p(\\rd x'|s,a)| |u_t(x')| \\leq Ln^{-\\alpha}\\|u_t\\|_\\infty,\\\\\n        \\left| \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_t(x')^2 - \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t(x')^2\\right| \\leq Ln^{-\\alpha}\\|u_t\\|_\\infty^2,\n    \\end{align*}\n    %\n    and following a similar argument as above, $|\\sigma_t(x,a) - \\sigma_t(s,a)| \\leq 4Ln^{-\\alpha}\\|u_t\\|_\\infty^2$.\n\\end{proof}\n    By Bernstein's inequality (\\Cref{fact:bernstein}) and a union bound over $\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$\\mathcal{S}_n\\times\\mathcal{A}\\times[H], then, with probability at least $1-\\frac{\\delta}{4}$1-\\frac{\\delta}{4}, for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H],\n    \n        |\\widetilde{\\mu}_t(s,a) - \\mu_t(s,a)| \\leq \\sqrt{\\frac{2\\sigma_t(s,a)}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}} + \\frac{2\\|u_t\\|_\\infty}{3m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}, \n    \n    given the first inequality. Regarding the second inequality, by Hoeffding's inequality (\\Cref{fact:hoeffding}) and a union bound over $\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$\\mathcal{S}_n\\times\\mathcal{A}\\times[H], with probability at least $1-\\frac{\\delta}{2}$1-\\frac{\\delta}{2}, for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H],\n    \n        |\\widetilde{\\mu}_t(s,a) - \\mu_t(s,a)| &\\leq \\sqrt{\\frac{2}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}} \\|u_t\\|_\\infty,\\\\\n        \\left|\\frac{1}{m}\\sum_{i=1}i=1^m u_t(x^{(i)}(i)_{s,a}s,a)^2 - \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|s,a)u_t(x')^2\\right| &\\leq \\sqrt{\\frac{2}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}} \\|u_t\\|_\\infty^2.\n    \n    Call $\\theta := \\frac{1}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}$\\theta := \\frac{1}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}. Now notice that, conditioned on the above,\n    \n        |\\widetilde{\\mu}_t(s,a)^2 - \\mu_t(s,a)^2| &= |\\widetilde{\\mu}_t(s,a) + \\mu_t(s,a)||\\widetilde{\\mu}_t(s,a) - \\mu_t(s,a)|\\\\\n        &\\leq (2\\mu_t(s,a) + \\sqrt{2\\theta} \\|u_t\\|_\\infty )\\sqrt{2\\theta} \\|u_t\\|_\\infty\\\\\n        &\\leq (2\\sqrt{2\\theta} + 2\\theta)\\|u_t\\|_\\infty^2 \\tag{$\\mu_t(s,a) \\leq \\|u_t\\|_\\infty$}$\\mu_t(s,a) \\leq \\|u_t\\|_\\infty$\\mu_t(s,a) \\leq \\|u_t\\|_\\infty\\\\\n        &\\leq 3\\sqrt{2\\theta}\\|u_t\\|_\\infty^2,\n    \n    assuming that $m$m is large enough so that $2\\theta \\leq 1$2\\theta \\leq 1. This means that, with probability at least $1-\\frac{\\delta}{2}$1-\\frac{\\delta}{2}, for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H],\n    \n        |\\widetilde{\\sigma}_t(s,a) - \\sigma_t(s,a)| &\\leq |\\widetilde{\\mu}_t(s,a)^2 - \\mu_t(s,a)^2| + \\left|\\frac{1}{m}\\sum_{i=1}i=1^m u_t(x^{(i)}(i)_{s,a}s,a)^2 - \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)u_t(x')^2\\right|\\\\\n        &\\leq 4\\sqrt{\\frac{2}{m}\\ln\\frac{8H|\\mathcal{S}_n|A}{\\delta}}\\|u_t\\|_\\infty^2,\n    \n    as required. Finally, by \\Cref{ass:Lipschitz}, for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n, $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s), $a\\in\\mathcal{A}$a\\in\\mathcal{A}, and $t\\in[H]$t\\in[H],\n    \n        |\\mu_t(x,a) - \\mu_t(s,a)| \\leq \\int_{\\mathcal{X}}\\mathcal{X}|p(\\rd x'|x,a) -p(\\rd x'|s,a)| |u_t(x')| \\leq Ln^{-\\alpha}-\\alpha\\|u_t\\|_\\infty,\\\\\n        \\left| \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)u_t(x')^2 - \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|s,a)u_t(x')^2\\right| \\leq Ln^{-\\alpha}-\\alpha\\|u_t\\|_\\infty^2,\n    \n    and following a similar argument as above, $|\\sigma_t(x,a) - \\sigma_t(s,a)| \\leq 4Ln^{-\\alpha}\\|u_t\\|_\\infty^2$|\\sigma_t(x,a) - \\sigma_t(s,a)| \\leq 4Ln^{-\\alpha}-\\alpha\\|u_t\\|_\\infty^2.\n\n\nWe now analyse the empirical estimation error of $\\int_{\\mathcal{X}}p(\\text{d}x'|x,a)(u^{(t+1)}_{k}(x') - u^{(t+1)}_{k-1}(x'))$\\int_{\\mathcal{X}}\\mathcal{X}p(\\text{d}x'|x,a)(u^{(t+1)}(t+1)_{k}k(x') - u^{(t+1)}(t+1)_{k-1}k-1(x')) from \\Cref{line:classical_backward_line3} in \\Cref{algo:classical_backward_recursion}.\n\\begin{lemma}\\label{lem:hoeffding_classical_backward}\n    Let $\\mathcal{X}$ be a compact set with $\\frac{1}{n}$-net $\\mathcal{S}_n$ and $\\mathcal{A}$ a finite set. For each $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, consider samples $x_{s,a}^{(1)},\\dots,x_{s,a}^{(\\ell)}\\in\\mathcal{X}$ from the distribution $p(\\cdot|s,a)$, where $\\ell := \\big\\lceil 2^9 H^2\\ln\\frac{2H|\\mathcal{S}_n|A}{\\delta}\\big\\rceil$. Let $u,v\\in\\mathcal{B}(\\mathcal{X})$ such that $\\|u - v\\|_\\infty \\leq \\epsilon$ for $\\epsilon\\geq 0$. Let the empirical estimates\n    %\n    \\begin{align*}\n        \\forall s\\in\\mathcal{S}_n, x\\in\\mathcal{X}(s), a\\in\\mathcal{A}:\\qquad \\widehat{\\beta}(x,a) := \\widehat{\\beta}(s,a) := \\frac{1}{\\ell}\\sum_{i=1}^\\ell  u(x^{(i)}_{s,a}) - v(x^{(i)}_{s,a}) - \\frac{\\epsilon}{16H} - L n^{-\\alpha}\\epsilon.\n    \\end{align*}\n    %\n    Then, with probability at least $1 - \\frac{\\delta}{H}$, for all $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$,\n    %\n    \\begin{align*}\n        \\int_{\\mathcal{X}}p(\\rd x'|x,a)(u(x') - v(x')) - \\frac{\\epsilon}{8H} - 2L n^{-\\alpha}\\epsilon \\leq \\widehat{\\beta}(x,a) \\leq \\int_{\\mathcal{X}}p(\\rd x'|x,a)(u(x') - v(x')).\n    \\end{align*}\n\\end{lemma}\\begin{lemma}\\label{lem:hoeffding_classical_backward}\n    Let $\\mathcal{X}$ be a compact set with $\\frac{1}{n}$-net $\\mathcal{S}_n$ and $\\mathcal{A}$ a finite set. For each $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, consider samples $x_{s,a}^{(1)},\\dots,x_{s,a}^{(\\ell)}\\in\\mathcal{X}$ from the distribution $p(\\cdot|s,a)$, where $\\ell := \\big\\lceil 2^9 H^2\\ln\\frac{2H|\\mathcal{S}_n|A}{\\delta}\\big\\rceil$. Let $u,v\\in\\mathcal{B}(\\mathcal{X})$ such that $\\|u - v\\|_\\infty \\leq \\epsilon$ for $\\epsilon\\geq 0$. Let the empirical estimates\n    %\n    \\begin{align*}\n        \\forall s\\in\\mathcal{S}_n, x\\in\\mathcal{X}(s), a\\in\\mathcal{A}:\\qquad \\widehat{\\beta}(x,a) := \\widehat{\\beta}(s,a) := \\frac{1}{\\ell}\\sum_{i=1}^\\ell  u(x^{(i)}_{s,a}) - v(x^{(i)}_{s,a}) - \\frac{\\epsilon}{16H} - L n^{-\\alpha}\\epsilon.\n    \\end{align*}\n    %\n    Then, with probability at least $1 - \\frac{\\delta}{H}$, for all $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$,\n    %\n    \\begin{align*}\n        \\int_{\\mathcal{X}}p(\\rd x'|x,a)(u(x') - v(x')) - \\frac{\\epsilon}{8H} - 2L n^{-\\alpha}\\epsilon \\leq \\widehat{\\beta}(x,a) \\leq \\int_{\\mathcal{X}}p(\\rd x'|x,a)(u(x') - v(x')).\n    \\end{align*}\n\\end{lemma}\\label{lem:hoeffding_classical_backward}\n    Let $\\mathcal{X}$\\mathcal{X} be a compact set with $\\frac{1}{n}$\\frac{1}{n}-net $\\mathcal{S}_n$\\mathcal{S}_n and $\\mathcal{A}$\\mathcal{A} a finite set. For each $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}, consider samples $x_{s,a}^{(1)},\\dots,x_{s,a}^{(\\ell)}\\in\\mathcal{X}$x_{s,a}s,a^{(1)}(1),\\dots,x_{s,a}s,a^{(\\ell)}(\\ell)\\in\\mathcal{X} from the distribution $p(\\cdot|s,a)$p(\\cdot|s,a), where $\\ell := \\big\\lceil 2^9 H^2\\ln\\frac{2H|\\mathcal{S}_n|A}{\\delta}\\big\\rceil$\\ell := \\big\\lceil 2^9 H^2\\ln\\frac{2H|\\mathcal{S}_n|A}{\\delta}\\big\\rceil. Let $u,v\\in\\mathcal{B}(\\mathcal{X})$u,v\\in\\mathcal{B}(\\mathcal{X}) such that $\\|u - v\\|_\\infty \\leq \\epsilon$\\|u - v\\|_\\infty \\leq \\epsilon for $\\epsilon\\geq 0$\\epsilon\\geq 0. Let the empirical estimates\n    \n        \\forall s\\in\\mathcal{S}_n, x\\in\\mathcal{X}(s), a\\in\\mathcal{A}:\\qquad \\widehat{\\beta}(x,a) := \\widehat{\\beta}(s,a) := \\frac{1}{\\ell}\\sum_{i=1}i=1^\\ell  u(x^{(i)}(i)_{s,a}s,a) - v(x^{(i)}(i)_{s,a}s,a) - \\frac{\\epsilon}{16H} - L n^{-\\alpha}-\\alpha\\epsilon.\n    \n    Then, with probability at least $1 - \\frac{\\delta}{H}$1 - \\frac{\\delta}{H}, for all $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$(x,a)\\in\\mathcal{X}\\times\\mathcal{A},\n    \n        \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)(u(x') - v(x')) - \\frac{\\epsilon}{8H} - 2L n^{-\\alpha}-\\alpha\\epsilon \\leq \\widehat{\\beta}(x,a) \\leq \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)(u(x') - v(x')).\n    \n\n\\begin{proof}\n    By a Hoeffding's inequality (\\Cref{fact:hoeffding}), \\Cref{ass:Lipschitz}, and a union bound over $\\mathcal{S}_n\\times\\mathcal{A}$, with probability at least $1-\\frac{\\delta}{H}$, for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, and $a\\in\\mathcal{A}$,\n    %\n    \\begin{align*}\n        \\left|\\frac{1}{\\ell}\\sum_{i=1}^\\ell u(x^{(i)}_{s,a}) - v(x^{(i)}_{s,a}) - \\int_{\\mathcal{X}}p(\\rd x'|x,a)(u(x') - v(x')) \\right| &\\leq \\left(\\sqrt{\\frac{2}{\\ell}\\ln\\frac{2H|\\mathcal{S}_n|A}{\\delta}} + Ln^{-\\alpha}\\right)\\|u-v\\|_\\infty\\\\\n        &\\leq \\frac{\\epsilon}{16H} + L n^{-\\alpha}\\epsilon.\n    \\end{align*}\n    %\n    By shifting the estimate to have one-sided error, we obtain the one-sided error $\\frac{\\epsilon}{8H} + 2L n^{-\\alpha}\\epsilon$.\n\\end{proof}\\begin{proof}\n    By a Hoeffding's inequality (\\Cref{fact:hoeffding}), \\Cref{ass:Lipschitz}, and a union bound over $\\mathcal{S}_n\\times\\mathcal{A}$, with probability at least $1-\\frac{\\delta}{H}$, for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, and $a\\in\\mathcal{A}$,\n    %\n    \\begin{align*}\n        \\left|\\frac{1}{\\ell}\\sum_{i=1}^\\ell u(x^{(i)}_{s,a}) - v(x^{(i)}_{s,a}) - \\int_{\\mathcal{X}}p(\\rd x'|x,a)(u(x') - v(x')) \\right| &\\leq \\left(\\sqrt{\\frac{2}{\\ell}\\ln\\frac{2H|\\mathcal{S}_n|A}{\\delta}} + Ln^{-\\alpha}\\right)\\|u-v\\|_\\infty\\\\\n        &\\leq \\frac{\\epsilon}{16H} + L n^{-\\alpha}\\epsilon.\n    \\end{align*}\n    %\n    By shifting the estimate to have one-sided error, we obtain the one-sided error $\\frac{\\epsilon}{8H} + 2L n^{-\\alpha}\\epsilon$.\n\\end{proof}\n    By a Hoeffding's inequality (\\Cref{fact:hoeffding}), \\Cref{ass:Lipschitz}, and a union bound over $\\mathcal{S}_n\\times\\mathcal{A}$\\mathcal{S}_n\\times\\mathcal{A}, with probability at least $1-\\frac{\\delta}{H}$1-\\frac{\\delta}{H}, for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n, $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s), and $a\\in\\mathcal{A}$a\\in\\mathcal{A},\n    \n        \\left|\\frac{1}{\\ell}\\sum_{i=1}i=1^\\ell u(x^{(i)}(i)_{s,a}s,a) - v(x^{(i)}(i)_{s,a}s,a) - \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)(u(x') - v(x')) \\right| &\\leq \\left(\\sqrt{\\frac{2}{\\ell}\\ln\\frac{2H|\\mathcal{S}_n|A}{\\delta}} + Ln^{-\\alpha}-\\alpha\\right)\\|u-v\\|_\\infty\\\\\n        &\\leq \\frac{\\epsilon}{16H} + L n^{-\\alpha}-\\alpha\\epsilon.\n    \n    By shifting the estimate to have one-sided error, we obtain the one-sided error $\\frac{\\epsilon}{8H} + 2L n^{-\\alpha}\\epsilon$\\frac{\\epsilon}{8H} + 2L n^{-\\alpha}-\\alpha\\epsilon.\n\n\n\\begin{algorithm}[t!]\n    \\caption{Classical approximate backward induction algorithm}\n    \\label{algo:classical_backward_recursion}\n    \\begin{algorithmic}[1]  \n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$, finite action space $\\mathcal{A}$, horizon $H$, classical sampling access to probability kernels $p$, failure probability $\\delta\\in(0, 1)$, error $\\varepsilon > 0$.\n\n    \\Ensure $(\\varepsilon + 12Ln^{-\\alpha}H^2)$-optimal policy $\\pi^{(K)} \\in \\Pi^{\\rm D}$. \n\n    \\State Initialise $u^{(0)}_t \\equiv 0$ for all $t\\in[H]$ and $\\pi^{(0)}\\in\\Pi^{\\rm D}$ arbitrary\n\n    \\For{$k=1$ to $K:=\\lceil\\log_2(H/\\varepsilon)\\rceil$}\n\n        \\State $\\epsilon_k \\gets H/2^k$\n\n        \\State $m_k \\gets \\big\\lceil\\frac{128H^3}{\\min\\{\\epsilon^2_{k},1\\}}\\ln\\frac{16H|\\mathcal{S}_n|AK}{\\delta}\\big\\rceil$, $\\ell_k \\gets \\big\\lceil 512 H^2\\ln\\frac{4H|\\mathcal{S}_n|AK}{\\delta}\\big\\rceil$, $\\theta_k \\gets \\frac{1}{m_k}\\ln\\frac{16H|\\mathcal{S}_n|AK}{\\delta}$\n    \n        \\State For each $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, sample $x^{(1)}_{s,a},x^{(2)}_{s,a},\\dots,x^{(m_k)}_{s,a}\\in\\mathcal{X}$\n    \n        \\For{$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$}\n\n            \\State $\\widetilde{\\sigma}^{(k)}_t(s,a) \\gets \\frac{1}{m_k}\\sum_{i=1}^{m_k}(u^{(k-1)}_{t}(x^{(i)}_{s,a}))^2 - \\big(\\frac{1}{m_k}\\sum_{i=1}^{m_k}u^{(k-1)}_{t}(x^{(i)}_{s,a}) \\big)^2$ \\label{line:classical_backward_line1}\n    \n            \\State $\\widehat{\\mu}^{(k)}_t(s,a) \\gets \\frac{1}{m_k}\\sum_{i=1}^{m_k}u^{(k-1)}_{t}(x^{(i)}_{s,a}) - \\sqrt{2\\theta_k\\widetilde{\\sigma}_t^{(k)}(s,a)} - \\left(\\frac{2}{3}\\theta_k +  2(2\\theta_k)^{3/4} + Ln^{-\\alpha}\\right)H$ \\label{line:classical_backward_line2}\n    \n        \\EndFor\n\n        \\State $\\widehat{\\mu}_t^{(k)}(x,a) \\gets \\widehat{\\mu}_t^{(k)}(s,a)$ for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, $a\\in\\mathcal{A}$, and $t\\in[H]$\n\n        \\State $u^{(k)}_H(x) \\gets u^{(k-1)}_{H}(x)$ for all $x\\in\\mathcal{X}$ and $\\pi_H^{(k)}\\gets \\pi_{H}^{(k-1)}$\n\n        \\For{$t=H-1,H-2,\\dots,1$}\n\n            \\For{$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$}\n\n            \\State Sample $\\bar{x}^{(1)}_{s,a},\\bar{x}^{(2)}_{s,a},\\dots,\\bar{x}^{(\\ell_k)}_{s,a}\\in\\mathcal{X}$\n\n            \\State $\\widehat{\\beta}_{t+1}^{(k)}(s,a) \\gets \\frac{1}{\\ell_k}\\sum_{i=1}^{\\ell_k}\\big(u^{(k)}_{t+1}(\\bar{x}^{(i)}_{s,a}) -  u^{(k-1)}_{t+1}(\\bar{x}^{(i)}_{s,a})\\big) - \\frac{\\epsilon_{k}}{4H} - \\frac{3}{2}Ln^{-\\alpha}H$ \\label{line:classical_backward_line3}\n\n            \\EndFor\n\n            \\For{$s\\in\\mathcal{S}_n$}\n\n            \\State $u^{(k)}_t(s) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\widehat{\\mu}_{t+1}^{(k)}(s,a) + \\widehat{\\beta}_{t+1}^{(k)}(s,a)\\} - Ln^{-\\alpha}$\n            \n            \\State $\\pi_t^{(k)}(s) \\gets \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\widehat{\\mu}_{t+1}^{(k)}(s,a) + \\widehat{\\beta}_{t+1}^{(k)}(s,a)\\}$\n\n            \\State If $u^{(k)}_t(s) \\leq u^{(k-1)}_{t}(s)$, then $u^{(k)}_t(s) \\!\\gets\\! u^{(k-1)}_{t}(s)$ and $\\pi_t^{(k)}(s) \\!\\gets\\! \\pi_{t}^{(k-1)}(s)$ $\\forall x\\in\\mathcal{X}(s)$ \\label{line:classical_backward_line4}\n\n            %\\State $u_t^{(k)}(x) \\gets u_t^{(k)}(s)$ and $\\pi_t^{(k)}(x) \\gets \\pi_t^{(k)}(s)$ for all $x\\in\\mathcal{X}(s)$\n\n            \\EndFor\n\n        \\EndFor\n\n    \\EndFor\n    \n    \\State \\Return $\\pi^{(K)}\\in\\Pi^{\\rm D}$\n\\end{algorithmic}\n\\end{algorithm}\\begin{algorithm}[t!]\n    \\caption{Classical approximate backward induction algorithm}\n    \\label{algo:classical_backward_recursion}\n    \\begin{algorithmic}[1]  \n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$, finite action space $\\mathcal{A}$, horizon $H$, classical sampling access to probability kernels $p$, failure probability $\\delta\\in(0, 1)$, error $\\varepsilon > 0$.\n\n    \\Ensure $(\\varepsilon + 12Ln^{-\\alpha}H^2)$-optimal policy $\\pi^{(K)} \\in \\Pi^{\\rm D}$. \n\n    \\State Initialise $u^{(0)}_t \\equiv 0$ for all $t\\in[H]$ and $\\pi^{(0)}\\in\\Pi^{\\rm D}$ arbitrary\n\n    \\For{$k=1$ to $K:=\\lceil\\log_2(H/\\varepsilon)\\rceil$}\n\n        \\State $\\epsilon_k \\gets H/2^k$\n\n        \\State $m_k \\gets \\big\\lceil\\frac{128H^3}{\\min\\{\\epsilon^2_{k},1\\}}\\ln\\frac{16H|\\mathcal{S}_n|AK}{\\delta}\\big\\rceil$, $\\ell_k \\gets \\big\\lceil 512 H^2\\ln\\frac{4H|\\mathcal{S}_n|AK}{\\delta}\\big\\rceil$, $\\theta_k \\gets \\frac{1}{m_k}\\ln\\frac{16H|\\mathcal{S}_n|AK}{\\delta}$\n    \n        \\State For each $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, sample $x^{(1)}_{s,a},x^{(2)}_{s,a},\\dots,x^{(m_k)}_{s,a}\\in\\mathcal{X}$\n    \n        \\For{$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$}\n\n            \\State $\\widetilde{\\sigma}^{(k)}_t(s,a) \\gets \\frac{1}{m_k}\\sum_{i=1}^{m_k}(u^{(k-1)}_{t}(x^{(i)}_{s,a}))^2 - \\big(\\frac{1}{m_k}\\sum_{i=1}^{m_k}u^{(k-1)}_{t}(x^{(i)}_{s,a}) \\big)^2$ \\label{line:classical_backward_line1}\n    \n            \\State $\\widehat{\\mu}^{(k)}_t(s,a) \\gets \\frac{1}{m_k}\\sum_{i=1}^{m_k}u^{(k-1)}_{t}(x^{(i)}_{s,a}) - \\sqrt{2\\theta_k\\widetilde{\\sigma}_t^{(k)}(s,a)} - \\left(\\frac{2}{3}\\theta_k +  2(2\\theta_k)^{3/4} + Ln^{-\\alpha}\\right)H$ \\label{line:classical_backward_line2}\n    \n        \\EndFor\n\n        \\State $\\widehat{\\mu}_t^{(k)}(x,a) \\gets \\widehat{\\mu}_t^{(k)}(s,a)$ for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, $a\\in\\mathcal{A}$, and $t\\in[H]$\n\n        \\State $u^{(k)}_H(x) \\gets u^{(k-1)}_{H}(x)$ for all $x\\in\\mathcal{X}$ and $\\pi_H^{(k)}\\gets \\pi_{H}^{(k-1)}$\n\n        \\For{$t=H-1,H-2,\\dots,1$}\n\n            \\For{$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$}\n\n            \\State Sample $\\bar{x}^{(1)}_{s,a},\\bar{x}^{(2)}_{s,a},\\dots,\\bar{x}^{(\\ell_k)}_{s,a}\\in\\mathcal{X}$\n\n            \\State $\\widehat{\\beta}_{t+1}^{(k)}(s,a) \\gets \\frac{1}{\\ell_k}\\sum_{i=1}^{\\ell_k}\\big(u^{(k)}_{t+1}(\\bar{x}^{(i)}_{s,a}) -  u^{(k-1)}_{t+1}(\\bar{x}^{(i)}_{s,a})\\big) - \\frac{\\epsilon_{k}}{4H} - \\frac{3}{2}Ln^{-\\alpha}H$ \\label{line:classical_backward_line3}\n\n            \\EndFor\n\n            \\For{$s\\in\\mathcal{S}_n$}\n\n            \\State $u^{(k)}_t(s) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\widehat{\\mu}_{t+1}^{(k)}(s,a) + \\widehat{\\beta}_{t+1}^{(k)}(s,a)\\} - Ln^{-\\alpha}$\n            \n            \\State $\\pi_t^{(k)}(s) \\gets \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\widehat{\\mu}_{t+1}^{(k)}(s,a) + \\widehat{\\beta}_{t+1}^{(k)}(s,a)\\}$\n\n            \\State If $u^{(k)}_t(s) \\leq u^{(k-1)}_{t}(s)$, then $u^{(k)}_t(s) \\!\\gets\\! u^{(k-1)}_{t}(s)$ and $\\pi_t^{(k)}(s) \\!\\gets\\! \\pi_{t}^{(k-1)}(s)$ $\\forall x\\in\\mathcal{X}(s)$ \\label{line:classical_backward_line4}\n\n            %\\State $u_t^{(k)}(x) \\gets u_t^{(k)}(s)$ and $\\pi_t^{(k)}(x) \\gets \\pi_t^{(k)}(s)$ for all $x\\in\\mathcal{X}(s)$\n\n            \\EndFor\n\n        \\EndFor\n\n    \\EndFor\n    \n    \\State \\Return $\\pi^{(K)}\\in\\Pi^{\\rm D}$\n\\end{algorithmic}\n\\end{algorithm}[t!]\n    \\caption{Classical approximate backward induction algorithm}\n    \\label{algo:classical_backward_recursion}\n    [1]  \n    \\Require Compact state space $\\mathcal{X}$\\mathcal{X} with $\\frac{1}{n}$\\frac{1}{n}-net $\\mathcal{S}_n$\\mathcal{S}_n, finite action space $\\mathcal{A}$\\mathcal{A}, horizon $H$H, classical sampling access to probability kernels $p$p, failure probability $\\delta\\in(0, 1)$\\delta\\in(0, 1), error $\\varepsilon > 0$\\varepsilon > 0.\n\n    \\Ensure $(\\varepsilon + 12Ln^{-\\alpha}H^2)$(\\varepsilon + 12Ln^{-\\alpha}-\\alphaH^2)-optimal policy $\\pi^{(K)} \\in \\Pi^{\\rm D}$\\pi^{(K)}(K) \\in \\Pi^{\\rm D}\\rm D. \n\n    \\State Initialise $u^{(0)}_t \\equiv 0$u^{(0)}(0)_t \\equiv 0 for all $t\\in[H]$t\\in[H] and $\\pi^{(0)}\\in\\Pi^{\\rm D}$\\pi^{(0)}(0)\\in\\Pi^{\\rm D}\\rm D arbitrary\n\n    \\For{$k=1$ to $K:=\\lceil\\log_2(H/\\varepsilon)\\rceil$}$k=1$k=1 to $K:=\\lceil\\log_2(H/\\varepsilon)\\rceil$K:=\\lceil\\log_2(H/\\varepsilon)\\rceil\n\n        \\State $\\epsilon_k \\gets H/2^k$\\epsilon_k \\gets H/2^k\n\n        \\State $m_k \\gets \\big\\lceil\\frac{128H^3}{\\min\\{\\epsilon^2_{k},1\\}}\\ln\\frac{16H|\\mathcal{S}_n|AK}{\\delta}\\big\\rceil$m_k \\gets \\big\\lceil\\frac{128H^3}{\\min\\{\\epsilon^2_{k},1\\}}\\ln\\frac{16H|\\mathcal{S}_n|AK}{\\delta}\\big\\rceil, $\\ell_k \\gets \\big\\lceil 512 H^2\\ln\\frac{4H|\\mathcal{S}_n|AK}{\\delta}\\big\\rceil$\\ell_k \\gets \\big\\lceil 512 H^2\\ln\\frac{4H|\\mathcal{S}_n|AK}{\\delta}\\big\\rceil, $\\theta_k \\gets \\frac{1}{m_k}\\ln\\frac{16H|\\mathcal{S}_n|AK}{\\delta}$\\theta_k \\gets \\frac{1}{m_k}\\ln\\frac{16H|\\mathcal{S}_n|AK}{\\delta}\n    \n        \\State For each $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}, sample $x^{(1)}_{s,a},x^{(2)}_{s,a},\\dots,x^{(m_k)}_{s,a}\\in\\mathcal{X}$x^{(1)}(1)_{s,a}s,a,x^{(2)}(2)_{s,a}s,a,\\dots,x^{(m_k)}(m_k)_{s,a}s,a\\in\\mathcal{X}\n    \n        \\For{$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$}$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]\n\n            \\State $\\widetilde{\\sigma}^{(k)}_t(s,a) \\gets \\frac{1}{m_k}\\sum_{i=1}^{m_k}(u^{(k-1)}_{t}(x^{(i)}_{s,a}))^2 - \\big(\\frac{1}{m_k}\\sum_{i=1}^{m_k}u^{(k-1)}_{t}(x^{(i)}_{s,a}) \\big)^2$\\widetilde{\\sigma}^{(k)}(k)_t(s,a) \\gets \\frac{1}{m_k}\\sum_{i=1}i=1^{m_k}m_k(u^{(k-1)}(k-1)_{t}t(x^{(i)}(i)_{s,a}s,a))^2 - \\big(\\frac{1}{m_k}\\sum_{i=1}i=1^{m_k}m_ku^{(k-1)}(k-1)_{t}t(x^{(i)}(i)_{s,a}s,a) \\big)^2 \\label{line:classical_backward_line1}\n    \n            \\State $\\widehat{\\mu}^{(k)}_t(s,a) \\gets \\frac{1}{m_k}\\sum_{i=1}^{m_k}u^{(k-1)}_{t}(x^{(i)}_{s,a}) - \\sqrt{2\\theta_k\\widetilde{\\sigma}_t^{(k)}(s,a)} - \\left(\\frac{2}{3}\\theta_k +  2(2\\theta_k)^{3/4} + Ln^{-\\alpha}\\right)H$\\widehat{\\mu}^{(k)}(k)_t(s,a) \\gets \\frac{1}{m_k}\\sum_{i=1}i=1^{m_k}m_ku^{(k-1)}(k-1)_{t}t(x^{(i)}(i)_{s,a}s,a) - \\sqrt{2\\theta_k\\widetilde{\\sigma}_t^{(k)}(s,a)} - \\left(\\frac{2}{3}\\theta_k +  2(2\\theta_k)^{3/4}3/4 + Ln^{-\\alpha}-\\alpha\\right)H \\label{line:classical_backward_line2}\n    \n        \\EndFor\n\n        \\State $\\widehat{\\mu}_t^{(k)}(x,a) \\gets \\widehat{\\mu}_t^{(k)}(s,a)$\\widehat{\\mu}_t^{(k)}(k)(x,a) \\gets \\widehat{\\mu}_t^{(k)}(k)(s,a) for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n, $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s), $a\\in\\mathcal{A}$a\\in\\mathcal{A}, and $t\\in[H]$t\\in[H]\n\n        \\State $u^{(k)}_H(x) \\gets u^{(k-1)}_{H}(x)$u^{(k)}(k)_H(x) \\gets u^{(k-1)}(k-1)_{H}H(x) for all $x\\in\\mathcal{X}$x\\in\\mathcal{X} and $\\pi_H^{(k)}\\gets \\pi_{H}^{(k-1)}$\\pi_H^{(k)}(k)\\gets \\pi_{H}H^{(k-1)}(k-1)\n\n        \\For{$t=H-1,H-2,\\dots,1$}$t=H-1,H-2,\\dots,1$t=H-1,H-2,\\dots,1\n\n            \\For{$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$}$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}\n\n            \\State Sample $\\bar{x}^{(1)}_{s,a},\\bar{x}^{(2)}_{s,a},\\dots,\\bar{x}^{(\\ell_k)}_{s,a}\\in\\mathcal{X}$\\bar{x}^{(1)}(1)_{s,a}s,a,\\bar{x}^{(2)}(2)_{s,a}s,a,\\dots,\\bar{x}^{(\\ell_k)}(\\ell_k)_{s,a}s,a\\in\\mathcal{X}\n\n            \\State $\\widehat{\\beta}_{t+1}^{(k)}(s,a) \\gets \\frac{1}{\\ell_k}\\sum_{i=1}^{\\ell_k}\\big(u^{(k)}_{t+1}(\\bar{x}^{(i)}_{s,a}) -  u^{(k-1)}_{t+1}(\\bar{x}^{(i)}_{s,a})\\big) - \\frac{\\epsilon_{k}}{4H} - \\frac{3}{2}Ln^{-\\alpha}H$\\widehat{\\beta}_{t+1}t+1^{(k)}(k)(s,a) \\gets \\frac{1}{\\ell_k}\\sum_{i=1}i=1^{\\ell_k}\\ell_k\\big(u^{(k)}(k)_{t+1}t+1(\\bar{x}^{(i)}(i)_{s,a}s,a) -  u^{(k-1)}(k-1)_{t+1}t+1(\\bar{x}^{(i)}(i)_{s,a}s,a)\\big) - \\frac{\\epsilon_{k}}{4H} - \\frac{3}{2}Ln^{-\\alpha}-\\alphaH \\label{line:classical_backward_line3}\n\n            \\EndFor\n\n            \\For{$s\\in\\mathcal{S}_n$}$s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n\n\n            \\State $u^{(k)}_t(s) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\widehat{\\mu}_{t+1}^{(k)}(s,a) + \\widehat{\\beta}_{t+1}^{(k)}(s,a)\\} - Ln^{-\\alpha}$u^{(k)}(k)_t(s) \\gets \\max_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{r(s,a) + \\widehat{\\mu}_{t+1}t+1^{(k)}(k)(s,a) + \\widehat{\\beta}_{t+1}t+1^{(k)}(k)(s,a)\\} - Ln^{-\\alpha}-\\alpha\n            \n            \\State $\\pi_t^{(k)}(s) \\gets \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\widehat{\\mu}_{t+1}^{(k)}(s,a) + \\widehat{\\beta}_{t+1}^{(k)}(s,a)\\}$\\pi_t^{(k)}(k)(s) \\gets \\argmax_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{r(s,a) + \\widehat{\\mu}_{t+1}t+1^{(k)}(k)(s,a) + \\widehat{\\beta}_{t+1}t+1^{(k)}(k)(s,a)\\}\n\n            \\State If $u^{(k)}_t(s) \\leq u^{(k-1)}_{t}(s)$u^{(k)}(k)_t(s) \\leq u^{(k-1)}(k-1)_{t}t(s), then $u^{(k)}_t(s) \\!\\gets\\! u^{(k-1)}_{t}(s)$u^{(k)}(k)_t(s) \\!\\gets\\! u^{(k-1)}(k-1)_{t}t(s) and $\\pi_t^{(k)}(s) \\!\\gets\\! \\pi_{t}^{(k-1)}(s)$\\pi_t^{(k)}(k)(s) \\!\\gets\\! \\pi_{t}t^{(k-1)}(k-1)(s) $\\forall x\\in\\mathcal{X}(s)$\\forall x\\in\\mathcal{X}(s) \\label{line:classical_backward_line4}\n\n            \n\n            \\EndFor\n\n        \\EndFor\n\n    \\EndFor\n    \n    \\State \\Return $\\pi^{(K)}\\in\\Pi^{\\rm D}$\\pi^{(K)}(K)\\in\\Pi^{\\rm D}\\rm D\n\n\n\nWe are almost ready to prove the correctness of \\Cref{algo:classical_backward_recursion} and its complexity. Before that, we state a useful upper bound on the variance.\n\\begin{fact}[{\\cite[Lemma~F.4]{sidford2018near}}]\\label{fact:upper_bound_variance}\n    Let $\\sigma_t^\\pi(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)V_{t}^\\pi(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)V^\\pi_{t}(x')\\big)^2$ for a given policy $\\pi$. For any policy $\\pi$ and $(x,t)\\in\\mathcal{X}\\times[H]$,\\footnote{We note the typo in~\\cite[Lemma~F.4]{sidford2018near} where $\\|\\cdot\\|_\\infty^2$ should be $\\|\\cdot\\|_\\infty$.}\n    %\n    \\begin{align*}\n        \\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_t}(\\rd x_{t+1}|x) p_{\\pi_{t+1}}(\\rd x_{t+2}|x_{t+1})\\cdots p_{\\pi_{t'-1}}(\\rd x_{t'}|x_{t'-1})\\sqrt{\\sigma_{t'+1}^\\pi(x_{t'},\\pi_{t'}(x_{t'}))} \\leq H^{3/2},\n    \\end{align*}\n    %\n    where $\\sigma_{t+1}^\\pi(x_{t},\\pi_{t}(x_{t})) = \\sigma_{t+1}^\\pi(x,\\pi_t(x))$ for $t'=t$.\n\\end{fact}\\begin{fact}[{\\cite[Lemma~F.4]{sidford2018near}}]\\label{fact:upper_bound_variance}\n    Let $\\sigma_t^\\pi(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)V_{t}^\\pi(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)V^\\pi_{t}(x')\\big)^2$ for a given policy $\\pi$. For any policy $\\pi$ and $(x,t)\\in\\mathcal{X}\\times[H]$,\\footnote{We note the typo in~\\cite[Lemma~F.4]{sidford2018near} where $\\|\\cdot\\|_\\infty^2$ should be $\\|\\cdot\\|_\\infty$.}\n    %\n    \\begin{align*}\n        \\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_t}(\\rd x_{t+1}|x) p_{\\pi_{t+1}}(\\rd x_{t+2}|x_{t+1})\\cdots p_{\\pi_{t'-1}}(\\rd x_{t'}|x_{t'-1})\\sqrt{\\sigma_{t'+1}^\\pi(x_{t'},\\pi_{t'}(x_{t'}))} \\leq H^{3/2},\n    \\end{align*}\n    %\n    where $\\sigma_{t+1}^\\pi(x_{t},\\pi_{t}(x_{t})) = \\sigma_{t+1}^\\pi(x,\\pi_t(x))$ for $t'=t$.\n\\end{fact}[{\\cite[Lemma~F.4]{sidford2018near}}\\cite[Lemma~F.4]{sidford2018near}]\\label{fact:upper_bound_variance}\n    Let $\\sigma_t^\\pi(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)V_{t}^\\pi(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)V^\\pi_{t}(x')\\big)^2$\\sigma_t^\\pi(x,a) := \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)V_{t}t^\\pi(x')^2 - \\big(\\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)V^\\pi_{t}t(x')\\big)^2 for a given policy $\\pi$\\pi. For any policy $\\pi$\\pi and $(x,t)\\in\\mathcal{X}\\times[H]$(x,t)\\in\\mathcal{X}\\times[H],\\footnote{We note the typo in~\\cite[Lemma~F.4]{sidford2018near} where $\\|\\cdot\\|_\\infty^2$ should be $\\|\\cdot\\|_\\infty$.}\n    \n        \\sum_{t'=t}t'=t^{H-1}H-1 \\idotsint_{\\mathcal{X}^{t'-t}}\\mathcal{X}^{t'-t}t'-t p_{\\pi_t}\\pi_t(\\rd x_{t+1}t+1|x) p_{\\pi_{t+1}}\\pi_{t+1}t+1(\\rd x_{t+2}t+2|x_{t+1}t+1)\\cdots p_{\\pi_{t'-1}}\\pi_{t'-1}t'-1(\\rd x_{t'}t'|x_{t'-1}t'-1)\\sqrt{\\sigma_{t'+1}^\\pi(x_{t'},\\pi_{t'}(x_{t'}))} \\leq H^{3/2}3/2,\n    \n    where $\\sigma_{t+1}^\\pi(x_{t},\\pi_{t}(x_{t})) = \\sigma_{t+1}^\\pi(x,\\pi_t(x))$\\sigma_{t+1}t+1^\\pi(x_{t}t,\\pi_{t}t(x_{t}t)) = \\sigma_{t+1}t+1^\\pi(x,\\pi_t(x)) for $t'=t$t'=t.\n\n\n\\begin{theorem}[Classical finite-horizon generative algorithm]\\label{thr:classical_finite-horizon_sampling}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},H, p, r\\rangle$ be a finite-horizon MDP and $\\mathcal{S}_n$ a $\\frac{1}{n}$-net for $\\mathcal{X}$. Let $K := \\lceil\\log_2(H/\\varepsilon)\\rceil$ for a given $\\varepsilon > 0$ and $\\epsilon_k := H/2^k$ for all $k\\in[K]$. Under {\\rm \\Cref{ass:Lipschitz}} with $Ln^{-\\alpha} \\leq \\frac{1}{16H}$, {\\rm \\Cref{algo:classical_backward_recursion}} computes functions $\\{u^{(k)}_t\\in\\mathscr{B}(\\mathcal{X})\\}_{t\\in[H],k\\in[K]}$ and policies $\\{\\pi^{(k)}\\in\\Pi^{\\rm D}\\}_{k\\in[K]}$ such that, with probability at least $1-\\delta$,\n    %\n    \\begin{align*}\n        V^\\ast_t(x) - \\epsilon_k - 12Ln^{-\\alpha}H^2 \\leq u^{(k)}_t(x) \\leq V^{\\pi^{(k)}}_t(x) \\leq  V^\\ast_t(x) \\qquad\\forall (x,t,k)\\in\\mathcal{X}\\times[H]\\times [K].\n    \\end{align*}\n    %\n    In particular, $\\pi^{(K)} \\in \\Pi^{\\rm D}$ is such that $V^\\ast_1(x) - \\varepsilon - 12Ln^{-\\alpha}H^2 \\leq V^{\\pi^{(K)}}_1(x) \\leq  V^\\ast_1(x)$ for all $x\\in\\mathcal{X}$. The $\\mathcal{C}_p$-query complexity is\n    %\n    \\begin{align*}\n        O\\left(\\frac{H^3|\\mathcal{S}_n|A}{\\varepsilon^2}\\log\\left(\\frac{H|\\mathcal{S}_n|A}{\\delta}\\log\\frac{H}{\\varepsilon} \\right) \\right).\n    \\end{align*}\n\\end{theorem}\\begin{theorem}[Classical finite-horizon generative algorithm]\\label{thr:classical_finite-horizon_sampling}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},H, p, r\\rangle$ be a finite-horizon MDP and $\\mathcal{S}_n$ a $\\frac{1}{n}$-net for $\\mathcal{X}$. Let $K := \\lceil\\log_2(H/\\varepsilon)\\rceil$ for a given $\\varepsilon > 0$ and $\\epsilon_k := H/2^k$ for all $k\\in[K]$. Under {\\rm \\Cref{ass:Lipschitz}} with $Ln^{-\\alpha} \\leq \\frac{1}{16H}$, {\\rm \\Cref{algo:classical_backward_recursion}} computes functions $\\{u^{(k)}_t\\in\\mathscr{B}(\\mathcal{X})\\}_{t\\in[H],k\\in[K]}$ and policies $\\{\\pi^{(k)}\\in\\Pi^{\\rm D}\\}_{k\\in[K]}$ such that, with probability at least $1-\\delta$,\n    %\n    \\begin{align*}\n        V^\\ast_t(x) - \\epsilon_k - 12Ln^{-\\alpha}H^2 \\leq u^{(k)}_t(x) \\leq V^{\\pi^{(k)}}_t(x) \\leq  V^\\ast_t(x) \\qquad\\forall (x,t,k)\\in\\mathcal{X}\\times[H]\\times [K].\n    \\end{align*}\n    %\n    In particular, $\\pi^{(K)} \\in \\Pi^{\\rm D}$ is such that $V^\\ast_1(x) - \\varepsilon - 12Ln^{-\\alpha}H^2 \\leq V^{\\pi^{(K)}}_1(x) \\leq  V^\\ast_1(x)$ for all $x\\in\\mathcal{X}$. The $\\mathcal{C}_p$-query complexity is\n    %\n    \\begin{align*}\n        O\\left(\\frac{H^3|\\mathcal{S}_n|A}{\\varepsilon^2}\\log\\left(\\frac{H|\\mathcal{S}_n|A}{\\delta}\\log\\frac{H}{\\varepsilon} \\right) \\right).\n    \\end{align*}\n\\end{theorem}\\label{thr:classical_finite-horizon_sampling}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},H, p, r\\rangle$M = \\langle \\mathcal{X},\\mathcal{A},H, p, r\\rangle be a finite-horizon MDP and $\\mathcal{S}_n$\\mathcal{S}_n a $\\frac{1}{n}$\\frac{1}{n}-net for $\\mathcal{X}$\\mathcal{X}. Let $K := \\lceil\\log_2(H/\\varepsilon)\\rceil$K := \\lceil\\log_2(H/\\varepsilon)\\rceil for a given $\\varepsilon > 0$\\varepsilon > 0 and $\\epsilon_k := H/2^k$\\epsilon_k := H/2^k for all $k\\in[K]$k\\in[K]. Under {\\rm \\Cref{ass:Lipschitz}}\\rm \\Cref{ass:Lipschitz} with $Ln^{-\\alpha} \\leq \\frac{1}{16H}$Ln^{-\\alpha}-\\alpha \\leq \\frac{1}{16H}, {\\rm \\Cref{algo:classical_backward_recursion}}\\rm \\Cref{algo:classical_backward_recursion} computes functions $\\{u^{(k)}_t\\in\\mathscr{B}(\\mathcal{X})\\}_{t\\in[H],k\\in[K]}$\\{u^{(k)}(k)_t\\in\\mathscr{B}(\\mathcal{X})\\}_{t\\in[H],k\\in[K]}t\\in[H],k\\in[K] and policies $\\{\\pi^{(k)}\\in\\Pi^{\\rm D}\\}_{k\\in[K]}$\\{\\pi^{(k)}(k)\\in\\Pi^{\\rm D}\\rm D\\}_{k\\in[K]}k\\in[K] such that, with probability at least $1-\\delta$1-\\delta,\n    \n        V^\\ast_t(x) - \\epsilon_k - 12Ln^{-\\alpha}-\\alphaH^2 \\leq u^{(k)}(k)_t(x) \\leq V^{\\pi^{(k)}}\\pi^{(k)}(k)_t(x) \\leq  V^\\ast_t(x) \\qquad\\forall (x,t,k)\\in\\mathcal{X}\\times[H]\\times [K].\n    \n    In particular, $\\pi^{(K)} \\in \\Pi^{\\rm D}$\\pi^{(K)}(K) \\in \\Pi^{\\rm D}\\rm D is such that $V^\\ast_1(x) - \\varepsilon - 12Ln^{-\\alpha}H^2 \\leq V^{\\pi^{(K)}}_1(x) \\leq  V^\\ast_1(x)$V^\\ast_1(x) - \\varepsilon - 12Ln^{-\\alpha}-\\alphaH^2 \\leq V^{\\pi^{(K)}}\\pi^{(K)}(K)_1(x) \\leq  V^\\ast_1(x) for all $x\\in\\mathcal{X}$x\\in\\mathcal{X}. The $\\mathcal{C}_p$\\mathcal{C}_p-query complexity is\n    \n        O\\left(\\frac{H^3|\\mathcal{S}_n|A}{\\varepsilon^2}\\log\\left(\\frac{H|\\mathcal{S}_n|A}{\\delta}\\log\\frac{H}{\\varepsilon} \\right) \\right).\n    \n\n\\begin{proof}\n    The proof is by induction on $k=0,1,\\dots,K$. The base case $k=0$ is trivial since $\\epsilon_0 = H$, $u_t^{(0)} \\equiv 0$, and $V^\\ast_t(x) \\leq H+1-t$ for all $t\\in[H]$. Assume then that \\Cref{algo:classical_backward_recursion} has properly computed functions and policies such that\n    %\n    \\begin{align}\\label{eq:classical_backward_eq0}\n        V^\\ast_t(x) - \\epsilon_{k'} - 12Ln^{-\\alpha}H^2 \\leq u^{(k')}_{t}(x) \\leq (\\mathcal{L}_{\\pi^{(k')}_{t}}u^{(k')}_{t+1})(x) \\leq  V^\\ast_t(x) \\qquad \\forall k'\\in[k-1], t\\in[H],\n    \\end{align}\n    %\n    and consider now epoch $k\\in[K]$. We first analyse the approximate quantities $\\widehat{\\mu}_t^{(k)}$ and $\\widetilde{\\sigma}_t^{(k)}$ from \\Cref{line:classical_backward_line1,line:classical_backward_line2}. Define $\\widetilde{\\mu}_t^{(k)}(s,a) := \\frac{1}{m_k}\\sum_{i=1}^{m_k}u_{t}^{(k-1)}(x^{(i)}_{s,a})$ for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$. Define $\\mu^{(k)}_t(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_{t}^{(k-1)}(x')$ and $\\sigma_t^{(k)}(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_{t}^{(k-1)}(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)u_{t}^{(k-1)}(x')\\big)^2$. Then, according to \\Cref{lem:bernstein_inequality_classical_backward} and already using that $\\|u_t^{(k-1)}\\|_\\infty \\leq \\|V_t^\\ast\\|_\\infty \\leq H$ by the induction hypothesis, with probability at least $1-\\frac{\\delta}{2K}$, for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$, (let $\\theta_k := \\frac{1}{m_k}\\ln\\frac{16H|\\mathcal{S}_n|AK}{\\delta} \\leq \\frac{1}{128 H^3}\\min\\{\\epsilon_{k}^2, 1\\}$)\n    %\n    \\begin{subequations}\n    \\begin{align}\n        |\\widetilde{\\mu}^{(k)}_t(s,a) - \\mu^{(k)}_t(s,a)| &\\leq \\sqrt{2\\theta_k\\sigma_t^{(k)}(s,a)} + \\frac{2}{3}\\theta_k H, \\label{eq:inequality_1}\\\\\n        |\\widetilde{\\sigma}_t^{(k)}(s,a) - \\sigma_t^{(k)}(s,a)| &\\leq 4\\sqrt{2\\theta_k} H^2, \\label{eq:inequality_2}\n    \\end{align}\n    \\end{subequations}\n    %\n    which we condition on. By using \\Cref{eq:inequality_2} onto \\Cref{eq:inequality_1}, we get that\n    %\n    \\begin{align*}\n        |\\widetilde{\\mu}^{(k)}_t(s,a) - \\mu^{(k)}_t(s,a)| &\\leq \\sqrt{2\\theta_k\\widetilde{\\sigma}_t^{(k)}(s,a)} + \\left(\\frac{2}{3}\\theta_k +  2(2\\theta_k)^{3/4}\\right) H,\n    \\end{align*}\n    %\n    from which we define $\\widehat{\\mu}^{(k)}_t(x,a)$, for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, $a\\in\\mathcal{A}$, as\n    %\n    \\begin{align*}\n        \\widehat{\\mu}^{(k)}_t(x,a) := \\widetilde{\\mu}^{(k)}_t(s,a) - \\sqrt{2\\theta_k\\widetilde{\\sigma}_t^{(k)}(s,a)} - \\left(\\frac{2}{3}\\theta_k +  2(2\\theta_k)^{3/4} + Ln^{-\\alpha}\\right) H.\n    \\end{align*}\n    %\n    The quantity $\\widehat{\\mu}^{(k)}_t(x,a)$ has one-sided error: for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, $a\\in\\mathcal{A}$,\n    %\n    \\begin{align*}\n        \\mu_t^{(k)}(x,a) \\geq \\widehat{\\mu}^{(k)}_t(x,a) \\geq \\mu_t^{(k)}(x,a) - 2\\sqrt{2\\theta_k\\widetilde{\\sigma}_t^{(k)}(s,a)} - \\left(\\frac{4}{3}\\theta_k +  4(2\\theta_k)^{3/4} + 2Ln^{-\\alpha}\\right) H.\n    \\end{align*}\n    %\n    We can express the above inequality using the variance $\\sigma_t^\\ast(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)V_t^\\ast(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)V_t^\\ast(x')\\big)^2$ since, for $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, and $a\\in\\mathcal{A}$,\n    %\n    \\begin{align*}\n        \\sqrt{\\widetilde{\\sigma}_t^{(k)}(s,a)} &\\leq \\sqrt{\\sigma_t^{(k)}(s,a)} + 2(2\\theta_k)^{1/4} H \\tag{by \\Cref{eq:inequality_2}}\\\\\n        &\\leq \\sqrt{\\sigma_t^{(k)}(x,a)} + 2(\\sqrt{Ln^{-\\alpha}} + (2\\theta_k)^{1/4}) H \\tag{by $|\\sigma_t^{(k)}(x,a) - \\sigma_t^{(k)}(s,a)| \\leq 4Ln^{-\\alpha} H^2$}\\\\\n        &\\leq \\sqrt{\\sigma_t^{\\ast}(x,a)} + 2\\epsilon_{k} + 12Ln^{-\\alpha}H^2 + 2(\\sqrt{Ln^{-\\alpha}} + (2\\theta_k)^{1/4}) H, \n    \\end{align*}\n    %\n    where we used that $\\operatorname{Var}[u^{(k-1)}_{t}] \\leq \\operatorname{Var}[V^\\ast_t] + \\operatorname{Var}[V^\\ast_t - u^{(k-1)}_{t}]$ and that $\\operatorname{Var}[V^\\ast_t - u^{(k-1)}_{t}] \\leq (2\\epsilon_{k} + 12Ln^{-\\alpha}H^2)^2$ if $\\|V^\\ast_t - u^{(k-1)}_{t}\\|_\\infty \\leq 2\\epsilon_{k} + 12Ln^{-\\alpha}H^2$ according to the induction hypothesis. This means that, for all $(x,a,t)\\in\\mathcal{X}\\times\\mathcal{A}\\times[H]$, with probability at least $1-\\frac{\\delta}{2K}$,\n    %\n    \\begin{subequations}\\label{eq:classical_backward_eq1}\n    \\begin{align}\n        \\widehat{\\mu}^{(k)}_t \\!(x,a) \\!&\\leq\\! \\mu_t^{(k)} \\!(x,a), \\label{eq:classical_backward_eq1a} \\\\\n        \\widehat{\\mu}^{(k)}_t \\!(x,a) \\!&\\geq \\! \\mu_t^{(k)} \\!(x,a) \\!-\\! \\sqrt{8\\theta_k\\sigma_t^\\ast(x,a)} \\!-\\! \\sqrt{8\\theta_k}(2\\epsilon_{k} +12Ln^{-\\alpha}H^2) \\!-\\! \\left(\\!\\frac{16}{3}\\theta_k \\!+\\! 8(2\\theta_k)^{3/4} \\!+\\! 4Ln^{-\\alpha}\\!\\right)\\! H.\\label{eq:classical_backward_eq1b}\n    \\end{align}\n    \\end{subequations}\n    \n    Given the above inequalities, which we condition on, we now analyse the backward iteration that happens within epoch $k\\in[K]$ when calculating $u^{(k)}_t\\in\\mathscr{B}(\\mathcal{X})$ for $t=H,H-1,\\dots, 1$. First let us focus on proving that $u_t^{(k)}(x) \\leq V_t^{\\pi^{(k)}}(x) \\leq V^\\ast_t(x)$. For such, assume by induction that\n    %\n    \\begin{align*}\n        u_{t'}^{(k)}(x) \\leq (\\mathcal{L}_{\\pi^{(k)}_{t'}}u^{(k)}_{t'+1})(x) \\leq V_{t'}^{\\pi^{(k)}}(x) \\leq V^\\ast_{t'}(x) \\qquad\\forall x\\in\\mathcal{X}, t'=t+1,t+2,\\dots,H+1.\n    \\end{align*}\n    %\n    The case $t=H+1$ is trivial since $V_{H+1}^\\ast, u^{(k)}_{H+1} \\equiv 0$. For $t\\leq H$, consider, $\\forall s\\in\\mathcal{S}_n,x\\in\\mathcal{X}(s),a\\in\\mathcal{A}$, the quantities from \\Cref{line:classical_backward_line3},\n    %\n    \\begin{align*}\n        \\widehat{\\beta}_{t+1}^{(k)} (x,a) := \\frac{1}{\\ell_k}\\sum_{i=1}^{\\ell_k} \\big( u^{(k)}_{t+1}(\\bar{x}^{(i)}_{s,a}) - u^{(k-1)}_{t+1}(\\bar{x}^{(i)}_{s,a}) \\big) - \\frac{\\epsilon_{k}}{4H} - \\frac{3}{2}Ln^{-\\alpha}H.\n    \\end{align*}\n    %\n    Note that $u^{(k-1)}_{t+1}(x) \\leq u^{(k)}_{t+1}(x)$ holds, since it is enforced by design (\\Cref{line:classical_backward_line4}). Due to that and by the induction hypothesis (both in $t$ and $k$), then $\\|u^{(k)}_{t+1} - u^{(k-1)}_{t+1}\\|_\\infty \\leq 2\\epsilon_k + 12Ln^{-\\alpha}H^2$. Moreover, $\\big(\\frac{1}{16H} + Ln^{-\\alpha}\\big)(2\\epsilon_{k} + 12Ln^{-\\alpha}H^2) \\leq \\frac{\\epsilon_{k}}{4H} + \\frac{3}{2}Ln^{-\\alpha}H$ using that $Ln^{-\\alpha} \\leq \\frac{1}{16H}$. Hence, according to \\Cref{lem:hoeffding_classical_backward} with $\\ell_k = \\big\\lceil2^9 H^2\\ln\\frac{4H|\\mathcal{S}_n|AK}{\\delta} \\big\\rceil$, with probability at least $1-\\frac{\\delta}{2KH}$, for all $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$,%\n    %\n    \\begin{subequations}\\label{eq:classical_backward_eq2}\n    \\begin{align}\n        \\widehat{\\beta}^{(k)}_{t+1}(x,a) &\\leq \\int_{\\mathcal{X}}  p(\\rd x'|x,a)\\big(u^{(k)}_{t}(x') - u^{(k-1)}_{t+1}(x') \\big), \\label{eq:classical_backward_eq2a}\\\\\n        \\widehat{\\beta}^{(k)}_{t+1}(x,a) &\\geq \\int_{\\mathcal{X}}  p(\\rd x'|x,a)\\big(u^{(k)}_{t+1}(x') - u^{(k-1)}_{t+1}(x') \\big) - \\frac{\\epsilon_{k}}{2H}- 3Ln^{-\\alpha}H. \\label{eq:classical_backward_eq2b}\n    \\end{align}\n    \\end{subequations}\n    %\n    Conditioned on the above, there are two cases to consider. \\textbf{Case 1: $\\pi^{(k)}_t \\neq \\pi^{(k-1)}_{t}$.} If $\\pi^{(k)}_t(s) \\neq \\pi^{(k-1)}_{t}(s)$ for some $s\\in\\mathcal{S}_n$, then it means that, for all $x\\in\\mathcal{X}(s)$,\n    %\n    \\begin{align*}\n        u_t^{(k)}(x) := u_t^{(k)}(s) &= r(s,\\pi^{(k)}_t(s)) - Ln^{-\\alpha} + \\widehat{\\mu}_{t+1}^{(k)}(s,\\pi^{(k)}_t(s)) + \\widehat{\\beta}_{t+1}^{(k)}(s,\\pi^{(k)}_t(s)) \\\\\n        &\\leq r(x,\\pi^{(k)}_t(x)) + \\widehat{\\mu}_{t+1}^{(k)}(x,\\pi^{(k)}_t(x)) + \\widehat{\\beta}_{t+1}^{(k)}(x,\\pi^{(k)}_t(x))\\\\\n        &\\leq r(x,\\pi^{(k)}_t(x)) + \\int_{\\mathcal{X}}p(\\text{d}x'|x,\\pi^{(k)}_t(x)) u^{(k)}_{t+1}(x') \\tag{by \\Cref{eq:classical_backward_eq1a,eq:classical_backward_eq2a}}\\\\\n        &= (\\mathcal{L}_{\\pi^{(k)}_t}u^{(k)}_{t+1})(x).\n    \\end{align*}\n    %\n    \\textbf{Case 2: $\\pi^{(k)}_t = \\pi^{(k-1)}_{t}$.} If both policies are equal, then, for all $x\\in\\mathcal{X}$,\n    %\n    \\begin{align*}\n        u^{(k)}_t(x) = u^{(k-1)}_{t}(x) \\leq (\\mathcal{L}_{\\pi^{(k-1)}_{t}}u^{(k-1)}_{t+1})(x) \\leq (\\mathcal{L}_{\\pi^{(k-1)}_{t}}u^{(k)}_{t+1})(x) = (\\mathcal{L}_{\\pi^{(k)}_{t}}u^{(k)}_{t+1})(x),\n    \\end{align*}\n    %\n    where we used the induction hypothesis to argue that $u^{(k-1)}_{t}(x) \\leq (\\mathcal{L}_{\\pi^{(k-1)}_{t}}u^{(k-1)}_{t+1})(x)$. From the above, we can readily see that $u^{(k)}_t(x) \\leq V_t^\\ast(x)$, which completes the induction on $t$.\n\n    We now move on to proving that $u^{(k)}_t(x) \\geq V^\\ast_t(x) - \\epsilon_k - 12Ln^{-\\alpha}H^2$ (it is implicitly assumed that all $u^{(k)}_{t'+1}$ for $t' = t,\\dots, H-1$ have already been computed by the backward iteration). For such, first note that\n    %\n    \\begin{align*}\n        V^\\ast_t(x) - u^{(k)}_t(x) &= (\\mathcal{L}V^\\ast_{t+1})(x) - u^{(k)}_t(x)\\\\\n        &\\leq \\max_{a\\in\\mathcal{A}}\\left\\{r(x,a) + \\int_{\\mathcal{X}}p(\\rd x'|x,a)V_{t+1}^\\ast(x') \\right\\} \\\\\n        &~- \\max_{a\\in\\mathcal{A}}\\left\\{r(x,a) - 2Ln^{-\\alpha} + \\widehat{\\mu}_{t+1}^{(k)}(x,a) + \\widehat{\\beta}_{t+1}^{(k)}(x,a) \\right\\} \\tag{$u_t^{(k)}(x) \\!\\geq\\! u_t^{(k)}(s) \\!-\\! Ln^{-\\alpha}$}\\\\\n        &\\leq \\int_{\\mathcal{X}} p(\\rd x'|x,\\pi^{\\ast}_{t}(x))\\big(V_{t+1}^\\ast(x') - u^{(k)}_{t+1}(x')\\big) + \\xi_{t+1}^{(k)}(x), \\tag{by \\Cref{eq:classical_backward_eq1b,eq:classical_backward_eq2b}}\n    \\end{align*}\n    %\n    where we defined\n    %\n    \\begin{align*}\n        \\xi_{t+1}^{(k)}(x) \\!:=\\! \\sqrt{8\\theta_k\\sigma^\\ast_{t+1}(x,\\pi^\\ast_{t}(x))} + \\frac{\\epsilon_{k}}{2H} + \\sqrt{8\\theta_k}(2\\epsilon_{k} +  12Ln^{-\\alpha}H^2)\n        + \\left(\\!\\frac{16}{3}\\theta_k + 8(2\\theta_k)^{3/4} + 9 Ln^{-\\alpha}\\!\\right)\\! H.\n    \\end{align*}\n    %\n    Solving the recursion, we obtain that\n    %\n    \\begin{align*}\n        V^\\ast_t(x) - u^{(k)}_t(x) \\leq \\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x) p_{\\pi_{t+1}^\\ast}(\\rd x_{t+2}|x_{t+1})\\cdots p_{\\pi_{t'-1}^\\ast}(\\rd x_{t'}|x_{t'-1}) \\xi_{t'+1}^{(k)}(x_{t'}).\n    \\end{align*}\n    %\n    By employing that $\\|\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x)\\cdots p_{\\pi_{t'-1}^\\ast}(\\rd x_{t'}|x_{t'-1})\\|_\\infty \\leq H - t + 1$ and that\n    %\n    \\begin{align*}\n        \\left\\|\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x)\\cdots p_{\\pi_{t'-1}^\\ast}(\\rd x_{t'}|x_{t'-1})\\sqrt{\\sigma_{t'+1}^\\ast(x_{t'},\\pi^\\ast_{t'}(x_{t'}))}\\right\\|_\\infty \\leq H^{3/2}, \\tag{\\Cref{fact:upper_bound_variance}}\n    \\end{align*}\n    %\n    we finally obtain that\n    %\n    \\begin{align*}\n        V^\\ast_t(x) - u^{(k)}_t(x) &\\leq \\frac{\\epsilon_{k}}{2} + 2\\sqrt{2\\theta_k}H^{3/2} + 4\\sqrt{2\\theta_k}\\epsilon_{k}H +  \\left(\\frac{16}{3}\\theta_k + 8(2\\theta_k)^{3/4}\\right)H^2\\\\\n        &~~~~~+ 24\\sqrt{2\\theta_k}Ln^{-\\alpha}H^3 + 9Ln^{-\\alpha}H^2 \\\\\n        &\\leq \\epsilon_{k}\\left(\\frac{1}{2} + \\frac{2\\sqrt{2}}{\\sqrt{128}} + \\frac{4\\sqrt{2}}{\\sqrt{128}} + \\frac{16}{3\\cdot 128} + \\frac{8\\cdot 2^{3/4}}{128^{3/4}}\\right)\\\\\n        &~~~~~+ \\frac{24\\sqrt{2}}{\\sqrt{128}}Ln^{-\\alpha}H^{3/2} + 9Ln^{-\\alpha} H^2 \\tag{$\\theta_k\\leq \\frac{1}{128 H^3}\\min\\{\\epsilon_{k}^2, 1\\}$}\\\\\n        &< \\epsilon_k + 12Ln^{-\\alpha} H^2,\n    \\end{align*}\n    %\n    This concludes the proof of \\Cref{eq:classical_backward_eq0} for epoch $k\\in[K]$ and thus for all epochs by induction on $k$.\n\n    Regarding the failure probability, \\Cref{eq:classical_backward_eq1} holds with probability $1-\\frac{\\delta}{2K}$ for a single epoch, while \\Cref{eq:classical_backward_eq2} holds with probability $1-\\frac{\\delta}{2HK}$ for a single epoch and time step. Therefore, across all epochs and time steps, the success probability is at least $1-\\delta$, as required. Finally, the total number of samples used is\n    %\n    \\[\n        O\\bigg(\\sum_{k=1}^K (m_k + H\\ell_k)|\\mathcal{S}_n|A\\bigg) = O\\left(\\frac{H^3|\\mathcal{S}_n|A}{\\varepsilon^2}\\log\\left(\\frac{H|\\mathcal{S}_n|A}{\\delta}\\log\\frac{H}{\\varepsilon}\\right) \\right). \\qedhere\n    \\]\n\\end{proof}\\begin{proof}\n    The proof is by induction on $k=0,1,\\dots,K$. The base case $k=0$ is trivial since $\\epsilon_0 = H$, $u_t^{(0)} \\equiv 0$, and $V^\\ast_t(x) \\leq H+1-t$ for all $t\\in[H]$. Assume then that \\Cref{algo:classical_backward_recursion} has properly computed functions and policies such that\n    %\n    \\begin{align}\\label{eq:classical_backward_eq0}\n        V^\\ast_t(x) - \\epsilon_{k'} - 12Ln^{-\\alpha}H^2 \\leq u^{(k')}_{t}(x) \\leq (\\mathcal{L}_{\\pi^{(k')}_{t}}u^{(k')}_{t+1})(x) \\leq  V^\\ast_t(x) \\qquad \\forall k'\\in[k-1], t\\in[H],\n    \\end{align}\n    %\n    and consider now epoch $k\\in[K]$. We first analyse the approximate quantities $\\widehat{\\mu}_t^{(k)}$ and $\\widetilde{\\sigma}_t^{(k)}$ from \\Cref{line:classical_backward_line1,line:classical_backward_line2}. Define $\\widetilde{\\mu}_t^{(k)}(s,a) := \\frac{1}{m_k}\\sum_{i=1}^{m_k}u_{t}^{(k-1)}(x^{(i)}_{s,a})$ for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$. Define $\\mu^{(k)}_t(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_{t}^{(k-1)}(x')$ and $\\sigma_t^{(k)}(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_{t}^{(k-1)}(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)u_{t}^{(k-1)}(x')\\big)^2$. Then, according to \\Cref{lem:bernstein_inequality_classical_backward} and already using that $\\|u_t^{(k-1)}\\|_\\infty \\leq \\|V_t^\\ast\\|_\\infty \\leq H$ by the induction hypothesis, with probability at least $1-\\frac{\\delta}{2K}$, for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$, (let $\\theta_k := \\frac{1}{m_k}\\ln\\frac{16H|\\mathcal{S}_n|AK}{\\delta} \\leq \\frac{1}{128 H^3}\\min\\{\\epsilon_{k}^2, 1\\}$)\n    %\n    \\begin{subequations}\n    \\begin{align}\n        |\\widetilde{\\mu}^{(k)}_t(s,a) - \\mu^{(k)}_t(s,a)| &\\leq \\sqrt{2\\theta_k\\sigma_t^{(k)}(s,a)} + \\frac{2}{3}\\theta_k H, \\label{eq:inequality_1}\\\\\n        |\\widetilde{\\sigma}_t^{(k)}(s,a) - \\sigma_t^{(k)}(s,a)| &\\leq 4\\sqrt{2\\theta_k} H^2, \\label{eq:inequality_2}\n    \\end{align}\n    \\end{subequations}\n    %\n    which we condition on. By using \\Cref{eq:inequality_2} onto \\Cref{eq:inequality_1}, we get that\n    %\n    \\begin{align*}\n        |\\widetilde{\\mu}^{(k)}_t(s,a) - \\mu^{(k)}_t(s,a)| &\\leq \\sqrt{2\\theta_k\\widetilde{\\sigma}_t^{(k)}(s,a)} + \\left(\\frac{2}{3}\\theta_k +  2(2\\theta_k)^{3/4}\\right) H,\n    \\end{align*}\n    %\n    from which we define $\\widehat{\\mu}^{(k)}_t(x,a)$, for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, $a\\in\\mathcal{A}$, as\n    %\n    \\begin{align*}\n        \\widehat{\\mu}^{(k)}_t(x,a) := \\widetilde{\\mu}^{(k)}_t(s,a) - \\sqrt{2\\theta_k\\widetilde{\\sigma}_t^{(k)}(s,a)} - \\left(\\frac{2}{3}\\theta_k +  2(2\\theta_k)^{3/4} + Ln^{-\\alpha}\\right) H.\n    \\end{align*}\n    %\n    The quantity $\\widehat{\\mu}^{(k)}_t(x,a)$ has one-sided error: for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, $a\\in\\mathcal{A}$,\n    %\n    \\begin{align*}\n        \\mu_t^{(k)}(x,a) \\geq \\widehat{\\mu}^{(k)}_t(x,a) \\geq \\mu_t^{(k)}(x,a) - 2\\sqrt{2\\theta_k\\widetilde{\\sigma}_t^{(k)}(s,a)} - \\left(\\frac{4}{3}\\theta_k +  4(2\\theta_k)^{3/4} + 2Ln^{-\\alpha}\\right) H.\n    \\end{align*}\n    %\n    We can express the above inequality using the variance $\\sigma_t^\\ast(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)V_t^\\ast(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)V_t^\\ast(x')\\big)^2$ since, for $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, and $a\\in\\mathcal{A}$,\n    %\n    \\begin{align*}\n        \\sqrt{\\widetilde{\\sigma}_t^{(k)}(s,a)} &\\leq \\sqrt{\\sigma_t^{(k)}(s,a)} + 2(2\\theta_k)^{1/4} H \\tag{by \\Cref{eq:inequality_2}}\\\\\n        &\\leq \\sqrt{\\sigma_t^{(k)}(x,a)} + 2(\\sqrt{Ln^{-\\alpha}} + (2\\theta_k)^{1/4}) H \\tag{by $|\\sigma_t^{(k)}(x,a) - \\sigma_t^{(k)}(s,a)| \\leq 4Ln^{-\\alpha} H^2$}\\\\\n        &\\leq \\sqrt{\\sigma_t^{\\ast}(x,a)} + 2\\epsilon_{k} + 12Ln^{-\\alpha}H^2 + 2(\\sqrt{Ln^{-\\alpha}} + (2\\theta_k)^{1/4}) H, \n    \\end{align*}\n    %\n    where we used that $\\operatorname{Var}[u^{(k-1)}_{t}] \\leq \\operatorname{Var}[V^\\ast_t] + \\operatorname{Var}[V^\\ast_t - u^{(k-1)}_{t}]$ and that $\\operatorname{Var}[V^\\ast_t - u^{(k-1)}_{t}] \\leq (2\\epsilon_{k} + 12Ln^{-\\alpha}H^2)^2$ if $\\|V^\\ast_t - u^{(k-1)}_{t}\\|_\\infty \\leq 2\\epsilon_{k} + 12Ln^{-\\alpha}H^2$ according to the induction hypothesis. This means that, for all $(x,a,t)\\in\\mathcal{X}\\times\\mathcal{A}\\times[H]$, with probability at least $1-\\frac{\\delta}{2K}$,\n    %\n    \\begin{subequations}\\label{eq:classical_backward_eq1}\n    \\begin{align}\n        \\widehat{\\mu}^{(k)}_t \\!(x,a) \\!&\\leq\\! \\mu_t^{(k)} \\!(x,a), \\label{eq:classical_backward_eq1a} \\\\\n        \\widehat{\\mu}^{(k)}_t \\!(x,a) \\!&\\geq \\! \\mu_t^{(k)} \\!(x,a) \\!-\\! \\sqrt{8\\theta_k\\sigma_t^\\ast(x,a)} \\!-\\! \\sqrt{8\\theta_k}(2\\epsilon_{k} +12Ln^{-\\alpha}H^2) \\!-\\! \\left(\\!\\frac{16}{3}\\theta_k \\!+\\! 8(2\\theta_k)^{3/4} \\!+\\! 4Ln^{-\\alpha}\\!\\right)\\! H.\\label{eq:classical_backward_eq1b}\n    \\end{align}\n    \\end{subequations}\n    \n    Given the above inequalities, which we condition on, we now analyse the backward iteration that happens within epoch $k\\in[K]$ when calculating $u^{(k)}_t\\in\\mathscr{B}(\\mathcal{X})$ for $t=H,H-1,\\dots, 1$. First let us focus on proving that $u_t^{(k)}(x) \\leq V_t^{\\pi^{(k)}}(x) \\leq V^\\ast_t(x)$. For such, assume by induction that\n    %\n    \\begin{align*}\n        u_{t'}^{(k)}(x) \\leq (\\mathcal{L}_{\\pi^{(k)}_{t'}}u^{(k)}_{t'+1})(x) \\leq V_{t'}^{\\pi^{(k)}}(x) \\leq V^\\ast_{t'}(x) \\qquad\\forall x\\in\\mathcal{X}, t'=t+1,t+2,\\dots,H+1.\n    \\end{align*}\n    %\n    The case $t=H+1$ is trivial since $V_{H+1}^\\ast, u^{(k)}_{H+1} \\equiv 0$. For $t\\leq H$, consider, $\\forall s\\in\\mathcal{S}_n,x\\in\\mathcal{X}(s),a\\in\\mathcal{A}$, the quantities from \\Cref{line:classical_backward_line3},\n    %\n    \\begin{align*}\n        \\widehat{\\beta}_{t+1}^{(k)} (x,a) := \\frac{1}{\\ell_k}\\sum_{i=1}^{\\ell_k} \\big( u^{(k)}_{t+1}(\\bar{x}^{(i)}_{s,a}) - u^{(k-1)}_{t+1}(\\bar{x}^{(i)}_{s,a}) \\big) - \\frac{\\epsilon_{k}}{4H} - \\frac{3}{2}Ln^{-\\alpha}H.\n    \\end{align*}\n    %\n    Note that $u^{(k-1)}_{t+1}(x) \\leq u^{(k)}_{t+1}(x)$ holds, since it is enforced by design (\\Cref{line:classical_backward_line4}). Due to that and by the induction hypothesis (both in $t$ and $k$), then $\\|u^{(k)}_{t+1} - u^{(k-1)}_{t+1}\\|_\\infty \\leq 2\\epsilon_k + 12Ln^{-\\alpha}H^2$. Moreover, $\\big(\\frac{1}{16H} + Ln^{-\\alpha}\\big)(2\\epsilon_{k} + 12Ln^{-\\alpha}H^2) \\leq \\frac{\\epsilon_{k}}{4H} + \\frac{3}{2}Ln^{-\\alpha}H$ using that $Ln^{-\\alpha} \\leq \\frac{1}{16H}$. Hence, according to \\Cref{lem:hoeffding_classical_backward} with $\\ell_k = \\big\\lceil2^9 H^2\\ln\\frac{4H|\\mathcal{S}_n|AK}{\\delta} \\big\\rceil$, with probability at least $1-\\frac{\\delta}{2KH}$, for all $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$,%\n    %\n    \\begin{subequations}\\label{eq:classical_backward_eq2}\n    \\begin{align}\n        \\widehat{\\beta}^{(k)}_{t+1}(x,a) &\\leq \\int_{\\mathcal{X}}  p(\\rd x'|x,a)\\big(u^{(k)}_{t}(x') - u^{(k-1)}_{t+1}(x') \\big), \\label{eq:classical_backward_eq2a}\\\\\n        \\widehat{\\beta}^{(k)}_{t+1}(x,a) &\\geq \\int_{\\mathcal{X}}  p(\\rd x'|x,a)\\big(u^{(k)}_{t+1}(x') - u^{(k-1)}_{t+1}(x') \\big) - \\frac{\\epsilon_{k}}{2H}- 3Ln^{-\\alpha}H. \\label{eq:classical_backward_eq2b}\n    \\end{align}\n    \\end{subequations}\n    %\n    Conditioned on the above, there are two cases to consider. \\textbf{Case 1: $\\pi^{(k)}_t \\neq \\pi^{(k-1)}_{t}$.} If $\\pi^{(k)}_t(s) \\neq \\pi^{(k-1)}_{t}(s)$ for some $s\\in\\mathcal{S}_n$, then it means that, for all $x\\in\\mathcal{X}(s)$,\n    %\n    \\begin{align*}\n        u_t^{(k)}(x) := u_t^{(k)}(s) &= r(s,\\pi^{(k)}_t(s)) - Ln^{-\\alpha} + \\widehat{\\mu}_{t+1}^{(k)}(s,\\pi^{(k)}_t(s)) + \\widehat{\\beta}_{t+1}^{(k)}(s,\\pi^{(k)}_t(s)) \\\\\n        &\\leq r(x,\\pi^{(k)}_t(x)) + \\widehat{\\mu}_{t+1}^{(k)}(x,\\pi^{(k)}_t(x)) + \\widehat{\\beta}_{t+1}^{(k)}(x,\\pi^{(k)}_t(x))\\\\\n        &\\leq r(x,\\pi^{(k)}_t(x)) + \\int_{\\mathcal{X}}p(\\text{d}x'|x,\\pi^{(k)}_t(x)) u^{(k)}_{t+1}(x') \\tag{by \\Cref{eq:classical_backward_eq1a,eq:classical_backward_eq2a}}\\\\\n        &= (\\mathcal{L}_{\\pi^{(k)}_t}u^{(k)}_{t+1})(x).\n    \\end{align*}\n    %\n    \\textbf{Case 2: $\\pi^{(k)}_t = \\pi^{(k-1)}_{t}$.} If both policies are equal, then, for all $x\\in\\mathcal{X}$,\n    %\n    \\begin{align*}\n        u^{(k)}_t(x) = u^{(k-1)}_{t}(x) \\leq (\\mathcal{L}_{\\pi^{(k-1)}_{t}}u^{(k-1)}_{t+1})(x) \\leq (\\mathcal{L}_{\\pi^{(k-1)}_{t}}u^{(k)}_{t+1})(x) = (\\mathcal{L}_{\\pi^{(k)}_{t}}u^{(k)}_{t+1})(x),\n    \\end{align*}\n    %\n    where we used the induction hypothesis to argue that $u^{(k-1)}_{t}(x) \\leq (\\mathcal{L}_{\\pi^{(k-1)}_{t}}u^{(k-1)}_{t+1})(x)$. From the above, we can readily see that $u^{(k)}_t(x) \\leq V_t^\\ast(x)$, which completes the induction on $t$.\n\n    We now move on to proving that $u^{(k)}_t(x) \\geq V^\\ast_t(x) - \\epsilon_k - 12Ln^{-\\alpha}H^2$ (it is implicitly assumed that all $u^{(k)}_{t'+1}$ for $t' = t,\\dots, H-1$ have already been computed by the backward iteration). For such, first note that\n    %\n    \\begin{align*}\n        V^\\ast_t(x) - u^{(k)}_t(x) &= (\\mathcal{L}V^\\ast_{t+1})(x) - u^{(k)}_t(x)\\\\\n        &\\leq \\max_{a\\in\\mathcal{A}}\\left\\{r(x,a) + \\int_{\\mathcal{X}}p(\\rd x'|x,a)V_{t+1}^\\ast(x') \\right\\} \\\\\n        &~- \\max_{a\\in\\mathcal{A}}\\left\\{r(x,a) - 2Ln^{-\\alpha} + \\widehat{\\mu}_{t+1}^{(k)}(x,a) + \\widehat{\\beta}_{t+1}^{(k)}(x,a) \\right\\} \\tag{$u_t^{(k)}(x) \\!\\geq\\! u_t^{(k)}(s) \\!-\\! Ln^{-\\alpha}$}\\\\\n        &\\leq \\int_{\\mathcal{X}} p(\\rd x'|x,\\pi^{\\ast}_{t}(x))\\big(V_{t+1}^\\ast(x') - u^{(k)}_{t+1}(x')\\big) + \\xi_{t+1}^{(k)}(x), \\tag{by \\Cref{eq:classical_backward_eq1b,eq:classical_backward_eq2b}}\n    \\end{align*}\n    %\n    where we defined\n    %\n    \\begin{align*}\n        \\xi_{t+1}^{(k)}(x) \\!:=\\! \\sqrt{8\\theta_k\\sigma^\\ast_{t+1}(x,\\pi^\\ast_{t}(x))} + \\frac{\\epsilon_{k}}{2H} + \\sqrt{8\\theta_k}(2\\epsilon_{k} +  12Ln^{-\\alpha}H^2)\n        + \\left(\\!\\frac{16}{3}\\theta_k + 8(2\\theta_k)^{3/4} + 9 Ln^{-\\alpha}\\!\\right)\\! H.\n    \\end{align*}\n    %\n    Solving the recursion, we obtain that\n    %\n    \\begin{align*}\n        V^\\ast_t(x) - u^{(k)}_t(x) \\leq \\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x) p_{\\pi_{t+1}^\\ast}(\\rd x_{t+2}|x_{t+1})\\cdots p_{\\pi_{t'-1}^\\ast}(\\rd x_{t'}|x_{t'-1}) \\xi_{t'+1}^{(k)}(x_{t'}).\n    \\end{align*}\n    %\n    By employing that $\\|\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x)\\cdots p_{\\pi_{t'-1}^\\ast}(\\rd x_{t'}|x_{t'-1})\\|_\\infty \\leq H - t + 1$ and that\n    %\n    \\begin{align*}\n        \\left\\|\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x)\\cdots p_{\\pi_{t'-1}^\\ast}(\\rd x_{t'}|x_{t'-1})\\sqrt{\\sigma_{t'+1}^\\ast(x_{t'},\\pi^\\ast_{t'}(x_{t'}))}\\right\\|_\\infty \\leq H^{3/2}, \\tag{\\Cref{fact:upper_bound_variance}}\n    \\end{align*}\n    %\n    we finally obtain that\n    %\n    \\begin{align*}\n        V^\\ast_t(x) - u^{(k)}_t(x) &\\leq \\frac{\\epsilon_{k}}{2} + 2\\sqrt{2\\theta_k}H^{3/2} + 4\\sqrt{2\\theta_k}\\epsilon_{k}H +  \\left(\\frac{16}{3}\\theta_k + 8(2\\theta_k)^{3/4}\\right)H^2\\\\\n        &~~~~~+ 24\\sqrt{2\\theta_k}Ln^{-\\alpha}H^3 + 9Ln^{-\\alpha}H^2 \\\\\n        &\\leq \\epsilon_{k}\\left(\\frac{1}{2} + \\frac{2\\sqrt{2}}{\\sqrt{128}} + \\frac{4\\sqrt{2}}{\\sqrt{128}} + \\frac{16}{3\\cdot 128} + \\frac{8\\cdot 2^{3/4}}{128^{3/4}}\\right)\\\\\n        &~~~~~+ \\frac{24\\sqrt{2}}{\\sqrt{128}}Ln^{-\\alpha}H^{3/2} + 9Ln^{-\\alpha} H^2 \\tag{$\\theta_k\\leq \\frac{1}{128 H^3}\\min\\{\\epsilon_{k}^2, 1\\}$}\\\\\n        &< \\epsilon_k + 12Ln^{-\\alpha} H^2,\n    \\end{align*}\n    %\n    This concludes the proof of \\Cref{eq:classical_backward_eq0} for epoch $k\\in[K]$ and thus for all epochs by induction on $k$.\n\n    Regarding the failure probability, \\Cref{eq:classical_backward_eq1} holds with probability $1-\\frac{\\delta}{2K}$ for a single epoch, while \\Cref{eq:classical_backward_eq2} holds with probability $1-\\frac{\\delta}{2HK}$ for a single epoch and time step. Therefore, across all epochs and time steps, the success probability is at least $1-\\delta$, as required. Finally, the total number of samples used is\n    %\n    \\[\n        O\\bigg(\\sum_{k=1}^K (m_k + H\\ell_k)|\\mathcal{S}_n|A\\bigg) = O\\left(\\frac{H^3|\\mathcal{S}_n|A}{\\varepsilon^2}\\log\\left(\\frac{H|\\mathcal{S}_n|A}{\\delta}\\log\\frac{H}{\\varepsilon}\\right) \\right). \\qedhere\n    \\]\n\\end{proof}\n    The proof is by induction on $k=0,1,\\dots,K$k=0,1,\\dots,K. The base case $k=0$k=0 is trivial since $\\epsilon_0 = H$\\epsilon_0 = H, $u_t^{(0)} \\equiv 0$u_t^{(0)}(0) \\equiv 0, and $V^\\ast_t(x) \\leq H+1-t$V^\\ast_t(x) \\leq H+1-t for all $t\\in[H]$t\\in[H]. Assume then that \\Cref{algo:classical_backward_recursion} has properly computed functions and policies such that\n    \\label{eq:classical_backward_eq0}\n        V^\\ast_t(x) - \\epsilon_{k'}k' - 12Ln^{-\\alpha}-\\alphaH^2 \\leq u^{(k')}(k')_{t}t(x) \\leq (\\mathcal{L}_{\\pi^{(k')}_{t}}\\pi^{(k')}(k')_{t}tu^{(k')}(k')_{t+1}t+1)(x) \\leq  V^\\ast_t(x) \\qquad \\forall k'\\in[k-1], t\\in[H],\n    \n    and consider now epoch $k\\in[K]$k\\in[K]. We first analyse the approximate quantities $\\widehat{\\mu}_t^{(k)}$\\widehat{\\mu}_t^{(k)}(k) and $\\widetilde{\\sigma}_t^{(k)}$\\widetilde{\\sigma}_t^{(k)}(k) from \\Cref{line:classical_backward_line1,line:classical_backward_line2}. Define $\\widetilde{\\mu}_t^{(k)}(s,a) := \\frac{1}{m_k}\\sum_{i=1}^{m_k}u_{t}^{(k-1)}(x^{(i)}_{s,a})$\\widetilde{\\mu}_t^{(k)}(k)(s,a) := \\frac{1}{m_k}\\sum_{i=1}i=1^{m_k}m_ku_{t}t^{(k-1)}(k-1)(x^{(i)}(i)_{s,a}s,a) for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]. Define $\\mu^{(k)}_t(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_{t}^{(k-1)}(x')$\\mu^{(k)}(k)_t(x,a) := \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)u_{t}t^{(k-1)}(k-1)(x') and $\\sigma_t^{(k)}(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_{t}^{(k-1)}(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)u_{t}^{(k-1)}(x')\\big)^2$\\sigma_t^{(k)}(k)(x,a) := \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)u_{t}t^{(k-1)}(k-1)(x')^2 - \\big(\\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)u_{t}t^{(k-1)}(k-1)(x')\\big)^2. Then, according to \\Cref{lem:bernstein_inequality_classical_backward} and already using that $\\|u_t^{(k-1)}\\|_\\infty \\leq \\|V_t^\\ast\\|_\\infty \\leq H$\\|u_t^{(k-1)}(k-1)\\|_\\infty \\leq \\|V_t^\\ast\\|_\\infty \\leq H by the induction hypothesis, with probability at least $1-\\frac{\\delta}{2K}$1-\\frac{\\delta}{2K}, for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H], (let $\\theta_k := \\frac{1}{m_k}\\ln\\frac{16H|\\mathcal{S}_n|AK}{\\delta} \\leq \\frac{1}{128 H^3}\\min\\{\\epsilon_{k}^2, 1\\}$\\theta_k := \\frac{1}{m_k}\\ln\\frac{16H|\\mathcal{S}_n|AK}{\\delta} \\leq \\frac{1}{128 H^3}\\min\\{\\epsilon_{k}k^2, 1\\})\n    \n    \n        |\\widetilde{\\mu}^{(k)}(k)_t(s,a) - \\mu^{(k)}(k)_t(s,a)| &\\leq \\sqrt{2\\theta_k\\sigma_t^{(k)}(s,a)} + \\frac{2}{3}\\theta_k H, \\label{eq:inequality_1}\\\\\n        |\\widetilde{\\sigma}_t^{(k)}(k)(s,a) - \\sigma_t^{(k)}(k)(s,a)| &\\leq 4\\sqrt{2\\theta_k} H^2, \\label{eq:inequality_2}\n    \n    \n    which we condition on. By using \\Cref{eq:inequality_2} onto \\Cref{eq:inequality_1}, we get that\n    \n        |\\widetilde{\\mu}^{(k)}(k)_t(s,a) - \\mu^{(k)}(k)_t(s,a)| &\\leq \\sqrt{2\\theta_k\\widetilde{\\sigma}_t^{(k)}(s,a)} + \\left(\\frac{2}{3}\\theta_k +  2(2\\theta_k)^{3/4}3/4\\right) H,\n    \n    from which we define $\\widehat{\\mu}^{(k)}_t(x,a)$\\widehat{\\mu}^{(k)}(k)_t(x,a), for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n, $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s), $a\\in\\mathcal{A}$a\\in\\mathcal{A}, as\n    \n        \\widehat{\\mu}^{(k)}(k)_t(x,a) := \\widetilde{\\mu}^{(k)}(k)_t(s,a) - \\sqrt{2\\theta_k\\widetilde{\\sigma}_t^{(k)}(s,a)} - \\left(\\frac{2}{3}\\theta_k +  2(2\\theta_k)^{3/4}3/4 + Ln^{-\\alpha}-\\alpha\\right) H.\n    \n    The quantity $\\widehat{\\mu}^{(k)}_t(x,a)$\\widehat{\\mu}^{(k)}(k)_t(x,a) has one-sided error: for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n, $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s), $a\\in\\mathcal{A}$a\\in\\mathcal{A},\n    \n        \\mu_t^{(k)}(k)(x,a) \\geq \\widehat{\\mu}^{(k)}(k)_t(x,a) \\geq \\mu_t^{(k)}(k)(x,a) - 2\\sqrt{2\\theta_k\\widetilde{\\sigma}_t^{(k)}(s,a)} - \\left(\\frac{4}{3}\\theta_k +  4(2\\theta_k)^{3/4}3/4 + 2Ln^{-\\alpha}-\\alpha\\right) H.\n    \n    We can express the above inequality using the variance $\\sigma_t^\\ast(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)V_t^\\ast(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)V_t^\\ast(x')\\big)^2$\\sigma_t^\\ast(x,a) := \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)V_t^\\ast(x')^2 - \\big(\\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)V_t^\\ast(x')\\big)^2 since, for $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n, $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s), and $a\\in\\mathcal{A}$a\\in\\mathcal{A},\n    \n        \\sqrt{\\widetilde{\\sigma}_t^{(k)}(s,a)} &\\leq \\sqrt{\\sigma_t^{(k)}(s,a)} + 2(2\\theta_k)^{1/4}1/4 H \\tag{by \\Cref{eq:inequality_2}}by \\Cref{eq:inequality_2}\\\\\n        &\\leq \\sqrt{\\sigma_t^{(k)}(x,a)} + 2(\\sqrt{Ln^{-\\alpha}} + (2\\theta_k)^{1/4}1/4) H \\tag{by $|\\sigma_t^{(k)}(x,a) - \\sigma_t^{(k)}(s,a)| \\leq 4Ln^{-\\alpha} H^2$}by $|\\sigma_t^{(k)}(x,a) - \\sigma_t^{(k)}(s,a)| \\leq 4Ln^{-\\alpha} H^2$|\\sigma_t^{(k)}(k)(x,a) - \\sigma_t^{(k)}(k)(s,a)| \\leq 4Ln^{-\\alpha}-\\alpha H^2\\\\\n        &\\leq \\sqrt{\\sigma_t^{\\ast}(x,a)} + 2\\epsilon_{k}k + 12Ln^{-\\alpha}-\\alphaH^2 + 2(\\sqrt{Ln^{-\\alpha}} + (2\\theta_k)^{1/4}1/4) H, \n    \n    where we used that $\\operatorname{Var}[u^{(k-1)}_{t}] \\leq \\operatorname{Var}[V^\\ast_t] + \\operatorname{Var}[V^\\ast_t - u^{(k-1)}_{t}]$\\operatorname{Var}Var[u^{(k-1)}(k-1)_{t}t] \\leq \\operatorname{Var}Var[V^\\ast_t] + \\operatorname{Var}Var[V^\\ast_t - u^{(k-1)}(k-1)_{t}t] and that $\\operatorname{Var}[V^\\ast_t - u^{(k-1)}_{t}] \\leq (2\\epsilon_{k} + 12Ln^{-\\alpha}H^2)^2$\\operatorname{Var}Var[V^\\ast_t - u^{(k-1)}(k-1)_{t}t] \\leq (2\\epsilon_{k}k + 12Ln^{-\\alpha}-\\alphaH^2)^2 if $\\|V^\\ast_t - u^{(k-1)}_{t}\\|_\\infty \\leq 2\\epsilon_{k} + 12Ln^{-\\alpha}H^2$\\|V^\\ast_t - u^{(k-1)}(k-1)_{t}t\\|_\\infty \\leq 2\\epsilon_{k}k + 12Ln^{-\\alpha}-\\alphaH^2 according to the induction hypothesis. This means that, for all $(x,a,t)\\in\\mathcal{X}\\times\\mathcal{A}\\times[H]$(x,a,t)\\in\\mathcal{X}\\times\\mathcal{A}\\times[H], with probability at least $1-\\frac{\\delta}{2K}$1-\\frac{\\delta}{2K},\n    \\label{eq:classical_backward_eq1}\n    \n        \\widehat{\\mu}^{(k)}(k)_t \\!(x,a) \\!&\\leq\\! \\mu_t^{(k)}(k) \\!(x,a), \\label{eq:classical_backward_eq1a} \\\\\n        \\widehat{\\mu}^{(k)}(k)_t \\!(x,a) \\!&\\geq \\! \\mu_t^{(k)}(k) \\!(x,a) \\!-\\! \\sqrt{8\\theta_k\\sigma_t^\\ast(x,a)} \\!-\\! \\sqrt{8\\theta_k}(2\\epsilon_{k}k +12Ln^{-\\alpha}-\\alphaH^2) \\!-\\! \\left(\\!\\frac{16}{3}\\theta_k \\!+\\! 8(2\\theta_k)^{3/4}3/4 \\!+\\! 4Ln^{-\\alpha}-\\alpha\\!\\right)\\! H.\\label{eq:classical_backward_eq1b}\n    \n    \n    \n    Given the above inequalities, which we condition on, we now analyse the backward iteration that happens within epoch $k\\in[K]$k\\in[K] when calculating $u^{(k)}_t\\in\\mathscr{B}(\\mathcal{X})$u^{(k)}(k)_t\\in\\mathscr{B}(\\mathcal{X}) for $t=H,H-1,\\dots, 1$t=H,H-1,\\dots, 1. First let us focus on proving that $u_t^{(k)}(x) \\leq V_t^{\\pi^{(k)}}(x) \\leq V^\\ast_t(x)$u_t^{(k)}(k)(x) \\leq V_t^{\\pi^{(k)}}\\pi^{(k)}(k)(x) \\leq V^\\ast_t(x). For such, assume by induction that\n    \n        u_{t'}t'^{(k)}(k)(x) \\leq (\\mathcal{L}_{\\pi^{(k)}_{t'}}\\pi^{(k)}(k)_{t'}t'u^{(k)}(k)_{t'+1}t'+1)(x) \\leq V_{t'}t'^{\\pi^{(k)}}\\pi^{(k)}(k)(x) \\leq V^\\ast_{t'}t'(x) \\qquad\\forall x\\in\\mathcal{X}, t'=t+1,t+2,\\dots,H+1.\n    \n    The case $t=H+1$t=H+1 is trivial since $V_{H+1}^\\ast, u^{(k)}_{H+1} \\equiv 0$V_{H+1}H+1^\\ast, u^{(k)}(k)_{H+1}H+1 \\equiv 0. For $t\\leq H$t\\leq H, consider, $\\forall s\\in\\mathcal{S}_n,x\\in\\mathcal{X}(s),a\\in\\mathcal{A}$\\forall s\\in\\mathcal{S}_n,x\\in\\mathcal{X}(s),a\\in\\mathcal{A}, the quantities from \\Cref{line:classical_backward_line3},\n    \n        \\widehat{\\beta}_{t+1}t+1^{(k)}(k) (x,a) := \\frac{1}{\\ell_k}\\sum_{i=1}i=1^{\\ell_k}\\ell_k \\big( u^{(k)}(k)_{t+1}t+1(\\bar{x}^{(i)}(i)_{s,a}s,a) - u^{(k-1)}(k-1)_{t+1}t+1(\\bar{x}^{(i)}(i)_{s,a}s,a) \\big) - \\frac{\\epsilon_{k}}{4H} - \\frac{3}{2}Ln^{-\\alpha}-\\alphaH.\n    \n    Note that $u^{(k-1)}_{t+1}(x) \\leq u^{(k)}_{t+1}(x)$u^{(k-1)}(k-1)_{t+1}t+1(x) \\leq u^{(k)}(k)_{t+1}t+1(x) holds, since it is enforced by design (\\Cref{line:classical_backward_line4}). Due to that and by the induction hypothesis (both in $t$t and $k$k), then $\\|u^{(k)}_{t+1} - u^{(k-1)}_{t+1}\\|_\\infty \\leq 2\\epsilon_k + 12Ln^{-\\alpha}H^2$\\|u^{(k)}(k)_{t+1}t+1 - u^{(k-1)}(k-1)_{t+1}t+1\\|_\\infty \\leq 2\\epsilon_k + 12Ln^{-\\alpha}-\\alphaH^2. Moreover, $\\big(\\frac{1}{16H} + Ln^{-\\alpha}\\big)(2\\epsilon_{k} + 12Ln^{-\\alpha}H^2) \\leq \\frac{\\epsilon_{k}}{4H} + \\frac{3}{2}Ln^{-\\alpha}H$\\big(\\frac{1}{16H} + Ln^{-\\alpha}-\\alpha\\big)(2\\epsilon_{k}k + 12Ln^{-\\alpha}-\\alphaH^2) \\leq \\frac{\\epsilon_{k}}{4H} + \\frac{3}{2}Ln^{-\\alpha}-\\alphaH using that $Ln^{-\\alpha} \\leq \\frac{1}{16H}$Ln^{-\\alpha}-\\alpha \\leq \\frac{1}{16H}. Hence, according to \\Cref{lem:hoeffding_classical_backward} with $\\ell_k = \\big\\lceil2^9 H^2\\ln\\frac{4H|\\mathcal{S}_n|AK}{\\delta} \\big\\rceil$\\ell_k = \\big\\lceil2^9 H^2\\ln\\frac{4H|\\mathcal{S}_n|AK}{\\delta} \\big\\rceil, with probability at least $1-\\frac{\\delta}{2KH}$1-\\frac{\\delta}{2KH}, for all $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$(x,a)\\in\\mathcal{X}\\times\\mathcal{A},\\label{eq:classical_backward_eq2}\n    \n        \\widehat{\\beta}^{(k)}(k)_{t+1}t+1(x,a) &\\leq \\int_{\\mathcal{X}}\\mathcal{X}  p(\\rd x'|x,a)\\big(u^{(k)}(k)_{t}t(x') - u^{(k-1)}(k-1)_{t+1}t+1(x') \\big), \\label{eq:classical_backward_eq2a}\\\\\n        \\widehat{\\beta}^{(k)}(k)_{t+1}t+1(x,a) &\\geq \\int_{\\mathcal{X}}\\mathcal{X}  p(\\rd x'|x,a)\\big(u^{(k)}(k)_{t+1}t+1(x') - u^{(k-1)}(k-1)_{t+1}t+1(x') \\big) - \\frac{\\epsilon_{k}}{2H}- 3Ln^{-\\alpha}-\\alphaH. \\label{eq:classical_backward_eq2b}\n    \n    \n    Conditioned on the above, there are two cases to consider. \\textbf{Case 1: $\\pi^{(k)}_t \\neq \\pi^{(k-1)}_{t}$.} If $\\pi^{(k)}_t(s) \\neq \\pi^{(k-1)}_{t}(s)$\\pi^{(k)}(k)_t(s) \\neq \\pi^{(k-1)}(k-1)_{t}t(s) for some $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n, then it means that, for all $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s),\n    \n        u_t^{(k)}(k)(x) := u_t^{(k)}(k)(s) &= r(s,\\pi^{(k)}(k)_t(s)) - Ln^{-\\alpha}-\\alpha + \\widehat{\\mu}_{t+1}t+1^{(k)}(k)(s,\\pi^{(k)}(k)_t(s)) + \\widehat{\\beta}_{t+1}t+1^{(k)}(k)(s,\\pi^{(k)}(k)_t(s)) \\\\\n        &\\leq r(x,\\pi^{(k)}(k)_t(x)) + \\widehat{\\mu}_{t+1}t+1^{(k)}(k)(x,\\pi^{(k)}(k)_t(x)) + \\widehat{\\beta}_{t+1}t+1^{(k)}(k)(x,\\pi^{(k)}(k)_t(x))\\\\\n        &\\leq r(x,\\pi^{(k)}(k)_t(x)) + \\int_{\\mathcal{X}}\\mathcal{X}p(\\text{d}x'|x,\\pi^{(k)}(k)_t(x)) u^{(k)}(k)_{t+1}t+1(x') \\tag{by \\Cref{eq:classical_backward_eq1a,eq:classical_backward_eq2a}}by \\Cref{eq:classical_backward_eq1a,eq:classical_backward_eq2a}\\\\\n        &= (\\mathcal{L}_{\\pi^{(k)}_t}\\pi^{(k)}(k)_tu^{(k)}(k)_{t+1}t+1)(x).\n    \n    \\textbf{Case 2: $\\pi^{(k)}_t = \\pi^{(k-1)}_{t}$.} If both policies are equal, then, for all $x\\in\\mathcal{X}$x\\in\\mathcal{X},\n    \n        u^{(k)}(k)_t(x) = u^{(k-1)}(k-1)_{t}t(x) \\leq (\\mathcal{L}_{\\pi^{(k-1)}_{t}}\\pi^{(k-1)}(k-1)_{t}tu^{(k-1)}(k-1)_{t+1}t+1)(x) \\leq (\\mathcal{L}_{\\pi^{(k-1)}_{t}}\\pi^{(k-1)}(k-1)_{t}tu^{(k)}(k)_{t+1}t+1)(x) = (\\mathcal{L}_{\\pi^{(k)}_{t}}\\pi^{(k)}(k)_{t}tu^{(k)}(k)_{t+1}t+1)(x),\n    \n    where we used the induction hypothesis to argue that $u^{(k-1)}_{t}(x) \\leq (\\mathcal{L}_{\\pi^{(k-1)}_{t}}u^{(k-1)}_{t+1})(x)$u^{(k-1)}(k-1)_{t}t(x) \\leq (\\mathcal{L}_{\\pi^{(k-1)}_{t}}\\pi^{(k-1)}(k-1)_{t}tu^{(k-1)}(k-1)_{t+1}t+1)(x). From the above, we can readily see that $u^{(k)}_t(x) \\leq V_t^\\ast(x)$u^{(k)}(k)_t(x) \\leq V_t^\\ast(x), which completes the induction on $t$t.\n\n    We now move on to proving that $u^{(k)}_t(x) \\geq V^\\ast_t(x) - \\epsilon_k - 12Ln^{-\\alpha}H^2$u^{(k)}(k)_t(x) \\geq V^\\ast_t(x) - \\epsilon_k - 12Ln^{-\\alpha}-\\alphaH^2 (it is implicitly assumed that all $u^{(k)}_{t'+1}$u^{(k)}(k)_{t'+1}t'+1 for $t' = t,\\dots, H-1$t' = t,\\dots, H-1 have already been computed by the backward iteration). For such, first note that\n    \n        V^\\ast_t(x) - u^{(k)}(k)_t(x) &= (\\mathcal{L}V^\\ast_{t+1}t+1)(x) - u^{(k)}(k)_t(x)\\\\\n        &\\leq \\max_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\left\\{r(x,a) + \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)V_{t+1}t+1^\\ast(x') \\right\\} \\\\\n        &~- \\max_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\left\\{r(x,a) - 2Ln^{-\\alpha}-\\alpha + \\widehat{\\mu}_{t+1}t+1^{(k)}(k)(x,a) + \\widehat{\\beta}_{t+1}t+1^{(k)}(k)(x,a) \\right\\} \\tag{$u_t^{(k)}(x) \\!\\geq\\! u_t^{(k)}(s) \\!-\\! Ln^{-\\alpha}$}$u_t^{(k)}(x) \\!\\geq\\! u_t^{(k)}(s) \\!-\\! Ln^{-\\alpha}$u_t^{(k)}(k)(x) \\!\\geq\\! u_t^{(k)}(k)(s) \\!-\\! Ln^{-\\alpha}-\\alpha\\\\\n        &\\leq \\int_{\\mathcal{X}}\\mathcal{X} p(\\rd x'|x,\\pi^{\\ast}\\ast_{t}t(x))\\big(V_{t+1}t+1^\\ast(x') - u^{(k)}(k)_{t+1}t+1(x')\\big) + \\xi_{t+1}t+1^{(k)}(k)(x), \\tag{by \\Cref{eq:classical_backward_eq1b,eq:classical_backward_eq2b}}by \\Cref{eq:classical_backward_eq1b,eq:classical_backward_eq2b}\n    \n    where we defined\n    \n        \\xi_{t+1}t+1^{(k)}(k)(x) \\!:=\\! \\sqrt{8\\theta_k\\sigma^\\ast_{t+1}(x,\\pi^\\ast_{t}(x))} + \\frac{\\epsilon_{k}}{2H} + \\sqrt{8\\theta_k}(2\\epsilon_{k}k +  12Ln^{-\\alpha}-\\alphaH^2)\n        + \\left(\\!\\frac{16}{3}\\theta_k + 8(2\\theta_k)^{3/4}3/4 + 9 Ln^{-\\alpha}-\\alpha\\!\\right)\\! H.\n    \n    Solving the recursion, we obtain that\n    \n        V^\\ast_t(x) - u^{(k)}(k)_t(x) \\leq \\sum_{t'=t}t'=t^{H-1}H-1 \\idotsint_{\\mathcal{X}^{t'-t}}\\mathcal{X}^{t'-t}t'-t p_{\\pi_{t}^\\ast}\\pi_{t}t^\\ast(\\rd x_{t+1}t+1|x) p_{\\pi_{t+1}^\\ast}\\pi_{t+1}t+1^\\ast(\\rd x_{t+2}t+2|x_{t+1}t+1)\\cdots p_{\\pi_{t'-1}^\\ast}\\pi_{t'-1}t'-1^\\ast(\\rd x_{t'}t'|x_{t'-1}t'-1) \\xi_{t'+1}t'+1^{(k)}(k)(x_{t'}t').\n    \n    By employing that $\\|\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x)\\cdots p_{\\pi_{t'-1}^\\ast}(\\rd x_{t'}|x_{t'-1})\\|_\\infty \\leq H - t + 1$\\|\\sum_{t'=t}t'=t^{H-1}H-1 \\idotsint_{\\mathcal{X}^{t'-t}}\\mathcal{X}^{t'-t}t'-t p_{\\pi_{t}^\\ast}\\pi_{t}t^\\ast(\\rd x_{t+1}t+1|x)\\cdots p_{\\pi_{t'-1}^\\ast}\\pi_{t'-1}t'-1^\\ast(\\rd x_{t'}t'|x_{t'-1}t'-1)\\|_\\infty \\leq H - t + 1 and that\n    \n        \\left\\|\\sum_{t'=t}t'=t^{H-1}H-1 \\idotsint_{\\mathcal{X}^{t'-t}}\\mathcal{X}^{t'-t}t'-t p_{\\pi_{t}^\\ast}\\pi_{t}t^\\ast(\\rd x_{t+1}t+1|x)\\cdots p_{\\pi_{t'-1}^\\ast}\\pi_{t'-1}t'-1^\\ast(\\rd x_{t'}t'|x_{t'-1}t'-1)\\sqrt{\\sigma_{t'+1}^\\ast(x_{t'},\\pi^\\ast_{t'}(x_{t'}))}\\right\\|_\\infty \\leq H^{3/2}3/2, \\tag{\\Cref{fact:upper_bound_variance}}\\Cref{fact:upper_bound_variance}\n    \n    we finally obtain that\n    \n        V^\\ast_t(x) - u^{(k)}(k)_t(x) &\\leq \\frac{\\epsilon_{k}}{2} + 2\\sqrt{2\\theta_k}H^{3/2}3/2 + 4\\sqrt{2\\theta_k}\\epsilon_{k}kH +  \\left(\\frac{16}{3}\\theta_k + 8(2\\theta_k)^{3/4}3/4\\right)H^2\\\\\n        &~~~~~+ 24\\sqrt{2\\theta_k}Ln^{-\\alpha}-\\alphaH^3 + 9Ln^{-\\alpha}-\\alphaH^2 \\\\\n        &\\leq \\epsilon_{k}k\\left(\\frac{1}{2} + \\frac{2\\sqrt{2}}{\\sqrt{128}} + \\frac{4\\sqrt{2}}{\\sqrt{128}} + \\frac{16}{3\\cdot 128} + \\frac{8\\cdot 2^{3/4}}{128^{3/4}}\\right)\\\\\n        &~~~~~+ \\frac{24\\sqrt{2}}{\\sqrt{128}}Ln^{-\\alpha}-\\alphaH^{3/2}3/2 + 9Ln^{-\\alpha}-\\alpha H^2 \\tag{$\\theta_k\\leq \\frac{1}{128 H^3}\\min\\{\\epsilon_{k}^2, 1\\}$}$\\theta_k\\leq \\frac{1}{128 H^3}\\min\\{\\epsilon_{k}^2, 1\\}$\\theta_k\\leq \\frac{1}{128 H^3}\\min\\{\\epsilon_{k}k^2, 1\\}\\\\\n        &< \\epsilon_k + 12Ln^{-\\alpha}-\\alpha H^2,\n    \n    This concludes the proof of \\Cref{eq:classical_backward_eq0} for epoch $k\\in[K]$k\\in[K] and thus for all epochs by induction on $k$k.\n\n    Regarding the failure probability, \\Cref{eq:classical_backward_eq1} holds with probability $1-\\frac{\\delta}{2K}$1-\\frac{\\delta}{2K} for a single epoch, while \\Cref{eq:classical_backward_eq2} holds with probability $1-\\frac{\\delta}{2HK}$1-\\frac{\\delta}{2HK} for a single epoch and time step. Therefore, across all epochs and time steps, the success probability is at least $1-\\delta$1-\\delta, as required. Finally, the total number of samples used is\n    \\[\n        O\\bigg(\\sum_{k=1}^K (m_k + H\\ell_k)|\\mathcal{S}_n|A\\bigg) = O\\left(\\frac{H^3|\\mathcal{S}_n|A}{\\varepsilon^2}\\log\\left(\\frac{H|\\mathcal{S}_n|A}{\\delta}\\log\\frac{H}{\\varepsilon}\\right) \\right). \\qedhere\n    \\]\n        O\\bigg(\\sum_{k=1}k=1^K (m_k + H\\ell_k)|\\mathcal{S}_n|A\\bigg) = O\\left(\\frac{H^3|\\mathcal{S}_n|A}{\\varepsilon^2}\\log\\left(\\frac{H|\\mathcal{S}_n|A}{\\delta}\\log\\frac{H}{\\varepsilon}\\right) \\right). \\qedhere\n    \n\n\n\n\n\\subsection{Quantum backward induction algorithm}\n\\label{sec:quantum_backward_induction}\n\nIn this section, we propose two quantum algorithms that output an $\\varepsilon$\\varepsilon-optimal policy given sampling access to the probability kernel via the oracle $\\mathcal{O}_p$\\mathcal{O}_p. The first quantum algorithm is analogous to the classical approximate backward induction algorithm from the previous section while the second quantum algorithm is based on a much simpler backward induction algorithm. For both algorithms, the main idea is to employ quantum mean estimation subroutines~\\cite{cornelissen2022near,Kothari2023mean} in order to estimate the quantities $\\int_{\\mathcal{X}}p(\\rd x'|x,a)u^{(k-1)}_{t}(x')$\\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)u^{(k-1)}(k-1)_{t}t(x') and $\\int_{\\mathcal{X}}p(\\rd x'|x,a)\\big(u^{(k)}_{t}(x') - u^{(k-1)}_{t}(x')\\big)$\\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)\\big(u^{(k)}(k)_{t}t(x') - u^{(k-1)}(k-1)_{t}t(x')\\big). Although we are not aware of any works on quantum algorithms for finite-horizon MDPs with a generative model, Wang \\emph{et al.}~\\cite{wang2021quantum} considered the case of infinite-horizon \\emph{discounted} MDPs with a generative model, which shares several similarities with the finite-horizon case. For example, an infinite-horizon discounted MDP can be interpreted as having an effective time horizon of $(1-\\gamma)^{-1}$(1-\\gamma)^{-1}-1, where $\\gamma\\in(0,1)$\\gamma\\in(0,1) is the discount factor. Thus, at least in the classical case, several complexities for finite-horizon MDPs can be obtained from infinite-horizon discounted MDPs by replacing $(1-\\gamma)^{-1}$(1-\\gamma)^{-1}-1 with $H$H. The following analysis thus takes inspiration from the work of Wang \\emph{et al.}~\\cite{wang2021quantum}, who also employed quantum mean estimation subroutines to speed-up the corresponding classical value iteration algorithm for infinite-horizon discounted MDPs from Sidford \\emph{et al.}~\\cite{sidford2018near}. Still, there are a few subtle differences, e.g., requiring the estimation of $H$H different means $\\int_{\\mathcal{X}}p(\\rd x'|x,a)u^{(k-1)}_{t}(x')$\\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)u^{(k-1)}(k-1)_{t}t(x') at the beginning of each epoch instead of just one as for infinite-horizon discounted MDPs. To get around this issue, we employ the quantum multivariate mean estimation subroutine from Cornelissen, Hamoudi, and Jerbi~\\cite{cornelissen2022near} to estimate these $H$H average quantities at once with only an $O(\\sqrt{H})$O(\\sqrt{H}) overhead. This is, however, the best one can hope for: there is no concept of ``reusing'' quantum samples like in classical algorithms, which ultimately hinders a full quadratic speed-up in terms of $H$H compared to the classical complexity. The remaining quantities $\\int_{\\mathcal{X}}p(\\rd x'|x,a)\\big(u^{(k)}_{t}(x') - u^{(k-1)}_{t}(x')\\big)$\\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)\\big(u^{(k)}(k)_{t}t(x') - u^{(k-1)}(k-1)_{t}t(x')\\big) during the backward recursion are estimated using the quantum mean estimation subroutine from Kothari and O'Donnell~\\cite{Kothari2023mean}.\n\nOur first quantum algorithm, shown in \\Cref{algo:quantum_backward_recursion}, is based on the modern backward induction algorithm from the previous section and relies on monotonicity, variance reduction, and total-variance techniques.\nThe overall correctness proof for \\Cref{algo:quantum_backward_recursion} is very similar to the one for \\Cref{algo:classical_backward_recursion}, hence, we will skip parts in which both proofs are virtually the same. We shall need a slightly different version of \\Cref{fact:upper_bound_variance}.\n\n\\begin{lemma}\\label{fact:upper_bound_variance2}\n    Let $\\sigma_t^\\pi(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)V_{t}^\\pi(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)V^\\pi_{t}(x')\\big)^2$ for some policy $\\pi$. For any policy $\\pi$ and $(x,t)\\in\\mathcal{X}\\times[H]$,\n    %\n    \\begin{align*}\n        \\sum_{k=1}^H\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_t}(\\rd x_{t+1}|x) p_{\\pi_{t+1}}(\\rd x_{t+2}|x_{t+1})\\cdots p_{\\pi_{t'-1}}(\\rd x_{t'}|x_{t'-1})\\sigma_{k}^\\pi(x_{t'},\\pi_{t'}(x_{t'})) \\leq 4H^{3},\n    \\end{align*}\n    %\n    where $\\sigma_{k}^\\pi(x_{t},\\pi_{t}(x_{t})) = \\sigma_{k}^\\pi(x,\\pi_t(x))$ for  $t'=t$.\n\\end{lemma}\\begin{lemma}\\label{fact:upper_bound_variance2}\n    Let $\\sigma_t^\\pi(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)V_{t}^\\pi(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)V^\\pi_{t}(x')\\big)^2$ for some policy $\\pi$. For any policy $\\pi$ and $(x,t)\\in\\mathcal{X}\\times[H]$,\n    %\n    \\begin{align*}\n        \\sum_{k=1}^H\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_t}(\\rd x_{t+1}|x) p_{\\pi_{t+1}}(\\rd x_{t+2}|x_{t+1})\\cdots p_{\\pi_{t'-1}}(\\rd x_{t'}|x_{t'-1})\\sigma_{k}^\\pi(x_{t'},\\pi_{t'}(x_{t'})) \\leq 4H^{3},\n    \\end{align*}\n    %\n    where $\\sigma_{k}^\\pi(x_{t},\\pi_{t}(x_{t})) = \\sigma_{k}^\\pi(x,\\pi_t(x))$ for  $t'=t$.\n\\end{lemma}\\label{fact:upper_bound_variance2}\n    Let $\\sigma_t^\\pi(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)V_{t}^\\pi(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)V^\\pi_{t}(x')\\big)^2$\\sigma_t^\\pi(x,a) := \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)V_{t}t^\\pi(x')^2 - \\big(\\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)V^\\pi_{t}t(x')\\big)^2 for some policy $\\pi$\\pi. For any policy $\\pi$\\pi and $(x,t)\\in\\mathcal{X}\\times[H]$(x,t)\\in\\mathcal{X}\\times[H],\n    \n        \\sum_{k=1}k=1^H\\sum_{t'=t}t'=t^{H-1}H-1 \\idotsint_{\\mathcal{X}^{t'-t}}\\mathcal{X}^{t'-t}t'-t p_{\\pi_t}\\pi_t(\\rd x_{t+1}t+1|x) p_{\\pi_{t+1}}\\pi_{t+1}t+1(\\rd x_{t+2}t+2|x_{t+1}t+1)\\cdots p_{\\pi_{t'-1}}\\pi_{t'-1}t'-1(\\rd x_{t'}t'|x_{t'-1}t'-1)\\sigma_{k}k^\\pi(x_{t'}t',\\pi_{t'}t'(x_{t'}t')) \\leq 4H^{3}3,\n    \n    where $\\sigma_{k}^\\pi(x_{t},\\pi_{t}(x_{t})) = \\sigma_{k}^\\pi(x,\\pi_t(x))$\\sigma_{k}k^\\pi(x_{t}t,\\pi_{t}t(x_{t}t)) = \\sigma_{k}k^\\pi(x,\\pi_t(x)) for  $t'=t$t'=t.\n\n\\begin{proof}\n    Given $u\\in\\mathscr{B}(\\mathcal{X})$, define the notation $P_t^\\pi u \\in \\mathscr{B}(\\mathcal{X})$ as $(P_t^\\pi u)(x) = \\int_{\\mathcal{X}} p_{\\pi_t}(\\rd x'|x) u(x')$ for simplicity. Then $V^\\pi_t = r^\\pi_{t} + P^{\\pi}_t V^\\pi_{t+1}$ for $t\\in[H]$ and $V^\\pi_{H+1} \\equiv 0$, where $r^\\pi_{t}(x) = r(x,\\pi_t(x))$. Moreover, write $\\operatorname{Var}_{P_{t}^\\pi}(V_{t'}^\\pi) = \\sigma_{t'}^{\\pi}(x,\\pi_{t}(x)) = \\int_{\\mathcal{X}}p(\\rd x'|x,\\pi_t(x))V_{t'}^\\pi(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,\\pi_t(x))V^\\pi_{t'}(x')\\big)^2$. If we let $(u\\circ v)(x) = u(x)v(x)$ denote the Hadamard product for $u,v\\in\\mathscr{B}(\\mathcal{X})$, then we can write\n    %\n    \\begin{align*}\n        \\operatorname{Var}_{P_t^\\pi}(V^\\pi_{k+1}) &= P_t^\\pi(V_{k+1}^\\pi \\circ V_{k+1}^\\pi) - (P_t^\\pi V^\\pi_{k+1})\\circ (P_t^\\pi V^\\pi_{k+1})\\\\\n        &= P_t^\\pi(V_{k+1}^\\pi \\circ V_{k+1}^\\pi) - (V^\\pi_k - r^\\pi_k)\\circ (V^\\pi_k - r^\\pi_k) \\\\\n        &= P_t^\\pi(V_{k+1}^\\pi \\circ V_{k+1}^\\pi)  - V^\\pi_k\\circ V^\\pi_k + 2V^\\pi_k\\circ r_k^\\pi - r^\\pi_k\\circ r^\\pi_k \\\\\n        &\\leq P_t^\\pi(V_{k+1}^\\pi \\circ V_{k+1}^\\pi)  - V^\\pi_k\\circ V^\\pi_k + 2V^\\pi_k\\circ r_k^\\pi.\n    \\end{align*}\n    %\n    Notice that the left-hand side of our sought-after statement can be rewritten more compactly as $\\sum_{k=1}^H \\sum_{j=t}^{H-1} \\prod_{i=t}^{j-1}P^\\pi_i \\operatorname{Var}_{P_{j}^\\pi}(V_k^\\pi)$. We bound it as\n    %\n    \\begin{align*}\n        &\\sum_{k=1}^H \\sum_{j=t}^{H-1} \\prod_{i=t}^{j-1}P^\\pi_i \\operatorname{Var}_{P_{j}^\\pi}(V_k^\\pi) \\\\\n        &\\leq \\sum_{j=t}^{H-1} \\prod_{i=t}^{j-1}P^\\pi_i \\operatorname{Var}_{P_{j}^\\pi}(V_1^\\pi) + \\sum_{k=1}^{H-1} \\sum_{j=t}^{H-1} \\prod_{i=t}^{j-1}P^\\pi_i\\big( P_j^\\pi (V_{k+1}^\\pi \\circ V_{k+1}^\\pi)  - V^\\pi_k\\circ V^\\pi_k + 2V^\\pi_k\\circ r_{k}^\\pi\\big)\\\\\n        &\\leq 3H^3 + \\sum_{k=1}^{H-1} \\sum_{j=t}^{H-1} \\left(\\prod_{i=t}^j P_i^\\pi (V_{k+1}^\\pi \\circ V_{k+1}^\\pi) - \\prod_{i=t}^{j-1}P_i^\\pi (V_{k}^\\pi \\circ V_{k}^\\pi) \\right) \\tag{since $\\|\\operatorname{Var}_{P_{j}^\\pi}(V_1^\\pi)\\|_\\infty \\leq H^2$, $\\|V_k^\\pi\\|_\\infty \\leq H$, $\\|\\sum_{k=1}^{H-1}r_k^\\pi\\|_\\infty \\leq H$}\\\\\n        &= 3H^3 + \\sum_{k=2}^{H} \\sum_{j=t}^{H-1} \\prod_{i=t}^j P_i^\\pi (V_{k}^\\pi \\circ V_{k}^\\pi) - \\sum_{k=1}^{H-1} \\sum_{j=t}^{H-2} \\prod_{i=t}^{j}P_i^\\pi (V_{k}^\\pi \\circ V_{k}^\\pi) - \\sum_{k=1}^{H-1} (V_{k}^\\pi \\circ V_{k}^\\pi) \\\\\n        &= 3H^3 + \\sum_{k=2}^H \\prod_{i=t}^{H-1} P_i^\\pi (V^\\pi_k\\circ V^\\pi_k) + \\sum_{j=t}^{H-2} \\prod_{i=t}^j P_i^\\pi (V^\\pi_H \\circ V^\\pi_H) - \\sum_{j=t}^{H-2} \\prod_{i=t}^j P_i^\\pi (V^\\pi_1 \\circ V^\\pi_1) - \\sum_{k=1}^{H-1} (V_{k}^\\pi \\circ V_{k}^\\pi) \\\\\n        &\\leq 3H^3 + \\sum_{j=t}^{H-1} \\prod_{i=t}^j P_i^\\pi (V^\\pi_H \\circ V^\\pi_H) - \\sum_{j=t}^{H-2} \\prod_{i=t}^j P_i^\\pi (V^\\pi_1 \\circ V^\\pi_1) \\tag{$\\|\\prod_{i=t}^{H-1} P_i^\\pi\\|_1 \\leq 1$} \\\\\n        &\\leq 4H^3. \\tag*{($\\|V^\\pi_H\\|_\\infty \\leq 1$) \\qquad\\qedhere}\n    \\end{align*}\n\\end{proof}\\begin{proof}\n    Given $u\\in\\mathscr{B}(\\mathcal{X})$, define the notation $P_t^\\pi u \\in \\mathscr{B}(\\mathcal{X})$ as $(P_t^\\pi u)(x) = \\int_{\\mathcal{X}} p_{\\pi_t}(\\rd x'|x) u(x')$ for simplicity. Then $V^\\pi_t = r^\\pi_{t} + P^{\\pi}_t V^\\pi_{t+1}$ for $t\\in[H]$ and $V^\\pi_{H+1} \\equiv 0$, where $r^\\pi_{t}(x) = r(x,\\pi_t(x))$. Moreover, write $\\operatorname{Var}_{P_{t}^\\pi}(V_{t'}^\\pi) = \\sigma_{t'}^{\\pi}(x,\\pi_{t}(x)) = \\int_{\\mathcal{X}}p(\\rd x'|x,\\pi_t(x))V_{t'}^\\pi(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,\\pi_t(x))V^\\pi_{t'}(x')\\big)^2$. If we let $(u\\circ v)(x) = u(x)v(x)$ denote the Hadamard product for $u,v\\in\\mathscr{B}(\\mathcal{X})$, then we can write\n    %\n    \\begin{align*}\n        \\operatorname{Var}_{P_t^\\pi}(V^\\pi_{k+1}) &= P_t^\\pi(V_{k+1}^\\pi \\circ V_{k+1}^\\pi) - (P_t^\\pi V^\\pi_{k+1})\\circ (P_t^\\pi V^\\pi_{k+1})\\\\\n        &= P_t^\\pi(V_{k+1}^\\pi \\circ V_{k+1}^\\pi) - (V^\\pi_k - r^\\pi_k)\\circ (V^\\pi_k - r^\\pi_k) \\\\\n        &= P_t^\\pi(V_{k+1}^\\pi \\circ V_{k+1}^\\pi)  - V^\\pi_k\\circ V^\\pi_k + 2V^\\pi_k\\circ r_k^\\pi - r^\\pi_k\\circ r^\\pi_k \\\\\n        &\\leq P_t^\\pi(V_{k+1}^\\pi \\circ V_{k+1}^\\pi)  - V^\\pi_k\\circ V^\\pi_k + 2V^\\pi_k\\circ r_k^\\pi.\n    \\end{align*}\n    %\n    Notice that the left-hand side of our sought-after statement can be rewritten more compactly as $\\sum_{k=1}^H \\sum_{j=t}^{H-1} \\prod_{i=t}^{j-1}P^\\pi_i \\operatorname{Var}_{P_{j}^\\pi}(V_k^\\pi)$. We bound it as\n    %\n    \\begin{align*}\n        &\\sum_{k=1}^H \\sum_{j=t}^{H-1} \\prod_{i=t}^{j-1}P^\\pi_i \\operatorname{Var}_{P_{j}^\\pi}(V_k^\\pi) \\\\\n        &\\leq \\sum_{j=t}^{H-1} \\prod_{i=t}^{j-1}P^\\pi_i \\operatorname{Var}_{P_{j}^\\pi}(V_1^\\pi) + \\sum_{k=1}^{H-1} \\sum_{j=t}^{H-1} \\prod_{i=t}^{j-1}P^\\pi_i\\big( P_j^\\pi (V_{k+1}^\\pi \\circ V_{k+1}^\\pi)  - V^\\pi_k\\circ V^\\pi_k + 2V^\\pi_k\\circ r_{k}^\\pi\\big)\\\\\n        &\\leq 3H^3 + \\sum_{k=1}^{H-1} \\sum_{j=t}^{H-1} \\left(\\prod_{i=t}^j P_i^\\pi (V_{k+1}^\\pi \\circ V_{k+1}^\\pi) - \\prod_{i=t}^{j-1}P_i^\\pi (V_{k}^\\pi \\circ V_{k}^\\pi) \\right) \\tag{since $\\|\\operatorname{Var}_{P_{j}^\\pi}(V_1^\\pi)\\|_\\infty \\leq H^2$, $\\|V_k^\\pi\\|_\\infty \\leq H$, $\\|\\sum_{k=1}^{H-1}r_k^\\pi\\|_\\infty \\leq H$}\\\\\n        &= 3H^3 + \\sum_{k=2}^{H} \\sum_{j=t}^{H-1} \\prod_{i=t}^j P_i^\\pi (V_{k}^\\pi \\circ V_{k}^\\pi) - \\sum_{k=1}^{H-1} \\sum_{j=t}^{H-2} \\prod_{i=t}^{j}P_i^\\pi (V_{k}^\\pi \\circ V_{k}^\\pi) - \\sum_{k=1}^{H-1} (V_{k}^\\pi \\circ V_{k}^\\pi) \\\\\n        &= 3H^3 + \\sum_{k=2}^H \\prod_{i=t}^{H-1} P_i^\\pi (V^\\pi_k\\circ V^\\pi_k) + \\sum_{j=t}^{H-2} \\prod_{i=t}^j P_i^\\pi (V^\\pi_H \\circ V^\\pi_H) - \\sum_{j=t}^{H-2} \\prod_{i=t}^j P_i^\\pi (V^\\pi_1 \\circ V^\\pi_1) - \\sum_{k=1}^{H-1} (V_{k}^\\pi \\circ V_{k}^\\pi) \\\\\n        &\\leq 3H^3 + \\sum_{j=t}^{H-1} \\prod_{i=t}^j P_i^\\pi (V^\\pi_H \\circ V^\\pi_H) - \\sum_{j=t}^{H-2} \\prod_{i=t}^j P_i^\\pi (V^\\pi_1 \\circ V^\\pi_1) \\tag{$\\|\\prod_{i=t}^{H-1} P_i^\\pi\\|_1 \\leq 1$} \\\\\n        &\\leq 4H^3. \\tag*{($\\|V^\\pi_H\\|_\\infty \\leq 1$) \\qquad\\qedhere}\n    \\end{align*}\n\\end{proof}\n    Given $u\\in\\mathscr{B}(\\mathcal{X})$u\\in\\mathscr{B}(\\mathcal{X}), define the notation $P_t^\\pi u \\in \\mathscr{B}(\\mathcal{X})$P_t^\\pi u \\in \\mathscr{B}(\\mathcal{X}) as $(P_t^\\pi u)(x) = \\int_{\\mathcal{X}} p_{\\pi_t}(\\rd x'|x) u(x')$(P_t^\\pi u)(x) = \\int_{\\mathcal{X}}\\mathcal{X} p_{\\pi_t}\\pi_t(\\rd x'|x) u(x') for simplicity. Then $V^\\pi_t = r^\\pi_{t} + P^{\\pi}_t V^\\pi_{t+1}$V^\\pi_t = r^\\pi_{t}t + P^{\\pi}\\pi_t V^\\pi_{t+1}t+1 for $t\\in[H]$t\\in[H] and $V^\\pi_{H+1} \\equiv 0$V^\\pi_{H+1}H+1 \\equiv 0, where $r^\\pi_{t}(x) = r(x,\\pi_t(x))$r^\\pi_{t}t(x) = r(x,\\pi_t(x)). Moreover, write $\\operatorname{Var}_{P_{t}^\\pi}(V_{t'}^\\pi) = \\sigma_{t'}^{\\pi}(x,\\pi_{t}(x)) = \\int_{\\mathcal{X}}p(\\rd x'|x,\\pi_t(x))V_{t'}^\\pi(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,\\pi_t(x))V^\\pi_{t'}(x')\\big)^2$\\operatorname{Var}Var_{P_{t}^\\pi}P_{t}t^\\pi(V_{t'}t'^\\pi) = \\sigma_{t'}t'^{\\pi}\\pi(x,\\pi_{t}t(x)) = \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,\\pi_t(x))V_{t'}t'^\\pi(x')^2 - \\big(\\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,\\pi_t(x))V^\\pi_{t'}t'(x')\\big)^2. If we let $(u\\circ v)(x) = u(x)v(x)$(u\\circ v)(x) = u(x)v(x) denote the Hadamard product for $u,v\\in\\mathscr{B}(\\mathcal{X})$u,v\\in\\mathscr{B}(\\mathcal{X}), then we can write\n    \n        \\operatorname{Var}Var_{P_t^\\pi}P_t^\\pi(V^\\pi_{k+1}k+1) &= P_t^\\pi(V_{k+1}k+1^\\pi \\circ V_{k+1}k+1^\\pi) - (P_t^\\pi V^\\pi_{k+1}k+1)\\circ (P_t^\\pi V^\\pi_{k+1}k+1)\\\\\n        &= P_t^\\pi(V_{k+1}k+1^\\pi \\circ V_{k+1}k+1^\\pi) - (V^\\pi_k - r^\\pi_k)\\circ (V^\\pi_k - r^\\pi_k) \\\\\n        &= P_t^\\pi(V_{k+1}k+1^\\pi \\circ V_{k+1}k+1^\\pi)  - V^\\pi_k\\circ V^\\pi_k + 2V^\\pi_k\\circ r_k^\\pi - r^\\pi_k\\circ r^\\pi_k \\\\\n        &\\leq P_t^\\pi(V_{k+1}k+1^\\pi \\circ V_{k+1}k+1^\\pi)  - V^\\pi_k\\circ V^\\pi_k + 2V^\\pi_k\\circ r_k^\\pi.\n    \n    Notice that the left-hand side of our sought-after statement can be rewritten more compactly as $\\sum_{k=1}^H \\sum_{j=t}^{H-1} \\prod_{i=t}^{j-1}P^\\pi_i \\operatorname{Var}_{P_{j}^\\pi}(V_k^\\pi)$\\sum_{k=1}k=1^H \\sum_{j=t}j=t^{H-1}H-1 \\prod_{i=t}i=t^{j-1}j-1P^\\pi_i \\operatorname{Var}Var_{P_{j}^\\pi}P_{j}j^\\pi(V_k^\\pi). We bound it as\n    \n        &\\sum_{k=1}k=1^H \\sum_{j=t}j=t^{H-1}H-1 \\prod_{i=t}i=t^{j-1}j-1P^\\pi_i \\operatorname{Var}Var_{P_{j}^\\pi}P_{j}j^\\pi(V_k^\\pi) \\\\\n        &\\leq \\sum_{j=t}j=t^{H-1}H-1 \\prod_{i=t}i=t^{j-1}j-1P^\\pi_i \\operatorname{Var}Var_{P_{j}^\\pi}P_{j}j^\\pi(V_1^\\pi) + \\sum_{k=1}k=1^{H-1}H-1 \\sum_{j=t}j=t^{H-1}H-1 \\prod_{i=t}i=t^{j-1}j-1P^\\pi_i\\big( P_j^\\pi (V_{k+1}k+1^\\pi \\circ V_{k+1}k+1^\\pi)  - V^\\pi_k\\circ V^\\pi_k + 2V^\\pi_k\\circ r_{k}k^\\pi\\big)\\\\\n        &\\leq 3H^3 + \\sum_{k=1}k=1^{H-1}H-1 \\sum_{j=t}j=t^{H-1}H-1 \\left(\\prod_{i=t}i=t^j P_i^\\pi (V_{k+1}k+1^\\pi \\circ V_{k+1}k+1^\\pi) - \\prod_{i=t}i=t^{j-1}j-1P_i^\\pi (V_{k}k^\\pi \\circ V_{k}k^\\pi) \\right) \\tag{since $\\|\\operatorname{Var}_{P_{j}^\\pi}(V_1^\\pi)\\|_\\infty \\leq H^2$, $\\|V_k^\\pi\\|_\\infty \\leq H$, $\\|\\sum_{k=1}^{H-1}r_k^\\pi\\|_\\infty \\leq H$}since $\\|\\operatorname{Var}_{P_{j}^\\pi}(V_1^\\pi)\\|_\\infty \\leq H^2$\\|\\operatorname{Var}Var_{P_{j}^\\pi}P_{j}j^\\pi(V_1^\\pi)\\|_\\infty \\leq H^2, $\\|V_k^\\pi\\|_\\infty \\leq H$\\|V_k^\\pi\\|_\\infty \\leq H, $\\|\\sum_{k=1}^{H-1}r_k^\\pi\\|_\\infty \\leq H$\\|\\sum_{k=1}k=1^{H-1}H-1r_k^\\pi\\|_\\infty \\leq H\\\\\n        &= 3H^3 + \\sum_{k=2}k=2^{H}H \\sum_{j=t}j=t^{H-1}H-1 \\prod_{i=t}i=t^j P_i^\\pi (V_{k}k^\\pi \\circ V_{k}k^\\pi) - \\sum_{k=1}k=1^{H-1}H-1 \\sum_{j=t}j=t^{H-2}H-2 \\prod_{i=t}i=t^{j}jP_i^\\pi (V_{k}k^\\pi \\circ V_{k}k^\\pi) - \\sum_{k=1}k=1^{H-1}H-1 (V_{k}k^\\pi \\circ V_{k}k^\\pi) \\\\\n        &= 3H^3 + \\sum_{k=2}k=2^H \\prod_{i=t}i=t^{H-1}H-1 P_i^\\pi (V^\\pi_k\\circ V^\\pi_k) + \\sum_{j=t}j=t^{H-2}H-2 \\prod_{i=t}i=t^j P_i^\\pi (V^\\pi_H \\circ V^\\pi_H) - \\sum_{j=t}j=t^{H-2}H-2 \\prod_{i=t}i=t^j P_i^\\pi (V^\\pi_1 \\circ V^\\pi_1) - \\sum_{k=1}k=1^{H-1}H-1 (V_{k}k^\\pi \\circ V_{k}k^\\pi) \\\\\n        &\\leq 3H^3 + \\sum_{j=t}j=t^{H-1}H-1 \\prod_{i=t}i=t^j P_i^\\pi (V^\\pi_H \\circ V^\\pi_H) - \\sum_{j=t}j=t^{H-2}H-2 \\prod_{i=t}i=t^j P_i^\\pi (V^\\pi_1 \\circ V^\\pi_1) \\tag{$\\|\\prod_{i=t}^{H-1} P_i^\\pi\\|_1 \\leq 1$}$\\|\\prod_{i=t}^{H-1} P_i^\\pi\\|_1 \\leq 1$\\|\\prod_{i=t}i=t^{H-1}H-1 P_i^\\pi\\|_1 \\leq 1 \\\\\n        &\\leq 4H^3. \\tag*{($\\|V^\\pi_H\\|_\\infty \\leq 1$) \\qquad\\qedhere}($\\|V^\\pi_H\\|_\\infty \\leq 1$\\|V^\\pi_H\\|_\\infty \\leq 1) \\qquad\\qedhere\n    \n\n\n\n\n\\begin{algorithm}[t!]\n    \\caption{Modern quantum backward induction algorithm}\n    \\label{algo:quantum_backward_recursion}\n    \\begin{algorithmic}[1]  \n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$,  finite action space $\\mathcal{A}$, horizon $H$, quantum sampling access to probability kernels $p$, failure probability $\\delta\\in(0, 1)$, error $\\varepsilon > 0$.\n\n    \\Ensure $(\\varepsilon + 8Ln^{-\\alpha}H^2)$-optimal policy $\\pi^{(K)} \\in \\Pi^{\\rm D}$. \n\n    \\State Initialise $u^{(0)}_t \\equiv 0$ for all $t\\in[H]$ and $\\pi^{(0)}\\in\\Pi^{\\rm D}$ arbitrary\n\n    \\For{$k=1$ to $K:=\\lceil\\log_2(H/\\varepsilon)\\rceil$}\n\n        \\State $\\epsilon_k \\gets H/2^k$ and $\\theta_k \\gets \\frac{\\min\\{\\epsilon_k,1\\}}{20 H^{3/2}}$\n    \n%        \\State For each $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, sample $x^{(1)}_{s,a},x^{(2)}_{s,a},\\dots,x^{(m_k)}_{s,a}\\in\\mathcal{X}$\n    \n        \\For{$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$}\n\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use \\Cref{fact:quantum_multidimensional_mean_estimation_covariance_matrix} to get $\\widetilde{\\sigma}^{(k)}_t(s,a)$ such that $\\big|\\widetilde{\\sigma}^{(k)}_t(s,a) - \\sigma^{(k)}_t(s,a)\\big| \\leq \\theta_kH^2$, where $\\sigma^{(k)}_t(s,a) := \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t^{(k-1)}(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t^{(k-1)}(x')\\big)^2$ \\label{line:quantum_backward_line1}}\n    \n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use \\Cref{fact:quantum_multidimensional_mean_estimation_covariance_matrix} to get $\\widetilde{\\mu}^{(k)}_t(s,a)$ such that $\\big| \\widetilde{\\mu}^{(k)}_t(s,a) - \\mu_t^{(k)}(s,a)\\big| \\leq \\theta_k \\sqrt{\\frac{1}{H}\\sum_{t'=1}^H \\sigma_{t'}^{(k)}(s,a)}$, where $\\mu_t^{(k)}(s,a) := \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t^{(k-1)}(x')$ \\label{line:quantum_backward_line2}}\n    \n        \\EndFor\n\n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent}{$\\widehat{\\mu}_t^{(k)}(x,a) \\gets \\widetilde{\\mu}_t^{(k)}(s,a) - \\theta_k\\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\widetilde{\\sigma}_{t'}^{(k)}(s,a)} - (\\theta_k^{3/2} + Ln^{-\\alpha})H$ for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$, $x\\in\\mathcal{X}(s)$}\n\n        \\State $u^{(k)}_H(x) \\gets u^{(k-1)}_{H}(x)$ for all $x\\in\\mathcal{X}$ and $\\pi_H^{(k)}\\gets \\pi_{H}^{(k-1)}$\n\n        \\For{$t=H-1,H-2,\\dots,1$}\n\n            \\For{$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$}\n\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent-\\algorithmicindent}{Use \\Cref{lem:quantum_mean_estimation} to get $\\widetilde{\\beta}_{t+1}^{(k)}\\!(s,a)$ such that $\\big|\\widetilde{\\beta}_{t+1}^{(k)}\\!(s,a) - \\beta^{(k)}_{t+1}\\!(s,a) \\big| \\!\\leq\\! \\frac{\\epsilon_k}{4H} - Ln^{-\\alpha}H$, where $\\beta^{(k)}_{t+1}(s,a) := \\int_{\\mathcal{X}}p(\\rd x'|s,a)\\big(u^{(k)}_{t+1}(x') -  u^{(k-1)}_{t+1}(x')\\big)$ \\label{line:quantum_backward_line3}}\n\n            \\State $\\widehat{\\beta}^{(k)}_{t+1}(s,a) \\gets \\widetilde{\\beta}_{t+1}^{(k)}(s,a) - \\frac{\\epsilon_k}{4H} - Ln^{-\\alpha}H$\n\n            \\EndFor\n\n            \\For{$s\\in\\mathcal{S}_n$}\n\n            \\State $u^{(k)}_t(s) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\widehat{\\mu}_{t+1}^{(k)}(s,a) + \\widehat{\\beta}_{t+1}^{(k)}(s,a)\\} - Ln^{-\\alpha}$\n            \n            \\State $\\pi_t^{(k)}(s) \\gets \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\widehat{\\mu}_{t+1}^{(k)}(s,a) + \\widehat{\\beta}_{t+1}^{(k)}(s,a)\\}$\n\n            \\State If $u^{(k)}_t(s) \\leq u^{(k-1)}_{t}(s)$, then $u^{(k)}_t(x) \\!\\gets\\! u^{(k-1)}_{t}(x)$ and $\\pi_t^{(k)}(x) \\!\\gets\\! \\pi_{t}^{(k-1)}(x)$  $\\forall x\\in\\mathcal{X}(s)$\n            \n            \\EndFor\n\n        \\EndFor\n\n    \\EndFor\n    \n    \\State \\Return $\\pi^{(K)}\\in\\Pi^{\\rm D}$\n\\end{algorithmic}\n\\end{algorithm}\\begin{algorithm}[t!]\n    \\caption{Modern quantum backward induction algorithm}\n    \\label{algo:quantum_backward_recursion}\n    \\begin{algorithmic}[1]  \n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$,  finite action space $\\mathcal{A}$, horizon $H$, quantum sampling access to probability kernels $p$, failure probability $\\delta\\in(0, 1)$, error $\\varepsilon > 0$.\n\n    \\Ensure $(\\varepsilon + 8Ln^{-\\alpha}H^2)$-optimal policy $\\pi^{(K)} \\in \\Pi^{\\rm D}$. \n\n    \\State Initialise $u^{(0)}_t \\equiv 0$ for all $t\\in[H]$ and $\\pi^{(0)}\\in\\Pi^{\\rm D}$ arbitrary\n\n    \\For{$k=1$ to $K:=\\lceil\\log_2(H/\\varepsilon)\\rceil$}\n\n        \\State $\\epsilon_k \\gets H/2^k$ and $\\theta_k \\gets \\frac{\\min\\{\\epsilon_k,1\\}}{20 H^{3/2}}$\n    \n%        \\State For each $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, sample $x^{(1)}_{s,a},x^{(2)}_{s,a},\\dots,x^{(m_k)}_{s,a}\\in\\mathcal{X}$\n    \n        \\For{$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$}\n\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use \\Cref{fact:quantum_multidimensional_mean_estimation_covariance_matrix} to get $\\widetilde{\\sigma}^{(k)}_t(s,a)$ such that $\\big|\\widetilde{\\sigma}^{(k)}_t(s,a) - \\sigma^{(k)}_t(s,a)\\big| \\leq \\theta_kH^2$, where $\\sigma^{(k)}_t(s,a) := \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t^{(k-1)}(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t^{(k-1)}(x')\\big)^2$ \\label{line:quantum_backward_line1}}\n    \n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use \\Cref{fact:quantum_multidimensional_mean_estimation_covariance_matrix} to get $\\widetilde{\\mu}^{(k)}_t(s,a)$ such that $\\big| \\widetilde{\\mu}^{(k)}_t(s,a) - \\mu_t^{(k)}(s,a)\\big| \\leq \\theta_k \\sqrt{\\frac{1}{H}\\sum_{t'=1}^H \\sigma_{t'}^{(k)}(s,a)}$, where $\\mu_t^{(k)}(s,a) := \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t^{(k-1)}(x')$ \\label{line:quantum_backward_line2}}\n    \n        \\EndFor\n\n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent}{$\\widehat{\\mu}_t^{(k)}(x,a) \\gets \\widetilde{\\mu}_t^{(k)}(s,a) - \\theta_k\\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\widetilde{\\sigma}_{t'}^{(k)}(s,a)} - (\\theta_k^{3/2} + Ln^{-\\alpha})H$ for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$, $x\\in\\mathcal{X}(s)$}\n\n        \\State $u^{(k)}_H(x) \\gets u^{(k-1)}_{H}(x)$ for all $x\\in\\mathcal{X}$ and $\\pi_H^{(k)}\\gets \\pi_{H}^{(k-1)}$\n\n        \\For{$t=H-1,H-2,\\dots,1$}\n\n            \\For{$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$}\n\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent-\\algorithmicindent}{Use \\Cref{lem:quantum_mean_estimation} to get $\\widetilde{\\beta}_{t+1}^{(k)}\\!(s,a)$ such that $\\big|\\widetilde{\\beta}_{t+1}^{(k)}\\!(s,a) - \\beta^{(k)}_{t+1}\\!(s,a) \\big| \\!\\leq\\! \\frac{\\epsilon_k}{4H} - Ln^{-\\alpha}H$, where $\\beta^{(k)}_{t+1}(s,a) := \\int_{\\mathcal{X}}p(\\rd x'|s,a)\\big(u^{(k)}_{t+1}(x') -  u^{(k-1)}_{t+1}(x')\\big)$ \\label{line:quantum_backward_line3}}\n\n            \\State $\\widehat{\\beta}^{(k)}_{t+1}(s,a) \\gets \\widetilde{\\beta}_{t+1}^{(k)}(s,a) - \\frac{\\epsilon_k}{4H} - Ln^{-\\alpha}H$\n\n            \\EndFor\n\n            \\For{$s\\in\\mathcal{S}_n$}\n\n            \\State $u^{(k)}_t(s) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\widehat{\\mu}_{t+1}^{(k)}(s,a) + \\widehat{\\beta}_{t+1}^{(k)}(s,a)\\} - Ln^{-\\alpha}$\n            \n            \\State $\\pi_t^{(k)}(s) \\gets \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\widehat{\\mu}_{t+1}^{(k)}(s,a) + \\widehat{\\beta}_{t+1}^{(k)}(s,a)\\}$\n\n            \\State If $u^{(k)}_t(s) \\leq u^{(k-1)}_{t}(s)$, then $u^{(k)}_t(x) \\!\\gets\\! u^{(k-1)}_{t}(x)$ and $\\pi_t^{(k)}(x) \\!\\gets\\! \\pi_{t}^{(k-1)}(x)$  $\\forall x\\in\\mathcal{X}(s)$\n            \n            \\EndFor\n\n        \\EndFor\n\n    \\EndFor\n    \n    \\State \\Return $\\pi^{(K)}\\in\\Pi^{\\rm D}$\n\\end{algorithmic}\n\\end{algorithm}[t!]\n    \\caption{Modern quantum backward induction algorithm}\n    \\label{algo:quantum_backward_recursion}\n    [1]  \n    \\Require Compact state space $\\mathcal{X}$\\mathcal{X} with $\\frac{1}{n}$\\frac{1}{n}-net $\\mathcal{S}_n$\\mathcal{S}_n,  finite action space $\\mathcal{A}$\\mathcal{A}, horizon $H$H, quantum sampling access to probability kernels $p$p, failure probability $\\delta\\in(0, 1)$\\delta\\in(0, 1), error $\\varepsilon > 0$\\varepsilon > 0.\n\n    \\Ensure $(\\varepsilon + 8Ln^{-\\alpha}H^2)$(\\varepsilon + 8Ln^{-\\alpha}-\\alphaH^2)-optimal policy $\\pi^{(K)} \\in \\Pi^{\\rm D}$\\pi^{(K)}(K) \\in \\Pi^{\\rm D}\\rm D. \n\n    \\State Initialise $u^{(0)}_t \\equiv 0$u^{(0)}(0)_t \\equiv 0 for all $t\\in[H]$t\\in[H] and $\\pi^{(0)}\\in\\Pi^{\\rm D}$\\pi^{(0)}(0)\\in\\Pi^{\\rm D}\\rm D arbitrary\n\n    \\For{$k=1$ to $K:=\\lceil\\log_2(H/\\varepsilon)\\rceil$}$k=1$k=1 to $K:=\\lceil\\log_2(H/\\varepsilon)\\rceil$K:=\\lceil\\log_2(H/\\varepsilon)\\rceil\n\n        \\State $\\epsilon_k \\gets H/2^k$\\epsilon_k \\gets H/2^k and $\\theta_k \\gets \\frac{\\min\\{\\epsilon_k,1\\}}{20 H^{3/2}}$\\theta_k \\gets \\frac{\\min\\{\\epsilon_k,1\\}}{20 H^{3/2}}\n    \n\\For{$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$}$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]\n\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent{Use \\Cref{fact:quantum_multidimensional_mean_estimation_covariance_matrix} to get $\\widetilde{\\sigma}^{(k)}_t(s,a)$ such that $\\big|\\widetilde{\\sigma}^{(k)}_t(s,a) - \\sigma^{(k)}_t(s,a)\\big| \\leq \\theta_kH^2$, where $\\sigma^{(k)}_t(s,a) := \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t^{(k-1)}(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t^{(k-1)}(x')\\big)^2$ \\label{line:quantum_backward_line1}}Use \\Cref{fact:quantum_multidimensional_mean_estimation_covariance_matrix} to get $\\widetilde{\\sigma}^{(k)}_t(s,a)$\\widetilde{\\sigma}^{(k)}(k)_t(s,a) such that $\\big|\\widetilde{\\sigma}^{(k)}_t(s,a) - \\sigma^{(k)}_t(s,a)\\big| \\leq \\theta_kH^2$\\big|\\widetilde{\\sigma}^{(k)}(k)_t(s,a) - \\sigma^{(k)}(k)_t(s,a)\\big| \\leq \\theta_kH^2, where $\\sigma^{(k)}_t(s,a) := \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t^{(k-1)}(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t^{(k-1)}(x')\\big)^2$\\sigma^{(k)}(k)_t(s,a) := \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|s,a)u_t^{(k-1)}(k-1)(x')^2 - \\big(\\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|s,a)u_t^{(k-1)}(k-1)(x')\\big)^2 \\label{line:quantum_backward_line1}\n    \n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent{Use \\Cref{fact:quantum_multidimensional_mean_estimation_covariance_matrix} to get $\\widetilde{\\mu}^{(k)}_t(s,a)$ such that $\\big| \\widetilde{\\mu}^{(k)}_t(s,a) - \\mu_t^{(k)}(s,a)\\big| \\leq \\theta_k \\sqrt{\\frac{1}{H}\\sum_{t'=1}^H \\sigma_{t'}^{(k)}(s,a)}$, where $\\mu_t^{(k)}(s,a) := \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t^{(k-1)}(x')$ \\label{line:quantum_backward_line2}}Use \\Cref{fact:quantum_multidimensional_mean_estimation_covariance_matrix} to get $\\widetilde{\\mu}^{(k)}_t(s,a)$\\widetilde{\\mu}^{(k)}(k)_t(s,a) such that $\\big| \\widetilde{\\mu}^{(k)}_t(s,a) - \\mu_t^{(k)}(s,a)\\big| \\leq \\theta_k \\sqrt{\\frac{1}{H}\\sum_{t'=1}^H \\sigma_{t'}^{(k)}(s,a)}$\\big| \\widetilde{\\mu}^{(k)}(k)_t(s,a) - \\mu_t^{(k)}(k)(s,a)\\big| \\leq \\theta_k \\sqrt{\\frac{1}{H}\\sum_{t'=1}^H \\sigma_{t'}^{(k)}(s,a)}, where $\\mu_t^{(k)}(s,a) := \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t^{(k-1)}(x')$\\mu_t^{(k)}(k)(s,a) := \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|s,a)u_t^{(k-1)}(k-1)(x') \\label{line:quantum_backward_line2}\n    \n        \\EndFor\n\n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent}\\dimexpr\\linewidth-\\algorithmicindent{$\\widehat{\\mu}_t^{(k)}(x,a) \\gets \\widetilde{\\mu}_t^{(k)}(s,a) - \\theta_k\\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\widetilde{\\sigma}_{t'}^{(k)}(s,a)} - (\\theta_k^{3/2} + Ln^{-\\alpha})H$ for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$, $x\\in\\mathcal{X}(s)$}$\\widehat{\\mu}_t^{(k)}(x,a) \\gets \\widetilde{\\mu}_t^{(k)}(s,a) - \\theta_k\\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\widetilde{\\sigma}_{t'}^{(k)}(s,a)} - (\\theta_k^{3/2} + Ln^{-\\alpha})H$\\widehat{\\mu}_t^{(k)}(k)(x,a) \\gets \\widetilde{\\mu}_t^{(k)}(k)(s,a) - \\theta_k\\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\widetilde{\\sigma}_{t'}^{(k)}(s,a)} - (\\theta_k^{3/2}3/2 + Ln^{-\\alpha}-\\alpha)H for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H], $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s)\n\n        \\State $u^{(k)}_H(x) \\gets u^{(k-1)}_{H}(x)$u^{(k)}(k)_H(x) \\gets u^{(k-1)}(k-1)_{H}H(x) for all $x\\in\\mathcal{X}$x\\in\\mathcal{X} and $\\pi_H^{(k)}\\gets \\pi_{H}^{(k-1)}$\\pi_H^{(k)}(k)\\gets \\pi_{H}H^{(k-1)}(k-1)\n\n        \\For{$t=H-1,H-2,\\dots,1$}$t=H-1,H-2,\\dots,1$t=H-1,H-2,\\dots,1\n\n            \\For{$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$}$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}\n\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent-\\algorithmicindent}\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent-\\algorithmicindent{Use \\Cref{lem:quantum_mean_estimation} to get $\\widetilde{\\beta}_{t+1}^{(k)}\\!(s,a)$ such that $\\big|\\widetilde{\\beta}_{t+1}^{(k)}\\!(s,a) - \\beta^{(k)}_{t+1}\\!(s,a) \\big| \\!\\leq\\! \\frac{\\epsilon_k}{4H} - Ln^{-\\alpha}H$, where $\\beta^{(k)}_{t+1}(s,a) := \\int_{\\mathcal{X}}p(\\rd x'|s,a)\\big(u^{(k)}_{t+1}(x') -  u^{(k-1)}_{t+1}(x')\\big)$ \\label{line:quantum_backward_line3}}Use \\Cref{lem:quantum_mean_estimation} to get $\\widetilde{\\beta}_{t+1}^{(k)}\\!(s,a)$\\widetilde{\\beta}_{t+1}t+1^{(k)}(k)\\!(s,a) such that $\\big|\\widetilde{\\beta}_{t+1}^{(k)}\\!(s,a) - \\beta^{(k)}_{t+1}\\!(s,a) \\big| \\!\\leq\\! \\frac{\\epsilon_k}{4H} - Ln^{-\\alpha}H$\\big|\\widetilde{\\beta}_{t+1}t+1^{(k)}(k)\\!(s,a) - \\beta^{(k)}(k)_{t+1}t+1\\!(s,a) \\big| \\!\\leq\\! \\frac{\\epsilon_k}{4H} - Ln^{-\\alpha}-\\alphaH, where $\\beta^{(k)}_{t+1}(s,a) := \\int_{\\mathcal{X}}p(\\rd x'|s,a)\\big(u^{(k)}_{t+1}(x') -  u^{(k-1)}_{t+1}(x')\\big)$\\beta^{(k)}(k)_{t+1}t+1(s,a) := \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|s,a)\\big(u^{(k)}(k)_{t+1}t+1(x') -  u^{(k-1)}(k-1)_{t+1}t+1(x')\\big) \\label{line:quantum_backward_line3}\n\n            \\State $\\widehat{\\beta}^{(k)}_{t+1}(s,a) \\gets \\widetilde{\\beta}_{t+1}^{(k)}(s,a) - \\frac{\\epsilon_k}{4H} - Ln^{-\\alpha}H$\\widehat{\\beta}^{(k)}(k)_{t+1}t+1(s,a) \\gets \\widetilde{\\beta}_{t+1}t+1^{(k)}(k)(s,a) - \\frac{\\epsilon_k}{4H} - Ln^{-\\alpha}-\\alphaH\n\n            \\EndFor\n\n            \\For{$s\\in\\mathcal{S}_n$}$s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n\n\n            \\State $u^{(k)}_t(s) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\widehat{\\mu}_{t+1}^{(k)}(s,a) + \\widehat{\\beta}_{t+1}^{(k)}(s,a)\\} - Ln^{-\\alpha}$u^{(k)}(k)_t(s) \\gets \\max_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{r(s,a) + \\widehat{\\mu}_{t+1}t+1^{(k)}(k)(s,a) + \\widehat{\\beta}_{t+1}t+1^{(k)}(k)(s,a)\\} - Ln^{-\\alpha}-\\alpha\n            \n            \\State $\\pi_t^{(k)}(s) \\gets \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\widehat{\\mu}_{t+1}^{(k)}(s,a) + \\widehat{\\beta}_{t+1}^{(k)}(s,a)\\}$\\pi_t^{(k)}(k)(s) \\gets \\argmax_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{r(s,a) + \\widehat{\\mu}_{t+1}t+1^{(k)}(k)(s,a) + \\widehat{\\beta}_{t+1}t+1^{(k)}(k)(s,a)\\}\n\n            \\State If $u^{(k)}_t(s) \\leq u^{(k-1)}_{t}(s)$u^{(k)}(k)_t(s) \\leq u^{(k-1)}(k-1)_{t}t(s), then $u^{(k)}_t(x) \\!\\gets\\! u^{(k-1)}_{t}(x)$u^{(k)}(k)_t(x) \\!\\gets\\! u^{(k-1)}(k-1)_{t}t(x) and $\\pi_t^{(k)}(x) \\!\\gets\\! \\pi_{t}^{(k-1)}(x)$\\pi_t^{(k)}(k)(x) \\!\\gets\\! \\pi_{t}t^{(k-1)}(k-1)(x)  $\\forall x\\in\\mathcal{X}(s)$\\forall x\\in\\mathcal{X}(s)\n            \n            \\EndFor\n\n        \\EndFor\n\n    \\EndFor\n    \n    \\State \\Return $\\pi^{(K)}\\in\\Pi^{\\rm D}$\\pi^{(K)}(K)\\in\\Pi^{\\rm D}\\rm D\n\n\n\n\\begin{theorem}[Modern quantum finite-horizon generative algorithm]\\label{thr:quantum_finite-horizon}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},H, p, r\\rangle$ be a finite-horizon MDP and let $\\mathcal{S}_n$ be a $\\frac{1}{n}$-net for $\\mathcal{X}$. Let $K := \\lceil\\log_2(H/\\varepsilon)\\rceil$ for a given $\\varepsilon > 0$ and $\\epsilon_k := H/2^k$ for all $k\\in[K]$. Under {\\rm \\Cref{ass:Lipschitz}} with $Ln^{-\\alpha} \\leq \\frac{1}{16H}$, {\\rm \\Cref{algo:quantum_backward_recursion}} computes functions $\\{u^{(k)}_t\\in\\mathscr{B}(\\mathcal{X})\\}_{t\\in[H],k\\in[K]}$ and policies $\\{\\pi^{(k)}\\in\\Pi^{\\rm D}\\}_{k\\in[K]}$ such that, with probability $1-\\delta$,\n    %\n    \\begin{align*}\n        V^\\ast_t(x) - \\epsilon_k - 8 Ln^{-\\alpha}H^2 \\leq u^{(k)}_t(x) \\leq V^{\\pi^{(k)}}_t(x) \\leq  V^\\ast_t(x) \\qquad\\forall (x,t,k)\\in\\mathcal{X}\\times[H]\\times[K].\n    \\end{align*}\n    %\n    In particular, $\\pi^{(K)} \\in \\Pi^{\\rm D}$ is such that $V^\\ast_1(x) - \\varepsilon - 8 Ln^{-\\alpha}H^2 \\leq V^{\\pi^{(K)}}_1(x) \\leq  V^\\ast_1(x)$ for all $x\\in\\mathcal{X}$. The $\\mathcal{O}_p,\\mathcal{O}_p^\\dagger$-query complexity is \n    %\n    \\begin{align*}\n        O\\left(\\frac{H^{2}|\\mathcal{S}_n|A}{\\varepsilon}\\log\\left(\\frac{H|\\mathcal{S}_n|A}{\\delta}\\log\\frac{H}{\\varepsilon} \\right) \\right).\n    \\end{align*}\n\\end{theorem}\\begin{theorem}[Modern quantum finite-horizon generative algorithm]\\label{thr:quantum_finite-horizon}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},H, p, r\\rangle$ be a finite-horizon MDP and let $\\mathcal{S}_n$ be a $\\frac{1}{n}$-net for $\\mathcal{X}$. Let $K := \\lceil\\log_2(H/\\varepsilon)\\rceil$ for a given $\\varepsilon > 0$ and $\\epsilon_k := H/2^k$ for all $k\\in[K]$. Under {\\rm \\Cref{ass:Lipschitz}} with $Ln^{-\\alpha} \\leq \\frac{1}{16H}$, {\\rm \\Cref{algo:quantum_backward_recursion}} computes functions $\\{u^{(k)}_t\\in\\mathscr{B}(\\mathcal{X})\\}_{t\\in[H],k\\in[K]}$ and policies $\\{\\pi^{(k)}\\in\\Pi^{\\rm D}\\}_{k\\in[K]}$ such that, with probability $1-\\delta$,\n    %\n    \\begin{align*}\n        V^\\ast_t(x) - \\epsilon_k - 8 Ln^{-\\alpha}H^2 \\leq u^{(k)}_t(x) \\leq V^{\\pi^{(k)}}_t(x) \\leq  V^\\ast_t(x) \\qquad\\forall (x,t,k)\\in\\mathcal{X}\\times[H]\\times[K].\n    \\end{align*}\n    %\n    In particular, $\\pi^{(K)} \\in \\Pi^{\\rm D}$ is such that $V^\\ast_1(x) - \\varepsilon - 8 Ln^{-\\alpha}H^2 \\leq V^{\\pi^{(K)}}_1(x) \\leq  V^\\ast_1(x)$ for all $x\\in\\mathcal{X}$. The $\\mathcal{O}_p,\\mathcal{O}_p^\\dagger$-query complexity is \n    %\n    \\begin{align*}\n        O\\left(\\frac{H^{2}|\\mathcal{S}_n|A}{\\varepsilon}\\log\\left(\\frac{H|\\mathcal{S}_n|A}{\\delta}\\log\\frac{H}{\\varepsilon} \\right) \\right).\n    \\end{align*}\n\\end{theorem}\\label{thr:quantum_finite-horizon}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},H, p, r\\rangle$M = \\langle \\mathcal{X},\\mathcal{A},H, p, r\\rangle be a finite-horizon MDP and let $\\mathcal{S}_n$\\mathcal{S}_n be a $\\frac{1}{n}$\\frac{1}{n}-net for $\\mathcal{X}$\\mathcal{X}. Let $K := \\lceil\\log_2(H/\\varepsilon)\\rceil$K := \\lceil\\log_2(H/\\varepsilon)\\rceil for a given $\\varepsilon > 0$\\varepsilon > 0 and $\\epsilon_k := H/2^k$\\epsilon_k := H/2^k for all $k\\in[K]$k\\in[K]. Under {\\rm \\Cref{ass:Lipschitz}}\\rm \\Cref{ass:Lipschitz} with $Ln^{-\\alpha} \\leq \\frac{1}{16H}$Ln^{-\\alpha}-\\alpha \\leq \\frac{1}{16H}, {\\rm \\Cref{algo:quantum_backward_recursion}}\\rm \\Cref{algo:quantum_backward_recursion} computes functions $\\{u^{(k)}_t\\in\\mathscr{B}(\\mathcal{X})\\}_{t\\in[H],k\\in[K]}$\\{u^{(k)}(k)_t\\in\\mathscr{B}(\\mathcal{X})\\}_{t\\in[H],k\\in[K]}t\\in[H],k\\in[K] and policies $\\{\\pi^{(k)}\\in\\Pi^{\\rm D}\\}_{k\\in[K]}$\\{\\pi^{(k)}(k)\\in\\Pi^{\\rm D}\\rm D\\}_{k\\in[K]}k\\in[K] such that, with probability $1-\\delta$1-\\delta,\n    \n        V^\\ast_t(x) - \\epsilon_k - 8 Ln^{-\\alpha}-\\alphaH^2 \\leq u^{(k)}(k)_t(x) \\leq V^{\\pi^{(k)}}\\pi^{(k)}(k)_t(x) \\leq  V^\\ast_t(x) \\qquad\\forall (x,t,k)\\in\\mathcal{X}\\times[H]\\times[K].\n    \n    In particular, $\\pi^{(K)} \\in \\Pi^{\\rm D}$\\pi^{(K)}(K) \\in \\Pi^{\\rm D}\\rm D is such that $V^\\ast_1(x) - \\varepsilon - 8 Ln^{-\\alpha}H^2 \\leq V^{\\pi^{(K)}}_1(x) \\leq  V^\\ast_1(x)$V^\\ast_1(x) - \\varepsilon - 8 Ln^{-\\alpha}-\\alphaH^2 \\leq V^{\\pi^{(K)}}\\pi^{(K)}(K)_1(x) \\leq  V^\\ast_1(x) for all $x\\in\\mathcal{X}$x\\in\\mathcal{X}. The $\\mathcal{O}_p,\\mathcal{O}_p^\\dagger$\\mathcal{O}_p,\\mathcal{O}_p^\\dagger-query complexity is \n    \n        O\\left(\\frac{H^{2}|\\mathcal{S}_n|A}{\\varepsilon}\\log\\left(\\frac{H|\\mathcal{S}_n|A}{\\delta}\\log\\frac{H}{\\varepsilon} \\right) \\right).\n    \n\n\\begin{proof}\n    Again the proof is by induction on the epoch $k=0,1,\\dots,K$, the base case $k=0$ being trivial. Assume then that \\Cref{algo:quantum_backward_recursion} has properly computed functions and policies such that\n    %\n    \\begin{align}\\label{eq:quantum_backward_eq0}\n        V^\\ast_t(x) - \\epsilon_{k'} - 8Ln^{-\\alpha}H^b \\leq u^{(k')}_{t}(x) \\leq (\\mathcal{L}_{\\pi^{(k')}_{t}}u^{(k')}_{t+1})(x) \\leq  V^\\ast_t(x) \\qquad \\forall k'\\in[k-1], t\\in[H],\n    \\end{align}\n    %\n    and consider epoch $k\\in[K]$. Let $\\theta_k := \\frac{1}{20 H^{3/2}}\\min\\{\\epsilon_k, 1\\}$. We start the analysis with the quantities $\\widetilde{\\mu}_t^{(k)}$ and $\\widetilde{\\sigma}_t^{(k)}$ from \\Cref{line:quantum_backward_line1,line:quantum_backward_line2}. Define once again the true quantities $\\mu^{(k)}_t(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_{t}^{(k-1)}(x')$ and $\\sigma_t^{(k)}(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_{t}^{(k-1)}(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)u_{t}^{(k-1)}(x')\\big)^2$. For all $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, we employ the quantum multivariate mean estimation from \\Cref{fact:quantum_multidimensional_mean_estimation_covariance_matrix} with\n    %\n    \\begin{align*}\n        m_k := O\\left(\\frac{\\sqrt{H}}{\\theta_k}\\log\\frac{H|\\mathcal{S}_n|AK}{\\delta}\\right) = O\\left(\\frac{H^{2}}{\\min\\{\\epsilon_k,1\\}}\\log\\frac{H|\\mathcal{S}_n|AK}{\\delta}\\right)\n    \\end{align*}\n    %\n    queries to $\\mathcal{O}_p$ to obtain $(\\widetilde{\\sigma}_1^{(k)}(s,a), \\dots, \\widetilde{\\sigma}_H^{(k)}(s,a))\\in\\mathbb{R}^H$ (the vector $(\\sigma_1^{(k)}(s,a),\\dots,\\sigma_H^{(k)}(s,a))$ has standard deviation at most $H^{5/2}$). Likewise, for all $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, we employ the quantum mean estimation subroutine from \\Cref{fact:quantum_multidimensional_mean_estimation_covariance_matrix} with\n    %\n    \\begin{align*}\n        n_k := O\\left(\\frac{\\sqrt{H}}{\\theta_k}\\log\\frac{H|\\mathcal{S}_n|AK}{\\delta}\\right) = O\\left(\\frac{H^{2}}{\\min\\{\\epsilon_k,1\\}}\\log\\frac{H|\\mathcal{S}_n|AK}{\\delta}\\right)\n    \\end{align*}\n    %\n    queries $\\mathcal{O}_p$ to obtain $(\\widetilde{\\mu}_1^{(k)}(s,a),\\dots,\\widetilde{\\mu}_H^{(k)}(s,a))\\in\\mathbb{R}^H$. These quantities are such that, with probability $1 - \\frac{\\delta}{2K}$, for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$,\n    %\n    \\begin{subequations}\n    \\begin{align}\n        |\\widetilde{\\mu}^{(k)}_t(s,a) - \\mu^{(k)}_t(s,a)| &\\leq \\theta_k\\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\sigma_{t'}^{(k)}(s,a)}, \\label{eq:quantum_inequality_1}\\\\\n        |\\widetilde{\\sigma}_t^{(k)}(s,a) - \\sigma_t^{(k)}(s,a)| &\\leq \\theta_k H^2, \\label{eq:quantum_inequality_2}\n    \\end{align}\n    \\end{subequations}\n    %\n    similarly to the classical case, and which we condition on. By using \\Cref{eq:quantum_inequality_2} onto \\Cref{eq:quantum_inequality_1}, then\n    %\n    \\begin{align*}\n        |\\widetilde{\\mu}^{(k)}_t(s,a) - \\mu^{(k)}_t(s,a)| \\leq \\theta_k \\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\widetilde{\\sigma}_{t'}^{(k)}(s,a)} + \\theta_k^{3/2} H,\n    \\end{align*}\n    %\n    from which we define $\\widetilde{\\mu}_t^{(k)}(x,a)$, for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, $a\\in\\mathcal{A}$, as\n    %\n    \\begin{align*}\n        \\widehat{\\mu}^{(k)}_t(x,a) := \\widetilde{\\mu}^{(k)}_t(s,a) - \\theta_k\\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\widetilde{\\sigma}_{t'}^{(k)}(s,a)} - \\theta_k^{3/2} H - Ln^{-\\alpha}H,\n    \\end{align*}\n    %\n    which has one-sided error. We can express the above quantity using the variance $\\sigma_t^\\ast(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)V_t^\\ast(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)V_t^\\ast(x')\\big)^2$, since, for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, and $a\\in\\mathcal{A}$,\n    %\n    \\begin{align*}\n        \\sqrt{\\widetilde{\\sigma}_t^{(k)}(s,a)} &\\leq \\sqrt{\\sigma_t^{(k)}(s,a)} + \\sqrt{\\theta_k}H \\tag{by \\Cref{eq:quantum_inequality_2}}\\\\\n        &\\leq \\sqrt{\\sigma_t^{(k)}(x,a)} + \\sqrt{\\theta_k} H + 2\\sqrt{Ln^{-\\alpha}} H \\tag{by $|\\sigma_t^{(k)}(x,a) - \\sigma_t^{(k)}(s,a)| \\leq 4Ln^{-\\alpha} H^2$}\\\\\n        &\\leq \\sqrt{\\sigma^\\ast_t(x,a)} + 2\\epsilon_k + 8Ln^{-\\alpha}H^2 + \\sqrt{\\theta_k}H + 2\\sqrt{Ln^{-\\alpha}} H\n    \\end{align*}\n    %\n    using that $\\operatorname{Var}[u^{(k-1)}_{t}] \\leq \\operatorname{Var}[V^\\ast_t] + \\operatorname{Var}[V^\\ast_t - u^{(k-1)}_{t}]$ and $\\operatorname{Var}[V^\\ast_t - u^{(k-1)}_{t}] \\leq (2\\epsilon_{k} + 8Ln^{-\\alpha}H^2)^2$ if $\\|V^\\ast_t - u^{(k-1)}_{t}\\|_\\infty \\leq 2\\epsilon_{k} + 8Ln^{-\\alpha}H^2$ according to the induction hypothesis. This means that, for all $(x,a,t)\\in\\mathcal{X}\\times\\mathcal{A}\\times[H]$, with probability at least $1-\\frac{\\delta}{2K}$,\n    %\n    \\begin{subequations}\\label{eq:quantum_backward_eq1}\n    \\begin{align}\n        \\widehat{\\mu}^{(k)}_t(x,a) &\\!\\leq\\! \\mu_t^{(k)}(x,a), \\label{eq:quantum_backward_eq1a} \\\\\n            \\widehat{\\mu}^{(k)}_t(x,a) &\\!\\geq\\! \\mu_t^{(k)}(x,a) - 2\\theta_k\\sqrt{\\!\\frac{1}{H}\\!\\sum_{t'=1}^H \\!\\sigma_{t'}^\\ast(x,a)} - 2\\theta_k(2\\epsilon_{k} + 8Ln^{-\\alpha}H^2) - 6\\theta_k^{3/2} H - 4Ln^{-\\alpha}H . \\label{eq:quantum_backward_eq1b}\n    \\end{align}\n    \\end{subequations}\n\n    We now proceed to the backward iteration that happens within epoch $k\\in[K]$. Proving that $u_t^{(k)}(x) \\leq V_t^{\\pi^{(k)}}(x) \\leq V^\\ast_t(x)$ is very similar to \\Cref{thr:classical_finite-horizon_sampling}. Assume by induction that\n    %\n    \\begin{align*}\n        u_{t'}^{(k)}(x) \\leq (\\mathcal{L}_{\\pi^{(k)}_{t'}}u^{(k)}_{t'+1})(x) \\leq V_{t'}^{\\pi^{(k)}}(x) \\leq V^\\ast_{t'}(x) \\qquad\\forall x\\in\\mathcal{X}, t'=t+1,t+2,\\dots,H+1.\n    \\end{align*}\n    %\n    Once $u_{t'+1}^{(k)}\\in\\mathscr{B}(\\mathcal{X})$ has been computed for all $t'=t,t+1,\\dots, H-1$, we can compute the quantities $\\widetilde{\\beta}_{t+1}^{(k)}$ from \\Cref{line:quantum_backward_line3} at time step $t\\in[H]$. For all $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, we employ the quantum mean estimation subroutine from \\Cref{lem:quantum_mean_estimation} with $\\ell_k := O\\big(H\\log\\frac{H|\\mathcal{S}_n|AK}{\\delta}\\big)$ queries to $\\mathcal{O}_p$ to obtain $\\widetilde{\\beta}_{t+1}^{(k)}(s,a)$ such that, with probability $1-\\frac{\\delta}{2HK}$, for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, $a\\in\\mathcal{A}$,\n    %\n    \\begin{align*}\n       \\left| \\widetilde{\\beta}_{t+1}^{(k)}(s,a) - \\int_{\\mathcal{X}}p(\\rd x'|x,a)\\big(u^{(k)}_{t+1}(x') - u^{(k-1)}_{t+1}(x')\\big) \\right| \\leq \\left(\\frac{1}{16H} + Ln^{-\\alpha}\\right)(2\\epsilon_k + 8Ln^{-\\alpha}H^2)\n    \\end{align*}\n    %\n    (using that $\\|u^{(k)}_{t+1} - u^{(k-1)}_{t+1}\\|_\\infty \\leq 2\\epsilon_k + 8Ln^{-\\alpha}H^2$ due to $u^{(k-1)}_{t+1}(x) \\leq u^{(k)}_{t+1}(x)$ and the induction hypothesis both in $k$ and $t$).\n    Using that $\\big(\\frac{1}{16H} + Ln^{-\\alpha}\\big)(2\\epsilon_{k} + 8Ln^{-\\alpha}H^2) \\leq \\frac{\\epsilon_{k}}{4H} + Ln^{-\\alpha}H$ since $Ln^{-\\alpha} \\leq \\frac{1}{16H}$, we define, for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, $a\\in\\mathcal{A}$,\n    %\n    \\begin{align*}\n        \\widehat{\\beta}^{(k)}_{t+1}(x,a) := \\widetilde{\\beta}_{t+1}^{(k)}(s,a) - \\frac{\\epsilon_{k}}{4H} - Ln^{-\\alpha}H,\n    \\end{align*}\n    %\n    from which, with probability at least $1-\\frac{\\delta}{2KH}$, for all $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$, it holds that\n    %\n    \\begin{subequations}\\label{eq:quantum_backward_eq2}\n    \\begin{align}\n        \\widehat{\\beta}^{(k)}_{t+1}(x,a) &\\leq \\int_{\\mathcal{X}}  p(\\rd x'|x,a)\\big(u^{(k)}_{t+1}(x') - u^{(k-1)}_{t+1}(x') \\big), \\label{eq:quantum_backward_eq2a}\\\\\n        \\widehat{\\beta}^{(k)}_{t+1}(x,a) &\\geq \\int_{\\mathcal{X}}  p(\\rd x'|x,a)\\big(u^{(k)}_{t+1}(x') - u^{(k-1)}_{t+1}(x') \\big) - \\frac{\\epsilon_{k}}{2H}- 2Ln^{-\\alpha}H. \\label{eq:quantum_backward_eq2b}\n    \\end{align}\n    \\end{subequations}\n    %\n    From here, the proof of $u_t^{(k)}(x) \\leq V_t^{\\pi^{(k)}}(x) \\leq V^\\ast_t(x)$ is the same as \\Cref{thr:classical_finite-horizon_sampling}, so we omit it and move on to proving that $u^{(k)}_t(x) \\geq V^\\ast_t(x) - \\epsilon_k - 8Ln^{-\\alpha}H$ (assume that all $u^{(k)}_{t'+1}$ for $t' = t,\\dots, H-1$ have already been computed by the backward iteration). Again,\n    %\n    \\begin{align*}\n        V^\\ast_t(x) - u^{(k)}_t(x) &\\leq \\max_{a\\in\\mathcal{A}}\\left\\{r(x,a) + \\int_{\\mathcal{X}}p(\\rd x'|x,a)V_{t+1}^\\ast(x') \\right\\} \\\\\n        &~- \\max_{a\\in\\mathcal{A}}\\left\\{r(x,a) - 2Ln^{-\\alpha} + \\widehat{\\mu}_{t+1}^{(k)}(x,a) + \\widehat{\\beta}_{t+1}^{(k)}(x,a) \\right\\} \\tag{$u_t^{(k)}(x) \\!\\geq\\! u_t^{(k)}(s) \\!-\\! Ln^{-\\alpha}$}\\\\\n        &\\leq \\int_{\\mathcal{X}} p(\\rd x'|x,\\pi^{\\ast}_{t}(x))\\big(V_{t+1}^\\ast(x') - u^{(k)}_{t+1}(x')\\big) + \\xi_{t+1}^{(k)}(x), \\tag{by \\Cref{eq:quantum_backward_eq1b,eq:quantum_backward_eq2b}}\n    \\end{align*}\n    %\n    where we defined\n    %\n    \\begin{align*}\n        \\xi_{t+1}^{(k)}(x) := 2\\theta_k\\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\sigma^\\ast_{t'}(x,\\pi^\\ast_{t}(x))} + \\frac{\\epsilon_{k}}{2H} + 2\\theta_k(2\\epsilon_{k} +  8Ln^{-\\alpha}H^2) + 6\\theta_k^{3/2}H + 6Ln^{-\\alpha}H .\n    \\end{align*}\n    %\n    Solving the above recursion, using $\\|\\!\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x)\\cdots p_{\\pi_{t'-1}^\\ast}(\\rd x_{t'}|x_{t'-1})\\|_\\infty \\!\\leq\\! H$, and  \n    %\n    \\begin{align*}\n        &\\left\\|\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x)\\cdots p_{\\pi_{t'}^\\ast}(\\rd x_{t'-1}|x_{t'-1}) \\sqrt{\\frac{1}{H}\\sum_{t''=1}^H \\sigma_{t''}^\\ast(x_{t'},\\pi^\\ast_{t'}(x_{t'}))} \\right\\|_\\infty \\\\\n        &\\leq \\left\\| \\sqrt{\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x)\\cdots p_{\\pi_{t'-1}^\\ast}(\\rd x_{t'}|x_{t'-1})\\sum_{t''=1}^H \\sigma_{t''}^\\ast(x_{t'},\\pi^\\ast_{t'}(x_{t'}))} \\right\\|_\\infty \\leq 2H^{3/2}, \\tag{by Cauchy\u2013Schwarz inequality and \\Cref{fact:upper_bound_variance2}}\n    \\end{align*}\n    %\n    we finally get that\n    %\n    \\begin{align*}\n        V^\\ast_t(x) - u^{(k)}_t(x) &\\leq 4\\theta_k H^{3/2} + \\frac{\\epsilon_{k}}{2} + 2\\theta_k(2\\epsilon_{k} +  8Ln^{-\\alpha}H^2)H + 6\\theta_k^{3/2}H^2 + 6 Ln^{-\\alpha}H^2  \\\\\n        &\\leq \\epsilon_k\\left(\\frac{1}{2} + \\frac{4}{20} + \\frac{4}{20 H^{3/2}} + \\frac{6}{20^{3/2}H^{1/4}} \\right) + \\left(\\frac{2\\cdot 8}{20\\sqrt{H}} + 6\\right)Ln^{-\\alpha}H^2 \\tag{$\\theta_k = \\frac{1}{20 H^{3/2}}\\min\\{\\epsilon_k, 1\\}$}\\\\\n        &\\leq \\epsilon_k + 8Ln^{-\\alpha}H^2,\n    \\end{align*}\n    %\n    This concludes the proof of \\Cref{eq:quantum_backward_eq0} for epoch $k\\in[K]$ and thus for all epochs by induction on $k$.\n\n    Regarding the failure probability, \\Cref{eq:quantum_backward_eq1} holds with probability $1-\\frac{\\delta}{2K}$ for a single epoch, while \\Cref{eq:quantum_backward_eq2} holds with probability $1-\\frac{\\delta}{2HK}$ for a single epoch and time step. Therefore, across all epochs and time steps, the success probability is at least $1-\\delta$, as required. Finally, the total number of samples used is\n    %\n    \\[\n        O\\bigg(\\sum_{k=1}^K (m_k + n_k + H\\ell_k)|\\mathcal{S}_n|A\\bigg) = O\\left(\\frac{H^{2}|\\mathcal{S}_n|A}{\\varepsilon}\\log\\left(\\frac{H|\\mathcal{S}_n|A}{\\delta}\\log\\frac{H}{\\varepsilon}\\right) \\right). \\qedhere\n    \\]\n\\end{proof}\\begin{proof}\n    Again the proof is by induction on the epoch $k=0,1,\\dots,K$, the base case $k=0$ being trivial. Assume then that \\Cref{algo:quantum_backward_recursion} has properly computed functions and policies such that\n    %\n    \\begin{align}\\label{eq:quantum_backward_eq0}\n        V^\\ast_t(x) - \\epsilon_{k'} - 8Ln^{-\\alpha}H^b \\leq u^{(k')}_{t}(x) \\leq (\\mathcal{L}_{\\pi^{(k')}_{t}}u^{(k')}_{t+1})(x) \\leq  V^\\ast_t(x) \\qquad \\forall k'\\in[k-1], t\\in[H],\n    \\end{align}\n    %\n    and consider epoch $k\\in[K]$. Let $\\theta_k := \\frac{1}{20 H^{3/2}}\\min\\{\\epsilon_k, 1\\}$. We start the analysis with the quantities $\\widetilde{\\mu}_t^{(k)}$ and $\\widetilde{\\sigma}_t^{(k)}$ from \\Cref{line:quantum_backward_line1,line:quantum_backward_line2}. Define once again the true quantities $\\mu^{(k)}_t(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_{t}^{(k-1)}(x')$ and $\\sigma_t^{(k)}(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_{t}^{(k-1)}(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)u_{t}^{(k-1)}(x')\\big)^2$. For all $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, we employ the quantum multivariate mean estimation from \\Cref{fact:quantum_multidimensional_mean_estimation_covariance_matrix} with\n    %\n    \\begin{align*}\n        m_k := O\\left(\\frac{\\sqrt{H}}{\\theta_k}\\log\\frac{H|\\mathcal{S}_n|AK}{\\delta}\\right) = O\\left(\\frac{H^{2}}{\\min\\{\\epsilon_k,1\\}}\\log\\frac{H|\\mathcal{S}_n|AK}{\\delta}\\right)\n    \\end{align*}\n    %\n    queries to $\\mathcal{O}_p$ to obtain $(\\widetilde{\\sigma}_1^{(k)}(s,a), \\dots, \\widetilde{\\sigma}_H^{(k)}(s,a))\\in\\mathbb{R}^H$ (the vector $(\\sigma_1^{(k)}(s,a),\\dots,\\sigma_H^{(k)}(s,a))$ has standard deviation at most $H^{5/2}$). Likewise, for all $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, we employ the quantum mean estimation subroutine from \\Cref{fact:quantum_multidimensional_mean_estimation_covariance_matrix} with\n    %\n    \\begin{align*}\n        n_k := O\\left(\\frac{\\sqrt{H}}{\\theta_k}\\log\\frac{H|\\mathcal{S}_n|AK}{\\delta}\\right) = O\\left(\\frac{H^{2}}{\\min\\{\\epsilon_k,1\\}}\\log\\frac{H|\\mathcal{S}_n|AK}{\\delta}\\right)\n    \\end{align*}\n    %\n    queries $\\mathcal{O}_p$ to obtain $(\\widetilde{\\mu}_1^{(k)}(s,a),\\dots,\\widetilde{\\mu}_H^{(k)}(s,a))\\in\\mathbb{R}^H$. These quantities are such that, with probability $1 - \\frac{\\delta}{2K}$, for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$,\n    %\n    \\begin{subequations}\n    \\begin{align}\n        |\\widetilde{\\mu}^{(k)}_t(s,a) - \\mu^{(k)}_t(s,a)| &\\leq \\theta_k\\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\sigma_{t'}^{(k)}(s,a)}, \\label{eq:quantum_inequality_1}\\\\\n        |\\widetilde{\\sigma}_t^{(k)}(s,a) - \\sigma_t^{(k)}(s,a)| &\\leq \\theta_k H^2, \\label{eq:quantum_inequality_2}\n    \\end{align}\n    \\end{subequations}\n    %\n    similarly to the classical case, and which we condition on. By using \\Cref{eq:quantum_inequality_2} onto \\Cref{eq:quantum_inequality_1}, then\n    %\n    \\begin{align*}\n        |\\widetilde{\\mu}^{(k)}_t(s,a) - \\mu^{(k)}_t(s,a)| \\leq \\theta_k \\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\widetilde{\\sigma}_{t'}^{(k)}(s,a)} + \\theta_k^{3/2} H,\n    \\end{align*}\n    %\n    from which we define $\\widetilde{\\mu}_t^{(k)}(x,a)$, for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, $a\\in\\mathcal{A}$, as\n    %\n    \\begin{align*}\n        \\widehat{\\mu}^{(k)}_t(x,a) := \\widetilde{\\mu}^{(k)}_t(s,a) - \\theta_k\\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\widetilde{\\sigma}_{t'}^{(k)}(s,a)} - \\theta_k^{3/2} H - Ln^{-\\alpha}H,\n    \\end{align*}\n    %\n    which has one-sided error. We can express the above quantity using the variance $\\sigma_t^\\ast(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)V_t^\\ast(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)V_t^\\ast(x')\\big)^2$, since, for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, and $a\\in\\mathcal{A}$,\n    %\n    \\begin{align*}\n        \\sqrt{\\widetilde{\\sigma}_t^{(k)}(s,a)} &\\leq \\sqrt{\\sigma_t^{(k)}(s,a)} + \\sqrt{\\theta_k}H \\tag{by \\Cref{eq:quantum_inequality_2}}\\\\\n        &\\leq \\sqrt{\\sigma_t^{(k)}(x,a)} + \\sqrt{\\theta_k} H + 2\\sqrt{Ln^{-\\alpha}} H \\tag{by $|\\sigma_t^{(k)}(x,a) - \\sigma_t^{(k)}(s,a)| \\leq 4Ln^{-\\alpha} H^2$}\\\\\n        &\\leq \\sqrt{\\sigma^\\ast_t(x,a)} + 2\\epsilon_k + 8Ln^{-\\alpha}H^2 + \\sqrt{\\theta_k}H + 2\\sqrt{Ln^{-\\alpha}} H\n    \\end{align*}\n    %\n    using that $\\operatorname{Var}[u^{(k-1)}_{t}] \\leq \\operatorname{Var}[V^\\ast_t] + \\operatorname{Var}[V^\\ast_t - u^{(k-1)}_{t}]$ and $\\operatorname{Var}[V^\\ast_t - u^{(k-1)}_{t}] \\leq (2\\epsilon_{k} + 8Ln^{-\\alpha}H^2)^2$ if $\\|V^\\ast_t - u^{(k-1)}_{t}\\|_\\infty \\leq 2\\epsilon_{k} + 8Ln^{-\\alpha}H^2$ according to the induction hypothesis. This means that, for all $(x,a,t)\\in\\mathcal{X}\\times\\mathcal{A}\\times[H]$, with probability at least $1-\\frac{\\delta}{2K}$,\n    %\n    \\begin{subequations}\\label{eq:quantum_backward_eq1}\n    \\begin{align}\n        \\widehat{\\mu}^{(k)}_t(x,a) &\\!\\leq\\! \\mu_t^{(k)}(x,a), \\label{eq:quantum_backward_eq1a} \\\\\n            \\widehat{\\mu}^{(k)}_t(x,a) &\\!\\geq\\! \\mu_t^{(k)}(x,a) - 2\\theta_k\\sqrt{\\!\\frac{1}{H}\\!\\sum_{t'=1}^H \\!\\sigma_{t'}^\\ast(x,a)} - 2\\theta_k(2\\epsilon_{k} + 8Ln^{-\\alpha}H^2) - 6\\theta_k^{3/2} H - 4Ln^{-\\alpha}H . \\label{eq:quantum_backward_eq1b}\n    \\end{align}\n    \\end{subequations}\n\n    We now proceed to the backward iteration that happens within epoch $k\\in[K]$. Proving that $u_t^{(k)}(x) \\leq V_t^{\\pi^{(k)}}(x) \\leq V^\\ast_t(x)$ is very similar to \\Cref{thr:classical_finite-horizon_sampling}. Assume by induction that\n    %\n    \\begin{align*}\n        u_{t'}^{(k)}(x) \\leq (\\mathcal{L}_{\\pi^{(k)}_{t'}}u^{(k)}_{t'+1})(x) \\leq V_{t'}^{\\pi^{(k)}}(x) \\leq V^\\ast_{t'}(x) \\qquad\\forall x\\in\\mathcal{X}, t'=t+1,t+2,\\dots,H+1.\n    \\end{align*}\n    %\n    Once $u_{t'+1}^{(k)}\\in\\mathscr{B}(\\mathcal{X})$ has been computed for all $t'=t,t+1,\\dots, H-1$, we can compute the quantities $\\widetilde{\\beta}_{t+1}^{(k)}$ from \\Cref{line:quantum_backward_line3} at time step $t\\in[H]$. For all $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, we employ the quantum mean estimation subroutine from \\Cref{lem:quantum_mean_estimation} with $\\ell_k := O\\big(H\\log\\frac{H|\\mathcal{S}_n|AK}{\\delta}\\big)$ queries to $\\mathcal{O}_p$ to obtain $\\widetilde{\\beta}_{t+1}^{(k)}(s,a)$ such that, with probability $1-\\frac{\\delta}{2HK}$, for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, $a\\in\\mathcal{A}$,\n    %\n    \\begin{align*}\n       \\left| \\widetilde{\\beta}_{t+1}^{(k)}(s,a) - \\int_{\\mathcal{X}}p(\\rd x'|x,a)\\big(u^{(k)}_{t+1}(x') - u^{(k-1)}_{t+1}(x')\\big) \\right| \\leq \\left(\\frac{1}{16H} + Ln^{-\\alpha}\\right)(2\\epsilon_k + 8Ln^{-\\alpha}H^2)\n    \\end{align*}\n    %\n    (using that $\\|u^{(k)}_{t+1} - u^{(k-1)}_{t+1}\\|_\\infty \\leq 2\\epsilon_k + 8Ln^{-\\alpha}H^2$ due to $u^{(k-1)}_{t+1}(x) \\leq u^{(k)}_{t+1}(x)$ and the induction hypothesis both in $k$ and $t$).\n    Using that $\\big(\\frac{1}{16H} + Ln^{-\\alpha}\\big)(2\\epsilon_{k} + 8Ln^{-\\alpha}H^2) \\leq \\frac{\\epsilon_{k}}{4H} + Ln^{-\\alpha}H$ since $Ln^{-\\alpha} \\leq \\frac{1}{16H}$, we define, for all $s\\in\\mathcal{S}_n$, $x\\in\\mathcal{X}(s)$, $a\\in\\mathcal{A}$,\n    %\n    \\begin{align*}\n        \\widehat{\\beta}^{(k)}_{t+1}(x,a) := \\widetilde{\\beta}_{t+1}^{(k)}(s,a) - \\frac{\\epsilon_{k}}{4H} - Ln^{-\\alpha}H,\n    \\end{align*}\n    %\n    from which, with probability at least $1-\\frac{\\delta}{2KH}$, for all $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$, it holds that\n    %\n    \\begin{subequations}\\label{eq:quantum_backward_eq2}\n    \\begin{align}\n        \\widehat{\\beta}^{(k)}_{t+1}(x,a) &\\leq \\int_{\\mathcal{X}}  p(\\rd x'|x,a)\\big(u^{(k)}_{t+1}(x') - u^{(k-1)}_{t+1}(x') \\big), \\label{eq:quantum_backward_eq2a}\\\\\n        \\widehat{\\beta}^{(k)}_{t+1}(x,a) &\\geq \\int_{\\mathcal{X}}  p(\\rd x'|x,a)\\big(u^{(k)}_{t+1}(x') - u^{(k-1)}_{t+1}(x') \\big) - \\frac{\\epsilon_{k}}{2H}- 2Ln^{-\\alpha}H. \\label{eq:quantum_backward_eq2b}\n    \\end{align}\n    \\end{subequations}\n    %\n    From here, the proof of $u_t^{(k)}(x) \\leq V_t^{\\pi^{(k)}}(x) \\leq V^\\ast_t(x)$ is the same as \\Cref{thr:classical_finite-horizon_sampling}, so we omit it and move on to proving that $u^{(k)}_t(x) \\geq V^\\ast_t(x) - \\epsilon_k - 8Ln^{-\\alpha}H$ (assume that all $u^{(k)}_{t'+1}$ for $t' = t,\\dots, H-1$ have already been computed by the backward iteration). Again,\n    %\n    \\begin{align*}\n        V^\\ast_t(x) - u^{(k)}_t(x) &\\leq \\max_{a\\in\\mathcal{A}}\\left\\{r(x,a) + \\int_{\\mathcal{X}}p(\\rd x'|x,a)V_{t+1}^\\ast(x') \\right\\} \\\\\n        &~- \\max_{a\\in\\mathcal{A}}\\left\\{r(x,a) - 2Ln^{-\\alpha} + \\widehat{\\mu}_{t+1}^{(k)}(x,a) + \\widehat{\\beta}_{t+1}^{(k)}(x,a) \\right\\} \\tag{$u_t^{(k)}(x) \\!\\geq\\! u_t^{(k)}(s) \\!-\\! Ln^{-\\alpha}$}\\\\\n        &\\leq \\int_{\\mathcal{X}} p(\\rd x'|x,\\pi^{\\ast}_{t}(x))\\big(V_{t+1}^\\ast(x') - u^{(k)}_{t+1}(x')\\big) + \\xi_{t+1}^{(k)}(x), \\tag{by \\Cref{eq:quantum_backward_eq1b,eq:quantum_backward_eq2b}}\n    \\end{align*}\n    %\n    where we defined\n    %\n    \\begin{align*}\n        \\xi_{t+1}^{(k)}(x) := 2\\theta_k\\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\sigma^\\ast_{t'}(x,\\pi^\\ast_{t}(x))} + \\frac{\\epsilon_{k}}{2H} + 2\\theta_k(2\\epsilon_{k} +  8Ln^{-\\alpha}H^2) + 6\\theta_k^{3/2}H + 6Ln^{-\\alpha}H .\n    \\end{align*}\n    %\n    Solving the above recursion, using $\\|\\!\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x)\\cdots p_{\\pi_{t'-1}^\\ast}(\\rd x_{t'}|x_{t'-1})\\|_\\infty \\!\\leq\\! H$, and  \n    %\n    \\begin{align*}\n        &\\left\\|\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x)\\cdots p_{\\pi_{t'}^\\ast}(\\rd x_{t'-1}|x_{t'-1}) \\sqrt{\\frac{1}{H}\\sum_{t''=1}^H \\sigma_{t''}^\\ast(x_{t'},\\pi^\\ast_{t'}(x_{t'}))} \\right\\|_\\infty \\\\\n        &\\leq \\left\\| \\sqrt{\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x)\\cdots p_{\\pi_{t'-1}^\\ast}(\\rd x_{t'}|x_{t'-1})\\sum_{t''=1}^H \\sigma_{t''}^\\ast(x_{t'},\\pi^\\ast_{t'}(x_{t'}))} \\right\\|_\\infty \\leq 2H^{3/2}, \\tag{by Cauchy\u2013Schwarz inequality and \\Cref{fact:upper_bound_variance2}}\n    \\end{align*}\n    %\n    we finally get that\n    %\n    \\begin{align*}\n        V^\\ast_t(x) - u^{(k)}_t(x) &\\leq 4\\theta_k H^{3/2} + \\frac{\\epsilon_{k}}{2} + 2\\theta_k(2\\epsilon_{k} +  8Ln^{-\\alpha}H^2)H + 6\\theta_k^{3/2}H^2 + 6 Ln^{-\\alpha}H^2  \\\\\n        &\\leq \\epsilon_k\\left(\\frac{1}{2} + \\frac{4}{20} + \\frac{4}{20 H^{3/2}} + \\frac{6}{20^{3/2}H^{1/4}} \\right) + \\left(\\frac{2\\cdot 8}{20\\sqrt{H}} + 6\\right)Ln^{-\\alpha}H^2 \\tag{$\\theta_k = \\frac{1}{20 H^{3/2}}\\min\\{\\epsilon_k, 1\\}$}\\\\\n        &\\leq \\epsilon_k + 8Ln^{-\\alpha}H^2,\n    \\end{align*}\n    %\n    This concludes the proof of \\Cref{eq:quantum_backward_eq0} for epoch $k\\in[K]$ and thus for all epochs by induction on $k$.\n\n    Regarding the failure probability, \\Cref{eq:quantum_backward_eq1} holds with probability $1-\\frac{\\delta}{2K}$ for a single epoch, while \\Cref{eq:quantum_backward_eq2} holds with probability $1-\\frac{\\delta}{2HK}$ for a single epoch and time step. Therefore, across all epochs and time steps, the success probability is at least $1-\\delta$, as required. Finally, the total number of samples used is\n    %\n    \\[\n        O\\bigg(\\sum_{k=1}^K (m_k + n_k + H\\ell_k)|\\mathcal{S}_n|A\\bigg) = O\\left(\\frac{H^{2}|\\mathcal{S}_n|A}{\\varepsilon}\\log\\left(\\frac{H|\\mathcal{S}_n|A}{\\delta}\\log\\frac{H}{\\varepsilon}\\right) \\right). \\qedhere\n    \\]\n\\end{proof}\n    Again the proof is by induction on the epoch $k=0,1,\\dots,K$k=0,1,\\dots,K, the base case $k=0$k=0 being trivial. Assume then that \\Cref{algo:quantum_backward_recursion} has properly computed functions and policies such that\n    \\label{eq:quantum_backward_eq0}\n        V^\\ast_t(x) - \\epsilon_{k'}k' - 8Ln^{-\\alpha}-\\alphaH^b \\leq u^{(k')}(k')_{t}t(x) \\leq (\\mathcal{L}_{\\pi^{(k')}_{t}}\\pi^{(k')}(k')_{t}tu^{(k')}(k')_{t+1}t+1)(x) \\leq  V^\\ast_t(x) \\qquad \\forall k'\\in[k-1], t\\in[H],\n    \n    and consider epoch $k\\in[K]$k\\in[K]. Let $\\theta_k := \\frac{1}{20 H^{3/2}}\\min\\{\\epsilon_k, 1\\}$\\theta_k := \\frac{1}{20 H^{3/2}}\\min\\{\\epsilon_k, 1\\}. We start the analysis with the quantities $\\widetilde{\\mu}_t^{(k)}$\\widetilde{\\mu}_t^{(k)}(k) and $\\widetilde{\\sigma}_t^{(k)}$\\widetilde{\\sigma}_t^{(k)}(k) from \\Cref{line:quantum_backward_line1,line:quantum_backward_line2}. Define once again the true quantities $\\mu^{(k)}_t(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_{t}^{(k-1)}(x')$\\mu^{(k)}(k)_t(x,a) := \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)u_{t}t^{(k-1)}(k-1)(x') and $\\sigma_t^{(k)}(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)u_{t}^{(k-1)}(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)u_{t}^{(k-1)}(x')\\big)^2$\\sigma_t^{(k)}(k)(x,a) := \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)u_{t}t^{(k-1)}(k-1)(x')^2 - \\big(\\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)u_{t}t^{(k-1)}(k-1)(x')\\big)^2. For all $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}, we employ the quantum multivariate mean estimation from \\Cref{fact:quantum_multidimensional_mean_estimation_covariance_matrix} with\n    \n        m_k := O\\left(\\frac{\\sqrt{H}}{\\theta_k}\\log\\frac{H|\\mathcal{S}_n|AK}{\\delta}\\right) = O\\left(\\frac{H^{2}}{\\min\\{\\epsilon_k,1\\}}\\log\\frac{H|\\mathcal{S}_n|AK}{\\delta}\\right)\n    \n    queries to $\\mathcal{O}_p$\\mathcal{O}_p to obtain $(\\widetilde{\\sigma}_1^{(k)}(s,a), \\dots, \\widetilde{\\sigma}_H^{(k)}(s,a))\\in\\mathbb{R}^H$(\\widetilde{\\sigma}_1^{(k)}(k)(s,a), \\dots, \\widetilde{\\sigma}_H^{(k)}(k)(s,a))\\in\\mathbb{R}^H (the vector $(\\sigma_1^{(k)}(s,a),\\dots,\\sigma_H^{(k)}(s,a))$(\\sigma_1^{(k)}(k)(s,a),\\dots,\\sigma_H^{(k)}(k)(s,a)) has standard deviation at most $H^{5/2}$H^{5/2}5/2). Likewise, for all $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}, we employ the quantum mean estimation subroutine from \\Cref{fact:quantum_multidimensional_mean_estimation_covariance_matrix} with\n    \n        n_k := O\\left(\\frac{\\sqrt{H}}{\\theta_k}\\log\\frac{H|\\mathcal{S}_n|AK}{\\delta}\\right) = O\\left(\\frac{H^{2}}{\\min\\{\\epsilon_k,1\\}}\\log\\frac{H|\\mathcal{S}_n|AK}{\\delta}\\right)\n    \n    queries $\\mathcal{O}_p$\\mathcal{O}_p to obtain $(\\widetilde{\\mu}_1^{(k)}(s,a),\\dots,\\widetilde{\\mu}_H^{(k)}(s,a))\\in\\mathbb{R}^H$(\\widetilde{\\mu}_1^{(k)}(k)(s,a),\\dots,\\widetilde{\\mu}_H^{(k)}(k)(s,a))\\in\\mathbb{R}^H. These quantities are such that, with probability $1 - \\frac{\\delta}{2K}$1 - \\frac{\\delta}{2K}, for all $(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H]$(s,a,t)\\in\\mathcal{S}_n\\times\\mathcal{A}\\times[H],\n    \n    \n        |\\widetilde{\\mu}^{(k)}(k)_t(s,a) - \\mu^{(k)}(k)_t(s,a)| &\\leq \\theta_k\\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\sigma_{t'}^{(k)}(s,a)}, \\label{eq:quantum_inequality_1}\\\\\n        |\\widetilde{\\sigma}_t^{(k)}(k)(s,a) - \\sigma_t^{(k)}(k)(s,a)| &\\leq \\theta_k H^2, \\label{eq:quantum_inequality_2}\n    \n    \n    similarly to the classical case, and which we condition on. By using \\Cref{eq:quantum_inequality_2} onto \\Cref{eq:quantum_inequality_1}, then\n    \n        |\\widetilde{\\mu}^{(k)}(k)_t(s,a) - \\mu^{(k)}(k)_t(s,a)| \\leq \\theta_k \\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\widetilde{\\sigma}_{t'}^{(k)}(s,a)} + \\theta_k^{3/2}3/2 H,\n    \n    from which we define $\\widetilde{\\mu}_t^{(k)}(x,a)$\\widetilde{\\mu}_t^{(k)}(k)(x,a), for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n, $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s), $a\\in\\mathcal{A}$a\\in\\mathcal{A}, as\n    \n        \\widehat{\\mu}^{(k)}(k)_t(x,a) := \\widetilde{\\mu}^{(k)}(k)_t(s,a) - \\theta_k\\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\widetilde{\\sigma}_{t'}^{(k)}(s,a)} - \\theta_k^{3/2}3/2 H - Ln^{-\\alpha}-\\alphaH,\n    \n    which has one-sided error. We can express the above quantity using the variance $\\sigma_t^\\ast(x,a) := \\int_{\\mathcal{X}}p(\\rd x'|x,a)V_t^\\ast(x')^2 - \\big(\\int_{\\mathcal{X}}p(\\rd x'|x,a)V_t^\\ast(x')\\big)^2$\\sigma_t^\\ast(x,a) := \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)V_t^\\ast(x')^2 - \\big(\\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)V_t^\\ast(x')\\big)^2, since, for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n, $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s), and $a\\in\\mathcal{A}$a\\in\\mathcal{A},\n    \n        \\sqrt{\\widetilde{\\sigma}_t^{(k)}(s,a)} &\\leq \\sqrt{\\sigma_t^{(k)}(s,a)} + \\sqrt{\\theta_k}H \\tag{by \\Cref{eq:quantum_inequality_2}}by \\Cref{eq:quantum_inequality_2}\\\\\n        &\\leq \\sqrt{\\sigma_t^{(k)}(x,a)} + \\sqrt{\\theta_k} H + 2\\sqrt{Ln^{-\\alpha}} H \\tag{by $|\\sigma_t^{(k)}(x,a) - \\sigma_t^{(k)}(s,a)| \\leq 4Ln^{-\\alpha} H^2$}by $|\\sigma_t^{(k)}(x,a) - \\sigma_t^{(k)}(s,a)| \\leq 4Ln^{-\\alpha} H^2$|\\sigma_t^{(k)}(k)(x,a) - \\sigma_t^{(k)}(k)(s,a)| \\leq 4Ln^{-\\alpha}-\\alpha H^2\\\\\n        &\\leq \\sqrt{\\sigma^\\ast_t(x,a)} + 2\\epsilon_k + 8Ln^{-\\alpha}-\\alphaH^2 + \\sqrt{\\theta_k}H + 2\\sqrt{Ln^{-\\alpha}} H\n    \n    using that $\\operatorname{Var}[u^{(k-1)}_{t}] \\leq \\operatorname{Var}[V^\\ast_t] + \\operatorname{Var}[V^\\ast_t - u^{(k-1)}_{t}]$\\operatorname{Var}Var[u^{(k-1)}(k-1)_{t}t] \\leq \\operatorname{Var}Var[V^\\ast_t] + \\operatorname{Var}Var[V^\\ast_t - u^{(k-1)}(k-1)_{t}t] and $\\operatorname{Var}[V^\\ast_t - u^{(k-1)}_{t}] \\leq (2\\epsilon_{k} + 8Ln^{-\\alpha}H^2)^2$\\operatorname{Var}Var[V^\\ast_t - u^{(k-1)}(k-1)_{t}t] \\leq (2\\epsilon_{k}k + 8Ln^{-\\alpha}-\\alphaH^2)^2 if $\\|V^\\ast_t - u^{(k-1)}_{t}\\|_\\infty \\leq 2\\epsilon_{k} + 8Ln^{-\\alpha}H^2$\\|V^\\ast_t - u^{(k-1)}(k-1)_{t}t\\|_\\infty \\leq 2\\epsilon_{k}k + 8Ln^{-\\alpha}-\\alphaH^2 according to the induction hypothesis. This means that, for all $(x,a,t)\\in\\mathcal{X}\\times\\mathcal{A}\\times[H]$(x,a,t)\\in\\mathcal{X}\\times\\mathcal{A}\\times[H], with probability at least $1-\\frac{\\delta}{2K}$1-\\frac{\\delta}{2K},\n    \\label{eq:quantum_backward_eq1}\n    \n        \\widehat{\\mu}^{(k)}(k)_t(x,a) &\\!\\leq\\! \\mu_t^{(k)}(k)(x,a), \\label{eq:quantum_backward_eq1a} \\\\\n            \\widehat{\\mu}^{(k)}(k)_t(x,a) &\\!\\geq\\! \\mu_t^{(k)}(k)(x,a) - 2\\theta_k\\sqrt{\\!\\frac{1}{H}\\!\\sum_{t'=1}^H \\!\\sigma_{t'}^\\ast(x,a)} - 2\\theta_k(2\\epsilon_{k}k + 8Ln^{-\\alpha}-\\alphaH^2) - 6\\theta_k^{3/2}3/2 H - 4Ln^{-\\alpha}-\\alphaH . \\label{eq:quantum_backward_eq1b}\n    \n    \n\n    We now proceed to the backward iteration that happens within epoch $k\\in[K]$k\\in[K]. Proving that $u_t^{(k)}(x) \\leq V_t^{\\pi^{(k)}}(x) \\leq V^\\ast_t(x)$u_t^{(k)}(k)(x) \\leq V_t^{\\pi^{(k)}}\\pi^{(k)}(k)(x) \\leq V^\\ast_t(x) is very similar to \\Cref{thr:classical_finite-horizon_sampling}. Assume by induction that\n    \n        u_{t'}t'^{(k)}(k)(x) \\leq (\\mathcal{L}_{\\pi^{(k)}_{t'}}\\pi^{(k)}(k)_{t'}t'u^{(k)}(k)_{t'+1}t'+1)(x) \\leq V_{t'}t'^{\\pi^{(k)}}\\pi^{(k)}(k)(x) \\leq V^\\ast_{t'}t'(x) \\qquad\\forall x\\in\\mathcal{X}, t'=t+1,t+2,\\dots,H+1.\n    \n    Once $u_{t'+1}^{(k)}\\in\\mathscr{B}(\\mathcal{X})$u_{t'+1}t'+1^{(k)}(k)\\in\\mathscr{B}(\\mathcal{X}) has been computed for all $t'=t,t+1,\\dots, H-1$t'=t,t+1,\\dots, H-1, we can compute the quantities $\\widetilde{\\beta}_{t+1}^{(k)}$\\widetilde{\\beta}_{t+1}t+1^{(k)}(k) from \\Cref{line:quantum_backward_line3} at time step $t\\in[H]$t\\in[H]. For all $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}, we employ the quantum mean estimation subroutine from \\Cref{lem:quantum_mean_estimation} with $\\ell_k := O\\big(H\\log\\frac{H|\\mathcal{S}_n|AK}{\\delta}\\big)$\\ell_k := O\\big(H\\log\\frac{H|\\mathcal{S}_n|AK}{\\delta}\\big) queries to $\\mathcal{O}_p$\\mathcal{O}_p to obtain $\\widetilde{\\beta}_{t+1}^{(k)}(s,a)$\\widetilde{\\beta}_{t+1}t+1^{(k)}(k)(s,a) such that, with probability $1-\\frac{\\delta}{2HK}$1-\\frac{\\delta}{2HK}, for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n, $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s), $a\\in\\mathcal{A}$a\\in\\mathcal{A},\n    \n       \\left| \\widetilde{\\beta}_{t+1}t+1^{(k)}(k)(s,a) - \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)\\big(u^{(k)}(k)_{t+1}t+1(x') - u^{(k-1)}(k-1)_{t+1}t+1(x')\\big) \\right| \\leq \\left(\\frac{1}{16H} + Ln^{-\\alpha}-\\alpha\\right)(2\\epsilon_k + 8Ln^{-\\alpha}-\\alphaH^2)\n    \n    (using that $\\|u^{(k)}_{t+1} - u^{(k-1)}_{t+1}\\|_\\infty \\leq 2\\epsilon_k + 8Ln^{-\\alpha}H^2$\\|u^{(k)}(k)_{t+1}t+1 - u^{(k-1)}(k-1)_{t+1}t+1\\|_\\infty \\leq 2\\epsilon_k + 8Ln^{-\\alpha}-\\alphaH^2 due to $u^{(k-1)}_{t+1}(x) \\leq u^{(k)}_{t+1}(x)$u^{(k-1)}(k-1)_{t+1}t+1(x) \\leq u^{(k)}(k)_{t+1}t+1(x) and the induction hypothesis both in $k$k and $t$t).\n    Using that $\\big(\\frac{1}{16H} + Ln^{-\\alpha}\\big)(2\\epsilon_{k} + 8Ln^{-\\alpha}H^2) \\leq \\frac{\\epsilon_{k}}{4H} + Ln^{-\\alpha}H$\\big(\\frac{1}{16H} + Ln^{-\\alpha}-\\alpha\\big)(2\\epsilon_{k}k + 8Ln^{-\\alpha}-\\alphaH^2) \\leq \\frac{\\epsilon_{k}}{4H} + Ln^{-\\alpha}-\\alphaH since $Ln^{-\\alpha} \\leq \\frac{1}{16H}$Ln^{-\\alpha}-\\alpha \\leq \\frac{1}{16H}, we define, for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n, $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s), $a\\in\\mathcal{A}$a\\in\\mathcal{A},\n    \n        \\widehat{\\beta}^{(k)}(k)_{t+1}t+1(x,a) := \\widetilde{\\beta}_{t+1}t+1^{(k)}(k)(s,a) - \\frac{\\epsilon_{k}}{4H} - Ln^{-\\alpha}-\\alphaH,\n    \n    from which, with probability at least $1-\\frac{\\delta}{2KH}$1-\\frac{\\delta}{2KH}, for all $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$(x,a)\\in\\mathcal{X}\\times\\mathcal{A}, it holds that\n    \\label{eq:quantum_backward_eq2}\n    \n        \\widehat{\\beta}^{(k)}(k)_{t+1}t+1(x,a) &\\leq \\int_{\\mathcal{X}}\\mathcal{X}  p(\\rd x'|x,a)\\big(u^{(k)}(k)_{t+1}t+1(x') - u^{(k-1)}(k-1)_{t+1}t+1(x') \\big), \\label{eq:quantum_backward_eq2a}\\\\\n        \\widehat{\\beta}^{(k)}(k)_{t+1}t+1(x,a) &\\geq \\int_{\\mathcal{X}}\\mathcal{X}  p(\\rd x'|x,a)\\big(u^{(k)}(k)_{t+1}t+1(x') - u^{(k-1)}(k-1)_{t+1}t+1(x') \\big) - \\frac{\\epsilon_{k}}{2H}- 2Ln^{-\\alpha}-\\alphaH. \\label{eq:quantum_backward_eq2b}\n    \n    \n    From here, the proof of $u_t^{(k)}(x) \\leq V_t^{\\pi^{(k)}}(x) \\leq V^\\ast_t(x)$u_t^{(k)}(k)(x) \\leq V_t^{\\pi^{(k)}}\\pi^{(k)}(k)(x) \\leq V^\\ast_t(x) is the same as \\Cref{thr:classical_finite-horizon_sampling}, so we omit it and move on to proving that $u^{(k)}_t(x) \\geq V^\\ast_t(x) - \\epsilon_k - 8Ln^{-\\alpha}H$u^{(k)}(k)_t(x) \\geq V^\\ast_t(x) - \\epsilon_k - 8Ln^{-\\alpha}-\\alphaH (assume that all $u^{(k)}_{t'+1}$u^{(k)}(k)_{t'+1}t'+1 for $t' = t,\\dots, H-1$t' = t,\\dots, H-1 have already been computed by the backward iteration). Again,\n    \n        V^\\ast_t(x) - u^{(k)}(k)_t(x) &\\leq \\max_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\left\\{r(x,a) + \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)V_{t+1}t+1^\\ast(x') \\right\\} \\\\\n        &~- \\max_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\left\\{r(x,a) - 2Ln^{-\\alpha}-\\alpha + \\widehat{\\mu}_{t+1}t+1^{(k)}(k)(x,a) + \\widehat{\\beta}_{t+1}t+1^{(k)}(k)(x,a) \\right\\} \\tag{$u_t^{(k)}(x) \\!\\geq\\! u_t^{(k)}(s) \\!-\\! Ln^{-\\alpha}$}$u_t^{(k)}(x) \\!\\geq\\! u_t^{(k)}(s) \\!-\\! Ln^{-\\alpha}$u_t^{(k)}(k)(x) \\!\\geq\\! u_t^{(k)}(k)(s) \\!-\\! Ln^{-\\alpha}-\\alpha\\\\\n        &\\leq \\int_{\\mathcal{X}}\\mathcal{X} p(\\rd x'|x,\\pi^{\\ast}\\ast_{t}t(x))\\big(V_{t+1}t+1^\\ast(x') - u^{(k)}(k)_{t+1}t+1(x')\\big) + \\xi_{t+1}t+1^{(k)}(k)(x), \\tag{by \\Cref{eq:quantum_backward_eq1b,eq:quantum_backward_eq2b}}by \\Cref{eq:quantum_backward_eq1b,eq:quantum_backward_eq2b}\n    \n    where we defined\n    \n        \\xi_{t+1}t+1^{(k)}(k)(x) := 2\\theta_k\\sqrt{\\frac{1}{H}\\sum_{t'=1}^H\\sigma^\\ast_{t'}(x,\\pi^\\ast_{t}(x))} + \\frac{\\epsilon_{k}}{2H} + 2\\theta_k(2\\epsilon_{k}k +  8Ln^{-\\alpha}-\\alphaH^2) + 6\\theta_k^{3/2}3/2H + 6Ln^{-\\alpha}-\\alphaH .\n    \n    Solving the above recursion, using $\\|\\!\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x)\\cdots p_{\\pi_{t'-1}^\\ast}(\\rd x_{t'}|x_{t'-1})\\|_\\infty \\!\\leq\\! H$\\|\\!\\sum_{t'=t}t'=t^{H-1}H-1 \\idotsint_{\\mathcal{X}^{t'-t}}\\mathcal{X}^{t'-t}t'-t p_{\\pi_{t}^\\ast}\\pi_{t}t^\\ast(\\rd x_{t+1}t+1|x)\\cdots p_{\\pi_{t'-1}^\\ast}\\pi_{t'-1}t'-1^\\ast(\\rd x_{t'}t'|x_{t'-1}t'-1)\\|_\\infty \\!\\leq\\! H, and  \n    \n        &\\left\\|\\sum_{t'=t}t'=t^{H-1}H-1 \\idotsint_{\\mathcal{X}^{t'-t}}\\mathcal{X}^{t'-t}t'-t p_{\\pi_{t}^\\ast}\\pi_{t}t^\\ast(\\rd x_{t+1}t+1|x)\\cdots p_{\\pi_{t'}^\\ast}\\pi_{t'}t'^\\ast(\\rd x_{t'-1}t'-1|x_{t'-1}t'-1) \\sqrt{\\frac{1}{H}\\sum_{t''=1}^H \\sigma_{t''}^\\ast(x_{t'},\\pi^\\ast_{t'}(x_{t'}))} \\right\\|_\\infty \\\\\n        &\\leq \\left\\| \\sqrt{\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x)\\cdots p_{\\pi_{t'-1}^\\ast}(\\rd x_{t'}|x_{t'-1})\\sum_{t''=1}^H \\sigma_{t''}^\\ast(x_{t'},\\pi^\\ast_{t'}(x_{t'}))} \\right\\|_\\infty \\leq 2H^{3/2}3/2, \\tag{by Cauchy\u2013Schwarz inequality and \\Cref{fact:upper_bound_variance2}}by Cauchy\u2013Schwarz inequality and \\Cref{fact:upper_bound_variance2}\n    \n    we finally get that\n    \n        V^\\ast_t(x) - u^{(k)}(k)_t(x) &\\leq 4\\theta_k H^{3/2}3/2 + \\frac{\\epsilon_{k}}{2} + 2\\theta_k(2\\epsilon_{k}k +  8Ln^{-\\alpha}-\\alphaH^2)H + 6\\theta_k^{3/2}3/2H^2 + 6 Ln^{-\\alpha}-\\alphaH^2  \\\\\n        &\\leq \\epsilon_k\\left(\\frac{1}{2} + \\frac{4}{20} + \\frac{4}{20 H^{3/2}} + \\frac{6}{20^{3/2}H^{1/4}} \\right) + \\left(\\frac{2\\cdot 8}{20\\sqrt{H}} + 6\\right)Ln^{-\\alpha}-\\alphaH^2 \\tag{$\\theta_k = \\frac{1}{20 H^{3/2}}\\min\\{\\epsilon_k, 1\\}$}$\\theta_k = \\frac{1}{20 H^{3/2}}\\min\\{\\epsilon_k, 1\\}$\\theta_k = \\frac{1}{20 H^{3/2}}\\min\\{\\epsilon_k, 1\\}\\\\\n        &\\leq \\epsilon_k + 8Ln^{-\\alpha}-\\alphaH^2,\n    \n    This concludes the proof of \\Cref{eq:quantum_backward_eq0} for epoch $k\\in[K]$k\\in[K] and thus for all epochs by induction on $k$k.\n\n    Regarding the failure probability, \\Cref{eq:quantum_backward_eq1} holds with probability $1-\\frac{\\delta}{2K}$1-\\frac{\\delta}{2K} for a single epoch, while \\Cref{eq:quantum_backward_eq2} holds with probability $1-\\frac{\\delta}{2HK}$1-\\frac{\\delta}{2HK} for a single epoch and time step. Therefore, across all epochs and time steps, the success probability is at least $1-\\delta$1-\\delta, as required. Finally, the total number of samples used is\n    \\[\n        O\\bigg(\\sum_{k=1}^K (m_k + n_k + H\\ell_k)|\\mathcal{S}_n|A\\bigg) = O\\left(\\frac{H^{2}|\\mathcal{S}_n|A}{\\varepsilon}\\log\\left(\\frac{H|\\mathcal{S}_n|A}{\\delta}\\log\\frac{H}{\\varepsilon}\\right) \\right). \\qedhere\n    \\]\n        O\\bigg(\\sum_{k=1}k=1^K (m_k + n_k + H\\ell_k)|\\mathcal{S}_n|A\\bigg) = O\\left(\\frac{H^{2}|\\mathcal{S}_n|A}{\\varepsilon}\\log\\left(\\frac{H|\\mathcal{S}_n|A}{\\delta}\\log\\frac{H}{\\varepsilon}\\right) \\right). \\qedhere\n    \n\n\n\n\n\\begin{algorithm}[t!]\n    \\caption{Simple quantum backward induction algorithm}\n    \\label{algo:quantum_backward_recursion2}\n    \\begin{algorithmic}[1]  \n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$,  finite action space $\\mathcal{A}$, horizon $H$, quantum sampling access to probability kernels $p$, failure probability $\\delta\\in(0, 1)$, error $\\varepsilon > 0$.\n\n    \\Ensure $(\\varepsilon + (1+H)HLn^{-\\alpha})$-optimal policy $\\pi \\in \\Pi^{\\rm D}$. \n\n\n    \\State $\\widehat{u}_H(s) \\gets \\min_{a\\in\\mathcal{A}}\\{r(s,a)\\}$ and $\\pi_H(s) \\gets \\argmin_{a\\in\\mathcal{A}}\\{r(s,a)\\}$ with probability $1-\\frac{\\delta}{H|\\mathcal{S}_n|}$ for all $s\\in\\mathcal{S}_n$ (\\Cref{fact:quantum_minimum_finding})\n    \n    \\State $u_H(x) \\gets \\widehat{u}_H(s) - Ln^{-\\alpha}$ and $\\pi_H(x) \\gets \\pi_H(s)$ for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$\n    \n        \\For{$t=H-1,H-2,\\dots,1$}\n\n            \\For{$s\\in\\mathcal{S}_n$}\n\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use \\Cref{lem:quantum_mean_estimation} to obtain a unitary $\\mathcal{U}_s^{(t+1)}:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|\\mu_{t+1}(s,a)\\rangle$ for all $a\\in\\mathcal{A}$, where $|\\mu_{t+1}(s,a) - \\int_{\\mathcal{X}}p(\\rd x'|s,a) u_{t+1}(x')| \\leq \\frac{\\varepsilon}{2H}$ with high probability}\n\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use quantum maximum finding with unitary $\\mathcal{U}_s^{(t+1)}$ (\\Cref{fact:quantum_minimum_finding} with $\\mathcal{U}_s^{(t+1)}$) to obtain $\\widehat{u}_t(s)$ and $a_t(s)$ such that, with probability $1-\\frac{\\delta}{H|\\mathcal{S}_n|}$,\n\t\t\t%\n\t\t\t\\begin{align*}\n\t\t\t\t\\widehat{u}_{t}(s) = \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\} \\quad\\text{and}\\quad\n\t\t\t\t \\pi_t(s) = \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}\n\t\t\t\\end{align*}\\label{line:quantum_simple}}\n\n\t\t\t\\State $u_t(x) \\gets \\widehat{u}_t(s) - \\frac{\\varepsilon}{2H} - (1+H)Ln^{-\\alpha}$ and $\\pi_t(x) \\gets \\pi_t(s)$ for all $x\\in\\mathcal{X}$\n\n\t\t\t\\State If $u_t(s) \\leq u_{t+1}(s)$, then $u_t(x) \\gets u_{t+1}(x)$ and $\\pi_t(x) \\gets \\pi_{t+1}(x)$ for all $x\\in\\mathcal{X}(s)$ \\label{line:simple_quantum_enforcement}\n\t\t\n            \\EndFor\n\n            \n        \\EndFor\n    \n    \\State \\Return $\\pi = (\\pi_1,\\dots,\\pi_H)\\in\\Pi^{\\rm D}$\n\\end{algorithmic}\n\\end{algorithm}\\begin{algorithm}[t!]\n    \\caption{Simple quantum backward induction algorithm}\n    \\label{algo:quantum_backward_recursion2}\n    \\begin{algorithmic}[1]  \n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$,  finite action space $\\mathcal{A}$, horizon $H$, quantum sampling access to probability kernels $p$, failure probability $\\delta\\in(0, 1)$, error $\\varepsilon > 0$.\n\n    \\Ensure $(\\varepsilon + (1+H)HLn^{-\\alpha})$-optimal policy $\\pi \\in \\Pi^{\\rm D}$. \n\n\n    \\State $\\widehat{u}_H(s) \\gets \\min_{a\\in\\mathcal{A}}\\{r(s,a)\\}$ and $\\pi_H(s) \\gets \\argmin_{a\\in\\mathcal{A}}\\{r(s,a)\\}$ with probability $1-\\frac{\\delta}{H|\\mathcal{S}_n|}$ for all $s\\in\\mathcal{S}_n$ (\\Cref{fact:quantum_minimum_finding})\n    \n    \\State $u_H(x) \\gets \\widehat{u}_H(s) - Ln^{-\\alpha}$ and $\\pi_H(x) \\gets \\pi_H(s)$ for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$\n    \n        \\For{$t=H-1,H-2,\\dots,1$}\n\n            \\For{$s\\in\\mathcal{S}_n$}\n\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use \\Cref{lem:quantum_mean_estimation} to obtain a unitary $\\mathcal{U}_s^{(t+1)}:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|\\mu_{t+1}(s,a)\\rangle$ for all $a\\in\\mathcal{A}$, where $|\\mu_{t+1}(s,a) - \\int_{\\mathcal{X}}p(\\rd x'|s,a) u_{t+1}(x')| \\leq \\frac{\\varepsilon}{2H}$ with high probability}\n\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use quantum maximum finding with unitary $\\mathcal{U}_s^{(t+1)}$ (\\Cref{fact:quantum_minimum_finding} with $\\mathcal{U}_s^{(t+1)}$) to obtain $\\widehat{u}_t(s)$ and $a_t(s)$ such that, with probability $1-\\frac{\\delta}{H|\\mathcal{S}_n|}$,\n\t\t\t%\n\t\t\t\\begin{align*}\n\t\t\t\t\\widehat{u}_{t}(s) = \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\} \\quad\\text{and}\\quad\n\t\t\t\t \\pi_t(s) = \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}\n\t\t\t\\end{align*}\\label{line:quantum_simple}}\n\n\t\t\t\\State $u_t(x) \\gets \\widehat{u}_t(s) - \\frac{\\varepsilon}{2H} - (1+H)Ln^{-\\alpha}$ and $\\pi_t(x) \\gets \\pi_t(s)$ for all $x\\in\\mathcal{X}$\n\n\t\t\t\\State If $u_t(s) \\leq u_{t+1}(s)$, then $u_t(x) \\gets u_{t+1}(x)$ and $\\pi_t(x) \\gets \\pi_{t+1}(x)$ for all $x\\in\\mathcal{X}(s)$ \\label{line:simple_quantum_enforcement}\n\t\t\n            \\EndFor\n\n            \n        \\EndFor\n    \n    \\State \\Return $\\pi = (\\pi_1,\\dots,\\pi_H)\\in\\Pi^{\\rm D}$\n\\end{algorithmic}\n\\end{algorithm}[t!]\n    \\caption{Simple quantum backward induction algorithm}\n    \\label{algo:quantum_backward_recursion2}\n    [1]  \n    \\Require Compact state space $\\mathcal{X}$\\mathcal{X} with $\\frac{1}{n}$\\frac{1}{n}-net $\\mathcal{S}_n$\\mathcal{S}_n,  finite action space $\\mathcal{A}$\\mathcal{A}, horizon $H$H, quantum sampling access to probability kernels $p$p, failure probability $\\delta\\in(0, 1)$\\delta\\in(0, 1), error $\\varepsilon > 0$\\varepsilon > 0.\n\n    \\Ensure $(\\varepsilon + (1+H)HLn^{-\\alpha})$(\\varepsilon + (1+H)HLn^{-\\alpha}-\\alpha)-optimal policy $\\pi \\in \\Pi^{\\rm D}$\\pi \\in \\Pi^{\\rm D}\\rm D. \n\n\n    \\State $\\widehat{u}_H(s) \\gets \\min_{a\\in\\mathcal{A}}\\{r(s,a)\\}$\\widehat{u}_H(s) \\gets \\min_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{r(s,a)\\} and $\\pi_H(s) \\gets \\argmin_{a\\in\\mathcal{A}}\\{r(s,a)\\}$\\pi_H(s) \\gets \\argmin_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{r(s,a)\\} with probability $1-\\frac{\\delta}{H|\\mathcal{S}_n|}$1-\\frac{\\delta}{H|\\mathcal{S}_n|} for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n (\\Cref{fact:quantum_minimum_finding})\n    \n    \\State $u_H(x) \\gets \\widehat{u}_H(s) - Ln^{-\\alpha}$u_H(x) \\gets \\widehat{u}_H(s) - Ln^{-\\alpha}-\\alpha and $\\pi_H(x) \\gets \\pi_H(s)$\\pi_H(x) \\gets \\pi_H(s) for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n and $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s)\n    \n        \\For{$t=H-1,H-2,\\dots,1$}$t=H-1,H-2,\\dots,1$t=H-1,H-2,\\dots,1\n\n            \\For{$s\\in\\mathcal{S}_n$}$s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n\n\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent{Use \\Cref{lem:quantum_mean_estimation} to obtain a unitary $\\mathcal{U}_s^{(t+1)}:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|\\mu_{t+1}(s,a)\\rangle$ for all $a\\in\\mathcal{A}$, where $|\\mu_{t+1}(s,a) - \\int_{\\mathcal{X}}p(\\rd x'|s,a) u_{t+1}(x')| \\leq \\frac{\\varepsilon}{2H}$ with high probability}Use \\Cref{lem:quantum_mean_estimation} to obtain a unitary $\\mathcal{U}_s^{(t+1)}:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|\\mu_{t+1}(s,a)\\rangle$\\mathcal{U}_s^{(t+1)}(t+1):|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|\\mu_{t+1}t+1(s,a)\\rangle for all $a\\in\\mathcal{A}$a\\in\\mathcal{A}, where $|\\mu_{t+1}(s,a) - \\int_{\\mathcal{X}}p(\\rd x'|s,a) u_{t+1}(x')| \\leq \\frac{\\varepsilon}{2H}$|\\mu_{t+1}t+1(s,a) - \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|s,a) u_{t+1}t+1(x')| \\leq \\frac{\\varepsilon}{2H} with high probability\n\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent{Use quantum maximum finding with unitary $\\mathcal{U}_s^{(t+1)}$ (\\Cref{fact:quantum_minimum_finding} with $\\mathcal{U}_s^{(t+1)}$) to obtain $\\widehat{u}_t(s)$ and $a_t(s)$ such that, with probability $1-\\frac{\\delta}{H|\\mathcal{S}_n|}$,\n\t\t\t%\n\t\t\t\\begin{align*}\n\t\t\t\t\\widehat{u}_{t}(s) = \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\} \\quad\\text{and}\\quad\n\t\t\t\t \\pi_t(s) = \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}\n\t\t\t\\end{align*}\\label{line:quantum_simple}}Use quantum maximum finding with unitary $\\mathcal{U}_s^{(t+1)}$\\mathcal{U}_s^{(t+1)}(t+1) (\\Cref{fact:quantum_minimum_finding} with $\\mathcal{U}_s^{(t+1)}$\\mathcal{U}_s^{(t+1)}(t+1)) to obtain $\\widehat{u}_t(s)$\\widehat{u}_t(s) and $a_t(s)$a_t(s) such that, with probability $1-\\frac{\\delta}{H|\\mathcal{S}_n|}$1-\\frac{\\delta}{H|\\mathcal{S}_n|},\n\t\t\t\n\t\t\t\t\\widehat{u}_{t}t(s) = \\max_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{r(s,a) + \\mu_{t+1}t+1(s,a)\\} \\quad\\text{and}\\quad\n\t\t\t\t \\pi_t(s) = \\argmax_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{r(s,a) + \\mu_{t+1}t+1(s,a)\\}\n\t\t\t\\label{line:quantum_simple}\n\n\t\t\t\\State $u_t(x) \\gets \\widehat{u}_t(s) - \\frac{\\varepsilon}{2H} - (1+H)Ln^{-\\alpha}$u_t(x) \\gets \\widehat{u}_t(s) - \\frac{\\varepsilon}{2H} - (1+H)Ln^{-\\alpha}-\\alpha and $\\pi_t(x) \\gets \\pi_t(s)$\\pi_t(x) \\gets \\pi_t(s) for all $x\\in\\mathcal{X}$x\\in\\mathcal{X}\n\n\t\t\t\\State If $u_t(s) \\leq u_{t+1}(s)$u_t(s) \\leq u_{t+1}t+1(s), then $u_t(x) \\gets u_{t+1}(x)$u_t(x) \\gets u_{t+1}t+1(x) and $\\pi_t(x) \\gets \\pi_{t+1}(x)$\\pi_t(x) \\gets \\pi_{t+1}t+1(x) for all $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s) \\label{line:simple_quantum_enforcement}\n\t\t\n            \\EndFor\n\n            \n        \\EndFor\n    \n    \\State \\Return $\\pi = (\\pi_1,\\dots,\\pi_H)\\in\\Pi^{\\rm D}$\\pi = (\\pi_1,\\dots,\\pi_H)\\in\\Pi^{\\rm D}\\rm D\n\n\n\nOur second quantum algorithm (\\Cref{algo:quantum_backward_recursion2}) is based on a simpler backward induction algorithm without variance reduction and total-variance techniques: we simply compute $u_t(x) = (\\mathcal{L}u_{t+1})(x)$u_t(x) = (\\mathcal{L}u_{t+1}t+1)(x) for $t=H,H-1,\\dots,1$t=H,H-1,\\dots,1, i.e., in a backward fashion, using quantum subroutines. Although this will inevitably lead to a worse sample complexity on the horizon $H$H, it is possible now to employ quantum minimum finding~\\cite{durr1996quantum} together with quantum mean estimation, which brings down the sample complexity on the action space size from $O(A)$O(A) down to $O(\\sqrt{A})$O(\\sqrt{A}).\n\n\\begin{theorem}[Simple quantum finite-horizon generative algorithm]\\label{thr:quantum_finite-horizon2}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},H, p, r\\rangle$ be a finite-horizon MDP and let $\\mathcal{S}_n$ be a $\\frac{1}{n}$-net for $\\mathcal{X}$. Let $\\varepsilon > 0$ and $\\delta\\in(0,1)$. Under {\\rm \\Cref{ass:Lipschitz}} with parameters $L,\\alpha\\geq 0$, {\\rm \\Cref{algo:quantum_backward_recursion2}} computes functions $\\{u_t\\in\\mathscr{B}(\\mathcal{X})\\}_{t\\in[H]}$ and policy $\\pi\\in\\Pi^{\\rm D}$ such that, with probability $1-\\delta$,\n    %\n    \\begin{align*}\n        V^\\ast_t(x) - \\varepsilon - 2(1+H)HLn^{-\\alpha} \\leq u_t(x) \\leq V^{\\pi}_t(x) \\leq  V^\\ast_t(x) \\qquad\\forall (x,t)\\in\\mathcal{X}\\times[H].\n    \\end{align*}\n    %\n    The $\\mathcal{O}_p,\\mathcal{O}_p^\\dagger$-query complexity is\n    %\n    \\begin{align*}\n        O\\bigg(\\frac{H^{3}|\\mathcal{S}_n|\\sqrt{A}}{\\varepsilon}\\log\\left(\\frac{H|\\mathcal{S}_n|A}{\\delta} \\right)\\log\\left(\\frac{H|\\mathcal{S}_n|}{\\delta} \\right) \\bigg).\n    \\end{align*}\n\\end{theorem}\\begin{theorem}[Simple quantum finite-horizon generative algorithm]\\label{thr:quantum_finite-horizon2}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},H, p, r\\rangle$ be a finite-horizon MDP and let $\\mathcal{S}_n$ be a $\\frac{1}{n}$-net for $\\mathcal{X}$. Let $\\varepsilon > 0$ and $\\delta\\in(0,1)$. Under {\\rm \\Cref{ass:Lipschitz}} with parameters $L,\\alpha\\geq 0$, {\\rm \\Cref{algo:quantum_backward_recursion2}} computes functions $\\{u_t\\in\\mathscr{B}(\\mathcal{X})\\}_{t\\in[H]}$ and policy $\\pi\\in\\Pi^{\\rm D}$ such that, with probability $1-\\delta$,\n    %\n    \\begin{align*}\n        V^\\ast_t(x) - \\varepsilon - 2(1+H)HLn^{-\\alpha} \\leq u_t(x) \\leq V^{\\pi}_t(x) \\leq  V^\\ast_t(x) \\qquad\\forall (x,t)\\in\\mathcal{X}\\times[H].\n    \\end{align*}\n    %\n    The $\\mathcal{O}_p,\\mathcal{O}_p^\\dagger$-query complexity is\n    %\n    \\begin{align*}\n        O\\bigg(\\frac{H^{3}|\\mathcal{S}_n|\\sqrt{A}}{\\varepsilon}\\log\\left(\\frac{H|\\mathcal{S}_n|A}{\\delta} \\right)\\log\\left(\\frac{H|\\mathcal{S}_n|}{\\delta} \\right) \\bigg).\n    \\end{align*}\n\\end{theorem}\\label{thr:quantum_finite-horizon2}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},H, p, r\\rangle$M = \\langle \\mathcal{X},\\mathcal{A},H, p, r\\rangle be a finite-horizon MDP and let $\\mathcal{S}_n$\\mathcal{S}_n be a $\\frac{1}{n}$\\frac{1}{n}-net for $\\mathcal{X}$\\mathcal{X}. Let $\\varepsilon > 0$\\varepsilon > 0 and $\\delta\\in(0,1)$\\delta\\in(0,1). Under {\\rm \\Cref{ass:Lipschitz}}\\rm \\Cref{ass:Lipschitz} with parameters $L,\\alpha\\geq 0$L,\\alpha\\geq 0, {\\rm \\Cref{algo:quantum_backward_recursion2}}\\rm \\Cref{algo:quantum_backward_recursion2} computes functions $\\{u_t\\in\\mathscr{B}(\\mathcal{X})\\}_{t\\in[H]}$\\{u_t\\in\\mathscr{B}(\\mathcal{X})\\}_{t\\in[H]}t\\in[H] and policy $\\pi\\in\\Pi^{\\rm D}$\\pi\\in\\Pi^{\\rm D}\\rm D such that, with probability $1-\\delta$1-\\delta,\n    \n        V^\\ast_t(x) - \\varepsilon - 2(1+H)HLn^{-\\alpha}-\\alpha \\leq u_t(x) \\leq V^{\\pi}\\pi_t(x) \\leq  V^\\ast_t(x) \\qquad\\forall (x,t)\\in\\mathcal{X}\\times[H].\n    \n    The $\\mathcal{O}_p,\\mathcal{O}_p^\\dagger$\\mathcal{O}_p,\\mathcal{O}_p^\\dagger-query complexity is\n    \n        O\\bigg(\\frac{H^{3}|\\mathcal{S}_n|\\sqrt{A}}{\\varepsilon}\\log\\left(\\frac{H|\\mathcal{S}_n|A}{\\delta} \\right)\\log\\left(\\frac{H|\\mathcal{S}_n|}{\\delta} \\right) \\bigg).\n    \n\n\\begin{proof}\n    The proof is by induction on $t\\in[H]$. We start by analysing the quantity $\\widehat{u}_t$ in \\Cref{line:quantum_simple} at time step $t\\in[H]$. First notice that, for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$,\n    %\n    \\begin{align*}\n        |(\\mathcal{L}u_{t+1})(s) - (\\mathcal{L}u_{t+1})(x)| &\\leq |r(s,a) - r(x,a)| + \\left|\\int_{\\mathcal{X}}(p(\\rd x'|s,a) - p(\\rd x'|x,a))u_{t+1}(x') \\right| \\\\\n        &\\leq (1+H)Ln^{-\\alpha}, \\tag{$\\|u_{t+1}\\|_\\infty \\leq H$ by induction}\n    \\end{align*}\n    %\n    where $a = \\argmax_{a'\\in\\mathcal{A}}(\\mathcal{L}_{a'}u_{t+1})(s)$.\n    With probability $1 - \\frac{\\delta}{H|\\mathcal{S}_n|}$, $|\\widehat{u}_t(s) - (\\mathcal{L}u_{t+1})(s)| \\leq \\frac{\\varepsilon}{2H}$ and thus $u_t(x) := \\widehat{u}_t(s) - \\frac{\\varepsilon}{2H} - (1+H)Ln^{-\\alpha}$ for $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$ is such that\n    %\n    \\begin{align}\\label{eq:simple_quantum_eq1}\n        u_t(x) \\leq (\\mathcal{L}u_{t+1})(x) \\quad\\text{and}\\quad\n        u_t(x) \\geq (\\mathcal{L}u_{t+1})(x) - \\frac{\\varepsilon}{H} - 2(1+H)Ln^{-\\alpha}.\n    \\end{align}\n    %\n    From here, proving the inequality $u_t(x) \\leq V_t^\\pi(x)$ is done exactly as in \\Cref{thr:classical_finite-horizon_sampling,thr:quantum_finite-horizon} as long as $u_t(x) \\leq (\\mathcal{L}u_{t+1})(x)$ (\\Cref{eq:simple_quantum_eq1}) and $u_{t+1}(x) \\leq u_t(x)$, which is enforced by design (\\Cref{line:simple_quantum_enforcement}). To prove that $V_t^\\ast(x) - \\varepsilon - (1+H)HLn^{-\\alpha} \\leq u_t(x)$, we follow the proof of \\Cref{thr:classical_finite-horizon_sampling,thr:quantum_finite-horizon},\n    %\n    \\begin{align*}\n        V_t^\\ast(x) - u_t(x) &\\leq (\\mathcal{L}V^\\ast_{t+1})(x) - (\\mathcal{L}u_{t+1})(x) + \\frac{\\varepsilon}{H} + 2(1+H)Ln^{-\\alpha}\\\\\n        &\\leq \\int_{\\mathcal{X}} p(\\rd x'|x, \\pi_t^\\ast(x))\\big(V^\\ast_{t+1}(x') - u_{t+1}(x')\\big) + \\frac{\\varepsilon}{H} + 2(1+H)Ln^{-\\alpha}.\n    \\end{align*}\n    %\n    Solving the above recursion and using $\\|\\!\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x)\\cdots p_{\\pi_{t'-1}^\\ast}(\\rd x_{t'}|x_{t'-1})\\|_\\infty \\!\\leq\\! H$, we get that\n    %\n    \\begin{align*}\n        V_t^\\ast(x) - u_t(x) \\leq \\varepsilon + 2(1+H)HLn^{-\\alpha},\n    \\end{align*}\n    %\n    which concludes the proof of correctness. \n\n    We now analyse the success probability of \\Cref{algo:quantum_backward_recursion2}. To do so, we must analyse how quantum oracles fail (see~\\cite[Appendix~A]{chen2023quantum} for a similar argument as follows). Ideally, we would like to implement the unitary $\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)}:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|\\mu_{t+1}(s,a)\\rangle$ where $|\\mu_{t+1}(s,a)\\rangle$ contains the approximation $|\\mu_{t+1}(s,a) - \\int_{\\mathcal{X}}p(\\rd x'|s,a) u_{t+1}(x')| \\leq \\frac{\\varepsilon}{2H}$. In practice, however, we implement the unitary $\\mathcal{U}_{s}^{(t+1)}:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle(\\sqrt{1-\\delta_2}|\\mu_{t+1}(s,a)\\rangle + \\sqrt{\\delta_2}|\\phi_a\\rangle)$ for some $\\delta_2 \\in(0,1)$, where the register $|\\mu_{t+1}(s,a)\\rangle$ holds the desired approximation $\\mu_{t+1}(s,a)$ and $|\\phi_a\\rangle$ is a normalised quantum state orthogonal to $|\\mu_{t+1}(s,a)\\rangle$. Notice that\n    %\n    \\begin{align*}\n        \\forall a\\in\\mathcal{A}: \\qquad \\|(\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)} - \\mathcal{U}_{s}^{(t+1)})|a\\rangle|\\bar{0}\\rangle\\| = \\sqrt{(1 - \\sqrt{1-\\delta_2})^2 + \\delta_2} = \\sqrt{2-2\\sqrt{1-\\delta_2}} \\leq \\sqrt{2\\delta_2},\n    \\end{align*}\n    %\n    using that $\\sqrt{1-\\delta_2} \\geq 1 - \\delta_2$. Since quantum minimum finding does not take into account the action of $\\mathcal{U}_{s}^{(t+1)}$ onto states of the form $|a\\rangle|\\bar{0}^\\perp\\rangle$ for $|\\bar{0}^\\perp\\rangle$ orthogonal to $|\\bar{0}\\rangle$, we can, without of loss of generality, assume that $\\|\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)} - \\mathcal{U}_{s}^{(t+1)}\\| \\leq \\sqrt{2\\delta_2}$. The success probability of quantum minimum finding (\\Cref{fact:quantum_minimum_finding}) is $1-\\delta_1$ when employing $\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)}$, for some $\\delta_1\\in(0,1)$. However, since it employs $\\mathcal{U}_{s}^{(t+1)}$ instead, the success probability decreases by at most the spectral norm of the difference between the ``real'' and the ``ideal'' total unitaries. To be more precise, the ``ideal'' quantum minimum finding is a sequence of gates $\\mathcal{A} = U_1E_1U_2 E_2\\cdots U_{N}E_N$, where $U_i \\in \\{\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)},\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)\\dagger}\\}$, $E_i$ is a circuit of elementary gates, and $N = c\\sqrt{A}\\log\\frac{1}{\\delta_1}$ with $c$ constant is the number of queries to $\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)}$. The ``real'' implementation, on the other hand, is $\\widetilde{\\mathcal{A}} = \\widetilde{U}_1E_1\\widetilde{U}_2 E_2\\cdots \\widetilde{U}_{N}E_N$, where $\\widetilde{U}_i \\in \\{\\mathcal{U}_{s}^{(t+1)},\\mathcal{U}_{s}^{(t+1)\\dagger}\\}$. Then $\\|\\mathcal{A} - \\widetilde{\\mathcal{A}}\\| \\leq c\\sqrt{A}\\log\\!\\big(\\frac{1}{\\delta_1}\\big)\\|\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)} - \\mathcal{U}_{s}^{(t+1)}\\| \\leq c\\sqrt{A}\\log\\!\\big(\\frac{1}{\\delta_1}\\big)\\sqrt{2\\delta_2}$ and the failure probability is $\\delta_1 + c\\sqrt{A}\\log\\frac{1}{\\delta_1}\\sqrt{2\\delta_2}$. By taking $\\delta_1 = O\\big(\\frac{\\delta}{H|\\mathcal{S}_n|}\\big)$ and $\\delta_2 = O\\big(\\frac{\\delta_1^2}{A\\log^2(1/\\delta_1)}\\big)$, the failure probability in outputting $\\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}$ is at most $\\frac{\\delta}{H|\\mathcal{S}_n|}$. By a usual union bound over all $s\\in\\mathcal{S}_n$ and $t\\in[H]$, the failure probability is at most $\\delta$.\\footnote{Wang \\emph{et al.}\\ employ a similar but slightly incorrect argument for the failure probability~\\cite[Theorem~6]{wang2021quantum}. More precisely, they ignore the failure probability $\\delta_1$ coming from quantum minimum finding, or rather, assume it to be constant. This incorrectly leads to ignoring the factor $\\log\\frac{1}{\\delta_1}$ in their runtime~\\cite[Theorem~7]{wang2021quantum}.}\n    \n    Regarding the query complexity, for each $s\\in\\mathcal{S}_n$, one call to the unitary $\\mathcal{U}_s^{(t+1)}$ uses $O\\big(\\frac{\\|u_{t+1}\\|_\\infty}{\\varepsilon/H}\\log\\frac{1}{\\delta_2}\\big) = O\\big(\\frac{H^2}{\\varepsilon}\\log\\frac{H|\\mathcal{S}_n|A}{\\delta}\\big)$ queries to $\\mathcal{O}_p$, while quantum maximum finding makes $O\\big(\\sqrt{A}\\log\\frac{1}{\\delta_1}\\big) = O\\big(\\sqrt{A}\\log\\frac{H|\\mathcal{S}_n|}{\\delta}\\big)$ queries to $\\mathcal{U}_s^{(t+1)}$. Summing over all $\\mathcal{S}_n\\times[H]$, the total query complexity is\n    %\n    \\[\n        O\\bigg(\\frac{H^3|\\mathcal{S}_n|\\sqrt{A}}{\\varepsilon}\\log\\frac{H|\\mathcal{S}_n|A}{\\delta}\\log\\frac{H|\\mathcal{S}_n|}{\\delta}\\bigg). \\qedhere\n    \\]\n\\end{proof}\\begin{proof}\n    The proof is by induction on $t\\in[H]$. We start by analysing the quantity $\\widehat{u}_t$ in \\Cref{line:quantum_simple} at time step $t\\in[H]$. First notice that, for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$,\n    %\n    \\begin{align*}\n        |(\\mathcal{L}u_{t+1})(s) - (\\mathcal{L}u_{t+1})(x)| &\\leq |r(s,a) - r(x,a)| + \\left|\\int_{\\mathcal{X}}(p(\\rd x'|s,a) - p(\\rd x'|x,a))u_{t+1}(x') \\right| \\\\\n        &\\leq (1+H)Ln^{-\\alpha}, \\tag{$\\|u_{t+1}\\|_\\infty \\leq H$ by induction}\n    \\end{align*}\n    %\n    where $a = \\argmax_{a'\\in\\mathcal{A}}(\\mathcal{L}_{a'}u_{t+1})(s)$.\n    With probability $1 - \\frac{\\delta}{H|\\mathcal{S}_n|}$, $|\\widehat{u}_t(s) - (\\mathcal{L}u_{t+1})(s)| \\leq \\frac{\\varepsilon}{2H}$ and thus $u_t(x) := \\widehat{u}_t(s) - \\frac{\\varepsilon}{2H} - (1+H)Ln^{-\\alpha}$ for $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$ is such that\n    %\n    \\begin{align}\\label{eq:simple_quantum_eq1}\n        u_t(x) \\leq (\\mathcal{L}u_{t+1})(x) \\quad\\text{and}\\quad\n        u_t(x) \\geq (\\mathcal{L}u_{t+1})(x) - \\frac{\\varepsilon}{H} - 2(1+H)Ln^{-\\alpha}.\n    \\end{align}\n    %\n    From here, proving the inequality $u_t(x) \\leq V_t^\\pi(x)$ is done exactly as in \\Cref{thr:classical_finite-horizon_sampling,thr:quantum_finite-horizon} as long as $u_t(x) \\leq (\\mathcal{L}u_{t+1})(x)$ (\\Cref{eq:simple_quantum_eq1}) and $u_{t+1}(x) \\leq u_t(x)$, which is enforced by design (\\Cref{line:simple_quantum_enforcement}). To prove that $V_t^\\ast(x) - \\varepsilon - (1+H)HLn^{-\\alpha} \\leq u_t(x)$, we follow the proof of \\Cref{thr:classical_finite-horizon_sampling,thr:quantum_finite-horizon},\n    %\n    \\begin{align*}\n        V_t^\\ast(x) - u_t(x) &\\leq (\\mathcal{L}V^\\ast_{t+1})(x) - (\\mathcal{L}u_{t+1})(x) + \\frac{\\varepsilon}{H} + 2(1+H)Ln^{-\\alpha}\\\\\n        &\\leq \\int_{\\mathcal{X}} p(\\rd x'|x, \\pi_t^\\ast(x))\\big(V^\\ast_{t+1}(x') - u_{t+1}(x')\\big) + \\frac{\\varepsilon}{H} + 2(1+H)Ln^{-\\alpha}.\n    \\end{align*}\n    %\n    Solving the above recursion and using $\\|\\!\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x)\\cdots p_{\\pi_{t'-1}^\\ast}(\\rd x_{t'}|x_{t'-1})\\|_\\infty \\!\\leq\\! H$, we get that\n    %\n    \\begin{align*}\n        V_t^\\ast(x) - u_t(x) \\leq \\varepsilon + 2(1+H)HLn^{-\\alpha},\n    \\end{align*}\n    %\n    which concludes the proof of correctness. \n\n    We now analyse the success probability of \\Cref{algo:quantum_backward_recursion2}. To do so, we must analyse how quantum oracles fail (see~\\cite[Appendix~A]{chen2023quantum} for a similar argument as follows). Ideally, we would like to implement the unitary $\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)}:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|\\mu_{t+1}(s,a)\\rangle$ where $|\\mu_{t+1}(s,a)\\rangle$ contains the approximation $|\\mu_{t+1}(s,a) - \\int_{\\mathcal{X}}p(\\rd x'|s,a) u_{t+1}(x')| \\leq \\frac{\\varepsilon}{2H}$. In practice, however, we implement the unitary $\\mathcal{U}_{s}^{(t+1)}:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle(\\sqrt{1-\\delta_2}|\\mu_{t+1}(s,a)\\rangle + \\sqrt{\\delta_2}|\\phi_a\\rangle)$ for some $\\delta_2 \\in(0,1)$, where the register $|\\mu_{t+1}(s,a)\\rangle$ holds the desired approximation $\\mu_{t+1}(s,a)$ and $|\\phi_a\\rangle$ is a normalised quantum state orthogonal to $|\\mu_{t+1}(s,a)\\rangle$. Notice that\n    %\n    \\begin{align*}\n        \\forall a\\in\\mathcal{A}: \\qquad \\|(\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)} - \\mathcal{U}_{s}^{(t+1)})|a\\rangle|\\bar{0}\\rangle\\| = \\sqrt{(1 - \\sqrt{1-\\delta_2})^2 + \\delta_2} = \\sqrt{2-2\\sqrt{1-\\delta_2}} \\leq \\sqrt{2\\delta_2},\n    \\end{align*}\n    %\n    using that $\\sqrt{1-\\delta_2} \\geq 1 - \\delta_2$. Since quantum minimum finding does not take into account the action of $\\mathcal{U}_{s}^{(t+1)}$ onto states of the form $|a\\rangle|\\bar{0}^\\perp\\rangle$ for $|\\bar{0}^\\perp\\rangle$ orthogonal to $|\\bar{0}\\rangle$, we can, without of loss of generality, assume that $\\|\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)} - \\mathcal{U}_{s}^{(t+1)}\\| \\leq \\sqrt{2\\delta_2}$. The success probability of quantum minimum finding (\\Cref{fact:quantum_minimum_finding}) is $1-\\delta_1$ when employing $\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)}$, for some $\\delta_1\\in(0,1)$. However, since it employs $\\mathcal{U}_{s}^{(t+1)}$ instead, the success probability decreases by at most the spectral norm of the difference between the ``real'' and the ``ideal'' total unitaries. To be more precise, the ``ideal'' quantum minimum finding is a sequence of gates $\\mathcal{A} = U_1E_1U_2 E_2\\cdots U_{N}E_N$, where $U_i \\in \\{\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)},\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)\\dagger}\\}$, $E_i$ is a circuit of elementary gates, and $N = c\\sqrt{A}\\log\\frac{1}{\\delta_1}$ with $c$ constant is the number of queries to $\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)}$. The ``real'' implementation, on the other hand, is $\\widetilde{\\mathcal{A}} = \\widetilde{U}_1E_1\\widetilde{U}_2 E_2\\cdots \\widetilde{U}_{N}E_N$, where $\\widetilde{U}_i \\in \\{\\mathcal{U}_{s}^{(t+1)},\\mathcal{U}_{s}^{(t+1)\\dagger}\\}$. Then $\\|\\mathcal{A} - \\widetilde{\\mathcal{A}}\\| \\leq c\\sqrt{A}\\log\\!\\big(\\frac{1}{\\delta_1}\\big)\\|\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)} - \\mathcal{U}_{s}^{(t+1)}\\| \\leq c\\sqrt{A}\\log\\!\\big(\\frac{1}{\\delta_1}\\big)\\sqrt{2\\delta_2}$ and the failure probability is $\\delta_1 + c\\sqrt{A}\\log\\frac{1}{\\delta_1}\\sqrt{2\\delta_2}$. By taking $\\delta_1 = O\\big(\\frac{\\delta}{H|\\mathcal{S}_n|}\\big)$ and $\\delta_2 = O\\big(\\frac{\\delta_1^2}{A\\log^2(1/\\delta_1)}\\big)$, the failure probability in outputting $\\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}$ is at most $\\frac{\\delta}{H|\\mathcal{S}_n|}$. By a usual union bound over all $s\\in\\mathcal{S}_n$ and $t\\in[H]$, the failure probability is at most $\\delta$.\\footnote{Wang \\emph{et al.}\\ employ a similar but slightly incorrect argument for the failure probability~\\cite[Theorem~6]{wang2021quantum}. More precisely, they ignore the failure probability $\\delta_1$ coming from quantum minimum finding, or rather, assume it to be constant. This incorrectly leads to ignoring the factor $\\log\\frac{1}{\\delta_1}$ in their runtime~\\cite[Theorem~7]{wang2021quantum}.}\n    \n    Regarding the query complexity, for each $s\\in\\mathcal{S}_n$, one call to the unitary $\\mathcal{U}_s^{(t+1)}$ uses $O\\big(\\frac{\\|u_{t+1}\\|_\\infty}{\\varepsilon/H}\\log\\frac{1}{\\delta_2}\\big) = O\\big(\\frac{H^2}{\\varepsilon}\\log\\frac{H|\\mathcal{S}_n|A}{\\delta}\\big)$ queries to $\\mathcal{O}_p$, while quantum maximum finding makes $O\\big(\\sqrt{A}\\log\\frac{1}{\\delta_1}\\big) = O\\big(\\sqrt{A}\\log\\frac{H|\\mathcal{S}_n|}{\\delta}\\big)$ queries to $\\mathcal{U}_s^{(t+1)}$. Summing over all $\\mathcal{S}_n\\times[H]$, the total query complexity is\n    %\n    \\[\n        O\\bigg(\\frac{H^3|\\mathcal{S}_n|\\sqrt{A}}{\\varepsilon}\\log\\frac{H|\\mathcal{S}_n|A}{\\delta}\\log\\frac{H|\\mathcal{S}_n|}{\\delta}\\bigg). \\qedhere\n    \\]\n\\end{proof}\n    The proof is by induction on $t\\in[H]$t\\in[H]. We start by analysing the quantity $\\widehat{u}_t$\\widehat{u}_t in \\Cref{line:quantum_simple} at time step $t\\in[H]$t\\in[H]. First notice that, for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n and $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s),\n    \n        |(\\mathcal{L}u_{t+1}t+1)(s) - (\\mathcal{L}u_{t+1}t+1)(x)| &\\leq |r(s,a) - r(x,a)| + \\left|\\int_{\\mathcal{X}}\\mathcal{X}(p(\\rd x'|s,a) - p(\\rd x'|x,a))u_{t+1}t+1(x') \\right| \\\\\n        &\\leq (1+H)Ln^{-\\alpha}-\\alpha, \\tag{$\\|u_{t+1}\\|_\\infty \\leq H$ by induction}$\\|u_{t+1}\\|_\\infty \\leq H$\\|u_{t+1}t+1\\|_\\infty \\leq H by induction\n    \n    where $a = \\argmax_{a'\\in\\mathcal{A}}(\\mathcal{L}_{a'}u_{t+1})(s)$a = \\argmax_{a'\\in\\mathcal{A}}a'\\in\\mathcal{A}(\\mathcal{L}_{a'}a'u_{t+1}t+1)(s).\n    With probability $1 - \\frac{\\delta}{H|\\mathcal{S}_n|}$1 - \\frac{\\delta}{H|\\mathcal{S}_n|}, $|\\widehat{u}_t(s) - (\\mathcal{L}u_{t+1})(s)| \\leq \\frac{\\varepsilon}{2H}$|\\widehat{u}_t(s) - (\\mathcal{L}u_{t+1}t+1)(s)| \\leq \\frac{\\varepsilon}{2H} and thus $u_t(x) := \\widehat{u}_t(s) - \\frac{\\varepsilon}{2H} - (1+H)Ln^{-\\alpha}$u_t(x) := \\widehat{u}_t(s) - \\frac{\\varepsilon}{2H} - (1+H)Ln^{-\\alpha}-\\alpha for $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n and $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s) is such that\n    \\label{eq:simple_quantum_eq1}\n        u_t(x) \\leq (\\mathcal{L}u_{t+1}t+1)(x) \\quad\\text{and}\\quad\n        u_t(x) \\geq (\\mathcal{L}u_{t+1}t+1)(x) - \\frac{\\varepsilon}{H} - 2(1+H)Ln^{-\\alpha}-\\alpha.\n    \n    From here, proving the inequality $u_t(x) \\leq V_t^\\pi(x)$u_t(x) \\leq V_t^\\pi(x) is done exactly as in \\Cref{thr:classical_finite-horizon_sampling,thr:quantum_finite-horizon} as long as $u_t(x) \\leq (\\mathcal{L}u_{t+1})(x)$u_t(x) \\leq (\\mathcal{L}u_{t+1}t+1)(x) (\\Cref{eq:simple_quantum_eq1}) and $u_{t+1}(x) \\leq u_t(x)$u_{t+1}t+1(x) \\leq u_t(x), which is enforced by design (\\Cref{line:simple_quantum_enforcement}). To prove that $V_t^\\ast(x) - \\varepsilon - (1+H)HLn^{-\\alpha} \\leq u_t(x)$V_t^\\ast(x) - \\varepsilon - (1+H)HLn^{-\\alpha}-\\alpha \\leq u_t(x), we follow the proof of \\Cref{thr:classical_finite-horizon_sampling,thr:quantum_finite-horizon},\n    \n        V_t^\\ast(x) - u_t(x) &\\leq (\\mathcal{L}V^\\ast_{t+1}t+1)(x) - (\\mathcal{L}u_{t+1}t+1)(x) + \\frac{\\varepsilon}{H} + 2(1+H)Ln^{-\\alpha}-\\alpha\\\\\n        &\\leq \\int_{\\mathcal{X}}\\mathcal{X} p(\\rd x'|x, \\pi_t^\\ast(x))\\big(V^\\ast_{t+1}t+1(x') - u_{t+1}t+1(x')\\big) + \\frac{\\varepsilon}{H} + 2(1+H)Ln^{-\\alpha}-\\alpha.\n    \n    Solving the above recursion and using $\\|\\!\\sum_{t'=t}^{H-1} \\idotsint_{\\mathcal{X}^{t'-t}} p_{\\pi_{t}^\\ast}(\\rd x_{t+1}|x)\\cdots p_{\\pi_{t'-1}^\\ast}(\\rd x_{t'}|x_{t'-1})\\|_\\infty \\!\\leq\\! H$\\|\\!\\sum_{t'=t}t'=t^{H-1}H-1 \\idotsint_{\\mathcal{X}^{t'-t}}\\mathcal{X}^{t'-t}t'-t p_{\\pi_{t}^\\ast}\\pi_{t}t^\\ast(\\rd x_{t+1}t+1|x)\\cdots p_{\\pi_{t'-1}^\\ast}\\pi_{t'-1}t'-1^\\ast(\\rd x_{t'}t'|x_{t'-1}t'-1)\\|_\\infty \\!\\leq\\! H, we get that\n    \n        V_t^\\ast(x) - u_t(x) \\leq \\varepsilon + 2(1+H)HLn^{-\\alpha}-\\alpha,\n    \n    which concludes the proof of correctness. \n\n    We now analyse the success probability of \\Cref{algo:quantum_backward_recursion2}. To do so, we must analyse how quantum oracles fail (see~\\cite[Appendix~A]{chen2023quantum} for a similar argument as follows). Ideally, we would like to implement the unitary $\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)}:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|\\mu_{t+1}(s,a)\\rangle$\\mathcal{U}_{s, {\\rm ideal}}s, {\\rm ideal}\\rm ideal^{(t+1)}(t+1):|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|\\mu_{t+1}t+1(s,a)\\rangle where $|\\mu_{t+1}(s,a)\\rangle$|\\mu_{t+1}t+1(s,a)\\rangle contains the approximation $|\\mu_{t+1}(s,a) - \\int_{\\mathcal{X}}p(\\rd x'|s,a) u_{t+1}(x')| \\leq \\frac{\\varepsilon}{2H}$|\\mu_{t+1}t+1(s,a) - \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|s,a) u_{t+1}t+1(x')| \\leq \\frac{\\varepsilon}{2H}. In practice, however, we implement the unitary $\\mathcal{U}_{s}^{(t+1)}:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle(\\sqrt{1-\\delta_2}|\\mu_{t+1}(s,a)\\rangle + \\sqrt{\\delta_2}|\\phi_a\\rangle)$\\mathcal{U}_{s}s^{(t+1)}(t+1):|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle(\\sqrt{1-\\delta_2}|\\mu_{t+1}t+1(s,a)\\rangle + \\sqrt{\\delta_2}|\\phi_a\\rangle) for some $\\delta_2 \\in(0,1)$\\delta_2 \\in(0,1), where the register $|\\mu_{t+1}(s,a)\\rangle$|\\mu_{t+1}t+1(s,a)\\rangle holds the desired approximation $\\mu_{t+1}(s,a)$\\mu_{t+1}t+1(s,a) and $|\\phi_a\\rangle$|\\phi_a\\rangle is a normalised quantum state orthogonal to $|\\mu_{t+1}(s,a)\\rangle$|\\mu_{t+1}t+1(s,a)\\rangle. Notice that\n    \n        \\forall a\\in\\mathcal{A}: \\qquad \\|(\\mathcal{U}_{s, {\\rm ideal}}s, {\\rm ideal}\\rm ideal^{(t+1)}(t+1) - \\mathcal{U}_{s}s^{(t+1)}(t+1))|a\\rangle|\\bar{0}\\rangle\\| = \\sqrt{(1 - \\sqrt{1-\\delta_2})^2 + \\delta_2} = \\sqrt{2-2\\sqrt{1-\\delta_2}} \\leq \\sqrt{2\\delta_2},\n    \n    using that $\\sqrt{1-\\delta_2} \\geq 1 - \\delta_2$\\sqrt{1-\\delta_2} \\geq 1 - \\delta_2. Since quantum minimum finding does not take into account the action of $\\mathcal{U}_{s}^{(t+1)}$\\mathcal{U}_{s}s^{(t+1)}(t+1) onto states of the form $|a\\rangle|\\bar{0}^\\perp\\rangle$|a\\rangle|\\bar{0}^\\perp\\rangle for $|\\bar{0}^\\perp\\rangle$|\\bar{0}^\\perp\\rangle orthogonal to $|\\bar{0}\\rangle$|\\bar{0}\\rangle, we can, without of loss of generality, assume that $\\|\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)} - \\mathcal{U}_{s}^{(t+1)}\\| \\leq \\sqrt{2\\delta_2}$\\|\\mathcal{U}_{s, {\\rm ideal}}s, {\\rm ideal}\\rm ideal^{(t+1)}(t+1) - \\mathcal{U}_{s}s^{(t+1)}(t+1)\\| \\leq \\sqrt{2\\delta_2}. The success probability of quantum minimum finding (\\Cref{fact:quantum_minimum_finding}) is $1-\\delta_1$1-\\delta_1 when employing $\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)}$\\mathcal{U}_{s, {\\rm ideal}}s, {\\rm ideal}\\rm ideal^{(t+1)}(t+1), for some $\\delta_1\\in(0,1)$\\delta_1\\in(0,1). However, since it employs $\\mathcal{U}_{s}^{(t+1)}$\\mathcal{U}_{s}s^{(t+1)}(t+1) instead, the success probability decreases by at most the spectral norm of the difference between the ``real'' and the ``ideal'' total unitaries. To be more precise, the ``ideal'' quantum minimum finding is a sequence of gates $\\mathcal{A} = U_1E_1U_2 E_2\\cdots U_{N}E_N$\\mathcal{A} = U_1E_1U_2 E_2\\cdots U_{N}NE_N, where $U_i \\in \\{\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)},\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)\\dagger}\\}$U_i \\in \\{\\mathcal{U}_{s, {\\rm ideal}}s, {\\rm ideal}\\rm ideal^{(t+1)}(t+1),\\mathcal{U}_{s, {\\rm ideal}}s, {\\rm ideal}\\rm ideal^{(t+1)\\dagger}(t+1)\\dagger\\}, $E_i$E_i is a circuit of elementary gates, and $N = c\\sqrt{A}\\log\\frac{1}{\\delta_1}$N = c\\sqrt{A}\\log\\frac{1}{\\delta_1} with $c$c constant is the number of queries to $\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)}$\\mathcal{U}_{s, {\\rm ideal}}s, {\\rm ideal}\\rm ideal^{(t+1)}(t+1). The ``real'' implementation, on the other hand, is $\\widetilde{\\mathcal{A}} = \\widetilde{U}_1E_1\\widetilde{U}_2 E_2\\cdots \\widetilde{U}_{N}E_N$\\widetilde{\\mathcal{A}} = \\widetilde{U}_1E_1\\widetilde{U}_2 E_2\\cdots \\widetilde{U}_{N}NE_N, where $\\widetilde{U}_i \\in \\{\\mathcal{U}_{s}^{(t+1)},\\mathcal{U}_{s}^{(t+1)\\dagger}\\}$\\widetilde{U}_i \\in \\{\\mathcal{U}_{s}s^{(t+1)}(t+1),\\mathcal{U}_{s}s^{(t+1)\\dagger}(t+1)\\dagger\\}. Then $\\|\\mathcal{A} - \\widetilde{\\mathcal{A}}\\| \\leq c\\sqrt{A}\\log\\!\\big(\\frac{1}{\\delta_1}\\big)\\|\\mathcal{U}_{s, {\\rm ideal}}^{(t+1)} - \\mathcal{U}_{s}^{(t+1)}\\| \\leq c\\sqrt{A}\\log\\!\\big(\\frac{1}{\\delta_1}\\big)\\sqrt{2\\delta_2}$\\|\\mathcal{A} - \\widetilde{\\mathcal{A}}\\| \\leq c\\sqrt{A}\\log\\!\\big(\\frac{1}{\\delta_1}\\big)\\|\\mathcal{U}_{s, {\\rm ideal}}s, {\\rm ideal}\\rm ideal^{(t+1)}(t+1) - \\mathcal{U}_{s}s^{(t+1)}(t+1)\\| \\leq c\\sqrt{A}\\log\\!\\big(\\frac{1}{\\delta_1}\\big)\\sqrt{2\\delta_2} and the failure probability is $\\delta_1 + c\\sqrt{A}\\log\\frac{1}{\\delta_1}\\sqrt{2\\delta_2}$\\delta_1 + c\\sqrt{A}\\log\\frac{1}{\\delta_1}\\sqrt{2\\delta_2}. By taking $\\delta_1 = O\\big(\\frac{\\delta}{H|\\mathcal{S}_n|}\\big)$\\delta_1 = O\\big(\\frac{\\delta}{H|\\mathcal{S}_n|}\\big) and $\\delta_2 = O\\big(\\frac{\\delta_1^2}{A\\log^2(1/\\delta_1)}\\big)$\\delta_2 = O\\big(\\frac{\\delta_1^2}{A\\log^2(1/\\delta_1)}\\big), the failure probability in outputting $\\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}$\\max_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{r(s,a) + \\mu_{t+1}t+1(s,a)\\} is at most $\\frac{\\delta}{H|\\mathcal{S}_n|}$\\frac{\\delta}{H|\\mathcal{S}_n|}. By a usual union bound over all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n and $t\\in[H]$t\\in[H], the failure probability is at most $\\delta$\\delta.\\footnote{Wang \\emph{et al.}\\ employ a similar but slightly incorrect argument for the failure probability~\\cite[Theorem~6]{wang2021quantum}. More precisely, they ignore the failure probability $\\delta_1$ coming from quantum minimum finding, or rather, assume it to be constant. This incorrectly leads to ignoring the factor $\\log\\frac{1}{\\delta_1}$ in their runtime~\\cite[Theorem~7]{wang2021quantum}.}\n    \n    Regarding the query complexity, for each $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n, one call to the unitary $\\mathcal{U}_s^{(t+1)}$\\mathcal{U}_s^{(t+1)}(t+1) uses $O\\big(\\frac{\\|u_{t+1}\\|_\\infty}{\\varepsilon/H}\\log\\frac{1}{\\delta_2}\\big) = O\\big(\\frac{H^2}{\\varepsilon}\\log\\frac{H|\\mathcal{S}_n|A}{\\delta}\\big)$O\\big(\\frac{\\|u_{t+1}\\|_\\infty}{\\varepsilon/H}\\log\\frac{1}{\\delta_2}\\big) = O\\big(\\frac{H^2}{\\varepsilon}\\log\\frac{H|\\mathcal{S}_n|A}{\\delta}\\big) queries to $\\mathcal{O}_p$\\mathcal{O}_p, while quantum maximum finding makes $O\\big(\\sqrt{A}\\log\\frac{1}{\\delta_1}\\big) = O\\big(\\sqrt{A}\\log\\frac{H|\\mathcal{S}_n|}{\\delta}\\big)$O\\big(\\sqrt{A}\\log\\frac{1}{\\delta_1}\\big) = O\\big(\\sqrt{A}\\log\\frac{H|\\mathcal{S}_n|}{\\delta}\\big) queries to $\\mathcal{U}_s^{(t+1)}$\\mathcal{U}_s^{(t+1)}(t+1). Summing over all $\\mathcal{S}_n\\times[H]$\\mathcal{S}_n\\times[H], the total query complexity is\n    \\[\n        O\\bigg(\\frac{H^3|\\mathcal{S}_n|\\sqrt{A}}{\\varepsilon}\\log\\frac{H|\\mathcal{S}_n|A}{\\delta}\\log\\frac{H|\\mathcal{S}_n|}{\\delta}\\bigg). \\qedhere\n    \\]\n        O\\bigg(\\frac{H^3|\\mathcal{S}_n|\\sqrt{A}}{\\varepsilon}\\log\\frac{H|\\mathcal{S}_n|A}{\\delta}\\log\\frac{H|\\mathcal{S}_n|}{\\delta}\\bigg). \\qedhere\n    \n\n\n\n\n", "appendix": false}, "Computing optimal policies for infinite-horizon MDPs": {"content": "\n\\label{sec:value_iteration}\n\nIn this section, we focus on finding approximate optimal policies to infinite-horizon weakly communicating MDPs $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle such that $\\operatorname{sp}(h^\\ast) \\leq \\Lambda$\\operatorname{sp}sp(h^\\ast) \\leq \\Lambda. Several classical algorithms have been proposed for such task, one of them main ones being value iteration~\\cite{kearns1998finite, lutter2021value, hartmanns2020optimistic, bertsekas1998new, quatmann2018sound, shani2007forward, weng2013interactive}, which is quite similar to the backward induction algorithm covered in the last section: starting from some initial function $u_{0}\\in\\mathscr{B}(\\mathcal{X})$u_{0}0\\in\\mathscr{B}(\\mathcal{X}), normally $u_{0} \\equiv 0$u_{0}0 \\equiv 0, generate a sequence of functions $(u_{t})_{t\\in\\mathbb{N}}$(u_{t}t)_{t\\in\\mathbb{N}}t\\in\\mathbb{N} according to the update rule $u_{t+1} = \\mathcal{L}u_{t}$u_{t+1}t+1 = \\mathcal{L}u_{t}t. It is known that when $\\operatorname{sp}(u_{t+1} - u_{t}) \\leq \\varepsilon$\\operatorname{sp}sp(u_{t+1}t+1 - u_{t}t) \\leq \\varepsilon, the greedy policy with respect to $u_{t}$u_{t}t is $\\varepsilon$\\varepsilon-optimal~\\cite[Theorem~9.4.5]{puterman2014markov}.\n\nHere we are interested in robust versions of value iteration wherein errors in computing $\\mathcal{L}{u}_{t}$\\mathcal{L}{u}u_{t}t are taken into account. Approximate versions of value iteration have been studied and are used in various settings~\\cite{farahmand2010error, mann2015approximate, ernst2005approximate, munos2007performance, de2000existence, van2006performance}, and here we consider a robust analogue of value iteration which differs from its standard implementation by generating a sequence of functions $({u}_{t})_{t\\in\\mathbb{N}}$({u}u_{t}t)_{t\\in\\mathbb{N}}t\\in\\mathbb{N} such that\n\\begin{align}\\label{eq:approximate_value_iteration}\n    \\|{{u}}_{t+1} - \\mathcal{L}{{u}}_{t}\\|_\\infty \\leq \\varepsilon_u  \\quad\\text{for a given}~\\varepsilon_u \\geq 0, \\quad\\text{where}~t\\in\\mathbb{N}.\n\\end{align}\\begin{align}\\label{eq:approximate_value_iteration}\n    \\|{{u}}_{t+1} - \\mathcal{L}{{u}}_{t}\\|_\\infty \\leq \\varepsilon_u  \\quad\\text{for a given}~\\varepsilon_u \\geq 0, \\quad\\text{where}~t\\in\\mathbb{N}.\n\\end{align}\\label{eq:approximate_value_iteration}\n    \\|{{u}}{u}u_{t+1}t+1 - \\mathcal{L}{{u}}{u}u_{t}t\\|_\\infty \\leq \\varepsilon_u  \\quad\\text{for a given}~\\varepsilon_u \\geq 0, \\quad\\text{where}~t\\in\\mathbb{N}.\n\nThe error $\\varepsilon_u$\\varepsilon_u could come from, e.g., approximating $\\int_{\\mathcal{X}}p(\\text{d}x'|x,a)u_{t}(x')$\\int_{\\mathcal{X}}\\mathcal{X}p(\\text{d}x'|x,a)u_{t}t(x') or the maximum over $a\\in\\mathcal{A}$a\\in\\mathcal{A}. In the following, we prove a few convergence results for the robust value iteration, starting by proving that any sequence of vectors  generated as in \\Cref{eq:approximate_value_iteration} using some span contraction have bounded span.\n\n\\begin{lemma}\\label{lem:stopping_criteria}\n    Let $\\epsilon \\geq 0$ and $\\mathcal{N}:\\mathscr{B}(\\mathcal{X})\\to\\mathscr{B}(\\mathcal{X})$ a $1$-stage $\\nu$-span contraction for some $\\nu\\in[0,1)$. Let $({u}_{t})_{t\\in\\mathbb{N}}$ be a sequence of functions such that $\\|{u}_{t+1} - \\mathcal{N} {u}_{t}\\|_\\infty \\leq \\epsilon$ $\\forall t\\in\\mathbb{N}$. Then\n    %\n    \\begin{align*}\n        \\operatorname{sp}({u}_{t+1} -  {u}_{t}) \\leq \\nu^t(\\operatorname{sp}(\\mathcal{N}{u}_{0} - {u}_{0}) + 2\\epsilon) + 4\\epsilon \\frac{1-\\nu^t}{1-\\nu} \\qquad\\text{for all}~ t\\in\\mathbb{N}.\n    \\end{align*}\n\\end{lemma}\\begin{lemma}\\label{lem:stopping_criteria}\n    Let $\\epsilon \\geq 0$ and $\\mathcal{N}:\\mathscr{B}(\\mathcal{X})\\to\\mathscr{B}(\\mathcal{X})$ a $1$-stage $\\nu$-span contraction for some $\\nu\\in[0,1)$. Let $({u}_{t})_{t\\in\\mathbb{N}}$ be a sequence of functions such that $\\|{u}_{t+1} - \\mathcal{N} {u}_{t}\\|_\\infty \\leq \\epsilon$ $\\forall t\\in\\mathbb{N}$. Then\n    %\n    \\begin{align*}\n        \\operatorname{sp}({u}_{t+1} -  {u}_{t}) \\leq \\nu^t(\\operatorname{sp}(\\mathcal{N}{u}_{0} - {u}_{0}) + 2\\epsilon) + 4\\epsilon \\frac{1-\\nu^t}{1-\\nu} \\qquad\\text{for all}~ t\\in\\mathbb{N}.\n    \\end{align*}\n\\end{lemma}\\label{lem:stopping_criteria}\n    Let $\\epsilon \\geq 0$\\epsilon \\geq 0 and $\\mathcal{N}:\\mathscr{B}(\\mathcal{X})\\to\\mathscr{B}(\\mathcal{X})$\\mathcal{N}:\\mathscr{B}(\\mathcal{X})\\to\\mathscr{B}(\\mathcal{X}) a $1$1-stage $\\nu$\\nu-span contraction for some $\\nu\\in[0,1)$\\nu\\in[0,1). Let $({u}_{t})_{t\\in\\mathbb{N}}$({u}u_{t}t)_{t\\in\\mathbb{N}}t\\in\\mathbb{N} be a sequence of functions such that $\\|{u}_{t+1} - \\mathcal{N} {u}_{t}\\|_\\infty \\leq \\epsilon$\\|{u}u_{t+1}t+1 - \\mathcal{N} {u}u_{t}t\\|_\\infty \\leq \\epsilon $\\forall t\\in\\mathbb{N}$\\forall t\\in\\mathbb{N}. Then\n    \n        \\operatorname{sp}sp({u}u_{t+1}t+1 -  {u}u_{t}t) \\leq \\nu^t(\\operatorname{sp}sp(\\mathcal{N}{u}u_{0}0 - {u}u_{0}0) + 2\\epsilon) + 4\\epsilon \\frac{1-\\nu^t}{1-\\nu} \\qquad\\text{for all}~ t\\in\\mathbb{N}.\n    \n\n\\begin{proof}\n    We prove the main result by induction on $t$. For $t=0$,  $\\operatorname{sp}({{u}}_1 -  {{u}}_0) \\leq 2\\epsilon + \\operatorname{sp}(\\mathcal{N}{{u}}_{0} - {{u}}_{0})$ using that $\\operatorname{sp}({v}) \\leq 2\\|{v}\\|_\\infty$ for any ${v}\\in\\mathscr{B}(\\mathcal{X})$. For $t > 0$,\n    %\n    \\begin{align*}\n        \\operatorname{sp}({{u}}_{t+1} -  {{u}}_{t}) &\\leq \\operatorname{sp}({{u}}_{t+1} - \\mathcal{N} {{u}}_{t}) + \\operatorname{sp}( {{u}}_{t} - \\mathcal{N}{{u}}_{t-1}) + \\operatorname{sp}(\\mathcal{N} {{u}}_{t} - \\mathcal{N}{{u}}_{t-1})  \\tag{triangle inequality}\\\\\n        &\\leq 4\\epsilon + \\nu \\operatorname{sp}({{u}}_{t} -  {{u}}_{t-1}) \\tag{$1$-stage $\\nu$-span contraction and $\\|{u}_{t+1} - \\mathcal{N} {u}_{t}\\|_\\infty \\leq \\epsilon$}\\\\\n        &\\leq 4\\epsilon + \\nu\\left(\\nu^{t-1}(\\operatorname{sp}(\\mathcal{N}{{u}}_{0} - {{u}}_{0})+2\\epsilon) + 4\\epsilon \\frac{1-\\nu^{t-1}}{1-\\nu}\\right)\\tag{induction hypothesis}\\\\\n        &= \\nu^t(\\operatorname{sp}(\\mathcal{N}{{u}}_{0} - {{u}}_{0})+2\\epsilon) + 4\\epsilon\\frac{1-\\nu^t}{1-\\nu}. \\tag*{\\qedhere}\n    \\end{align*}\n\\end{proof}\\begin{proof}\n    We prove the main result by induction on $t$. For $t=0$,  $\\operatorname{sp}({{u}}_1 -  {{u}}_0) \\leq 2\\epsilon + \\operatorname{sp}(\\mathcal{N}{{u}}_{0} - {{u}}_{0})$ using that $\\operatorname{sp}({v}) \\leq 2\\|{v}\\|_\\infty$ for any ${v}\\in\\mathscr{B}(\\mathcal{X})$. For $t > 0$,\n    %\n    \\begin{align*}\n        \\operatorname{sp}({{u}}_{t+1} -  {{u}}_{t}) &\\leq \\operatorname{sp}({{u}}_{t+1} - \\mathcal{N} {{u}}_{t}) + \\operatorname{sp}( {{u}}_{t} - \\mathcal{N}{{u}}_{t-1}) + \\operatorname{sp}(\\mathcal{N} {{u}}_{t} - \\mathcal{N}{{u}}_{t-1})  \\tag{triangle inequality}\\\\\n        &\\leq 4\\epsilon + \\nu \\operatorname{sp}({{u}}_{t} -  {{u}}_{t-1}) \\tag{$1$-stage $\\nu$-span contraction and $\\|{u}_{t+1} - \\mathcal{N} {u}_{t}\\|_\\infty \\leq \\epsilon$}\\\\\n        &\\leq 4\\epsilon + \\nu\\left(\\nu^{t-1}(\\operatorname{sp}(\\mathcal{N}{{u}}_{0} - {{u}}_{0})+2\\epsilon) + 4\\epsilon \\frac{1-\\nu^{t-1}}{1-\\nu}\\right)\\tag{induction hypothesis}\\\\\n        &= \\nu^t(\\operatorname{sp}(\\mathcal{N}{{u}}_{0} - {{u}}_{0})+2\\epsilon) + 4\\epsilon\\frac{1-\\nu^t}{1-\\nu}. \\tag*{\\qedhere}\n    \\end{align*}\n\\end{proof}\n    We prove the main result by induction on $t$t. For $t=0$t=0,  $\\operatorname{sp}({{u}}_1 -  {{u}}_0) \\leq 2\\epsilon + \\operatorname{sp}(\\mathcal{N}{{u}}_{0} - {{u}}_{0})$\\operatorname{sp}sp({{u}}{u}u_1 -  {{u}}{u}u_0) \\leq 2\\epsilon + \\operatorname{sp}sp(\\mathcal{N}{{u}}{u}u_{0}0 - {{u}}{u}u_{0}0) using that $\\operatorname{sp}({v}) \\leq 2\\|{v}\\|_\\infty$\\operatorname{sp}sp({v}v) \\leq 2\\|{v}v\\|_\\infty for any ${v}\\in\\mathscr{B}(\\mathcal{X})${v}v\\in\\mathscr{B}(\\mathcal{X}). For $t > 0$t > 0,\n    \n        \\operatorname{sp}sp({{u}}{u}u_{t+1}t+1 -  {{u}}{u}u_{t}t) &\\leq \\operatorname{sp}sp({{u}}{u}u_{t+1}t+1 - \\mathcal{N} {{u}}{u}u_{t}t) + \\operatorname{sp}sp( {{u}}{u}u_{t}t - \\mathcal{N}{{u}}{u}u_{t-1}t-1) + \\operatorname{sp}sp(\\mathcal{N} {{u}}{u}u_{t}t - \\mathcal{N}{{u}}{u}u_{t-1}t-1)  \\tag{triangle inequality}triangle inequality\\\\\n        &\\leq 4\\epsilon + \\nu \\operatorname{sp}sp({{u}}{u}u_{t}t -  {{u}}{u}u_{t-1}t-1) \\tag{$1$-stage $\\nu$-span contraction and $\\|{u}_{t+1} - \\mathcal{N} {u}_{t}\\|_\\infty \\leq \\epsilon$}$1$1-stage $\\nu$\\nu-span contraction and $\\|{u}_{t+1} - \\mathcal{N} {u}_{t}\\|_\\infty \\leq \\epsilon$\\|{u}u_{t+1}t+1 - \\mathcal{N} {u}u_{t}t\\|_\\infty \\leq \\epsilon\\\\\n        &\\leq 4\\epsilon + \\nu\\left(\\nu^{t-1}t-1(\\operatorname{sp}sp(\\mathcal{N}{{u}}{u}u_{0}0 - {{u}}{u}u_{0}0)+2\\epsilon) + 4\\epsilon \\frac{1-\\nu^{t-1}}{1-\\nu}\\right)\\tag{induction hypothesis}induction hypothesis\\\\\n        &= \\nu^t(\\operatorname{sp}sp(\\mathcal{N}{{u}}{u}u_{0}0 - {{u}}{u}u_{0}0)+2\\epsilon) + 4\\epsilon\\frac{1-\\nu^t}{1-\\nu}. \\tag*{\\qedhere}\\qedhere\n    \n\n\n\nIf the error per iteration $\\varepsilon_u$\\varepsilon_u is small enough, the above result tells us that after several iterations, robust value iteration obtains a pair of vectors ${{u}}_{t+1}${{u}}{u}u_{t+1}t+1 and ${{u}}_{t}${{u}}{u}u_{t}t such that $\\operatorname{sp}({{u}}_{t+1} - {{u}}_{t})$\\operatorname{sp}sp({{u}}{u}u_{t+1}t+1 - {{u}}{u}u_{t}t) is sufficiently small. It remains to show that, once a stopping criteria of the form\n\\begin{align*}\n    \\operatorname{sp}({{u}}_{t+1} - {{u}}_{t}) \\leq \\varepsilon_{s} \\quad\\text{for some}~\\varepsilon_{s} >0\n\\end{align*}\\begin{align*}\n    \\operatorname{sp}({{u}}_{t+1} - {{u}}_{t}) \\leq \\varepsilon_{s} \\quad\\text{for some}~\\varepsilon_{s} >0\n\\end{align*}\n    \\operatorname{sp}sp({{u}}{u}u_{t+1}t+1 - {{u}}{u}u_{t}t) \\leq \\varepsilon_{s}s \\quad\\text{for some}~\\varepsilon_{s}s >0\n\nis achieved, a corresponding policy $d_t^\\infty$d_t^\\infty associated with $u_{t+1}$u_{t+1}t+1 is close to optimal. The next result generalises~\\cite[Theorem~8.5.6]{puterman2014markov}.\n\n\\begin{theorem}\\label{lem:value_iteration_guarantees}\n    Let $\\varepsilon_u,\\varepsilon_s>0$. Let $({u}_{t})_{t\\in\\mathbb{N}}$ be any sequence of vectors such that $\\|{u}_{t+1} - \\mathcal{L} {u}_{t}\\|_\\infty \\leq \\varepsilon_u$ for all $t\\in\\mathbb{N}$. Assume that $\\operatorname{sp}({{u}}_{t+1} - {{u}}_{t}) \\leq \\varepsilon_s$ for some $\\varepsilon_s > 0$ and $t\\geq t_{\\varepsilon}\\in\\mathbb{N}$. Let $d_\\varepsilon\\in\\mathcal{D}^{\\rm D}$ such that $\\|u_{t_\\varepsilon+1} - \\mathcal{L}_{d_\\varepsilon}u_{t_\\varepsilon}\\|_\\infty \\leq \\varepsilon_u$. Define the quantity\n    %\n    \\begin{align*}\n        g^{\\varepsilon} := \\frac{1}{2}\\left(\\max_{x\\in\\mathcal{X}}\\{{u}_{t_{\\varepsilon}+1}(x) - {u}_{t_{\\varepsilon}}(x)\\} + \\min_{x\\in\\mathcal{X}}\\{{u}_{t_{\\varepsilon}+1}(x) - {u}_{t_{\\varepsilon}}(x)\\} \\right).\n    \\end{align*}\n    %\n    Then $\\|g^{d_\\varepsilon^\\infty} - g^\\varepsilon e\\|_\\infty \\leq \\varepsilon_u + \\frac{\\varepsilon_s}{2}$ and $|g^\\varepsilon - g^\\ast| \\leq \\varepsilon_u + \\frac{\\varepsilon_s}{2}$, and so $d_\\varepsilon^\\infty$ is a $(2\\varepsilon_u + \\varepsilon_s)$-optimal policy. %Moreover, if $\\mathcal{L}$ is a $1$-stage $\\nu$-span contraction, then $\\operatorname{sp}(h^\\ast - {{u}}_{t}) \\leq \\frac{2\\varepsilon_u + \\varepsilon_s}{1-\\nu}$ for all $t\\geq t_\\varepsilon$.\n\\end{theorem}\\begin{theorem}\\label{lem:value_iteration_guarantees}\n    Let $\\varepsilon_u,\\varepsilon_s>0$. Let $({u}_{t})_{t\\in\\mathbb{N}}$ be any sequence of vectors such that $\\|{u}_{t+1} - \\mathcal{L} {u}_{t}\\|_\\infty \\leq \\varepsilon_u$ for all $t\\in\\mathbb{N}$. Assume that $\\operatorname{sp}({{u}}_{t+1} - {{u}}_{t}) \\leq \\varepsilon_s$ for some $\\varepsilon_s > 0$ and $t\\geq t_{\\varepsilon}\\in\\mathbb{N}$. Let $d_\\varepsilon\\in\\mathcal{D}^{\\rm D}$ such that $\\|u_{t_\\varepsilon+1} - \\mathcal{L}_{d_\\varepsilon}u_{t_\\varepsilon}\\|_\\infty \\leq \\varepsilon_u$. Define the quantity\n    %\n    \\begin{align*}\n        g^{\\varepsilon} := \\frac{1}{2}\\left(\\max_{x\\in\\mathcal{X}}\\{{u}_{t_{\\varepsilon}+1}(x) - {u}_{t_{\\varepsilon}}(x)\\} + \\min_{x\\in\\mathcal{X}}\\{{u}_{t_{\\varepsilon}+1}(x) - {u}_{t_{\\varepsilon}}(x)\\} \\right).\n    \\end{align*}\n    %\n    Then $\\|g^{d_\\varepsilon^\\infty} - g^\\varepsilon e\\|_\\infty \\leq \\varepsilon_u + \\frac{\\varepsilon_s}{2}$ and $|g^\\varepsilon - g^\\ast| \\leq \\varepsilon_u + \\frac{\\varepsilon_s}{2}$, and so $d_\\varepsilon^\\infty$ is a $(2\\varepsilon_u + \\varepsilon_s)$-optimal policy. %Moreover, if $\\mathcal{L}$ is a $1$-stage $\\nu$-span contraction, then $\\operatorname{sp}(h^\\ast - {{u}}_{t}) \\leq \\frac{2\\varepsilon_u + \\varepsilon_s}{1-\\nu}$ for all $t\\geq t_\\varepsilon$.\n\\end{theorem}\\label{lem:value_iteration_guarantees}\n    Let $\\varepsilon_u,\\varepsilon_s>0$\\varepsilon_u,\\varepsilon_s>0. Let $({u}_{t})_{t\\in\\mathbb{N}}$({u}u_{t}t)_{t\\in\\mathbb{N}}t\\in\\mathbb{N} be any sequence of vectors such that $\\|{u}_{t+1} - \\mathcal{L} {u}_{t}\\|_\\infty \\leq \\varepsilon_u$\\|{u}u_{t+1}t+1 - \\mathcal{L} {u}u_{t}t\\|_\\infty \\leq \\varepsilon_u for all $t\\in\\mathbb{N}$t\\in\\mathbb{N}. Assume that $\\operatorname{sp}({{u}}_{t+1} - {{u}}_{t}) \\leq \\varepsilon_s$\\operatorname{sp}sp({{u}}{u}u_{t+1}t+1 - {{u}}{u}u_{t}t) \\leq \\varepsilon_s for some $\\varepsilon_s > 0$\\varepsilon_s > 0 and $t\\geq t_{\\varepsilon}\\in\\mathbb{N}$t\\geq t_{\\varepsilon}\\varepsilon\\in\\mathbb{N}. Let $d_\\varepsilon\\in\\mathcal{D}^{\\rm D}$d_\\varepsilon\\in\\mathcal{D}^{\\rm D}\\rm D such that $\\|u_{t_\\varepsilon+1} - \\mathcal{L}_{d_\\varepsilon}u_{t_\\varepsilon}\\|_\\infty \\leq \\varepsilon_u$\\|u_{t_\\varepsilon+1}t_\\varepsilon+1 - \\mathcal{L}_{d_\\varepsilon}d_\\varepsilonu_{t_\\varepsilon}t_\\varepsilon\\|_\\infty \\leq \\varepsilon_u. Define the quantity\n    \n        g^{\\varepsilon}\\varepsilon := \\frac{1}{2}\\left(\\max_{x\\in\\mathcal{X}}x\\in\\mathcal{X}\\{{u}u_{t_{\\varepsilon}+1}t_{\\varepsilon}\\varepsilon+1(x) - {u}u_{t_{\\varepsilon}}t_{\\varepsilon}\\varepsilon(x)\\} + \\min_{x\\in\\mathcal{X}}x\\in\\mathcal{X}\\{{u}u_{t_{\\varepsilon}+1}t_{\\varepsilon}\\varepsilon+1(x) - {u}u_{t_{\\varepsilon}}t_{\\varepsilon}\\varepsilon(x)\\} \\right).\n    \n    Then $\\|g^{d_\\varepsilon^\\infty} - g^\\varepsilon e\\|_\\infty \\leq \\varepsilon_u + \\frac{\\varepsilon_s}{2}$\\|g^{d_\\varepsilon^\\infty}d_\\varepsilon^\\infty - g^\\varepsilon e\\|_\\infty \\leq \\varepsilon_u + \\frac{\\varepsilon_s}{2} and $|g^\\varepsilon - g^\\ast| \\leq \\varepsilon_u + \\frac{\\varepsilon_s}{2}$|g^\\varepsilon - g^\\ast| \\leq \\varepsilon_u + \\frac{\\varepsilon_s}{2}, and so $d_\\varepsilon^\\infty$d_\\varepsilon^\\infty is a $(2\\varepsilon_u + \\varepsilon_s)$(2\\varepsilon_u + \\varepsilon_s)-optimal policy. \n\\begin{proof}\n    By the definition of ${g}^{d_\\varepsilon^\\infty}$, the stochastic kernel $p_{d_\\varepsilon}\\!(\\cdot|x)$ is positive Harris recurrent with unique invariant probability measure $\\mu_x^{d_\\varepsilon}$ such that ${g}^{d_\\varepsilon^\\infty} \\!=\\! \\int_{\\mathcal{X}} \\! {r}_{d_\\varepsilon}(x')\\mu^{d_\\varepsilon}_x(\\text{d}x')$~\\cite[Theorem~2.5]{saldi2017asymptotic}. Furthermore, since $\\int_{\\mathcal{X}}\\mu^{d_\\varepsilon}_x(\\text{d}x') p_{d_\\varepsilon}(\\cdot|x) = \\mu^{d_\\varepsilon}_x(\\cdot)$, then \n    %\n    \\begin{align*}\n        {g}^{d_\\varepsilon^\\infty}(x) &= \\int_{\\mathcal{X}} \\mu_x^{d_\\varepsilon}(\\text{d}x') \\left(r_{d_\\varepsilon}(x') + \\int_{\\mathcal{X}} p_{d_\\varepsilon}(\\text{d}x''|x') u_{t_\\varepsilon}(x'') - {{u}}_{t_\\varepsilon}(x')\\right) \\\\\n        &= \\int_{\\mathcal{X}} \\mu_x^{d_\\varepsilon}(\\text{d}x')\\big( (\\mathcal{L}_{d_\\varepsilon}{{u}}_{t_\\varepsilon})(x') - {{u}}_{t_\\varepsilon}(x')\\big).\n    \\end{align*}\n    %\n    Since $\\inf_{x'\\in\\mathcal{X}}\\{{u}_{t_\\varepsilon+1}(x') - {u}_{t_\\varepsilon}(x')\\} \\leq {{u}}_{t_\\varepsilon+1}(x) - {{u}}_{t_\\varepsilon}(x) \\leq \\sup_{x'\\in\\mathcal{X}}\\{{u}_{t_\\varepsilon+1}(x') - {u}_{t_\\varepsilon+1}(x')\\} $ for all $x\\in\\mathcal{X}$, then\n    %\n    \\begin{align*}\n        \\inf_{x'\\in\\mathcal{X}}\\{{u}_{t_\\varepsilon+1}(x') - {u}_{t_\\varepsilon}(x')\\} \\leq \\int_{\\mathcal{X}} \\!\\mu_x^{d_\\varepsilon}(\\text{d}x')\\big( {{u}}_{t_\\varepsilon+1}(x') - {{u}}_{t_\\varepsilon}(x')\\big) \\leq \\sup_{x'\\in\\mathcal{X}}\\{{u}_{t_\\varepsilon+1}(x') - {u}_{t_\\varepsilon+1}(x')\\},\n    \\end{align*}\n    %\n    from which it follows that\n    %\n    \\begin{align*}\n        \\left| \\int_{\\mathcal{X}} \\!\\mu_x^{d_\\varepsilon}(\\text{d}x')\\big( {{u}}_{t_\\varepsilon+1}(x') - {{u}}_{t_\\varepsilon}(x')\\big) - g^\\varepsilon \\right| \\leq \\frac{1}{2} \\operatorname{sp}({{u}}_{t_\\varepsilon+1} \\!-\\! {{u}}_{t_\\varepsilon}) \\leq \\frac{\\varepsilon_s}{2}\n        \\!\\implies\\! \\|{g}^{d_\\varepsilon^\\infty} - g^\\varepsilon {e}\\|_\\infty \\leq \\varepsilon_u + \\frac{\\varepsilon_s}{2},\n    \\end{align*}\n    %\n    using that $\\|u_{t_\\varepsilon+1} - \\mathcal{L}_{d_\\varepsilon}u_{t_\\varepsilon}\\|_\\infty \\leq \\varepsilon_u$. By repeating the same procedure but with the optimal policy $(d^\\ast)^\\infty$ instead of $d_\\varepsilon^\\infty$, then $|g^\\ast - g^\\varepsilon| \\leq \\varepsilon_u+ \\frac{\\varepsilon_s}{2}$ (this time using that $\\|u_{t+1} - \\mathcal{L}u_{t}\\|_\\infty \\leq \\varepsilon_u$).\n\\end{proof}\\begin{proof}\n    By the definition of ${g}^{d_\\varepsilon^\\infty}$, the stochastic kernel $p_{d_\\varepsilon}\\!(\\cdot|x)$ is positive Harris recurrent with unique invariant probability measure $\\mu_x^{d_\\varepsilon}$ such that ${g}^{d_\\varepsilon^\\infty} \\!=\\! \\int_{\\mathcal{X}} \\! {r}_{d_\\varepsilon}(x')\\mu^{d_\\varepsilon}_x(\\text{d}x')$~\\cite[Theorem~2.5]{saldi2017asymptotic}. Furthermore, since $\\int_{\\mathcal{X}}\\mu^{d_\\varepsilon}_x(\\text{d}x') p_{d_\\varepsilon}(\\cdot|x) = \\mu^{d_\\varepsilon}_x(\\cdot)$, then \n    %\n    \\begin{align*}\n        {g}^{d_\\varepsilon^\\infty}(x) &= \\int_{\\mathcal{X}} \\mu_x^{d_\\varepsilon}(\\text{d}x') \\left(r_{d_\\varepsilon}(x') + \\int_{\\mathcal{X}} p_{d_\\varepsilon}(\\text{d}x''|x') u_{t_\\varepsilon}(x'') - {{u}}_{t_\\varepsilon}(x')\\right) \\\\\n        &= \\int_{\\mathcal{X}} \\mu_x^{d_\\varepsilon}(\\text{d}x')\\big( (\\mathcal{L}_{d_\\varepsilon}{{u}}_{t_\\varepsilon})(x') - {{u}}_{t_\\varepsilon}(x')\\big).\n    \\end{align*}\n    %\n    Since $\\inf_{x'\\in\\mathcal{X}}\\{{u}_{t_\\varepsilon+1}(x') - {u}_{t_\\varepsilon}(x')\\} \\leq {{u}}_{t_\\varepsilon+1}(x) - {{u}}_{t_\\varepsilon}(x) \\leq \\sup_{x'\\in\\mathcal{X}}\\{{u}_{t_\\varepsilon+1}(x') - {u}_{t_\\varepsilon+1}(x')\\} $ for all $x\\in\\mathcal{X}$, then\n    %\n    \\begin{align*}\n        \\inf_{x'\\in\\mathcal{X}}\\{{u}_{t_\\varepsilon+1}(x') - {u}_{t_\\varepsilon}(x')\\} \\leq \\int_{\\mathcal{X}} \\!\\mu_x^{d_\\varepsilon}(\\text{d}x')\\big( {{u}}_{t_\\varepsilon+1}(x') - {{u}}_{t_\\varepsilon}(x')\\big) \\leq \\sup_{x'\\in\\mathcal{X}}\\{{u}_{t_\\varepsilon+1}(x') - {u}_{t_\\varepsilon+1}(x')\\},\n    \\end{align*}\n    %\n    from which it follows that\n    %\n    \\begin{align*}\n        \\left| \\int_{\\mathcal{X}} \\!\\mu_x^{d_\\varepsilon}(\\text{d}x')\\big( {{u}}_{t_\\varepsilon+1}(x') - {{u}}_{t_\\varepsilon}(x')\\big) - g^\\varepsilon \\right| \\leq \\frac{1}{2} \\operatorname{sp}({{u}}_{t_\\varepsilon+1} \\!-\\! {{u}}_{t_\\varepsilon}) \\leq \\frac{\\varepsilon_s}{2}\n        \\!\\implies\\! \\|{g}^{d_\\varepsilon^\\infty} - g^\\varepsilon {e}\\|_\\infty \\leq \\varepsilon_u + \\frac{\\varepsilon_s}{2},\n    \\end{align*}\n    %\n    using that $\\|u_{t_\\varepsilon+1} - \\mathcal{L}_{d_\\varepsilon}u_{t_\\varepsilon}\\|_\\infty \\leq \\varepsilon_u$. By repeating the same procedure but with the optimal policy $(d^\\ast)^\\infty$ instead of $d_\\varepsilon^\\infty$, then $|g^\\ast - g^\\varepsilon| \\leq \\varepsilon_u+ \\frac{\\varepsilon_s}{2}$ (this time using that $\\|u_{t+1} - \\mathcal{L}u_{t}\\|_\\infty \\leq \\varepsilon_u$).\n\\end{proof}\n    By the definition of ${g}^{d_\\varepsilon^\\infty}${g}g^{d_\\varepsilon^\\infty}d_\\varepsilon^\\infty, the stochastic kernel $p_{d_\\varepsilon}\\!(\\cdot|x)$p_{d_\\varepsilon}d_\\varepsilon\\!(\\cdot|x) is positive Harris recurrent with unique invariant probability measure $\\mu_x^{d_\\varepsilon}$\\mu_x^{d_\\varepsilon}d_\\varepsilon such that ${g}^{d_\\varepsilon^\\infty} \\!=\\! \\int_{\\mathcal{X}} \\! {r}_{d_\\varepsilon}(x')\\mu^{d_\\varepsilon}_x(\\text{d}x')${g}g^{d_\\varepsilon^\\infty}d_\\varepsilon^\\infty \\!=\\! \\int_{\\mathcal{X}}\\mathcal{X} \\! {r}r_{d_\\varepsilon}d_\\varepsilon(x')\\mu^{d_\\varepsilon}d_\\varepsilon_x(\\text{d}x')~\\cite[Theorem~2.5]{saldi2017asymptotic}. Furthermore, since $\\int_{\\mathcal{X}}\\mu^{d_\\varepsilon}_x(\\text{d}x') p_{d_\\varepsilon}(\\cdot|x) = \\mu^{d_\\varepsilon}_x(\\cdot)$\\int_{\\mathcal{X}}\\mathcal{X}\\mu^{d_\\varepsilon}d_\\varepsilon_x(\\text{d}x') p_{d_\\varepsilon}d_\\varepsilon(\\cdot|x) = \\mu^{d_\\varepsilon}d_\\varepsilon_x(\\cdot), then \n    \n        {g}g^{d_\\varepsilon^\\infty}d_\\varepsilon^\\infty(x) &= \\int_{\\mathcal{X}}\\mathcal{X} \\mu_x^{d_\\varepsilon}d_\\varepsilon(\\text{d}x') \\left(r_{d_\\varepsilon}d_\\varepsilon(x') + \\int_{\\mathcal{X}}\\mathcal{X} p_{d_\\varepsilon}d_\\varepsilon(\\text{d}x''|x') u_{t_\\varepsilon}t_\\varepsilon(x'') - {{u}}{u}u_{t_\\varepsilon}t_\\varepsilon(x')\\right) \\\\\n        &= \\int_{\\mathcal{X}}\\mathcal{X} \\mu_x^{d_\\varepsilon}d_\\varepsilon(\\text{d}x')\\big( (\\mathcal{L}_{d_\\varepsilon}d_\\varepsilon{{u}}{u}u_{t_\\varepsilon}t_\\varepsilon)(x') - {{u}}{u}u_{t_\\varepsilon}t_\\varepsilon(x')\\big).\n    \n    Since $\\inf_{x'\\in\\mathcal{X}}\\{{u}_{t_\\varepsilon+1}(x') - {u}_{t_\\varepsilon}(x')\\} \\leq {{u}}_{t_\\varepsilon+1}(x) - {{u}}_{t_\\varepsilon}(x) \\leq \\sup_{x'\\in\\mathcal{X}}\\{{u}_{t_\\varepsilon+1}(x') - {u}_{t_\\varepsilon+1}(x')\\} $\\inf_{x'\\in\\mathcal{X}}x'\\in\\mathcal{X}\\{{u}u_{t_\\varepsilon+1}t_\\varepsilon+1(x') - {u}u_{t_\\varepsilon}t_\\varepsilon(x')\\} \\leq {{u}}{u}u_{t_\\varepsilon+1}t_\\varepsilon+1(x) - {{u}}{u}u_{t_\\varepsilon}t_\\varepsilon(x) \\leq \\sup_{x'\\in\\mathcal{X}}x'\\in\\mathcal{X}\\{{u}u_{t_\\varepsilon+1}t_\\varepsilon+1(x') - {u}u_{t_\\varepsilon+1}t_\\varepsilon+1(x')\\}  for all $x\\in\\mathcal{X}$x\\in\\mathcal{X}, then\n    \n        \\inf_{x'\\in\\mathcal{X}}x'\\in\\mathcal{X}\\{{u}u_{t_\\varepsilon+1}t_\\varepsilon+1(x') - {u}u_{t_\\varepsilon}t_\\varepsilon(x')\\} \\leq \\int_{\\mathcal{X}}\\mathcal{X} \\!\\mu_x^{d_\\varepsilon}d_\\varepsilon(\\text{d}x')\\big( {{u}}{u}u_{t_\\varepsilon+1}t_\\varepsilon+1(x') - {{u}}{u}u_{t_\\varepsilon}t_\\varepsilon(x')\\big) \\leq \\sup_{x'\\in\\mathcal{X}}x'\\in\\mathcal{X}\\{{u}u_{t_\\varepsilon+1}t_\\varepsilon+1(x') - {u}u_{t_\\varepsilon+1}t_\\varepsilon+1(x')\\},\n    \n    from which it follows that\n    \n        \\left| \\int_{\\mathcal{X}}\\mathcal{X} \\!\\mu_x^{d_\\varepsilon}d_\\varepsilon(\\text{d}x')\\big( {{u}}{u}u_{t_\\varepsilon+1}t_\\varepsilon+1(x') - {{u}}{u}u_{t_\\varepsilon}t_\\varepsilon(x')\\big) - g^\\varepsilon \\right| \\leq \\frac{1}{2} \\operatorname{sp}sp({{u}}{u}u_{t_\\varepsilon+1}t_\\varepsilon+1 \\!-\\! {{u}}{u}u_{t_\\varepsilon}t_\\varepsilon) \\leq \\frac{\\varepsilon_s}{2}\n        \\!\\implies\\! \\|{g}g^{d_\\varepsilon^\\infty}d_\\varepsilon^\\infty - g^\\varepsilon {e}e\\|_\\infty \\leq \\varepsilon_u + \\frac{\\varepsilon_s}{2},\n    \n    using that $\\|u_{t_\\varepsilon+1} - \\mathcal{L}_{d_\\varepsilon}u_{t_\\varepsilon}\\|_\\infty \\leq \\varepsilon_u$\\|u_{t_\\varepsilon+1}t_\\varepsilon+1 - \\mathcal{L}_{d_\\varepsilon}d_\\varepsilonu_{t_\\varepsilon}t_\\varepsilon\\|_\\infty \\leq \\varepsilon_u. By repeating the same procedure but with the optimal policy $(d^\\ast)^\\infty$(d^\\ast)^\\infty instead of $d_\\varepsilon^\\infty$d_\\varepsilon^\\infty, then $|g^\\ast - g^\\varepsilon| \\leq \\varepsilon_u+ \\frac{\\varepsilon_s}{2}$|g^\\ast - g^\\varepsilon| \\leq \\varepsilon_u+ \\frac{\\varepsilon_s}{2} (this time using that $\\|u_{t+1} - \\mathcal{L}u_{t}\\|_\\infty \\leq \\varepsilon_u$\\|u_{t+1}t+1 - \\mathcal{L}u_{t}t\\|_\\infty \\leq \\varepsilon_u).\n\n\n\n\\subsection{Classical value iteration algorithm under a generative model}\n\nA few different classical algorithms for computing optimal policies under a generative model for infinite-horizon MDPs have been proposed~\\cite{wang2017primal,jin2020efficiently,jin2021towards,wang2022near,zhang2023sharper,li2024stochastic},\none of the best complexities being $\\widetilde{O}\\big(\\frac{\\Lambda^2 SA}{\\varepsilon^2}\\big)$\\widetilde{O}\\big(\\frac{\\Lambda^2 SA}{\\varepsilon^2}\\big) due to Zhang and Xie~\\cite{zhang2023sharper}. Here we provide, for completeness, a simple approximate version of standard value iteration (\\Cref{algo:classical_extended_value_iteration}), already generalised for compact state spaces, wherein the quantities $\\int_{\\mathcal{X}}p(\\rd x'|x,a)u_{t}(x')$\\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)u_{t}t(x') are approximated using classical samples via a Hoeffding bound, similarly to the case of finite-horizon MDPs from the previous section or infinite-horizon discounted MDPs from~\\cite{sidford2018near,li2020breaking}.\n\n\\begin{algorithm}[t!]\n    \\caption{Classical approximate value iteration algorithm}\n    \\label{algo:classical_extended_value_iteration}\n    \\begin{algorithmic}[1]  \n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$, finite action space $\\mathcal{A}$, classical sampling access to probability kernels $p$, failure probability $\\delta\\in(0, 1)$, parameters $\\Lambda > 0$, $\\nu\\in[0,1)$, $L,\\alpha \\geq 0$, errors $\\varepsilon, \\varepsilon_u > 0$ such that $\\varepsilon_u = \\frac{1}{4}(1-\\nu)\\varepsilon$.\n\n    \\Ensure $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$-optimal stationary policy $\\widetilde{d}^\\infty$.\n\n    \\State $t\\gets 1$ and initialise ${u}_{0} \\gets {0}$ and $u_1(x) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a)\\}$ for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$\n\n    \\While {$\\operatorname{sp}({{u}}_{t} - {{u}}_{t-1}) > \\frac{3\\varepsilon}{2} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$}\n    \n    \\State $m_t \\gets \\big\\lceil \\frac{512}{(1-\\nu)^2 \\varepsilon^2}{\\min}\\big\\{ 4(1+\\Lambda)^2, \\frac{\\Lambda^2}{(1-\\nu)^2}\\big\\}\\ln\\frac{\\pi^2 t^2 |\\mathcal{S}_n|A}{6\\delta} \\big\\rceil$\n    \n    \\State For $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, sample $x^{(1)}_{s,a},x^{(2)}_{s,a},\\dots,x^{(m_t)}_{s,a} \\in\\mathcal{X}$\n        \n\n    \\State  \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent}{For all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$, $a_{t+1}(x) \\gets \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\frac{1}{m_t}\\sum_{i=1}^{m_t} u_t(x^{(i)}_{s,a})\\}$, $\\widetilde{u}_{t+1}(s) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\frac{1}{m_t}\\sum_{i=1}^{m_t} u_t(x^{(i)}_{s,a})\\}$, and \n    %\n    \\begin{align*}\n        u_{t+1}(x) \\gets \\begin{dcases}\n            \\max\\left\\{\\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~\\widetilde{u}_{t+1}(s) \\geq \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\left\\{\\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~ \\widetilde{u}_{t+1}(s) \\leq \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}(s) &\\text{otherwise}.\n        \\end{dcases}\n    \\end{align*}}\n\n\n    \\State $t \\gets t+1$\n\n    \\EndWhile\n    \n    \\State \\Return $\\widetilde{d}\\in\\mathcal{D}^{\\rm D}$ where $\\widetilde{d}(x) := a_t(x)$ for all $x\\in\\mathcal{X}$\n\\end{algorithmic}\n\\end{algorithm}\\begin{algorithm}[t!]\n    \\caption{Classical approximate value iteration algorithm}\n    \\label{algo:classical_extended_value_iteration}\n    \\begin{algorithmic}[1]  \n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$, finite action space $\\mathcal{A}$, classical sampling access to probability kernels $p$, failure probability $\\delta\\in(0, 1)$, parameters $\\Lambda > 0$, $\\nu\\in[0,1)$, $L,\\alpha \\geq 0$, errors $\\varepsilon, \\varepsilon_u > 0$ such that $\\varepsilon_u = \\frac{1}{4}(1-\\nu)\\varepsilon$.\n\n    \\Ensure $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$-optimal stationary policy $\\widetilde{d}^\\infty$.\n\n    \\State $t\\gets 1$ and initialise ${u}_{0} \\gets {0}$ and $u_1(x) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a)\\}$ for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$\n\n    \\While {$\\operatorname{sp}({{u}}_{t} - {{u}}_{t-1}) > \\frac{3\\varepsilon}{2} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$}\n    \n    \\State $m_t \\gets \\big\\lceil \\frac{512}{(1-\\nu)^2 \\varepsilon^2}{\\min}\\big\\{ 4(1+\\Lambda)^2, \\frac{\\Lambda^2}{(1-\\nu)^2}\\big\\}\\ln\\frac{\\pi^2 t^2 |\\mathcal{S}_n|A}{6\\delta} \\big\\rceil$\n    \n    \\State For $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, sample $x^{(1)}_{s,a},x^{(2)}_{s,a},\\dots,x^{(m_t)}_{s,a} \\in\\mathcal{X}$\n        \n\n    \\State  \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent}{For all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$, $a_{t+1}(x) \\gets \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\frac{1}{m_t}\\sum_{i=1}^{m_t} u_t(x^{(i)}_{s,a})\\}$, $\\widetilde{u}_{t+1}(s) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\frac{1}{m_t}\\sum_{i=1}^{m_t} u_t(x^{(i)}_{s,a})\\}$, and \n    %\n    \\begin{align*}\n        u_{t+1}(x) \\gets \\begin{dcases}\n            \\max\\left\\{\\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~\\widetilde{u}_{t+1}(s) \\geq \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\left\\{\\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~ \\widetilde{u}_{t+1}(s) \\leq \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}(s) &\\text{otherwise}.\n        \\end{dcases}\n    \\end{align*}}\n\n\n    \\State $t \\gets t+1$\n\n    \\EndWhile\n    \n    \\State \\Return $\\widetilde{d}\\in\\mathcal{D}^{\\rm D}$ where $\\widetilde{d}(x) := a_t(x)$ for all $x\\in\\mathcal{X}$\n\\end{algorithmic}\n\\end{algorithm}[t!]\n    \\caption{Classical approximate value iteration algorithm}\n    \\label{algo:classical_extended_value_iteration}\n    [1]  \n    \\Require Compact state space $\\mathcal{X}$\\mathcal{X} with $\\frac{1}{n}$\\frac{1}{n}-net $\\mathcal{S}_n$\\mathcal{S}_n, finite action space $\\mathcal{A}$\\mathcal{A}, classical sampling access to probability kernels $p$p, failure probability $\\delta\\in(0, 1)$\\delta\\in(0, 1), parameters $\\Lambda > 0$\\Lambda > 0, $\\nu\\in[0,1)$\\nu\\in[0,1), $L,\\alpha \\geq 0$L,\\alpha \\geq 0, errors $\\varepsilon, \\varepsilon_u > 0$\\varepsilon, \\varepsilon_u > 0 such that $\\varepsilon_u = \\frac{1}{4}(1-\\nu)\\varepsilon$\\varepsilon_u = \\frac{1}{4}(1-\\nu)\\varepsilon.\n\n    \\Ensure $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)-optimal stationary policy $\\widetilde{d}^\\infty$\\widetilde{d}^\\infty.\n\n    \\State $t\\gets 1$t\\gets 1 and initialise ${u}_{0} \\gets {0}${u}u_{0}0 \\gets {0}0 and $u_1(x) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a)\\}$u_1(x) \\gets \\max_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{r(s,a)\\} for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n and $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s)\n\n    \\While {$\\operatorname{sp}({{u}}_{t} - {{u}}_{t-1}) > \\frac{3\\varepsilon}{2} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$}$\\operatorname{sp}({{u}}_{t} - {{u}}_{t-1}) > \\frac{3\\varepsilon}{2} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$\\operatorname{sp}sp({{u}}{u}u_{t}t - {{u}}{u}u_{t-1}t-1) > \\frac{3\\varepsilon}{2} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\n    \n    \\State $m_t \\gets \\big\\lceil \\frac{512}{(1-\\nu)^2 \\varepsilon^2}{\\min}\\big\\{ 4(1+\\Lambda)^2, \\frac{\\Lambda^2}{(1-\\nu)^2}\\big\\}\\ln\\frac{\\pi^2 t^2 |\\mathcal{S}_n|A}{6\\delta} \\big\\rceil$m_t \\gets \\big\\lceil \\frac{512}{(1-\\nu)^2 \\varepsilon^2}{\\min}\\min\\big\\{ 4(1+\\Lambda)^2, \\frac{\\Lambda^2}{(1-\\nu)^2}\\big\\}\\ln\\frac{\\pi^2 t^2 |\\mathcal{S}_n|A}{6\\delta} \\big\\rceil\n    \n    \\State For $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}, sample $x^{(1)}_{s,a},x^{(2)}_{s,a},\\dots,x^{(m_t)}_{s,a} \\in\\mathcal{X}$x^{(1)}(1)_{s,a}s,a,x^{(2)}(2)_{s,a}s,a,\\dots,x^{(m_t)}(m_t)_{s,a}s,a \\in\\mathcal{X}\n        \n\n    \\State  \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent}\\dimexpr\\linewidth-\\algorithmicindent{For all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$, $a_{t+1}(x) \\gets \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\frac{1}{m_t}\\sum_{i=1}^{m_t} u_t(x^{(i)}_{s,a})\\}$, $\\widetilde{u}_{t+1}(s) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\frac{1}{m_t}\\sum_{i=1}^{m_t} u_t(x^{(i)}_{s,a})\\}$, and \n    %\n    \\begin{align*}\n        u_{t+1}(x) \\gets \\begin{dcases}\n            \\max\\left\\{\\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~\\widetilde{u}_{t+1}(s) \\geq \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\left\\{\\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~ \\widetilde{u}_{t+1}(s) \\leq \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}(s) &\\text{otherwise}.\n        \\end{dcases}\n    \\end{align*}}For all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n and $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s), $a_{t+1}(x) \\gets \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\frac{1}{m_t}\\sum_{i=1}^{m_t} u_t(x^{(i)}_{s,a})\\}$a_{t+1}t+1(x) \\gets \\argmax_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{r(s,a) + \\frac{1}{m_t}\\sum_{i=1}i=1^{m_t}m_t u_t(x^{(i)}(i)_{s,a}s,a)\\}, $\\widetilde{u}_{t+1}(s) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\frac{1}{m_t}\\sum_{i=1}^{m_t} u_t(x^{(i)}_{s,a})\\}$\\widetilde{u}_{t+1}t+1(s) \\gets \\max_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{r(s,a) + \\frac{1}{m_t}\\sum_{i=1}i=1^{m_t}m_t u_t(x^{(i)}(i)_{s,a}s,a)\\}, and \n    \n        u_{t+1}t+1(x) \\gets \n            \\max\\left\\{\\max_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s')\\right\\} &\\text{if}~\\widetilde{u}_{t+1}t+1(s) \\geq \\max_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\left\\{\\min_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s')\\right\\} &\\text{if}~ \\widetilde{u}_{t+1}t+1(s) \\leq \\min_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}t+1(s) &\\text{otherwise}.\n        \n    \n\n\n    \\State $t \\gets t+1$t \\gets t+1\n\n    \\EndWhile\n    \n    \\State \\Return $\\widetilde{d}\\in\\mathcal{D}^{\\rm D}$\\widetilde{d}\\in\\mathcal{D}^{\\rm D}\\rm D where $\\widetilde{d}(x) := a_t(x)$\\widetilde{d}(x) := a_t(x) for all $x\\in\\mathcal{X}$x\\in\\mathcal{X}\n\n\n\n\\begin{theorem}[Classical infinite-horizon generative algorithm]\\label{thr:classical_scopt_algorithm}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ be an infinite-horizon weakly communicating MDP with $\\operatorname{sp}({h}^\\ast) \\leq \\Lambda$. Let $\\mathcal{S}_n$ be a $\\frac{1}{n}$-net for $\\mathcal{X}$. Assume the optimal Bellman operator $\\mathcal{L}$ of $M$ is a $1$-stage $\\nu$-span contraction for $\\nu\\in[0,1)$. Assume classical sampling access to $p$ via oracle $\\mathcal{C}_p$. Let $\\delta\\in (0, 1)$ and $\\varepsilon \\in(0,\\frac{2}{\\nu}]$. Under {\\rm \\Cref{ass:Lipschitz}} with parameters $L,\\alpha\\geq 0$ such that $Ln^{-\\alpha} \\leq \\frac{1-\\nu}{\\nu}$, {\\rm \\cref{algo:classical_extended_value_iteration}} outputs an $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$-optimal policy $\\widetilde{d}^\\infty$ and $g^\\varepsilon$ such that $|g^\\varepsilon - g^\\ast| < \\varepsilon + \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$ with probability $1-\\delta$. Its $\\mathcal{C}_p$-query complexity is (up to $\\poly\\log\\log$ factors in $1/\\varepsilon$ and $\\nu$)\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\min\\left\\{(1+\\Lambda)^2,\\frac{\\Lambda^2}{(1-\\nu)^2}\\right\\}\\frac{|\\mathcal{S}_n|A}{(1-\\nu)^2\\varepsilon^2}\\frac{\\log\\frac{1}{\\varepsilon}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|A}{\\delta}\\right).\n    \\end{align*}\n\\end{theorem}\\begin{theorem}[Classical infinite-horizon generative algorithm]\\label{thr:classical_scopt_algorithm}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ be an infinite-horizon weakly communicating MDP with $\\operatorname{sp}({h}^\\ast) \\leq \\Lambda$. Let $\\mathcal{S}_n$ be a $\\frac{1}{n}$-net for $\\mathcal{X}$. Assume the optimal Bellman operator $\\mathcal{L}$ of $M$ is a $1$-stage $\\nu$-span contraction for $\\nu\\in[0,1)$. Assume classical sampling access to $p$ via oracle $\\mathcal{C}_p$. Let $\\delta\\in (0, 1)$ and $\\varepsilon \\in(0,\\frac{2}{\\nu}]$. Under {\\rm \\Cref{ass:Lipschitz}} with parameters $L,\\alpha\\geq 0$ such that $Ln^{-\\alpha} \\leq \\frac{1-\\nu}{\\nu}$, {\\rm \\cref{algo:classical_extended_value_iteration}} outputs an $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$-optimal policy $\\widetilde{d}^\\infty$ and $g^\\varepsilon$ such that $|g^\\varepsilon - g^\\ast| < \\varepsilon + \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$ with probability $1-\\delta$. Its $\\mathcal{C}_p$-query complexity is (up to $\\poly\\log\\log$ factors in $1/\\varepsilon$ and $\\nu$)\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\min\\left\\{(1+\\Lambda)^2,\\frac{\\Lambda^2}{(1-\\nu)^2}\\right\\}\\frac{|\\mathcal{S}_n|A}{(1-\\nu)^2\\varepsilon^2}\\frac{\\log\\frac{1}{\\varepsilon}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|A}{\\delta}\\right).\n    \\end{align*}\n\\end{theorem}\\label{thr:classical_scopt_algorithm}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle be an infinite-horizon weakly communicating MDP with $\\operatorname{sp}({h}^\\ast) \\leq \\Lambda$\\operatorname{sp}sp({h}h^\\ast) \\leq \\Lambda. Let $\\mathcal{S}_n$\\mathcal{S}_n be a $\\frac{1}{n}$\\frac{1}{n}-net for $\\mathcal{X}$\\mathcal{X}. Assume the optimal Bellman operator $\\mathcal{L}$\\mathcal{L} of $M$M is a $1$1-stage $\\nu$\\nu-span contraction for $\\nu\\in[0,1)$\\nu\\in[0,1). Assume classical sampling access to $p$p via oracle $\\mathcal{C}_p$\\mathcal{C}_p. Let $\\delta\\in (0, 1)$\\delta\\in (0, 1) and $\\varepsilon \\in(0,\\frac{2}{\\nu}]$\\varepsilon \\in(0,\\frac{2}{\\nu}]. Under {\\rm \\Cref{ass:Lipschitz}}\\rm \\Cref{ass:Lipschitz} with parameters $L,\\alpha\\geq 0$L,\\alpha\\geq 0 such that $Ln^{-\\alpha} \\leq \\frac{1-\\nu}{\\nu}$Ln^{-\\alpha}-\\alpha \\leq \\frac{1-\\nu}{\\nu}, {\\rm \\cref{algo:classical_extended_value_iteration}}\\rm \\cref{algo:classical_extended_value_iteration} outputs an $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)-optimal policy $\\widetilde{d}^\\infty$\\widetilde{d}^\\infty and $g^\\varepsilon$g^\\varepsilon such that $|g^\\varepsilon - g^\\ast| < \\varepsilon + \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$|g^\\varepsilon - g^\\ast| < \\varepsilon + \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} with probability $1-\\delta$1-\\delta. Its $\\mathcal{C}_p$\\mathcal{C}_p-query complexity is (up to $\\poly\\log\\log$\\poly\\log\\log factors in $1/\\varepsilon$1/\\varepsilon and $\\nu$\\nu)\n    \n        \\widetilde{O}\\left(\\min\\left\\{(1+\\Lambda)^2,\\frac{\\Lambda^2}{(1-\\nu)^2}\\right\\}\\frac{|\\mathcal{S}_n|A}{(1-\\nu)^2\\varepsilon^2}\\frac{\\log\\frac{1}{\\varepsilon}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|A}{\\delta}\\right).\n    \n\n\\begin{proof}\n    In the following, let $\\varepsilon_u, \\bar{\\varepsilon}_u > 0$ be such that $\\varepsilon_u := \\frac{1}{4}(1-\\nu)\\varepsilon$ and $\\bar{\\varepsilon}_u := \\varepsilon_u + (1+\\Lambda)Ln^{-\\alpha}$. We start by proving that \\Cref{algo:classical_extended_value_iteration} generates a sequence of functions $(u_t)_{t\\in\\mathbb{N}}$ such that $\\|u_{t+1} - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}$, $\\operatorname{sp}(u_{t+1}) \\leq \\operatorname{sp}(\\mathcal{L}u_t)$, $\\operatorname{sp}(u_{t+1}) \\leq \\frac{2\\Lambda}{1-\\nu}$, and $\\operatorname{sp}(u_t - h^\\ast) \\leq \\nu^t \\Lambda + \\frac{2\\bar{\\varepsilon}_u}{1-\\nu}$, which also implies that\n    %\n    \\begin{align*}\n        \\operatorname{sp}(u_{t+1}) \\leq \\operatorname{sp}(\\mathcal{L}u_t) \\leq \\operatorname{sp}(h^\\ast) + \\operatorname{sp}(\\mathcal{L}u_t - \\mathcal{L}h^\\ast) \\leq (1+\\nu^{t+1})\\Lambda + \\frac{2\\nu\\bar{\\varepsilon}_u}{1-\\nu}  \\leq 4\\Lambda + 3,\n    \\end{align*}\n    %\n    assuming that $Ln^{-\\alpha} \\leq \\frac{1-\\nu}{\\nu}$ and $\\varepsilon_u \\leq \\frac{1-\\nu}{2\\nu}$. For $t=0$, $u_1(x) = \\max_{a\\in\\mathcal{A}}\\{r(s,a)\\}$ for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$ and then $\\|u_1 - \\mathcal{L}0\\|_\\infty = \\max_{x\\in\\mathcal{X}}\\{u_1(x) - \\max_{a\\in\\mathcal{A}}r(x,a)\\} \\leq Ln^{-\\alpha}$. Moreover,\n    %\n    \\begin{align*}\n        \\operatorname{sp}(u_1 - h^\\ast) &\\leq \\operatorname{sp}(\\mathcal{L}0 - \\mathcal{L}h^\\ast) + \\operatorname{sp}(u_1 - \\mathcal{L}0) \\leq \\nu \\Lambda + 2Ln^{-\\alpha} \\leq \\nu\\Lambda + \\frac{2\\bar{\\varepsilon}_u}{1-\\nu},\\\\\n        \\operatorname{sp}(u_1) &\\leq \\operatorname{sp}(\\mathcal{L}0) \\leq \\operatorname{sp}(h^\\ast) + \\operatorname{sp}(\\mathcal{L}0 - \\mathcal{L}h^\\ast) \\leq (1+\\nu)\\Lambda \\leq \\frac{2\\Lambda}{1-\\nu},\n    \\end{align*}\n    % \\begin{align*}\n    %     \\operatorname{sp}(u_1) \\leq \\operatorname{sp}(\\mathcal{L}0) \\leq \\operatorname{sp}(h^\\ast) + \\operatorname{sp}(h^\\ast - \\mathcal{L}0) \\leq \\operatorname{sp}(h^\\ast) + \\operatorname{sp}(\\mathcal{L}h^\\ast - \\mathcal{L}0) \\leq (1+\\nu)\\operatorname{sp}(h^\\ast) \\leq (1+\\nu)\\Lambda,\n    % \\end{align*}\n    %\n    where we used that $\\mathcal{L}h^\\ast = h^\\ast + g^\\ast e$ and the span-contractive property of $\\mathcal{L}$.\n    \n    Fix now $t\\geq 1$ and assume by induction that $(u_{t'})_{t'\\in[t]}$ have been computed. By a Hoeffding bound (\\Cref{fact:hoeffding}) we have that, for all $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, with probability $1-\\frac{6\\delta}{\\pi^2t^2|\\mathcal{S}_n|A}$,\n    %\n    \\begin{align*}\n        \\left|\\mu_{t+1}(s,a) - \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t(x') \\right| \\leq \\sqrt{\\frac{2}{m_t}\\ln\\frac{\\pi^2t^2|\\mathcal{S}_n|A}{6\\delta}} \\operatorname{sp}(u_t) \\leq \\frac{\\varepsilon_u}{2},\n    \\end{align*}\n    %\n    using that $m_t := \\big\\lceil\\frac{32}{\\varepsilon_u^2}{\\min}\\big\\{4(1+\\Lambda)^2,\\frac{\\Lambda^2}{(1-\\nu)^2}\\big\\}\\ln\\frac{\\pi^2 t^2|\\mathcal{S}_n|A}{6\\delta}\\big\\rceil$ since $\\operatorname{sp}(u_t) \\leq {\\min}\\big\\{4(1+\\Lambda),\\frac{2\\Lambda}{1-\\nu}\\big\\}$ by induction\n    (we can subtract $\\min_{x'\\in\\mathcal{X}}u_t(x')$ from $u_t(x)$ so that $\\|u_t - \\min_{x'\\in\\mathcal{X}}\\{u_t(x')\\}e\\|_\\infty = \\operatorname{sp}(u_t)$ since $u_t(x) \\geq 0$). For $s\\in\\mathcal{S}_n$, define $\\widetilde{u}_{t+1}(s) := \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}$ and\n    %\n    \\begin{align*}\n        u_{t+1}(s) \\!=\\! \\begin{cases}\n            \\max\\{\\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\} &\\text{if}~\\widetilde{u}_{t+1}(s) \\geq \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\{\\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\} &\\text{if}~ \\widetilde{u}_{t+1}(s) \\leq \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}(s) &\\text{otherwise}.\n        \\end{cases}\n    \\end{align*}\n    %\n    In other words, the smaller entries of $\\widetilde{u}_{t+1}$ are increases by $\\frac{\\varepsilon_u}{2}$, while its larger entries are decreased by $\\frac{\\varepsilon_u}{2}$. This means that $\\operatorname{sp}(u_{t+1}) \\leq \\operatorname{sp}(\\mathcal{L}u_t)$. Moreover, $|u_{t+1}(s) - (\\mathcal{L}u_t)(s)| \\leq \\varepsilon_u$ for all $s\\in\\mathcal{S}_n$. Observe that, for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$,\n    %\n    \\begin{align*}\n        |(\\mathcal{L}u_{t})(s) - (\\mathcal{L}u_{t})(x)| &\\leq |r(s,a) - r(x,a)| + \\left|\\int_{\\mathcal{X}} \\!(p(\\rd x'|s,a) - p(\\rd x'|x,a)) \\!\\left(\\! u_{t}(x') - \\min_{x''\\in\\mathcal{X}}u_t(x'') \\!\\right) \\right| \\\\\n        &\\leq 4(1+\\Lambda)Ln^{-\\alpha}, \\tag{$\\operatorname{sp}(u_t) \\leq 4\\Lambda + 3$ by induction}\n    \\end{align*}\n    %\n    where $a = \\argmax_{a'\\in\\mathcal{A}}(\\mathcal{L}_{a'}u_{t})(s)$. By defining $u_{t+1}(x) = u_{t+1}(s)$ for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$, then $\\|u_{t+1} - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}$. Consider now the operator $\\bar{\\mathcal{L}}:\\mathscr{B}(\\mathcal{X})\\to\\mathscr{B}(\\mathcal{X})$ such that $(\\bar{\\mathcal{L}}v)(x) = (\\mathcal{L}v)(s)$ for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$. Notice that $\\|\\mathcal{L}h^\\ast - \\bar{\\mathcal{L}}h^\\ast\\|_\\infty \\leq (1+\\Lambda)Ln^{-\\alpha}$ similarly to above. Therefore,\n    %\n    \\begin{align*}\n        \\operatorname{sp}(u_{t+1} - h^\\ast) &\\leq \\operatorname{sp}(u_{t+1} - \\bar{\\mathcal{L}}u_t) + \\operatorname{sp}(\\bar{\\mathcal{L}}u_t - \\bar{\\mathcal{L}}h^\\ast) + \\operatorname{sp}(\\bar{\\mathcal{L}}h^\\ast - \\mathcal{L}h^\\ast)  \\tag{triangle inequality}\\\\\n        &\\leq 2\\varepsilon_u + \\nu\\operatorname{sp}(u_t - h^\\ast) + 2(1+\\Lambda)Ln^{-\\alpha}  \\tag{$\\operatorname{sp}(v) \\leq 2\\|v\\|_\\infty$ and $\\mathcal{L}$ contraction}\\\\\n        &\\leq 2\\bar{\\varepsilon}_u + \\nu^{t+1}\\Lambda + \\frac{2\\nu}{1-\\nu}\\bar{\\varepsilon}_u \\tag{induction hypothesis}\\\\\n        &= \\nu^{t+1}\\Lambda + \\frac{2\\bar{\\varepsilon}_u}{1-\\nu}.\n    \\end{align*}\n    %\n    On the other hand, it is also true that\n    %\n    \\begin{align*}\n        \\operatorname{sp}(u_{t+1}) \\leq \\operatorname{sp}(\\mathcal{L}u_t) \\leq \\operatorname{sp}(h^\\ast) + \\operatorname{sp}(\\mathcal{L}u_t - \\mathcal{L}h^\\ast) \\leq \\Lambda + \\nu\\frac{2\\Lambda}{1-\\nu} \\leq \\frac{2\\Lambda}{1-\\nu}.\n    \\end{align*}\n    %\n    This concludes the proof by induction that $\\|u_{t+1} - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}$ and $\\operatorname{sp}(u_{t+1}) \\leq {\\min}\\big\\{4\\Lambda + 3, \\frac{2\\Lambda}{1-\\nu}\\big\\}$.\n    \n    We now note that, since $\\|u_{t+1} - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}$, \\Cref{lem:stopping_criteria} yields that\n    %\n    \\begin{align*}\n        \\operatorname{sp}(u_{t+1} - u_t) \\leq \\nu^t(1 + 2\\varepsilon_u + 8(1+\\Lambda)Ln^{-\\alpha}) + \\frac{4\\varepsilon_u + 16(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\qquad\\forall t\\in\\mathbb{N},\n    \\end{align*}\n    %\n    using that $\\operatorname{sp}(\\mathcal{L}0) \\leq 1$.\n    Therefore, $\\operatorname{sp}(u_{t+1} - u_t) \\leq \\frac{6\\varepsilon_u + 18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$ if\n    %\n    \\begin{align*}\n        t\\geq t^\\ast := \\left\\lceil \\frac{\\log\\frac{(1-\\nu)(1+2\\varepsilon_u + 8(1+\\Lambda)Ln^{-\\alpha})}{2\\varepsilon_u + 2(1+\\Lambda)Ln^{-\\alpha}}}{\\log\\frac{1}{\\nu}} \\right\\rceil = O\\left(\\frac{\\log\\frac{1}{\\varepsilon}}{\\log\\frac{1}{\\nu}} \\right).\n    \\end{align*}\n    %\n    We employ \\cref{lem:value_iteration_guarantees} to argue that, once $\\operatorname{sp}({{u}}_{t+1} - {{u}}_{t}) \\leq \\frac{6\\varepsilon_u + 18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$ for $t\\geq t^\\ast$, then\n    %\n    \\begin{align*}\n        g^\\varepsilon = \\frac{1}{2}\\left(\\max_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast + 1}(x) - {u}_{t^\\ast}(x)\\} + \\min_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast + 1}(x) - {u}_{t^\\ast}(x)\\}\\right)\n    \\end{align*}\n    %\n    is such that $|g^\\varepsilon - g^\\ast| \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha} + \\frac{3\\varepsilon_u + 9(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} < \\varepsilon + \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$, while the policy $\\widetilde{d}^\\infty\\in\\Pi^{\\rm D}$ with $\\widetilde{d}(x) := \\argmax_{a\\in\\mathcal{A}}\\{u_{t^\\ast}(x)\\}$ for all $x\\in\\mathcal{X}$ is $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$-optimal.\n\n    We now analyse the query complexity of \\Cref{algo:classical_extended_value_iteration}. Computing $u_{t+1}$ for a fixed step $t\\in\\mathbb{N}$ requires $m_t|\\mathcal{S}_n|A $ queries to $\\mathcal{C}_p$, i.e., $O\\big({\\min}\\big\\{(1+\\Lambda)^2, \\frac{\\Lambda^2}{(1-\\nu)^2} \\big\\}\\frac{|\\mathcal{S}_n|A}{(1-\\nu)^2\\varepsilon^2} \\log\\frac{t|\\mathcal{S}_n|A}{\\delta}\\big)$ samples. The total query complexity is then $O\\big(t^\\ast {\\min}\\big\\{(1+\\Lambda)^2, \\frac{\\Lambda^2}{(1-\\nu)^2} \\big\\} \\frac{|\\mathcal{S}_n|A}{(1-\\nu)^2\\varepsilon^2}\\log\\frac{t|\\mathcal{S}_n|A}{\\delta}\\big)$. Finally, the success probability comes from a simple union bound and the identity $\\sum_{t=1}^\\infty \\frac{1}{t^2} = \\frac{\\pi^2}{6}$.\n\\end{proof}\\begin{proof}\n    In the following, let $\\varepsilon_u, \\bar{\\varepsilon}_u > 0$ be such that $\\varepsilon_u := \\frac{1}{4}(1-\\nu)\\varepsilon$ and $\\bar{\\varepsilon}_u := \\varepsilon_u + (1+\\Lambda)Ln^{-\\alpha}$. We start by proving that \\Cref{algo:classical_extended_value_iteration} generates a sequence of functions $(u_t)_{t\\in\\mathbb{N}}$ such that $\\|u_{t+1} - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}$, $\\operatorname{sp}(u_{t+1}) \\leq \\operatorname{sp}(\\mathcal{L}u_t)$, $\\operatorname{sp}(u_{t+1}) \\leq \\frac{2\\Lambda}{1-\\nu}$, and $\\operatorname{sp}(u_t - h^\\ast) \\leq \\nu^t \\Lambda + \\frac{2\\bar{\\varepsilon}_u}{1-\\nu}$, which also implies that\n    %\n    \\begin{align*}\n        \\operatorname{sp}(u_{t+1}) \\leq \\operatorname{sp}(\\mathcal{L}u_t) \\leq \\operatorname{sp}(h^\\ast) + \\operatorname{sp}(\\mathcal{L}u_t - \\mathcal{L}h^\\ast) \\leq (1+\\nu^{t+1})\\Lambda + \\frac{2\\nu\\bar{\\varepsilon}_u}{1-\\nu}  \\leq 4\\Lambda + 3,\n    \\end{align*}\n    %\n    assuming that $Ln^{-\\alpha} \\leq \\frac{1-\\nu}{\\nu}$ and $\\varepsilon_u \\leq \\frac{1-\\nu}{2\\nu}$. For $t=0$, $u_1(x) = \\max_{a\\in\\mathcal{A}}\\{r(s,a)\\}$ for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$ and then $\\|u_1 - \\mathcal{L}0\\|_\\infty = \\max_{x\\in\\mathcal{X}}\\{u_1(x) - \\max_{a\\in\\mathcal{A}}r(x,a)\\} \\leq Ln^{-\\alpha}$. Moreover,\n    %\n    \\begin{align*}\n        \\operatorname{sp}(u_1 - h^\\ast) &\\leq \\operatorname{sp}(\\mathcal{L}0 - \\mathcal{L}h^\\ast) + \\operatorname{sp}(u_1 - \\mathcal{L}0) \\leq \\nu \\Lambda + 2Ln^{-\\alpha} \\leq \\nu\\Lambda + \\frac{2\\bar{\\varepsilon}_u}{1-\\nu},\\\\\n        \\operatorname{sp}(u_1) &\\leq \\operatorname{sp}(\\mathcal{L}0) \\leq \\operatorname{sp}(h^\\ast) + \\operatorname{sp}(\\mathcal{L}0 - \\mathcal{L}h^\\ast) \\leq (1+\\nu)\\Lambda \\leq \\frac{2\\Lambda}{1-\\nu},\n    \\end{align*}\n    % \\begin{align*}\n    %     \\operatorname{sp}(u_1) \\leq \\operatorname{sp}(\\mathcal{L}0) \\leq \\operatorname{sp}(h^\\ast) + \\operatorname{sp}(h^\\ast - \\mathcal{L}0) \\leq \\operatorname{sp}(h^\\ast) + \\operatorname{sp}(\\mathcal{L}h^\\ast - \\mathcal{L}0) \\leq (1+\\nu)\\operatorname{sp}(h^\\ast) \\leq (1+\\nu)\\Lambda,\n    % \\end{align*}\n    %\n    where we used that $\\mathcal{L}h^\\ast = h^\\ast + g^\\ast e$ and the span-contractive property of $\\mathcal{L}$.\n    \n    Fix now $t\\geq 1$ and assume by induction that $(u_{t'})_{t'\\in[t]}$ have been computed. By a Hoeffding bound (\\Cref{fact:hoeffding}) we have that, for all $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$, with probability $1-\\frac{6\\delta}{\\pi^2t^2|\\mathcal{S}_n|A}$,\n    %\n    \\begin{align*}\n        \\left|\\mu_{t+1}(s,a) - \\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t(x') \\right| \\leq \\sqrt{\\frac{2}{m_t}\\ln\\frac{\\pi^2t^2|\\mathcal{S}_n|A}{6\\delta}} \\operatorname{sp}(u_t) \\leq \\frac{\\varepsilon_u}{2},\n    \\end{align*}\n    %\n    using that $m_t := \\big\\lceil\\frac{32}{\\varepsilon_u^2}{\\min}\\big\\{4(1+\\Lambda)^2,\\frac{\\Lambda^2}{(1-\\nu)^2}\\big\\}\\ln\\frac{\\pi^2 t^2|\\mathcal{S}_n|A}{6\\delta}\\big\\rceil$ since $\\operatorname{sp}(u_t) \\leq {\\min}\\big\\{4(1+\\Lambda),\\frac{2\\Lambda}{1-\\nu}\\big\\}$ by induction\n    (we can subtract $\\min_{x'\\in\\mathcal{X}}u_t(x')$ from $u_t(x)$ so that $\\|u_t - \\min_{x'\\in\\mathcal{X}}\\{u_t(x')\\}e\\|_\\infty = \\operatorname{sp}(u_t)$ since $u_t(x) \\geq 0$). For $s\\in\\mathcal{S}_n$, define $\\widetilde{u}_{t+1}(s) := \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}$ and\n    %\n    \\begin{align*}\n        u_{t+1}(s) \\!=\\! \\begin{cases}\n            \\max\\{\\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\} &\\text{if}~\\widetilde{u}_{t+1}(s) \\geq \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\{\\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\} &\\text{if}~ \\widetilde{u}_{t+1}(s) \\leq \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}(s) &\\text{otherwise}.\n        \\end{cases}\n    \\end{align*}\n    %\n    In other words, the smaller entries of $\\widetilde{u}_{t+1}$ are increases by $\\frac{\\varepsilon_u}{2}$, while its larger entries are decreased by $\\frac{\\varepsilon_u}{2}$. This means that $\\operatorname{sp}(u_{t+1}) \\leq \\operatorname{sp}(\\mathcal{L}u_t)$. Moreover, $|u_{t+1}(s) - (\\mathcal{L}u_t)(s)| \\leq \\varepsilon_u$ for all $s\\in\\mathcal{S}_n$. Observe that, for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$,\n    %\n    \\begin{align*}\n        |(\\mathcal{L}u_{t})(s) - (\\mathcal{L}u_{t})(x)| &\\leq |r(s,a) - r(x,a)| + \\left|\\int_{\\mathcal{X}} \\!(p(\\rd x'|s,a) - p(\\rd x'|x,a)) \\!\\left(\\! u_{t}(x') - \\min_{x''\\in\\mathcal{X}}u_t(x'') \\!\\right) \\right| \\\\\n        &\\leq 4(1+\\Lambda)Ln^{-\\alpha}, \\tag{$\\operatorname{sp}(u_t) \\leq 4\\Lambda + 3$ by induction}\n    \\end{align*}\n    %\n    where $a = \\argmax_{a'\\in\\mathcal{A}}(\\mathcal{L}_{a'}u_{t})(s)$. By defining $u_{t+1}(x) = u_{t+1}(s)$ for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$, then $\\|u_{t+1} - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}$. Consider now the operator $\\bar{\\mathcal{L}}:\\mathscr{B}(\\mathcal{X})\\to\\mathscr{B}(\\mathcal{X})$ such that $(\\bar{\\mathcal{L}}v)(x) = (\\mathcal{L}v)(s)$ for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$. Notice that $\\|\\mathcal{L}h^\\ast - \\bar{\\mathcal{L}}h^\\ast\\|_\\infty \\leq (1+\\Lambda)Ln^{-\\alpha}$ similarly to above. Therefore,\n    %\n    \\begin{align*}\n        \\operatorname{sp}(u_{t+1} - h^\\ast) &\\leq \\operatorname{sp}(u_{t+1} - \\bar{\\mathcal{L}}u_t) + \\operatorname{sp}(\\bar{\\mathcal{L}}u_t - \\bar{\\mathcal{L}}h^\\ast) + \\operatorname{sp}(\\bar{\\mathcal{L}}h^\\ast - \\mathcal{L}h^\\ast)  \\tag{triangle inequality}\\\\\n        &\\leq 2\\varepsilon_u + \\nu\\operatorname{sp}(u_t - h^\\ast) + 2(1+\\Lambda)Ln^{-\\alpha}  \\tag{$\\operatorname{sp}(v) \\leq 2\\|v\\|_\\infty$ and $\\mathcal{L}$ contraction}\\\\\n        &\\leq 2\\bar{\\varepsilon}_u + \\nu^{t+1}\\Lambda + \\frac{2\\nu}{1-\\nu}\\bar{\\varepsilon}_u \\tag{induction hypothesis}\\\\\n        &= \\nu^{t+1}\\Lambda + \\frac{2\\bar{\\varepsilon}_u}{1-\\nu}.\n    \\end{align*}\n    %\n    On the other hand, it is also true that\n    %\n    \\begin{align*}\n        \\operatorname{sp}(u_{t+1}) \\leq \\operatorname{sp}(\\mathcal{L}u_t) \\leq \\operatorname{sp}(h^\\ast) + \\operatorname{sp}(\\mathcal{L}u_t - \\mathcal{L}h^\\ast) \\leq \\Lambda + \\nu\\frac{2\\Lambda}{1-\\nu} \\leq \\frac{2\\Lambda}{1-\\nu}.\n    \\end{align*}\n    %\n    This concludes the proof by induction that $\\|u_{t+1} - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}$ and $\\operatorname{sp}(u_{t+1}) \\leq {\\min}\\big\\{4\\Lambda + 3, \\frac{2\\Lambda}{1-\\nu}\\big\\}$.\n    \n    We now note that, since $\\|u_{t+1} - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}$, \\Cref{lem:stopping_criteria} yields that\n    %\n    \\begin{align*}\n        \\operatorname{sp}(u_{t+1} - u_t) \\leq \\nu^t(1 + 2\\varepsilon_u + 8(1+\\Lambda)Ln^{-\\alpha}) + \\frac{4\\varepsilon_u + 16(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\qquad\\forall t\\in\\mathbb{N},\n    \\end{align*}\n    %\n    using that $\\operatorname{sp}(\\mathcal{L}0) \\leq 1$.\n    Therefore, $\\operatorname{sp}(u_{t+1} - u_t) \\leq \\frac{6\\varepsilon_u + 18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$ if\n    %\n    \\begin{align*}\n        t\\geq t^\\ast := \\left\\lceil \\frac{\\log\\frac{(1-\\nu)(1+2\\varepsilon_u + 8(1+\\Lambda)Ln^{-\\alpha})}{2\\varepsilon_u + 2(1+\\Lambda)Ln^{-\\alpha}}}{\\log\\frac{1}{\\nu}} \\right\\rceil = O\\left(\\frac{\\log\\frac{1}{\\varepsilon}}{\\log\\frac{1}{\\nu}} \\right).\n    \\end{align*}\n    %\n    We employ \\cref{lem:value_iteration_guarantees} to argue that, once $\\operatorname{sp}({{u}}_{t+1} - {{u}}_{t}) \\leq \\frac{6\\varepsilon_u + 18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$ for $t\\geq t^\\ast$, then\n    %\n    \\begin{align*}\n        g^\\varepsilon = \\frac{1}{2}\\left(\\max_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast + 1}(x) - {u}_{t^\\ast}(x)\\} + \\min_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast + 1}(x) - {u}_{t^\\ast}(x)\\}\\right)\n    \\end{align*}\n    %\n    is such that $|g^\\varepsilon - g^\\ast| \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha} + \\frac{3\\varepsilon_u + 9(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} < \\varepsilon + \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$, while the policy $\\widetilde{d}^\\infty\\in\\Pi^{\\rm D}$ with $\\widetilde{d}(x) := \\argmax_{a\\in\\mathcal{A}}\\{u_{t^\\ast}(x)\\}$ for all $x\\in\\mathcal{X}$ is $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$-optimal.\n\n    We now analyse the query complexity of \\Cref{algo:classical_extended_value_iteration}. Computing $u_{t+1}$ for a fixed step $t\\in\\mathbb{N}$ requires $m_t|\\mathcal{S}_n|A $ queries to $\\mathcal{C}_p$, i.e., $O\\big({\\min}\\big\\{(1+\\Lambda)^2, \\frac{\\Lambda^2}{(1-\\nu)^2} \\big\\}\\frac{|\\mathcal{S}_n|A}{(1-\\nu)^2\\varepsilon^2} \\log\\frac{t|\\mathcal{S}_n|A}{\\delta}\\big)$ samples. The total query complexity is then $O\\big(t^\\ast {\\min}\\big\\{(1+\\Lambda)^2, \\frac{\\Lambda^2}{(1-\\nu)^2} \\big\\} \\frac{|\\mathcal{S}_n|A}{(1-\\nu)^2\\varepsilon^2}\\log\\frac{t|\\mathcal{S}_n|A}{\\delta}\\big)$. Finally, the success probability comes from a simple union bound and the identity $\\sum_{t=1}^\\infty \\frac{1}{t^2} = \\frac{\\pi^2}{6}$.\n\\end{proof}\n    In the following, let $\\varepsilon_u, \\bar{\\varepsilon}_u > 0$\\varepsilon_u, \\bar{\\varepsilon}_u > 0 be such that $\\varepsilon_u := \\frac{1}{4}(1-\\nu)\\varepsilon$\\varepsilon_u := \\frac{1}{4}(1-\\nu)\\varepsilon and $\\bar{\\varepsilon}_u := \\varepsilon_u + (1+\\Lambda)Ln^{-\\alpha}$\\bar{\\varepsilon}_u := \\varepsilon_u + (1+\\Lambda)Ln^{-\\alpha}-\\alpha. We start by proving that \\Cref{algo:classical_extended_value_iteration} generates a sequence of functions $(u_t)_{t\\in\\mathbb{N}}$(u_t)_{t\\in\\mathbb{N}}t\\in\\mathbb{N} such that $\\|u_{t+1} - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}$\\|u_{t+1}t+1 - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}-\\alpha, $\\operatorname{sp}(u_{t+1}) \\leq \\operatorname{sp}(\\mathcal{L}u_t)$\\operatorname{sp}sp(u_{t+1}t+1) \\leq \\operatorname{sp}sp(\\mathcal{L}u_t), $\\operatorname{sp}(u_{t+1}) \\leq \\frac{2\\Lambda}{1-\\nu}$\\operatorname{sp}sp(u_{t+1}t+1) \\leq \\frac{2\\Lambda}{1-\\nu}, and $\\operatorname{sp}(u_t - h^\\ast) \\leq \\nu^t \\Lambda + \\frac{2\\bar{\\varepsilon}_u}{1-\\nu}$\\operatorname{sp}sp(u_t - h^\\ast) \\leq \\nu^t \\Lambda + \\frac{2\\bar{\\varepsilon}_u}{1-\\nu}, which also implies that\n    \n        \\operatorname{sp}sp(u_{t+1}t+1) \\leq \\operatorname{sp}sp(\\mathcal{L}u_t) \\leq \\operatorname{sp}sp(h^\\ast) + \\operatorname{sp}sp(\\mathcal{L}u_t - \\mathcal{L}h^\\ast) \\leq (1+\\nu^{t+1}t+1)\\Lambda + \\frac{2\\nu\\bar{\\varepsilon}_u}{1-\\nu}  \\leq 4\\Lambda + 3,\n    \n    assuming that $Ln^{-\\alpha} \\leq \\frac{1-\\nu}{\\nu}$Ln^{-\\alpha}-\\alpha \\leq \\frac{1-\\nu}{\\nu} and $\\varepsilon_u \\leq \\frac{1-\\nu}{2\\nu}$\\varepsilon_u \\leq \\frac{1-\\nu}{2\\nu}. For $t=0$t=0, $u_1(x) = \\max_{a\\in\\mathcal{A}}\\{r(s,a)\\}$u_1(x) = \\max_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{r(s,a)\\} for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n and $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s) and then $\\|u_1 - \\mathcal{L}0\\|_\\infty = \\max_{x\\in\\mathcal{X}}\\{u_1(x) - \\max_{a\\in\\mathcal{A}}r(x,a)\\} \\leq Ln^{-\\alpha}$\\|u_1 - \\mathcal{L}0\\|_\\infty = \\max_{x\\in\\mathcal{X}}x\\in\\mathcal{X}\\{u_1(x) - \\max_{a\\in\\mathcal{A}}a\\in\\mathcal{A}r(x,a)\\} \\leq Ln^{-\\alpha}-\\alpha. Moreover,\n    \n        \\operatorname{sp}sp(u_1 - h^\\ast) &\\leq \\operatorname{sp}sp(\\mathcal{L}0 - \\mathcal{L}h^\\ast) + \\operatorname{sp}sp(u_1 - \\mathcal{L}0) \\leq \\nu \\Lambda + 2Ln^{-\\alpha}-\\alpha \\leq \\nu\\Lambda + \\frac{2\\bar{\\varepsilon}_u}{1-\\nu},\\\\\n        \\operatorname{sp}sp(u_1) &\\leq \\operatorname{sp}sp(\\mathcal{L}0) \\leq \\operatorname{sp}sp(h^\\ast) + \\operatorname{sp}sp(\\mathcal{L}0 - \\mathcal{L}h^\\ast) \\leq (1+\\nu)\\Lambda \\leq \\frac{2\\Lambda}{1-\\nu},\n    \n    where we used that $\\mathcal{L}h^\\ast = h^\\ast + g^\\ast e$\\mathcal{L}h^\\ast = h^\\ast + g^\\ast e and the span-contractive property of $\\mathcal{L}$\\mathcal{L}.\n    \n    Fix now $t\\geq 1$t\\geq 1 and assume by induction that $(u_{t'})_{t'\\in[t]}$(u_{t'}t')_{t'\\in[t]}t'\\in[t] have been computed. By a Hoeffding bound (\\Cref{fact:hoeffding}) we have that, for all $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}, with probability $1-\\frac{6\\delta}{\\pi^2t^2|\\mathcal{S}_n|A}$1-\\frac{6\\delta}{\\pi^2t^2|\\mathcal{S}_n|A},\n    \n        \\left|\\mu_{t+1}t+1(s,a) - \\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|s,a)u_t(x') \\right| \\leq \\sqrt{\\frac{2}{m_t}\\ln\\frac{\\pi^2t^2|\\mathcal{S}_n|A}{6\\delta}} \\operatorname{sp}sp(u_t) \\leq \\frac{\\varepsilon_u}{2},\n    \n    using that $m_t := \\big\\lceil\\frac{32}{\\varepsilon_u^2}{\\min}\\big\\{4(1+\\Lambda)^2,\\frac{\\Lambda^2}{(1-\\nu)^2}\\big\\}\\ln\\frac{\\pi^2 t^2|\\mathcal{S}_n|A}{6\\delta}\\big\\rceil$m_t := \\big\\lceil\\frac{32}{\\varepsilon_u^2}{\\min}\\min\\big\\{4(1+\\Lambda)^2,\\frac{\\Lambda^2}{(1-\\nu)^2}\\big\\}\\ln\\frac{\\pi^2 t^2|\\mathcal{S}_n|A}{6\\delta}\\big\\rceil since $\\operatorname{sp}(u_t) \\leq {\\min}\\big\\{4(1+\\Lambda),\\frac{2\\Lambda}{1-\\nu}\\big\\}$\\operatorname{sp}sp(u_t) \\leq {\\min}\\min\\big\\{4(1+\\Lambda),\\frac{2\\Lambda}{1-\\nu}\\big\\} by induction\n    (we can subtract $\\min_{x'\\in\\mathcal{X}}u_t(x')$\\min_{x'\\in\\mathcal{X}}x'\\in\\mathcal{X}u_t(x') from $u_t(x)$u_t(x) so that $\\|u_t - \\min_{x'\\in\\mathcal{X}}\\{u_t(x')\\}e\\|_\\infty = \\operatorname{sp}(u_t)$\\|u_t - \\min_{x'\\in\\mathcal{X}}x'\\in\\mathcal{X}\\{u_t(x')\\}e\\|_\\infty = \\operatorname{sp}sp(u_t) since $u_t(x) \\geq 0$u_t(x) \\geq 0). For $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n, define $\\widetilde{u}_{t+1}(s) := \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}$\\widetilde{u}_{t+1}t+1(s) := \\max_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{r(s,a) + \\mu_{t+1}t+1(s,a)\\} and\n    \n        u_{t+1}t+1(s) \\!=\\! \n            \\max\\{\\max_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s')\\} &\\text{if}~\\widetilde{u}_{t+1}t+1(s) \\geq \\max_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\{\\min_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s')\\} &\\text{if}~ \\widetilde{u}_{t+1}t+1(s) \\leq \\min_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}t+1(s) &\\text{otherwise}.\n        \n    \n    In other words, the smaller entries of $\\widetilde{u}_{t+1}$\\widetilde{u}_{t+1}t+1 are increases by $\\frac{\\varepsilon_u}{2}$\\frac{\\varepsilon_u}{2}, while its larger entries are decreased by $\\frac{\\varepsilon_u}{2}$\\frac{\\varepsilon_u}{2}. This means that $\\operatorname{sp}(u_{t+1}) \\leq \\operatorname{sp}(\\mathcal{L}u_t)$\\operatorname{sp}sp(u_{t+1}t+1) \\leq \\operatorname{sp}sp(\\mathcal{L}u_t). Moreover, $|u_{t+1}(s) - (\\mathcal{L}u_t)(s)| \\leq \\varepsilon_u$|u_{t+1}t+1(s) - (\\mathcal{L}u_t)(s)| \\leq \\varepsilon_u for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n. Observe that, for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n and $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s),\n    \n        |(\\mathcal{L}u_{t}t)(s) - (\\mathcal{L}u_{t}t)(x)| &\\leq |r(s,a) - r(x,a)| + \\left|\\int_{\\mathcal{X}}\\mathcal{X} \\!(p(\\rd x'|s,a) - p(\\rd x'|x,a)) \\!\\left(\\! u_{t}t(x') - \\min_{x''\\in\\mathcal{X}}x''\\in\\mathcal{X}u_t(x'') \\!\\right) \\right| \\\\\n        &\\leq 4(1+\\Lambda)Ln^{-\\alpha}-\\alpha, \\tag{$\\operatorname{sp}(u_t) \\leq 4\\Lambda + 3$ by induction}$\\operatorname{sp}(u_t) \\leq 4\\Lambda + 3$\\operatorname{sp}sp(u_t) \\leq 4\\Lambda + 3 by induction\n    \n    where $a = \\argmax_{a'\\in\\mathcal{A}}(\\mathcal{L}_{a'}u_{t})(s)$a = \\argmax_{a'\\in\\mathcal{A}}a'\\in\\mathcal{A}(\\mathcal{L}_{a'}a'u_{t}t)(s). By defining $u_{t+1}(x) = u_{t+1}(s)$u_{t+1}t+1(x) = u_{t+1}t+1(s) for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n and $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s), then $\\|u_{t+1} - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}$\\|u_{t+1}t+1 - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}-\\alpha. Consider now the operator $\\bar{\\mathcal{L}}:\\mathscr{B}(\\mathcal{X})\\to\\mathscr{B}(\\mathcal{X})$\\bar{\\mathcal{L}}:\\mathscr{B}(\\mathcal{X})\\to\\mathscr{B}(\\mathcal{X}) such that $(\\bar{\\mathcal{L}}v)(x) = (\\mathcal{L}v)(s)$(\\bar{\\mathcal{L}}v)(x) = (\\mathcal{L}v)(s) for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n and $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s). Notice that $\\|\\mathcal{L}h^\\ast - \\bar{\\mathcal{L}}h^\\ast\\|_\\infty \\leq (1+\\Lambda)Ln^{-\\alpha}$\\|\\mathcal{L}h^\\ast - \\bar{\\mathcal{L}}h^\\ast\\|_\\infty \\leq (1+\\Lambda)Ln^{-\\alpha}-\\alpha similarly to above. Therefore,\n    \n        \\operatorname{sp}sp(u_{t+1}t+1 - h^\\ast) &\\leq \\operatorname{sp}sp(u_{t+1}t+1 - \\bar{\\mathcal{L}}u_t) + \\operatorname{sp}sp(\\bar{\\mathcal{L}}u_t - \\bar{\\mathcal{L}}h^\\ast) + \\operatorname{sp}sp(\\bar{\\mathcal{L}}h^\\ast - \\mathcal{L}h^\\ast)  \\tag{triangle inequality}triangle inequality\\\\\n        &\\leq 2\\varepsilon_u + \\nu\\operatorname{sp}sp(u_t - h^\\ast) + 2(1+\\Lambda)Ln^{-\\alpha}-\\alpha  \\tag{$\\operatorname{sp}(v) \\leq 2\\|v\\|_\\infty$ and $\\mathcal{L}$ contraction}$\\operatorname{sp}(v) \\leq 2\\|v\\|_\\infty$\\operatorname{sp}sp(v) \\leq 2\\|v\\|_\\infty and $\\mathcal{L}$\\mathcal{L} contraction\\\\\n        &\\leq 2\\bar{\\varepsilon}_u + \\nu^{t+1}t+1\\Lambda + \\frac{2\\nu}{1-\\nu}\\bar{\\varepsilon}_u \\tag{induction hypothesis}induction hypothesis\\\\\n        &= \\nu^{t+1}t+1\\Lambda + \\frac{2\\bar{\\varepsilon}_u}{1-\\nu}.\n    \n    On the other hand, it is also true that\n    \n        \\operatorname{sp}sp(u_{t+1}t+1) \\leq \\operatorname{sp}sp(\\mathcal{L}u_t) \\leq \\operatorname{sp}sp(h^\\ast) + \\operatorname{sp}sp(\\mathcal{L}u_t - \\mathcal{L}h^\\ast) \\leq \\Lambda + \\nu\\frac{2\\Lambda}{1-\\nu} \\leq \\frac{2\\Lambda}{1-\\nu}.\n    \n    This concludes the proof by induction that $\\|u_{t+1} - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}$\\|u_{t+1}t+1 - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}-\\alpha and $\\operatorname{sp}(u_{t+1}) \\leq {\\min}\\big\\{4\\Lambda + 3, \\frac{2\\Lambda}{1-\\nu}\\big\\}$\\operatorname{sp}sp(u_{t+1}t+1) \\leq {\\min}\\min\\big\\{4\\Lambda + 3, \\frac{2\\Lambda}{1-\\nu}\\big\\}.\n    \n    We now note that, since $\\|u_{t+1} - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}$\\|u_{t+1}t+1 - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}-\\alpha, \\Cref{lem:stopping_criteria} yields that\n    \n        \\operatorname{sp}sp(u_{t+1}t+1 - u_t) \\leq \\nu^t(1 + 2\\varepsilon_u + 8(1+\\Lambda)Ln^{-\\alpha}-\\alpha) + \\frac{4\\varepsilon_u + 16(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\qquad\\forall t\\in\\mathbb{N},\n    \n    using that $\\operatorname{sp}(\\mathcal{L}0) \\leq 1$\\operatorname{sp}sp(\\mathcal{L}0) \\leq 1.\n    Therefore, $\\operatorname{sp}(u_{t+1} - u_t) \\leq \\frac{6\\varepsilon_u + 18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$\\operatorname{sp}sp(u_{t+1}t+1 - u_t) \\leq \\frac{6\\varepsilon_u + 18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} if\n    \n        t\\geq t^\\ast := \\left\\lceil \\frac{\\log\\frac{(1-\\nu)(1+2\\varepsilon_u + 8(1+\\Lambda)Ln^{-\\alpha})}{2\\varepsilon_u + 2(1+\\Lambda)Ln^{-\\alpha}}}{\\log\\frac{1}{\\nu}} \\right\\rceil = O\\left(\\frac{\\log\\frac{1}{\\varepsilon}}{\\log\\frac{1}{\\nu}} \\right).\n    \n    We employ \\cref{lem:value_iteration_guarantees} to argue that, once $\\operatorname{sp}({{u}}_{t+1} - {{u}}_{t}) \\leq \\frac{6\\varepsilon_u + 18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$\\operatorname{sp}sp({{u}}{u}u_{t+1}t+1 - {{u}}{u}u_{t}t) \\leq \\frac{6\\varepsilon_u + 18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} for $t\\geq t^\\ast$t\\geq t^\\ast, then\n    \n        g^\\varepsilon = \\frac{1}{2}\\left(\\max_{x\\in\\mathcal{X}}x\\in\\mathcal{X}\\{{u}u_{t^\\ast + 1}t^\\ast + 1(x) - {u}u_{t^\\ast}t^\\ast(x)\\} + \\min_{x\\in\\mathcal{X}}x\\in\\mathcal{X}\\{{u}u_{t^\\ast + 1}t^\\ast + 1(x) - {u}u_{t^\\ast}t^\\ast(x)\\}\\right)\n    \n    is such that $|g^\\varepsilon - g^\\ast| \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha} + \\frac{3\\varepsilon_u + 9(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} < \\varepsilon + \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$|g^\\varepsilon - g^\\ast| \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}-\\alpha + \\frac{3\\varepsilon_u + 9(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} < \\varepsilon + \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}, while the policy $\\widetilde{d}^\\infty\\in\\Pi^{\\rm D}$\\widetilde{d}^\\infty\\in\\Pi^{\\rm D}\\rm D with $\\widetilde{d}(x) := \\argmax_{a\\in\\mathcal{A}}\\{u_{t^\\ast}(x)\\}$\\widetilde{d}(x) := \\argmax_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{u_{t^\\ast}t^\\ast(x)\\} for all $x\\in\\mathcal{X}$x\\in\\mathcal{X} is $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)-optimal.\n\n    We now analyse the query complexity of \\Cref{algo:classical_extended_value_iteration}. Computing $u_{t+1}$u_{t+1}t+1 for a fixed step $t\\in\\mathbb{N}$t\\in\\mathbb{N} requires $m_t|\\mathcal{S}_n|A $m_t|\\mathcal{S}_n|A  queries to $\\mathcal{C}_p$\\mathcal{C}_p, i.e., $O\\big({\\min}\\big\\{(1+\\Lambda)^2, \\frac{\\Lambda^2}{(1-\\nu)^2} \\big\\}\\frac{|\\mathcal{S}_n|A}{(1-\\nu)^2\\varepsilon^2} \\log\\frac{t|\\mathcal{S}_n|A}{\\delta}\\big)$O\\big({\\min}\\min\\big\\{(1+\\Lambda)^2, \\frac{\\Lambda^2}{(1-\\nu)^2} \\big\\}\\frac{|\\mathcal{S}_n|A}{(1-\\nu)^2\\varepsilon^2} \\log\\frac{t|\\mathcal{S}_n|A}{\\delta}\\big) samples. The total query complexity is then $O\\big(t^\\ast {\\min}\\big\\{(1+\\Lambda)^2, \\frac{\\Lambda^2}{(1-\\nu)^2} \\big\\} \\frac{|\\mathcal{S}_n|A}{(1-\\nu)^2\\varepsilon^2}\\log\\frac{t|\\mathcal{S}_n|A}{\\delta}\\big)$O\\big(t^\\ast {\\min}\\min\\big\\{(1+\\Lambda)^2, \\frac{\\Lambda^2}{(1-\\nu)^2} \\big\\} \\frac{|\\mathcal{S}_n|A}{(1-\\nu)^2\\varepsilon^2}\\log\\frac{t|\\mathcal{S}_n|A}{\\delta}\\big). Finally, the success probability comes from a simple union bound and the identity $\\sum_{t=1}^\\infty \\frac{1}{t^2} = \\frac{\\pi^2}{6}$\\sum_{t=1}t=1^\\infty \\frac{1}{t^2} = \\frac{\\pi^2}{6}.\n\n\n\n\n\\subsection{Quantum value iteration algorithm under a generative model}\n\nIn the same way that we quantised the finite-horizon classical backward iteration algorithm in \\Cref{sec:optimal_policies_finite-horizon} by approximating the quantities $\\int_{\\mathcal{X}}p(\\rd x'|x,a)u_{t}(x')$\\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|x,a)u_{t}t(x') using quantum mean estimation, a similar procedure can be performed for the aforementioned classical value iteration algorithm on infinite-horizon MDPs. The resulting algorithm, shown in \\cref{algo:quantum_extended_value_iteration}, achieves a better query complexity dependence on the action space size $A$A, error parameter $\\varepsilon$\\varepsilon, and bias span upper bound $\\Lambda$\\Lambda.\n\n\\begin{algorithm}[t!]\n    \\caption{Quantum value iteration algorithm}\n    \\label{algo:quantum_extended_value_iteration}\n    \\begin{algorithmic}[1]  \n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$, finite action space $\\mathcal{A}$, quantum sampling access to probability kernels $p$, failure probability $\\delta\\in(0, 1)$, parameters $\\Lambda>0$, $\\nu\\in[0,1)$, $L,\\alpha \\geq 0$, errors $\\varepsilon_u, \\varepsilon > 0$ such that $\\varepsilon_u = \\frac{1}{4}(1-\\nu)\\varepsilon$.\n\n    \\Ensure $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$-optimal stationary policy $\\widetilde{d}^\\infty$. \n\n    \\State $t\\gets 1$ and initialise ${u}_{0} \\gets {0}$ \n    \\State $u_{1}(x) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a)\\}$ for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$ with probability $1-\\frac{6\\delta}{\\pi^2 |\\mathcal{S}_n|}$ (\\Cref{fact:quantum_minimum_finding})\n\n    \\While {$\\operatorname{sp}({{u}}_{t} - {{u}}_{t-1}) > \\frac{3\\varepsilon}{2} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$}\n    \n    \\State Build quantum access to ${{u}}_{t}$\n    \\For{$s\\in\\mathcal{S}_n$}\n        %\\State $\\widetilde\\mu_{\\max}(s, a)\\gets$ Run Algorithm~\\ref{algo:inner_maximization} with additive error $\\frac{\\epsilon}{2}$ and success probability $1-\\frac{\\delta}{3}$ using Lemma~\\ref{lem:quantum_inner_maximization}. \\label{algo_line:quantum_inner_maximization}\n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use \\cref{lem:quantum_mean_estimation} to obtain a unitary $\\mathcal{U}^{(t)}_s:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|{\\mu}_{t}(s,a)\\rangle$ for all $a\\in\\mathcal{A}$, where $|{\\mu}_{t}(s,a) - \\int_{\\mathcal{X}} p(\\rd x'|s,a) {u}_{t}(x') | \\leq \\frac{\\varepsilon_u}{2}$ with high probability}\n        \n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use quantum maximum finding with unitary $\\mathcal{U}_s^{(t)}$ (\\cref{fact:quantum_minimum_finding} with $\\mathcal{U}^{(t)}_s$) to get $\\widetilde{u}_{t+1}(s)$ and $a_{t+1}(s)$ such that, with probability $1-\\frac{6\\delta}{\\pi^2 t^2 |\\mathcal{S}_n|}$,\n        %\n        \\begin{align*}\n            \\widetilde{u}_{t+1}(s) = \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\} \\quad\\text{and}\\quad a_{t+1}(s) = \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}\n        \\end{align*}}\n    \\EndFor\n\n    \\State For all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$, $a_{t+1}(x) \\gets a_{t+1}(s)$ and\n    %\n    \\begin{align*}\n        u_{t+1}(x) \\gets \\begin{dcases}\n            \\max\\left\\{\\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~\\widetilde{u}_{t+1}(s) \\geq \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\left\\{\\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~ \\widetilde{u}_{t+1}(s) \\leq \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}(s) &\\text{otherwise}.\n        \\end{dcases}\n    \\end{align*}\n\n\n    \\State $t \\gets t+1$\n\n    \\EndWhile\n    \n    \\State \\Return $\\widetilde{d}\\in\\mathcal{D}^{\\rm D}$ where $\\widetilde{d}(x) := a_t(x)$ for all $x\\in\\mathcal{X}$\n\\end{algorithmic}\n\\end{algorithm}\\begin{algorithm}[t!]\n    \\caption{Quantum value iteration algorithm}\n    \\label{algo:quantum_extended_value_iteration}\n    \\begin{algorithmic}[1]  \n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$, finite action space $\\mathcal{A}$, quantum sampling access to probability kernels $p$, failure probability $\\delta\\in(0, 1)$, parameters $\\Lambda>0$, $\\nu\\in[0,1)$, $L,\\alpha \\geq 0$, errors $\\varepsilon_u, \\varepsilon > 0$ such that $\\varepsilon_u = \\frac{1}{4}(1-\\nu)\\varepsilon$.\n\n    \\Ensure $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$-optimal stationary policy $\\widetilde{d}^\\infty$. \n\n    \\State $t\\gets 1$ and initialise ${u}_{0} \\gets {0}$ \n    \\State $u_{1}(x) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a)\\}$ for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$ with probability $1-\\frac{6\\delta}{\\pi^2 |\\mathcal{S}_n|}$ (\\Cref{fact:quantum_minimum_finding})\n\n    \\While {$\\operatorname{sp}({{u}}_{t} - {{u}}_{t-1}) > \\frac{3\\varepsilon}{2} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$}\n    \n    \\State Build quantum access to ${{u}}_{t}$\n    \\For{$s\\in\\mathcal{S}_n$}\n        %\\State $\\widetilde\\mu_{\\max}(s, a)\\gets$ Run Algorithm~\\ref{algo:inner_maximization} with additive error $\\frac{\\epsilon}{2}$ and success probability $1-\\frac{\\delta}{3}$ using Lemma~\\ref{lem:quantum_inner_maximization}. \\label{algo_line:quantum_inner_maximization}\n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use \\cref{lem:quantum_mean_estimation} to obtain a unitary $\\mathcal{U}^{(t)}_s:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|{\\mu}_{t}(s,a)\\rangle$ for all $a\\in\\mathcal{A}$, where $|{\\mu}_{t}(s,a) - \\int_{\\mathcal{X}} p(\\rd x'|s,a) {u}_{t}(x') | \\leq \\frac{\\varepsilon_u}{2}$ with high probability}\n        \n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{Use quantum maximum finding with unitary $\\mathcal{U}_s^{(t)}$ (\\cref{fact:quantum_minimum_finding} with $\\mathcal{U}^{(t)}_s$) to get $\\widetilde{u}_{t+1}(s)$ and $a_{t+1}(s)$ such that, with probability $1-\\frac{6\\delta}{\\pi^2 t^2 |\\mathcal{S}_n|}$,\n        %\n        \\begin{align*}\n            \\widetilde{u}_{t+1}(s) = \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\} \\quad\\text{and}\\quad a_{t+1}(s) = \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}\n        \\end{align*}}\n    \\EndFor\n\n    \\State For all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$, $a_{t+1}(x) \\gets a_{t+1}(s)$ and\n    %\n    \\begin{align*}\n        u_{t+1}(x) \\gets \\begin{dcases}\n            \\max\\left\\{\\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~\\widetilde{u}_{t+1}(s) \\geq \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\left\\{\\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\right\\} &\\text{if}~ \\widetilde{u}_{t+1}(s) \\leq \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}(s) &\\text{otherwise}.\n        \\end{dcases}\n    \\end{align*}\n\n\n    \\State $t \\gets t+1$\n\n    \\EndWhile\n    \n    \\State \\Return $\\widetilde{d}\\in\\mathcal{D}^{\\rm D}$ where $\\widetilde{d}(x) := a_t(x)$ for all $x\\in\\mathcal{X}$\n\\end{algorithmic}\n\\end{algorithm}[t!]\n    \\caption{Quantum value iteration algorithm}\n    \\label{algo:quantum_extended_value_iteration}\n    [1]  \n    \\Require Compact state space $\\mathcal{X}$\\mathcal{X} with $\\frac{1}{n}$\\frac{1}{n}-net $\\mathcal{S}_n$\\mathcal{S}_n, finite action space $\\mathcal{A}$\\mathcal{A}, quantum sampling access to probability kernels $p$p, failure probability $\\delta\\in(0, 1)$\\delta\\in(0, 1), parameters $\\Lambda>0$\\Lambda>0, $\\nu\\in[0,1)$\\nu\\in[0,1), $L,\\alpha \\geq 0$L,\\alpha \\geq 0, errors $\\varepsilon_u, \\varepsilon > 0$\\varepsilon_u, \\varepsilon > 0 such that $\\varepsilon_u = \\frac{1}{4}(1-\\nu)\\varepsilon$\\varepsilon_u = \\frac{1}{4}(1-\\nu)\\varepsilon.\n\n    \\Ensure $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)-optimal stationary policy $\\widetilde{d}^\\infty$\\widetilde{d}^\\infty. \n\n    \\State $t\\gets 1$t\\gets 1 and initialise ${u}_{0} \\gets {0}${u}u_{0}0 \\gets {0}0 \n    \\State $u_{1}(x) \\gets \\max_{a\\in\\mathcal{A}}\\{r(s,a)\\}$u_{1}1(x) \\gets \\max_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{r(s,a)\\} for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n and $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s) with probability $1-\\frac{6\\delta}{\\pi^2 |\\mathcal{S}_n|}$1-\\frac{6\\delta}{\\pi^2 |\\mathcal{S}_n|} (\\Cref{fact:quantum_minimum_finding})\n\n    \\While {$\\operatorname{sp}({{u}}_{t} - {{u}}_{t-1}) > \\frac{3\\varepsilon}{2} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$}$\\operatorname{sp}({{u}}_{t} - {{u}}_{t-1}) > \\frac{3\\varepsilon}{2} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$\\operatorname{sp}sp({{u}}{u}u_{t}t - {{u}}{u}u_{t-1}t-1) > \\frac{3\\varepsilon}{2} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\n    \n    \\State Build quantum access to ${{u}}_{t}${{u}}{u}u_{t}t\n    \\For{$s\\in\\mathcal{S}_n$}$s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n\n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent{Use \\cref{lem:quantum_mean_estimation} to obtain a unitary $\\mathcal{U}^{(t)}_s:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|{\\mu}_{t}(s,a)\\rangle$ for all $a\\in\\mathcal{A}$, where $|{\\mu}_{t}(s,a) - \\int_{\\mathcal{X}} p(\\rd x'|s,a) {u}_{t}(x') | \\leq \\frac{\\varepsilon_u}{2}$ with high probability}Use \\cref{lem:quantum_mean_estimation} to obtain a unitary $\\mathcal{U}^{(t)}_s:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|{\\mu}_{t}(s,a)\\rangle$\\mathcal{U}^{(t)}(t)_s:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|{\\mu}\\mu_{t}t(s,a)\\rangle for all $a\\in\\mathcal{A}$a\\in\\mathcal{A}, where $|{\\mu}_{t}(s,a) - \\int_{\\mathcal{X}} p(\\rd x'|s,a) {u}_{t}(x') | \\leq \\frac{\\varepsilon_u}{2}$|{\\mu}\\mu_{t}t(s,a) - \\int_{\\mathcal{X}}\\mathcal{X} p(\\rd x'|s,a) {u}u_{t}t(x') | \\leq \\frac{\\varepsilon_u}{2} with high probability\n        \n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent{Use quantum maximum finding with unitary $\\mathcal{U}_s^{(t)}$ (\\cref{fact:quantum_minimum_finding} with $\\mathcal{U}^{(t)}_s$) to get $\\widetilde{u}_{t+1}(s)$ and $a_{t+1}(s)$ such that, with probability $1-\\frac{6\\delta}{\\pi^2 t^2 |\\mathcal{S}_n|}$,\n        %\n        \\begin{align*}\n            \\widetilde{u}_{t+1}(s) = \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\} \\quad\\text{and}\\quad a_{t+1}(s) = \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}\n        \\end{align*}}Use quantum maximum finding with unitary $\\mathcal{U}_s^{(t)}$\\mathcal{U}_s^{(t)}(t) (\\cref{fact:quantum_minimum_finding} with $\\mathcal{U}^{(t)}_s$\\mathcal{U}^{(t)}(t)_s) to get $\\widetilde{u}_{t+1}(s)$\\widetilde{u}_{t+1}t+1(s) and $a_{t+1}(s)$a_{t+1}t+1(s) such that, with probability $1-\\frac{6\\delta}{\\pi^2 t^2 |\\mathcal{S}_n|}$1-\\frac{6\\delta}{\\pi^2 t^2 |\\mathcal{S}_n|},\n        \n            \\widetilde{u}_{t+1}t+1(s) = \\max_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{r(s,a) + \\mu_{t+1}t+1(s,a)\\} \\quad\\text{and}\\quad a_{t+1}t+1(s) = \\argmax_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{r(s,a) + \\mu_{t+1}t+1(s,a)\\}\n        \n    \\EndFor\n\n    \\State For all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n and $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s), $a_{t+1}(x) \\gets a_{t+1}(s)$a_{t+1}t+1(x) \\gets a_{t+1}t+1(s) and\n    \n        u_{t+1}t+1(x) \\gets \n            \\max\\left\\{\\max_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s')\\right\\} &\\text{if}~\\widetilde{u}_{t+1}t+1(s) \\geq \\max_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\left\\{\\min_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s')\\right\\} &\\text{if}~ \\widetilde{u}_{t+1}t+1(s) \\leq \\min_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}t+1(s) &\\text{otherwise}.\n        \n    \n\n\n    \\State $t \\gets t+1$t \\gets t+1\n\n    \\EndWhile\n    \n    \\State \\Return $\\widetilde{d}\\in\\mathcal{D}^{\\rm D}$\\widetilde{d}\\in\\mathcal{D}^{\\rm D}\\rm D where $\\widetilde{d}(x) := a_t(x)$\\widetilde{d}(x) := a_t(x) for all $x\\in\\mathcal{X}$x\\in\\mathcal{X}\n\n\n\n\n\\begin{theorem}[Quantum infinite-horizon generative algorithm]\\label{thr:quantum_scopt_algorithm}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ be an infinite-horizon weakly communicating MDP with $\\operatorname{sp}({h}^\\ast) \\leq \\Lambda$. Let $\\mathcal{S}_n$ be a $\\frac{1}{n}$-net for $\\mathcal{X}$. Assume the optimal Bellman operator $\\mathcal{L}$ of $M$ is a $1$-stage $\\nu$-span contraction for $\\nu\\in[0,1)$. Assume quantum quantum sampling access to $p$ via oracle $\\mathcal{O}_p$. Let $\\delta\\in (0, 1)$ and $\\varepsilon \\in (0,\\frac{2}{\\nu}]$. Under {\\rm \\Cref{ass:Lipschitz}} with parameters $L,\\alpha \\geq 0$ such that $Ln^{-\\alpha} \\leq \\frac{1-\\nu}{\\nu}$, {\\rm \\cref{algo:quantum_extended_value_iteration}} outputs an $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$-optimal policy $\\widetilde{d}^\\infty$ and $g^\\varepsilon$ such that $|g^\\varepsilon - g^\\ast| < \\varepsilon + \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$ with probability $1-\\delta$. Its $\\mathcal{O}_p,\\mathcal{O}_p^\\dagger$-query complexity is (up to $\\poly\\log\\log$ factors in $1/\\varepsilon$ and $\\nu$)\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\min\\left\\{1+\\Lambda,\\frac{\\Lambda}{1-\\nu}\\right\\}\\frac{|\\mathcal{S}_n|\\sqrt{A}}{(1-\\nu)\\varepsilon}\\frac{\\log\\frac{1}{\\varepsilon}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|A}{\\delta}\\log\\frac{|\\mathcal{S}_n|}{\\delta}\\right).\n    \\end{align*}\n    % \n    % plus $\\widetilde{O}\\Big(\\frac{J^3}{(1-\\nu)^2} \\frac{\\Lambda|\\mathcal{S}_n|\\sqrt{A}}{\\varepsilon}\\Big)$ elementary gates.\n\\end{theorem}\\begin{theorem}[Quantum infinite-horizon generative algorithm]\\label{thr:quantum_scopt_algorithm}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ be an infinite-horizon weakly communicating MDP with $\\operatorname{sp}({h}^\\ast) \\leq \\Lambda$. Let $\\mathcal{S}_n$ be a $\\frac{1}{n}$-net for $\\mathcal{X}$. Assume the optimal Bellman operator $\\mathcal{L}$ of $M$ is a $1$-stage $\\nu$-span contraction for $\\nu\\in[0,1)$. Assume quantum quantum sampling access to $p$ via oracle $\\mathcal{O}_p$. Let $\\delta\\in (0, 1)$ and $\\varepsilon \\in (0,\\frac{2}{\\nu}]$. Under {\\rm \\Cref{ass:Lipschitz}} with parameters $L,\\alpha \\geq 0$ such that $Ln^{-\\alpha} \\leq \\frac{1-\\nu}{\\nu}$, {\\rm \\cref{algo:quantum_extended_value_iteration}} outputs an $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$-optimal policy $\\widetilde{d}^\\infty$ and $g^\\varepsilon$ such that $|g^\\varepsilon - g^\\ast| < \\varepsilon + \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$ with probability $1-\\delta$. Its $\\mathcal{O}_p,\\mathcal{O}_p^\\dagger$-query complexity is (up to $\\poly\\log\\log$ factors in $1/\\varepsilon$ and $\\nu$)\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\min\\left\\{1+\\Lambda,\\frac{\\Lambda}{1-\\nu}\\right\\}\\frac{|\\mathcal{S}_n|\\sqrt{A}}{(1-\\nu)\\varepsilon}\\frac{\\log\\frac{1}{\\varepsilon}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|A}{\\delta}\\log\\frac{|\\mathcal{S}_n|}{\\delta}\\right).\n    \\end{align*}\n    % \n    % plus $\\widetilde{O}\\Big(\\frac{J^3}{(1-\\nu)^2} \\frac{\\Lambda|\\mathcal{S}_n|\\sqrt{A}}{\\varepsilon}\\Big)$ elementary gates.\n\\end{theorem}\\label{thr:quantum_scopt_algorithm}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle be an infinite-horizon weakly communicating MDP with $\\operatorname{sp}({h}^\\ast) \\leq \\Lambda$\\operatorname{sp}sp({h}h^\\ast) \\leq \\Lambda. Let $\\mathcal{S}_n$\\mathcal{S}_n be a $\\frac{1}{n}$\\frac{1}{n}-net for $\\mathcal{X}$\\mathcal{X}. Assume the optimal Bellman operator $\\mathcal{L}$\\mathcal{L} of $M$M is a $1$1-stage $\\nu$\\nu-span contraction for $\\nu\\in[0,1)$\\nu\\in[0,1). Assume quantum quantum sampling access to $p$p via oracle $\\mathcal{O}_p$\\mathcal{O}_p. Let $\\delta\\in (0, 1)$\\delta\\in (0, 1) and $\\varepsilon \\in (0,\\frac{2}{\\nu}]$\\varepsilon \\in (0,\\frac{2}{\\nu}]. Under {\\rm \\Cref{ass:Lipschitz}}\\rm \\Cref{ass:Lipschitz} with parameters $L,\\alpha \\geq 0$L,\\alpha \\geq 0 such that $Ln^{-\\alpha} \\leq \\frac{1-\\nu}{\\nu}$Ln^{-\\alpha}-\\alpha \\leq \\frac{1-\\nu}{\\nu}, {\\rm \\cref{algo:quantum_extended_value_iteration}}\\rm \\cref{algo:quantum_extended_value_iteration} outputs an $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)-optimal policy $\\widetilde{d}^\\infty$\\widetilde{d}^\\infty and $g^\\varepsilon$g^\\varepsilon such that $|g^\\varepsilon - g^\\ast| < \\varepsilon + \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$|g^\\varepsilon - g^\\ast| < \\varepsilon + \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} with probability $1-\\delta$1-\\delta. Its $\\mathcal{O}_p,\\mathcal{O}_p^\\dagger$\\mathcal{O}_p,\\mathcal{O}_p^\\dagger-query complexity is (up to $\\poly\\log\\log$\\poly\\log\\log factors in $1/\\varepsilon$1/\\varepsilon and $\\nu$\\nu)\n    \n        \\widetilde{O}\\left(\\min\\left\\{1+\\Lambda,\\frac{\\Lambda}{1-\\nu}\\right\\}\\frac{|\\mathcal{S}_n|\\sqrt{A}}{(1-\\nu)\\varepsilon}\\frac{\\log\\frac{1}{\\varepsilon}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|A}{\\delta}\\log\\frac{|\\mathcal{S}_n|}{\\delta}\\right).\n    \n    \n\\begin{proof}\n    In the following, let $\\varepsilon_u, \\bar{\\varepsilon}_u > 0$ such that $\\varepsilon_u := \\frac{1}{4}(1-\\nu)\\varepsilon$ and $\\bar{\\varepsilon}_u := \\varepsilon_u + (1+\\Lambda)Ln^{-\\alpha}$. The proof of correctness is basically the same as \\Cref{thr:classical_scopt_algorithm}, the main difference being the computation of $\\mu_{t+1}(s,a)$ and thus $u_{t+1}$. Again by induction on $t$ we would like to prove that \\Cref{algo:quantum_extended_value_iteration} outputs a sequence of functions $(u_t)_{t\\in\\mathbb{N}}$ such that $\\|u_{t+1} - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}$, $\\operatorname{sp}(u_{t+1}) \\leq \\operatorname{sp}(\\mathcal{L}u_t)$, $\\operatorname{sp}(u_{t+1}) \\leq {\\min}\\big\\{4\\Lambda + 3, \\frac{2\\Lambda}{1-\\nu}\\big\\}$, and $\\operatorname{sp}(u_t - h^\\ast) \\leq \\nu^t \\Lambda + \\frac{2\\bar{\\varepsilon}_u}{1-\\nu}$.\n\n    The case $t=0$ is the same as in \\Cref{thr:classical_scopt_algorithm}. For $t\\geq 1$, we must show how to approximate $\\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t(x')$ for all $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$. We claim that \\cref{lem:quantum_mean_estimation} can be adapted to perform all its steps in superposition without any need for intermediary measurements. This means that, given an initial state $\\sum_{a\\in\\mathcal{A}}\\alpha_a |s,a\\rangle|\\bar{0}\\rangle$ for fixed $s\\in\\mathcal{S}_n$ and some $\\{\\alpha_a\\}_{a\\in\\mathcal{A}}\\subset\\mathbb{C}$, \\cref{lem:quantum_mean_estimation} can generate the mapping\n    %\n    \\begin{align*}\n        \\sum_{a\\in\\mathcal{A}}\\alpha_a |s,a\\rangle|\\bar{0}\\rangle \\mapsto \\sum_{a\\in\\mathcal{A}}\\alpha_a |s,a\\rangle|\\mu_{t+1}(s,a)\\rangle|\\operatorname{garbage}(a)\\rangle,\n    \\end{align*}\n    %\n    where $\\{|\\operatorname{garbage}(a)\\rangle\\}_{a\\in\\mathcal{A}}$ are ``garbage'' unit complex vectors accumulated through the computation that we ignore and\n    %\n    \\begin{align*}\n        \\left| {\\mu}_{t+1}(s, a) -\\int_{\\mathcal{X}} p(\\rd x'|s,a) {u}_{t}(x') \\right| \\leq \\frac{\\varepsilon_u}{2} \\qquad \\forall(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}.\n    \\end{align*}\n    %\n    It is then possible to effectively construct a black-box unitary $\\mathcal{U}_s^{(t)}:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|{\\mu}_{t+1}(s,a)\\rangle$ (up to garbage states) using oracle $\\mathcal{O}_p$. The maximum over $a\\in\\mathcal{A}$ of $r(s,a) + \\mu_{t+1}(s,a)$ can thus be found by using the quantum max-finding subroutine (\\cref{fact:quantum_minimum_finding}) with unitary $\\mathcal{U}_s^{(t)}$. This leads to $\\widetilde{u}_{t+1}(s) = \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}$ and $a_{t+1}(s) = \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}$ such that $|\\widetilde{u}_{t+1}(s) - (\\mathcal{L} {{u}}_{t})(s)| \\leq \\frac{\\varepsilon_u}{2}$ for all $s\\in\\mathcal{S}_n$. Again, by subtracting $\\frac{\\varepsilon_u}{2}$ from the larger entries of $\\widetilde{u}_{t+1}$ and adding $\\frac{\\varepsilon_u}{2}$ to the smaller entries of $\\widetilde{u}_{t+1}$, we define, for all $s\\in\\mathcal{S}_n$ and $\\mathcal{X}(s)$,\n    %\n    \\begin{align*}\n        u_{t+1}(x) \\!=\\! \\begin{cases}\n            \\max\\{\\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\} &\\text{if}~\\widetilde{u}_{t+1}(s) \\geq \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\{\\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\} &\\text{if}~ \\widetilde{u}_{t+1}(s) \\leq \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}(s) &\\text{otherwise}.\n        \\end{cases}\n    \\end{align*}\n    %\n    Using that $|(\\mathcal{L}u_t)(s) - (\\mathcal{L}u_t)(x)| \\leq 4(1+\\Lambda)Ln^{-\\alpha}$ for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$ (since $\\operatorname{sp}(u_t) \\leq 4\\Lambda + 3$ by induction), we once again conclude that $\\|u_{t+1} - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}$. Proving that $\\operatorname{sp}(u_{t+1}) \\leq {\\min}\\big\\{4\\Lambda + 3,\\frac{2\\Lambda}{1-\\nu} \\big\\}$ is done exactly the same as in \\Cref{thr:classical_scopt_algorithm}.\n\n    By \\Cref{lem:stopping_criteria}, $\\operatorname{sp}(u_{t+1} - u_t) \\leq \\frac{6\\varepsilon_u + 18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$ if $t \\geq t^\\ast = O(\\log(1/\\varepsilon)/\\log(1/\\nu))$. We employ \\cref{lem:value_iteration_guarantees} to argue that, once $\\operatorname{sp}({{u}}_{t+1} - {{u}}_{t}) \\leq \\frac{6\\varepsilon_u + 18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$ for $t\\geq t^\\ast$, the output\n    %\n    \\begin{align*}\n        g^\\varepsilon = \\frac{1}{2}\\left(\\max_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast + 1}(x) - {u}_{t^\\ast}(x)\\} + \\min_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast + 1}(x) - {u}_{t^\\ast}(x)\\}\\right)\n    \\end{align*}\n    %\n    is such that $|g^\\varepsilon - g^\\ast| \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha} + \\frac{3\\varepsilon_u + 9(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} < \\varepsilon + \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$, while the policy $\\widetilde{d}^\\infty\\in\\Pi^{\\rm D}$ with $\\widetilde{d}(x) := \\argmax_{a\\in\\mathcal{A}}\\{u_{t^\\ast}(x)\\}$ for all $x\\in\\mathcal{X}$ is $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$-optimal.\n\n    The analysis of the success probability of \\Cref{algo:quantum_extended_value_iteration} is very similar to the one in \\Cref{thr:quantum_finite-horizon2}. Taking into account the oracle failure probabilities, outputting $\\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}$ fails with probability at most $\\frac{6\\delta}{\\pi^2 t^2 |\\mathcal{S}_n|}$. By a usual union bound over all $s\\in\\mathcal{S}_n$ and $t\\in\\mathbb{N}$, the failure probability is at most $\\delta$, since $\\sum_{t=1}^\\infty \\frac{1}{t^2} = \\frac{\\pi^2}{6}$.\n    \n    Finally, for the query complexity of \\Cref{algo:quantum_extended_value_iteration}, we start from obtaining ${{u}}_{t+1}$ given ${{u}}_{t}$. For every $s\\in\\mathcal S_n$, each call to the unitary $\\mathcal{U}_s^{(t)}$ uses $O\\big(\\frac{\\operatorname{sp}(u_t)}{\\varepsilon_u}\\log\\frac{t|\\mathcal{S}_n|A}{\\delta}\\big)$ queries to $\\mathcal{O}_p$, while quantum maximum finding makes $O\\big(\\sqrt{A}\\log\\frac{t|\\mathcal{S}_n|}{\\delta}\\big)$ queries to $\\mathcal{U}_s^{(t)}$. Thus the query complexity of computing $u_{t+1}(s)$ (and $a_{t+1}(s)$) is $O\\big({\\min}\\big\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu}\\big\\}\\frac{\\sqrt{A}}{\\varepsilon_u}\\log\\frac{t|\\mathcal{S}_n|A}{\\delta}\\log\\frac{t|\\mathcal{S}_n|}{\\delta}\\big)$, already using that $\\operatorname{sp}(u_t) \\leq {\\min}\\big\\{4\\Lambda + 3, \\frac{2\\Lambda}{1-\\nu}\\big\\}$. Summing over all $s\\in\\mathcal{S}_n$ and $t\\leq t^\\ast$, the total query complexity of \\Cref{algo:quantum_extended_value_iteration} is simply $O\\big(t^\\ast {\\min}\\big\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu} \\big\\}\\frac{|\\mathcal{S}_n|\\sqrt{A}}{(1-\\nu)\\varepsilon}\\log\\frac{t^\\ast|\\mathcal{S}_n|A}{\\delta}\\log\\frac{t^\\ast|\\mathcal{S}_n|}{\\delta}\\big)$.\n\\end{proof}\\begin{proof}\n    In the following, let $\\varepsilon_u, \\bar{\\varepsilon}_u > 0$ such that $\\varepsilon_u := \\frac{1}{4}(1-\\nu)\\varepsilon$ and $\\bar{\\varepsilon}_u := \\varepsilon_u + (1+\\Lambda)Ln^{-\\alpha}$. The proof of correctness is basically the same as \\Cref{thr:classical_scopt_algorithm}, the main difference being the computation of $\\mu_{t+1}(s,a)$ and thus $u_{t+1}$. Again by induction on $t$ we would like to prove that \\Cref{algo:quantum_extended_value_iteration} outputs a sequence of functions $(u_t)_{t\\in\\mathbb{N}}$ such that $\\|u_{t+1} - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}$, $\\operatorname{sp}(u_{t+1}) \\leq \\operatorname{sp}(\\mathcal{L}u_t)$, $\\operatorname{sp}(u_{t+1}) \\leq {\\min}\\big\\{4\\Lambda + 3, \\frac{2\\Lambda}{1-\\nu}\\big\\}$, and $\\operatorname{sp}(u_t - h^\\ast) \\leq \\nu^t \\Lambda + \\frac{2\\bar{\\varepsilon}_u}{1-\\nu}$.\n\n    The case $t=0$ is the same as in \\Cref{thr:classical_scopt_algorithm}. For $t\\geq 1$, we must show how to approximate $\\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t(x')$ for all $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$. We claim that \\cref{lem:quantum_mean_estimation} can be adapted to perform all its steps in superposition without any need for intermediary measurements. This means that, given an initial state $\\sum_{a\\in\\mathcal{A}}\\alpha_a |s,a\\rangle|\\bar{0}\\rangle$ for fixed $s\\in\\mathcal{S}_n$ and some $\\{\\alpha_a\\}_{a\\in\\mathcal{A}}\\subset\\mathbb{C}$, \\cref{lem:quantum_mean_estimation} can generate the mapping\n    %\n    \\begin{align*}\n        \\sum_{a\\in\\mathcal{A}}\\alpha_a |s,a\\rangle|\\bar{0}\\rangle \\mapsto \\sum_{a\\in\\mathcal{A}}\\alpha_a |s,a\\rangle|\\mu_{t+1}(s,a)\\rangle|\\operatorname{garbage}(a)\\rangle,\n    \\end{align*}\n    %\n    where $\\{|\\operatorname{garbage}(a)\\rangle\\}_{a\\in\\mathcal{A}}$ are ``garbage'' unit complex vectors accumulated through the computation that we ignore and\n    %\n    \\begin{align*}\n        \\left| {\\mu}_{t+1}(s, a) -\\int_{\\mathcal{X}} p(\\rd x'|s,a) {u}_{t}(x') \\right| \\leq \\frac{\\varepsilon_u}{2} \\qquad \\forall(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}.\n    \\end{align*}\n    %\n    It is then possible to effectively construct a black-box unitary $\\mathcal{U}_s^{(t)}:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|{\\mu}_{t+1}(s,a)\\rangle$ (up to garbage states) using oracle $\\mathcal{O}_p$. The maximum over $a\\in\\mathcal{A}$ of $r(s,a) + \\mu_{t+1}(s,a)$ can thus be found by using the quantum max-finding subroutine (\\cref{fact:quantum_minimum_finding}) with unitary $\\mathcal{U}_s^{(t)}$. This leads to $\\widetilde{u}_{t+1}(s) = \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}$ and $a_{t+1}(s) = \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}$ such that $|\\widetilde{u}_{t+1}(s) - (\\mathcal{L} {{u}}_{t})(s)| \\leq \\frac{\\varepsilon_u}{2}$ for all $s\\in\\mathcal{S}_n$. Again, by subtracting $\\frac{\\varepsilon_u}{2}$ from the larger entries of $\\widetilde{u}_{t+1}$ and adding $\\frac{\\varepsilon_u}{2}$ to the smaller entries of $\\widetilde{u}_{t+1}$, we define, for all $s\\in\\mathcal{S}_n$ and $\\mathcal{X}(s)$,\n    %\n    \\begin{align*}\n        u_{t+1}(x) \\!=\\! \\begin{cases}\n            \\max\\{\\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\} &\\text{if}~\\widetilde{u}_{t+1}(s) \\geq \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\{\\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s')\\} &\\text{if}~ \\widetilde{u}_{t+1}(s) \\leq \\min_{s'\\in\\mathcal{S}_n}\\widetilde{u}_{t+1}(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}(s) &\\text{otherwise}.\n        \\end{cases}\n    \\end{align*}\n    %\n    Using that $|(\\mathcal{L}u_t)(s) - (\\mathcal{L}u_t)(x)| \\leq 4(1+\\Lambda)Ln^{-\\alpha}$ for all $s\\in\\mathcal{S}_n$ and $x\\in\\mathcal{X}(s)$ (since $\\operatorname{sp}(u_t) \\leq 4\\Lambda + 3$ by induction), we once again conclude that $\\|u_{t+1} - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}$. Proving that $\\operatorname{sp}(u_{t+1}) \\leq {\\min}\\big\\{4\\Lambda + 3,\\frac{2\\Lambda}{1-\\nu} \\big\\}$ is done exactly the same as in \\Cref{thr:classical_scopt_algorithm}.\n\n    By \\Cref{lem:stopping_criteria}, $\\operatorname{sp}(u_{t+1} - u_t) \\leq \\frac{6\\varepsilon_u + 18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$ if $t \\geq t^\\ast = O(\\log(1/\\varepsilon)/\\log(1/\\nu))$. We employ \\cref{lem:value_iteration_guarantees} to argue that, once $\\operatorname{sp}({{u}}_{t+1} - {{u}}_{t}) \\leq \\frac{6\\varepsilon_u + 18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$ for $t\\geq t^\\ast$, the output\n    %\n    \\begin{align*}\n        g^\\varepsilon = \\frac{1}{2}\\left(\\max_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast + 1}(x) - {u}_{t^\\ast}(x)\\} + \\min_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast + 1}(x) - {u}_{t^\\ast}(x)\\}\\right)\n    \\end{align*}\n    %\n    is such that $|g^\\varepsilon - g^\\ast| \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha} + \\frac{3\\varepsilon_u + 9(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} < \\varepsilon + \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$, while the policy $\\widetilde{d}^\\infty\\in\\Pi^{\\rm D}$ with $\\widetilde{d}(x) := \\argmax_{a\\in\\mathcal{A}}\\{u_{t^\\ast}(x)\\}$ for all $x\\in\\mathcal{X}$ is $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$-optimal.\n\n    The analysis of the success probability of \\Cref{algo:quantum_extended_value_iteration} is very similar to the one in \\Cref{thr:quantum_finite-horizon2}. Taking into account the oracle failure probabilities, outputting $\\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}$ fails with probability at most $\\frac{6\\delta}{\\pi^2 t^2 |\\mathcal{S}_n|}$. By a usual union bound over all $s\\in\\mathcal{S}_n$ and $t\\in\\mathbb{N}$, the failure probability is at most $\\delta$, since $\\sum_{t=1}^\\infty \\frac{1}{t^2} = \\frac{\\pi^2}{6}$.\n    \n    Finally, for the query complexity of \\Cref{algo:quantum_extended_value_iteration}, we start from obtaining ${{u}}_{t+1}$ given ${{u}}_{t}$. For every $s\\in\\mathcal S_n$, each call to the unitary $\\mathcal{U}_s^{(t)}$ uses $O\\big(\\frac{\\operatorname{sp}(u_t)}{\\varepsilon_u}\\log\\frac{t|\\mathcal{S}_n|A}{\\delta}\\big)$ queries to $\\mathcal{O}_p$, while quantum maximum finding makes $O\\big(\\sqrt{A}\\log\\frac{t|\\mathcal{S}_n|}{\\delta}\\big)$ queries to $\\mathcal{U}_s^{(t)}$. Thus the query complexity of computing $u_{t+1}(s)$ (and $a_{t+1}(s)$) is $O\\big({\\min}\\big\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu}\\big\\}\\frac{\\sqrt{A}}{\\varepsilon_u}\\log\\frac{t|\\mathcal{S}_n|A}{\\delta}\\log\\frac{t|\\mathcal{S}_n|}{\\delta}\\big)$, already using that $\\operatorname{sp}(u_t) \\leq {\\min}\\big\\{4\\Lambda + 3, \\frac{2\\Lambda}{1-\\nu}\\big\\}$. Summing over all $s\\in\\mathcal{S}_n$ and $t\\leq t^\\ast$, the total query complexity of \\Cref{algo:quantum_extended_value_iteration} is simply $O\\big(t^\\ast {\\min}\\big\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu} \\big\\}\\frac{|\\mathcal{S}_n|\\sqrt{A}}{(1-\\nu)\\varepsilon}\\log\\frac{t^\\ast|\\mathcal{S}_n|A}{\\delta}\\log\\frac{t^\\ast|\\mathcal{S}_n|}{\\delta}\\big)$.\n\\end{proof}\n    In the following, let $\\varepsilon_u, \\bar{\\varepsilon}_u > 0$\\varepsilon_u, \\bar{\\varepsilon}_u > 0 such that $\\varepsilon_u := \\frac{1}{4}(1-\\nu)\\varepsilon$\\varepsilon_u := \\frac{1}{4}(1-\\nu)\\varepsilon and $\\bar{\\varepsilon}_u := \\varepsilon_u + (1+\\Lambda)Ln^{-\\alpha}$\\bar{\\varepsilon}_u := \\varepsilon_u + (1+\\Lambda)Ln^{-\\alpha}-\\alpha. The proof of correctness is basically the same as \\Cref{thr:classical_scopt_algorithm}, the main difference being the computation of $\\mu_{t+1}(s,a)$\\mu_{t+1}t+1(s,a) and thus $u_{t+1}$u_{t+1}t+1. Again by induction on $t$t we would like to prove that \\Cref{algo:quantum_extended_value_iteration} outputs a sequence of functions $(u_t)_{t\\in\\mathbb{N}}$(u_t)_{t\\in\\mathbb{N}}t\\in\\mathbb{N} such that $\\|u_{t+1} - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}$\\|u_{t+1}t+1 - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}-\\alpha, $\\operatorname{sp}(u_{t+1}) \\leq \\operatorname{sp}(\\mathcal{L}u_t)$\\operatorname{sp}sp(u_{t+1}t+1) \\leq \\operatorname{sp}sp(\\mathcal{L}u_t), $\\operatorname{sp}(u_{t+1}) \\leq {\\min}\\big\\{4\\Lambda + 3, \\frac{2\\Lambda}{1-\\nu}\\big\\}$\\operatorname{sp}sp(u_{t+1}t+1) \\leq {\\min}\\min\\big\\{4\\Lambda + 3, \\frac{2\\Lambda}{1-\\nu}\\big\\}, and $\\operatorname{sp}(u_t - h^\\ast) \\leq \\nu^t \\Lambda + \\frac{2\\bar{\\varepsilon}_u}{1-\\nu}$\\operatorname{sp}sp(u_t - h^\\ast) \\leq \\nu^t \\Lambda + \\frac{2\\bar{\\varepsilon}_u}{1-\\nu}.\n\n    The case $t=0$t=0 is the same as in \\Cref{thr:classical_scopt_algorithm}. For $t\\geq 1$t\\geq 1, we must show how to approximate $\\int_{\\mathcal{X}}p(\\rd x'|s,a)u_t(x')$\\int_{\\mathcal{X}}\\mathcal{X}p(\\rd x'|s,a)u_t(x') for all $(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}$(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}. We claim that \\cref{lem:quantum_mean_estimation} can be adapted to perform all its steps in superposition without any need for intermediary measurements. This means that, given an initial state $\\sum_{a\\in\\mathcal{A}}\\alpha_a |s,a\\rangle|\\bar{0}\\rangle$\\sum_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\alpha_a |s,a\\rangle|\\bar{0}\\rangle for fixed $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n and some $\\{\\alpha_a\\}_{a\\in\\mathcal{A}}\\subset\\mathbb{C}$\\{\\alpha_a\\}_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\subset\\mathbb{C}, \\cref{lem:quantum_mean_estimation} can generate the mapping\n    \n        \\sum_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\alpha_a |s,a\\rangle|\\bar{0}\\rangle \\mapsto \\sum_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\alpha_a |s,a\\rangle|\\mu_{t+1}t+1(s,a)\\rangle|\\operatorname{garbage}garbage(a)\\rangle,\n    \n    where $\\{|\\operatorname{garbage}(a)\\rangle\\}_{a\\in\\mathcal{A}}$\\{|\\operatorname{garbage}garbage(a)\\rangle\\}_{a\\in\\mathcal{A}}a\\in\\mathcal{A} are ``garbage'' unit complex vectors accumulated through the computation that we ignore and\n    \n        \\left| {\\mu}\\mu_{t+1}t+1(s, a) -\\int_{\\mathcal{X}}\\mathcal{X} p(\\rd x'|s,a) {u}u_{t}t(x') \\right| \\leq \\frac{\\varepsilon_u}{2} \\qquad \\forall(s,a)\\in\\mathcal{S}_n\\times\\mathcal{A}.\n    \n    It is then possible to effectively construct a black-box unitary $\\mathcal{U}_s^{(t)}:|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|{\\mu}_{t+1}(s,a)\\rangle$\\mathcal{U}_s^{(t)}(t):|a\\rangle|\\bar{0}\\rangle \\mapsto |a\\rangle|{\\mu}\\mu_{t+1}t+1(s,a)\\rangle (up to garbage states) using oracle $\\mathcal{O}_p$\\mathcal{O}_p. The maximum over $a\\in\\mathcal{A}$a\\in\\mathcal{A} of $r(s,a) + \\mu_{t+1}(s,a)$r(s,a) + \\mu_{t+1}t+1(s,a) can thus be found by using the quantum max-finding subroutine (\\cref{fact:quantum_minimum_finding}) with unitary $\\mathcal{U}_s^{(t)}$\\mathcal{U}_s^{(t)}(t). This leads to $\\widetilde{u}_{t+1}(s) = \\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}$\\widetilde{u}_{t+1}t+1(s) = \\max_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{r(s,a) + \\mu_{t+1}t+1(s,a)\\} and $a_{t+1}(s) = \\argmax_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}$a_{t+1}t+1(s) = \\argmax_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{r(s,a) + \\mu_{t+1}t+1(s,a)\\} such that $|\\widetilde{u}_{t+1}(s) - (\\mathcal{L} {{u}}_{t})(s)| \\leq \\frac{\\varepsilon_u}{2}$|\\widetilde{u}_{t+1}t+1(s) - (\\mathcal{L} {{u}}{u}u_{t}t)(s)| \\leq \\frac{\\varepsilon_u}{2} for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n. Again, by subtracting $\\frac{\\varepsilon_u}{2}$\\frac{\\varepsilon_u}{2} from the larger entries of $\\widetilde{u}_{t+1}$\\widetilde{u}_{t+1}t+1 and adding $\\frac{\\varepsilon_u}{2}$\\frac{\\varepsilon_u}{2} to the smaller entries of $\\widetilde{u}_{t+1}$\\widetilde{u}_{t+1}t+1, we define, for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n and $\\mathcal{X}(s)$\\mathcal{X}(s),\n    \n        u_{t+1}t+1(x) \\!=\\! \n            \\max\\{\\max_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s') - \\frac{\\varepsilon_u}{2}, \\min_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s')\\} &\\text{if}~\\widetilde{u}_{t+1}t+1(s) \\geq \\max_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s') - \\frac{\\varepsilon_u}{2},\\\\\n            \\min\\{\\min_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s') + \\frac{\\varepsilon_u}{2}, \\max_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s')\\} &\\text{if}~ \\widetilde{u}_{t+1}t+1(s) \\leq \\min_{s'\\in\\mathcal{S}_n}s'\\in\\mathcal{S}_n\\widetilde{u}_{t+1}t+1(s') + \\frac{\\varepsilon_u}{2},\\\\\n            \\widetilde{u}_{t+1}t+1(s) &\\text{otherwise}.\n        \n    \n    Using that $|(\\mathcal{L}u_t)(s) - (\\mathcal{L}u_t)(x)| \\leq 4(1+\\Lambda)Ln^{-\\alpha}$|(\\mathcal{L}u_t)(s) - (\\mathcal{L}u_t)(x)| \\leq 4(1+\\Lambda)Ln^{-\\alpha}-\\alpha for all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n and $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s) (since $\\operatorname{sp}(u_t) \\leq 4\\Lambda + 3$\\operatorname{sp}sp(u_t) \\leq 4\\Lambda + 3 by induction), we once again conclude that $\\|u_{t+1} - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}$\\|u_{t+1}t+1 - \\mathcal{L}u_t\\|_\\infty \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}-\\alpha. Proving that $\\operatorname{sp}(u_{t+1}) \\leq {\\min}\\big\\{4\\Lambda + 3,\\frac{2\\Lambda}{1-\\nu} \\big\\}$\\operatorname{sp}sp(u_{t+1}t+1) \\leq {\\min}\\min\\big\\{4\\Lambda + 3,\\frac{2\\Lambda}{1-\\nu} \\big\\} is done exactly the same as in \\Cref{thr:classical_scopt_algorithm}.\n\n    By \\Cref{lem:stopping_criteria}, $\\operatorname{sp}(u_{t+1} - u_t) \\leq \\frac{6\\varepsilon_u + 18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$\\operatorname{sp}sp(u_{t+1}t+1 - u_t) \\leq \\frac{6\\varepsilon_u + 18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} if $t \\geq t^\\ast = O(\\log(1/\\varepsilon)/\\log(1/\\nu))$t \\geq t^\\ast = O(\\log(1/\\varepsilon)/\\log(1/\\nu)). We employ \\cref{lem:value_iteration_guarantees} to argue that, once $\\operatorname{sp}({{u}}_{t+1} - {{u}}_{t}) \\leq \\frac{6\\varepsilon_u + 18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$\\operatorname{sp}sp({{u}}{u}u_{t+1}t+1 - {{u}}{u}u_{t}t) \\leq \\frac{6\\varepsilon_u + 18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} for $t\\geq t^\\ast$t\\geq t^\\ast, the output\n    \n        g^\\varepsilon = \\frac{1}{2}\\left(\\max_{x\\in\\mathcal{X}}x\\in\\mathcal{X}\\{{u}u_{t^\\ast + 1}t^\\ast + 1(x) - {u}u_{t^\\ast}t^\\ast(x)\\} + \\min_{x\\in\\mathcal{X}}x\\in\\mathcal{X}\\{{u}u_{t^\\ast + 1}t^\\ast + 1(x) - {u}u_{t^\\ast}t^\\ast(x)\\}\\right)\n    \n    is such that $|g^\\varepsilon - g^\\ast| \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha} + \\frac{3\\varepsilon_u + 9(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} < \\varepsilon + \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$|g^\\varepsilon - g^\\ast| \\leq \\varepsilon_u + 4(1+\\Lambda)Ln^{-\\alpha}-\\alpha + \\frac{3\\varepsilon_u + 9(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} < \\varepsilon + \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}, while the policy $\\widetilde{d}^\\infty\\in\\Pi^{\\rm D}$\\widetilde{d}^\\infty\\in\\Pi^{\\rm D}\\rm D with $\\widetilde{d}(x) := \\argmax_{a\\in\\mathcal{A}}\\{u_{t^\\ast}(x)\\}$\\widetilde{d}(x) := \\argmax_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{u_{t^\\ast}t^\\ast(x)\\} for all $x\\in\\mathcal{X}$x\\in\\mathcal{X} is $\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)$\\big(2\\varepsilon + \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\big)-optimal.\n\n    The analysis of the success probability of \\Cref{algo:quantum_extended_value_iteration} is very similar to the one in \\Cref{thr:quantum_finite-horizon2}. Taking into account the oracle failure probabilities, outputting $\\max_{a\\in\\mathcal{A}}\\{r(s,a) + \\mu_{t+1}(s,a)\\}$\\max_{a\\in\\mathcal{A}}a\\in\\mathcal{A}\\{r(s,a) + \\mu_{t+1}t+1(s,a)\\} fails with probability at most $\\frac{6\\delta}{\\pi^2 t^2 |\\mathcal{S}_n|}$\\frac{6\\delta}{\\pi^2 t^2 |\\mathcal{S}_n|}. By a usual union bound over all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n and $t\\in\\mathbb{N}$t\\in\\mathbb{N}, the failure probability is at most $\\delta$\\delta, since $\\sum_{t=1}^\\infty \\frac{1}{t^2} = \\frac{\\pi^2}{6}$\\sum_{t=1}t=1^\\infty \\frac{1}{t^2} = \\frac{\\pi^2}{6}.\n    \n    Finally, for the query complexity of \\Cref{algo:quantum_extended_value_iteration}, we start from obtaining ${{u}}_{t+1}${{u}}{u}u_{t+1}t+1 given ${{u}}_{t}${{u}}{u}u_{t}t. For every $s\\in\\mathcal S_n$s\\in\\mathcal S_n, each call to the unitary $\\mathcal{U}_s^{(t)}$\\mathcal{U}_s^{(t)}(t) uses $O\\big(\\frac{\\operatorname{sp}(u_t)}{\\varepsilon_u}\\log\\frac{t|\\mathcal{S}_n|A}{\\delta}\\big)$O\\big(\\frac{\\operatorname{sp}(u_t)}{\\varepsilon_u}\\log\\frac{t|\\mathcal{S}_n|A}{\\delta}\\big) queries to $\\mathcal{O}_p$\\mathcal{O}_p, while quantum maximum finding makes $O\\big(\\sqrt{A}\\log\\frac{t|\\mathcal{S}_n|}{\\delta}\\big)$O\\big(\\sqrt{A}\\log\\frac{t|\\mathcal{S}_n|}{\\delta}\\big) queries to $\\mathcal{U}_s^{(t)}$\\mathcal{U}_s^{(t)}(t). Thus the query complexity of computing $u_{t+1}(s)$u_{t+1}t+1(s) (and $a_{t+1}(s)$a_{t+1}t+1(s)) is $O\\big({\\min}\\big\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu}\\big\\}\\frac{\\sqrt{A}}{\\varepsilon_u}\\log\\frac{t|\\mathcal{S}_n|A}{\\delta}\\log\\frac{t|\\mathcal{S}_n|}{\\delta}\\big)$O\\big({\\min}\\min\\big\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu}\\big\\}\\frac{\\sqrt{A}}{\\varepsilon_u}\\log\\frac{t|\\mathcal{S}_n|A}{\\delta}\\log\\frac{t|\\mathcal{S}_n|}{\\delta}\\big), already using that $\\operatorname{sp}(u_t) \\leq {\\min}\\big\\{4\\Lambda + 3, \\frac{2\\Lambda}{1-\\nu}\\big\\}$\\operatorname{sp}sp(u_t) \\leq {\\min}\\min\\big\\{4\\Lambda + 3, \\frac{2\\Lambda}{1-\\nu}\\big\\}. Summing over all $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n and $t\\leq t^\\ast$t\\leq t^\\ast, the total query complexity of \\Cref{algo:quantum_extended_value_iteration} is simply $O\\big(t^\\ast {\\min}\\big\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu} \\big\\}\\frac{|\\mathcal{S}_n|\\sqrt{A}}{(1-\\nu)\\varepsilon}\\log\\frac{t^\\ast|\\mathcal{S}_n|A}{\\delta}\\log\\frac{t^\\ast|\\mathcal{S}_n|}{\\delta}\\big)$O\\big(t^\\ast {\\min}\\min\\big\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu} \\big\\}\\frac{|\\mathcal{S}_n|\\sqrt{A}}{(1-\\nu)\\varepsilon}\\log\\frac{t^\\ast|\\mathcal{S}_n|A}{\\delta}\\log\\frac{t^\\ast|\\mathcal{S}_n|}{\\delta}\\big).\n\n\n\n", "appendix": false}, "Algorithms for online learning of finite-horizon MDPs": {"content": "\n\\label{sec:finite-horizon_MDPs}\n\nIn this section, we give quantum and classical online algorithms for learning finite-horizon MDPs under the generative-exploration model of \\Cref{sec:RL_model}. Thanks to the freedom in interacting with the environment during generative episodes using oracles $\\mathcal{O}_p$\\mathcal{O}_p or $\\mathcal{C}_p$\\mathcal{C}_p, our algorithms achieve improved regret bounds.\n\nBoth our quantum and classical algorithms, presented together in \\cref{algo:quantum_UCCRL_finite-horizon}, are schematically simple. Like several classical RL algorithms for finite-horizon MDPs~\\cite{Kearns2002near,brafman2002r,strehl2006pac,auer2008near,dann2017unifying}, they proceed in episodes. Each episode is split into a generative phase and an exploration phase, which were explained in \\Cref{sec:RL_model}. During a generative phase, an approximate optimal policy is chosen for the subsequent exploration phases, while during an exploration phase, the agent gets to interact using the environment in a classical manner through the previously chosen policy and thus to accumulate rewards. We stress that each exploration phase has a fixed duration equal to the horizon $H$H, i.e., the online agent-environment interaction lasts for $H$H time steps, after which the episode ends. \n\nRegarding the generative phase more specifically, the choice for an approximate optimal policy is done using a doubling trick: a new policy is only chosen after the agent has interacted enough times with the environment during exploration phases. In our case, this means only after the total number of interactions (time steps) has doubled since the last policy update. When this condition is reached at the $k$k-th episode, the classical or quantum backward induction algorithms (\\Cref{algo:classical_backward_recursion,algo:quantum_backward_recursion,algo:quantum_backward_recursion2}) with sampling access to stochastic kernels $p$p are employed to obtained an approximately optimal policy. The algorithm is allowed to use the oracle $\\mathcal{C}_p$\\mathcal{C}_p or $\\mathcal{O}_p$\\mathcal{O}_p a number of $\\frac{1}{2}H \\log_2{k}$\\frac{1}{2}H \\log_2{k}k times, the amount of time steps in exploration phases since the last update. During $K$K episodes, meaning $T=HK$T=HK time steps, the policy is updated at most $\\lceil\\log_2{K}\\rceil$\\lceil\\log_2{K}K\\rceil times. In episodes where the doubling trick condition has not been satisfied yet, the policy from the previous episode is selected as the new current policy.\n\nUnlike prior RL algorithms for finite-horizon MDPs~\\cite{Kearns2002near,brafman2002r,strehl2006pac,auer2008near,dann2017unifying,wang2021quantum}, \\cref{algo:quantum_UCCRL_finite-horizon} does not keep an approximation $\\widetilde{p}$\\widetilde{p} for the true stochastic kernel $p$p via the state-action pairs $(x_t,a_t)$(x_t,a_t) observed during exploration phases and therefore does not employ $\\widetilde{p}$\\widetilde{p} to calculate approximate optimal policies. Since it is possible to interact with the true MDP $M$M in a generative manner using oracles $\\mathcal{C}_p$\\mathcal{C}_p or $\\mathcal{O}_p$\\mathcal{O}_p, picking policies can be done in a more straightforward way, which ultimately is the cause for our improved regrets.\n\n\\begin{algorithm}[t!]\n    \\caption{Classical/quantum online-learning algorithm for finite-horizon MDPs}\n    \\label{algo:quantum_UCCRL_finite-horizon}\n    \\begin{algorithmic}[1]\n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$, finite action space $\\mathcal A$, horizon $H$, rewards $r:\\mathcal{X}\\times\\mathcal{A}\\to [0,1]$, failure probability $\\delta\\in(0,1)$, parameters $L,\\alpha\\geq 0$.\n\n    \\For{episodes $k\\in[K]$}\n        \n        \\phase{Generative phase}\n\n        \\If{$k=2^{\\lfloor \\log_2{k}\\rfloor}$}\n    \n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{\\textbf{Classical:} By using \\cref{algo:classical_backward_recursion}, choose a policy $\\pi^{(k)}\\in\\Pi^{\\rm D}$ such that, with failure probability at most $\\frac{\\delta}{\\lceil\\log_2{K}\\rceil}$ and for some constant $c>0$,\n            \\begin{align*}\n                \\|V_1^{\\pi^{(k)}} - V_1^\\ast\\|_\\infty \\leq  c\\sqrt{\\frac{H^3|\\mathcal{S}_n|A}{k}\\log\\left(\\frac{H|\\mathcal{S}_n|A\\log{K}}{\\delta}\\right)} - cLn^{-\\alpha}H^2 \n            \\end{align*}}\n\n            \\setcounter{ALG@line}{2}\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{\\textbf{Quantum:} By using \\cref{algo:quantum_backward_recursion} or \\Cref{algo:quantum_backward_recursion2}, choose a policy $\\pi^{(k)}\\in\\Pi^{\\rm D}$ such that, with failure probability at most $\\frac{\\delta}{\\lceil\\log_2{K}\\rceil}$ and for some constant $c>0$,\n            \\begin{align*}\n                \\|V_1^{\\pi^{(k)}} - V_1^\\ast\\|_\\infty \\leq c \\frac{|\\mathcal{S}_n|}{k}\\min\\{HA,H^2\\sqrt{A}\\}\\log^2\\left(\\frac{H|\\mathcal{S}_n|A\\log{K}}{\\delta}\\right) - cLn^{-\\alpha}H^2\n            \\end{align*}}\n        \n        \\Else\n            \\State $\\pi^{(k)} \\gets \\pi^{(k-1)}$\n        \\EndIf\n\n        \\phase{Exploration phase}\n        \\State Observe initial state $x^{(k)}_1$\n        \\For{$t\\in[H]$}\n            \\State Choose action $a^{(k)}_t = \\pi_t^{(k)}(x_t^{(k)})$ and obtain reward $r^{(k)}_t \\gets r(x^{(k)}_t,a^{(k)}_t)$\n            \\State Observe next state $x^{(k)}_{t+1} \\sim p(\\cdot|x^{(k)}_t,a_t^{(k)})$\n        \\EndFor\n        \n    \\EndFor\n    %\\Ensure \n\\end{algorithmic}\n\\end{algorithm}\\begin{algorithm}[t!]\n    \\caption{Classical/quantum online-learning algorithm for finite-horizon MDPs}\n    \\label{algo:quantum_UCCRL_finite-horizon}\n    \\begin{algorithmic}[1]\n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$, finite action space $\\mathcal A$, horizon $H$, rewards $r:\\mathcal{X}\\times\\mathcal{A}\\to [0,1]$, failure probability $\\delta\\in(0,1)$, parameters $L,\\alpha\\geq 0$.\n\n    \\For{episodes $k\\in[K]$}\n        \n        \\phase{Generative phase}\n\n        \\If{$k=2^{\\lfloor \\log_2{k}\\rfloor}$}\n    \n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{\\textbf{Classical:} By using \\cref{algo:classical_backward_recursion}, choose a policy $\\pi^{(k)}\\in\\Pi^{\\rm D}$ such that, with failure probability at most $\\frac{\\delta}{\\lceil\\log_2{K}\\rceil}$ and for some constant $c>0$,\n            \\begin{align*}\n                \\|V_1^{\\pi^{(k)}} - V_1^\\ast\\|_\\infty \\leq  c\\sqrt{\\frac{H^3|\\mathcal{S}_n|A}{k}\\log\\left(\\frac{H|\\mathcal{S}_n|A\\log{K}}{\\delta}\\right)} - cLn^{-\\alpha}H^2 \n            \\end{align*}}\n\n            \\setcounter{ALG@line}{2}\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}{\\textbf{Quantum:} By using \\cref{algo:quantum_backward_recursion} or \\Cref{algo:quantum_backward_recursion2}, choose a policy $\\pi^{(k)}\\in\\Pi^{\\rm D}$ such that, with failure probability at most $\\frac{\\delta}{\\lceil\\log_2{K}\\rceil}$ and for some constant $c>0$,\n            \\begin{align*}\n                \\|V_1^{\\pi^{(k)}} - V_1^\\ast\\|_\\infty \\leq c \\frac{|\\mathcal{S}_n|}{k}\\min\\{HA,H^2\\sqrt{A}\\}\\log^2\\left(\\frac{H|\\mathcal{S}_n|A\\log{K}}{\\delta}\\right) - cLn^{-\\alpha}H^2\n            \\end{align*}}\n        \n        \\Else\n            \\State $\\pi^{(k)} \\gets \\pi^{(k-1)}$\n        \\EndIf\n\n        \\phase{Exploration phase}\n        \\State Observe initial state $x^{(k)}_1$\n        \\For{$t\\in[H]$}\n            \\State Choose action $a^{(k)}_t = \\pi_t^{(k)}(x_t^{(k)})$ and obtain reward $r^{(k)}_t \\gets r(x^{(k)}_t,a^{(k)}_t)$\n            \\State Observe next state $x^{(k)}_{t+1} \\sim p(\\cdot|x^{(k)}_t,a_t^{(k)})$\n        \\EndFor\n        \n    \\EndFor\n    %\\Ensure \n\\end{algorithmic}\n\\end{algorithm}[t!]\n    \\caption{Classical/quantum online-learning algorithm for finite-horizon MDPs}\n    \\label{algo:quantum_UCCRL_finite-horizon}\n    [1]\n    \\Require Compact state space $\\mathcal{X}$\\mathcal{X} with $\\frac{1}{n}$\\frac{1}{n}-net $\\mathcal{S}_n$\\mathcal{S}_n, finite action space $\\mathcal A$\\mathcal A, horizon $H$H, rewards $r:\\mathcal{X}\\times\\mathcal{A}\\to [0,1]$r:\\mathcal{X}\\times\\mathcal{A}\\to [0,1], failure probability $\\delta\\in(0,1)$\\delta\\in(0,1), parameters $L,\\alpha\\geq 0$L,\\alpha\\geq 0.\n\n    \\For{episodes $k\\in[K]$}episodes $k\\in[K]$k\\in[K]\n        \n        \\phase{Generative phase}Generative phase\n\n        \\If{$k=2^{\\lfloor \\log_2{k}\\rfloor}$}$k=2^{\\lfloor \\log_2{k}\\rfloor}$k=2^{\\lfloor \\log_2{k}\\rfloor}\\lfloor \\log_2{k}k\\rfloor\n    \n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent{\\textbf{Classical:} By using \\cref{algo:classical_backward_recursion}, choose a policy $\\pi^{(k)}\\in\\Pi^{\\rm D}$ such that, with failure probability at most $\\frac{\\delta}{\\lceil\\log_2{K}\\rceil}$ and for some constant $c>0$,\n            \\begin{align*}\n                \\|V_1^{\\pi^{(k)}} - V_1^\\ast\\|_\\infty \\leq  c\\sqrt{\\frac{H^3|\\mathcal{S}_n|A}{k}\\log\\left(\\frac{H|\\mathcal{S}_n|A\\log{K}}{\\delta}\\right)} - cLn^{-\\alpha}H^2 \n            \\end{align*}}\\textbf{Classical:} By using \\cref{algo:classical_backward_recursion}, choose a policy $\\pi^{(k)}\\in\\Pi^{\\rm D}$\\pi^{(k)}(k)\\in\\Pi^{\\rm D}\\rm D such that, with failure probability at most $\\frac{\\delta}{\\lceil\\log_2{K}\\rceil}$\\frac{\\delta}{\\lceil\\log_2{K}\\rceil} and for some constant $c>0$c>0,\n            \n                \\|V_1^{\\pi^{(k)}}\\pi^{(k)}(k) - V_1^\\ast\\|_\\infty \\leq  c\\sqrt{\\frac{H^3|\\mathcal{S}_n|A}{k}\\log\\left(\\frac{H|\\mathcal{S}_n|A\\log{K}}{\\delta}\\right)} - cLn^{-\\alpha}-\\alphaH^2 \n            \n\n            \\setcounter{ALG@line}{2}\n            \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent}\\dimexpr\\linewidth-\\algorithmicindent-\\algorithmicindent{\\textbf{Quantum:} By using \\cref{algo:quantum_backward_recursion} or \\Cref{algo:quantum_backward_recursion2}, choose a policy $\\pi^{(k)}\\in\\Pi^{\\rm D}$ such that, with failure probability at most $\\frac{\\delta}{\\lceil\\log_2{K}\\rceil}$ and for some constant $c>0$,\n            \\begin{align*}\n                \\|V_1^{\\pi^{(k)}} - V_1^\\ast\\|_\\infty \\leq c \\frac{|\\mathcal{S}_n|}{k}\\min\\{HA,H^2\\sqrt{A}\\}\\log^2\\left(\\frac{H|\\mathcal{S}_n|A\\log{K}}{\\delta}\\right) - cLn^{-\\alpha}H^2\n            \\end{align*}}\\textbf{Quantum:} By using \\cref{algo:quantum_backward_recursion} or \\Cref{algo:quantum_backward_recursion2}, choose a policy $\\pi^{(k)}\\in\\Pi^{\\rm D}$\\pi^{(k)}(k)\\in\\Pi^{\\rm D}\\rm D such that, with failure probability at most $\\frac{\\delta}{\\lceil\\log_2{K}\\rceil}$\\frac{\\delta}{\\lceil\\log_2{K}\\rceil} and for some constant $c>0$c>0,\n            \n                \\|V_1^{\\pi^{(k)}}\\pi^{(k)}(k) - V_1^\\ast\\|_\\infty \\leq c \\frac{|\\mathcal{S}_n|}{k}\\min\\{HA,H^2\\sqrt{A}\\}\\log^2\\left(\\frac{H|\\mathcal{S}_n|A\\log{K}}{\\delta}\\right) - cLn^{-\\alpha}-\\alphaH^2\n            \n        \n        \\Else\n            \\State $\\pi^{(k)} \\gets \\pi^{(k-1)}$\\pi^{(k)}(k) \\gets \\pi^{(k-1)}(k-1)\n        \\EndIf\n\n        \\phase{Exploration phase}Exploration phase\n        \\State Observe initial state $x^{(k)}_1$x^{(k)}(k)_1\n        \\For{$t\\in[H]$}$t\\in[H]$t\\in[H]\n            \\State Choose action $a^{(k)}_t = \\pi_t^{(k)}(x_t^{(k)})$a^{(k)}(k)_t = \\pi_t^{(k)}(k)(x_t^{(k)}(k)) and obtain reward $r^{(k)}_t \\gets r(x^{(k)}_t,a^{(k)}_t)$r^{(k)}(k)_t \\gets r(x^{(k)}(k)_t,a^{(k)}(k)_t)\n            \\State Observe next state $x^{(k)}_{t+1} \\sim p(\\cdot|x^{(k)}_t,a_t^{(k)})$x^{(k)}(k)_{t+1}t+1 \\sim p(\\cdot|x^{(k)}(k)_t,a_t^{(k)}(k))\n        \\EndFor\n        \n    \\EndFor\n    \n\n\nWe start by analysing in the theorem below the regret bound of the classical version of \\cref{algo:quantum_UCCRL_finite-horizon}, after which we move on to its quantum version.\n\n\\begin{theorem}[Classical finite-horizon regret bound]\\label{thr:classical_finite-horizon_regret}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},H,p,r\\rangle$ be a finite-horizon MDP. Then the regret $\\operatorname{Regret}_{H}(T)$ of the classical version of {\\rm \\cref{algo:quantum_UCCRL_finite-horizon}} is upper-bounded after $T$ steps, with probability at least $1 - \\delta$, by \n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\sqrt{H|\\mathcal{S}_n|AT\\log\\frac{H|\\mathcal{S}_n|A}{\\delta}} + HTLn^{-\\alpha} \\right),\n    \\end{align*}\n    %\n    where $\\widetilde{O}(\\cdot)$ omits $\\poly\\log\\log$ terms. If $\\mathcal{X} = [0,1]^D$, in which case $|\\mathcal{S}_n| = O(n^D)$, then setting $n = T^{\\frac{1}{D+2\\alpha}}$ yields the regret (for $L$ constant)\n    %\n    \\begin{align*}\n    \t\\widetilde{O}\\left(T^{\\frac{D+\\alpha}{D+2\\alpha}}\\left(H + \\sqrt{HA\\log\\frac{HAT}{\\delta}}\\right) \\right).\n    \\end{align*}\n    %\n    If $\\mathcal{X} = \\mathcal{S}$ is finite with size $S$, in which case $L=0$, then the regret is\n    %\n    \\begin{align*}\n    \t\\widetilde{O}\\left(\\sqrt{HSAT\\log\\frac{HSA}{\\delta}} \\right). \n    \\end{align*}\n\\end{theorem}\\begin{theorem}[Classical finite-horizon regret bound]\\label{thr:classical_finite-horizon_regret}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},H,p,r\\rangle$ be a finite-horizon MDP. Then the regret $\\operatorname{Regret}_{H}(T)$ of the classical version of {\\rm \\cref{algo:quantum_UCCRL_finite-horizon}} is upper-bounded after $T$ steps, with probability at least $1 - \\delta$, by \n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\sqrt{H|\\mathcal{S}_n|AT\\log\\frac{H|\\mathcal{S}_n|A}{\\delta}} + HTLn^{-\\alpha} \\right),\n    \\end{align*}\n    %\n    where $\\widetilde{O}(\\cdot)$ omits $\\poly\\log\\log$ terms. If $\\mathcal{X} = [0,1]^D$, in which case $|\\mathcal{S}_n| = O(n^D)$, then setting $n = T^{\\frac{1}{D+2\\alpha}}$ yields the regret (for $L$ constant)\n    %\n    \\begin{align*}\n    \t\\widetilde{O}\\left(T^{\\frac{D+\\alpha}{D+2\\alpha}}\\left(H + \\sqrt{HA\\log\\frac{HAT}{\\delta}}\\right) \\right).\n    \\end{align*}\n    %\n    If $\\mathcal{X} = \\mathcal{S}$ is finite with size $S$, in which case $L=0$, then the regret is\n    %\n    \\begin{align*}\n    \t\\widetilde{O}\\left(\\sqrt{HSAT\\log\\frac{HSA}{\\delta}} \\right). \n    \\end{align*}\n\\end{theorem}\\label{thr:classical_finite-horizon_regret}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},H,p,r\\rangle$M = \\langle \\mathcal{X},\\mathcal{A},H,p,r\\rangle be a finite-horizon MDP. Then the regret $\\operatorname{Regret}_{H}(T)$\\operatorname{Regret}Regret_{H}H(T) of the classical version of {\\rm \\cref{algo:quantum_UCCRL_finite-horizon}}\\rm \\cref{algo:quantum_UCCRL_finite-horizon} is upper-bounded after $T$T steps, with probability at least $1 - \\delta$1 - \\delta, by \n    \n        \\widetilde{O}\\left(\\sqrt{H|\\mathcal{S}_n|AT\\log\\frac{H|\\mathcal{S}_n|A}{\\delta}} + HTLn^{-\\alpha}-\\alpha \\right),\n    \n    where $\\widetilde{O}(\\cdot)$\\widetilde{O}(\\cdot) omits $\\poly\\log\\log$\\poly\\log\\log terms. If $\\mathcal{X} = [0,1]^D$\\mathcal{X} = [0,1]^D, in which case $|\\mathcal{S}_n| = O(n^D)$|\\mathcal{S}_n| = O(n^D), then setting $n = T^{\\frac{1}{D+2\\alpha}}$n = T^{\\frac{1}{D+2\\alpha}}\\frac{1}{D+2\\alpha} yields the regret (for $L$L constant)\n    \n    \t\\widetilde{O}\\left(T^{\\frac{D+\\alpha}{D+2\\alpha}}\\frac{D+\\alpha}{D+2\\alpha}\\left(H + \\sqrt{HA\\log\\frac{HAT}{\\delta}}\\right) \\right).\n    \n    If $\\mathcal{X} = \\mathcal{S}$\\mathcal{X} = \\mathcal{S} is finite with size $S$S, in which case $L=0$L=0, then the regret is\n    \n    \t\\widetilde{O}\\left(\\sqrt{HSAT\\log\\frac{HSA}{\\delta}} \\right). \n    \n\n\\begin{proof}\n    The total regret over $K$ episodes ($T = HK$ time steps) is\n    %\n    \\begin{align*}\n        \\operatorname{Regret}_{H}(T) := \\sum_{k=1}^K\\big( V_1^\\ast(x_1^{(k)}) - V_1^{\\pi^{(k)}}(x_1^{(k)})\\big).\n    \\end{align*}\n    %\n    The policy $\\pi^{(k)}$ is updated every time $k$ is a power of two ($k = 2^{\\lfloor \\log_2{k}\\rfloor}$), to a total of $\\lceil \\log_2{K}\\rceil$ times. At each update, we employ \\Cref{algo:classical_backward_recursion} with $O(Hk)$ calls to oracle $\\mathcal{C}_p$ in order to obtain a policy $\\pi^{(k)}$ such that, according to \\Cref{thr:classical_finite-horizon_sampling}, with probability at least $1-\\frac{\\delta}{\\lceil\\log_2{K}\\rceil}$, for all $x\\in\\mathcal{X}$,\n    %\n    \\begin{align*}\n        V_1^\\ast(x) - \\frac{C}{\\sqrt{k}} - 12H^2Ln^{-\\alpha} \\leq V_1^{\\pi^{(k)}}(x) \\leq V_1^\\ast(x), \\quad\\text{where}~ C = \\widetilde{O}\\left(\\sqrt{H^2|\\mathcal{S}_n|A \\log\\frac{H|\\mathcal{S}_n|A}{\\delta} } \\right)\n    \\end{align*}\n    %\n    and $\\widetilde{O}(\\cdot)$ omits $\\poly\\log\\log$ terms. This means that, with probability at least $1-\\delta$, the regret is upper-bounded as\n    %\n    \\begin{align*}\n        \\operatorname{Regret}_{H}(T) \\leq \\sum_{k=1}^K \\!\\left(\\!\\frac{C}{\\sqrt{k}} + 12H^2Ln^{-\\alpha}\\!\\right)  \\leq \\frac{C\\sqrt{K}}{\\sqrt{2}-1} + 12H^2KLn^{-\\alpha} = \\frac{C}{\\sqrt{2}-1}\\sqrt{\\frac{T}{H}} + 12HTLn^{-\\alpha},\n    \\end{align*}\n    %\n    where we used \\Cref{fact:useful_inequality} in order to bound $\\sum_{k=1}^K \\frac{1}{\\sqrt{k}} \\leq \\frac{\\sqrt{K}}{\\sqrt{2}-1}$.\n\\end{proof}\\begin{proof}\n    The total regret over $K$ episodes ($T = HK$ time steps) is\n    %\n    \\begin{align*}\n        \\operatorname{Regret}_{H}(T) := \\sum_{k=1}^K\\big( V_1^\\ast(x_1^{(k)}) - V_1^{\\pi^{(k)}}(x_1^{(k)})\\big).\n    \\end{align*}\n    %\n    The policy $\\pi^{(k)}$ is updated every time $k$ is a power of two ($k = 2^{\\lfloor \\log_2{k}\\rfloor}$), to a total of $\\lceil \\log_2{K}\\rceil$ times. At each update, we employ \\Cref{algo:classical_backward_recursion} with $O(Hk)$ calls to oracle $\\mathcal{C}_p$ in order to obtain a policy $\\pi^{(k)}$ such that, according to \\Cref{thr:classical_finite-horizon_sampling}, with probability at least $1-\\frac{\\delta}{\\lceil\\log_2{K}\\rceil}$, for all $x\\in\\mathcal{X}$,\n    %\n    \\begin{align*}\n        V_1^\\ast(x) - \\frac{C}{\\sqrt{k}} - 12H^2Ln^{-\\alpha} \\leq V_1^{\\pi^{(k)}}(x) \\leq V_1^\\ast(x), \\quad\\text{where}~ C = \\widetilde{O}\\left(\\sqrt{H^2|\\mathcal{S}_n|A \\log\\frac{H|\\mathcal{S}_n|A}{\\delta} } \\right)\n    \\end{align*}\n    %\n    and $\\widetilde{O}(\\cdot)$ omits $\\poly\\log\\log$ terms. This means that, with probability at least $1-\\delta$, the regret is upper-bounded as\n    %\n    \\begin{align*}\n        \\operatorname{Regret}_{H}(T) \\leq \\sum_{k=1}^K \\!\\left(\\!\\frac{C}{\\sqrt{k}} + 12H^2Ln^{-\\alpha}\\!\\right)  \\leq \\frac{C\\sqrt{K}}{\\sqrt{2}-1} + 12H^2KLn^{-\\alpha} = \\frac{C}{\\sqrt{2}-1}\\sqrt{\\frac{T}{H}} + 12HTLn^{-\\alpha},\n    \\end{align*}\n    %\n    where we used \\Cref{fact:useful_inequality} in order to bound $\\sum_{k=1}^K \\frac{1}{\\sqrt{k}} \\leq \\frac{\\sqrt{K}}{\\sqrt{2}-1}$.\n\\end{proof}\n    The total regret over $K$K episodes ($T = HK$T = HK time steps) is\n    \n        \\operatorname{Regret}Regret_{H}H(T) := \\sum_{k=1}k=1^K\\big( V_1^\\ast(x_1^{(k)}(k)) - V_1^{\\pi^{(k)}}\\pi^{(k)}(k)(x_1^{(k)}(k))\\big).\n    \n    The policy $\\pi^{(k)}$\\pi^{(k)}(k) is updated every time $k$k is a power of two ($k = 2^{\\lfloor \\log_2{k}\\rfloor}$k = 2^{\\lfloor \\log_2{k}\\rfloor}\\lfloor \\log_2{k}k\\rfloor), to a total of $\\lceil \\log_2{K}\\rceil$\\lceil \\log_2{K}K\\rceil times. At each update, we employ \\Cref{algo:classical_backward_recursion} with $O(Hk)$O(Hk) calls to oracle $\\mathcal{C}_p$\\mathcal{C}_p in order to obtain a policy $\\pi^{(k)}$\\pi^{(k)}(k) such that, according to \\Cref{thr:classical_finite-horizon_sampling}, with probability at least $1-\\frac{\\delta}{\\lceil\\log_2{K}\\rceil}$1-\\frac{\\delta}{\\lceil\\log_2{K}\\rceil}, for all $x\\in\\mathcal{X}$x\\in\\mathcal{X},\n    \n        V_1^\\ast(x) - \\frac{C}{\\sqrt{k}} - 12H^2Ln^{-\\alpha}-\\alpha \\leq V_1^{\\pi^{(k)}}\\pi^{(k)}(k)(x) \\leq V_1^\\ast(x), \\quad\\text{where}~ C = \\widetilde{O}\\left(\\sqrt{H^2|\\mathcal{S}_n|A \\log\\frac{H|\\mathcal{S}_n|A}{\\delta} } \\right)\n    \n    and $\\widetilde{O}(\\cdot)$\\widetilde{O}(\\cdot) omits $\\poly\\log\\log$\\poly\\log\\log terms. This means that, with probability at least $1-\\delta$1-\\delta, the regret is upper-bounded as\n    \n        \\operatorname{Regret}Regret_{H}H(T) \\leq \\sum_{k=1}k=1^K \\!\\left(\\!\\frac{C}{\\sqrt{k}} + 12H^2Ln^{-\\alpha}-\\alpha\\!\\right)  \\leq \\frac{C\\sqrt{K}}{\\sqrt{2}-1} + 12H^2KLn^{-\\alpha}-\\alpha = \\frac{C}{\\sqrt{2}-1}\\sqrt{\\frac{T}{H}} + 12HTLn^{-\\alpha}-\\alpha,\n    \n    where we used \\Cref{fact:useful_inequality} in order to bound $\\sum_{k=1}^K \\frac{1}{\\sqrt{k}} \\leq \\frac{\\sqrt{K}}{\\sqrt{2}-1}$\\sum_{k=1}k=1^K \\frac{1}{\\sqrt{k}} \\leq \\frac{\\sqrt{K}}{\\sqrt{2}-1}.\n\n\n\\begin{theorem}[Quantum finite-horizon regret bound]\\label{thr:quantum_finite_horizon_regret}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},H,p,r\\rangle$ be a finite-horizon MDP. Then the regret $\\operatorname{Regret}_{H}(T)$ of the quantum version of {\\rm \\cref{algo:quantum_UCCRL_finite-horizon}} is upper-bounded after $T$ steps, with probability at least $1 - \\delta$, by\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(|\\mathcal{S}_n|\\min\\left\\{HA, H^2\\sqrt{A}\\log\\frac{H|\\mathcal{S}_n|}{\\delta}\\right\\}\\log\\frac{T}{H}\\log\\frac{H|\\mathcal{S}_n|A}{\\delta} + HTLn^{-\\alpha} \\right),\n    \\end{align*}\n    %\n    where $\\widetilde{O}(\\cdot)$ omits $\\poly\\log\\log$ terms. If $\\mathcal{X} = [0,1]^D$, in which case $|\\mathcal{S}_n| = O(n^D)$, then setting $n = T^{\\frac{1}{D+\\alpha}}$ yields the regret (for $L$ constant)\n    %\n    \\begin{align*}\n    \t\\widetilde{O}\\left(T^{\\frac{D}{D+\\alpha}}\\min\\left\\{HA,H^2\\sqrt{A}\\log\\frac{HT}{\\delta}\\right\\}\\log\\frac{T}{H}\\log\\frac{HTA}{\\delta} \\right).\n    \\end{align*}\n    %\n    If $\\mathcal{X} = \\mathcal{S}$ is finite with size $S$, in which case $L=0$, then the regret is\n    %\n    \\begin{align*}\n    \t\\widetilde{O}\\left(S\\min\\left\\{HA,H^2\\sqrt{A}\\log\\frac{HS}{\\delta}\\right\\}\\log\\frac{T}{H}\\log\\frac{HSA}{\\delta} \\right). \n    \\end{align*}\n\\end{theorem}\\begin{theorem}[Quantum finite-horizon regret bound]\\label{thr:quantum_finite_horizon_regret}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},H,p,r\\rangle$ be a finite-horizon MDP. Then the regret $\\operatorname{Regret}_{H}(T)$ of the quantum version of {\\rm \\cref{algo:quantum_UCCRL_finite-horizon}} is upper-bounded after $T$ steps, with probability at least $1 - \\delta$, by\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(|\\mathcal{S}_n|\\min\\left\\{HA, H^2\\sqrt{A}\\log\\frac{H|\\mathcal{S}_n|}{\\delta}\\right\\}\\log\\frac{T}{H}\\log\\frac{H|\\mathcal{S}_n|A}{\\delta} + HTLn^{-\\alpha} \\right),\n    \\end{align*}\n    %\n    where $\\widetilde{O}(\\cdot)$ omits $\\poly\\log\\log$ terms. If $\\mathcal{X} = [0,1]^D$, in which case $|\\mathcal{S}_n| = O(n^D)$, then setting $n = T^{\\frac{1}{D+\\alpha}}$ yields the regret (for $L$ constant)\n    %\n    \\begin{align*}\n    \t\\widetilde{O}\\left(T^{\\frac{D}{D+\\alpha}}\\min\\left\\{HA,H^2\\sqrt{A}\\log\\frac{HT}{\\delta}\\right\\}\\log\\frac{T}{H}\\log\\frac{HTA}{\\delta} \\right).\n    \\end{align*}\n    %\n    If $\\mathcal{X} = \\mathcal{S}$ is finite with size $S$, in which case $L=0$, then the regret is\n    %\n    \\begin{align*}\n    \t\\widetilde{O}\\left(S\\min\\left\\{HA,H^2\\sqrt{A}\\log\\frac{HS}{\\delta}\\right\\}\\log\\frac{T}{H}\\log\\frac{HSA}{\\delta} \\right). \n    \\end{align*}\n\\end{theorem}\\label{thr:quantum_finite_horizon_regret}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},H,p,r\\rangle$M = \\langle \\mathcal{X},\\mathcal{A},H,p,r\\rangle be a finite-horizon MDP. Then the regret $\\operatorname{Regret}_{H}(T)$\\operatorname{Regret}Regret_{H}H(T) of the quantum version of {\\rm \\cref{algo:quantum_UCCRL_finite-horizon}}\\rm \\cref{algo:quantum_UCCRL_finite-horizon} is upper-bounded after $T$T steps, with probability at least $1 - \\delta$1 - \\delta, by\n    \n        \\widetilde{O}\\left(|\\mathcal{S}_n|\\min\\left\\{HA, H^2\\sqrt{A}\\log\\frac{H|\\mathcal{S}_n|}{\\delta}\\right\\}\\log\\frac{T}{H}\\log\\frac{H|\\mathcal{S}_n|A}{\\delta} + HTLn^{-\\alpha}-\\alpha \\right),\n    \n    where $\\widetilde{O}(\\cdot)$\\widetilde{O}(\\cdot) omits $\\poly\\log\\log$\\poly\\log\\log terms. If $\\mathcal{X} = [0,1]^D$\\mathcal{X} = [0,1]^D, in which case $|\\mathcal{S}_n| = O(n^D)$|\\mathcal{S}_n| = O(n^D), then setting $n = T^{\\frac{1}{D+\\alpha}}$n = T^{\\frac{1}{D+\\alpha}}\\frac{1}{D+\\alpha} yields the regret (for $L$L constant)\n    \n    \t\\widetilde{O}\\left(T^{\\frac{D}{D+\\alpha}}\\frac{D}{D+\\alpha}\\min\\left\\{HA,H^2\\sqrt{A}\\log\\frac{HT}{\\delta}\\right\\}\\log\\frac{T}{H}\\log\\frac{HTA}{\\delta} \\right).\n    \n    If $\\mathcal{X} = \\mathcal{S}$\\mathcal{X} = \\mathcal{S} is finite with size $S$S, in which case $L=0$L=0, then the regret is\n    \n    \t\\widetilde{O}\\left(S\\min\\left\\{HA,H^2\\sqrt{A}\\log\\frac{HS}{\\delta}\\right\\}\\log\\frac{T}{H}\\log\\frac{HSA}{\\delta} \\right). \n    \n\n\\begin{proof}\n    The proof is the same as \\Cref{thr:classical_finite-horizon_regret}, the main difference being that now we employ \\Cref{algo:quantum_backward_recursion} or \\Cref{algo:quantum_backward_recursion2} with $O(Hk)$ calls to oracle $\\mathcal{O}_p$ in order to obtain a policy $\\pi^{(k)}$ such that, according to \\Cref{thr:quantum_finite-horizon,thr:quantum_finite-horizon2}, with probability at least $1 - \\frac{\\delta}{\\lceil\\log_2{K}\\rceil}$, for all $x\\in\\mathcal{X}$,\n    %\n    \\begin{align*}\n        V_1^\\ast(x) &- \\frac{C}{k} - 8H^2Ln^{-\\alpha} \\leq V_1^{\\pi^{(k)}}(x) \\leq V_1^\\ast(x), \\\\\n        \\text{where}~& C = \\widetilde{O}\\left(|\\mathcal{S}_n|\\min\\left\\{HA,H^2\\sqrt{A}\\log\\frac{H|\\mathcal{S}_n|}{\\delta}\\right\\} \\log\\frac{H|\\mathcal{S}_n|A}{\\delta} \\right)\n    \\end{align*}\n    %\n    and $\\widetilde{O}(\\cdot)$ omits $\\poly\\log\\log$ terms. This means that, with probability at least $1-\\delta$, the regret is upper-bounded as\n    %\n    \\[\n        \\operatorname{Regret}_{H}(T) \\leq \\sum_{k=1}^K \\left(\\frac{C}{k} + 8H^2Ln^{-\\alpha}\\right) \\leq C\\ln{K} + 8H^2K Ln^{-\\alpha} = C\\ln\\frac{T}{H} + 8HTLn^{-\\alpha}. \\qedhere\n    \\]\n\\end{proof}\\begin{proof}\n    The proof is the same as \\Cref{thr:classical_finite-horizon_regret}, the main difference being that now we employ \\Cref{algo:quantum_backward_recursion} or \\Cref{algo:quantum_backward_recursion2} with $O(Hk)$ calls to oracle $\\mathcal{O}_p$ in order to obtain a policy $\\pi^{(k)}$ such that, according to \\Cref{thr:quantum_finite-horizon,thr:quantum_finite-horizon2}, with probability at least $1 - \\frac{\\delta}{\\lceil\\log_2{K}\\rceil}$, for all $x\\in\\mathcal{X}$,\n    %\n    \\begin{align*}\n        V_1^\\ast(x) &- \\frac{C}{k} - 8H^2Ln^{-\\alpha} \\leq V_1^{\\pi^{(k)}}(x) \\leq V_1^\\ast(x), \\\\\n        \\text{where}~& C = \\widetilde{O}\\left(|\\mathcal{S}_n|\\min\\left\\{HA,H^2\\sqrt{A}\\log\\frac{H|\\mathcal{S}_n|}{\\delta}\\right\\} \\log\\frac{H|\\mathcal{S}_n|A}{\\delta} \\right)\n    \\end{align*}\n    %\n    and $\\widetilde{O}(\\cdot)$ omits $\\poly\\log\\log$ terms. This means that, with probability at least $1-\\delta$, the regret is upper-bounded as\n    %\n    \\[\n        \\operatorname{Regret}_{H}(T) \\leq \\sum_{k=1}^K \\left(\\frac{C}{k} + 8H^2Ln^{-\\alpha}\\right) \\leq C\\ln{K} + 8H^2K Ln^{-\\alpha} = C\\ln\\frac{T}{H} + 8HTLn^{-\\alpha}. \\qedhere\n    \\]\n\\end{proof}\n    The proof is the same as \\Cref{thr:classical_finite-horizon_regret}, the main difference being that now we employ \\Cref{algo:quantum_backward_recursion} or \\Cref{algo:quantum_backward_recursion2} with $O(Hk)$O(Hk) calls to oracle $\\mathcal{O}_p$\\mathcal{O}_p in order to obtain a policy $\\pi^{(k)}$\\pi^{(k)}(k) such that, according to \\Cref{thr:quantum_finite-horizon,thr:quantum_finite-horizon2}, with probability at least $1 - \\frac{\\delta}{\\lceil\\log_2{K}\\rceil}$1 - \\frac{\\delta}{\\lceil\\log_2{K}\\rceil}, for all $x\\in\\mathcal{X}$x\\in\\mathcal{X},\n    \n        V_1^\\ast(x) &- \\frac{C}{k} - 8H^2Ln^{-\\alpha}-\\alpha \\leq V_1^{\\pi^{(k)}}\\pi^{(k)}(k)(x) \\leq V_1^\\ast(x), \\\\\n        \\text{where}~& C = \\widetilde{O}\\left(|\\mathcal{S}_n|\\min\\left\\{HA,H^2\\sqrt{A}\\log\\frac{H|\\mathcal{S}_n|}{\\delta}\\right\\} \\log\\frac{H|\\mathcal{S}_n|A}{\\delta} \\right)\n    \n    and $\\widetilde{O}(\\cdot)$\\widetilde{O}(\\cdot) omits $\\poly\\log\\log$\\poly\\log\\log terms. This means that, with probability at least $1-\\delta$1-\\delta, the regret is upper-bounded as\n    \\[\n        \\operatorname{Regret}_{H}(T) \\leq \\sum_{k=1}^K \\left(\\frac{C}{k} + 8H^2Ln^{-\\alpha}\\right) \\leq C\\ln{K} + 8H^2K Ln^{-\\alpha} = C\\ln\\frac{T}{H} + 8HTLn^{-\\alpha}. \\qedhere\n    \\]\n        \\operatorname{Regret}Regret_{H}H(T) \\leq \\sum_{k=1}k=1^K \\left(\\frac{C}{k} + 8H^2Ln^{-\\alpha}-\\alpha\\right) \\leq C\\ln{K}K + 8H^2K Ln^{-\\alpha}-\\alpha = C\\ln\\frac{T}{H} + 8HTLn^{-\\alpha}-\\alpha. \\qedhere\n    \n\n\n\n", "appendix": false}, "Algorithms for online learning of infinite-horizon MDPs": {"content": "\n\\label{sec:Quantum_UCCRL}\n\nIn this section, we give quantum and classical online algorithms for learning infinite-horizon MDPs under the generative-exploration model of \\Cref{sec:RL_model}. Once again, thanks to the freedom in interacting with the environment during generative episodes using oracles $\\mathcal{O}_p$\\mathcal{O}_p or $\\mathcal{C}_p$\\mathcal{C}_p, our algorithms achieve improved regret bounds.\n\nBoth our quantum and classical algorithms, presented together in \\cref{algo:quantum_UCCRL}, are schematically simple. Like several classical RL algorithms~\\cite{ortner2012online,auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,fruit2018efficient}, they proceed in episodes, during each of which a chosen policy remains fixed. Each episode is split into a generative phase and an exploration phase, which were explained in \\Cref{sec:RL_model}. In our quantum RL algorithm, during the $k$k-th generative phase, a $O(\\log(\\tau_k)/\\tau_k)$O(\\log(\\tau_k)/\\tau_k)-optimal policy for the exploration phase is chosen using \\Cref{algo:quantum_extended_value_iteration} with quantum sampling access to stochastic kernels $p$p and quantum access to rewards $r$r, where $\\tau_k$\\tau_k is the number of time steps taken in the previous exploration phase. Similarly, our classical RL algorithm chooses a $O(\\sqrt{\\log(\\tau_k)/\\tau_k})$O(\\sqrt{\\log(\\tau_k)/\\tau_k})-optimal policy using \\Cref{algo:classical_extended_value_iteration} instead. After the generative phase ends, the exploration phase starts, during which the chosen policy is employed to accumulate rewards until a termination criteria is reached. Unlike the case for finite-horizon MDPs where each exploration phase lasts the horizon $H$H, for infinite-horizon MPDs the exploration phases can be arbitrarily long and it is dependent on the agent to end the episode and initiate the next generate phase. In our case, we again use a doubling trick to move into a new episode once the number of time steps within all exploration phases doubles. In other words, the doubling trick is employed here to end an exploration phase, while for finite-horizon MPDs it was employed to decide whether to update a policy within a generate phase or not.\n\nUnlike prior classical RL algorithms~\\cite{ortner2012online,auer2006logarithmic,auer2008near,bartlett2009regal,lakshmanan2015improved,fruit2018efficient}, \\cref{algo:quantum_UCCRL} does not keep estimates of the true stochastic kernels given the state-action pairs $(x_t,a_t)$(x_t,a_t) observed during exploration phases, and therefore, it does not adhere to the standard ``optimism-in-the-face-of-uncertainty'' principle of maintaining a set of plausible MDPs $\\mathcal{M}$\\mathcal{M} that contains the true MDP $M$M with high probability. Since it is possible to directly interact with the true MDP $M$M via quantum or classical-accessible environments (oracles $\\mathcal{O}_p$\\mathcal{O}_p or $\\mathcal{C}_p$\\mathcal{C}_p), the choice for an approximate optimal policy does not need to take all MDPs in $\\mathcal{M}$\\mathcal{M} into account.\n\n\\subsection{In-path regret}\n\nWe study the performance of \\cref{algo:quantum_UCCRL} both in terms of the in-path regret $\\operatorname{Regret}_{\\infty}^{\\rm path}(T)$\\operatorname{Regret}Regret_{\\infty}\\infty^{\\rm path}\\rm path(T) and the expected regret $\\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T)$\\operatorname{Regret}Regret_{\\infty}\\infty^{\\mathbb{E}}\\mathbb{E}(T), starting with the former.\n\n\\begin{algorithm}[t!]\n    \\caption{Classical/quantum online-learning algorithm for infinite-horizon MDPs}\n    \\label{algo:quantum_UCCRL}\n    \\begin{algorithmic}[1]\n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$, finite action space $\\mathcal{A}$, upper bound $\\Lambda$ on optimal bias span, rewards $r:\\mathcal{X}\\times\\mathcal{A}\\to [0,1]$, failure probability $\\delta\\in(0,1)$, parameters $L,\\alpha \\geq 0$.\n    \n    \\State $t \\gets 1$ and $\\tau_1 \\gets 1$\n    \n    \\For {episodes $k=1, 2, \\dots$}\n        \n        \\phase{Generative phase}\n\n        \n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent}{\\textbf{Classical:} By using \\cref{algo:classical_extended_value_iteration}, choose decision rule $\\widetilde{d}_k\\in\\mathcal{D}^{\\rm D}$ such that, with failure probability at most $\\frac{\\delta}{8T^{5/4}}$ and for some constant $c>0$,\n        %\n        \\begin{align*}\n            g^{\\widetilde{d}_k^\\infty}(x) \\geq  g^* - c\\Lambda\\sqrt{\\frac{|\\mathcal{S}_n|A\\log{T}}{\\tau_k}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}} - c(1+\\Lambda)Ln^{-\\alpha} \n        \\end{align*}}\n\n        \\setcounter{ALG@line}{2}\n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent}{\\textbf{Quantum:} By using \\cref{algo:quantum_extended_value_iteration}, choose decision rule $\\widetilde{d}_k\\in\\mathcal{D}^{\\rm D}$ such that, with failure probability at most $\\frac{\\delta}{8T^{5/4}}$ and for some constant $c>0$,\n        %\n        \\begin{align*}\n            g^{\\widetilde{d}_k^\\infty}(x) \\geq  g^* - c\\Lambda \\frac{|\\mathcal{S}_n|\\sqrt{A}\\log{T}}{\\tau_k}\\log^2\\frac{|\\mathcal{S}_n|AT}{\\delta} - c(1+\\Lambda)Ln^{-\\alpha} \n        \\end{align*}}\n\n        \\phase{Exploration phase}\n        \\State $\\tau_{k+1} \\gets t$ and observe random initial state $x_{t}$\n        \\While {$t < 2\\tau_{k+1}$}\n            \\State Choose an action $a_{t}\\sim \\widetilde{d}_k(x_{t})$ and obtain reward $r_{t}\\gets r(x_{t},a_{t})$\n            \\State Observe next state $x_{t+1}\\sim p(\\cdot|x_{t},a_{t})$\n            \\State $t \\gets t + 1$\n        \\EndWhile\n        \n    \\EndFor\n    %\\Ensure \n\\end{algorithmic}\n\\end{algorithm}\\begin{algorithm}[t!]\n    \\caption{Classical/quantum online-learning algorithm for infinite-horizon MDPs}\n    \\label{algo:quantum_UCCRL}\n    \\begin{algorithmic}[1]\n    \\Require Compact state space $\\mathcal{X}$ with $\\frac{1}{n}$-net $\\mathcal{S}_n$, finite action space $\\mathcal{A}$, upper bound $\\Lambda$ on optimal bias span, rewards $r:\\mathcal{X}\\times\\mathcal{A}\\to [0,1]$, failure probability $\\delta\\in(0,1)$, parameters $L,\\alpha \\geq 0$.\n    \n    \\State $t \\gets 1$ and $\\tau_1 \\gets 1$\n    \n    \\For {episodes $k=1, 2, \\dots$}\n        \n        \\phase{Generative phase}\n\n        \n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent}{\\textbf{Classical:} By using \\cref{algo:classical_extended_value_iteration}, choose decision rule $\\widetilde{d}_k\\in\\mathcal{D}^{\\rm D}$ such that, with failure probability at most $\\frac{\\delta}{8T^{5/4}}$ and for some constant $c>0$,\n        %\n        \\begin{align*}\n            g^{\\widetilde{d}_k^\\infty}(x) \\geq  g^* - c\\Lambda\\sqrt{\\frac{|\\mathcal{S}_n|A\\log{T}}{\\tau_k}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}} - c(1+\\Lambda)Ln^{-\\alpha} \n        \\end{align*}}\n\n        \\setcounter{ALG@line}{2}\n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent}{\\textbf{Quantum:} By using \\cref{algo:quantum_extended_value_iteration}, choose decision rule $\\widetilde{d}_k\\in\\mathcal{D}^{\\rm D}$ such that, with failure probability at most $\\frac{\\delta}{8T^{5/4}}$ and for some constant $c>0$,\n        %\n        \\begin{align*}\n            g^{\\widetilde{d}_k^\\infty}(x) \\geq  g^* - c\\Lambda \\frac{|\\mathcal{S}_n|\\sqrt{A}\\log{T}}{\\tau_k}\\log^2\\frac{|\\mathcal{S}_n|AT}{\\delta} - c(1+\\Lambda)Ln^{-\\alpha} \n        \\end{align*}}\n\n        \\phase{Exploration phase}\n        \\State $\\tau_{k+1} \\gets t$ and observe random initial state $x_{t}$\n        \\While {$t < 2\\tau_{k+1}$}\n            \\State Choose an action $a_{t}\\sim \\widetilde{d}_k(x_{t})$ and obtain reward $r_{t}\\gets r(x_{t},a_{t})$\n            \\State Observe next state $x_{t+1}\\sim p(\\cdot|x_{t},a_{t})$\n            \\State $t \\gets t + 1$\n        \\EndWhile\n        \n    \\EndFor\n    %\\Ensure \n\\end{algorithmic}\n\\end{algorithm}[t!]\n    \\caption{Classical/quantum online-learning algorithm for infinite-horizon MDPs}\n    \\label{algo:quantum_UCCRL}\n    [1]\n    \\Require Compact state space $\\mathcal{X}$\\mathcal{X} with $\\frac{1}{n}$\\frac{1}{n}-net $\\mathcal{S}_n$\\mathcal{S}_n, finite action space $\\mathcal{A}$\\mathcal{A}, upper bound $\\Lambda$\\Lambda on optimal bias span, rewards $r:\\mathcal{X}\\times\\mathcal{A}\\to [0,1]$r:\\mathcal{X}\\times\\mathcal{A}\\to [0,1], failure probability $\\delta\\in(0,1)$\\delta\\in(0,1), parameters $L,\\alpha \\geq 0$L,\\alpha \\geq 0.\n    \n    \\State $t \\gets 1$t \\gets 1 and $\\tau_1 \\gets 1$\\tau_1 \\gets 1\n    \n    \\For {episodes $k=1, 2, \\dots$}episodes $k=1, 2, \\dots$k=1, 2, \\dots\n        \n        \\phase{Generative phase}Generative phase\n\n        \n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent}\\dimexpr\\linewidth-\\algorithmicindent{\\textbf{Classical:} By using \\cref{algo:classical_extended_value_iteration}, choose decision rule $\\widetilde{d}_k\\in\\mathcal{D}^{\\rm D}$ such that, with failure probability at most $\\frac{\\delta}{8T^{5/4}}$ and for some constant $c>0$,\n        %\n        \\begin{align*}\n            g^{\\widetilde{d}_k^\\infty}(x) \\geq  g^* - c\\Lambda\\sqrt{\\frac{|\\mathcal{S}_n|A\\log{T}}{\\tau_k}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}} - c(1+\\Lambda)Ln^{-\\alpha} \n        \\end{align*}}\\textbf{Classical:} By using \\cref{algo:classical_extended_value_iteration}, choose decision rule $\\widetilde{d}_k\\in\\mathcal{D}^{\\rm D}$\\widetilde{d}_k\\in\\mathcal{D}^{\\rm D}\\rm D such that, with failure probability at most $\\frac{\\delta}{8T^{5/4}}$\\frac{\\delta}{8T^{5/4}} and for some constant $c>0$c>0,\n        \n            g^{\\widetilde{d}_k^\\infty}\\widetilde{d}_k^\\infty(x) \\geq  g^* - c\\Lambda\\sqrt{\\frac{|\\mathcal{S}_n|A\\log{T}}{\\tau_k}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}} - c(1+\\Lambda)Ln^{-\\alpha}-\\alpha \n        \n\n        \\setcounter{ALG@line}{2}\n        \\State \\parbox[t]{\\dimexpr\\linewidth-\\algorithmicindent}\\dimexpr\\linewidth-\\algorithmicindent{\\textbf{Quantum:} By using \\cref{algo:quantum_extended_value_iteration}, choose decision rule $\\widetilde{d}_k\\in\\mathcal{D}^{\\rm D}$ such that, with failure probability at most $\\frac{\\delta}{8T^{5/4}}$ and for some constant $c>0$,\n        %\n        \\begin{align*}\n            g^{\\widetilde{d}_k^\\infty}(x) \\geq  g^* - c\\Lambda \\frac{|\\mathcal{S}_n|\\sqrt{A}\\log{T}}{\\tau_k}\\log^2\\frac{|\\mathcal{S}_n|AT}{\\delta} - c(1+\\Lambda)Ln^{-\\alpha} \n        \\end{align*}}\\textbf{Quantum:} By using \\cref{algo:quantum_extended_value_iteration}, choose decision rule $\\widetilde{d}_k\\in\\mathcal{D}^{\\rm D}$\\widetilde{d}_k\\in\\mathcal{D}^{\\rm D}\\rm D such that, with failure probability at most $\\frac{\\delta}{8T^{5/4}}$\\frac{\\delta}{8T^{5/4}} and for some constant $c>0$c>0,\n        \n            g^{\\widetilde{d}_k^\\infty}\\widetilde{d}_k^\\infty(x) \\geq  g^* - c\\Lambda \\frac{|\\mathcal{S}_n|\\sqrt{A}\\log{T}}{\\tau_k}\\log^2\\frac{|\\mathcal{S}_n|AT}{\\delta} - c(1+\\Lambda)Ln^{-\\alpha}-\\alpha \n        \n\n        \\phase{Exploration phase}Exploration phase\n        \\State $\\tau_{k+1} \\gets t$\\tau_{k+1}k+1 \\gets t and observe random initial state $x_{t}$x_{t}t\n        \\While {$t < 2\\tau_{k+1}$}$t < 2\\tau_{k+1}$t < 2\\tau_{k+1}k+1\n            \\State Choose an action $a_{t}\\sim \\widetilde{d}_k(x_{t})$a_{t}t\\sim \\widetilde{d}_k(x_{t}t) and obtain reward $r_{t}\\gets r(x_{t},a_{t})$r_{t}t\\gets r(x_{t}t,a_{t}t)\n            \\State Observe next state $x_{t+1}\\sim p(\\cdot|x_{t},a_{t})$x_{t+1}t+1\\sim p(\\cdot|x_{t}t,a_{t}t)\n            \\State $t \\gets t + 1$t \\gets t + 1\n        \\EndWhile\n        \n    \\EndFor\n    \n\n\n\n\n\\begin{theorem}[Classical infinite-horizon in-path regret bound]\\label{thr:classical_RL_algorithm_continuous}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ be an infinite-horizon weakly communicating MDP with $\\operatorname{sp}({h}^\\ast) \\leq \\Lambda$. Let $\\mathcal{S}_n$ be an $\\frac{1}{n}$-net for $\\mathcal{X}$. Assume that $p$ and $r$ satisfy {\\rm \\Cref{ass:Lipschitz}} with parameters $L,\\alpha\\geq 0$. Assume the optimal Bellman operator $\\mathcal{L}$ of $M$ is an $1$-stage $\\nu$-span contraction for $\\nu\\in[0,1)$. For any $T,n\\in\\mathbb{N}$, the in-path regret $\\operatorname{Regret}_{\\infty}^{\\rm path}(T)$ of the classical version of {\\rm \\cref{algo:quantum_UCCRL}} is upper-bounded after $T$ steps, with probability at least $1 - \\delta$, by\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\min\\left\\{\\Lambda+1,\\frac{\\Lambda}{1-\\nu}\\right\\} \\sqrt{\\frac{|\\mathcal{S}_n| A}{(1-\\nu)^2}\\frac{T\\log{T}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}} + \\frac{T(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\right),\n    \\end{align*}\n    %\n    where $\\widetilde{O}(\\cdot)$ omits $\\poly\\log\\log$ terms.\n    If $\\mathcal{X} = [0,1]^D$, in which case $|\\mathcal{S}_n| = O(n^D)$, and setting $n = T^{\\frac{1}{D+2\\alpha}}$, the in-path regret is (for constant $\\nu,L$) \n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\Lambda T^{\\frac{D+\\alpha}{D+2\\alpha}}\\sqrt{A\\log{T}\\log\\frac{AT}{\\delta}}  \\right).\n    \\end{align*}\n    %\n    If $\\mathcal{X} = \\mathcal{S}$ is finite with size $S$, in which case $L=0$, then the in-path regret is (for constant $\\nu$) \n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\Lambda\\sqrt{SAT\\log{T}\\log\\frac{SAT}{\\delta}}\\right).\n    \\end{align*}\n\\end{theorem}\\begin{theorem}[Classical infinite-horizon in-path regret bound]\\label{thr:classical_RL_algorithm_continuous}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ be an infinite-horizon weakly communicating MDP with $\\operatorname{sp}({h}^\\ast) \\leq \\Lambda$. Let $\\mathcal{S}_n$ be an $\\frac{1}{n}$-net for $\\mathcal{X}$. Assume that $p$ and $r$ satisfy {\\rm \\Cref{ass:Lipschitz}} with parameters $L,\\alpha\\geq 0$. Assume the optimal Bellman operator $\\mathcal{L}$ of $M$ is an $1$-stage $\\nu$-span contraction for $\\nu\\in[0,1)$. For any $T,n\\in\\mathbb{N}$, the in-path regret $\\operatorname{Regret}_{\\infty}^{\\rm path}(T)$ of the classical version of {\\rm \\cref{algo:quantum_UCCRL}} is upper-bounded after $T$ steps, with probability at least $1 - \\delta$, by\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\min\\left\\{\\Lambda+1,\\frac{\\Lambda}{1-\\nu}\\right\\} \\sqrt{\\frac{|\\mathcal{S}_n| A}{(1-\\nu)^2}\\frac{T\\log{T}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}} + \\frac{T(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\right),\n    \\end{align*}\n    %\n    where $\\widetilde{O}(\\cdot)$ omits $\\poly\\log\\log$ terms.\n    If $\\mathcal{X} = [0,1]^D$, in which case $|\\mathcal{S}_n| = O(n^D)$, and setting $n = T^{\\frac{1}{D+2\\alpha}}$, the in-path regret is (for constant $\\nu,L$) \n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\Lambda T^{\\frac{D+\\alpha}{D+2\\alpha}}\\sqrt{A\\log{T}\\log\\frac{AT}{\\delta}}  \\right).\n    \\end{align*}\n    %\n    If $\\mathcal{X} = \\mathcal{S}$ is finite with size $S$, in which case $L=0$, then the in-path regret is (for constant $\\nu$) \n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\Lambda\\sqrt{SAT\\log{T}\\log\\frac{SAT}{\\delta}}\\right).\n    \\end{align*}\n\\end{theorem}\\label{thr:classical_RL_algorithm_continuous}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle be an infinite-horizon weakly communicating MDP with $\\operatorname{sp}({h}^\\ast) \\leq \\Lambda$\\operatorname{sp}sp({h}h^\\ast) \\leq \\Lambda. Let $\\mathcal{S}_n$\\mathcal{S}_n be an $\\frac{1}{n}$\\frac{1}{n}-net for $\\mathcal{X}$\\mathcal{X}. Assume that $p$p and $r$r satisfy {\\rm \\Cref{ass:Lipschitz}}\\rm \\Cref{ass:Lipschitz} with parameters $L,\\alpha\\geq 0$L,\\alpha\\geq 0. Assume the optimal Bellman operator $\\mathcal{L}$\\mathcal{L} of $M$M is an $1$1-stage $\\nu$\\nu-span contraction for $\\nu\\in[0,1)$\\nu\\in[0,1). For any $T,n\\in\\mathbb{N}$T,n\\in\\mathbb{N}, the in-path regret $\\operatorname{Regret}_{\\infty}^{\\rm path}(T)$\\operatorname{Regret}Regret_{\\infty}\\infty^{\\rm path}\\rm path(T) of the classical version of {\\rm \\cref{algo:quantum_UCCRL}}\\rm \\cref{algo:quantum_UCCRL} is upper-bounded after $T$T steps, with probability at least $1 - \\delta$1 - \\delta, by\n    \n        \\widetilde{O}\\left(\\min\\left\\{\\Lambda+1,\\frac{\\Lambda}{1-\\nu}\\right\\} \\sqrt{\\frac{|\\mathcal{S}_n| A}{(1-\\nu)^2}\\frac{T\\log{T}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}} + \\frac{T(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\right),\n    \n    where $\\widetilde{O}(\\cdot)$\\widetilde{O}(\\cdot) omits $\\poly\\log\\log$\\poly\\log\\log terms.\n    If $\\mathcal{X} = [0,1]^D$\\mathcal{X} = [0,1]^D, in which case $|\\mathcal{S}_n| = O(n^D)$|\\mathcal{S}_n| = O(n^D), and setting $n = T^{\\frac{1}{D+2\\alpha}}$n = T^{\\frac{1}{D+2\\alpha}}\\frac{1}{D+2\\alpha}, the in-path regret is (for constant $\\nu,L$\\nu,L) \n    \n        \\widetilde{O}\\left(\\Lambda T^{\\frac{D+\\alpha}{D+2\\alpha}}\\frac{D+\\alpha}{D+2\\alpha}\\sqrt{A\\log{T}\\log\\frac{AT}{\\delta}}  \\right).\n    \n    If $\\mathcal{X} = \\mathcal{S}$\\mathcal{X} = \\mathcal{S} is finite with size $S$S, in which case $L=0$L=0, then the in-path regret is (for constant $\\nu$\\nu) \n    \n        \\widetilde{O}\\left(\\Lambda\\sqrt{SAT\\log{T}\\log\\frac{SAT}{\\delta}}\\right).\n    \n\n\n\\begin{proof}\n    Let $t_k$ be the starting time of episode $k$ and $\\tau_k = t_{k+1} - t_{k}$ the length of episode $k$. Given $s\\in\\mathcal{S}_n$, let $n_k(s, a) := |\\{t_k\\leq \\tau < t_{k+1}: x_{\\tau} \\in\\mathcal{X}(s), a_{\\tau} = a\\}|$ be the total number of visits in state-action pairs $(x,a)$ during (exploitation phase in) episode $k$ such that $x\\in\\mathcal{X}(s)$ and $n_k(s) := |\\{t_k\\leq \\tau < t_{k+1}: x_{\\tau} \\in \\mathcal{X}(s)\\}| = \\sum_{a\\in\\mathcal{A}}n_k(s,a)$. We shall abuse notation and let $n_k(x, a) := |\\{t_k\\leq \\tau < t_{k+1}: (x_{\\tau}, a_{\\tau}) = (x,a)\\}|$ be the total number of visits in state-action pairs $(x,a)$ during (exploitation phase in) episode $k$. This means that $n_k(x, a)$ is either zero or a Dirac delta depending on $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$ from the exploitation phase of episode $k$. We note that $\\int_{x\\in\\mathcal{X}}\\sum_{a\\in\\mathcal{A}} n_k(x,a)\\text{d}x = \\sum_{s\\in\\mathcal{S}_n,a\\in\\mathcal{A}}n_k(s,a)$.\n\n    Let the in-path regret in episode $k$ be\n    %\n    \\begin{align*}\n        \\Delta_k := \\int_{\\mathcal{X}} n_k(\\rd x) \\big(g^* - r_{\\widetilde{d}_k}(x)\\big).\n    \\end{align*}\n    %\n    Let $m$ denote the number of episodes. Since the policy $\\widetilde{d}_k^\\infty$ changes at most $\\lceil\\log_2{T}\\rceil$ times during $m$ episodes, $m\\leq \\lceil\\log_2{T}\\rceil$. The in-path regret of \\cref{algo:quantum_UCCRL} is thus $\\operatorname{Regret}_{\\infty}^{\\rm path}(T) = \\sum_{k=1}^m \\Delta_k$.\n    \n    Now let $\\widetilde{g}_k := \\frac{1}{2}\\big(\\max_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast+1}(x) - {u}_{t^\\ast}(x)\\} + \\min_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast+1}(x) - {u}_{t^\\ast}(x)\\}\\big)$ be the output of \\Cref{algo:classical_extended_value_iteration} that uses $O(\\tau_k)$ calls to oracle $\\mathcal{C}_p$. According to \\Cref{thr:classical_scopt_algorithm}, with probability at least $1-\\frac{\\delta}{8T^{5/4}}$,\n    %\n    \\begin{align*}\n        \\widetilde{g}_k &\\geq g^{\\ast} - C\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}} - \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\\\\n        \\text{where}~ C &= O\\left(\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu}\\right\\}\\sqrt{\\frac{|\\mathcal{S}_n|A}{(1-\\nu)^2}\\frac{1}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}}\\right),\n    \\end{align*}\n    %\n    which implies that\n    %\n    \\begin{align*}\n        \\Delta_k &\\leq \\int_{\\mathcal{X}} n_k(\\rd x) \\big(\\widetilde{g}_k -  r_{\\widetilde{d}_k}(x) \\big) + \\sum_{s\\in\\mathcal{S}_n} n_k(s)\\left(\\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} + C\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}} \\right)\\\\\n        &= \\int_{\\mathcal{X}} n_k(\\rd x) \\big(\\widetilde{g}_k -  r_{\\widetilde{d}_k}(x) \\big)  + \\tau_k \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} + C\\sum_{s\\in\\mathcal{S}_n} n_k(s)\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}}.\n    \\end{align*}\n    %\n    We first bound the term $\\int_{\\mathcal{X}} n_k(\\rd x)(\\widetilde{g}_k - \\sum_{a\\in\\mathcal{A}} r_{\\widetilde{d}_k}(x)) $. Once $\\operatorname{sp}({u}_{t^\\ast+1} - {u}_{t^\\ast}) \\leq \\frac{3\\varepsilon}{2} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$ at iteration $t^\\ast$ in \\Cref{algo:classical_extended_value_iteration}, then\n    %\n    \\begin{align*}\n        |u_{t^\\ast+1}(x) - u_{t^\\ast}(x) - \\widetilde{g}_k| \\leq \\frac{3C}{2}\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\qquad\\forall x\\in\\mathcal{X}.\n    \\end{align*}\n    %\n    We can expand\n    %\n    \\begin{align*}\n        (\\mathcal{L} {u}_{t^\\ast})(x) = r_{\\widetilde{d}_k}(x) + \\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x'|x) u_{t^\\ast}(x').\n    \\end{align*}\n    %\n    Together with the fact that $\\|{u}_{t^\\ast+1} - \\mathcal{L} {u}_{t^\\ast}\\|_\\infty \\leq  \\frac{C(1-\\nu)}{4}\\sqrt{\\frac{\\log{\\tau_k}}{\\tau_k}} + 4(1+\\Lambda)Ln^{-\\alpha}$, this implies that, $\\forall x\\in\\mathcal{X}$,\n    %\n    \\begin{align*}\n        \\left|\\big(\\widetilde{g}_k - r_{\\widetilde{d}_k}(x) \\big) - \\left(\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x'|x) u_{t^\\ast}(x') - u_{t^\\ast}(x) \\right) \\right| \\leq 2C \\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}} + \\frac{22(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu},\n    \\end{align*}\n    %\n    and so\n    %\n    \\begin{align}\n        \\Delta_k &\\leq \\int_{\\mathcal{X}} n_k(\\rd x) \\left(\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x'|x) u_{t^\\ast}(x') - u_{t^\\ast}(x) \\right) + \\tau_k \\frac{35(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} + 3C\\sum_{s\\in\\mathcal{S}_n}  n_k(s)\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}}. \\label{eq:regret_infinite_horizon_eq1}\n    \\end{align}\n    %\n    Since $\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x'|x) = 1$ for all $x\\in\\mathcal{X}$, we can replace $u_{t^\\ast}(x)$ in \\Cref{eq:regret_infinite_horizon_eq1} with \n    %\n    \\begin{align*}\n        w_k(x) := u_{t^\\ast}(x) - \\frac{1}{2}\\left(\\max_{x'\\in\\mathcal{X}}\\{u_{t^\\ast}(x')\\} + \\min_{x'\\in\\mathcal{X}}\\{u_{t^\\ast}(x')\\}\\right).\n    \\end{align*}\n    %\n    Note that $\\operatorname{sp}({w}_k) = \\operatorname{sp}({u}_{t^\\ast}) \\leq {\\min}\\big\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\big\\}$ since all functions $(u_t)_{t\\in\\mathbb{N}}$ generated by \\Cref{algo:classical_extended_value_iteration} have bounded bias span. Also, $\\|{w}_k\\|_\\infty = \\frac{1}{2}\\operatorname{sp}({w}_k) \\leq {\\min}\\big\\{2\\Lambda + 2, \\frac{\\Lambda}{1-\\nu}\\big\\}$. We now rewrite the first term in \\Cref{eq:regret_infinite_horizon_eq1} as\n    %\n    \\begin{align*}\n        \\int_{\\mathcal{X}} n_k(\\rd x) &\\left(\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x'|x) w_k(x') - w_k(x) \\right)\\\\\n        &= \\sum_{t=t_k}^{t_{k+1}-1}\\left(\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\rd x'|x_t) w_k(x') - w_k(x_t)\\right)\\\\\n        &= w_k(x_{t_{k+1}}) - w_k(x_{t_{k}}) + \\sum_{t=t_k}^{t_{k+1}-1}\\left(\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\rd x'|x_t) w_k(x') - w_k(x_{t+1}) \\right) \\\\\n        &= w_k(x_{t_{k+1}}) - w_k(x_{t_{k}}) + \\sum_{t=t_k}^{t_{k+1}-1}X_t.\n    \\end{align*}\n    %\n    Notice that the sequence $X_t := \\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x|x_{t}) w_k(x) - w_k(x_{t+1})$ is a sequence of martingale differences. Since $|X_t| \\leq 2\\|{w}_k\\|_\\infty \\leq {\\min}\\big\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\big\\}$, by the Azuma-Hoeffding inequality, \n    %\n    \\begin{align*}\n        \\operatorname{Pr}\\left[\\sum_{t=1}^T X_t \\geq \\min\\left\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\right\\}\\sqrt{2T\\ln\\frac{8T^{5/4}}{\\delta}}\\right] \\leq \\frac{\\delta}{8T^{5/4}}.\n    \\end{align*}\n    %\n    Thus, we can bound the desired term as (already summing over all episodes)\n    %\n    \\begin{align*}\n        \\sum_{k=1}^m \\int_{\\mathcal{X}} \\! n_k(\\rd x) \\!\\left(\\int_{\\mathcal{X}} \\! p_{\\widetilde{d}_k}(\\text{d}x'|x) w_k(x') - w_k(x) \\right) \\leq \\min\\!\\left\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\right\\}\\!\\left(\\!\\lceil\\log_2{T}\\rceil + \\sqrt{2T\\ln\\frac{8T^{5/4}}{\\delta}}\\right)\n    \\end{align*}\n    %\n    with probability at least $1-\\frac{\\delta}{8T^{5/4}}$. \n    \n    Regarding $\\sum_{k=1}^m \\sum_{s\\in\\mathcal{S}_n}n_k(s)\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}} \\leq \\sqrt{2\\lceil\\log_2{T}\\rceil} \\sum_{k=1}^m \\sum_{s\\in\\mathcal{S}_n}\\frac{n_k(s)}{\\sqrt{t_k}}$ (since $\\tau_k \\geq \\frac{t_k}{2}$), we can use \\Cref{fact:useful_inequality} in order to bound\n    %\n    \\begin{align*}\n        \\sum_{k=1}^m \\sum_{s\\in\\mathcal{S}_n}\\frac{n_k(s)}{\\sqrt{t_k}} = \\sum_{k=1}^m \\frac{\\sum_{s\\in\\mathcal{S}_n} n_k(s)}{\\sqrt{\\sum_{i=1}^{k-1} \\sum_{s\\in\\mathcal{S}} n_k(s)}} \\leq \\frac{\\sqrt{T}}{\\sqrt{2}-1}.\n    \\end{align*}\n    %\n    Putting everything together, with probability at least $1-\\frac{\\delta}{4T^{5/4}}$, the total in-path regret is\n    %\n    \\begin{align*}\n        \\operatorname{Regret}_{\\infty}^{\\rm path}(T) \\leq 2\\min\\left\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\right\\}\\sqrt{2T\\ln \\frac{8T^{5/4}}{\\delta}} + \\frac{3\\sqrt{2}C}{\\sqrt{2}-1}\\sqrt{T\\lceil\\log_2{T}\\rceil} + T\\frac{35(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}.\n    \\end{align*}\n    %\n    Finally, since $\\sum_{T=1}^\\infty \\frac{\\delta}{4T^{5/4}} < \\delta$, the above bound is valid for all $T\\in\\mathbb{N}$.\n\\end{proof}\\begin{proof}\n    Let $t_k$ be the starting time of episode $k$ and $\\tau_k = t_{k+1} - t_{k}$ the length of episode $k$. Given $s\\in\\mathcal{S}_n$, let $n_k(s, a) := |\\{t_k\\leq \\tau < t_{k+1}: x_{\\tau} \\in\\mathcal{X}(s), a_{\\tau} = a\\}|$ be the total number of visits in state-action pairs $(x,a)$ during (exploitation phase in) episode $k$ such that $x\\in\\mathcal{X}(s)$ and $n_k(s) := |\\{t_k\\leq \\tau < t_{k+1}: x_{\\tau} \\in \\mathcal{X}(s)\\}| = \\sum_{a\\in\\mathcal{A}}n_k(s,a)$. We shall abuse notation and let $n_k(x, a) := |\\{t_k\\leq \\tau < t_{k+1}: (x_{\\tau}, a_{\\tau}) = (x,a)\\}|$ be the total number of visits in state-action pairs $(x,a)$ during (exploitation phase in) episode $k$. This means that $n_k(x, a)$ is either zero or a Dirac delta depending on $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$ from the exploitation phase of episode $k$. We note that $\\int_{x\\in\\mathcal{X}}\\sum_{a\\in\\mathcal{A}} n_k(x,a)\\text{d}x = \\sum_{s\\in\\mathcal{S}_n,a\\in\\mathcal{A}}n_k(s,a)$.\n\n    Let the in-path regret in episode $k$ be\n    %\n    \\begin{align*}\n        \\Delta_k := \\int_{\\mathcal{X}} n_k(\\rd x) \\big(g^* - r_{\\widetilde{d}_k}(x)\\big).\n    \\end{align*}\n    %\n    Let $m$ denote the number of episodes. Since the policy $\\widetilde{d}_k^\\infty$ changes at most $\\lceil\\log_2{T}\\rceil$ times during $m$ episodes, $m\\leq \\lceil\\log_2{T}\\rceil$. The in-path regret of \\cref{algo:quantum_UCCRL} is thus $\\operatorname{Regret}_{\\infty}^{\\rm path}(T) = \\sum_{k=1}^m \\Delta_k$.\n    \n    Now let $\\widetilde{g}_k := \\frac{1}{2}\\big(\\max_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast+1}(x) - {u}_{t^\\ast}(x)\\} + \\min_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast+1}(x) - {u}_{t^\\ast}(x)\\}\\big)$ be the output of \\Cref{algo:classical_extended_value_iteration} that uses $O(\\tau_k)$ calls to oracle $\\mathcal{C}_p$. According to \\Cref{thr:classical_scopt_algorithm}, with probability at least $1-\\frac{\\delta}{8T^{5/4}}$,\n    %\n    \\begin{align*}\n        \\widetilde{g}_k &\\geq g^{\\ast} - C\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}} - \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\\\\n        \\text{where}~ C &= O\\left(\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu}\\right\\}\\sqrt{\\frac{|\\mathcal{S}_n|A}{(1-\\nu)^2}\\frac{1}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}}\\right),\n    \\end{align*}\n    %\n    which implies that\n    %\n    \\begin{align*}\n        \\Delta_k &\\leq \\int_{\\mathcal{X}} n_k(\\rd x) \\big(\\widetilde{g}_k -  r_{\\widetilde{d}_k}(x) \\big) + \\sum_{s\\in\\mathcal{S}_n} n_k(s)\\left(\\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} + C\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}} \\right)\\\\\n        &= \\int_{\\mathcal{X}} n_k(\\rd x) \\big(\\widetilde{g}_k -  r_{\\widetilde{d}_k}(x) \\big)  + \\tau_k \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} + C\\sum_{s\\in\\mathcal{S}_n} n_k(s)\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}}.\n    \\end{align*}\n    %\n    We first bound the term $\\int_{\\mathcal{X}} n_k(\\rd x)(\\widetilde{g}_k - \\sum_{a\\in\\mathcal{A}} r_{\\widetilde{d}_k}(x)) $. Once $\\operatorname{sp}({u}_{t^\\ast+1} - {u}_{t^\\ast}) \\leq \\frac{3\\varepsilon}{2} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$ at iteration $t^\\ast$ in \\Cref{algo:classical_extended_value_iteration}, then\n    %\n    \\begin{align*}\n        |u_{t^\\ast+1}(x) - u_{t^\\ast}(x) - \\widetilde{g}_k| \\leq \\frac{3C}{2}\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\qquad\\forall x\\in\\mathcal{X}.\n    \\end{align*}\n    %\n    We can expand\n    %\n    \\begin{align*}\n        (\\mathcal{L} {u}_{t^\\ast})(x) = r_{\\widetilde{d}_k}(x) + \\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x'|x) u_{t^\\ast}(x').\n    \\end{align*}\n    %\n    Together with the fact that $\\|{u}_{t^\\ast+1} - \\mathcal{L} {u}_{t^\\ast}\\|_\\infty \\leq  \\frac{C(1-\\nu)}{4}\\sqrt{\\frac{\\log{\\tau_k}}{\\tau_k}} + 4(1+\\Lambda)Ln^{-\\alpha}$, this implies that, $\\forall x\\in\\mathcal{X}$,\n    %\n    \\begin{align*}\n        \\left|\\big(\\widetilde{g}_k - r_{\\widetilde{d}_k}(x) \\big) - \\left(\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x'|x) u_{t^\\ast}(x') - u_{t^\\ast}(x) \\right) \\right| \\leq 2C \\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}} + \\frac{22(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu},\n    \\end{align*}\n    %\n    and so\n    %\n    \\begin{align}\n        \\Delta_k &\\leq \\int_{\\mathcal{X}} n_k(\\rd x) \\left(\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x'|x) u_{t^\\ast}(x') - u_{t^\\ast}(x) \\right) + \\tau_k \\frac{35(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} + 3C\\sum_{s\\in\\mathcal{S}_n}  n_k(s)\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}}. \\label{eq:regret_infinite_horizon_eq1}\n    \\end{align}\n    %\n    Since $\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x'|x) = 1$ for all $x\\in\\mathcal{X}$, we can replace $u_{t^\\ast}(x)$ in \\Cref{eq:regret_infinite_horizon_eq1} with \n    %\n    \\begin{align*}\n        w_k(x) := u_{t^\\ast}(x) - \\frac{1}{2}\\left(\\max_{x'\\in\\mathcal{X}}\\{u_{t^\\ast}(x')\\} + \\min_{x'\\in\\mathcal{X}}\\{u_{t^\\ast}(x')\\}\\right).\n    \\end{align*}\n    %\n    Note that $\\operatorname{sp}({w}_k) = \\operatorname{sp}({u}_{t^\\ast}) \\leq {\\min}\\big\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\big\\}$ since all functions $(u_t)_{t\\in\\mathbb{N}}$ generated by \\Cref{algo:classical_extended_value_iteration} have bounded bias span. Also, $\\|{w}_k\\|_\\infty = \\frac{1}{2}\\operatorname{sp}({w}_k) \\leq {\\min}\\big\\{2\\Lambda + 2, \\frac{\\Lambda}{1-\\nu}\\big\\}$. We now rewrite the first term in \\Cref{eq:regret_infinite_horizon_eq1} as\n    %\n    \\begin{align*}\n        \\int_{\\mathcal{X}} n_k(\\rd x) &\\left(\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x'|x) w_k(x') - w_k(x) \\right)\\\\\n        &= \\sum_{t=t_k}^{t_{k+1}-1}\\left(\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\rd x'|x_t) w_k(x') - w_k(x_t)\\right)\\\\\n        &= w_k(x_{t_{k+1}}) - w_k(x_{t_{k}}) + \\sum_{t=t_k}^{t_{k+1}-1}\\left(\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\rd x'|x_t) w_k(x') - w_k(x_{t+1}) \\right) \\\\\n        &= w_k(x_{t_{k+1}}) - w_k(x_{t_{k}}) + \\sum_{t=t_k}^{t_{k+1}-1}X_t.\n    \\end{align*}\n    %\n    Notice that the sequence $X_t := \\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x|x_{t}) w_k(x) - w_k(x_{t+1})$ is a sequence of martingale differences. Since $|X_t| \\leq 2\\|{w}_k\\|_\\infty \\leq {\\min}\\big\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\big\\}$, by the Azuma-Hoeffding inequality, \n    %\n    \\begin{align*}\n        \\operatorname{Pr}\\left[\\sum_{t=1}^T X_t \\geq \\min\\left\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\right\\}\\sqrt{2T\\ln\\frac{8T^{5/4}}{\\delta}}\\right] \\leq \\frac{\\delta}{8T^{5/4}}.\n    \\end{align*}\n    %\n    Thus, we can bound the desired term as (already summing over all episodes)\n    %\n    \\begin{align*}\n        \\sum_{k=1}^m \\int_{\\mathcal{X}} \\! n_k(\\rd x) \\!\\left(\\int_{\\mathcal{X}} \\! p_{\\widetilde{d}_k}(\\text{d}x'|x) w_k(x') - w_k(x) \\right) \\leq \\min\\!\\left\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\right\\}\\!\\left(\\!\\lceil\\log_2{T}\\rceil + \\sqrt{2T\\ln\\frac{8T^{5/4}}{\\delta}}\\right)\n    \\end{align*}\n    %\n    with probability at least $1-\\frac{\\delta}{8T^{5/4}}$. \n    \n    Regarding $\\sum_{k=1}^m \\sum_{s\\in\\mathcal{S}_n}n_k(s)\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}} \\leq \\sqrt{2\\lceil\\log_2{T}\\rceil} \\sum_{k=1}^m \\sum_{s\\in\\mathcal{S}_n}\\frac{n_k(s)}{\\sqrt{t_k}}$ (since $\\tau_k \\geq \\frac{t_k}{2}$), we can use \\Cref{fact:useful_inequality} in order to bound\n    %\n    \\begin{align*}\n        \\sum_{k=1}^m \\sum_{s\\in\\mathcal{S}_n}\\frac{n_k(s)}{\\sqrt{t_k}} = \\sum_{k=1}^m \\frac{\\sum_{s\\in\\mathcal{S}_n} n_k(s)}{\\sqrt{\\sum_{i=1}^{k-1} \\sum_{s\\in\\mathcal{S}} n_k(s)}} \\leq \\frac{\\sqrt{T}}{\\sqrt{2}-1}.\n    \\end{align*}\n    %\n    Putting everything together, with probability at least $1-\\frac{\\delta}{4T^{5/4}}$, the total in-path regret is\n    %\n    \\begin{align*}\n        \\operatorname{Regret}_{\\infty}^{\\rm path}(T) \\leq 2\\min\\left\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\right\\}\\sqrt{2T\\ln \\frac{8T^{5/4}}{\\delta}} + \\frac{3\\sqrt{2}C}{\\sqrt{2}-1}\\sqrt{T\\lceil\\log_2{T}\\rceil} + T\\frac{35(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}.\n    \\end{align*}\n    %\n    Finally, since $\\sum_{T=1}^\\infty \\frac{\\delta}{4T^{5/4}} < \\delta$, the above bound is valid for all $T\\in\\mathbb{N}$.\n\\end{proof}\n    Let $t_k$t_k be the starting time of episode $k$k and $\\tau_k = t_{k+1} - t_{k}$\\tau_k = t_{k+1}k+1 - t_{k}k the length of episode $k$k. Given $s\\in\\mathcal{S}_n$s\\in\\mathcal{S}_n, let $n_k(s, a) := |\\{t_k\\leq \\tau < t_{k+1}: x_{\\tau} \\in\\mathcal{X}(s), a_{\\tau} = a\\}|$n_k(s, a) := |\\{t_k\\leq \\tau < t_{k+1}k+1: x_{\\tau}\\tau \\in\\mathcal{X}(s), a_{\\tau}\\tau = a\\}| be the total number of visits in state-action pairs $(x,a)$(x,a) during (exploitation phase in) episode $k$k such that $x\\in\\mathcal{X}(s)$x\\in\\mathcal{X}(s) and $n_k(s) := |\\{t_k\\leq \\tau < t_{k+1}: x_{\\tau} \\in \\mathcal{X}(s)\\}| = \\sum_{a\\in\\mathcal{A}}n_k(s,a)$n_k(s) := |\\{t_k\\leq \\tau < t_{k+1}k+1: x_{\\tau}\\tau \\in \\mathcal{X}(s)\\}| = \\sum_{a\\in\\mathcal{A}}a\\in\\mathcal{A}n_k(s,a). We shall abuse notation and let $n_k(x, a) := |\\{t_k\\leq \\tau < t_{k+1}: (x_{\\tau}, a_{\\tau}) = (x,a)\\}|$n_k(x, a) := |\\{t_k\\leq \\tau < t_{k+1}k+1: (x_{\\tau}\\tau, a_{\\tau}\\tau) = (x,a)\\}| be the total number of visits in state-action pairs $(x,a)$(x,a) during (exploitation phase in) episode $k$k. This means that $n_k(x, a)$n_k(x, a) is either zero or a Dirac delta depending on $(x,a)\\in\\mathcal{X}\\times\\mathcal{A}$(x,a)\\in\\mathcal{X}\\times\\mathcal{A} from the exploitation phase of episode $k$k. We note that $\\int_{x\\in\\mathcal{X}}\\sum_{a\\in\\mathcal{A}} n_k(x,a)\\text{d}x = \\sum_{s\\in\\mathcal{S}_n,a\\in\\mathcal{A}}n_k(s,a)$\\int_{x\\in\\mathcal{X}}x\\in\\mathcal{X}\\sum_{a\\in\\mathcal{A}}a\\in\\mathcal{A} n_k(x,a)\\text{d}x = \\sum_{s\\in\\mathcal{S}_n,a\\in\\mathcal{A}}s\\in\\mathcal{S}_n,a\\in\\mathcal{A}n_k(s,a).\n\n    Let the in-path regret in episode $k$k be\n    \n        \\Delta_k := \\int_{\\mathcal{X}}\\mathcal{X} n_k(\\rd x) \\big(g^* - r_{\\widetilde{d}_k}\\widetilde{d}_k(x)\\big).\n    \n    Let $m$m denote the number of episodes. Since the policy $\\widetilde{d}_k^\\infty$\\widetilde{d}_k^\\infty changes at most $\\lceil\\log_2{T}\\rceil$\\lceil\\log_2{T}T\\rceil times during $m$m episodes, $m\\leq \\lceil\\log_2{T}\\rceil$m\\leq \\lceil\\log_2{T}T\\rceil. The in-path regret of \\cref{algo:quantum_UCCRL} is thus $\\operatorname{Regret}_{\\infty}^{\\rm path}(T) = \\sum_{k=1}^m \\Delta_k$\\operatorname{Regret}Regret_{\\infty}\\infty^{\\rm path}\\rm path(T) = \\sum_{k=1}k=1^m \\Delta_k.\n    \n    Now let $\\widetilde{g}_k := \\frac{1}{2}\\big(\\max_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast+1}(x) - {u}_{t^\\ast}(x)\\} + \\min_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast+1}(x) - {u}_{t^\\ast}(x)\\}\\big)$\\widetilde{g}_k := \\frac{1}{2}\\big(\\max_{x\\in\\mathcal{X}}x\\in\\mathcal{X}\\{{u}u_{t^\\ast+1}t^\\ast+1(x) - {u}u_{t^\\ast}t^\\ast(x)\\} + \\min_{x\\in\\mathcal{X}}x\\in\\mathcal{X}\\{{u}u_{t^\\ast+1}t^\\ast+1(x) - {u}u_{t^\\ast}t^\\ast(x)\\}\\big) be the output of \\Cref{algo:classical_extended_value_iteration} that uses $O(\\tau_k)$O(\\tau_k) calls to oracle $\\mathcal{C}_p$\\mathcal{C}_p. According to \\Cref{thr:classical_scopt_algorithm}, with probability at least $1-\\frac{\\delta}{8T^{5/4}}$1-\\frac{\\delta}{8T^{5/4}},\n    \n        \\widetilde{g}_k &\\geq g^{\\ast}\\ast - C\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}} - \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\\\\n        \\text{where}~ C &= O\\left(\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu}\\right\\}\\sqrt{\\frac{|\\mathcal{S}_n|A}{(1-\\nu)^2}\\frac{1}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}}\\right),\n    \n    which implies that\n    \n        \\Delta_k &\\leq \\int_{\\mathcal{X}}\\mathcal{X} n_k(\\rd x) \\big(\\widetilde{g}_k -  r_{\\widetilde{d}_k}\\widetilde{d}_k(x) \\big) + \\sum_{s\\in\\mathcal{S}_n}s\\in\\mathcal{S}_n n_k(s)\\left(\\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} + C\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}} \\right)\\\\\n        &= \\int_{\\mathcal{X}}\\mathcal{X} n_k(\\rd x) \\big(\\widetilde{g}_k -  r_{\\widetilde{d}_k}\\widetilde{d}_k(x) \\big)  + \\tau_k \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} + C\\sum_{s\\in\\mathcal{S}_n}s\\in\\mathcal{S}_n n_k(s)\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}}.\n    \n    We first bound the term $\\int_{\\mathcal{X}} n_k(\\rd x)(\\widetilde{g}_k - \\sum_{a\\in\\mathcal{A}} r_{\\widetilde{d}_k}(x)) $\\int_{\\mathcal{X}}\\mathcal{X} n_k(\\rd x)(\\widetilde{g}_k - \\sum_{a\\in\\mathcal{A}}a\\in\\mathcal{A} r_{\\widetilde{d}_k}\\widetilde{d}_k(x)) . Once $\\operatorname{sp}({u}_{t^\\ast+1} - {u}_{t^\\ast}) \\leq \\frac{3\\varepsilon}{2} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}$\\operatorname{sp}sp({u}u_{t^\\ast+1}t^\\ast+1 - {u}u_{t^\\ast}t^\\ast) \\leq \\frac{3\\varepsilon}{2} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} at iteration $t^\\ast$t^\\ast in \\Cref{algo:classical_extended_value_iteration}, then\n    \n        |u_{t^\\ast+1}t^\\ast+1(x) - u_{t^\\ast}t^\\ast(x) - \\widetilde{g}_k| \\leq \\frac{3C}{2}\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}} + \\frac{18(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\qquad\\forall x\\in\\mathcal{X}.\n    \n    We can expand\n    \n        (\\mathcal{L} {u}u_{t^\\ast}t^\\ast)(x) = r_{\\widetilde{d}_k}\\widetilde{d}_k(x) + \\int_{\\mathcal{X}}\\mathcal{X} p_{\\widetilde{d}_k}\\widetilde{d}_k(\\text{d}x'|x) u_{t^\\ast}t^\\ast(x').\n    \n    Together with the fact that $\\|{u}_{t^\\ast+1} - \\mathcal{L} {u}_{t^\\ast}\\|_\\infty \\leq  \\frac{C(1-\\nu)}{4}\\sqrt{\\frac{\\log{\\tau_k}}{\\tau_k}} + 4(1+\\Lambda)Ln^{-\\alpha}$\\|{u}u_{t^\\ast+1}t^\\ast+1 - \\mathcal{L} {u}u_{t^\\ast}t^\\ast\\|_\\infty \\leq  \\frac{C(1-\\nu)}{4}\\sqrt{\\frac{\\log{\\tau_k}}{\\tau_k}} + 4(1+\\Lambda)Ln^{-\\alpha}-\\alpha, this implies that, $\\forall x\\in\\mathcal{X}$\\forall x\\in\\mathcal{X},\n    \n        \\left|\\big(\\widetilde{g}_k - r_{\\widetilde{d}_k}\\widetilde{d}_k(x) \\big) - \\left(\\int_{\\mathcal{X}}\\mathcal{X} p_{\\widetilde{d}_k}\\widetilde{d}_k(\\text{d}x'|x) u_{t^\\ast}t^\\ast(x') - u_{t^\\ast}t^\\ast(x) \\right) \\right| \\leq 2C \\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}} + \\frac{22(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu},\n    \n    and so\n    \n        \\Delta_k &\\leq \\int_{\\mathcal{X}}\\mathcal{X} n_k(\\rd x) \\left(\\int_{\\mathcal{X}}\\mathcal{X} p_{\\widetilde{d}_k}\\widetilde{d}_k(\\text{d}x'|x) u_{t^\\ast}t^\\ast(x') - u_{t^\\ast}t^\\ast(x) \\right) + \\tau_k \\frac{35(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} + 3C\\sum_{s\\in\\mathcal{S}_n}s\\in\\mathcal{S}_n  n_k(s)\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}}. \\label{eq:regret_infinite_horizon_eq1}\n    \n    Since $\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x'|x) = 1$\\int_{\\mathcal{X}}\\mathcal{X} p_{\\widetilde{d}_k}\\widetilde{d}_k(\\text{d}x'|x) = 1 for all $x\\in\\mathcal{X}$x\\in\\mathcal{X}, we can replace $u_{t^\\ast}(x)$u_{t^\\ast}t^\\ast(x) in \\Cref{eq:regret_infinite_horizon_eq1} with \n    \n        w_k(x) := u_{t^\\ast}t^\\ast(x) - \\frac{1}{2}\\left(\\max_{x'\\in\\mathcal{X}}x'\\in\\mathcal{X}\\{u_{t^\\ast}t^\\ast(x')\\} + \\min_{x'\\in\\mathcal{X}}x'\\in\\mathcal{X}\\{u_{t^\\ast}t^\\ast(x')\\}\\right).\n    \n    Note that $\\operatorname{sp}({w}_k) = \\operatorname{sp}({u}_{t^\\ast}) \\leq {\\min}\\big\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\big\\}$\\operatorname{sp}sp({w}w_k) = \\operatorname{sp}sp({u}u_{t^\\ast}t^\\ast) \\leq {\\min}\\min\\big\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\big\\} since all functions $(u_t)_{t\\in\\mathbb{N}}$(u_t)_{t\\in\\mathbb{N}}t\\in\\mathbb{N} generated by \\Cref{algo:classical_extended_value_iteration} have bounded bias span. Also, $\\|{w}_k\\|_\\infty = \\frac{1}{2}\\operatorname{sp}({w}_k) \\leq {\\min}\\big\\{2\\Lambda + 2, \\frac{\\Lambda}{1-\\nu}\\big\\}$\\|{w}w_k\\|_\\infty = \\frac{1}{2}\\operatorname{sp}sp({w}w_k) \\leq {\\min}\\min\\big\\{2\\Lambda + 2, \\frac{\\Lambda}{1-\\nu}\\big\\}. We now rewrite the first term in \\Cref{eq:regret_infinite_horizon_eq1} as\n    \n        \\int_{\\mathcal{X}}\\mathcal{X} n_k(\\rd x) &\\left(\\int_{\\mathcal{X}}\\mathcal{X} p_{\\widetilde{d}_k}\\widetilde{d}_k(\\text{d}x'|x) w_k(x') - w_k(x) \\right)\\\\\n        &= \\sum_{t=t_k}t=t_k^{t_{k+1}-1}t_{k+1}k+1-1\\left(\\int_{\\mathcal{X}}\\mathcal{X} p_{\\widetilde{d}_k}\\widetilde{d}_k(\\rd x'|x_t) w_k(x') - w_k(x_t)\\right)\\\\\n        &= w_k(x_{t_{k+1}}t_{k+1}k+1) - w_k(x_{t_{k}}t_{k}k) + \\sum_{t=t_k}t=t_k^{t_{k+1}-1}t_{k+1}k+1-1\\left(\\int_{\\mathcal{X}}\\mathcal{X} p_{\\widetilde{d}_k}\\widetilde{d}_k(\\rd x'|x_t) w_k(x') - w_k(x_{t+1}t+1) \\right) \\\\\n        &= w_k(x_{t_{k+1}}t_{k+1}k+1) - w_k(x_{t_{k}}t_{k}k) + \\sum_{t=t_k}t=t_k^{t_{k+1}-1}t_{k+1}k+1-1X_t.\n    \n    Notice that the sequence $X_t := \\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x|x_{t}) w_k(x) - w_k(x_{t+1})$X_t := \\int_{\\mathcal{X}}\\mathcal{X} p_{\\widetilde{d}_k}\\widetilde{d}_k(\\text{d}x|x_{t}t) w_k(x) - w_k(x_{t+1}t+1) is a sequence of martingale differences. Since $|X_t| \\leq 2\\|{w}_k\\|_\\infty \\leq {\\min}\\big\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\big\\}$|X_t| \\leq 2\\|{w}w_k\\|_\\infty \\leq {\\min}\\min\\big\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\big\\}, by the Azuma-Hoeffding inequality, \n    \n        \\operatorname{Pr}Pr\\left[\\sum_{t=1}t=1^T X_t \\geq \\min\\left\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\right\\}\\sqrt{2T\\ln\\frac{8T^{5/4}}{\\delta}}\\right] \\leq \\frac{\\delta}{8T^{5/4}}.\n    \n    Thus, we can bound the desired term as (already summing over all episodes)\n    \n        \\sum_{k=1}k=1^m \\int_{\\mathcal{X}}\\mathcal{X} \\! n_k(\\rd x) \\!\\left(\\int_{\\mathcal{X}}\\mathcal{X} \\! p_{\\widetilde{d}_k}\\widetilde{d}_k(\\text{d}x'|x) w_k(x') - w_k(x) \\right) \\leq \\min\\!\\left\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\right\\}\\!\\left(\\!\\lceil\\log_2{T}T\\rceil + \\sqrt{2T\\ln\\frac{8T^{5/4}}{\\delta}}\\right)\n    \n    with probability at least $1-\\frac{\\delta}{8T^{5/4}}$1-\\frac{\\delta}{8T^{5/4}}. \n    \n    Regarding $\\sum_{k=1}^m \\sum_{s\\in\\mathcal{S}_n}n_k(s)\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}} \\leq \\sqrt{2\\lceil\\log_2{T}\\rceil} \\sum_{k=1}^m \\sum_{s\\in\\mathcal{S}_n}\\frac{n_k(s)}{\\sqrt{t_k}}$\\sum_{k=1}k=1^m \\sum_{s\\in\\mathcal{S}_n}s\\in\\mathcal{S}_nn_k(s)\\sqrt{\\frac{\\log_2{\\tau_k}}{\\tau_k}} \\leq \\sqrt{2\\lceil\\log_2{T}\\rceil} \\sum_{k=1}k=1^m \\sum_{s\\in\\mathcal{S}_n}s\\in\\mathcal{S}_n\\frac{n_k(s)}{\\sqrt{t_k}} (since $\\tau_k \\geq \\frac{t_k}{2}$\\tau_k \\geq \\frac{t_k}{2}), we can use \\Cref{fact:useful_inequality} in order to bound\n    \n        \\sum_{k=1}k=1^m \\sum_{s\\in\\mathcal{S}_n}s\\in\\mathcal{S}_n\\frac{n_k(s)}{\\sqrt{t_k}} = \\sum_{k=1}k=1^m \\frac{\\sum_{s\\in\\mathcal{S}_n} n_k(s)}{\\sqrt{\\sum_{i=1}^{k-1} \\sum_{s\\in\\mathcal{S}} n_k(s)}} \\leq \\frac{\\sqrt{T}}{\\sqrt{2}-1}.\n    \n    Putting everything together, with probability at least $1-\\frac{\\delta}{4T^{5/4}}$1-\\frac{\\delta}{4T^{5/4}}, the total in-path regret is\n    \n        \\operatorname{Regret}Regret_{\\infty}\\infty^{\\rm path}\\rm path(T) \\leq 2\\min\\left\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\right\\}\\sqrt{2T\\ln \\frac{8T^{5/4}}{\\delta}} + \\frac{3\\sqrt{2}C}{\\sqrt{2}-1}\\sqrt{T\\lceil\\log_2{T}\\rceil} + T\\frac{35(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}.\n    \n    Finally, since $\\sum_{T=1}^\\infty \\frac{\\delta}{4T^{5/4}} < \\delta$\\sum_{T=1}T=1^\\infty \\frac{\\delta}{4T^{5/4}} < \\delta, the above bound is valid for all $T\\in\\mathbb{N}$T\\in\\mathbb{N}.\n\n\n\nWe now prove upper bounds on the in-path regret for the quantum version of \\Cref{algo:quantum_UCCRL}, where the choice for an approximate optimal policy is done quantumly via \\Cref{algo:quantum_extended_value_iteration}.\n\n\\begin{theorem}[Quantum infinite-horizon in-path regret bound]\\label{thr:quantum_RL_algorithm_continuous}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ be an infinite-horizon weakly communicating MDP with $\\operatorname{sp}({h}^\\ast) \\leq \\Lambda$. Let $\\mathcal{S}_n$ be an $\\frac{1}{n}$-net for $\\mathcal{X}$. Assume that $p$ and $r$ satisfy {\\rm \\Cref{ass:Lipschitz}} with parameters $L,\\alpha\\geq 0$. Assume the optimal Bellman operator $\\mathcal{L}$ of $M$ is an $1$-stage $\\nu$-span contraction for $\\nu\\in[0,1)$. For any $T,n\\in\\mathbb{N}$, the in-path regret $\\operatorname{Regret}_{\\infty}^{\\rm path}(T)$ of the quantum version of {\\rm \\cref{algo:quantum_UCCRL}} is upper-bounded after $T$ steps, with probability at least $1 - \\delta$, by\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\!{\\min}\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu} \\right\\}\\!\\left(\\!\\sqrt{T\\log\\frac{T}{\\delta}} + \\frac{|\\mathcal{S}_n| \\sqrt{A}}{1-\\nu}\\frac{\\log^2{T}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}\\log\\frac{|\\mathcal{S}_n|T}{\\delta}\\!\\right) + \\frac{T(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\right), \n    \\end{align*}\n    %\n    where $\\widetilde{O}(\\cdot)$ omits $\\poly\\log\\log$ terms.\n    If $\\mathcal{X} = [0,1]^D$, in which case $|\\mathcal{S}_n| = O(n^D)$, and setting $n = T^{\\frac{1}{D+\\alpha}}$, the in-path regret is (for constant $\\nu,L$)\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\Lambda\\sqrt{T\\log\\frac{T}{\\delta}} + \\Lambda T^{\\frac{D}{D+\\alpha}}\\sqrt{A} \\log^2{T}\\log\\frac{AT}{\\delta}\\log\\frac{T}{\\delta} \\right).\n    \\end{align*}\n    %\n    If $\\mathcal{X} = \\mathcal{S}$ is finite with size $S$, in which case $L=0$, then the in-path regret is (for constant $\\nu$)\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\Lambda\\sqrt{T\\log\\frac{T}{\\delta}} + \\Lambda S\\sqrt{A} \\log^2{T}\\log\\frac{SAT}{\\delta}\\log\\frac{ST}{\\delta}\\right).\n    \\end{align*}\n\\end{theorem}\\begin{theorem}[Quantum infinite-horizon in-path regret bound]\\label{thr:quantum_RL_algorithm_continuous}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ be an infinite-horizon weakly communicating MDP with $\\operatorname{sp}({h}^\\ast) \\leq \\Lambda$. Let $\\mathcal{S}_n$ be an $\\frac{1}{n}$-net for $\\mathcal{X}$. Assume that $p$ and $r$ satisfy {\\rm \\Cref{ass:Lipschitz}} with parameters $L,\\alpha\\geq 0$. Assume the optimal Bellman operator $\\mathcal{L}$ of $M$ is an $1$-stage $\\nu$-span contraction for $\\nu\\in[0,1)$. For any $T,n\\in\\mathbb{N}$, the in-path regret $\\operatorname{Regret}_{\\infty}^{\\rm path}(T)$ of the quantum version of {\\rm \\cref{algo:quantum_UCCRL}} is upper-bounded after $T$ steps, with probability at least $1 - \\delta$, by\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\!{\\min}\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu} \\right\\}\\!\\left(\\!\\sqrt{T\\log\\frac{T}{\\delta}} + \\frac{|\\mathcal{S}_n| \\sqrt{A}}{1-\\nu}\\frac{\\log^2{T}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}\\log\\frac{|\\mathcal{S}_n|T}{\\delta}\\!\\right) + \\frac{T(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\right), \n    \\end{align*}\n    %\n    where $\\widetilde{O}(\\cdot)$ omits $\\poly\\log\\log$ terms.\n    If $\\mathcal{X} = [0,1]^D$, in which case $|\\mathcal{S}_n| = O(n^D)$, and setting $n = T^{\\frac{1}{D+\\alpha}}$, the in-path regret is (for constant $\\nu,L$)\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\Lambda\\sqrt{T\\log\\frac{T}{\\delta}} + \\Lambda T^{\\frac{D}{D+\\alpha}}\\sqrt{A} \\log^2{T}\\log\\frac{AT}{\\delta}\\log\\frac{T}{\\delta} \\right).\n    \\end{align*}\n    %\n    If $\\mathcal{X} = \\mathcal{S}$ is finite with size $S$, in which case $L=0$, then the in-path regret is (for constant $\\nu$)\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\Lambda\\sqrt{T\\log\\frac{T}{\\delta}} + \\Lambda S\\sqrt{A} \\log^2{T}\\log\\frac{SAT}{\\delta}\\log\\frac{ST}{\\delta}\\right).\n    \\end{align*}\n\\end{theorem}\\label{thr:quantum_RL_algorithm_continuous}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle be an infinite-horizon weakly communicating MDP with $\\operatorname{sp}({h}^\\ast) \\leq \\Lambda$\\operatorname{sp}sp({h}h^\\ast) \\leq \\Lambda. Let $\\mathcal{S}_n$\\mathcal{S}_n be an $\\frac{1}{n}$\\frac{1}{n}-net for $\\mathcal{X}$\\mathcal{X}. Assume that $p$p and $r$r satisfy {\\rm \\Cref{ass:Lipschitz}}\\rm \\Cref{ass:Lipschitz} with parameters $L,\\alpha\\geq 0$L,\\alpha\\geq 0. Assume the optimal Bellman operator $\\mathcal{L}$\\mathcal{L} of $M$M is an $1$1-stage $\\nu$\\nu-span contraction for $\\nu\\in[0,1)$\\nu\\in[0,1). For any $T,n\\in\\mathbb{N}$T,n\\in\\mathbb{N}, the in-path regret $\\operatorname{Regret}_{\\infty}^{\\rm path}(T)$\\operatorname{Regret}Regret_{\\infty}\\infty^{\\rm path}\\rm path(T) of the quantum version of {\\rm \\cref{algo:quantum_UCCRL}}\\rm \\cref{algo:quantum_UCCRL} is upper-bounded after $T$T steps, with probability at least $1 - \\delta$1 - \\delta, by\n    \n        \\widetilde{O}\\left(\\!{\\min}\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu} \\right\\}\\!\\left(\\!\\sqrt{T\\log\\frac{T}{\\delta}} + \\frac{|\\mathcal{S}_n| \\sqrt{A}}{1-\\nu}\\frac{\\log^2{T}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}\\log\\frac{|\\mathcal{S}_n|T}{\\delta}\\!\\right) + \\frac{T(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\right), \n    \n    where $\\widetilde{O}(\\cdot)$\\widetilde{O}(\\cdot) omits $\\poly\\log\\log$\\poly\\log\\log terms.\n    If $\\mathcal{X} = [0,1]^D$\\mathcal{X} = [0,1]^D, in which case $|\\mathcal{S}_n| = O(n^D)$|\\mathcal{S}_n| = O(n^D), and setting $n = T^{\\frac{1}{D+\\alpha}}$n = T^{\\frac{1}{D+\\alpha}}\\frac{1}{D+\\alpha}, the in-path regret is (for constant $\\nu,L$\\nu,L)\n    \n        \\widetilde{O}\\left(\\Lambda\\sqrt{T\\log\\frac{T}{\\delta}} + \\Lambda T^{\\frac{D}{D+\\alpha}}\\frac{D}{D+\\alpha}\\sqrt{A} \\log^2{T}T\\log\\frac{AT}{\\delta}\\log\\frac{T}{\\delta} \\right).\n    \n    If $\\mathcal{X} = \\mathcal{S}$\\mathcal{X} = \\mathcal{S} is finite with size $S$S, in which case $L=0$L=0, then the in-path regret is (for constant $\\nu$\\nu)\n    \n        \\widetilde{O}\\left(\\Lambda\\sqrt{T\\log\\frac{T}{\\delta}} + \\Lambda S\\sqrt{A} \\log^2{T}T\\log\\frac{SAT}{\\delta}\\log\\frac{ST}{\\delta}\\right).\n    \n\n\\begin{proof}\n    The proof is similar to \\Cref{thr:classical_RL_algorithm_continuous}, so we shall point out the main changes. Once again, let $\\widetilde{g}_k := \\frac{1}{2}\\big(\\max_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast+1}(x) - {u}_{t^\\ast}(x)\\} + \\min_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast+1}(x) - {u}_{t^\\ast}(x)\\}\\big)$ be the output of \\Cref{algo:quantum_extended_value_iteration} that uses $O(\\tau_k)$ calls to oracle $\\mathcal{O}_p$ and $\\mathcal{O}_p^\\dagger$. According to \\Cref{thr:quantum_scopt_algorithm}, with probability at least $1-\\frac{\\delta}{8T^{5/4}}$,\n    %\n    \\begin{align*}\n        \\widetilde{g}_k &\\geq g^{\\ast} - C\\frac{\\log_2{\\tau_k}}{\\tau_k} - \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\\\\n        \\text{where}~ C &= O\\left(\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu}\\right\\}\\frac{|\\mathcal{S}_n|\\sqrt{A}}{1-\\nu}\\frac{1}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}\\log\\frac{|\\mathcal{S}_n|T}{\\delta}\\right),\n    \\end{align*}\n    %\n    which implies that\n    %\n    \\begin{align*}\n        \\Delta_k \\leq \\int_{\\mathcal{X}} n_k(\\rd x) \\big(\\widetilde{g}_k -  r_{\\widetilde{d}_k}(x) \\big)  + \\tau_k \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} + C\\sum_{s\\in\\mathcal{S}_n} n_k(s)\\frac{\\log_2{\\tau_k}}{\\tau_k}.\n    \\end{align*}\n    %\n    We can follow the same steps as the proof of \\Cref{thr:classical_RL_algorithm_continuous} (approximate $\\widetilde{g}_k$ with $u_{t^\\ast+1}(x) - u_{t^\\ast}(x)$) to obtain the bound\n    %\n    \\begin{align}\n        \\Delta_k &\\leq \\int_{\\mathcal{X}} n_k(\\rd x) \\left(\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x'|x) u_{t^\\ast}(x') - u_{t^\\ast}(x) \\right) + \\tau_k \\frac{35(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} + 3C\\sum_{s\\in\\mathcal{S}_n}  n_k(s)\\frac{\\log_2{\\tau_k}}{\\tau_k}. \\label{eq:regret_infinite_horizon_eq1_classical}\n    \\end{align}\n    %\n    By replacing $u_{t^\\ast}(x)$ with $w_k(x) := u_{t^\\ast}(x) - \\frac{1}{2}(\\max_{x'\\in\\mathcal{X}}\\{u_{t^\\ast}(x')\\} + \\min_{x'\\in\\mathcal{X}}\\{u_{t^\\ast}(x')\\})$, we can once again, by the Azuma-Hoeffding inequality, bound the first term in \\Cref{eq:regret_infinite_horizon_eq1_classical} with probability at least $1-\\frac{\\delta}{8T^{5/4}}$ as (already summing over all episodes)\n    %\n    \\begin{align*}\n        \\sum_{k=1}^m \\int_{\\mathcal{X}} \\!\\! n_k(\\rd x) \\!\\left(\\int_{\\mathcal{X}} \\! p_{\\widetilde{d}_k}(\\text{d}x'|x) w_k(x') - w_k(x) \\!\\right) \\leq \\min\\!\\left\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\right\\}\\!\\left(\\!\\lceil\\log_2{T}\\rceil + \\sqrt{2T\\ln\\!\\frac{8T^{5/4}}{\\delta}}\\right).\n    \\end{align*}\n    %\n    Regarding the term $\\sum_{k=1}^m \\sum_{s\\in\\mathcal{S}_n}n_k(s)\\frac{\\log_2{\\tau_k}}{\\tau_k} \\leq 2\\lceil\\log_2{T}\\rceil \\sum_{k=1}^m \\sum_{s\\in\\mathcal{S}_n}\\frac{n_k(s)}{t_k}$ (since $\\tau_k \\geq \\frac{t_k}{2}$), we can use \\Cref{lem:regret} to obtain\n    %\n    \\begin{align*}\n        \\sum_{k=1}^m \\sum_{s\\in\\mathcal{S}}\\frac{n_k(s)}{t_k} = \\sum_{k=1}^m \\frac{\\sum_{s\\in\\mathcal{S}} n_k(s)}{\\sum_{i=1}^{k-1} \\sum_{s\\in\\mathcal{S}} n_k(s)} \\leq 4 \\log_2{T}.\n    \\end{align*}\n    %\n    This means that, with probability at least $1-\\frac{\\delta}{4T^{5/4}}$, the total in-path regret is\n    %\n    \\begin{align*}\n        \\operatorname{Regret}_{\\infty}^{\\rm path}(T) \\leq 2\\min\\left\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\right\\}\\sqrt{2T\\ln \\frac{8T^{5/4}}{\\delta}} + 24 C\\lceil\\log_2{T}\\rceil^2 + T\\frac{35(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}.\n    \\end{align*}\n    %\n    Finally, since $\\sum_{T=1}^\\infty \\frac{\\delta}{4T^{5/4}} < \\delta$, the above bound is valid for all $T\\in\\mathbb{N}$.\n\\end{proof}\\begin{proof}\n    The proof is similar to \\Cref{thr:classical_RL_algorithm_continuous}, so we shall point out the main changes. Once again, let $\\widetilde{g}_k := \\frac{1}{2}\\big(\\max_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast+1}(x) - {u}_{t^\\ast}(x)\\} + \\min_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast+1}(x) - {u}_{t^\\ast}(x)\\}\\big)$ be the output of \\Cref{algo:quantum_extended_value_iteration} that uses $O(\\tau_k)$ calls to oracle $\\mathcal{O}_p$ and $\\mathcal{O}_p^\\dagger$. According to \\Cref{thr:quantum_scopt_algorithm}, with probability at least $1-\\frac{\\delta}{8T^{5/4}}$,\n    %\n    \\begin{align*}\n        \\widetilde{g}_k &\\geq g^{\\ast} - C\\frac{\\log_2{\\tau_k}}{\\tau_k} - \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\\\\n        \\text{where}~ C &= O\\left(\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu}\\right\\}\\frac{|\\mathcal{S}_n|\\sqrt{A}}{1-\\nu}\\frac{1}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}\\log\\frac{|\\mathcal{S}_n|T}{\\delta}\\right),\n    \\end{align*}\n    %\n    which implies that\n    %\n    \\begin{align*}\n        \\Delta_k \\leq \\int_{\\mathcal{X}} n_k(\\rd x) \\big(\\widetilde{g}_k -  r_{\\widetilde{d}_k}(x) \\big)  + \\tau_k \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} + C\\sum_{s\\in\\mathcal{S}_n} n_k(s)\\frac{\\log_2{\\tau_k}}{\\tau_k}.\n    \\end{align*}\n    %\n    We can follow the same steps as the proof of \\Cref{thr:classical_RL_algorithm_continuous} (approximate $\\widetilde{g}_k$ with $u_{t^\\ast+1}(x) - u_{t^\\ast}(x)$) to obtain the bound\n    %\n    \\begin{align}\n        \\Delta_k &\\leq \\int_{\\mathcal{X}} n_k(\\rd x) \\left(\\int_{\\mathcal{X}} p_{\\widetilde{d}_k}(\\text{d}x'|x) u_{t^\\ast}(x') - u_{t^\\ast}(x) \\right) + \\tau_k \\frac{35(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} + 3C\\sum_{s\\in\\mathcal{S}_n}  n_k(s)\\frac{\\log_2{\\tau_k}}{\\tau_k}. \\label{eq:regret_infinite_horizon_eq1_classical}\n    \\end{align}\n    %\n    By replacing $u_{t^\\ast}(x)$ with $w_k(x) := u_{t^\\ast}(x) - \\frac{1}{2}(\\max_{x'\\in\\mathcal{X}}\\{u_{t^\\ast}(x')\\} + \\min_{x'\\in\\mathcal{X}}\\{u_{t^\\ast}(x')\\})$, we can once again, by the Azuma-Hoeffding inequality, bound the first term in \\Cref{eq:regret_infinite_horizon_eq1_classical} with probability at least $1-\\frac{\\delta}{8T^{5/4}}$ as (already summing over all episodes)\n    %\n    \\begin{align*}\n        \\sum_{k=1}^m \\int_{\\mathcal{X}} \\!\\! n_k(\\rd x) \\!\\left(\\int_{\\mathcal{X}} \\! p_{\\widetilde{d}_k}(\\text{d}x'|x) w_k(x') - w_k(x) \\!\\right) \\leq \\min\\!\\left\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\right\\}\\!\\left(\\!\\lceil\\log_2{T}\\rceil + \\sqrt{2T\\ln\\!\\frac{8T^{5/4}}{\\delta}}\\right).\n    \\end{align*}\n    %\n    Regarding the term $\\sum_{k=1}^m \\sum_{s\\in\\mathcal{S}_n}n_k(s)\\frac{\\log_2{\\tau_k}}{\\tau_k} \\leq 2\\lceil\\log_2{T}\\rceil \\sum_{k=1}^m \\sum_{s\\in\\mathcal{S}_n}\\frac{n_k(s)}{t_k}$ (since $\\tau_k \\geq \\frac{t_k}{2}$), we can use \\Cref{lem:regret} to obtain\n    %\n    \\begin{align*}\n        \\sum_{k=1}^m \\sum_{s\\in\\mathcal{S}}\\frac{n_k(s)}{t_k} = \\sum_{k=1}^m \\frac{\\sum_{s\\in\\mathcal{S}} n_k(s)}{\\sum_{i=1}^{k-1} \\sum_{s\\in\\mathcal{S}} n_k(s)} \\leq 4 \\log_2{T}.\n    \\end{align*}\n    %\n    This means that, with probability at least $1-\\frac{\\delta}{4T^{5/4}}$, the total in-path regret is\n    %\n    \\begin{align*}\n        \\operatorname{Regret}_{\\infty}^{\\rm path}(T) \\leq 2\\min\\left\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\right\\}\\sqrt{2T\\ln \\frac{8T^{5/4}}{\\delta}} + 24 C\\lceil\\log_2{T}\\rceil^2 + T\\frac{35(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}.\n    \\end{align*}\n    %\n    Finally, since $\\sum_{T=1}^\\infty \\frac{\\delta}{4T^{5/4}} < \\delta$, the above bound is valid for all $T\\in\\mathbb{N}$.\n\\end{proof}\n    The proof is similar to \\Cref{thr:classical_RL_algorithm_continuous}, so we shall point out the main changes. Once again, let $\\widetilde{g}_k := \\frac{1}{2}\\big(\\max_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast+1}(x) - {u}_{t^\\ast}(x)\\} + \\min_{x\\in\\mathcal{X}}\\{{u}_{t^\\ast+1}(x) - {u}_{t^\\ast}(x)\\}\\big)$\\widetilde{g}_k := \\frac{1}{2}\\big(\\max_{x\\in\\mathcal{X}}x\\in\\mathcal{X}\\{{u}u_{t^\\ast+1}t^\\ast+1(x) - {u}u_{t^\\ast}t^\\ast(x)\\} + \\min_{x\\in\\mathcal{X}}x\\in\\mathcal{X}\\{{u}u_{t^\\ast+1}t^\\ast+1(x) - {u}u_{t^\\ast}t^\\ast(x)\\}\\big) be the output of \\Cref{algo:quantum_extended_value_iteration} that uses $O(\\tau_k)$O(\\tau_k) calls to oracle $\\mathcal{O}_p$\\mathcal{O}_p and $\\mathcal{O}_p^\\dagger$\\mathcal{O}_p^\\dagger. According to \\Cref{thr:quantum_scopt_algorithm}, with probability at least $1-\\frac{\\delta}{8T^{5/4}}$1-\\frac{\\delta}{8T^{5/4}},\n    \n        \\widetilde{g}_k &\\geq g^{\\ast}\\ast - C\\frac{\\log_2{\\tau_k}}{\\tau_k} - \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\\\\n        \\text{where}~ C &= O\\left(\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu}\\right\\}\\frac{|\\mathcal{S}_n|\\sqrt{A}}{1-\\nu}\\frac{1}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}\\log\\frac{|\\mathcal{S}_n|T}{\\delta}\\right),\n    \n    which implies that\n    \n        \\Delta_k \\leq \\int_{\\mathcal{X}}\\mathcal{X} n_k(\\rd x) \\big(\\widetilde{g}_k -  r_{\\widetilde{d}_k}\\widetilde{d}_k(x) \\big)  + \\tau_k \\frac{13(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} + C\\sum_{s\\in\\mathcal{S}_n}s\\in\\mathcal{S}_n n_k(s)\\frac{\\log_2{\\tau_k}}{\\tau_k}.\n    \n    We can follow the same steps as the proof of \\Cref{thr:classical_RL_algorithm_continuous} (approximate $\\widetilde{g}_k$\\widetilde{g}_k with $u_{t^\\ast+1}(x) - u_{t^\\ast}(x)$u_{t^\\ast+1}t^\\ast+1(x) - u_{t^\\ast}t^\\ast(x)) to obtain the bound\n    \n        \\Delta_k &\\leq \\int_{\\mathcal{X}}\\mathcal{X} n_k(\\rd x) \\left(\\int_{\\mathcal{X}}\\mathcal{X} p_{\\widetilde{d}_k}\\widetilde{d}_k(\\text{d}x'|x) u_{t^\\ast}t^\\ast(x') - u_{t^\\ast}t^\\ast(x) \\right) + \\tau_k \\frac{35(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} + 3C\\sum_{s\\in\\mathcal{S}_n}s\\in\\mathcal{S}_n  n_k(s)\\frac{\\log_2{\\tau_k}}{\\tau_k}. \\label{eq:regret_infinite_horizon_eq1_classical}\n    \n    By replacing $u_{t^\\ast}(x)$u_{t^\\ast}t^\\ast(x) with $w_k(x) := u_{t^\\ast}(x) - \\frac{1}{2}(\\max_{x'\\in\\mathcal{X}}\\{u_{t^\\ast}(x')\\} + \\min_{x'\\in\\mathcal{X}}\\{u_{t^\\ast}(x')\\})$w_k(x) := u_{t^\\ast}t^\\ast(x) - \\frac{1}{2}(\\max_{x'\\in\\mathcal{X}}x'\\in\\mathcal{X}\\{u_{t^\\ast}t^\\ast(x')\\} + \\min_{x'\\in\\mathcal{X}}x'\\in\\mathcal{X}\\{u_{t^\\ast}t^\\ast(x')\\}), we can once again, by the Azuma-Hoeffding inequality, bound the first term in \\Cref{eq:regret_infinite_horizon_eq1_classical} with probability at least $1-\\frac{\\delta}{8T^{5/4}}$1-\\frac{\\delta}{8T^{5/4}} as (already summing over all episodes)\n    \n        \\sum_{k=1}k=1^m \\int_{\\mathcal{X}}\\mathcal{X} \\!\\! n_k(\\rd x) \\!\\left(\\int_{\\mathcal{X}}\\mathcal{X} \\! p_{\\widetilde{d}_k}\\widetilde{d}_k(\\text{d}x'|x) w_k(x') - w_k(x) \\!\\right) \\leq \\min\\!\\left\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\right\\}\\!\\left(\\!\\lceil\\log_2{T}T\\rceil + \\sqrt{2T\\ln\\!\\frac{8T^{5/4}}{\\delta}}\\right).\n    \n    Regarding the term $\\sum_{k=1}^m \\sum_{s\\in\\mathcal{S}_n}n_k(s)\\frac{\\log_2{\\tau_k}}{\\tau_k} \\leq 2\\lceil\\log_2{T}\\rceil \\sum_{k=1}^m \\sum_{s\\in\\mathcal{S}_n}\\frac{n_k(s)}{t_k}$\\sum_{k=1}k=1^m \\sum_{s\\in\\mathcal{S}_n}s\\in\\mathcal{S}_nn_k(s)\\frac{\\log_2{\\tau_k}}{\\tau_k} \\leq 2\\lceil\\log_2{T}T\\rceil \\sum_{k=1}k=1^m \\sum_{s\\in\\mathcal{S}_n}s\\in\\mathcal{S}_n\\frac{n_k(s)}{t_k} (since $\\tau_k \\geq \\frac{t_k}{2}$\\tau_k \\geq \\frac{t_k}{2}), we can use \\Cref{lem:regret} to obtain\n    \n        \\sum_{k=1}k=1^m \\sum_{s\\in\\mathcal{S}}s\\in\\mathcal{S}\\frac{n_k(s)}{t_k} = \\sum_{k=1}k=1^m \\frac{\\sum_{s\\in\\mathcal{S}} n_k(s)}{\\sum_{i=1}^{k-1} \\sum_{s\\in\\mathcal{S}} n_k(s)} \\leq 4 \\log_2{T}T.\n    \n    This means that, with probability at least $1-\\frac{\\delta}{4T^{5/4}}$1-\\frac{\\delta}{4T^{5/4}}, the total in-path regret is\n    \n        \\operatorname{Regret}Regret_{\\infty}\\infty^{\\rm path}\\rm path(T) \\leq 2\\min\\left\\{4\\Lambda + 4, \\frac{2\\Lambda}{1-\\nu}\\right\\}\\sqrt{2T\\ln \\frac{8T^{5/4}}{\\delta}} + 24 C\\lceil\\log_2{T}T\\rceil^2 + T\\frac{35(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}.\n    \n    Finally, since $\\sum_{T=1}^\\infty \\frac{\\delta}{4T^{5/4}} < \\delta$\\sum_{T=1}T=1^\\infty \\frac{\\delta}{4T^{5/4}} < \\delta, the above bound is valid for all $T\\in\\mathbb{N}$T\\in\\mathbb{N}.\n\n\n\n\\subsection{Expected regret}\n\nWe now turn our attention to the expected regret $\\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T)$\\operatorname{Regret}Regret_{\\infty}\\infty^{\\mathbb{E}}\\mathbb{E}(T) of \\Cref{algo:quantum_UCCRL} instead of its in-path regret. Fortunately, the analysis is much simpler in this case, similarly to the finite-horizon case in \\Cref{thr:quantum_finite_horizon_regret,thr:classical_finite-horizon_regret}.\n\n\\begin{theorem}[Classical infinite-horizon expected regret bound]\\label{thr:classical_RL_algorithm_expected_regret}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ be an infinite-horizon weakly communicating MDP with $\\operatorname{sp}({h}^\\ast) \\leq \\Lambda$. Let $\\mathcal{S}_n$ be an $\\frac{1}{n}$-net for $\\mathcal{X}$. Assume that $p$ and $r$ satisfy {\\rm \\Cref{ass:Lipschitz}} with parameters $L,\\alpha\\geq 0$. Assume the optimal Bellman operator $\\mathcal{L}$ of $M$ is an $1$-stage $\\nu$-span contraction for $\\nu\\in[0,1)$. For any $T,n\\in\\mathbb{N}$, the expected regret $\\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T)$ of the classical version of {\\rm \\cref{algo:quantum_UCCRL}} is upper-bounded after $T$ steps, with probability at least $1 - \\delta$, by\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\min\\left\\{\\Lambda+1,\\frac{\\Lambda}{1-\\nu}\\right\\} \\sqrt{\\frac{|\\mathcal{S}_n| A}{(1-\\nu)^2}\\frac{T\\log{T}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}} + \\frac{T(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\right),\n    \\end{align*}\n    %\n    where $\\widetilde{O}(\\cdot)$ omits $\\poly\\log\\log$ terms.\n    If $\\mathcal{X} = [0,1]^D$, in which case $|\\mathcal{S}_n| = O(n^D)$, and setting $n = T^{\\frac{1}{D+2\\alpha}}\\!$, the expected regret is (for constant $\\nu,L$) \n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\Lambda T^{\\frac{D+\\alpha}{D+2\\alpha}}\\sqrt{A\\log{T}\\log\\frac{AT}{\\delta}}  \\right).\n    \\end{align*}\n    %\n    If $\\mathcal{X} = \\mathcal{S}$ is finite with size $S$, in which case $L=0$, then the expected regret is (for constant $\\nu$) \n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\Lambda\\sqrt{SAT\\log{T}\\log\\frac{SAT}{\\delta}}\\right).\n    \\end{align*}\n\\end{theorem}\\begin{theorem}[Classical infinite-horizon expected regret bound]\\label{thr:classical_RL_algorithm_expected_regret}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ be an infinite-horizon weakly communicating MDP with $\\operatorname{sp}({h}^\\ast) \\leq \\Lambda$. Let $\\mathcal{S}_n$ be an $\\frac{1}{n}$-net for $\\mathcal{X}$. Assume that $p$ and $r$ satisfy {\\rm \\Cref{ass:Lipschitz}} with parameters $L,\\alpha\\geq 0$. Assume the optimal Bellman operator $\\mathcal{L}$ of $M$ is an $1$-stage $\\nu$-span contraction for $\\nu\\in[0,1)$. For any $T,n\\in\\mathbb{N}$, the expected regret $\\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T)$ of the classical version of {\\rm \\cref{algo:quantum_UCCRL}} is upper-bounded after $T$ steps, with probability at least $1 - \\delta$, by\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\min\\left\\{\\Lambda+1,\\frac{\\Lambda}{1-\\nu}\\right\\} \\sqrt{\\frac{|\\mathcal{S}_n| A}{(1-\\nu)^2}\\frac{T\\log{T}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}} + \\frac{T(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\right),\n    \\end{align*}\n    %\n    where $\\widetilde{O}(\\cdot)$ omits $\\poly\\log\\log$ terms.\n    If $\\mathcal{X} = [0,1]^D$, in which case $|\\mathcal{S}_n| = O(n^D)$, and setting $n = T^{\\frac{1}{D+2\\alpha}}\\!$, the expected regret is (for constant $\\nu,L$) \n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\Lambda T^{\\frac{D+\\alpha}{D+2\\alpha}}\\sqrt{A\\log{T}\\log\\frac{AT}{\\delta}}  \\right).\n    \\end{align*}\n    %\n    If $\\mathcal{X} = \\mathcal{S}$ is finite with size $S$, in which case $L=0$, then the expected regret is (for constant $\\nu$) \n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\Lambda\\sqrt{SAT\\log{T}\\log\\frac{SAT}{\\delta}}\\right).\n    \\end{align*}\n\\end{theorem}\\label{thr:classical_RL_algorithm_expected_regret}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle be an infinite-horizon weakly communicating MDP with $\\operatorname{sp}({h}^\\ast) \\leq \\Lambda$\\operatorname{sp}sp({h}h^\\ast) \\leq \\Lambda. Let $\\mathcal{S}_n$\\mathcal{S}_n be an $\\frac{1}{n}$\\frac{1}{n}-net for $\\mathcal{X}$\\mathcal{X}. Assume that $p$p and $r$r satisfy {\\rm \\Cref{ass:Lipschitz}}\\rm \\Cref{ass:Lipschitz} with parameters $L,\\alpha\\geq 0$L,\\alpha\\geq 0. Assume the optimal Bellman operator $\\mathcal{L}$\\mathcal{L} of $M$M is an $1$1-stage $\\nu$\\nu-span contraction for $\\nu\\in[0,1)$\\nu\\in[0,1). For any $T,n\\in\\mathbb{N}$T,n\\in\\mathbb{N}, the expected regret $\\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T)$\\operatorname{Regret}Regret_{\\infty}\\infty^{\\mathbb{E}}\\mathbb{E}(T) of the classical version of {\\rm \\cref{algo:quantum_UCCRL}}\\rm \\cref{algo:quantum_UCCRL} is upper-bounded after $T$T steps, with probability at least $1 - \\delta$1 - \\delta, by\n    \n        \\widetilde{O}\\left(\\min\\left\\{\\Lambda+1,\\frac{\\Lambda}{1-\\nu}\\right\\} \\sqrt{\\frac{|\\mathcal{S}_n| A}{(1-\\nu)^2}\\frac{T\\log{T}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}} + \\frac{T(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\right),\n    \n    where $\\widetilde{O}(\\cdot)$\\widetilde{O}(\\cdot) omits $\\poly\\log\\log$\\poly\\log\\log terms.\n    If $\\mathcal{X} = [0,1]^D$\\mathcal{X} = [0,1]^D, in which case $|\\mathcal{S}_n| = O(n^D)$|\\mathcal{S}_n| = O(n^D), and setting $n = T^{\\frac{1}{D+2\\alpha}}\\!$n = T^{\\frac{1}{D+2\\alpha}}\\frac{1}{D+2\\alpha}\\!, the expected regret is (for constant $\\nu,L$\\nu,L) \n    \n        \\widetilde{O}\\left(\\Lambda T^{\\frac{D+\\alpha}{D+2\\alpha}}\\frac{D+\\alpha}{D+2\\alpha}\\sqrt{A\\log{T}\\log\\frac{AT}{\\delta}}  \\right).\n    \n    If $\\mathcal{X} = \\mathcal{S}$\\mathcal{X} = \\mathcal{S} is finite with size $S$S, in which case $L=0$L=0, then the expected regret is (for constant $\\nu$\\nu) \n    \n        \\widetilde{O}\\left(\\Lambda\\sqrt{SAT\\log{T}\\log\\frac{SAT}{\\delta}}\\right).\n    \n\n\\begin{proof}\n    The total expected regret after $T$ time steps spread over $m \\leq \\lceil\\log_2 T\\rceil$ episodes is\n    %\n    \\begin{align*}\n        \\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T) = \\sum_{k=1}^m \\tau_k\\left(g^\\ast - \\min_{x\\in\\mathcal{X}} g^{\\widetilde{d}_k^\\infty}(x) \\right),\n    \\end{align*}\n    %\n    where $\\widetilde{d}_k\\in\\mathcal{D}^{\\rm D}$ is the decision rule employed in episode $k$ and $\\tau_k$ is the length of episode $k$. According to \\Cref{thr:classical_scopt_algorithm}, the output policy $\\widetilde{d}_k^\\infty\\in\\Pi^{\\rm D}$ of \\Cref{algo:classical_extended_value_iteration} using $O(\\tau_k)$ calls to oracle $\\mathcal{C}_p$ is such that, with probability at least $1 - \\frac{\\delta}{4T^{5/4}}$, for all $x\\in\\mathcal{X}$,\n    %\n    \\begin{align*}\n        g^{\\widetilde{d}_k^\\infty}(x) &\\geq g^\\ast - C\\sqrt{\\frac{\\log_2\\tau_k}{\\tau_k}} - \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu},\\\\\n        \\text{where}~ C &= O\\left(\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu}\\right\\}\\sqrt{\\frac{|\\mathcal{S}_n|A}{(1-\\nu)^2}\\frac{1}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}}\\right),\n    \\end{align*}\n    %\n    which implies that\n    %\n    \\begin{align*}\n        \\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T) \\!\\leq\\! \\sum_{k=1}^m \\!\\left(\\! C\\sqrt{\\tau_k\\log_2\\tau_k} + \\tau_k \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\right) \\!\\leq\\! \\frac{\\sqrt{2}C}{\\sqrt{2}-1}\\sqrt{T\\lceil\\log_2 T\\rceil} + T\\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu},\n    \\end{align*}\n    %\n    using \\Cref{fact:useful_inequality} in order to bound $\\sum_{k=1}^m \\sqrt{\\tau_k\\log_2\\tau_k} \\leq \\sqrt{2\\lceil\\log_2 T\\rceil}\\sum_{k=1}^m \\frac{\\tau_k}{\\sqrt{t_k}} \\leq \\frac{\\sqrt{2}}{\\sqrt{2}-1}\\sqrt{T\\lceil\\log_2 T\\rceil}$.\n    \n    Finally, since $\\sum_{T=1}^\\infty \\frac{\\delta}{4T^{5/4}} < \\delta$, the above bound is valid for all $T\\in\\mathbb{N}$.\n\\end{proof}\\begin{proof}\n    The total expected regret after $T$ time steps spread over $m \\leq \\lceil\\log_2 T\\rceil$ episodes is\n    %\n    \\begin{align*}\n        \\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T) = \\sum_{k=1}^m \\tau_k\\left(g^\\ast - \\min_{x\\in\\mathcal{X}} g^{\\widetilde{d}_k^\\infty}(x) \\right),\n    \\end{align*}\n    %\n    where $\\widetilde{d}_k\\in\\mathcal{D}^{\\rm D}$ is the decision rule employed in episode $k$ and $\\tau_k$ is the length of episode $k$. According to \\Cref{thr:classical_scopt_algorithm}, the output policy $\\widetilde{d}_k^\\infty\\in\\Pi^{\\rm D}$ of \\Cref{algo:classical_extended_value_iteration} using $O(\\tau_k)$ calls to oracle $\\mathcal{C}_p$ is such that, with probability at least $1 - \\frac{\\delta}{4T^{5/4}}$, for all $x\\in\\mathcal{X}$,\n    %\n    \\begin{align*}\n        g^{\\widetilde{d}_k^\\infty}(x) &\\geq g^\\ast - C\\sqrt{\\frac{\\log_2\\tau_k}{\\tau_k}} - \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu},\\\\\n        \\text{where}~ C &= O\\left(\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu}\\right\\}\\sqrt{\\frac{|\\mathcal{S}_n|A}{(1-\\nu)^2}\\frac{1}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}}\\right),\n    \\end{align*}\n    %\n    which implies that\n    %\n    \\begin{align*}\n        \\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T) \\!\\leq\\! \\sum_{k=1}^m \\!\\left(\\! C\\sqrt{\\tau_k\\log_2\\tau_k} + \\tau_k \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\right) \\!\\leq\\! \\frac{\\sqrt{2}C}{\\sqrt{2}-1}\\sqrt{T\\lceil\\log_2 T\\rceil} + T\\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu},\n    \\end{align*}\n    %\n    using \\Cref{fact:useful_inequality} in order to bound $\\sum_{k=1}^m \\sqrt{\\tau_k\\log_2\\tau_k} \\leq \\sqrt{2\\lceil\\log_2 T\\rceil}\\sum_{k=1}^m \\frac{\\tau_k}{\\sqrt{t_k}} \\leq \\frac{\\sqrt{2}}{\\sqrt{2}-1}\\sqrt{T\\lceil\\log_2 T\\rceil}$.\n    \n    Finally, since $\\sum_{T=1}^\\infty \\frac{\\delta}{4T^{5/4}} < \\delta$, the above bound is valid for all $T\\in\\mathbb{N}$.\n\\end{proof}\n    The total expected regret after $T$T time steps spread over $m \\leq \\lceil\\log_2 T\\rceil$m \\leq \\lceil\\log_2 T\\rceil episodes is\n    \n        \\operatorname{Regret}Regret_{\\infty}\\infty^{\\mathbb{E}}\\mathbb{E}(T) = \\sum_{k=1}k=1^m \\tau_k\\left(g^\\ast - \\min_{x\\in\\mathcal{X}}x\\in\\mathcal{X} g^{\\widetilde{d}_k^\\infty}\\widetilde{d}_k^\\infty(x) \\right),\n    \n    where $\\widetilde{d}_k\\in\\mathcal{D}^{\\rm D}$\\widetilde{d}_k\\in\\mathcal{D}^{\\rm D}\\rm D is the decision rule employed in episode $k$k and $\\tau_k$\\tau_k is the length of episode $k$k. According to \\Cref{thr:classical_scopt_algorithm}, the output policy $\\widetilde{d}_k^\\infty\\in\\Pi^{\\rm D}$\\widetilde{d}_k^\\infty\\in\\Pi^{\\rm D}\\rm D of \\Cref{algo:classical_extended_value_iteration} using $O(\\tau_k)$O(\\tau_k) calls to oracle $\\mathcal{C}_p$\\mathcal{C}_p is such that, with probability at least $1 - \\frac{\\delta}{4T^{5/4}}$1 - \\frac{\\delta}{4T^{5/4}}, for all $x\\in\\mathcal{X}$x\\in\\mathcal{X},\n    \n        g^{\\widetilde{d}_k^\\infty}\\widetilde{d}_k^\\infty(x) &\\geq g^\\ast - C\\sqrt{\\frac{\\log_2\\tau_k}{\\tau_k}} - \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu},\\\\\n        \\text{where}~ C &= O\\left(\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu}\\right\\}\\sqrt{\\frac{|\\mathcal{S}_n|A}{(1-\\nu)^2}\\frac{1}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}}\\right),\n    \n    which implies that\n    \n        \\operatorname{Regret}Regret_{\\infty}\\infty^{\\mathbb{E}}\\mathbb{E}(T) \\!\\leq\\! \\sum_{k=1}k=1^m \\!\\left(\\! C\\sqrt{\\tau_k\\log_2\\tau_k} + \\tau_k \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\right) \\!\\leq\\! \\frac{\\sqrt{2}C}{\\sqrt{2}-1}\\sqrt{T\\lceil\\log_2 T\\rceil} + T\\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu},\n    \n    using \\Cref{fact:useful_inequality} in order to bound $\\sum_{k=1}^m \\sqrt{\\tau_k\\log_2\\tau_k} \\leq \\sqrt{2\\lceil\\log_2 T\\rceil}\\sum_{k=1}^m \\frac{\\tau_k}{\\sqrt{t_k}} \\leq \\frac{\\sqrt{2}}{\\sqrt{2}-1}\\sqrt{T\\lceil\\log_2 T\\rceil}$\\sum_{k=1}k=1^m \\sqrt{\\tau_k\\log_2\\tau_k} \\leq \\sqrt{2\\lceil\\log_2 T\\rceil}\\sum_{k=1}k=1^m \\frac{\\tau_k}{\\sqrt{t_k}} \\leq \\frac{\\sqrt{2}}{\\sqrt{2}-1}\\sqrt{T\\lceil\\log_2 T\\rceil}.\n    \n    Finally, since $\\sum_{T=1}^\\infty \\frac{\\delta}{4T^{5/4}} < \\delta$\\sum_{T=1}T=1^\\infty \\frac{\\delta}{4T^{5/4}} < \\delta, the above bound is valid for all $T\\in\\mathbb{N}$T\\in\\mathbb{N}.\n\n\n\\begin{theorem}[Quantum infinite-horizon expected regret bound]\\label{thr:quantum_RL_algorithm_expected_regret}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ be an infinite-horizon weakly communicating MDP with $\\operatorname{sp}({h}^\\ast) \\leq \\Lambda$. Let $\\mathcal{S}_n$ be an $\\frac{1}{n}$-net for $\\mathcal{X}$. Assume that $p$ and $r$ satisfy {\\rm \\Cref{ass:Lipschitz}} with parameters $L,\\alpha\\geq 0$. Assume the optimal Bellman operator $\\mathcal{L}$ of $M$ is an $1$-stage $\\nu$-span contraction for $\\nu\\in[0,1)$. For any $T,n\\in\\mathbb{N}$, the expected regret $\\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T)$ of the quantum version of {\\rm \\cref{algo:quantum_UCCRL}} is upper-bounded after $T$ steps, with probability at least $1 - \\delta$, by\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu} \\right\\}\\frac{|\\mathcal{S}_n| \\sqrt{A}}{1-\\nu}\\frac{\\log^2{T}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}\\log\\frac{|\\mathcal{S}_n|T}{\\delta} + \\frac{T(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\right), \n    \\end{align*}\n    %\n    where $\\widetilde{O}(\\cdot)$ omits $\\poly\\log\\log$ terms.\n    If $\\mathcal{X} = [0,1]^D$, in which case $|\\mathcal{S}_n| = O(n^D)$, and setting $n = T^{\\frac{1}{D+\\alpha}}$, the expected regret is (for constant $\\nu,L$)\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\Lambda T^{\\frac{D}{D+\\alpha}}\\sqrt{A} \\log^2{T}\\log\\frac{AT}{\\delta}\\log\\frac{T}{\\delta} \\right).\n    \\end{align*}\n    %\n    If $\\mathcal{X} = \\mathcal{S}$ is finite with size $S$, in which case $L=0$, then the expected regret is (for constant $\\nu$)\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\Lambda S\\sqrt{A} \\log^2{T}\\log\\frac{SAT}{\\delta}\\log\\frac{ST}{\\delta}\\right).\n    \\end{align*}\n\\end{theorem}\\begin{theorem}[Quantum infinite-horizon expected regret bound]\\label{thr:quantum_RL_algorithm_expected_regret}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$ be an infinite-horizon weakly communicating MDP with $\\operatorname{sp}({h}^\\ast) \\leq \\Lambda$. Let $\\mathcal{S}_n$ be an $\\frac{1}{n}$-net for $\\mathcal{X}$. Assume that $p$ and $r$ satisfy {\\rm \\Cref{ass:Lipschitz}} with parameters $L,\\alpha\\geq 0$. Assume the optimal Bellman operator $\\mathcal{L}$ of $M$ is an $1$-stage $\\nu$-span contraction for $\\nu\\in[0,1)$. For any $T,n\\in\\mathbb{N}$, the expected regret $\\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T)$ of the quantum version of {\\rm \\cref{algo:quantum_UCCRL}} is upper-bounded after $T$ steps, with probability at least $1 - \\delta$, by\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu} \\right\\}\\frac{|\\mathcal{S}_n| \\sqrt{A}}{1-\\nu}\\frac{\\log^2{T}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}\\log\\frac{|\\mathcal{S}_n|T}{\\delta} + \\frac{T(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\right), \n    \\end{align*}\n    %\n    where $\\widetilde{O}(\\cdot)$ omits $\\poly\\log\\log$ terms.\n    If $\\mathcal{X} = [0,1]^D$, in which case $|\\mathcal{S}_n| = O(n^D)$, and setting $n = T^{\\frac{1}{D+\\alpha}}$, the expected regret is (for constant $\\nu,L$)\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\Lambda T^{\\frac{D}{D+\\alpha}}\\sqrt{A} \\log^2{T}\\log\\frac{AT}{\\delta}\\log\\frac{T}{\\delta} \\right).\n    \\end{align*}\n    %\n    If $\\mathcal{X} = \\mathcal{S}$ is finite with size $S$, in which case $L=0$, then the expected regret is (for constant $\\nu$)\n    %\n    \\begin{align*}\n        \\widetilde{O}\\left(\\Lambda S\\sqrt{A} \\log^2{T}\\log\\frac{SAT}{\\delta}\\log\\frac{ST}{\\delta}\\right).\n    \\end{align*}\n\\end{theorem}\\label{thr:quantum_RL_algorithm_expected_regret}\n    Let $M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle$M = \\langle \\mathcal{X},\\mathcal{A},p,r\\rangle be an infinite-horizon weakly communicating MDP with $\\operatorname{sp}({h}^\\ast) \\leq \\Lambda$\\operatorname{sp}sp({h}h^\\ast) \\leq \\Lambda. Let $\\mathcal{S}_n$\\mathcal{S}_n be an $\\frac{1}{n}$\\frac{1}{n}-net for $\\mathcal{X}$\\mathcal{X}. Assume that $p$p and $r$r satisfy {\\rm \\Cref{ass:Lipschitz}}\\rm \\Cref{ass:Lipschitz} with parameters $L,\\alpha\\geq 0$L,\\alpha\\geq 0. Assume the optimal Bellman operator $\\mathcal{L}$\\mathcal{L} of $M$M is an $1$1-stage $\\nu$\\nu-span contraction for $\\nu\\in[0,1)$\\nu\\in[0,1). For any $T,n\\in\\mathbb{N}$T,n\\in\\mathbb{N}, the expected regret $\\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T)$\\operatorname{Regret}Regret_{\\infty}\\infty^{\\mathbb{E}}\\mathbb{E}(T) of the quantum version of {\\rm \\cref{algo:quantum_UCCRL}}\\rm \\cref{algo:quantum_UCCRL} is upper-bounded after $T$T steps, with probability at least $1 - \\delta$1 - \\delta, by\n    \n        \\widetilde{O}\\left(\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu} \\right\\}\\frac{|\\mathcal{S}_n| \\sqrt{A}}{1-\\nu}\\frac{\\log^2{T}}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}\\log\\frac{|\\mathcal{S}_n|T}{\\delta} + \\frac{T(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu} \\right), \n    \n    where $\\widetilde{O}(\\cdot)$\\widetilde{O}(\\cdot) omits $\\poly\\log\\log$\\poly\\log\\log terms.\n    If $\\mathcal{X} = [0,1]^D$\\mathcal{X} = [0,1]^D, in which case $|\\mathcal{S}_n| = O(n^D)$|\\mathcal{S}_n| = O(n^D), and setting $n = T^{\\frac{1}{D+\\alpha}}$n = T^{\\frac{1}{D+\\alpha}}\\frac{1}{D+\\alpha}, the expected regret is (for constant $\\nu,L$\\nu,L)\n    \n        \\widetilde{O}\\left(\\Lambda T^{\\frac{D}{D+\\alpha}}\\frac{D}{D+\\alpha}\\sqrt{A} \\log^2{T}T\\log\\frac{AT}{\\delta}\\log\\frac{T}{\\delta} \\right).\n    \n    If $\\mathcal{X} = \\mathcal{S}$\\mathcal{X} = \\mathcal{S} is finite with size $S$S, in which case $L=0$L=0, then the expected regret is (for constant $\\nu$\\nu)\n    \n        \\widetilde{O}\\left(\\Lambda S\\sqrt{A} \\log^2{T}T\\log\\frac{SAT}{\\delta}\\log\\frac{ST}{\\delta}\\right).\n    \n\n\\begin{proof}\n    The proof is basically the same as \\Cref{thr:classical_RL_algorithm_expected_regret}, the main difference being that now we employ \\Cref{algo:quantum_extended_value_iteration} with $O(\\tau_k)$ calls to oracle $\\mathcal{O}_p$ in order to obtain a stationary policy $\\widetilde{d}_k^\\infty\\in\\Pi^{\\rm D}$ such that, according to \\Cref{thr:quantum_scopt_algorithm}, with probability at least $1 - \\frac{\\delta}{4T^{5/4}}$, for all $x\\in\\mathcal{X}$,\n    %\n    \\begin{align*}\n        g^{\\widetilde{d}_k^\\infty}(x) &\\geq g^\\ast - C\\frac{\\log_2\\tau_k}{\\tau_k} - \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu},\\\\\n        \\text{where}~ C &= O\\left(\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu}\\right\\}\\frac{|\\mathcal{S}_n|\\sqrt{A}}{1-\\nu}\\frac{1}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}\\log\\frac{|\\mathcal{S}_n|T}{\\delta}\\right),\n    \\end{align*}\n    %\n    which implies that\n    %\n    \\begin{align*}\n        \\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T) \\leq \\sum_{k=1}^m \\left( C\\log_2\\tau_k + \\tau_k \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\right) \\leq C\\lceil\\log_2 T\\rceil^2 + T\\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}.\n    \\end{align*}\n    %\n    Since $\\sum_{T=1}^\\infty \\frac{\\delta}{4T^{5/4}} < \\delta$, the above bound is valid for all $T\\in\\mathbb{N}$.\n\\end{proof}\\begin{proof}\n    The proof is basically the same as \\Cref{thr:classical_RL_algorithm_expected_regret}, the main difference being that now we employ \\Cref{algo:quantum_extended_value_iteration} with $O(\\tau_k)$ calls to oracle $\\mathcal{O}_p$ in order to obtain a stationary policy $\\widetilde{d}_k^\\infty\\in\\Pi^{\\rm D}$ such that, according to \\Cref{thr:quantum_scopt_algorithm}, with probability at least $1 - \\frac{\\delta}{4T^{5/4}}$, for all $x\\in\\mathcal{X}$,\n    %\n    \\begin{align*}\n        g^{\\widetilde{d}_k^\\infty}(x) &\\geq g^\\ast - C\\frac{\\log_2\\tau_k}{\\tau_k} - \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu},\\\\\n        \\text{where}~ C &= O\\left(\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu}\\right\\}\\frac{|\\mathcal{S}_n|\\sqrt{A}}{1-\\nu}\\frac{1}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}\\log\\frac{|\\mathcal{S}_n|T}{\\delta}\\right),\n    \\end{align*}\n    %\n    which implies that\n    %\n    \\begin{align*}\n        \\operatorname{Regret}_{\\infty}^{\\mathbb{E}}(T) \\leq \\sum_{k=1}^m \\left( C\\log_2\\tau_k + \\tau_k \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\right) \\leq C\\lceil\\log_2 T\\rceil^2 + T\\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}.\n    \\end{align*}\n    %\n    Since $\\sum_{T=1}^\\infty \\frac{\\delta}{4T^{5/4}} < \\delta$, the above bound is valid for all $T\\in\\mathbb{N}$.\n\\end{proof}\n    The proof is basically the same as \\Cref{thr:classical_RL_algorithm_expected_regret}, the main difference being that now we employ \\Cref{algo:quantum_extended_value_iteration} with $O(\\tau_k)$O(\\tau_k) calls to oracle $\\mathcal{O}_p$\\mathcal{O}_p in order to obtain a stationary policy $\\widetilde{d}_k^\\infty\\in\\Pi^{\\rm D}$\\widetilde{d}_k^\\infty\\in\\Pi^{\\rm D}\\rm D such that, according to \\Cref{thr:quantum_scopt_algorithm}, with probability at least $1 - \\frac{\\delta}{4T^{5/4}}$1 - \\frac{\\delta}{4T^{5/4}}, for all $x\\in\\mathcal{X}$x\\in\\mathcal{X},\n    \n        g^{\\widetilde{d}_k^\\infty}\\widetilde{d}_k^\\infty(x) &\\geq g^\\ast - C\\frac{\\log_2\\tau_k}{\\tau_k} - \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu},\\\\\n        \\text{where}~ C &= O\\left(\\min\\left\\{1+\\Lambda, \\frac{\\Lambda}{1-\\nu}\\right\\}\\frac{|\\mathcal{S}_n|\\sqrt{A}}{1-\\nu}\\frac{1}{\\log\\frac{1}{\\nu}}\\log\\frac{|\\mathcal{S}_n|AT}{\\delta}\\log\\frac{|\\mathcal{S}_n|T}{\\delta}\\right),\n    \n    which implies that\n    \n        \\operatorname{Regret}Regret_{\\infty}\\infty^{\\mathbb{E}}\\mathbb{E}(T) \\leq \\sum_{k=1}k=1^m \\left( C\\log_2\\tau_k + \\tau_k \\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}\\right) \\leq C\\lceil\\log_2 T\\rceil^2 + T\\frac{26(1+\\Lambda)Ln^{-\\alpha}}{1-\\nu}.\n    \n    Since $\\sum_{T=1}^\\infty \\frac{\\delta}{4T^{5/4}} < \\delta$\\sum_{T=1}T=1^\\infty \\frac{\\delta}{4T^{5/4}} < \\delta, the above bound is valid for all $T\\in\\mathbb{N}$T\\in\\mathbb{N}.\n\n\nWe can see that, although the in-path and expected classical regrets are the same, the expected quantum regret can be exponentially better in its dependence on $T$T compared to its in-path counterpart. By considering a measure of regret that ``filters'' random oscillations around an average quantity, it is possible to bypass the term $\\sqrt{T}$\\sqrt{T} coming from the Azuma-Hoeffding bound, which was, ultimately, the obstacle to a $\\poly\\log{T}$\\poly\\log{T}T quantum regret bound.\n\n\n\n\n\n\n\n\n\n\n\n\n\\bibliographystyle{alpha}alpha\n\\bibliography{references}{}\n\n", "appendix": false}}, "categories": ["cs.LG", "cs.AI", "math.OC", "quant-ph", "stat.ML"], "published": "2025-07-30 17:24:23+00:00", "primary_category": "cs.LG", "summary": "We propose novel classical and quantum online algorithms for learning\nfinite-horizon and infinite-horizon average-reward Markov Decision Processes\n(MDPs). Our algorithms are based on a hybrid exploration-generative\nreinforcement learning (RL) model wherein the agent can, from time to time,\nfreely interact with the environment in a generative sampling fashion, i.e., by\nhaving access to a \"simulator\". By employing known classical and new quantum\nalgorithms for approximating optimal policies under a generative model within\nour learning algorithms, we show that it is possible to avoid several paradigms\nfrom RL like \"optimism in the face of uncertainty\" and \"posterior sampling\" and\ninstead compute and use optimal policies directly, which yields better regret\nbounds compared to previous works. For finite-horizon MDPs, our quantum\nalgorithms obtain regret bounds which only depend logarithmically on the number\nof time steps $T$, thus breaking the $O(\\sqrt{T})$ classical barrier. This\nmatches the time dependence of the prior quantum works of Ganguly et al.\n(arXiv'23) and Zhong et al. (ICML'24), but with improved dependence on other\nparameters like state space size $S$ and action space size $A$. For\ninfinite-horizon MDPs, our classical and quantum bounds still maintain the\n$O(\\sqrt{T})$ dependence but with better $S$ and $A$ factors. Nonetheless, we\npropose a novel measure of regret for infinite-horizon MDPs with respect to\nwhich our quantum algorithms have $\\operatorname{poly}\\log{T}$ regret,\nexponentially better compared to classical algorithms. Finally, we generalise\nall of our results to compact state spaces."}