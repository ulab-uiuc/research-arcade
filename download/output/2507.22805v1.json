{"title": "MoCHA: Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention", "author": "Yuqi Pang", "abstract": "\\begin{abstract}\nVision large language models (VLLMs) are focusing primarily on handling complex and fine-grained visual information by incorporating advanced vision encoders and scaling up visual models. However, these approaches face high training and inference costs, as well as challenges in extracting visual details, effectively bridging across modalities. In this work, we propose a novel visual framework, \\textbf{\\textit{MoCHA}}, to address these issues. Our framework integrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to extract complementary visual features and is equipped with a sparse \\textbf{\\textit{M}}ixture \\textbf{\\textit{o}}f Experts \\textbf{\\textit{C}}onnectors (MoECs) module to dynamically select experts tailored to different visual dimensions. To mitigate redundant or insufficient use of the visual information encoded by the MoECs module, we further design a \\textbf{\\textit{H}}ierarchical Group \\textbf{\\textit{A}}ttention (HGA) with intra- and inter-group operations and an adaptive gating strategy for encoded visual features. We train MoCHA on two mainstream LLMs (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance across various benchmarks. Notably, MoCHA outperforms state-of-the-art open-weight models on various tasks. For example, compared to CuMo (Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate hallucination by showing improvements of 3.25\\% in POPE and to follow visual instructions by raising 153 points on MME. Finally, ablation studies further confirm the effectiveness and robustness of the proposed MoECs and HGA in improving the overall performance of MoCHA. The code is available at \\url{https://github.com/Pbhgit/MoCHA}.\n\n\n\n\n\n\\end{abstract}", "citations": {"liu2024llavanext": {"bib_key": "liu2024llavanext", "bib_title": "LLaVA-NeXT: Improved reasoning, OCR, and world knowledge", "bib_author ": "Liu, Haotian", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Vision Large Language Models (VLLMs)~\\cite{liu2024llavanext,wang2024qwen2vlenhancingvisionlanguagemodels, bai2025qwen25vltechnicalreport}", "next_context": "have shown promising results in multimodal reasoning tasks by integrating visual encoders with Large Language Models (LLM)~\\cite{gpt,openai2024gpt4o,touvron2023llama}."}, {"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "LLaVA-NeXT~\\cite{liu2024llavanext}", "next_context": ""}, {"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "LLaVA-NeXT~\\cite{liu2024llavanext}", "next_context": ""}], "importance_score": 2.333333333333333}, "wang2024qwen2vlenhancingvisionlanguagemodels": {"bib_key": "wang2024qwen2vlenhancingvisionlanguagemodels", "bib_title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution", "bib_author ": "Peng Wang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Vision Large Language Models (VLLMs)~\\cite{liu2024llavanext,wang2024qwen2vlenhancingvisionlanguagemodels, bai2025qwen25vltechnicalreport}", "next_context": "have shown promising results in multimodal reasoning tasks by integrating visual encoders with Large Language Models (LLM)~\\cite{gpt,openai2024gpt4o,touvron2023llama}."}], "importance_score": 0.3333333333333333}, "bai2025qwen25vltechnicalreport": {"bib_key": "bai2025qwen25vltechnicalreport", "bib_title": "Qwen2.5-VL Technical Report", "bib_author ": "Shuai Bai", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Vision Large Language Models (VLLMs)~\\cite{liu2024llavanext,wang2024qwen2vlenhancingvisionlanguagemodels, bai2025qwen25vltechnicalreport}", "next_context": "have shown promising results in multimodal reasoning tasks by integrating visual encoders with Large Language Models (LLM)~\\cite{gpt,openai2024gpt4o,touvron2023llama}."}], "importance_score": 0.3333333333333333}, "gpt": {"bib_key": "gpt", "bib_title": "ChatGPT", "bib_author ": "OpenAI", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Vision Large Language Models (VLLMs)~\\cite{liu2024llavanext,wang2024qwen2vlenhancingvisionlanguagemodels, bai2025qwen25vltechnicalreport}have shown promising results in multimodal reasoning tasks by integrating visual encoders with Large Language Models (LLM)~\\cite{gpt,openai2024gpt4o,touvron2023llama}", "next_context": "."}], "importance_score": 0.3333333333333333}, "openai2024gpt4o": {"bib_key": "openai2024gpt4o", "bib_title": "{Hello gpt-4o}", "bib_author ": "{OpenAI}", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Vision Large Language Models (VLLMs)~\\cite{liu2024llavanext,wang2024qwen2vlenhancingvisionlanguagemodels, bai2025qwen25vltechnicalreport}have shown promising results in multimodal reasoning tasks by integrating visual encoders with Large Language Models (LLM)~\\cite{gpt,openai2024gpt4o,touvron2023llama}", "next_context": "."}], "importance_score": 0.3333333333333333}, "touvron2023llama": {"bib_key": "touvron2023llama", "bib_title": "Llama: Open and efficient foundation language models", "bib_author ": "Touvron, Hugo", "arxiv_id": null, "short_id": "2302.13971", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Vision Large Language Models (VLLMs)~\\cite{liu2024llavanext,wang2024qwen2vlenhancingvisionlanguagemodels, bai2025qwen25vltechnicalreport}have shown promising results in multimodal reasoning tasks by integrating visual encoders with Large Language Models (LLM)~\\cite{gpt,openai2024gpt4o,touvron2023llama}", "next_context": "."}], "importance_score": 0.3333333333333333}, "geirhos2020shortcut": {"bib_key": "geirhos2020shortcut", "bib_title": "Shortcut learning in deep neural networks", "bib_author ": "Geirhos, Robert", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Additionally, excessive early reliance on language in VLLMs may act as a shortcut~\\cite{geirhos2020shortcut,yuksekgonul2023visionlanguagemodelsbehavelike}", "next_context": "that results in suboptimal learning of effective visual representations."}], "importance_score": 0.5}, "yuksekgonul2023visionlanguagemodelsbehavelike": {"bib_key": "yuksekgonul2023visionlanguagemodelsbehavelike", "bib_title": "When and why vision-language models behave like bags-of-words, and what to do about it?", "bib_author ": "Mert Yuksekgonul", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Additionally, excessive early reliance on language in VLLMs may act as a shortcut~\\cite{geirhos2020shortcut,yuksekgonul2023visionlanguagemodelsbehavelike}", "next_context": "that results in suboptimal learning of effective visual representations."}], "importance_score": 0.5}, "zheng2023judgingllmasajudgemtbenchchatbot": {"bib_key": "zheng2023judgingllmasajudgemtbenchchatbot", "bib_title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena", "bib_author ": "Lianmin Zheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Previous works have focused on strengthening vision encoders by utilizing extensive and higher-quality visual instruction data~\\cite{zheng2023judgingllmasajudgemtbenchchatbot}", "next_context": ", scaling up model size~\\cite{chen2024internvl}, or decomposing images into low-resolution patches~\\cite{shi2024needlargervisionmodels}."}], "importance_score": 1.0}, "chen2024internvl": {"bib_key": "chen2024internvl", "bib_title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks", "bib_author ": "Chen, Zhe", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Previous works have focused on strengthening vision encoders by utilizing extensive and higher-quality visual instruction data~\\cite{zheng2023judgingllmasajudgemtbenchchatbot}, scaling up model size~\\cite{chen2024internvl}", "next_context": ", or decomposing images into low-resolution patches~\\cite{shi2024needlargervisionmodels}."}, {"section": "Related Work", "subsection": "Large Pre-trained Vision Models", "subsubsection": null, "prev_context": "Some concurrent works have leveraged external assistance from vision-only, self-supervised models such as DINOv2, MoCo-v3~\\cite{he2021maskedautoencodersscalablevision}, and other advanced frameworks~\\cite{caron2021emergingpropertiesselfsupervisedvision,chen2024internvl}", "next_context": ", as well as domain-specific expert models."}, {"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "InternVL-Chat~\\cite{chen2024internvl}", "next_context": ""}], "importance_score": 2.5}, "shi2024needlargervisionmodels": {"bib_key": "shi2024needlargervisionmodels", "bib_title": "When Do We Not Need Larger Vision Models?", "bib_author ": "Baifeng Shi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Previous works have focused on strengthening vision encoders by utilizing extensive and higher-quality visual instruction data~\\cite{zheng2023judgingllmasajudgemtbenchchatbot}, scaling up model size~\\cite{chen2024internvl}, or decomposing images into low-resolution patches~\\cite{shi2024needlargervisionmodels}", "next_context": "."}], "importance_score": 1.0}, "tong2024cambrian1fullyopenvisioncentric": {"bib_key": "tong2024cambrian1fullyopenvisioncentric", "bib_title": "Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs", "bib_author ": "Shengbang Tong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Furthermore, Cambrian-1~\\cite{tong2024cambrian1fullyopenvisioncentric}", "next_context": ", designed with a vision centric approach, introduces a novel paradigm for vision-language fusion through its combination of multiple vision encoders and spatial vision aggregator (SVA), but faces difficulties due to the lack of dynamic feature fusion and shortcomings in fine-grained perception."}, {"section": "Methodology", "subsection": "Selection of Vision Encoders", "subsubsection": null, "prev_context": "Although language-supervised models outperform self-supervised and other models across all benchmark categories, Cambrian-1~\\cite{tong2024cambrian1fullyopenvisioncentric}", "next_context": "highlights that well-trained self-supervised models like DINOv2 are capable of achieving competitive performance in vision-centric tasks."}], "importance_score": 2.0}, "fedus2022switch": {"bib_key": "fedus2022switch", "bib_title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity", "bib_author ": "Fedus, William", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Sparse Mixture of Experts (MoE)~\\cite{fedus2022switch}", "next_context": "has emerged as an efficient solution that dynamically activates only a subset of expert networks for each input to improve scalability and specialization."}], "importance_score": 1.0}, "radford2021learningtransferablevisualmodels": {"bib_key": "radford2021learningtransferablevisualmodels", "bib_title": "Learning Transferable Visual Models From Natural Language Supervision", "bib_author ": "Alec Radford", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "First, we leverage four distinct yet complementary vision backbones, including CLIP~\\cite{radford2021learningtransferablevisualmodels}", "next_context": ", SigLIP~\\cite{zhai2023sigmoidlosslanguageimage}, DINOv2~\\cite{oquab2024dinov2learningrobustvisual}, and ConvNeXt~\\cite{liu2022convnet2020s}, to extract diverse and robust image features."}], "importance_score": 1.0}, "zhai2023sigmoidlosslanguageimage": {"bib_key": "zhai2023sigmoidlosslanguageimage", "bib_title": "Sigmoid Loss for Language Image Pre-Training", "bib_author ": "Xiaohua Zhai", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "First, we leverage four distinct yet complementary vision backbones, including CLIP~\\cite{radford2021learningtransferablevisualmodels}, SigLIP~\\cite{zhai2023sigmoidlosslanguageimage}", "next_context": ", DINOv2~\\cite{oquab2024dinov2learningrobustvisual}, and ConvNeXt~\\cite{liu2022convnet2020s}, to extract diverse and robust image features."}], "importance_score": 1.0}, "oquab2024dinov2learningrobustvisual": {"bib_key": "oquab2024dinov2learningrobustvisual", "bib_title": "DINOv2: Learning Robust Visual Features without Supervision", "bib_author ": "Maxime Oquab", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "First, we leverage four distinct yet complementary vision backbones, including CLIP~\\cite{radford2021learningtransferablevisualmodels}, SigLIP~\\cite{zhai2023sigmoidlosslanguageimage}, DINOv2~\\cite{oquab2024dinov2learningrobustvisual}", "next_context": ", and ConvNeXt~\\cite{liu2022convnet2020s}, to extract diverse and robust image features."}], "importance_score": 1.0}, "liu2022convnet2020s": {"bib_key": "liu2022convnet2020s", "bib_title": "A ConvNet for the 2020s", "bib_author ": "Zhuang Liu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "First, we leverage four distinct yet complementary vision backbones, including CLIP~\\cite{radford2021learningtransferablevisualmodels}, SigLIP~\\cite{zhai2023sigmoidlosslanguageimage}, DINOv2~\\cite{oquab2024dinov2learningrobustvisual}, and ConvNeXt~\\cite{liu2022convnet2020s}", "next_context": ", to extract diverse and robust image features."}], "importance_score": 1.0}, "li2024cumoscalingmultimodalllm": {"bib_key": "li2024cumoscalingmultimodalllm", "bib_title": "CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts", "bib_author ": "Jiachen Li", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Notably, our 3B-sized MoCHA shows a 3.25\\%reduction in hallucination on POPE and delivers a 153-point improvement on general visual tasks on MME, outperforming the larger CuMo-7B model~\\cite{li2024cumoscalingmultimodalllm}", "next_context": "."}, {"section": "Related Work", "subsection": "Mixture of Experts", "subsubsection": null, "prev_context": "Furthermore, CuMo~\\cite{li2024cumoscalingmultimodalllm}", "next_context": "integrates Co-upcycled Top-K sparse-gating MoE blocks into the visual encoder, MLP connector, and language model, enhancing VLLM inference performance."}, {"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "CuMo~\\cite{li2024cumoscalingmultimodalllm}", "next_context": ""}], "importance_score": 3.0}, "dosovitskiy2021imageworth16x16words": {"bib_key": "dosovitskiy2021imageworth16x16words", "bib_title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale", "bib_author ": "Alexey Dosovitskiy", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": "Large Pre-trained Vision Models", "subsubsection": null, "prev_context": "The advent of pre-trained Vision Transformers (ViT)~\\cite{dosovitskiy2021imageworth16x16words}", "next_context": "has significantly propelled the advancement of computer vision."}], "importance_score": 1.0}, "he2021maskedautoencodersscalablevision": {"bib_key": "he2021maskedautoencodersscalablevision", "bib_title": "Masked Autoencoders Are Scalable Vision Learners", "bib_author ": "Kaiming He", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": "Large Pre-trained Vision Models", "subsubsection": null, "prev_context": "Some concurrent works have leveraged external assistance from vision-only, self-supervised models such as DINOv2, MoCo-v3~\\cite{he2021maskedautoencodersscalablevision}", "next_context": ", and other advanced frameworks~\\cite{caron2021emergingpropertiesselfsupervisedvision,chen2024internvl}, as well as domain-specific expert models."}], "importance_score": 1.0}, "caron2021emergingpropertiesselfsupervisedvision": {"bib_key": "caron2021emergingpropertiesselfsupervisedvision", "bib_title": "Emerging Properties in Self-Supervised Vision Transformers", "bib_author ": "Mathilde Caron", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": "Large Pre-trained Vision Models", "subsubsection": null, "prev_context": "Some concurrent works have leveraged external assistance from vision-only, self-supervised models such as DINOv2, MoCo-v3~\\cite{he2021maskedautoencodersscalablevision}, and other advanced frameworks~\\cite{caron2021emergingpropertiesselfsupervisedvision,chen2024internvl}", "next_context": ", as well as domain-specific expert models."}], "importance_score": 0.5}, "zoph2022stmoedesigningstabletransferable": {"bib_key": "zoph2022stmoedesigningstabletransferable", "bib_title": "ST-MoE: Designing Stable and Transferable Sparse Expert Models", "bib_author ": "Barret Zoph", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": "Mixture of Experts", "subsubsection": null, "prev_context": "ST-MoE~\\cite{zoph2022stmoedesigningstabletransferable}", "next_context": "employs load-balancing loss and router-z loss to ensure an even distribution of experts."}, {"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "This auxiliary loss consists of a load balancing loss~\\cite{wang2024auxiliarylossfreeloadbalancingstrategy}and a router z-loss~\\cite{zoph2022stmoedesigningstabletransferable}", "next_context": "."}], "importance_score": 2.0}, "dai2024deepseekmoeultimateexpertspecialization": {"bib_key": "dai2024deepseekmoeultimateexpertspecialization", "bib_title": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models", "bib_author ": "Damai Dai", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": "Mixture of Experts", "subsubsection": null, "prev_context": "The recently popular DeepSeek-V3 adopts the DeepSeekMoE~\\cite{dai2024deepseekmoeultimateexpertspecialization}", "next_context": "architecture, leveraging shared experts to capture common knowledge and reduce redundancy in routed experts."}], "importance_score": 1.0}, "mustafa2022multimodalcontrastivelearninglimoe": {"bib_key": "mustafa2022multimodalcontrastivelearninglimoe", "bib_title": "Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts", "bib_author ": "Basil Mustafa", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": "Mixture of Experts", "subsubsection": null, "prev_context": "LIMoE~\\cite{mustafa2022multimodalcontrastivelearninglimoe}", "next_context": "replaces dense MLP layers with MoE layers in CLIP, improving zero-shot image classification."}], "importance_score": 1.0}, "liu2024adamolefinetuninglargelanguage": {"bib_key": "liu2024adamolefinetuninglargelanguage", "bib_title": "AdaMoLE: Fine-Tuning Large Language Models with Adaptive Mixture of Low-Rank Adaptation Experts", "bib_author ": "Zefang Liu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": "Mixture of Experts", "subsubsection": null, "prev_context": "AdaMV-MoE~\\cite{liu2024adamolefinetuninglargelanguage}", "next_context": "introduces an adaptive MoE framework for multi-task learning."}], "importance_score": 1.0}, "vishniakov2024convnetvstransformersupervised": {"bib_key": "vishniakov2024convnetvstransformersupervised", "bib_title": "ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy", "bib_author ": "Kirill Vishniakov", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Selection of Vision Encoders", "subsubsection": null, "prev_context": "While ViTs remain the dominant architecture, ConvNet-based models such as OpenCLIP ConvNeXt are well-suited for high-resolution image processing~\\cite{vishniakov2024convnetvstransformersupervised}", "next_context": "delivering outstanding results on OCR, chart interpretation, and other vision-centric benchmarks."}], "importance_score": 1.0}, "shazeer2017outrageouslylargeneuralnetworks": {"bib_key": "shazeer2017outrageouslylargeneuralnetworks", "bib_title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer", "bib_author ": "Noam Shazeer", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Mixture of Experts Connectors (MoECs)", "subsubsection": null, "prev_context": "Previous mainstream approaches~\\cite{shazeer2017outrageouslylargeneuralnetworks}", "next_context": "replace dense MLP blocks with sparsely-gated mixture of experts blocks."}], "importance_score": 1.0}, "lin2023sphinxjointmixingweights": {"bib_key": "lin2023sphinxjointmixingweights", "bib_title": "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models", "bib_author ": "Ziyi Lin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Mixture of Experts Connectors (MoECs)", "subsubsection": "Sequence Append (Token Dimension): ", "prev_context": "While channel concatenation~\\cite{lin2023sphinxjointmixingweights}", "next_context": "achieves performance comparable to sequence append~\\cite{liu2024prismervisionlanguagemodelmultitask,kar2024bravebroadeningvisualencoding,fan2024mousipolyvisualexpertvisionlanguagemodels}, it requires strict spatial alignment of encoder outputs, rendering it less compatible with encoders of differing resolutions and architectures."}], "importance_score": 1.0}, "liu2024prismervisionlanguagemodelmultitask": {"bib_key": "liu2024prismervisionlanguagemodelmultitask", "bib_title": "Prismer: A Vision-Language Model with Multi-Task Experts", "bib_author ": "Shikun Liu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Mixture of Experts Connectors (MoECs)", "subsubsection": "Sequence Append (Token Dimension): ", "prev_context": "While channel concatenation~\\cite{lin2023sphinxjointmixingweights}achieves performance comparable to sequence append~\\cite{liu2024prismervisionlanguagemodelmultitask,kar2024bravebroadeningvisualencoding,fan2024mousipolyvisualexpertvisionlanguagemodels}", "next_context": ", it requires strict spatial alignment of encoder outputs, rendering it less compatible with encoders of differing resolutions and architectures."}], "importance_score": 0.3333333333333333}, "kar2024bravebroadeningvisualencoding": {"bib_key": "kar2024bravebroadeningvisualencoding", "bib_title": "BRAVE: Broadening the visual encoding of vision-language models", "bib_author ": "O\u011fuzhan Fatih Kar", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Mixture of Experts Connectors (MoECs)", "subsubsection": "Sequence Append (Token Dimension): ", "prev_context": "While channel concatenation~\\cite{lin2023sphinxjointmixingweights}achieves performance comparable to sequence append~\\cite{liu2024prismervisionlanguagemodelmultitask,kar2024bravebroadeningvisualencoding,fan2024mousipolyvisualexpertvisionlanguagemodels}", "next_context": ", it requires strict spatial alignment of encoder outputs, rendering it less compatible with encoders of differing resolutions and architectures."}], "importance_score": 0.3333333333333333}, "fan2024mousipolyvisualexpertvisionlanguagemodels": {"bib_key": "fan2024mousipolyvisualexpertvisionlanguagemodels", "bib_title": "MouSi: Poly-Visual-Expert Vision-Language Models", "bib_author ": "Xiaoran Fan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Mixture of Experts Connectors (MoECs)", "subsubsection": "Sequence Append (Token Dimension): ", "prev_context": "While channel concatenation~\\cite{lin2023sphinxjointmixingweights}achieves performance comparable to sequence append~\\cite{liu2024prismervisionlanguagemodelmultitask,kar2024bravebroadeningvisualencoding,fan2024mousipolyvisualexpertvisionlanguagemodels}", "next_context": ", it requires strict spatial alignment of encoder outputs, rendering it less compatible with encoders of differing resolutions and architectures."}], "importance_score": 0.3333333333333333}, "wang2024auxiliarylossfreeloadbalancingstrategy": {"bib_key": "wang2024auxiliarylossfreeloadbalancingstrategy", "bib_title": "Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts", "bib_author ": "Lean Wang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "This auxiliary loss consists of a load balancing loss~\\cite{wang2024auxiliarylossfreeloadbalancingstrategy}", "next_context": "and a router z-loss~\\cite{zoph2022stmoedesigningstabletransferable}."}], "importance_score": 1.0}, "bai2023qwenvlversatilevisionlanguagemodel": {"bib_key": "bai2023qwenvlversatilevisionlanguagemodel", "bib_title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond", "bib_author ": "Jinze Bai", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "\\midruleQwen-VL-Chat~\\cite{bai2023qwenvlversatilevisionlanguagemodel}", "next_context": ""}], "importance_score": 1.0}, "dai2023instructblipgeneralpurposevisionlanguagemodels": {"bib_key": "dai2023instructblipgeneralpurposevisionlanguagemodels", "bib_title": "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning", "bib_author ": "Wenliang Dai", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "InstructBLIP~\\cite{dai2023instructblipgeneralpurposevisionlanguagemodels}", "next_context": ""}, {"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "\\hdashlineInstructBLIP~\\cite{dai2023instructblipgeneralpurposevisionlanguagemodels}", "next_context": ""}], "importance_score": 2.0}, "contributorsxtuner": {"bib_key": "contributorsxtuner", "bib_title": "Xtuner: a toolkit for efficiently fine-tuning LLM", "bib_author ": "Contributors, XTuner", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "LLaVA-LLaMA3~\\cite{contributorsxtuner}", "next_context": ""}], "importance_score": 1.0}, "li2024minigeminiminingpotentialmultimodality": {"bib_key": "li2024minigeminiminingpotentialmultimodality", "bib_title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models", "bib_author ": "Yanwei Li", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "Mini-Gemini~\\cite{li2024minigeminiminingpotentialmultimodality}", "next_context": ""}, {"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "Mini-Gemini~\\cite{li2024minigeminiminingpotentialmultimodality}", "next_context": ""}], "importance_score": 2.0}, "liu2025sphinxxscalingdataparameters": {"bib_key": "liu2025sphinxxscalingdataparameters", "bib_title": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models", "bib_author ": "Dongyang Liu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "SPHINX-Intern2~\\cite{liu2025sphinxxscalingdataparameters}", "next_context": ""}, {"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "SPHINX-Plus~\\cite{liu2025sphinxxscalingdataparameters}", "next_context": ""}], "importance_score": 2.0}, "liu2023visual": {"bib_key": "liu2023visual", "bib_title": "Visual instruction tuning", "bib_author ": "Liu, Haotian", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "LLaVA-v1.5~\\cite{liu2023visual}", "next_context": ""}, {"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "LLaVA-v1.5~\\cite{liu2023visual}", "next_context": ""}], "importance_score": 2.0}, "lin2024vilapretrainingvisuallanguage": {"bib_key": "lin2024vilapretrainingvisuallanguage", "bib_title": "VILA: On Pre-training for Visual Language Models", "bib_author ": "Ji Lin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "VILA~\\cite{lin2024vilapretrainingvisuallanguage}", "next_context": ""}], "importance_score": 1.0}, "li2024llama": {"bib_key": "li2024llama", "bib_title": "Llama-vid: An image is worth 2 tokens in large language models", "bib_author ": "Li, Yanwei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "LLaMA-VID~\\cite{li2024llama}", "next_context": ""}], "importance_score": 1.0}, "chu2023mobilevlmfaststrong": {"bib_key": "chu2023mobilevlmfaststrong", "bib_title": "MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices", "bib_author ": "Xiangxiang Chu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "\\hdashlineMobileVLM~\\cite{chu2023mobilevlmfaststrong}", "next_context": ""}], "importance_score": 1.0}, "yuan2024tinygptvefficientmultimodallarge": {"bib_key": "yuan2024tinygptvefficientmultimodallarge", "bib_title": "TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones", "bib_author ": "Zhengqing Yuan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "TinyGPT-V~\\cite{yuan2024tinygptvefficientmultimodallarge}", "next_context": ""}], "importance_score": 1.0}, "zhu2024llava": {"bib_key": "zhu2024llava", "bib_title": "Llava-phi: Efficient multi-modal assistant with small language model", "bib_author ": "Zhu, Yichen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "LLaVA-Phi~\\cite{zhu2024llava}", "next_context": ""}], "importance_score": 1.0}, "lin2024moellavamixtureexpertslarge": {"bib_key": "lin2024moellavamixtureexpertslarge", "bib_title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models", "bib_author ": "Bin Lin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Loss Function", "subsubsection": null, "prev_context": "MoE-LLaVA-2.7B\u00d74-Top2~\\cite{lin2024moellavamixtureexpertslarge}", "next_context": ""}], "importance_score": 1.0}, "hudson2019gqa": {"bib_key": "hudson2019gqa", "bib_title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering", "bib_author ": "Hudson, Drew A", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Data Details", "subsubsection": "Evaluation Benchmarks.", "prev_context": "We focus on academic VQA datasets, including GQA~\\cite{hudson2019gqa}", "next_context": ", Science-QA~\\cite{lu2022learn}, and TextVQA~\\cite{singh2019towards}, as well as instruction-following VLLM benchmarks, such as POPE~\\cite{li2023evaluatingobjecthallucinationlarge}, MME~\\cite{fu2024mmecomprehensiveevaluationbenchmark}, MMBench~\\cite{liu2024mmbenchmultimodalmodelallaround}, and MM-Vet~\\cite{yu2024mmvetevaluatinglargemultimodal}."}], "importance_score": 1.0}, "lu2022learn": {"bib_key": "lu2022learn", "bib_title": "Learn to explain: Multimodal reasoning via thought chains for science question answering", "bib_author ": "Lu, Pan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Data Details", "subsubsection": "Evaluation Benchmarks.", "prev_context": "We focus on academic VQA datasets, including GQA~\\cite{hudson2019gqa}, Science-QA~\\cite{lu2022learn}", "next_context": ", and TextVQA~\\cite{singh2019towards}, as well as instruction-following VLLM benchmarks, such as POPE~\\cite{li2023evaluatingobjecthallucinationlarge}, MME~\\cite{fu2024mmecomprehensiveevaluationbenchmark}, MMBench~\\cite{liu2024mmbenchmultimodalmodelallaround}, and MM-Vet~\\cite{yu2024mmvetevaluatinglargemultimodal}."}], "importance_score": 1.0}, "singh2019towards": {"bib_key": "singh2019towards", "bib_title": "Towards vqa models that can read", "bib_author ": "Singh, Amanpreet", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Data Details", "subsubsection": "Evaluation Benchmarks.", "prev_context": "We focus on academic VQA datasets, including GQA~\\cite{hudson2019gqa}, Science-QA~\\cite{lu2022learn}, and TextVQA~\\cite{singh2019towards}", "next_context": ", as well as instruction-following VLLM benchmarks, such as POPE~\\cite{li2023evaluatingobjecthallucinationlarge}, MME~\\cite{fu2024mmecomprehensiveevaluationbenchmark}, MMBench~\\cite{liu2024mmbenchmultimodalmodelallaround}, and MM-Vet~\\cite{yu2024mmvetevaluatinglargemultimodal}."}], "importance_score": 1.0}, "li2023evaluatingobjecthallucinationlarge": {"bib_key": "li2023evaluatingobjecthallucinationlarge", "bib_title": "Evaluating Object Hallucination in Large Vision-Language Models", "bib_author ": "Yifan Li", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Data Details", "subsubsection": "Evaluation Benchmarks.", "prev_context": "We focus on academic VQA datasets, including GQA~\\cite{hudson2019gqa}, Science-QA~\\cite{lu2022learn}, and TextVQA~\\cite{singh2019towards}, as well as instruction-following VLLM benchmarks, such as POPE~\\cite{li2023evaluatingobjecthallucinationlarge}", "next_context": ", MME~\\cite{fu2024mmecomprehensiveevaluationbenchmark}, MMBench~\\cite{liu2024mmbenchmultimodalmodelallaround}, and MM-Vet~\\cite{yu2024mmvetevaluatinglargemultimodal}."}], "importance_score": 1.0}, "fu2024mmecomprehensiveevaluationbenchmark": {"bib_key": "fu2024mmecomprehensiveevaluationbenchmark", "bib_title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models", "bib_author ": "Chaoyou Fu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Data Details", "subsubsection": "Evaluation Benchmarks.", "prev_context": "We focus on academic VQA datasets, including GQA~\\cite{hudson2019gqa}, Science-QA~\\cite{lu2022learn}, and TextVQA~\\cite{singh2019towards}, as well as instruction-following VLLM benchmarks, such as POPE~\\cite{li2023evaluatingobjecthallucinationlarge}, MME~\\cite{fu2024mmecomprehensiveevaluationbenchmark}", "next_context": ", MMBench~\\cite{liu2024mmbenchmultimodalmodelallaround}, and MM-Vet~\\cite{yu2024mmvetevaluatinglargemultimodal}."}], "importance_score": 1.0}, "liu2024mmbenchmultimodalmodelallaround": {"bib_key": "liu2024mmbenchmultimodalmodelallaround", "bib_title": "MMBench: Is Your Multi-modal Model an All-around Player?", "bib_author ": "Yuan Liu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Data Details", "subsubsection": "Evaluation Benchmarks.", "prev_context": "We focus on academic VQA datasets, including GQA~\\cite{hudson2019gqa}, Science-QA~\\cite{lu2022learn}, and TextVQA~\\cite{singh2019towards}, as well as instruction-following VLLM benchmarks, such as POPE~\\cite{li2023evaluatingobjecthallucinationlarge}, MME~\\cite{fu2024mmecomprehensiveevaluationbenchmark}, MMBench~\\cite{liu2024mmbenchmultimodalmodelallaround}", "next_context": ", and MM-Vet~\\cite{yu2024mmvetevaluatinglargemultimodal}."}], "importance_score": 1.0}, "yu2024mmvetevaluatinglargemultimodal": {"bib_key": "yu2024mmvetevaluatinglargemultimodal", "bib_title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities", "bib_author ": "Weihao Yu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Data Details", "subsubsection": "Evaluation Benchmarks.", "prev_context": "We focus on academic VQA datasets, including GQA~\\cite{hudson2019gqa}, Science-QA~\\cite{lu2022learn}, and TextVQA~\\cite{singh2019towards}, as well as instruction-following VLLM benchmarks, such as POPE~\\cite{li2023evaluatingobjecthallucinationlarge}, MME~\\cite{fu2024mmecomprehensiveevaluationbenchmark}, MMBench~\\cite{liu2024mmbenchmultimodalmodelallaround}, and MM-Vet~\\cite{yu2024mmvetevaluatinglargemultimodal}", "next_context": "."}], "importance_score": 1.0}, "lu2024mathvistaevaluatingmathematicalreasoning": {"bib_key": "lu2024mathvistaevaluatingmathematicalreasoning", "bib_title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts", "bib_author ": "Pan Lu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Data Details", "subsubsection": "Evaluation Benchmarks.", "prev_context": "Furthermore, we evaluate the challenging MathVista~\\cite{lu2024mathvistaevaluatingmathematicalreasoning}", "next_context": "dataset to assess VLLM visual reasoning capabilities."}], "importance_score": 1.0}}, "refs": [], "table": [{"original": "\\begin{table*}[!ht]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{2.5pt}\n\\begin{tabular}{@{}!{\\color{white}\\vrule width 0pt}l!{\\color{black}\\vrule width 0.6pt}l!{\\color{black}\\vrule width 0.6pt}cccccccc!{\\color{white}\\vrule width 0pt}@{}}\n\\toprule\nMethod & LLM & GQA & \\makecell{SQA \\\\IMG} & \\makecell{Text \\\\ VQA} & \\makecell{MM\\\\Vet} & POPE & MME & \\makecell{MMB\\\\EN} & MathVista \\\\ \n\\midrule\n% \\multirow{7}{*}{7B-8B Models}  \nQwen-VL-Chat ~\\cite{bai2023qwenvlversatilevisionlanguagemodel}  & Qwen-7B & 57.5 & 68.2 & 61.5 & - & - & 1487.5 & 60.6 & -  \\\\\nInstructBLIP ~\\cite{dai2023instructblipgeneralpurposevisionlanguagemodels} & Vicuna-7B &49.2&60.5& 50.1& 26.2& -& -& 36.0& -\\\\\nLLaVA-LLaMA3~\\cite{contributorsxtuner}&LLaMA3-8B-IT&62.6& 72.9& 59.0& -& 86.4& 1469& \\underline{72.3}& -\\\\\nMini-Gemini~\\cite{li2024minigeminiminingpotentialmultimodality}&Vicuna-7B&-& 65.2& -& 40.8& -& 1523& 69.3& 31.4\\\\\nSPHINX-Intern2~\\cite{liu2025sphinxxscalingdataparameters}  & InternLM2-7B& 56.2 & 70.4& 58.1 & 36.5 & 86.9 & 1260.4 & 57.9 & 35.5 \\\\\nLLaVA-NeXT~\\cite{liu2024llavanext} &Vicuna-7B&64.2& 70.1& 64.9& 43.9& 86.5& 1519& 67.4& 34.6\\\\\nLLaVA-NeXT~\\cite{liu2024llavanext} &Mistral-7B&64.8&72.8&65.7&47.3&86.7&1498&68.7&\\textbf{37.7}\\\\\nLLaVA-v1.5~\\cite{liu2023visual} &Vicuna-7B&62.0& 66.8& 58.2& 30.5& 85.9& 1510.7& 64.3& -\\\\\nVILA~\\cite{lin2024vilapretrainingvisuallanguage} &Vicuna-7B&62.3& 68.2& 64.4& 34.9& 85.5& 1533& 68.9& -\\\\\nCuMo~\\cite{li2024cumoscalingmultimodalllm}& Mistral-7B& 64.9&\t73.9&\t\\textbf{67.0}&\t\\textbf{51.0}&\t86.7&\t1548.6&\t\\textbf{73.0}&\t35.1\\\\\n\\hdashline\n% \\multirow{5}{*}{13B Models}\nInstructBLIP~\\cite{dai2023instructblipgeneralpurposevisionlanguagemodels} &Vicuna-13B&49.5&63.1&50.7&25.6&78.9&1212.8&-&-\\\\\nLLaVA-v1.5~\\cite{liu2023visual}&Vicuna-13B&63.3&71.6&61.3&35.4&85.9&1531.3&67.7&27.6\\\\\nMini-Gemini~\\cite{li2024minigeminiminingpotentialmultimodality} &Vicuna-13B& -&65.9&-&46.0&-&1565&68.5&\\underline{37.0}\\\\\nInternVL-Chat~\\cite{chen2024internvl} &Vicuna-13B&\\textbf{66.6}&-&61.5&-&87.6&1586.4&-&-\\\\\nLLaMA-VID~\\cite{li2024llama} &Vicuna-13B&65.0&70.0&-&-&86.0&1542.3&66.6&-\\\\\nSPHINX-Plus~\\cite{liu2025sphinxxscalingdataparameters} &LLaMA2-13B&-&\\underline{74.2}&65.7&\\underline{47.9}&89.1&1457.7&71.0&36.8\\\\\n\\hdashline\n% \\multirow{4}{*}{2.7B Models}\nMobileVLM~\\cite{chu2023mobilevlmfaststrong} &MobileLLaMA-2.7B&59.0& 61.0&47.5& -& 84.9& 1288.9& 59.6& -\\\\\nTinyGPT-V~\\cite{yuan2024tinygptvefficientmultimodallarge} &Phi2-2.7B&33.6& -& -& -& -& -& -& -\\\\\nLLaVA-Phi~\\cite{zhu2024llava} &Phi2-2.7B&-& 68.4& 48.6& 28.9& 85.0& 1335.1& 59.8& -\\\\\nMoE-LLaVA-2.7B\u00d74-Top2~\\cite{lin2024moellavamixtureexpertslarge} &Phi2-2.7B&61.4& 68.5& 51.4& 34.3& 86.3& 1423& 65.2& -\\\\\n% & & & & & & & & & & \\\\\n% & & & & & & & & & & \\\\\n\\hdashline\n\\rowcolor{lightblue}\n\\multicolumn{1}{l!{\\color{black}\\vrule width 0.8pt}}{MoCHA (SigLIP+DINOv2+ConvNeXt+CLIP)} & \\multicolumn{1}{l!{\\color{black}\\vrule width 0.8pt}}{Phi2-2.7B} & 65.24 & 73.24 & 63.13 & 32.74 & \\underline{89.95} & \\underline{1701.60} & 69.67 & 32.24$^{\\dagger}$ \\\\\n\\rowcolor{lightblue}\n\\multicolumn{1}{l!{\\color{black}\\vrule width 0.8pt}}{MoCHA (SigLIP+DINOv2+ConvNeXt+CLIP)} & \\multicolumn{1}{l!{\\color{black}\\vrule width 0.8pt}}{Vicuna-7B} & \\underline{65.73} & \\textbf{74.59} & \\underline{65.72} & 36.68 & \\textbf{89.98}  & \\textbf{1744.09} &71.06 & 35.60$^{\\dagger}$ \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Comparisons between MoCHA and other VLLMs on competitive benchmarks. These models are grouped by the size of the base LLM. Numbers$^{\\dagger}$ are averaged by three inference runs of querying GPT API.}\n\\label{tab:22}\n\\end{table*}", "caption": "\\caption{Comparisons between MoCHA and other VLLMs on competitive benchmarks. These models are grouped by the size of the base LLM. Numbers$^{\\dagger}$ are averaged by three inference runs of querying GPT API.}", "label": "\\label{tab:22}", "tabular": "\\begin{tabular}{@{}!{\\color{white}\\vrule width 0pt}l!{\\color{black}\\vrule width 0.6pt}l!{\\color{black}\\vrule width 0.6pt}cccccccc!{\\color{white}\\vrule width 0pt}@{}}\n\\toprule\nMethod & LLM & GQA & \\makecell{SQA \\\\IMG} & \\makecell{Text \\\\ VQA} & \\makecell{MM\\\\Vet} & POPE & MME & \\makecell{MMB\\\\EN} & MathVista \\\\ \n\\midrule\n% \\multirow{7}{*}{7B-8B Models}  \nQwen-VL-Chat ~\\cite{bai2023qwenvlversatilevisionlanguagemodel}  & Qwen-7B & 57.5 & 68.2 & 61.5 & - & - & 1487.5 & 60.6 & -  \\\\\nInstructBLIP ~\\cite{dai2023instructblipgeneralpurposevisionlanguagemodels} & Vicuna-7B &49.2&60.5& 50.1& 26.2& -& -& 36.0& -\\\\\nLLaVA-LLaMA3~\\cite{contributorsxtuner}&LLaMA3-8B-IT&62.6& 72.9& 59.0& -& 86.4& 1469& \\underline{72.3}& -\\\\\nMini-Gemini~\\cite{li2024minigeminiminingpotentialmultimodality}&Vicuna-7B&-& 65.2& -& 40.8& -& 1523& 69.3& 31.4\\\\\nSPHINX-Intern2~\\cite{liu2025sphinxxscalingdataparameters}  & InternLM2-7B& 56.2 & 70.4& 58.1 & 36.5 & 86.9 & 1260.4 & 57.9 & 35.5 \\\\\nLLaVA-NeXT~\\cite{liu2024llavanext} &Vicuna-7B&64.2& 70.1& 64.9& 43.9& 86.5& 1519& 67.4& 34.6\\\\\nLLaVA-NeXT~\\cite{liu2024llavanext} &Mistral-7B&64.8&72.8&65.7&47.3&86.7&1498&68.7&\\textbf{37.7}\\\\\nLLaVA-v1.5~\\cite{liu2023visual} &Vicuna-7B&62.0& 66.8& 58.2& 30.5& 85.9& 1510.7& 64.3& -\\\\\nVILA~\\cite{lin2024vilapretrainingvisuallanguage} &Vicuna-7B&62.3& 68.2& 64.4& 34.9& 85.5& 1533& 68.9& -\\\\\nCuMo~\\cite{li2024cumoscalingmultimodalllm}& Mistral-7B& 64.9&\t73.9&\t\\textbf{67.0}&\t\\textbf{51.0}&\t86.7&\t1548.6&\t\\textbf{73.0}&\t35.1\\\\\n\\hdashline\n% \\multirow{5}{*}{13B Models}\nInstructBLIP~\\cite{dai2023instructblipgeneralpurposevisionlanguagemodels} &Vicuna-13B&49.5&63.1&50.7&25.6&78.9&1212.8&-&-\\\\\nLLaVA-v1.5~\\cite{liu2023visual}&Vicuna-13B&63.3&71.6&61.3&35.4&85.9&1531.3&67.7&27.6\\\\\nMini-Gemini~\\cite{li2024minigeminiminingpotentialmultimodality} &Vicuna-13B& -&65.9&-&46.0&-&1565&68.5&\\underline{37.0}\\\\\nInternVL-Chat~\\cite{chen2024internvl} &Vicuna-13B&\\textbf{66.6}&-&61.5&-&87.6&1586.4&-&-\\\\\nLLaMA-VID~\\cite{li2024llama} &Vicuna-13B&65.0&70.0&-&-&86.0&1542.3&66.6&-\\\\\nSPHINX-Plus~\\cite{liu2025sphinxxscalingdataparameters} &LLaMA2-13B&-&\\underline{74.2}&65.7&\\underline{47.9}&89.1&1457.7&71.0&36.8\\\\\n\\hdashline\n% \\multirow{4}{*}{2.7B Models}\nMobileVLM~\\cite{chu2023mobilevlmfaststrong} &MobileLLaMA-2.7B&59.0& 61.0&47.5& -& 84.9& 1288.9& 59.6& -\\\\\nTinyGPT-V~\\cite{yuan2024tinygptvefficientmultimodallarge} &Phi2-2.7B&33.6& -& -& -& -& -& -& -\\\\\nLLaVA-Phi~\\cite{zhu2024llava} &Phi2-2.7B&-& 68.4& 48.6& 28.9& 85.0& 1335.1& 59.8& -\\\\\nMoE-LLaVA-2.7B\u00d74-Top2~\\cite{lin2024moellavamixtureexpertslarge} &Phi2-2.7B&61.4& 68.5& 51.4& 34.3& 86.3& 1423& 65.2& -\\\\\n% & & & & & & & & & & \\\\\n% & & & & & & & & & & \\\\\n\\hdashline\n\\rowcolor{lightblue}\n\\multicolumn{1}{l!{\\color{black}\\vrule width 0.8pt}}{MoCHA (SigLIP+DINOv2+ConvNeXt+CLIP)} & \\multicolumn{1}{l!{\\color{black}\\vrule width 0.8pt}}{Phi2-2.7B} & 65.24 & 73.24 & 63.13 & 32.74 & \\underline{89.95} & \\underline{1701.60} & 69.67 & 32.24$^{\\dagger}$ \\\\\n\\rowcolor{lightblue}\n\\multicolumn{1}{l!{\\color{black}\\vrule width 0.8pt}}{MoCHA (SigLIP+DINOv2+ConvNeXt+CLIP)} & \\multicolumn{1}{l!{\\color{black}\\vrule width 0.8pt}}{Vicuna-7B} & \\underline{65.73} & \\textbf{74.59} & \\underline{65.72} & 36.68 & \\textbf{89.98}  & \\textbf{1744.09} &71.06 & 35.60$^{\\dagger}$ \\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[!ht]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{1.5pt}\n\\begin{tabular}{ll\n    >{\\centering\\arraybackslash}m{1.05cm}\n    >{\\centering\\arraybackslash}m{1.05cm}\n    >{\\centering\\arraybackslash}m{1.3cm}\n    c\n    }\n\\toprule\nModel & LLM & \\makecell{Avg.\\\\Infer.\\\\Time} & \\makecell{Total\\\\Params} & \\makecell{Trainable\\\\Params} & GFLOPs \\\\ \n\\midrule\nLLaVA-v1.5 & Vicuna-13B & 0.81s & 13.35B & 13.05B & 16894.72 \\\\\nLLaVA-v1.5 & Vicuna-7B&\t0.44s&\t7.06B&\t6.76B&\t8877.05 \\\\\nInternVL-Chat&\tVicuna-13B&\t0.83s&\t18.96B&\t13.06B&\t23334.93 \\\\\nMoE-LLaVA &\tPhi2-2.7B&\t\\textbf{0.41s}&\t5.61B&\t5.30B&\t\\textbf{7338.97}\\\\\nMoCHA&\tPhi2-2.7B&\t0.57s&\t\\textbf{4.97B}&\t\\textbf{4.07B}&\t12014.64\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Quantitative comparison of MoCHA and other VLLMs.}\n\\label{tab:quantitive}\n\\end{table}", "caption": "\\caption{Quantitative comparison of MoCHA and other VLLMs.}", "label": "\\label{tab:quantitive}", "tabular": "\\begin{tabular}{ll\n    >{\\centering\\arraybackslash}m{1.05cm}\n    >{\\centering\\arraybackslash}m{1.05cm}\n    >{\\centering\\arraybackslash}m{1.3cm}\n    c\n    }\n\\toprule\nModel & LLM & \\makecell{Avg.\\\\Infer.\\\\Time} & \\makecell{Total\\\\Params} & \\makecell{Trainable\\\\Params} & GFLOPs \\\\ \n\\midrule\nLLaVA-v1.5 & Vicuna-13B & 0.81s & 13.35B & 13.05B & 16894.72 \\\\\nLLaVA-v1.5 & Vicuna-7B&\t0.44s&\t7.06B&\t6.76B&\t8877.05 \\\\\nInternVL-Chat&\tVicuna-13B&\t0.83s&\t18.96B&\t13.06B&\t23334.93 \\\\\nMoE-LLaVA &\tPhi2-2.7B&\t\\textbf{0.41s}&\t5.61B&\t5.30B&\t\\textbf{7338.97}\\\\\nMoCHA&\tPhi2-2.7B&\t0.57s&\t\\textbf{4.97B}&\t\\textbf{4.07B}&\t12014.64\\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[t]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{2.5pt}\n\\begin{tabular}{l!{\\color{black}\\vrule width 0.6pt}cccc}\n\\toprule\n  Method (MLP)& GQA &\\makecell{SQA \\\\IMG}&POPE & \\makecell{MMB\\\\EN} \\\\ \n\\midrule\nSigLIP& 59&\t64.35&\t84.64&\t59.88 \\\\\n        SigLIP+ConvNeXt &60.78&\t67.18&\t86.67&\t64.95      \\\\\n        SigLIP+CLIP+ConvNeXt&57.21&\t65.39&\t85.72&\t58.42 \\\\\n        SigLIP+DINOv2+ConvNeXt+CLIP&\\textbf{61.86}&\t\\textbf{69.01}&\t\\textbf{88.47}&\\textbf{66.51}\\\\       \n\\bottomrule\n\\end{tabular}\n\\caption{Ablation study only on different vision encoder combinations.}\n\\label{tab:group}\n\\end{table}", "caption": "\\caption{Ablation study only on different vision encoder combinations.}", "label": "\\label{tab:group}", "tabular": "\\begin{tabular}{l!{\\color{black}\\vrule width 0.6pt}cccc}\n\\toprule\n  Method (MLP)& GQA &\\makecell{SQA \\\\IMG}&POPE & \\makecell{MMB\\\\EN} \\\\ \n\\midrule\nSigLIP& 59&\t64.35&\t84.64&\t59.88 \\\\\n        SigLIP+ConvNeXt &60.78&\t67.18&\t86.67&\t64.95      \\\\\n        SigLIP+CLIP+ConvNeXt&57.21&\t65.39&\t85.72&\t58.42 \\\\\n        SigLIP+DINOv2+ConvNeXt+CLIP&\\textbf{61.86}&\t\\textbf{69.01}&\t\\textbf{88.47}&\\textbf{66.51}\\\\       \n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[htbp]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{2.5pt}\n\\begin{tabular}{l!{\\color{black}\\vrule width 0.6pt}cccc}\n\\toprule\n  Method (MoEC) & GQA &\\makecell{SQA \\\\IMG}&POPE & \\makecell{MMB\\\\EN} \\\\ \n\\midrule\nSigLIP& 56.77&\t64.6&\t82.85&\t57.65 \\\\\n        SigLIP+ConvNeXt &57.77&\t67.92&\t86.72&63.49      \\\\\n        SigLIP+CLIP+ConvNeXt&59.1&\t66.58&\t87.06&\t60.83 \\\\\n        SigLIP+DINOv2+ConvNeXt+CLIP&\\textbf{63.35}&\t\\textbf{70.92}&\t\\textbf{89.01}&\\textbf{68.03}\\\\       \n\\bottomrule\n\\end{tabular}\n\\caption{Ablation study of different vision encoder combinations under MoEC settings.}\n\\label{tab:moe}\n\\end{table}", "caption": "\\caption{Ablation study of different vision encoder combinations under MoEC settings.}", "label": "\\label{tab:moe}", "tabular": "\\begin{tabular}{l!{\\color{black}\\vrule width 0.6pt}cccc}\n\\toprule\n  Method (MoEC) & GQA &\\makecell{SQA \\\\IMG}&POPE & \\makecell{MMB\\\\EN} \\\\ \n\\midrule\nSigLIP& 56.77&\t64.6&\t82.85&\t57.65 \\\\\n        SigLIP+ConvNeXt &57.77&\t67.92&\t86.72&63.49      \\\\\n        SigLIP+CLIP+ConvNeXt&59.1&\t66.58&\t87.06&\t60.83 \\\\\n        SigLIP+DINOv2+ConvNeXt+CLIP&\\textbf{63.35}&\t\\textbf{70.92}&\t\\textbf{89.01}&\\textbf{68.03}\\\\       \n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[htbp]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{2.5pt}\n% \\setlength{\\tabcolsep}{4pt}\n% \\renewcommand{\\arraystretch}{1.2}\n\\begin{tabular}{l!{\\color{black}\\vrule width 0.6pt}cccc}\n\\toprule\nMethod (MoEC + Attention) & GQA & \\makecell{SQA\\\\IMG} & POPE & \\makecell{MMB\\\\EN} \\\\ \n\\midrule\nSigLIP & 56.11 & 62.76 & 82.27 & 57.90 \\\\\nSigLIP+ConvNeXt & 57.02 & 63.97 & 87.84 & 61.06 \\\\\nSigLIP+CLIP+ConvNeXt & 60.34 & 68.73 & 88.07 & 62.11 \\\\\n\\rowcolor{lightblue} \\multicolumn{1}{l!{\\color{black}\\vrule width 0.8pt}} {SigLIP+DINOv2+ConvNeXt+CLIP }& \\textbf{65.24} & \\textbf{73.24} & \\textbf{89.95} & \\textbf{69.67} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Ablation Study of MoCHA with different vision encoder combinations. Settings for results in Table~\\ref{tab:22} are highlighted in \\colorbox{lightblue}{blue}.}\n\\label{tab:graph}\n\\end{table}", "caption": "\\caption{Ablation Study of MoCHA with different vision encoder combinations. Settings for results in Table~\\ref{tab:22} are highlighted in \\colorbox{lightblue}{blue}.}", "label": "\\label{tab:graph}", "tabular": "\\begin{tabular}{l!{\\color{black}\\vrule width 0.6pt}cccc}\n\\toprule\nMethod (MoEC + Attention) & GQA & \\makecell{SQA\\\\IMG} & POPE & \\makecell{MMB\\\\EN} \\\\ \n\\midrule\nSigLIP & 56.11 & 62.76 & 82.27 & 57.90 \\\\\nSigLIP+ConvNeXt & 57.02 & 63.97 & 87.84 & 61.06 \\\\\nSigLIP+CLIP+ConvNeXt & 60.34 & 68.73 & 88.07 & 62.11 \\\\\n\\rowcolor{lightblue} \\multicolumn{1}{l!{\\color{black}\\vrule width 0.8pt}} {SigLIP+DINOv2+ConvNeXt+CLIP }& \\textbf{65.24} & \\textbf{73.24} & \\textbf{89.95} & \\textbf{69.67} \\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[htbp]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{3pt}\n% \\setlength{\\tabcolsep}{4pt}\n\\begin{tabular}{l!{\\color{black}\\vrule width 0.6pt}cccc}\n\\toprule\n  Top-$K$ & GQA &\\makecell{SQA \\\\IMG}&POPE & \\makecell{MMB\\\\EN} \\\\ \n\\midrule\n1 &59.86 &68.13 &85.93 & 65.59\\\\\n \\rowcolor{lightblue} \\multicolumn{1}{l!{\\color{black}\\vrule width 0.8pt}} 2 & {\\textbf{65.24}}&\t\\textbf{73.24}&\t\\textbf{89.95}&\t\\textbf{69.67} \\\\   \n 3& 62.90&\t70.18&\t88.21&\t67.35\\\\\n 4& 58.32&\t67.64&\t86.02&\t64.97\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Ablation study on the value of Top-$K$ in MoCHA.}\n\\label{tab:expertnum}\n\\end{table}", "caption": "\\caption{Ablation study on the value of Top-$K$ in MoCHA.}", "label": "\\label{tab:expertnum}", "tabular": "\\begin{tabular}{l!{\\color{black}\\vrule width 0.6pt}cccc}\n\\toprule\n  Top-$K$ & GQA &\\makecell{SQA \\\\IMG}&POPE & \\makecell{MMB\\\\EN} \\\\ \n\\midrule\n1 &59.86 &68.13 &85.93 & 65.59\\\\\n \\rowcolor{lightblue} \\multicolumn{1}{l!{\\color{black}\\vrule width 0.8pt}} 2 & {\\textbf{65.24}}&\t\\textbf{73.24}&\t\\textbf{89.95}&\t\\textbf{69.67} \\\\   \n 3& 62.90&\t70.18&\t88.21&\t67.35\\\\\n 4& 58.32&\t67.64&\t86.02&\t64.97\\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[!h]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{2pt}\n% \\setlength{\\tabcolsep}{4pt}\n\\begin{tabular}{l!{\\color{black}\\vrule width 0.6pt}l!{\\color{black}\\vrule width 0.6pt}cccc}\n\\toprule\n Vision Model & LLM& GQA &\\makecell{MMB \\\\EN}&MME & \\makecell{MM\\\\Vet} \\\\ \n\\midrule\nDINO&\tPhi2-2.7B&\t\\textbf{61.00}&\t57.82&\t1555.61&\t22.3\\\\\nConvNeXt&\tPhi2-2.7B&\t58.51&\t64.00&\t1604.99\t&24.5\\\\\nCLIP&\tPhi2-2.7B&\t60.12&\t\\textbf{66.92}&\t1600.19\t&25.4\\\\\nSigLIP&\tPhi2-2.7B&\t59.00&\t59.88&\t\\textbf{1622.61}\t&\\textbf{25.7}\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Evaluation performance of individual vision encoders.}\n\\label{tab:single}\n\\end{table}", "caption": "\\caption{Evaluation performance of individual vision encoders.}", "label": "\\label{tab:single}", "tabular": "\\begin{tabular}{l!{\\color{black}\\vrule width 0.6pt}l!{\\color{black}\\vrule width 0.6pt}cccc}\n\\toprule\n Vision Model & LLM& GQA &\\makecell{MMB \\\\EN}&MME & \\makecell{MM\\\\Vet} \\\\ \n\\midrule\nDINO&\tPhi2-2.7B&\t\\textbf{61.00}&\t57.82&\t1555.61&\t22.3\\\\\nConvNeXt&\tPhi2-2.7B&\t58.51&\t64.00&\t1604.99\t&24.5\\\\\nCLIP&\tPhi2-2.7B&\t60.12&\t\\textbf{66.92}&\t1600.19\t&25.4\\\\\nSigLIP&\tPhi2-2.7B&\t59.00&\t59.88&\t\\textbf{1622.61}\t&\\textbf{25.7}\\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[!ht]\n\\centering\n  \\footnotesize\n\\setlength{\\tabcolsep}{1mm}\n\\begin{tabular}{l!{\\color{black}\\vrule width 0.6pt}cc}\n\\toprule\nConfig & Stage I & Stage II \\\\\n\\midrule\nExperts              & --       & --    \\\\\nTop-k                & --       & 2     \\\\ \\midrule\n% Deepspeed            & Zero2    & Zero3\\\\            \nImage encoder        & \\multicolumn{2}{c}{SigLIP, DINOv2, ConvNeXt, CLIP} \\\\\nFeature select layer & \\multicolumn{2}{c}{-2} \\\\\nImage projector      & \\multicolumn{2}{c}{MoECs (Multiple MoEC units)} \\\\\nImage resolution &  \\multicolumn{2}{c}{e.g., 384, 1024, 336}\\\\\nEpoch                & \\multicolumn{2}{c}{1} \\\\\nLearning rate        &  1e-3     &  2e-5  \\\\\nLearning rate schdule& \\multicolumn{2}{c}{Cosine} \\\\\nWeight decay         & \\multicolumn{2}{c}{0.0} \\\\\nText max length      & \\multicolumn{2}{c}{2560} \\\\\nBatch size per GPU   &   4      &     2   \\\\\nGPU                  & \\multicolumn{2}{c}{8 $\\times$ L40-48GB} \\\\\nPrecision            & \\multicolumn{2}{c}{Bf16} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Training hyperparameters.}\n\\label{training_hypers}\n\\end{table}", "caption": "\\caption{Training hyperparameters.}", "label": "\\label{training_hypers}", "tabular": "\\begin{tabular}{l!{\\color{black}\\vrule width 0.6pt}cc}\n\\toprule\nConfig & Stage I & Stage II \\\\\n\\midrule\nExperts              & --       & --    \\\\\nTop-k                & --       & 2     \\\\ \\midrule\n% Deepspeed            & Zero2    & Zero3\\\\            \nImage encoder        & \\multicolumn{2}{c}{SigLIP, DINOv2, ConvNeXt, CLIP} \\\\\nFeature select layer & \\multicolumn{2}{c}{-2} \\\\\nImage projector      & \\multicolumn{2}{c}{MoECs (Multiple MoEC units)} \\\\\nImage resolution &  \\multicolumn{2}{c}{e.g., 384, 1024, 336}\\\\\nEpoch                & \\multicolumn{2}{c}{1} \\\\\nLearning rate        &  1e-3     &  2e-5  \\\\\nLearning rate schdule& \\multicolumn{2}{c}{Cosine} \\\\\nWeight decay         & \\multicolumn{2}{c}{0.0} \\\\\nText max length      & \\multicolumn{2}{c}{2560} \\\\\nBatch size per GPU   &   4      &     2   \\\\\nGPU                  & \\multicolumn{2}{c}{8 $\\times$ L40-48GB} \\\\\nPrecision            & \\multicolumn{2}{c}{Bf16} \\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[!ht]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{2.5pt}\n\\begin{tabular}{@{}lccccc@{}}\n\\toprule\nVision Model & Total Tokens & \\makecell{Avg.Infer.Time\\\\(MLP vs MoEC)} & \\makecell{Total Params \\\\ (MLP vs MoEC)} & \\makecell{Trainable Params \\\\ (MLP vs MoEC)} & \\makecell{GFLOPs \\\\ (MLP vs MoEC)} \\\\\n\\midrule\nSigLIP & 440 & 0.34s vs 0.34s & 3.38B vs 3.41B & 2.78B vs 2.81B & 3000.09 vs 3008.17 \\\\\nSigLIP+ConvNeXt & 540 & 0.41s vs 0.31s & 4.23B vs 4.29B & 3.64B vs 3.69B & 4244.34 vs 4254.26 \\\\\nSigLIP+CLIP+ConvNeXt & 1116 & 0.45s vs 0.39s & 4.24B vs 4.33B & 3.65B vs 3.73B & 8021.32 vs 8048.25 \\\\\nSigLIP+DINOv2+ConvNeXt+CLIP & 1692 & 0.58s vs 0.57s & 4.86B vs 4.97B & 3.96B vs 4.07B & 11983.58 vs 12014.64 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Ablation study on inference efficiency and model complexity across single and multi-encoder variants.}\n\\label{tab:ablation_efficiency}\n\\end{table*}", "caption": "\\caption{Ablation study on inference efficiency and model complexity across single and multi-encoder variants.}", "label": "\\label{tab:ablation_efficiency}", "tabular": "\\begin{tabular}{@{}lccccc@{}}\n\\toprule\nVision Model & Total Tokens & \\makecell{Avg.Infer.Time\\\\(MLP vs MoEC)} & \\makecell{Total Params \\\\ (MLP vs MoEC)} & \\makecell{Trainable Params \\\\ (MLP vs MoEC)} & \\makecell{GFLOPs \\\\ (MLP vs MoEC)} \\\\\n\\midrule\nSigLIP & 440 & 0.34s vs 0.34s & 3.38B vs 3.41B & 2.78B vs 2.81B & 3000.09 vs 3008.17 \\\\\nSigLIP+ConvNeXt & 540 & 0.41s vs 0.31s & 4.23B vs 4.29B & 3.64B vs 3.69B & 4244.34 vs 4254.26 \\\\\nSigLIP+CLIP+ConvNeXt & 1116 & 0.45s vs 0.39s & 4.24B vs 4.33B & 3.65B vs 3.73B & 8021.32 vs 8048.25 \\\\\nSigLIP+DINOv2+ConvNeXt+CLIP & 1692 & 0.58s vs 0.57s & 4.86B vs 4.97B & 3.96B vs 4.07B & 11983.58 vs 12014.64 \\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[htbp]\n\\centering\n\\footnotesize\n\\begin{tabularx}{\\columnwidth}{l X}\n\\toprule\n\\multicolumn{2}{l}{\\textbf{Visual input example, Question and Image:}} \\\\\n\\midrule\n\\multicolumn{2}{c}{\\includegraphics[width=0.8\\linewidth,height=4cm,keepaspectratio]{v1_130.jpg}} \\\\\n\\addlinespace[2pt]\n\\textbf{User} & Does the man ride a horse?\\\\\n\\addlinespace[2pt]\n\\textbf{LLaVA (Vicuna-7B)} & No, the man does not ride a horse. He is riding on the back of an elephant. \\bmark \\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Phi2-2.7B)} & No, the man does not ride a horse. He is riding an elephant. \\bmark \\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Vicuna-7B)} & No, the man does not ride a horse. He is riding an elephant. \\bmark\\\\\n\\midrule\n\\multicolumn{2}{c}{\\includegraphics[width=0.8\\linewidth,height=4cm,keepaspectratio]{v1_122.jpg}} \\\\\n\\addlinespace[2pt]\n\\textbf{User} & Is the curtain on the right side or on the left of the picture? \\\\\n\\addlinespace[2pt]\n\\textbf{LLaVA (Vicuna-7B)} & The curtain is on the left side of the picture. \\bmark \\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Phi2-2.7B)} & The curtain is on the left side of the picture. \\bmark \\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Vicuna-7B)} & The curtain is on the left side of the picture. \\bmark\\\\\n\\bottomrule\n\\end{tabularx}\n\\caption{Example comparing LLaVA and MoCHA's visual reasoning capabilities in understanding fundamental secene.}\n\\label{fig:example1}\n\\end{table}", "caption": "\\caption{Example comparing LLaVA and MoCHA's visual reasoning capabilities in understanding fundamental secene.}", "label": "\\label{fig:example1}", "tabular": "\\begin{tabularx}{\\columnwidth}{l X}\n\\toprule\n\\multicolumn{2}{l}{\\textbf{Visual input example, Question and Image:}} \\\\\n\\midrule\n\\multicolumn{2}{c}{\\includegraphics[width=0.8\\linewidth,height=4cm,keepaspectratio]{v1_130.jpg}} \\\\\n\\addlinespace[2pt]\n\\textbf{User} & Does the man ride a horse?\\\\\n\\addlinespace[2pt]\n\\textbf{LLaVA (Vicuna-7B)} & No, the man does not ride a horse. He is riding on the back of an elephant. \\bmark \\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Phi2-2.7B)} & No, the man does not ride a horse. He is riding an elephant. \\bmark \\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Vicuna-7B)} & No, the man does not ride a horse. He is riding an elephant. \\bmark\\\\\n\\midrule\n\\multicolumn{2}{c}{\\includegraphics[width=0.8\\linewidth,height=4cm,keepaspectratio]{v1_122.jpg}} \\\\\n\\addlinespace[2pt]\n\\textbf{User} & Is the curtain on the right side or on the left of the picture? \\\\\n\\addlinespace[2pt]\n\\textbf{LLaVA (Vicuna-7B)} & The curtain is on the left side of the picture. \\bmark \\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Phi2-2.7B)} & The curtain is on the left side of the picture. \\bmark \\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Vicuna-7B)} & The curtain is on the left side of the picture. \\bmark\\\\\n\\bottomrule\n\\end{tabularx}", "subtables": []}, {"original": "\\begin{table}[htbp]\n\\centering\n\\footnotesize\n\\begin{tabularx}{\\columnwidth}{l X}\n\\toprule\n\\multicolumn{2}{l}{\\textbf{Visual input example, Tricky Question and Image:}} \\\\\n\\midrule\n\\multicolumn{2}{c}{\\includegraphics[width=0.8\\linewidth,height=4cm,keepaspectratio]{v1_67.png}} \\\\\n\\addlinespace[2pt]\n\\textbf{User} & Which one is unsweetened?\\\\\n\\addlinespace[2pt]\n\\textbf{LLaVA (Vicuna-7B)} & The unsweetened milk is on the left side of the image. \\bmark\\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Phi2-2.7B)} & \\textcolor{red}{The unsweetened milk is the one that is not sweetened.} \\xmark \\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Vicuna-7B)} & The unsweetened milk is the one on the left. \\bmark\\\\\n\\midrule\n\\multicolumn{2}{c}{\\includegraphics[width=0.8\\linewidth,height=4cm,keepaspectratio]{v1_28.jpg}} \\\\\n\\addlinespace[2pt]\n\\textbf{User} & What is the number displayed on the motorcycle ridden by the motorcyclist wearing a yellow suit?\\\\\n\\addlinespace[2pt]\n\\textbf{LLaVA (Vicuna-7B)} & The number displayed on the motorcycle ridden by the motorcyclist wearing a yellow suit is 19. \\xmark\\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Phi2-2.7B)} & \\textcolor{red}{The number displayed on the motorcycle on the right is 21. } \\xmark\\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Vicuna-7B)} &The motorcyclist wearing a yellow suit is riding a motorcycle with the number 16 on it.\\bmark\\\\\n\\bottomrule\n\\end{tabularx}\n\\caption{Example comparing LLaVA and MoCHA's visual reasoning capabilities in understanding multi-object recognition. }\n\\label{fig:example2}\n\\end{table}", "caption": "\\caption{Example comparing LLaVA and MoCHA's visual reasoning capabilities in understanding multi-object recognition. }", "label": "\\label{fig:example2}", "tabular": "\\begin{tabularx}{\\columnwidth}{l X}\n\\toprule\n\\multicolumn{2}{l}{\\textbf{Visual input example, Tricky Question and Image:}} \\\\\n\\midrule\n\\multicolumn{2}{c}{\\includegraphics[width=0.8\\linewidth,height=4cm,keepaspectratio]{v1_67.png}} \\\\\n\\addlinespace[2pt]\n\\textbf{User} & Which one is unsweetened?\\\\\n\\addlinespace[2pt]\n\\textbf{LLaVA (Vicuna-7B)} & The unsweetened milk is on the left side of the image. \\bmark\\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Phi2-2.7B)} & \\textcolor{red}{The unsweetened milk is the one that is not sweetened.} \\xmark \\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Vicuna-7B)} & The unsweetened milk is the one on the left. \\bmark\\\\\n\\midrule\n\\multicolumn{2}{c}{\\includegraphics[width=0.8\\linewidth,height=4cm,keepaspectratio]{v1_28.jpg}} \\\\\n\\addlinespace[2pt]\n\\textbf{User} & What is the number displayed on the motorcycle ridden by the motorcyclist wearing a yellow suit?\\\\\n\\addlinespace[2pt]\n\\textbf{LLaVA (Vicuna-7B)} & The number displayed on the motorcycle ridden by the motorcyclist wearing a yellow suit is 19. \\xmark\\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Phi2-2.7B)} & \\textcolor{red}{The number displayed on the motorcycle on the right is 21. } \\xmark\\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Vicuna-7B)} &The motorcyclist wearing a yellow suit is riding a motorcycle with the number 16 on it.\\bmark\\\\\n\\bottomrule\n\\end{tabularx}", "subtables": []}], "figure": [{"original": "\\begin{figure*}[!htbp]\n  \\centering\n  \\includegraphics[width=0.8\\linewidth]{architeture1.pdf}\n  \\caption{Architecture of MoCHA. MoCHA integrates multiple vision encoders (i.e., CLIP, SigLIP, ConvNeXt, and DINOv2), and consists of a Mixture of Experts Connectors (MoECs) module and Hierarchical Group Attention (HGA) module  .\n  % MoCHA consists of three main components: multiple vision encoders (i.e., CLIP, SigLIP, ConvNeXt, and DINOv2), MoECs (Four Mixture of Experts Connectors), and HGA (Hierarchical Group Attention).\n  }\n       \\label{fig:arch}\n\\end{figure*}", "caption": "\\caption{Architecture of MoCHA. MoCHA integrates multiple vision encoders (i.e., CLIP, SigLIP, ConvNeXt, and DINOv2), and consists of a Mixture of Experts Connectors (MoECs) module and Hierarchical Group Attention (HGA) module  .\n  % MoCHA consists of three main components: multiple vision encoders (i.e., CLIP, SigLIP, ConvNeXt, and DINOv2), MoECs (Four Mixture of Experts Connectors), and HGA (Hierarchical Group Attention).\n  }", "label": "\\label{fig:arch}", "subfigures": [], "figure_paths": ["architeture1.pdf"]}, {"original": "\\begin{figure}[htbp]\n  \\centering\n  \\includegraphics[width=0.87\\linewidth]{MoE.pdf}\n  \\caption{Sparse Top-$K$ MoEC block. Replacing MLP connector with MoEC for visual-text token alignment.}\n       \\label{fig:moe}\n\\end{figure}", "caption": "\\caption{Sparse Top-$K$ MoEC block. Replacing MLP connector with MoEC for visual-text token alignment.}", "label": "\\label{fig:moe}", "subfigures": [], "figure_paths": ["MoE.pdf"]}, {"original": "\\begin{figure}[!ht]\n  \\centering\n  \\includegraphics[width=0.9\\linewidth]{MASK11.pdf}\n  \\caption{Intra-group Attention. Each model selects and aggregates the Top-$M$ features for its tokens within the same group.}\n       \\label{fig:mask1}\n\\end{figure}", "caption": "\\caption{Intra-group Attention. Each model selects and aggregates the Top-$M$ features for its tokens within the same group.}", "label": "\\label{fig:mask1}", "subfigures": [], "figure_paths": ["MASK11.pdf"]}, {"original": "\\begin{figure}[!htbp]\n  \\centering\n  \\includegraphics[width=0.85\\linewidth]{MASK22.pdf}\n  \\caption{Inter-group Attention. SigLIP selects and aggregates the Top-$N$ features for each token across different models (groups).}\n       \\label{fig:mask2}\n\\end{figure}", "caption": "\\caption{Inter-group Attention. SigLIP selects and aggregates the Top-$N$ features for each token across different models (groups).}", "label": "\\label{fig:mask2}", "subfigures": [], "figure_paths": ["MASK22.pdf"]}, {"original": "\\begin{figure}[!htbp]\n\\centering\n\\includegraphics[width=0.89\\linewidth]{bar_grouped_vertical.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.\n\\caption{Ablation of Channel vs. Sequential Concatenation for MoECs with four encoders.}\n\\label{concat}\n\\end{figure}", "caption": "\\caption{Ablation of Channel vs. Sequential Concatenation for MoECs with four encoders.}", "label": "\\label{concat}", "subfigures": [], "figure_paths": ["bar_grouped_vertical.pdf"]}, {"original": "\\begin{figure}[ht]\n  \\centering\n  \\includegraphics[width=\\columnwidth]{train.pdf}\n  \\caption{Training stages of MoCHA.}\n       \\label{fig:tain}\n\\end{figure}", "caption": "\\caption{Training stages of MoCHA.}", "label": "\\label{fig:tain}", "subfigures": [], "figure_paths": ["train.pdf"]}], "equations": ["\\begin{equation}\n  \\begin{aligned}\n    \\textbf{SigLIP}(I) &= e_s \\in \\mathbb{R}^{s\\times D_s}, \\\\\n    \\textbf{DINOv2}(I) &= e_d \\in \\mathbb{R}^{d\\times D_d}, \\\\\n    \\textbf{ConvNeXt}(I) &= e_n \\in \\mathbb{R}^{n\\times D_n}, \\\\\n    \\textbf{CLIP}(I) &= e_c \\in \\mathbb{R}^{c\\times D_c},\n  \\end{aligned}\n\\end{equation}", "\\begin{aligned}\n    \\textbf{SigLIP}(I) &= e_s \\in \\mathbb{R}^{s\\times D_s}, \\\\\n    \\textbf{DINOv2}(I) &= e_d \\in \\mathbb{R}^{d\\times D_d}, \\\\\n    \\textbf{ConvNeXt}(I) &= e_n \\in \\mathbb{R}^{n\\times D_n}, \\\\\n    \\textbf{CLIP}(I) &= e_c \\in \\mathbb{R}^{c\\times D_c},\n  \\end{aligned}", "\\begin{equation}\n    e_{s,h}=\\text{MLP}(e_s)\\in \\mathbb{R}^{s\\times D},\n\\end{equation}", "\\begin{equation}\n  W_s=\\text{Softmax}(\\text{Linear}(e_s))\\in \\mathbb{R}^{s\\times E},\n\\end{equation}", "\\begin{equation}\n W_{s,K}=\\text{Softmax}(\\text{Top}K(W_s))\\in \\mathbb{R}^{s\\times K},\n\\end{equation}", "\\begin{equation}\ne_{s,h}=\\sum_{i}^{K}W_{s,K}^{i}\\circ \\text{MLP}_i(e_s)\\in \\mathbb{R}^{s\\times D}.\n\\end{equation}", "\\begin{equation}\n  \\begin{split}\n    X_{\\text{in}} = (\\text{Concatenate}[e_{s,h}, e_{d,h},\n    e_{n,h}, e_{c,h}], \\text{ dim=token}) \n    \\\\\\in \\mathbb{R}^{N\\times D},\n  \\end{split}\n\\end{equation}", "\\begin{equation}\nI_{\\text{intra},s,M}=\\text{Top} M(\\text{sim}(e_{s,h}, e_{s,h})\\odot (1-I)).\n\\end{equation}", "\\begin{equation}\n  I_{\\text{inter},s,N}=\\text{Top}N(\\text{sim}(e_{s,h},[e_{n,h},e_{c,h},e_{d,h}])).\n\\end{equation}", "\\begin{equation}\n\\begin{split}\n\\text{gate}&=\\text{Sigmoid}[10*(X_{\\text{in,agg}}-X_{\\text{in}}-0.2)],\\\\\nX_{\\text{out}}&=(1-\\text{gate})*X_{\\text{in}}+\\text{gate}*X_{\\text{in,agg}}.\n\\end{split}\n\\end{equation}", "\\begin{equation}\n  \\begin{split}\n   \\mathcal L=\\mathcal L_{ce}+\\alpha_{b} (\\mathcal L_{b,s}+\\mathcal L_{b,d}+\\mathcal L_{b,n}+\\mathcal L_{b,c})\\\\\n   +\\alpha_{z} (\\mathcal L_{z,s}+\\mathcal L_{z,d}+\\mathcal L_{z,n}+\\mathcal L_{z,c}),\n  \\end{split}\n\\end{equation}"], "algorithm": [], "sections": {"Introduction": {"content": "\nVision Large Language Models (VLLMs)~\\cite{liu2024llavanext,wang2024qwen2vlenhancingvisionlanguagemodels, bai2025qwen25vltechnicalreport} have shown promising results in multimodal reasoning tasks by integrating visual encoders with Large Language Models (LLM)~\\cite{gpt,openai2024gpt4o,touvron2023llama}.\nHowever, the large model size and extensive training data present significant computational challenges. Additionally, excessive early reliance on language in VLLMs may act as a shortcut~\\cite{geirhos2020shortcut,yuksekgonul2023visionlanguagemodelsbehavelike} that results in suboptimal learning of effective visual representations. \nTherefore, exploring a VLLM that balances efficient visual processing and performance is a crucial topic.\n\nPrevious works have focused on strengthening vision encoders by utilizing extensive and higher-quality visual instruction data~\\cite{zheng2023judgingllmasajudgemtbenchchatbot}, scaling up model size~\\cite{chen2024internvl}, or decomposing images into low-resolution patches~\\cite{shi2024needlargervisionmodels}.\nWhile effective, these approaches are inherently limited by high computational costs and the loss of important details, such as small objects, which in turn can result in hallucinations.\nFurthermore, Cambrian-1~\\cite{tong2024cambrian1fullyopenvisioncentric}, designed with a vision centric approach, introduces a novel paradigm for vision-language fusion through its combination of multiple vision encoders and spatial vision aggregator (SVA), but faces difficulties due to the lack of dynamic feature fusion and shortcomings in fine-grained perception. \nSpecifically, visual features are inherently high-dimensional and uncertain, encompassing objects, scenes, attributes, and spatial relationships. A single vision encoder, or a limited set thereof, is insufficient to comprehensively and adaptively capture these diverse aspects.\nSparse Mixture of Experts (MoE)~\\cite{fedus2022switch} has emerged as an efficient solution that dynamically activates only a subset of expert networks for each input to improve scalability and specialization.\nAlthough MoE has been widely adopted in state-of-the-art open-source LLMs, its potential remains largely underexplored in the design of connectors and the broader architecture of VLLMs.\n\n\nTo address the above limitations of existing vision encoders and enhance fine-grained visual understanding, we propose \\textbf{\\textit{MoCHA}}, a novel framework integrates multiple vision backbones under a sparse \\textbf{\\textit{M}}ixture \\textbf{\\textit{o}}f Experts \\textbf{\\textit{C}}onnector (MoECs) module and a \\textbf{\\textit{H}}ierarchical Group \\textbf{\\textit{A}}ttention (HGA) component for vision-extensive reasoning tasks.\nSpecifically, MoCHA consists of multiple vision encoders connected by a mixture of experts module and hierarchical group attention, as illustrated in Figure~\\ref{fig:arch}. \nFirst, we leverage four distinct yet complementary vision backbones, including CLIP~\\cite{radford2021learningtransferablevisualmodels}, SigLIP~\\cite{zhai2023sigmoidlosslanguageimage}, DINOv2~\\cite{oquab2024dinov2learningrobustvisual}, and ConvNeXt~\\cite{liu2022convnet2020s}, to extract diverse and robust image features. However, the efficient integration of these heterogeneous visual signals into a unified vision-language framework remains a challenge. To address this, we propose the mixture of experts connectors (MoECs) module, in which each connector\nis implemented as a Top-K sparsely-gated MoE for dynamic expert selection across visual dimensions. This facilitates efficient cross-modal interaction and reduces training costs. \nSequential concatenation of features from multiple vision encoders along the token dimension often leads to redundancy and feature overlap. We further design hierarchical group attention (HGA), which fuses features through intra- and inter-group attention. By adaptive gating mechanism, we dynamically balance the aggregated and original features to produce the image representation without additional parameters.\nOur MoCHA is fully trained on open-source datasets and demonstrates superior performance compared to other VLLMs of similar size. In detail, we implement MoCHA with a two-stage training process on Phi2-2.7B and Vicuna-7B LLMs, achieving higher scores than existing open-weight VLLMs on mainstream vision-language benchmarks. Notably, our 3B-sized MoCHA shows a 3.25\\% reduction in hallucination on POPE and delivers a 153-point improvement on general visual tasks on MME, outperforming the larger CuMo-7B model~\\cite{li2024cumoscalingmultimodalllm}. This demonstrates the efficacy of our MoCHA in mitigating\nVLM hallucinations and enhancing general perceptual abilities. We further conduct ablation studies to investigate the contribution of each component to overall performance.\n\n\n", "appendix": false}, "Related Work": {"content": "\n\\subsection{Large Pre-trained Vision Models}\nThe advent of pre-trained Vision Transformers (ViT)~\\cite{dosovitskiy2021imageworth16x16words} has significantly propelled the advancement of computer vision. As the original CLIP adopted by conventional visual instruction tuning approaches is trained on noisy image-text pairs, it exhibits specific visual shortcomings, and thus stronger backbones have been introduced to LLMs. For example, SigLIP introduced pairwise sigmoid loss during training, enabling the vision encoder to demonstrate more advanced visual perception capabilities. Some concurrent works have leveraged external assistance from vision-only, self-supervised models such as DINOv2, MoCo-v3~\\cite{he2021maskedautoencodersscalablevision}, and other advanced frameworks ~\\cite{caron2021emergingpropertiesselfsupervisedvision,chen2024internvl}, as well as domain-specific expert models. Additionally, ConvNeXt contributes to this progress by incorporating a purely convolutional backbone that achieves Transformer-level performance with improved computational efficiency. Collectively, these approaches foster robust multimodal integration and significantly enhance visual reasoning capabilities in contemporary VLLM frameworks by leveraging complementary vision encoders to enrich visual representations and improve fine-grained perception.\n\\subsection{Mixture of Experts}\nMixture of Experts (MoE) proposes a group of expert networks to handle specific tasks, with a gating network selecting the appropriate experts. Subsequent research on language MoE models has further expanded large MoE-based language models, improving expert stability and load balancing. ST-MoE~\\cite{zoph2022stmoedesigningstabletransferable} employs load-balancing loss and router-z loss to ensure an even distribution of experts. The recently popular DeepSeek-V3 adopts the DeepSeekMoE~\\cite{dai2024deepseekmoeultimateexpertspecialization} architecture, leveraging shared experts to capture common knowledge and reduce redundancy in routed experts. MoE\u2019s success has also extended to the vision domain. LIMoE~\\cite{mustafa2022multimodalcontrastivelearninglimoe} replaces dense MLP layers with MoE layers in CLIP, improving zero-shot image classification. AdaMV-MoE~\\cite{liu2024adamolefinetuninglargelanguage} introduces an adaptive MoE framework for multi-task learning. Furthermore, CuMo~\\cite{li2024cumoscalingmultimodalllm} integrates Co-upcycled Top-K sparse-gating MoE blocks into the visual encoder, MLP connector, and language model, enhancing VLLM inference performance.\n\n", "appendix": false}, "Methodology": {"content": "\n\\subsection{Overview}\n\\begin{figure*}[!htbp]\n  \\centering\n  \\includegraphics[width=0.8\\linewidth]{architeture1.pdf}\n  \\caption{Architecture of MoCHA. MoCHA integrates multiple vision encoders (i.e., CLIP, SigLIP, ConvNeXt, and DINOv2), and consists of a Mixture of Experts Connectors (MoECs) module and Hierarchical Group Attention (HGA) module  .\n  % MoCHA consists of three main components: multiple vision encoders (i.e., CLIP, SigLIP, ConvNeXt, and DINOv2), MoECs (Four Mixture of Experts Connectors), and HGA (Hierarchical Group Attention).\n  }\n       \\label{fig:arch}\n\\end{figure*}\n  \\centering\n  \\includegraphics[width=0.8\\linewidth]{architeture1.pdf}\n  \\caption{Architecture of MoCHA. MoCHA integrates multiple vision encoders (i.e., CLIP, SigLIP, ConvNeXt, and DINOv2), and consists of a Mixture of Experts Connectors (MoECs) module and Hierarchical Group Attention (HGA) module  .\n  % MoCHA consists of three main components: multiple vision encoders (i.e., CLIP, SigLIP, ConvNeXt, and DINOv2), MoECs (Four Mixture of Experts Connectors), and HGA (Hierarchical Group Attention).\n  }\n       \\label{fig:arch}\n\n\nTo address a critical gap where current VLLMs still struggle---solving vision-extensive multimodal tasks---we present our framework, MoCHA, in this section.\nBuilding upon the mainstream LLaVA architecture, MoCHA moves beyond the original CLIP encoder by integrating a diverse set of vision encoders, spanning different architectures and pre-training objectives. This enables the model to effectively capture and process visual information across varying levels of granularity.\nAs illustrated in Figure~\\ref{fig:arch}, our framework consists of multiple vision encoders (i.e., OpenAICLIP ViT-L/14@336, SigLIP ViT-L/16@384, OpenCLIP ConvNeXt-XXL@1024 and DINOv2 ViT-L/14@336) connected by a mixture of experts module and hierarchical group attention.\n\n\\subsection{Selection of Vision Encoders}\nAlthough language-supervised models outperform self-supervised and other models across all benchmark categories, Cambrian-1~\\cite{tong2024cambrian1fullyopenvisioncentric} highlights that well-trained self-supervised models like DINOv2 are capable of achieving competitive performance in vision-centric tasks. In particular, high-resolution models excel in chart-related and other vision-centric benchmarks while also demonstrating robust performance across both general VQA and knowledge-based VQA. While ViTs remain the dominant architecture, ConvNet-based models such as OpenCLIP ConvNeXt are well-suited for high-resolution image processing~\\cite{vishniakov2024convnetvstransformersupervised} delivering outstanding results on OCR, chart interpretation, and other vision-centric benchmarks. \n\nEach vision encoder excels in different aspects of VLLM performance. More analysis on the vision encoders can be found in the \\textbf{Appendix A}.\nSigLIP consistently excels in vision tasks due to its strong cross-modal semantic understanding and stability, and is therefore used as the baseline vision encoder. In this study, we explore the potential of combining CLIP, SigLIP, ConvNeXt, and DINOv2 to leverage their distinctive representations, aiming to enhance the exploitation of heterogeneous visual features. \n\\subsection{Complementary Visual Feature Extraction}\nTo address the challenge of varying input resolutions across different vision encoders, a common approach is to employ interpolation for aligning feature dimensions.\nHowever, this process can result in information loss. To mitigate this, we opt to forgo interpolation and instead ensure that the output features of all encoders share consistent channel dimensions. In our experiments, we apply pooling to the features produced by ConvNeXt-XXL@1024 to standardize the channel dimensions, while maintaining the configurations of the other encoders unchanged. Each vision encoder processes the image $I$I independently:\n\\begin{equation}\n  \\begin{aligned}\n    \\textbf{SigLIP}(I) &= e_s \\in \\mathbb{R}^{s\\times D_s}, \\\\\n    \\textbf{DINOv2}(I) &= e_d \\in \\mathbb{R}^{d\\times D_d}, \\\\\n    \\textbf{ConvNeXt}(I) &= e_n \\in \\mathbb{R}^{n\\times D_n}, \\\\\n    \\textbf{CLIP}(I) &= e_c \\in \\mathbb{R}^{c\\times D_c},\n  \\end{aligned}\n\\end{equation}\\begin{equation}\n  \\begin{aligned}\n    \\textbf{SigLIP}(I) &= e_s \\in \\mathbb{R}^{s\\times D_s}, \\\\\n    \\textbf{DINOv2}(I) &= e_d \\in \\mathbb{R}^{d\\times D_d}, \\\\\n    \\textbf{ConvNeXt}(I) &= e_n \\in \\mathbb{R}^{n\\times D_n}, \\\\\n    \\textbf{CLIP}(I) &= e_c \\in \\mathbb{R}^{c\\times D_c},\n  \\end{aligned}\n\\end{equation}\n  \n    \\textbf{SigLIP}(I) &= e_s \\in \\mathbb{R}^{s\\times D_s}s\\times D_s, \\\\\n    \\textbf{DINOv2}(I) &= e_d \\in \\mathbb{R}^{d\\times D_d}d\\times D_d, \\\\\n    \\textbf{ConvNeXt}(I) &= e_n \\in \\mathbb{R}^{n\\times D_n}n\\times D_n, \\\\\n    \\textbf{CLIP}(I) &= e_c \\in \\mathbb{R}^{c\\times D_c}c\\times D_c,\n  \n\nhere, $e_s$e_s, $e_d$e_d, $e_n$e_n and $e_c$e_c represent the embeddings of image $I$I generated by the four vision encoders. $s$s, $d$d, $n$n and $c$c denote the number of tokens for each encoder, and $D_s$D_s, $D_d$D_d, $D_n$D_n and $D_c$D_c represents the dimension of the feature channel.\n\n\\subsection{Mixture of Experts Connectors (MoECs)}\nThe output of LLMs is typically semantically uniform and exhibits minimal variation across dimensions, which renders specialized experts unnecessary. In contrast, vision models generate diverse information across multiple dimensions, such as objects, scenes, attributes, and spatial relationships, introducing greater uncertainty and variability. This complexity makes them well-suited for processing by multiple experts, each specializing in different visual aspects.\n\\begin{figure}[htbp]\n  \\centering\n  \\includegraphics[width=0.87\\linewidth]{MoE.pdf}\n  \\caption{Sparse Top-$K$ MoEC block. Replacing MLP connector with MoEC for visual-text token alignment.}\n       \\label{fig:moe}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.87\\linewidth]{MoE.pdf}\n  \\caption{Sparse Top-$K$ MoEC block. Replacing MLP connector with MoEC for visual-text token alignment.}\n       \\label{fig:moe}\n\n\nAs illustrated in Figure~\\ref{fig:moe}, we introduce an MoE into the connector of VLLM. We refer to this design as the Mixture of Experts Connector (MoEC).\nMoEC preserves the stability and generality of vision-language architectures while effectively capturing the multi-dimensional and uncertain nature of visual information. \nMoEC retains the core functionality of the connector by projecting visual tokens into the word embedding space and aligning them with textual representations. \n\nBy leveraging the connector\u2019s small scale, flexibility and focus on intermodal fusion, MoEC dynamically selects expert strategies tailored to different visual dimensions, thereby enhancing the efficiency of modality interaction and reducing training complexity. \n\nPrevious mainstream approaches~\\cite{shazeer2017outrageouslylargeneuralnetworks} replace dense MLP blocks with sparsely-gated mixture of experts blocks. As an example, consider the SigLIP vision encoder embedding. Given input $e_s\\in \\mathbb{R}^{s\\times D_s}$e_s\\in \\mathbb{R}^{s\\times D_s}s\\times D_s and an MLP block, the hidden representation is computed as: \n\\begin{equation}\n    e_{s,h}=\\text{MLP}(e_s)\\in \\mathbb{R}^{s\\times D},\n\\end{equation}\\begin{equation}\n    e_{s,h}=\\text{MLP}(e_s)\\in \\mathbb{R}^{s\\times D},\n\\end{equation}\n    e_{s,h}s,h=\\text{MLP}(e_s)\\in \\mathbb{R}^{s\\times D}s\\times D,\n\nTo scale up the model with multiple MLP blocks in parallel, a sparse MoE block incorporates a router network that selects the Top-$K$K experts from a total of $E$E  experts, as illustrated in Figure~\\ref{fig:moe}. The router network uses a linear layer to compute a normalized weight matrix based on the inputs $e_s$e_s, enabling expert selection:\n\\begin{equation}\n  W_s=\\text{Softmax}(\\text{Linear}(e_s))\\in \\mathbb{R}^{s\\times E},\n\\end{equation}\\begin{equation}\n  W_s=\\text{Softmax}(\\text{Linear}(e_s))\\in \\mathbb{R}^{s\\times E},\n\\end{equation}\n  W_s=\\text{Softmax}(\\text{Linear}(e_s))\\in \\mathbb{R}^{s\\times E}s\\times E,\n\nBased on $W_s$W_s, the Top-$K$K experts are selected for each token, and the corresponding weights are re-normalized as:\n\\begin{equation}\n W_{s,K}=\\text{Softmax}(\\text{Top}K(W_s))\\in \\mathbb{R}^{s\\times K},\n\\end{equation}\\begin{equation}\n W_{s,K}=\\text{Softmax}(\\text{Top}K(W_s))\\in \\mathbb{R}^{s\\times K},\n\\end{equation}\n W_{s,K}s,K=\\text{Softmax}(\\text{Top}K(W_s))\\in \\mathbb{R}^{s\\times K}s\\times K,\n\nEach selected expert corresponds to an MLP block, and the final hidden representation is obtained through a re-weighted sum:\n\\begin{equation}\ne_{s,h}=\\sum_{i}^{K}W_{s,K}^{i}\\circ \\text{MLP}_i(e_s)\\in \\mathbb{R}^{s\\times D}.\n\\end{equation}\\begin{equation}\ne_{s,h}=\\sum_{i}^{K}W_{s,K}^{i}\\circ \\text{MLP}_i(e_s)\\in \\mathbb{R}^{s\\times D}.\n\\end{equation}\ne_{s,h}s,h=\\sum_{i}i^{K}KW_{s,K}s,K^{i}i\\circ \\text{MLP}_i(e_s)\\in \\mathbb{R}^{s\\times D}s\\times D.\n\n\nThe hidden representation of SigLIP MoEC maintains the same dimensionality as that of a single dense MLP block. Similarly, the hidden representations for DINOv2 MoEC, ConvNeXt MoEC, and CLIP MoEC are obtained in the same way:\n$e_{d,h}\\in \\mathbb{R}^{d\\times D}$e_{d,h}d,h\\in \\mathbb{R}^{d\\times D}d\\times D, $e_{n,h}\\in \\mathbb{R}^{n\\times D}$e_{n,h}n,h\\in \\mathbb{R}^{n\\times D}n\\times D, $e_{c,h}\\in \\mathbb{R}^{c\\times D}$e_{c,h}c,h\\in \\mathbb{R}^{c\\times D}c\\times D.\n\n\\subsubsection{Sequence Append (Token Dimension): }\nWhile channel concatenation~\\cite{lin2023sphinxjointmixingweights} achieves performance comparable to sequence append~\\cite{liu2024prismervisionlanguagemodelmultitask,kar2024bravebroadeningvisualencoding,fan2024mousipolyvisualexpertvisionlanguagemodels}, it requires strict spatial alignment of encoder outputs, rendering it less compatible with encoders of differing resolutions and architectures. \nSequence append along the token dimension obviates token count alignment and removes the requirement for interpolation, resampling, or padding. As a result, encoders with different resolutions, patch sizes, or feature map dimensions can contribute arbitrary numbers of tokens. In this study, we adopt sequence append, concatenating features from different encoders directly along the token dimension:\n\\begin{equation}\n  \\begin{split}\n    X_{\\text{in}} = (\\text{Concatenate}[e_{s,h}, e_{d,h},\n    e_{n,h}, e_{c,h}], \\text{ dim=token}) \n    \\\\\\in \\mathbb{R}^{N\\times D},\n  \\end{split}\n\\end{equation}\\begin{equation}\n  \\begin{split}\n    X_{\\text{in}} = (\\text{Concatenate}[e_{s,h}, e_{d,h},\n    e_{n,h}, e_{c,h}], \\text{ dim=token}) \n    \\\\\\in \\mathbb{R}^{N\\times D},\n  \\end{split}\n\\end{equation}\n  \n    X_{\\text{in}}\\text{in} = (\\text{Concatenate}[e_{s,h}s,h, e_{d,h}d,h,\n    e_{n,h}n,h, e_{c,h}c,h], \\text{ dim=token}) \n    \\\\\\in \\mathbb{R}^{N\\times D}N\\times D,\n  \n\nwhere $s+d+n+c=N$s+d+n+c=N.\n\\begin{figure}[!ht]\n  \\centering\n  \\includegraphics[width=0.9\\linewidth]{MASK11.pdf}\n  \\caption{Intra-group Attention. Each model selects and aggregates the Top-$M$ features for its tokens within the same group.}\n       \\label{fig:mask1}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.9\\linewidth]{MASK11.pdf}\n  \\caption{Intra-group Attention. Each model selects and aggregates the Top-$M$ features for its tokens within the same group.}\n       \\label{fig:mask1}\n\n\\begin{figure}[!htbp]\n  \\centering\n  \\includegraphics[width=0.85\\linewidth]{MASK22.pdf}\n  \\caption{Inter-group Attention. SigLIP selects and aggregates the Top-$N$ features for each token across different models (groups).}\n       \\label{fig:mask2}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.85\\linewidth]{MASK22.pdf}\n  \\caption{Inter-group Attention. SigLIP selects and aggregates the Top-$N$ features for each token across different models (groups).}\n       \\label{fig:mask2}\n\n\n\n\\subsection{Hierarchical Group Attention}\nWhile sequence append is straightforward to implement, it has limitations, particularly the lack of selective attention to tokens, as all tokens are processed equally, which may introduce redundancy.\nTo address the above challenge, we propose a hierarchical group attention (HGA) that enables adaptive feature fusion via intra- and inter-group attention, allowing the model to selectively focus on visual tokens.\n\n\nAs illustrated in Figure~\\ref{fig:mask1}, output tokens from the four vision encoders (i.e., SigLIP, DINOv2, ConvNeXt and CLIP) are regarded as independent feature groups. The intra-group attention mechanism is used to select the Top-$M$M most salient token features within each group by computing pairwise similarity scores followed by a self-masking operation:\n\\begin{equation}\nI_{\\text{intra},s,M}=\\text{Top} M(\\text{sim}(e_{s,h}, e_{s,h})\\odot (1-I)).\n\\end{equation}\\begin{equation}\nI_{\\text{intra},s,M}=\\text{Top} M(\\text{sim}(e_{s,h}, e_{s,h})\\odot (1-I)).\n\\end{equation}\nI_{\\text{intra},s,M}\\text{intra},s,M=\\text{Top} M(\\text{sim}(e_{s,h}s,h, e_{s,h}s,h)\\odot (1-I)).\n\n\n\nInter-group attention captures semantic correlations across encoders in high-dimensional space, rather than simple feature redundancy. As illustrated in Figure~\\ref{fig:mask2}, it extracts complementary Top-$N$N token information from different encoder features:\n\\begin{equation}\n  I_{\\text{inter},s,N}=\\text{Top}N(\\text{sim}(e_{s,h},[e_{n,h},e_{c,h},e_{d,h}])).\n\\end{equation}\\begin{equation}\n  I_{\\text{inter},s,N}=\\text{Top}N(\\text{sim}(e_{s,h},[e_{n,h},e_{c,h},e_{d,h}])).\n\\end{equation}\n  I_{\\text{inter},s,N}\\text{inter},s,N=\\text{Top}N(\\text{sim}(e_{s,h}s,h,[e_{n,h}n,h,e_{c,h}c,h,e_{d,h}d,h])).\n\n\nHierarchical information from both intra- and inter-group attention is integrated into the aggregated feature representation $X_{in,agg}$X_{in,agg}in,agg for each token. An adaptive gating mechanism is then applied to balance the contributions of $X_{in,agg}$X_{in,agg}in,agg and the original features $X_{in}$X_{in}in:\n\\begin{equation}\n\\begin{split}\n\\text{gate}&=\\text{Sigmoid}[10*(X_{\\text{in,agg}}-X_{\\text{in}}-0.2)],\\\\\nX_{\\text{out}}&=(1-\\text{gate})*X_{\\text{in}}+\\text{gate}*X_{\\text{in,agg}}.\n\\end{split}\n\\end{equation}\\begin{equation}\n\\begin{split}\n\\text{gate}&=\\text{Sigmoid}[10*(X_{\\text{in,agg}}-X_{\\text{in}}-0.2)],\\\\\nX_{\\text{out}}&=(1-\\text{gate})*X_{\\text{in}}+\\text{gate}*X_{\\text{in,agg}}.\n\\end{split}\n\\end{equation}\n\n\\text{gate}&=\\text{Sigmoid}[10*(X_{\\text{in,agg}}\\text{in,agg}-X_{\\text{in}}\\text{in}-0.2)],\\\\\nX_{\\text{out}}\\text{out}&=(1-\\text{gate})*X_{\\text{in}}\\text{in}+\\text{gate}*X_{\\text{in,agg}}\\text{in,agg}.\n\n\n\n\n\n\n\\subsection{Loss Function}\nTo maintain a load balance among experts in each MoEC block, we adopt auxiliary losses based on the language modeling cross-entropy loss. This auxiliary loss consists of a load balancing loss~\\cite{wang2024auxiliarylossfreeloadbalancingstrategy} and a router z-loss~\\cite{zoph2022stmoedesigningstabletransferable}. \nHence, the total loss is:\n\\begin{equation}\n  \\begin{split}\n   \\mathcal L=\\mathcal L_{ce}+\\alpha_{b} (\\mathcal L_{b,s}+\\mathcal L_{b,d}+\\mathcal L_{b,n}+\\mathcal L_{b,c})\\\\\n   +\\alpha_{z} (\\mathcal L_{z,s}+\\mathcal L_{z,d}+\\mathcal L_{z,n}+\\mathcal L_{z,c}),\n  \\end{split}\n\\end{equation}\\begin{equation}\n  \\begin{split}\n   \\mathcal L=\\mathcal L_{ce}+\\alpha_{b} (\\mathcal L_{b,s}+\\mathcal L_{b,d}+\\mathcal L_{b,n}+\\mathcal L_{b,c})\\\\\n   +\\alpha_{z} (\\mathcal L_{z,s}+\\mathcal L_{z,d}+\\mathcal L_{z,n}+\\mathcal L_{z,c}),\n  \\end{split}\n\\end{equation}\n  \n   \\mathcal L=\\mathcal L_{ce}ce+\\alpha_{b}b (\\mathcal L_{b,s}b,s+\\mathcal L_{b,d}b,d+\\mathcal L_{b,n}b,n+\\mathcal L_{b,c}b,c)\\\\\n   +\\alpha_{z}z (\\mathcal L_{z,s}z,s+\\mathcal L_{z,d}z,d+\\mathcal L_{z,n}z,n+\\mathcal L_{z,c}z,c),\n  \n\nwhere $\\mathcal L_{ce}$\\mathcal L_{ce}ce represents the autoregressive language modeling loss, \nwhich computes the cross-entropy of next-token predictions. $\\alpha_{b}$\\alpha_{b}b and $\\alpha_{z}$\\alpha_{z}z denote coefficients for loading balance loss (i.e., $\\mathcal{L}_{b,s}$\\mathcal{L}_{b,s}b,s, $\\mathcal{L}_{b,d}$\\mathcal{L}_{b,d}b,d, $\\mathcal{L}_{b,n}$\\mathcal{L}_{b,n}b,n, $\\mathcal{L}_{b,c}$\\mathcal{L}_{b,c}b,c) and router z-loss (i.e., $\\mathcal{L}_{z,s}$\\mathcal{L}_{z,s}z,s, $\\mathcal{L}_{z,d}$\\mathcal{L}_{z,d}z,d, $\\mathcal{L}_{z,n}$\\mathcal{L}_{z,n}z,n, $\\mathcal{L}_{z,c}$\\mathcal{L}_{z,c}z,c), set to 0.1 and 0.01, respectively, across all experiments. These auxiliary losses are applied within the MoECs.\n\\begin{table*}[!ht]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{2.5pt}\n\\begin{tabular}{@{}!{\\color{white}\\vrule width 0pt}l!{\\color{black}\\vrule width 0.6pt}l!{\\color{black}\\vrule width 0.6pt}cccccccc!{\\color{white}\\vrule width 0pt}@{}}\n\\toprule\nMethod & LLM & GQA & \\makecell{SQA \\\\IMG} & \\makecell{Text \\\\ VQA} & \\makecell{MM\\\\Vet} & POPE & MME & \\makecell{MMB\\\\EN} & MathVista \\\\ \n\\midrule\n% \\multirow{7}{*}{7B-8B Models}  \nQwen-VL-Chat ~\\cite{bai2023qwenvlversatilevisionlanguagemodel}  & Qwen-7B & 57.5 & 68.2 & 61.5 & - & - & 1487.5 & 60.6 & -  \\\\\nInstructBLIP ~\\cite{dai2023instructblipgeneralpurposevisionlanguagemodels} & Vicuna-7B &49.2&60.5& 50.1& 26.2& -& -& 36.0& -\\\\\nLLaVA-LLaMA3~\\cite{contributorsxtuner}&LLaMA3-8B-IT&62.6& 72.9& 59.0& -& 86.4& 1469& \\underline{72.3}& -\\\\\nMini-Gemini~\\cite{li2024minigeminiminingpotentialmultimodality}&Vicuna-7B&-& 65.2& -& 40.8& -& 1523& 69.3& 31.4\\\\\nSPHINX-Intern2~\\cite{liu2025sphinxxscalingdataparameters}  & InternLM2-7B& 56.2 & 70.4& 58.1 & 36.5 & 86.9 & 1260.4 & 57.9 & 35.5 \\\\\nLLaVA-NeXT~\\cite{liu2024llavanext} &Vicuna-7B&64.2& 70.1& 64.9& 43.9& 86.5& 1519& 67.4& 34.6\\\\\nLLaVA-NeXT~\\cite{liu2024llavanext} &Mistral-7B&64.8&72.8&65.7&47.3&86.7&1498&68.7&\\textbf{37.7}\\\\\nLLaVA-v1.5~\\cite{liu2023visual} &Vicuna-7B&62.0& 66.8& 58.2& 30.5& 85.9& 1510.7& 64.3& -\\\\\nVILA~\\cite{lin2024vilapretrainingvisuallanguage} &Vicuna-7B&62.3& 68.2& 64.4& 34.9& 85.5& 1533& 68.9& -\\\\\nCuMo~\\cite{li2024cumoscalingmultimodalllm}& Mistral-7B& 64.9&\t73.9&\t\\textbf{67.0}&\t\\textbf{51.0}&\t86.7&\t1548.6&\t\\textbf{73.0}&\t35.1\\\\\n\\hdashline\n% \\multirow{5}{*}{13B Models}\nInstructBLIP~\\cite{dai2023instructblipgeneralpurposevisionlanguagemodels} &Vicuna-13B&49.5&63.1&50.7&25.6&78.9&1212.8&-&-\\\\\nLLaVA-v1.5~\\cite{liu2023visual}&Vicuna-13B&63.3&71.6&61.3&35.4&85.9&1531.3&67.7&27.6\\\\\nMini-Gemini~\\cite{li2024minigeminiminingpotentialmultimodality} &Vicuna-13B& -&65.9&-&46.0&-&1565&68.5&\\underline{37.0}\\\\\nInternVL-Chat~\\cite{chen2024internvl} &Vicuna-13B&\\textbf{66.6}&-&61.5&-&87.6&1586.4&-&-\\\\\nLLaMA-VID~\\cite{li2024llama} &Vicuna-13B&65.0&70.0&-&-&86.0&1542.3&66.6&-\\\\\nSPHINX-Plus~\\cite{liu2025sphinxxscalingdataparameters} &LLaMA2-13B&-&\\underline{74.2}&65.7&\\underline{47.9}&89.1&1457.7&71.0&36.8\\\\\n\\hdashline\n% \\multirow{4}{*}{2.7B Models}\nMobileVLM~\\cite{chu2023mobilevlmfaststrong} &MobileLLaMA-2.7B&59.0& 61.0&47.5& -& 84.9& 1288.9& 59.6& -\\\\\nTinyGPT-V~\\cite{yuan2024tinygptvefficientmultimodallarge} &Phi2-2.7B&33.6& -& -& -& -& -& -& -\\\\\nLLaVA-Phi~\\cite{zhu2024llava} &Phi2-2.7B&-& 68.4& 48.6& 28.9& 85.0& 1335.1& 59.8& -\\\\\nMoE-LLaVA-2.7B\u00d74-Top2~\\cite{lin2024moellavamixtureexpertslarge} &Phi2-2.7B&61.4& 68.5& 51.4& 34.3& 86.3& 1423& 65.2& -\\\\\n% & & & & & & & & & & \\\\\n% & & & & & & & & & & \\\\\n\\hdashline\n\\rowcolor{lightblue}\n\\multicolumn{1}{l!{\\color{black}\\vrule width 0.8pt}}{MoCHA (SigLIP+DINOv2+ConvNeXt+CLIP)} & \\multicolumn{1}{l!{\\color{black}\\vrule width 0.8pt}}{Phi2-2.7B} & 65.24 & 73.24 & 63.13 & 32.74 & \\underline{89.95} & \\underline{1701.60} & 69.67 & 32.24$^{\\dagger}$ \\\\\n\\rowcolor{lightblue}\n\\multicolumn{1}{l!{\\color{black}\\vrule width 0.8pt}}{MoCHA (SigLIP+DINOv2+ConvNeXt+CLIP)} & \\multicolumn{1}{l!{\\color{black}\\vrule width 0.8pt}}{Vicuna-7B} & \\underline{65.73} & \\textbf{74.59} & \\underline{65.72} & 36.68 & \\textbf{89.98}  & \\textbf{1744.09} &71.06 & 35.60$^{\\dagger}$ \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Comparisons between MoCHA and other VLLMs on competitive benchmarks. These models are grouped by the size of the base LLM. Numbers$^{\\dagger}$ are averaged by three inference runs of querying GPT API.}\n\\label{tab:22}\n\\end{table*}\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{2.5pt}\n\n\\toprule\nMethod & LLM & GQA & \\makecell{SQA \\\\IMG}SQA \\\\IMG & \\makecell{Text \\\\ VQA}Text \\\\ VQA & \\makecell{MM\\\\Vet}MM\\\\Vet & POPE & MME & \\makecell{MMB\\\\EN}MMB\\\\EN & MathVista \\\\ \n\\midrule\nQwen-VL-Chat ~\\cite{bai2023qwenvlversatilevisionlanguagemodel}  & Qwen-7B & 57.5 & 68.2 & 61.5 & - & - & 1487.5 & 60.6 & -  \\\\\nInstructBLIP ~\\cite{dai2023instructblipgeneralpurposevisionlanguagemodels} & Vicuna-7B &49.2&60.5& 50.1& 26.2& -& -& 36.0& -\\\\\nLLaVA-LLaMA3~\\cite{contributorsxtuner}&LLaMA3-8B-IT&62.6& 72.9& 59.0& -& 86.4& 1469& \\underline{72.3}& -\\\\\nMini-Gemini~\\cite{li2024minigeminiminingpotentialmultimodality}&Vicuna-7B&-& 65.2& -& 40.8& -& 1523& 69.3& 31.4\\\\\nSPHINX-Intern2~\\cite{liu2025sphinxxscalingdataparameters}  & InternLM2-7B& 56.2 & 70.4& 58.1 & 36.5 & 86.9 & 1260.4 & 57.9 & 35.5 \\\\\nLLaVA-NeXT~\\cite{liu2024llavanext} &Vicuna-7B&64.2& 70.1& 64.9& 43.9& 86.5& 1519& 67.4& 34.6\\\\\nLLaVA-NeXT~\\cite{liu2024llavanext} &Mistral-7B&64.8&72.8&65.7&47.3&86.7&1498&68.7&\\textbf{37.7}\\\\\nLLaVA-v1.5~\\cite{liu2023visual} &Vicuna-7B&62.0& 66.8& 58.2& 30.5& 85.9& 1510.7& 64.3& -\\\\\nVILA~\\cite{lin2024vilapretrainingvisuallanguage} &Vicuna-7B&62.3& 68.2& 64.4& 34.9& 85.5& 1533& 68.9& -\\\\\nCuMo~\\cite{li2024cumoscalingmultimodalllm}& Mistral-7B& 64.9&\t73.9&\t\\textbf{67.0}&\t\\textbf{51.0}&\t86.7&\t1548.6&\t\\textbf{73.0}&\t35.1\\\\\n\\hdashline\nInstructBLIP~\\cite{dai2023instructblipgeneralpurposevisionlanguagemodels} &Vicuna-13B&49.5&63.1&50.7&25.6&78.9&1212.8&-&-\\\\\nLLaVA-v1.5~\\cite{liu2023visual}&Vicuna-13B&63.3&71.6&61.3&35.4&85.9&1531.3&67.7&27.6\\\\\nMini-Gemini~\\cite{li2024minigeminiminingpotentialmultimodality} &Vicuna-13B& -&65.9&-&46.0&-&1565&68.5&\\underline{37.0}\\\\\nInternVL-Chat~\\cite{chen2024internvl} &Vicuna-13B&\\textbf{66.6}&-&61.5&-&87.6&1586.4&-&-\\\\\nLLaMA-VID~\\cite{li2024llama} &Vicuna-13B&65.0&70.0&-&-&86.0&1542.3&66.6&-\\\\\nSPHINX-Plus~\\cite{liu2025sphinxxscalingdataparameters} &LLaMA2-13B&-&\\underline{74.2}&65.7&\\underline{47.9}&89.1&1457.7&71.0&36.8\\\\\n\\hdashline\nMobileVLM~\\cite{chu2023mobilevlmfaststrong} &MobileLLaMA-2.7B&59.0& 61.0&47.5& -& 84.9& 1288.9& 59.6& -\\\\\nTinyGPT-V~\\cite{yuan2024tinygptvefficientmultimodallarge} &Phi2-2.7B&33.6& -& -& -& -& -& -& -\\\\\nLLaVA-Phi~\\cite{zhu2024llava} &Phi2-2.7B&-& 68.4& 48.6& 28.9& 85.0& 1335.1& 59.8& -\\\\\nMoE-LLaVA-2.7B\u00d74-Top2~\\cite{lin2024moellavamixtureexpertslarge} &Phi2-2.7B&61.4& 68.5& 51.4& 34.3& 86.3& 1423& 65.2& -\\\\\n\\hdashline\n\\rowcolor{lightblue}lightblue\n\\multicolumn{1}1{l!{\\color{black}\\vrule width 0.8pt}}l!{\\color{black}\\vrule width 0.8pt}\\color{black}\\vrule width 0.8pt{MoCHA (SigLIP+DINOv2+ConvNeXt+CLIP)}MoCHA (SigLIP+DINOv2+ConvNeXt+CLIP) & \\multicolumn{1}1{l!{\\color{black}\\vrule width 0.8pt}}l!{\\color{black}\\vrule width 0.8pt}\\color{black}\\vrule width 0.8pt{Phi2-2.7B}Phi2-2.7B & 65.24 & 73.24 & 63.13 & 32.74 & \\underline{89.95} & \\underline{1701.60} & 69.67 & 32.24$^{\\dagger}$^{\\dagger}\\dagger \\\\\n\\rowcolor{lightblue}lightblue\n\\multicolumn{1}1{l!{\\color{black}\\vrule width 0.8pt}}l!{\\color{black}\\vrule width 0.8pt}\\color{black}\\vrule width 0.8pt{MoCHA (SigLIP+DINOv2+ConvNeXt+CLIP)}MoCHA (SigLIP+DINOv2+ConvNeXt+CLIP) & \\multicolumn{1}1{l!{\\color{black}\\vrule width 0.8pt}}l!{\\color{black}\\vrule width 0.8pt}\\color{black}\\vrule width 0.8pt{Vicuna-7B}Vicuna-7B & \\underline{65.73} & \\textbf{74.59} & \\underline{65.72} & 36.68 & \\textbf{89.98}  & \\textbf{1744.09} &71.06 & 35.60$^{\\dagger}$^{\\dagger}\\dagger \\\\\n\\bottomrule\n\n\\caption{Comparisons between MoCHA and other VLLMs on competitive benchmarks. These models are grouped by the size of the base LLM. Numbers$^{\\dagger}$ are averaged by three inference runs of querying GPT API.}\n\\label{tab:22}\n\n\\begin{table}[!ht]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{1.5pt}\n\\begin{tabular}{ll\n    >{\\centering\\arraybackslash}m{1.05cm}\n    >{\\centering\\arraybackslash}m{1.05cm}\n    >{\\centering\\arraybackslash}m{1.3cm}\n    c\n    }\n\\toprule\nModel & LLM & \\makecell{Avg.\\\\Infer.\\\\Time} & \\makecell{Total\\\\Params} & \\makecell{Trainable\\\\Params} & GFLOPs \\\\ \n\\midrule\nLLaVA-v1.5 & Vicuna-13B & 0.81s & 13.35B & 13.05B & 16894.72 \\\\\nLLaVA-v1.5 & Vicuna-7B&\t0.44s&\t7.06B&\t6.76B&\t8877.05 \\\\\nInternVL-Chat&\tVicuna-13B&\t0.83s&\t18.96B&\t13.06B&\t23334.93 \\\\\nMoE-LLaVA &\tPhi2-2.7B&\t\\textbf{0.41s}&\t5.61B&\t5.30B&\t\\textbf{7338.97}\\\\\nMoCHA&\tPhi2-2.7B&\t0.57s&\t\\textbf{4.97B}&\t\\textbf{4.07B}&\t12014.64\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Quantitative comparison of MoCHA and other VLLMs.}\n\\label{tab:quantitive}\n\\end{table}\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{1.5pt}\n\n\\toprule\nModel & LLM & \\makecell{Avg.\\\\Infer.\\\\Time}Avg.\\\\Infer.\\\\Time & \\makecell{Total\\\\Params}Total\\\\Params & \\makecell{Trainable\\\\Params}Trainable\\\\Params & GFLOPs \\\\ \n\\midrule\nLLaVA-v1.5 & Vicuna-13B & 0.81s & 13.35B & 13.05B & 16894.72 \\\\\nLLaVA-v1.5 & Vicuna-7B&\t0.44s&\t7.06B&\t6.76B&\t8877.05 \\\\\nInternVL-Chat&\tVicuna-13B&\t0.83s&\t18.96B&\t13.06B&\t23334.93 \\\\\nMoE-LLaVA &\tPhi2-2.7B&\t\\textbf{0.41s}&\t5.61B&\t5.30B&\t\\textbf{7338.97}\\\\\nMoCHA&\tPhi2-2.7B&\t0.57s&\t\\textbf{4.97B}&\t\\textbf{4.07B}&\t12014.64\\\\\n\\bottomrule\n\n\\caption{Quantitative comparison of MoCHA and other VLLMs.}\n\\label{tab:quantitive}\n\n\n", "appendix": false}, "Experiments": {"content": "\n\\subsection{Data Details}\n\\subsubsection{Training Data.}The training process utilizes a 558K subset of the LAION-CC-SBU dataset with BLIP-generated captions (as in LLaVA-v1.5), followed by fine-tuning on a 665K mixture of instruction-following data from LLaVA-v1.5.\n\n\\subsubsection{Evaluation Benchmarks.}We focus on academic VQA datasets, including GQA~\\cite{hudson2019gqa}, Science-QA~\\cite{lu2022learn}, and TextVQA~\\cite{singh2019towards}, as well as instruction-following VLLM benchmarks, such as POPE~\\cite{li2023evaluatingobjecthallucinationlarge}, MME~\\cite{fu2024mmecomprehensiveevaluationbenchmark}, MMBench~\\cite{liu2024mmbenchmultimodalmodelallaround}, and MM-Vet~\\cite{yu2024mmvetevaluatinglargemultimodal}. Furthermore, we evaluate the challenging MathVista~\\cite{lu2024mathvistaevaluatingmathematicalreasoning} dataset to assess VLLM visual reasoning capabilities.\n\n\\subsection{Implementation Details}\n\\subsubsection{Two-Stage Training.}To enhance training stability, we adopt a two-stage training strategy. More details and hyperparameters of the training can be found in \\textbf{Appendix B}.\n\n\n\\subsubsection{Parameter Settings.}We train for one epoch using 8 NVIDIA L40 (48GB) GPUs. During the pre-training stage, the learning rate is set to 1e-3, with a batch size of $8\\times 16$8\\times 16 (1 vision encoder), $8\\times 8$8\\times 8 (2 vision encoders) and $8\\times 4$8\\times 4 (3 or 4 vision encoders). During visual instruction fine-tuning, the learning rate is reduced to 2e-5, with a batch size of $8\\times 8$8\\times 8 (1 vision encoder) and $8\\times 4 $8\\times 4  or $2$2 (2, 3, or 4 vision encoders). Each MoEC has $E=4$E=4 experts, with $K=2$K=2 activated per forward pass. For adaptive feature fusion through intra- and inter-group attention, we aggregate $M=3$M=3 tokens within each group and $N=7$N=7 tokens across groups.\n\n\\subsubsection{Training Settings.} We employ pre-trained OpenAI CLIP ViT-L/14@336, SigLIP ViT-L/16@384, OpenCLIP ConvNeXt-XXL@1024, and DINOv2 ViT-L/14@336 as vision encoders. The vision-language connector is implemented as an MoEC, which consists of a router and expert networks. Each expert consists of two linear layers.\nFor the language model, we utilize Phi2-2.7B and Vicuna-7B-v1.5. \n\n\n\\subsubsection{Evaluation Settings.}We follow the LLaVA series settings, using greedy decoding for all benchmarks. Data and questions are converted into visual instructions to prompt VLLMs. For GPT API-based evaluation, we use GPT-3.5-turbo for MathVista.\n\\subsection{Main Results}\nWe scale the LLM backbone from 2.7B to 7B parameters for both pre-training and fine-tuning, achieving competitive performance. Comparative analysis is conducted against other instruction-tuned VLLMs, categorized by backbone size (e.g., 2.7B, 7B, 8B and 13B).\n\n\\subsubsection{MoCHA Stands Out Among Other VLLMs.} In Table~\\ref{tab:22}, we evaluate MoCHA on 8 multimodal benchmarks spanning from general (e.g., GQA, MMVet) to math and reasoning multimodal tasks (e.g., SQA IMG, MathVista). \nUnder the same LLM and fine-tuning settings, MoCHA (Phi2-2.7B) consistently outperforms LLaVA-Phi and MoE-LLaVA (Phi2-2.7B) across all benchmarks. Moreover, MoCHA (Phi2-2.7B) achieves overall performance comparable to, and in some cases surpassing, many 7B-based VLLMs as well as larger models. Specifically, MoCHA (Phi2-2.7B) outperforms SPHINX-Plus (LLaMA2-13B) by 0.85\\% on POPE and 243.9 points on MME. \nThese findings indicate that smaller models can exhibit strong visual perception and excel at fine-grained detail recognition.\n\nAs we scale up the LLM backbone, MoCHA (Vicuna-7B) demonstrates stronger visual reasoning capabilities than MoCHA (Phi-2 2.7B). Notably, MoCHA (Vicuna-7B) also surpasses models with even larger LLMs across multiple visual benchmarks. For instance, compared to LLaVA-v1.5 (Vicuna-13B), it achieves improvements of 2.99\\%, 4.42\\%, and 4.08\\% on SQA IMG, TextVQA, and POPE, respectively.\nMoreover, in contrast to CuMo\u2014which applies a global mixture of experts (MoE) across the LLM, vision encoder, and connector\u2014MoCHA (Vicuna-7B) outperforms CuMo (Mistral-7B) on several key vision benchmarks.\nThese results provide strong evidence that, compared to the conventional single-vision-encoder paradigm, MoCHA's dynamic MoE design and customized attention mechanisms offer a more effective approach for multimodal learning.\nWe present more results regarding other visual capabilities such as multi-object recognition in \\textbf{Appendix D}.\n\n\n\\subsubsection{MoCHA Helps Reduce Visual Hallucination.} We follow the evaluation protocol of POPE, a popular benchmarking for evaluating visual object hallucination. \nMoCHA shows a marked improvement in mitigating hallucination issues in Table~\\ref{tab:22}, outperforming models with larger parameter sizes. It produces object descriptions that are highly consistent with the input images and exhibits a rich potential for hallucination inhibition. Specifically, we observe that MoCHA surpasses LLaVA in popular sampling, adversarial sampling, and random sampling, despite having fewer parameters. Additionally, the yes ratio of MoCHA remains relatively balanced, which indicates that our framework is capable of providing accurate feedback based on the given questions.\n\n\\subsubsection{The MoCHA Design Optimizes Model Efficiency.} To validate the efficiency use of our MoECs, we further present training GFLOPs, parameters, and average inference time of models in Table~\\ref{tab:quantitive}. \nWe make sure that all numbers are obtained under identical configurations to ensure the validity of the comparison. \nFrom the table, MoCHA achieves an inference speed of 0.57s with only 4.97B parameters and 12014.64 GFLOPs. This not only significantly reduces both the parameter size and computational cost, but also outperforms LLaVA-v1.5 (Vicuna-13B) and InternVL-Chat (Vicuna-13B) in inference efficiency. Furthermore, it approaches the inference speed of MoE-LLaVA, despite using even fewer parameters.\nThese notable advantages in both computational and parameter efficiency highlight the strong potential of scaling up MoCHA as a compelling alternative to existing vision encoder paradigms in multimodal learning.\n\\begin{figure}[!htbp]\n\\centering\n\\includegraphics[width=0.89\\linewidth]{bar_grouped_vertical.pdf} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.\n\\caption{Ablation of Channel vs. Sequential Concatenation for MoECs with four encoders.}\n\\label{concat}\n\\end{figure}\n\\centering\n\\includegraphics[width=0.89\\linewidth]{bar_grouped_vertical.pdf} \\caption{Ablation of Channel vs. Sequential Concatenation for MoECs with four encoders.}\n\\label{concat}\n\n\\begin{table}[t]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{2.5pt}\n\\begin{tabular}{l!{\\color{black}\\vrule width 0.6pt}cccc}\n\\toprule\n  Method (MLP)& GQA &\\makecell{SQA \\\\IMG}&POPE & \\makecell{MMB\\\\EN} \\\\ \n\\midrule\nSigLIP& 59&\t64.35&\t84.64&\t59.88 \\\\\n        SigLIP+ConvNeXt &60.78&\t67.18&\t86.67&\t64.95      \\\\\n        SigLIP+CLIP+ConvNeXt&57.21&\t65.39&\t85.72&\t58.42 \\\\\n        SigLIP+DINOv2+ConvNeXt+CLIP&\\textbf{61.86}&\t\\textbf{69.01}&\t\\textbf{88.47}&\\textbf{66.51}\\\\       \n\\bottomrule\n\\end{tabular}\n\\caption{Ablation study only on different vision encoder combinations.}\n\\label{tab:group}\n\\end{table}\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{2.5pt}\n\n\\toprule\n  Method (MLP)& GQA &\\makecell{SQA \\\\IMG}SQA \\\\IMG&POPE & \\makecell{MMB\\\\EN}MMB\\\\EN \\\\ \n\\midrule\nSigLIP& 59&\t64.35&\t84.64&\t59.88 \\\\\n        SigLIP+ConvNeXt &60.78&\t67.18&\t86.67&\t64.95      \\\\\n        SigLIP+CLIP+ConvNeXt&57.21&\t65.39&\t85.72&\t58.42 \\\\\n        SigLIP+DINOv2+ConvNeXt+CLIP&\\textbf{61.86}&\t\\textbf{69.01}&\t\\textbf{88.47}&\\textbf{66.51}\\\\       \n\\bottomrule\n\n\\caption{Ablation study only on different vision encoder combinations.}\n\\label{tab:group}\n\n\\subsection{Ablation Studies}\n\\subsubsection{Effect of Channel v.s. Sequential Concatenation.} We conduct ablation experiments on the heterogeneous features dynamically selected by the MoECs module with sequential concatenation and channel concatenation to evaluate the differences between the two strategies.\nAs illustrated in Figure~\\ref{concat}, sequence concatenation consistently outperforms channel concatenation across all vision tasks under our MoEC settings. Furthermore, channel concatenation aggregates features from multiple encoders within each visual token; if one encoder (e.g., CLIP) has dominant feature magnitudes, it may overshadow fine-grained information from other encoders, leading the model to focus disproportionately on its features.\n\\begin{table}[htbp]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{2.5pt}\n\\begin{tabular}{l!{\\color{black}\\vrule width 0.6pt}cccc}\n\\toprule\n  Method (MoEC) & GQA &\\makecell{SQA \\\\IMG}&POPE & \\makecell{MMB\\\\EN} \\\\ \n\\midrule\nSigLIP& 56.77&\t64.6&\t82.85&\t57.65 \\\\\n        SigLIP+ConvNeXt &57.77&\t67.92&\t86.72&63.49      \\\\\n        SigLIP+CLIP+ConvNeXt&59.1&\t66.58&\t87.06&\t60.83 \\\\\n        SigLIP+DINOv2+ConvNeXt+CLIP&\\textbf{63.35}&\t\\textbf{70.92}&\t\\textbf{89.01}&\\textbf{68.03}\\\\       \n\\bottomrule\n\\end{tabular}\n\\caption{Ablation study of different vision encoder combinations under MoEC settings.}\n\\label{tab:moe}\n\\end{table}\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{2.5pt}\n\n\\toprule\n  Method (MoEC) & GQA &\\makecell{SQA \\\\IMG}SQA \\\\IMG&POPE & \\makecell{MMB\\\\EN}MMB\\\\EN \\\\ \n\\midrule\nSigLIP& 56.77&\t64.6&\t82.85&\t57.65 \\\\\n        SigLIP+ConvNeXt &57.77&\t67.92&\t86.72&63.49      \\\\\n        SigLIP+CLIP+ConvNeXt&59.1&\t66.58&\t87.06&\t60.83 \\\\\n        SigLIP+DINOv2+ConvNeXt+CLIP&\\textbf{63.35}&\t\\textbf{70.92}&\t\\textbf{89.01}&\\textbf{68.03}\\\\       \n\\bottomrule\n\n\\caption{Ablation study of different vision encoder combinations under MoEC settings.}\n\\label{tab:moe}\n\n\n\\subsubsection{Effect of Multiple Vision Encoders.} \nTo investigate the use of different combinations of vision encoders under the MLP connector,\nablation studies were conducted by individually extracting features from each vision encoder. \nAs shown in Table~\\ref{tab:group}, our study indicates that only using the SigLIP exhibits inferior performance across various benchmarks. \nThe inclusion of ConvNeXt improves performance on benchmarks such as GQA, SQA IMG, and POPE. However, the subsequent addition of CLIP leads to a performance decline. \nThis phenomenon can be attributed to the substantial feature similarity and redundancy between SigLIP and CLIP, since both encoders are trained with large-scale image-text contrastive objectives. The configuration that integrates SigLIP, DINOv2, ConvNeXt, and CLIP achieves the best overall performance. In particular, DINOv2 provides a balanced contribution by capturing both local details and global semantic information.\n\n\\subsubsection{Upgrade MLP connector to MoEC.} \nTo evaluate MoEC\u2019s capability in efficiently integrating heterogeneous and multi-dimensional features,\nwe initiate this ablation study by replacing each MLP connector with an upcycled MoEC, as shown in Table~\\ref{tab:moe}. \nThe results in Table~\\ref{tab:group} indicate that with the plain MLP as the connector, the model performance with a single SigLIP remains weak. While the addition of ConvNeXt further improves performance, the full integration of CLIP yields the largest gain among all settings. \nOn the other hand, in Table~\\ref{tab:moe}, the combination of SigLIP, DINOv2, ConvNeXt, and CLIP under our MoEC results in notable performance improvements of 1.49\\%, 1.91\\%, 0.54\\%, and 1.52\\% on the GQA, SQA IMG, POPE, and MMBench benchmarks, respectively, compared to results using vanilla MLP in Table~\\ref{tab:group}. \nThese findings show that MoECs effectively handle diverse visual features\u2014including object recognition, scene understanding, attribute identification, and spatial reasoning. Even as the number of vision encoders grows, MoECs incur minimal parameter overhead while keeping inference time and computational cost on par with a standard MLP. Further quantitative results are provided in \\textbf{Appendix C}.\n\\begin{table}[htbp]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{2.5pt}\n% \\setlength{\\tabcolsep}{4pt}\n% \\renewcommand{\\arraystretch}{1.2}\n\\begin{tabular}{l!{\\color{black}\\vrule width 0.6pt}cccc}\n\\toprule\nMethod (MoEC + Attention) & GQA & \\makecell{SQA\\\\IMG} & POPE & \\makecell{MMB\\\\EN} \\\\ \n\\midrule\nSigLIP & 56.11 & 62.76 & 82.27 & 57.90 \\\\\nSigLIP+ConvNeXt & 57.02 & 63.97 & 87.84 & 61.06 \\\\\nSigLIP+CLIP+ConvNeXt & 60.34 & 68.73 & 88.07 & 62.11 \\\\\n\\rowcolor{lightblue} \\multicolumn{1}{l!{\\color{black}\\vrule width 0.8pt}} {SigLIP+DINOv2+ConvNeXt+CLIP }& \\textbf{65.24} & \\textbf{73.24} & \\textbf{89.95} & \\textbf{69.67} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Ablation Study of MoCHA with different vision encoder combinations. Settings for results in Table~\\ref{tab:22} are highlighted in \\colorbox{lightblue}{blue}.}\n\\label{tab:graph}\n\\end{table}\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{2.5pt}\n\n\\toprule\nMethod (MoEC + Attention) & GQA & \\makecell{SQA\\\\IMG}SQA\\\\IMG & POPE & \\makecell{MMB\\\\EN}MMB\\\\EN \\\\ \n\\midrule\nSigLIP & 56.11 & 62.76 & 82.27 & 57.90 \\\\\nSigLIP+ConvNeXt & 57.02 & 63.97 & 87.84 & 61.06 \\\\\nSigLIP+CLIP+ConvNeXt & 60.34 & 68.73 & 88.07 & 62.11 \\\\\n\\rowcolor{lightblue}lightblue \\multicolumn{1}1{l!{\\color{black}\\vrule width 0.8pt}}l!{\\color{black}\\vrule width 0.8pt}\\color{black}\\vrule width 0.8pt {SigLIP+DINOv2+ConvNeXt+CLIP }SigLIP+DINOv2+ConvNeXt+CLIP & \\textbf{65.24} & \\textbf{73.24} & \\textbf{89.95} & \\textbf{69.67} \\\\\n\\bottomrule\n\n\\caption{Ablation Study of MoCHA with different vision encoder combinations. Settings for results in Table~\\ref{tab:22} are highlighted in \\colorbox{lightblue}{blue}.}\n\\label{tab:graph}\n\n\\begin{table}[htbp]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{3pt}\n% \\setlength{\\tabcolsep}{4pt}\n\\begin{tabular}{l!{\\color{black}\\vrule width 0.6pt}cccc}\n\\toprule\n  Top-$K$ & GQA &\\makecell{SQA \\\\IMG}&POPE & \\makecell{MMB\\\\EN} \\\\ \n\\midrule\n1 &59.86 &68.13 &85.93 & 65.59\\\\\n \\rowcolor{lightblue} \\multicolumn{1}{l!{\\color{black}\\vrule width 0.8pt}} 2 & {\\textbf{65.24}}&\t\\textbf{73.24}&\t\\textbf{89.95}&\t\\textbf{69.67} \\\\   \n 3& 62.90&\t70.18&\t88.21&\t67.35\\\\\n 4& 58.32&\t67.64&\t86.02&\t64.97\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Ablation study on the value of Top-$K$ in MoCHA.}\n\\label{tab:expertnum}\n\\end{table}\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{3pt}\n\n\\toprule\n  Top-$K$K & GQA &\\makecell{SQA \\\\IMG}SQA \\\\IMG&POPE & \\makecell{MMB\\\\EN}MMB\\\\EN \\\\ \n\\midrule\n1 &59.86 &68.13 &85.93 & 65.59\\\\\n \\rowcolor{lightblue}lightblue \\multicolumn{1}1{l!{\\color{black}\\vrule width 0.8pt}}l!{\\color{black}\\vrule width 0.8pt}\\color{black}\\vrule width 0.8pt 2 & {\\textbf{65.24}}\\textbf{65.24}&\t\\textbf{73.24}&\t\\textbf{89.95}&\t\\textbf{69.67} \\\\   \n 3& 62.90&\t70.18&\t88.21&\t67.35\\\\\n 4& 58.32&\t67.64&\t86.02&\t64.97\\\\\n\\bottomrule\n\n\\caption{Ablation study on the value of Top-$K$ in MoCHA.}\n\\label{tab:expertnum}\n\n\\subsubsection{Effect of Hierarchical Group Attention (HGA).} We further ablate our hierarchical group attention (HGA) to evaluate its effectiveness in feature fusion.\nAs shown in Table~\\ref{tab:graph}, continual performance gains are observed as ConvNeXt, CLIP, and DINOv2 are introduced sequentially. This confirms that the proposed intra- and inter-group adaptive attention fusion in HGA enhances the synergy and complementarity among multiple vision encoders.\nCompared to previous results, the performance of the single SigLIP encoder slightly drops relative to both MoEC (Table~\\ref{tab:moe}) and the standard MLP baseline (Table~\\ref{tab:group}), suggesting that intra-group feature aggregation within a single encoder may introduce redundancy.\nNotably, incorporating ConvNeXt leads to a 5.57\\% improvement on the POPE benchmark, indicating that HGA effectively leverages ConvNeXt\u2019s fine-grained local features.\nThe full combination of SigLIP, DINOv2, ConvNeXt, and CLIP yields the best overall performance, further demonstrating the robustness of MoCHA.\n\n\n\n\\subsubsection{Effect of the Number of Activated Experts.}To evaluate the effect of the number of activated experts, we compare different Top-$K$K strategies. As shown in Table~\\ref{tab:expertnum}, $K=2$K=2 achieves the best performance, while larger $K$K values provides limited improvement, increases computational and memory costs, lowers parameter utilization, and may cause expert imbalance. Hardware constraints further restrict $K=4$K=4 to a batch size of 1, which may degrade performance. Accordingly, we set $K=2$K=2 to fully exploit the advantages of the MoEC architecture. \n\n\n", "appendix": false}, "Conclusion": {"content": "\nIn this work, we introduce MoCHA, a novel framework for efficiently training vision-language models with advanced visual reasoning capabilities. It addresses two key challenges in visual detail extraction and in heterogeneous feature fusion. \nThe MoECs module enables dynamic expert selection, effectively integrating diverse visual signals within a unified vision-language framework. Customized HGA further enhances the complementarity of heterogeneous features.\nOur framework can be readily adapted to incorporate additional vision backbones and larger language models.\nAlthough MoCHA demonstrates remarkable performance, MoEC may suffer from knowledge entanglement and redundancy, which could hinder expert specialization. Future work may explore fine-grained expert partitioning and shared expert isolation to further improve knowledge allocation within the model. \nWe hope this work provides a foundation and new insights for vision encoder design in VLLMs.\n\n\\bibliography{aaai2026}\n\n\\clearpage\n\\twocolumn\n", "appendix": true}, "Appendix": {"content": "\n\\subsection{A Analysis of Vision Encoders}\n\\label{vision}\nA single vision encoder is unlikely to achieve the best performance across all benchmark tasks. As shown in Table~\\ref{tab:single}, DINO performs best on the GQA benchmark, CLIP achieves the highest score on MMBench, while SigLIP demonstrates advanced performance on MME and MM-Vet tasks. These results indicate that different vision encoders exhibit complementary strengths in visual reasoning tasks.\n\nCLIP achieves vision-language alignment through language supervision, SigLIP employs a binary classification objective to improve training efficiency and zero-shot performance, ConvNeXt leverages a convolutional architecture for high-resolution local feature extraction, and DINOv2 captures pixel-level geometric structure via self-supervised learning. These backbones exhibit fundamental differences in\narchitecture (CNN vs. ViT), training paradigms (supervised vs. self-supervised), and information granularity (global vs. local), leading\nto naturally complementary feature representations that enhance\nboth diversity and robustness.\n\n\\begin{table}[!h]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{2pt}\n% \\setlength{\\tabcolsep}{4pt}\n\\begin{tabular}{l!{\\color{black}\\vrule width 0.6pt}l!{\\color{black}\\vrule width 0.6pt}cccc}\n\\toprule\n Vision Model & LLM& GQA &\\makecell{MMB \\\\EN}&MME & \\makecell{MM\\\\Vet} \\\\ \n\\midrule\nDINO&\tPhi2-2.7B&\t\\textbf{61.00}&\t57.82&\t1555.61&\t22.3\\\\\nConvNeXt&\tPhi2-2.7B&\t58.51&\t64.00&\t1604.99\t&24.5\\\\\nCLIP&\tPhi2-2.7B&\t60.12&\t\\textbf{66.92}&\t1600.19\t&25.4\\\\\nSigLIP&\tPhi2-2.7B&\t59.00&\t59.88&\t\\textbf{1622.61}\t&\\textbf{25.7}\\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Evaluation performance of individual vision encoders.}\n\\label{tab:single}\n\\end{table}\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{2pt}\n\n\\toprule\n Vision Model & LLM& GQA &\\makecell{MMB \\\\EN}MMB \\\\EN&MME & \\makecell{MM\\\\Vet}MM\\\\Vet \\\\ \n\\midrule\nDINO&\tPhi2-2.7B&\t\\textbf{61.00}&\t57.82&\t1555.61&\t22.3\\\\\nConvNeXt&\tPhi2-2.7B&\t58.51&\t64.00&\t1604.99\t&24.5\\\\\nCLIP&\tPhi2-2.7B&\t60.12&\t\\textbf{66.92}&\t1600.19\t&25.4\\\\\nSigLIP&\tPhi2-2.7B&\t59.00&\t59.88&\t\\textbf{1622.61}\t&\\textbf{25.7}\\\\\n\\bottomrule\n\n\\caption{Evaluation performance of individual vision encoders.}\n\\label{tab:single}\n\n\n\\subsection{B Train Details}   \n\\label{trian}\n\\begin{figure}[ht]\n  \\centering\n  \\includegraphics[width=\\columnwidth]{train.pdf}\n  \\caption{Training stages of MoCHA.}\n       \\label{fig:tain}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=\\columnwidth]{train.pdf}\n  \\caption{Training stages of MoCHA.}\n       \\label{fig:tain}\n\n\\begin{table}[!ht]\n\\centering\n  \\footnotesize\n\\setlength{\\tabcolsep}{1mm}\n\\begin{tabular}{l!{\\color{black}\\vrule width 0.6pt}cc}\n\\toprule\nConfig & Stage I & Stage II \\\\\n\\midrule\nExperts              & --       & --    \\\\\nTop-k                & --       & 2     \\\\ \\midrule\n% Deepspeed            & Zero2    & Zero3\\\\            \nImage encoder        & \\multicolumn{2}{c}{SigLIP, DINOv2, ConvNeXt, CLIP} \\\\\nFeature select layer & \\multicolumn{2}{c}{-2} \\\\\nImage projector      & \\multicolumn{2}{c}{MoECs (Multiple MoEC units)} \\\\\nImage resolution &  \\multicolumn{2}{c}{e.g., 384, 1024, 336}\\\\\nEpoch                & \\multicolumn{2}{c}{1} \\\\\nLearning rate        &  1e-3     &  2e-5  \\\\\nLearning rate schdule& \\multicolumn{2}{c}{Cosine} \\\\\nWeight decay         & \\multicolumn{2}{c}{0.0} \\\\\nText max length      & \\multicolumn{2}{c}{2560} \\\\\nBatch size per GPU   &   4      &     2   \\\\\nGPU                  & \\multicolumn{2}{c}{8 $\\times$ L40-48GB} \\\\\nPrecision            & \\multicolumn{2}{c}{Bf16} \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Training hyperparameters.}\n\\label{training_hypers}\n\\end{table}\n\\centering\n  \\footnotesize\n\\setlength{\\tabcolsep}{1mm}\n\n\\toprule\nConfig & Stage I & Stage II \\\\\n\\midrule\nExperts              & --       & --    \\\\\nTop-k                & --       & 2     \\\\ \\midrule\nImage encoder        & \\multicolumn{2}2{c}c{SigLIP, DINOv2, ConvNeXt, CLIP}SigLIP, DINOv2, ConvNeXt, CLIP \\\\\nFeature select layer & \\multicolumn{2}2{c}c{-2}-2 \\\\\nImage projector      & \\multicolumn{2}2{c}c{MoECs (Multiple MoEC units)}MoECs (Multiple MoEC units) \\\\\nImage resolution &  \\multicolumn{2}2{c}c{e.g., 384, 1024, 336}e.g., 384, 1024, 336\\\\\nEpoch                & \\multicolumn{2}2{c}c{1}1 \\\\\nLearning rate        &  1e-3     &  2e-5  \\\\\nLearning rate schdule& \\multicolumn{2}2{c}c{Cosine}Cosine \\\\\nWeight decay         & \\multicolumn{2}2{c}c{0.0}0.0 \\\\\nText max length      & \\multicolumn{2}2{c}c{2560}2560 \\\\\nBatch size per GPU   &   4      &     2   \\\\\nGPU                  & \\multicolumn{2}2{c}c{8 $\\times$ L40-48GB}8 $\\times$\\times L40-48GB \\\\\nPrecision            & \\multicolumn{2}2{c}c{Bf16}Bf16 \\\\\n\\bottomrule\n\n\\caption{Training hyperparameters.}\n\\label{training_hypers}\n\n\nAs illustrated in Figure~\\ref{fig:tain}, we now describe the corresponding MoCHA training strategy, which consists of three stages:\n\\begin{itemize}\n\\item \\textbf{Stage 1: Pre-training for Feature Alignment. }We freeze the vision encoders and the large language model, pre-training only the SigLIP MoEC, DINOv2 MoEC, ConvNeXt MoEC, and CLIP MoEC. \n\\item \\textbf{Stage 2: Visual Instruction Fine-tuning. }The weights of all four vision encoders remain frozen, while the MoECs and LLM pre-trained weights are further updated.\n\\end{itemize}\\begin{itemize}\n\\item \\textbf{Stage 1: Pre-training for Feature Alignment. }We freeze the vision encoders and the large language model, pre-training only the SigLIP MoEC, DINOv2 MoEC, ConvNeXt MoEC, and CLIP MoEC. \n\\item \\textbf{Stage 2: Visual Instruction Fine-tuning. }The weights of all four vision encoders remain frozen, while the MoECs and LLM pre-trained weights are further updated.\n\\end{itemize}\n\\item \\textbf{Stage 1: Pre-training for Feature Alignment. }We freeze the vision encoders and the large language model, pre-training only the SigLIP MoEC, DINOv2 MoEC, ConvNeXt MoEC, and CLIP MoEC. \n\\item \\textbf{Stage 2: Visual Instruction Fine-tuning. }The weights of all four vision encoders remain frozen, while the MoECs and LLM pre-trained weights are further updated.\n\n\\begin{table*}[!ht]\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{2.5pt}\n\\begin{tabular}{@{}lccccc@{}}\n\\toprule\nVision Model & Total Tokens & \\makecell{Avg.Infer.Time\\\\(MLP vs MoEC)} & \\makecell{Total Params \\\\ (MLP vs MoEC)} & \\makecell{Trainable Params \\\\ (MLP vs MoEC)} & \\makecell{GFLOPs \\\\ (MLP vs MoEC)} \\\\\n\\midrule\nSigLIP & 440 & 0.34s vs 0.34s & 3.38B vs 3.41B & 2.78B vs 2.81B & 3000.09 vs 3008.17 \\\\\nSigLIP+ConvNeXt & 540 & 0.41s vs 0.31s & 4.23B vs 4.29B & 3.64B vs 3.69B & 4244.34 vs 4254.26 \\\\\nSigLIP+CLIP+ConvNeXt & 1116 & 0.45s vs 0.39s & 4.24B vs 4.33B & 3.65B vs 3.73B & 8021.32 vs 8048.25 \\\\\nSigLIP+DINOv2+ConvNeXt+CLIP & 1692 & 0.58s vs 0.57s & 4.86B vs 4.97B & 3.96B vs 4.07B & 11983.58 vs 12014.64 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Ablation study on inference efficiency and model complexity across single and multi-encoder variants.}\n\\label{tab:ablation_efficiency}\n\\end{table*}\n\\centering\n\\footnotesize\n\\setlength{\\tabcolsep}{2.5pt}\n\n\\toprule\nVision Model & Total Tokens & \\makecell{Avg.Infer.Time\\\\(MLP vs MoEC)}Avg.Infer.Time\\\\(MLP vs MoEC) & \\makecell{Total Params \\\\ (MLP vs MoEC)}Total Params \\\\ (MLP vs MoEC) & \\makecell{Trainable Params \\\\ (MLP vs MoEC)}Trainable Params \\\\ (MLP vs MoEC) & \\makecell{GFLOPs \\\\ (MLP vs MoEC)}GFLOPs \\\\ (MLP vs MoEC) \\\\\n\\midrule\nSigLIP & 440 & 0.34s vs 0.34s & 3.38B vs 3.41B & 2.78B vs 2.81B & 3000.09 vs 3008.17 \\\\\nSigLIP+ConvNeXt & 540 & 0.41s vs 0.31s & 4.23B vs 4.29B & 3.64B vs 3.69B & 4244.34 vs 4254.26 \\\\\nSigLIP+CLIP+ConvNeXt & 1116 & 0.45s vs 0.39s & 4.24B vs 4.33B & 3.65B vs 3.73B & 8021.32 vs 8048.25 \\\\\nSigLIP+DINOv2+ConvNeXt+CLIP & 1692 & 0.58s vs 0.57s & 4.86B vs 4.97B & 3.96B vs 4.07B & 11983.58 vs 12014.64 \\\\\n\\bottomrule\n\n\\caption{Ablation study on inference efficiency and model complexity across single and multi-encoder variants.}\n\\label{tab:ablation_efficiency}\n\n\nAs shown in Table~\\ref{training_hypers}, we present the training hyperparameters for all models, which are applicable to Phi and Vicuna. For the training process in all stages, we consistently train for 1 epoch, as we find that the models overfit when training for 2 epochs. We enable the gradient checkpoint mode for all training stage.\n\n\n\\subsection{C Quantitative Analysis of Different Vision Encoder Combinations}\n\\label{quantitative}\nWe conduct detailed statistics on inference time, total parameters, trainable parameters, and GFLOPs when using either MLP or MoEC modules across single and multi-encoder variants, all evaluated on the same dataset.    \nAs shown in Table~\\ref{tab:ablation_efficiency}, the results demonstrate that MoEC significantly improves inference efficiency in multi-encoder settings (e.g., reducing inference time from 0.41s to 0.31s with the SigLIP+ConvNeXt combination), while introducing only minimal increases in total and trainable parameters (e.g., 4.97B/4.07B vs. 4.86B/3.96B in the SigLIP+DINOv2+ConvNeXt+CLIP configuration). MoECs can seamlessly support the integration of multiple vision encoders, owing to their sparse gating and expert allocation mechanisms. As a result, features from different vision encoders can be flexibly routed to the most appropriate experts for effective fusion. Notably, the hierarchical group attention module achieves adaptive feature fusion without adding extra trainable parameters. We also evaluate other VLLMs and our MoCHA on the same dataset, with identical batch size and hardware.\n\n\\subsection{D More Results}\nAs illustrated in Figure~\\ref{fig:example1}, \nboth MoCHA and LLaVA provide accurate answers when identifying that the person is riding an elephant rather than a horse or determining the position of the curtain. Both MoCHA and LLaVA demonstrate strong capabilities across diverse reasoning tasks, such as fundamental visual object recognition and simple spatial relationship inference.\n\nAs illustrated in Figure~\\ref{fig:example2}, MoCHA (Vicuna-7B) demonstrates advanced reasoning and comprehension abilities in multi-object recognition and fine-grained visual reasoning tasks. For example, it accurately identifies the position of unsweetened milk and the number on the motorcycle ridden by the motorcyclist in a yellow suit. In contrast, LLaVA (Vicuna-7B) and MoCHA (Phi2-2.7B) exhibit certain limitations in understanding complex scenes. We highlight that this improvement is not attributable to increased language model size, but rather to the effective enhancement of visual information introduced by the MoCHA framework.\n\n\\begin{table}[htbp]\n\\centering\n\\footnotesize\n\\begin{tabularx}{\\columnwidth}{l X}\n\\toprule\n\\multicolumn{2}{l}{\\textbf{Visual input example, Question and Image:}} \\\\\n\\midrule\n\\multicolumn{2}{c}{\\includegraphics[width=0.8\\linewidth,height=4cm,keepaspectratio]{v1_130.jpg}} \\\\\n\\addlinespace[2pt]\n\\textbf{User} & Does the man ride a horse?\\\\\n\\addlinespace[2pt]\n\\textbf{LLaVA (Vicuna-7B)} & No, the man does not ride a horse. He is riding on the back of an elephant. \\bmark \\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Phi2-2.7B)} & No, the man does not ride a horse. He is riding an elephant. \\bmark \\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Vicuna-7B)} & No, the man does not ride a horse. He is riding an elephant. \\bmark\\\\\n\\midrule\n\\multicolumn{2}{c}{\\includegraphics[width=0.8\\linewidth,height=4cm,keepaspectratio]{v1_122.jpg}} \\\\\n\\addlinespace[2pt]\n\\textbf{User} & Is the curtain on the right side or on the left of the picture? \\\\\n\\addlinespace[2pt]\n\\textbf{LLaVA (Vicuna-7B)} & The curtain is on the left side of the picture. \\bmark \\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Phi2-2.7B)} & The curtain is on the left side of the picture. \\bmark \\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Vicuna-7B)} & The curtain is on the left side of the picture. \\bmark\\\\\n\\bottomrule\n\\end{tabularx}\n\\caption{Example comparing LLaVA and MoCHA's visual reasoning capabilities in understanding fundamental secene.}\n\\label{fig:example1}\n\\end{table}\n\\centering\n\\footnotesize\n\n\\toprule\n\\multicolumn{2}2{l}l{\\textbf{Visual input example, Question and Image:}}\\textbf{Visual input example, Question and Image:} \\\\\n\\midrule\n\\multicolumn{2}2{c}c{\\includegraphics[width=0.8\\linewidth,height=4cm,keepaspectratio]{v1_130.jpg}}\\includegraphics[width=0.8\\linewidth,height=4cm,keepaspectratio]{v1_130.jpg} \\\\\n\\addlinespace[2pt]\n\\textbf{User} & Does the man ride a horse?\\\\\n\\addlinespace[2pt]\n\\textbf{LLaVA (Vicuna-7B)} & No, the man does not ride a horse. He is riding on the back of an elephant. \\bmark \\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Phi2-2.7B)} & No, the man does not ride a horse. He is riding an elephant. \\bmark \\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Vicuna-7B)} & No, the man does not ride a horse. He is riding an elephant. \\bmark\\\\\n\\midrule\n\\multicolumn{2}2{c}c{\\includegraphics[width=0.8\\linewidth,height=4cm,keepaspectratio]{v1_122.jpg}}\\includegraphics[width=0.8\\linewidth,height=4cm,keepaspectratio]{v1_122.jpg} \\\\\n\\addlinespace[2pt]\n\\textbf{User} & Is the curtain on the right side or on the left of the picture? \\\\\n\\addlinespace[2pt]\n\\textbf{LLaVA (Vicuna-7B)} & The curtain is on the left side of the picture. \\bmark \\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Phi2-2.7B)} & The curtain is on the left side of the picture. \\bmark \\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Vicuna-7B)} & The curtain is on the left side of the picture. \\bmark\\\\\n\\bottomrule\n\n\\caption{Example comparing LLaVA and MoCHA's visual reasoning capabilities in understanding fundamental secene.}\n\\label{fig:example1}\n\n\n\\begin{table}[htbp]\n\\centering\n\\footnotesize\n\\begin{tabularx}{\\columnwidth}{l X}\n\\toprule\n\\multicolumn{2}{l}{\\textbf{Visual input example, Tricky Question and Image:}} \\\\\n\\midrule\n\\multicolumn{2}{c}{\\includegraphics[width=0.8\\linewidth,height=4cm,keepaspectratio]{v1_67.png}} \\\\\n\\addlinespace[2pt]\n\\textbf{User} & Which one is unsweetened?\\\\\n\\addlinespace[2pt]\n\\textbf{LLaVA (Vicuna-7B)} & The unsweetened milk is on the left side of the image. \\bmark\\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Phi2-2.7B)} & \\textcolor{red}{The unsweetened milk is the one that is not sweetened.} \\xmark \\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Vicuna-7B)} & The unsweetened milk is the one on the left. \\bmark\\\\\n\\midrule\n\\multicolumn{2}{c}{\\includegraphics[width=0.8\\linewidth,height=4cm,keepaspectratio]{v1_28.jpg}} \\\\\n\\addlinespace[2pt]\n\\textbf{User} & What is the number displayed on the motorcycle ridden by the motorcyclist wearing a yellow suit?\\\\\n\\addlinespace[2pt]\n\\textbf{LLaVA (Vicuna-7B)} & The number displayed on the motorcycle ridden by the motorcyclist wearing a yellow suit is 19. \\xmark\\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Phi2-2.7B)} & \\textcolor{red}{The number displayed on the motorcycle on the right is 21. } \\xmark\\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Vicuna-7B)} &The motorcyclist wearing a yellow suit is riding a motorcycle with the number 16 on it.\\bmark\\\\\n\\bottomrule\n\\end{tabularx}\n\\caption{Example comparing LLaVA and MoCHA's visual reasoning capabilities in understanding multi-object recognition. }\n\\label{fig:example2}\n\\end{table}\n\\centering\n\\footnotesize\n\n\\toprule\n\\multicolumn{2}2{l}l{\\textbf{Visual input example, Tricky Question and Image:}}\\textbf{Visual input example, Tricky Question and Image:} \\\\\n\\midrule\n\\multicolumn{2}2{c}c{\\includegraphics[width=0.8\\linewidth,height=4cm,keepaspectratio]{v1_67.png}}\\includegraphics[width=0.8\\linewidth,height=4cm,keepaspectratio]{v1_67.png} \\\\\n\\addlinespace[2pt]\n\\textbf{User} & Which one is unsweetened?\\\\\n\\addlinespace[2pt]\n\\textbf{LLaVA (Vicuna-7B)} & The unsweetened milk is on the left side of the image. \\bmark\\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Phi2-2.7B)} & \\textcolor{red}{The unsweetened milk is the one that is not sweetened.} \\xmark \\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Vicuna-7B)} & The unsweetened milk is the one on the left. \\bmark\\\\\n\\midrule\n\\multicolumn{2}2{c}c{\\includegraphics[width=0.8\\linewidth,height=4cm,keepaspectratio]{v1_28.jpg}}\\includegraphics[width=0.8\\linewidth,height=4cm,keepaspectratio]{v1_28.jpg} \\\\\n\\addlinespace[2pt]\n\\textbf{User} & What is the number displayed on the motorcycle ridden by the motorcyclist wearing a yellow suit?\\\\\n\\addlinespace[2pt]\n\\textbf{LLaVA (Vicuna-7B)} & The number displayed on the motorcycle ridden by the motorcyclist wearing a yellow suit is 19. \\xmark\\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Phi2-2.7B)} & \\textcolor{red}{The number displayed on the motorcycle on the right is 21. } \\xmark\\\\\n\\addlinespace[2pt]\n\\textbf{MoCHA (Vicuna-7B)} &The motorcyclist wearing a yellow suit is riding a motorcycle with the number 16 on it.\\bmark\\\\\n\\bottomrule\n\n\\caption{Example comparing LLaVA and MoCHA's visual reasoning capabilities in understanding multi-object recognition. }\n\\label{fig:example2}\n\n\n", "appendix": false}}, "categories": ["cs.CV", "cs.AI"], "published": "2025-07-30 16:15:22+00:00", "primary_category": "cs.CV", "summary": "Vision large language models (VLLMs) are focusing primarily on handling\ncomplex and fine-grained visual information by incorporating advanced vision\nencoders and scaling up visual models. However, these approaches face high\ntraining and inference costs, as well as challenges in extracting visual\ndetails, effectively bridging across modalities. In this work, we propose a\nnovel visual framework, MoCHA, to address these issues. Our framework\nintegrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to\nextract complementary visual features and is equipped with a sparse Mixture of\nExperts Connectors (MoECs) module to dynamically select experts tailored to\ndifferent visual dimensions. To mitigate redundant or insufficient use of the\nvisual information encoded by the MoECs module, we further design a\nHierarchical Group Attention (HGA) with intra- and inter-group operations and\nan adaptive gating strategy for encoded visual features. We train MoCHA on two\nmainstream LLMs (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance\nacross various benchmarks. Notably, MoCHA outperforms state-of-the-art\nopen-weight models on various tasks. For example, compared to CuMo\n(Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate\nhallucination by showing improvements of 3.25% in POPE and to follow visual\ninstructions by raising 153 points on MME. Finally, ablation studies further\nconfirm the effectiveness and robustness of the proposed MoECs and HGA in\nimproving the overall performance of MoCHA."}