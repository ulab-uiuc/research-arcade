{"title": "CapRecover: A Cross-Modality Feature Inversion Attack Framework on Vision Language Models", "author": "Kedong Xiu", "abstract": "\\begin{abstract}\n \n% Vision Language Models (VLMs) have become an integral part of user commonly used applications, offering seamless multimodal interactions. \n% % However, the widespread usage of VLMs also exposes users to privacy risks, particularly when sensitive information embedded in intermediate data features is leaked. \n% However, as modern Vision-Language Models (VLMs) are increasingly deployed in split DNN configurations, it is common for the visual encoder (e.g., ResNet or ViT) to be executed on user-side devices, \n\nAs Vision-Language Models (VLMs) become increasingly integrated into user-facing applications, they are often deployed in split DNN configurations, where the visual encoder (e.g., ResNet or ViT) runs on user-side devices and only intermediate features are transmitted to the cloud for downstream processing. While this setup reduces communication overhead, the intermediate data features containing sensitive information can also expose users to privacy risks. Prior work has attempted to reconstruct images from these features to infer semantics, but such approaches often produce blurry images that obscure semantic details. In contrast, the potential to directly recover high-level semantic content \u2014 such as image labels or captions \u2014 via a cross-modality inversion attack remains largely unexplored. To address this gap, we propose \\textsc{CapRecover}, a general cross-modality feature inversion framework that directly decodes semantic information from intermediate features without requiring image reconstruction. Additionally, \\textsc{CapRecover} can be used to reverse engineer traditional neural networks for computer vision tasks, such as ViT, ResNet, and others.\n\n\n% remaining the potential for user privacy breaches via a cross-modality feature inversion attack as largely unexplored. To address this gap, we propose \\textsc{CapRecover}, a cross-modality feature inversion attack that directly decodes intermediate visual features into textual descriptions. Additionally, \\textsc{CapRecover} can be used to reverse engineer traditional neural networks for computer vision tasks, such as ViT, ResNet, and others.\n\n% \\textsc{CapRecover} utilizes a visual projection layer to align these intermediate features with a pre-trained large language model (LLM), generating coherent and contextually accurate captions that may reveal private information. \n\nWe evaluate \\textsc{CapRecover} across multiple widely used datasets and victim models. Our results demonstrate that \\textsc{CapRecover} can accurately recover both image labels and captions without reconstructing a single pixel. Specifically, it achieves up to 92.71\\% Top-1 accuracy on the CIFAR-10 dataset for label recovery, and generates fluent and relevant captions from ResNet50's intermediate features on COCO2017 dataset, with ROUGE-L scores up to 0.52. Furthermore, an in-depth analysis of ResNet-based models reveals that deeper convolutional layers encode significantly more semantic information, whereas shallow layers contribute minimally to semantic leakage. Furthermore, we propose a straightforward and effective protection approach that adds random noise to the intermediate image features at each middle layer and subsequently removes the noise in the following layer. Our experiments indicate that this approach effectively prevents information leakage without additional training costs.\n\n\n% The experimental results show that even with intermediate features extracted after the final linear projection layer, \\textsc{CapRecover} can still accurately recover image captions. Our in-depth analysis of ResNet-based victim models reveals that while shallow layers contribute minimally, the deeper convolutional layers capture rich semantic information that significantly increases the risk of privacy leakage. Furthermore, we propose a straightforward and effective protection approach that adds random noise to the intermediate image features at each middle layer and subsequently removes the noise in the following layer. Our experiments indicate that this approach effectively prevents information leakage without additional training costs.\n\n\\end{abstract}", "citations": {"openai_gpt4o": {"bib_key": "openai_gpt4o", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "\\cite{openai_gpt4o}", "next_context": "), text-to-image generation (e.g., Stable Diffusion\\cite{stable_diffusion}), and optical character recognition."}], "importance_score": 1.0}, "stable_diffusion": {"bib_key": "stable_diffusion", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "\\cite{openai_gpt4o}), text-to-image generation (e.g., Stable Diffusion\\cite{stable_diffusion}", "next_context": "), and optical character recognition."}], "importance_score": 1.0}, "clip": {"bib_key": "clip", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The success of architectures like CLIP\\cite{clip}", "next_context": "and BLIP2\\cite{blip2}underscores their potential to drive significant innovations in both research and practical applications."}, {"section": "Experiments on Caption Reconstruction", "subsection": "Experimental Settings", "subsubsection": "Victim Models", "prev_context": "In practice, ResNet (e.g., ResNet50 and ResNet101) serves as the image encoder in VLMs like CLIP\\cite{clip}", "next_context": "and UPL\\cite{huang2022unsupervised}."}, {"section": "Experiments on Caption Reconstruction", "subsection": "Experimental Settings", "subsubsection": "Victim Models", "prev_context": "ViT (e.g., ViT-16B and ViT-32B) serves as the visual module in VLMs such as CLIP\\cite{clip}", "next_context": "and LlaVa\\cite{liu2023llava}."}], "importance_score": 3.0}, "blip2": {"bib_key": "blip2", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The success of architectures like CLIP\\cite{clip}and BLIP2\\cite{blip2}", "next_context": "underscores their potential to drive significant innovations in both research and practical applications."}], "importance_score": 1.0}, "gong2023figstep": {"bib_key": "gong2023figstep", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Recent research has predominantly focused on security threats such as prompt jailbreaking\u2014where attackers manipulate models to produce harmful outputs\u2014and prompt-stealing attacks that extract sensitive user prompts from generated images\\cite{gong2023figstep,luo2024jailbreakv28k,shayegani2023plug,2024_usenix_prompt_stealing_attack}", "next_context": "."}], "importance_score": 0.25}, "luo2024jailbreakv28k": {"bib_key": "luo2024jailbreakv28k", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Recent research has predominantly focused on security threats such as prompt jailbreaking\u2014where attackers manipulate models to produce harmful outputs\u2014and prompt-stealing attacks that extract sensitive user prompts from generated images\\cite{gong2023figstep,luo2024jailbreakv28k,shayegani2023plug,2024_usenix_prompt_stealing_attack}", "next_context": "."}], "importance_score": 0.25}, "shayegani2023plug": {"bib_key": "shayegani2023plug", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Recent research has predominantly focused on security threats such as prompt jailbreaking\u2014where attackers manipulate models to produce harmful outputs\u2014and prompt-stealing attacks that extract sensitive user prompts from generated images\\cite{gong2023figstep,luo2024jailbreakv28k,shayegani2023plug,2024_usenix_prompt_stealing_attack}", "next_context": "."}], "importance_score": 0.25}, "2024_usenix_prompt_stealing_attack": {"bib_key": "2024_usenix_prompt_stealing_attack", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Recent research has predominantly focused on security threats such as prompt jailbreaking\u2014where attackers manipulate models to produce harmful outputs\u2014and prompt-stealing attacks that extract sensitive user prompts from generated images\\cite{gong2023figstep,luo2024jailbreakv28k,shayegani2023plug,2024_usenix_prompt_stealing_attack}", "next_context": "."}], "importance_score": 0.25}, "mudvari2024splitllm": {"bib_key": "mudvari2024splitllm", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "~\\cite{mudvari2024splitllm,zhang2024edgeshard,he2024large,lu2024merge}", "next_context": ", where a large model is divided into multiple blocks tailored to the computational capabilities of edge devices."}], "importance_score": 0.25}, "zhang2024edgeshard": {"bib_key": "zhang2024edgeshard", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "~\\cite{mudvari2024splitllm,zhang2024edgeshard,he2024large,lu2024merge}", "next_context": ", where a large model is divided into multiple blocks tailored to the computational capabilities of edge devices."}], "importance_score": 0.25}, "he2024large": {"bib_key": "he2024large", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "~\\cite{mudvari2024splitllm,zhang2024edgeshard,he2024large,lu2024merge}", "next_context": ", where a large model is divided into multiple blocks tailored to the computational capabilities of edge devices."}, {"section": "Threat Model", "subsection": null, "subsubsection": null, "prev_context": "We assume a reasonable deployment situation consistent with practical deployment\\cite{he2024large, lu2024merge}", "next_context": "where VLMs are deployed in user-facing applications or on edge devices, which commonly keep raw images and final captions locally private, yet may expose intermediate features (e.g., when features are transmitted to a cloud service or temporarily stored in device memory)."}], "importance_score": 0.75}, "lu2024merge": {"bib_key": "lu2024merge", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "~\\cite{mudvari2024splitllm,zhang2024edgeshard,he2024large,lu2024merge}", "next_context": ", where a large model is divided into multiple blocks tailored to the computational capabilities of edge devices."}, {"section": "Threat Model", "subsection": null, "subsubsection": null, "prev_context": "We assume a reasonable deployment situation consistent with practical deployment\\cite{he2024large, lu2024merge}", "next_context": "where VLMs are deployed in user-facing applications or on edge devices, which commonly keep raw images and final captions locally private, yet may expose intermediate features (e.g., when features are transmitted to a cloud service or temporarily stored in device memory)."}], "importance_score": 0.75}, "xu2024stealthy": {"bib_key": "xu2024stealthy", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "\\vspace{-0.5em}Prior work\\cite{xu2024stealthy, zhu2025passiveinfer, he2019model}", "next_context": "has explored reconstructing images from intermediate features to further infer their semantic content."}], "importance_score": 0.3333333333333333}, "zhu2025passiveinfer": {"bib_key": "zhu2025passiveinfer", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "\\vspace{-0.5em}Prior work\\cite{xu2024stealthy, zhu2025passiveinfer, he2019model}", "next_context": "has explored reconstructing images from intermediate features to further infer their semantic content."}], "importance_score": 0.3333333333333333}, "he2019model": {"bib_key": "he2019model", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "\\vspace{-0.5em}Prior work\\cite{xu2024stealthy, zhu2025passiveinfer, he2019model}", "next_context": "has explored reconstructing images from intermediate features to further infer their semantic content."}], "importance_score": 0.3333333333333333}, "lin2014microsoft": {"bib_key": "lin2014microsoft", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments on Caption Reconstruction", "subsection": "Experimental Settings", "subsubsection": "Datasets", "prev_context": "\\vspace{-1.0em}To comprehensively evaluate the effectiveness of\\textsc{CapRecover}, we adopt three widely-used datasets: COCO2017\\cite{lin2014microsoft}", "next_context": ", Flickr8K\\cite{dst_flickr8k}, and ImageNet-1K\\cite{dst_imagenet}."}], "importance_score": 1.0}, "dst_flickr8k": {"bib_key": "dst_flickr8k", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments on Caption Reconstruction", "subsection": "Experimental Settings", "subsubsection": "Datasets", "prev_context": "\\vspace{-1.0em}To comprehensively evaluate the effectiveness of\\textsc{CapRecover}, we adopt three widely-used datasets: COCO2017\\cite{lin2014microsoft}, Flickr8K\\cite{dst_flickr8k}", "next_context": ", and ImageNet-1K\\cite{dst_imagenet}."}], "importance_score": 1.0}, "dst_imagenet": {"bib_key": "dst_imagenet", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments on Caption Reconstruction", "subsection": "Experimental Settings", "subsubsection": "Datasets", "prev_context": "\\vspace{-1.0em}To comprehensively evaluate the effectiveness of\\textsc{CapRecover}, we adopt three widely-used datasets: COCO2017\\cite{lin2014microsoft}, Flickr8K\\cite{dst_flickr8k}, and ImageNet-1K\\cite{dst_imagenet}", "next_context": "."}], "importance_score": 1.0}, "yang2024qwen2": {"bib_key": "yang2024qwen2", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments on Caption Reconstruction", "subsection": "Experimental Settings", "subsubsection": "Datasets", "prev_context": "For the ImageNet-1K dataset, we use Qwen2.5\\cite{yang2024qwen2}", "next_context": "to generate captions for the images."}, {"section": "Experiments on Caption Reconstruction", "subsection": "Experimental Settings", "subsubsection": "Evaluation Metrics", "prev_context": "\\cite{yang2024qwen2}", "next_context": "to project both the generated and ground truth captions into a shared semantic space."}], "importance_score": 2.0}, "nguyen2023improving": {"bib_key": "nguyen2023improving", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments on Caption Reconstruction", "subsection": "Experimental Settings", "subsubsection": "Datasets", "prev_context": "\\cite{nguyen2023improving, lei2023image}", "next_context": "demonstrates the effectiveness and semantic accuracy of captions generated by advanced VLMs."}], "importance_score": 0.5}, "lei2023image": {"bib_key": "lei2023image", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments on Caption Reconstruction", "subsection": "Experimental Settings", "subsubsection": "Datasets", "prev_context": "\\cite{nguyen2023improving, lei2023image}", "next_context": "demonstrates the effectiveness and semantic accuracy of captions generated by advanced VLMs."}], "importance_score": 0.5}, "dosovitskiy2020image": {"bib_key": "dosovitskiy2020image", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments on Caption Reconstruction", "subsection": "Experimental Settings", "subsubsection": "Victim Models", "prev_context": "We focus on three widely adopted visual models commonly utilized in Vision-Language Models (VLMs) and deployed on edge devices: Vision Transformer\\cite{dosovitskiy2020image}", "next_context": "(ViT), ResNet\\cite{he2016deep}, and MobileNet (MobileNetV2\\cite{sandler2018mobilenetv2}and MobileNetV3\\cite{howard2019searching})."}], "importance_score": 1.0}, "he2016deep": {"bib_key": "he2016deep", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments on Caption Reconstruction", "subsection": "Experimental Settings", "subsubsection": "Victim Models", "prev_context": "We focus on three widely adopted visual models commonly utilized in Vision-Language Models (VLMs) and deployed on edge devices: Vision Transformer\\cite{dosovitskiy2020image}(ViT), ResNet\\cite{he2016deep}", "next_context": ", and MobileNet (MobileNetV2\\cite{sandler2018mobilenetv2}and MobileNetV3\\cite{howard2019searching})."}], "importance_score": 1.0}, "sandler2018mobilenetv2": {"bib_key": "sandler2018mobilenetv2", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments on Caption Reconstruction", "subsection": "Experimental Settings", "subsubsection": "Victim Models", "prev_context": "We focus on three widely adopted visual models commonly utilized in Vision-Language Models (VLMs) and deployed on edge devices: Vision Transformer\\cite{dosovitskiy2020image}(ViT), ResNet\\cite{he2016deep}, and MobileNet (MobileNetV2\\cite{sandler2018mobilenetv2}", "next_context": "and MobileNetV3\\cite{howard2019searching})."}], "importance_score": 1.0}, "howard2019searching": {"bib_key": "howard2019searching", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments on Caption Reconstruction", "subsection": "Experimental Settings", "subsubsection": "Victim Models", "prev_context": "We focus on three widely adopted visual models commonly utilized in Vision-Language Models (VLMs) and deployed on edge devices: Vision Transformer\\cite{dosovitskiy2020image}(ViT), ResNet\\cite{he2016deep}, and MobileNet (MobileNetV2\\cite{sandler2018mobilenetv2}and MobileNetV3\\cite{howard2019searching}", "next_context": ")."}], "importance_score": 1.0}, "huang2022unsupervised": {"bib_key": "huang2022unsupervised", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments on Caption Reconstruction", "subsection": "Experimental Settings", "subsubsection": "Victim Models", "prev_context": "In practice, ResNet (e.g., ResNet50 and ResNet101) serves as the image encoder in VLMs like CLIP\\cite{clip}and UPL\\cite{huang2022unsupervised}", "next_context": "."}], "importance_score": 1.0}, "liu2023llava": {"bib_key": "liu2023llava", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments on Caption Reconstruction", "subsection": "Experimental Settings", "subsubsection": "Victim Models", "prev_context": "ViT (e.g., ViT-16B and ViT-32B) serves as the visual module in VLMs such as CLIP\\cite{clip}and LlaVa\\cite{liu2023llava}", "next_context": "."}], "importance_score": 1.0}, "li2022efficientformer": {"bib_key": "li2022efficientformer", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments on Caption Reconstruction", "subsection": "Experimental Results", "subsubsection": "Overall results", "prev_context": "MobileNet utilizes depthwise separable convolutions and aggressive dimensionality reduction strategies designed for efficiency, which reduce model complexity but significantly compromise the model\u2019s ability to capture detailed and high-level semantic features\\cite{li2022efficientformer,li2023rethinking,vasu2023fastvit}", "next_context": "."}], "importance_score": 0.3333333333333333}, "li2023rethinking": {"bib_key": "li2023rethinking", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments on Caption Reconstruction", "subsection": "Experimental Results", "subsubsection": "Overall results", "prev_context": "MobileNet utilizes depthwise separable convolutions and aggressive dimensionality reduction strategies designed for efficiency, which reduce model complexity but significantly compromise the model\u2019s ability to capture detailed and high-level semantic features\\cite{li2022efficientformer,li2023rethinking,vasu2023fastvit}", "next_context": "."}], "importance_score": 0.3333333333333333}, "vasu2023fastvit": {"bib_key": "vasu2023fastvit", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments on Caption Reconstruction", "subsection": "Experimental Results", "subsubsection": "Overall results", "prev_context": "MobileNet utilizes depthwise separable convolutions and aggressive dimensionality reduction strategies designed for efficiency, which reduce model complexity but significantly compromise the model\u2019s ability to capture detailed and high-level semantic features\\cite{li2022efficientformer,li2023rethinking,vasu2023fastvit}", "next_context": "."}], "importance_score": 0.3333333333333333}, "wiki_def_he": {"bib_key": "wiki_def_he", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Discussion on Potential Defense Mechanisms", "subsection": "Potential for Homomorphic Encryption", "subsubsection": null, "prev_context": "Homomorphic Encryption (HE)\\cite{wiki_def_he}", "next_context": "represents a promising cryptographic approach to mitigating privacy risks associated with Vision-Language Models (VLMs)."}], "importance_score": 1.0}, "xu2025secure": {"bib_key": "xu2025secure", "bib_title": "author", "bib_author ": "%", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Discussion on Potential Defense Mechanisms", "subsection": "Potential for Homomorphic Encryption", "subsubsection": null, "prev_context": "\\cite{xu2025secure}", "next_context": ", employing CUDA acceleration to achieve efficient privacy-preserving federated learning."}], "importance_score": 1.0}}, "refs": [], "table": [{"original": "\\begin{table}[t]\n    \\centering\n    \\caption{Datasets Used in this paper.}\n    \\label{tab:dsts_used}\n    \\vspace{-1.0em}\n    \\begin{threeparttable}\n    \n    % \\fontsize{6.5}{8}\\selectfont % \u8c03\u6574\u5b57\u4f53\u5927\u5c0f\n    \\sizefive\n    \\setlength{\\tabcolsep}{4pt} % \u8c03\u6574\u5217\u95f4\u8ddd\n    \\renewcommand{\\arraystretch}{1.1} % \u8c03\u6574\u884c\u95f4\u8ddd\n    \\begin{tabular}{crrrr}\n    \\toprule\n    \\textbf{Dataset}{$^\\dagger$} & \\makecell[c]{\\textbf{Training size}} & \\makecell[c]{\\textbf{Sample size}} & \\makecell[c]{\\textbf{Test size}} & \\makecell[c]{\\textbf{Sample size}} \\\\ \n    \\midrule\n    COCO2017 & 118,287 & 30,000 & 5,000 & 5,000 \\\\\n    Flickr8K & 6,000 & 6,000 & 1,000 & 1,000 \\\\\n    ImageNet-1K & 1,281,167 & 12,000 & 50,000 & 900 \\\\\n    \\hline\n    CIFAR-10 & 50,000 & 50,000 & 10,000 & 10,000 \\\\\n    TinyImageNet & 100,000 & 100,000 & 10,000 & 10,000 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\begin{tablenotes}\n        \\item \\hspace{-1.2em}$^{\\dagger}$ {We use CIFAR-10 and TinyImageNet datasets for image label recovery and COCO2017, Flickr8K and ImageNet-1K datasets for image caption reconstruction, respectively.}\n    \\end{tablenotes}\n    \\end{threeparttable}\n    \\vspace{-0.9em}\n\\end{table}", "caption": "\\caption{Datasets Used in this paper.}", "label": "\\label{tab:dsts_used}", "tabular": "\\begin{tabular}{crrrr}\n    \\toprule\n    \\textbf{Dataset}{$^\\dagger$} & \\makecell[c]{\\textbf{Training size}} & \\makecell[c]{\\textbf{Sample size}} & \\makecell[c]{\\textbf{Test size}} & \\makecell[c]{\\textbf{Sample size}} \\\\ \n    \\midrule\n    COCO2017 & 118,287 & 30,000 & 5,000 & 5,000 \\\\\n    Flickr8K & 6,000 & 6,000 & 1,000 & 1,000 \\\\\n    ImageNet-1K & 1,281,167 & 12,000 & 50,000 & 900 \\\\\n    \\hline\n    CIFAR-10 & 50,000 & 50,000 & 10,000 & 10,000 \\\\\n    TinyImageNet & 100,000 & 100,000 & 10,000 & 10,000 \\\\\n    \\bottomrule\n    \\end{tabular}", "subtables": []}, {"original": "\\begin{threeparttable}\n    \n    % \\fontsize{6.5}{8}\\selectfont % \u8c03\u6574\u5b57\u4f53\u5927\u5c0f\n    \\sizefive\n    \\setlength{\\tabcolsep}{4pt} % \u8c03\u6574\u5217\u95f4\u8ddd\n    \\renewcommand{\\arraystretch}{1.1} % \u8c03\u6574\u884c\u95f4\u8ddd\n    \\begin{tabular}{crrrr}\n    \\toprule\n    \\textbf{Dataset}{$^\\dagger$} & \\makecell[c]{\\textbf{Training size}} & \\makecell[c]{\\textbf{Sample size}} & \\makecell[c]{\\textbf{Test size}} & \\makecell[c]{\\textbf{Sample size}} \\\\ \n    \\midrule\n    COCO2017 & 118,287 & 30,000 & 5,000 & 5,000 \\\\\n    Flickr8K & 6,000 & 6,000 & 1,000 & 1,000 \\\\\n    ImageNet-1K & 1,281,167 & 12,000 & 50,000 & 900 \\\\\n    \\hline\n    CIFAR-10 & 50,000 & 50,000 & 10,000 & 10,000 \\\\\n    TinyImageNet & 100,000 & 100,000 & 10,000 & 10,000 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\begin{tablenotes}\n        \\item \\hspace{-1.2em}$^{\\dagger}$ {We use CIFAR-10 and TinyImageNet datasets for image label recovery and COCO2017, Flickr8K and ImageNet-1K datasets for image caption reconstruction, respectively.}\n    \\end{tablenotes}\n    \\end{threeparttable}", "caption": "", "label": null, "tabular": "\\begin{tabular}{crrrr}\n    \\toprule\n    \\textbf{Dataset}{$^\\dagger$} & \\makecell[c]{\\textbf{Training size}} & \\makecell[c]{\\textbf{Sample size}} & \\makecell[c]{\\textbf{Test size}} & \\makecell[c]{\\textbf{Sample size}} \\\\ \n    \\midrule\n    COCO2017 & 118,287 & 30,000 & 5,000 & 5,000 \\\\\n    Flickr8K & 6,000 & 6,000 & 1,000 & 1,000 \\\\\n    ImageNet-1K & 1,281,167 & 12,000 & 50,000 & 900 \\\\\n    \\hline\n    CIFAR-10 & 50,000 & 50,000 & 10,000 & 10,000 \\\\\n    TinyImageNet & 100,000 & 100,000 & 10,000 & 10,000 \\\\\n    \\bottomrule\n    \\end{tabular}", "subtables": []}, {"original": "\\begin{tablenotes}\n        \\item \\hspace{-1.2em}$^{\\dagger}$ {We use CIFAR-10 and TinyImageNet datasets for image label recovery and COCO2017, Flickr8K and ImageNet-1K datasets for image caption reconstruction, respectively.}\n    \\end{tablenotes}", "caption": "", "label": null, "tabular": "", "subtables": []}, {"original": "\\begin{table}[t]\n    \\centering\n    \\caption{Information of different victim models and their intermediate layers' shapes.}\n    \\label{tab:victim_info}\n    \\vspace{-1.0em} \n    \\begin{threeparttable}\n        % \\fontsize{6.5}{8}\\selectfont\n        \\sizefive\n        \\setlength{\\tabcolsep}{4pt}\n        \\renewcommand{\\arraystretch}{1.05}\n        \\begin{tabular}{ccc}\n            \\toprule\n            \\textbf{Victim model} & \\textbf{Intermediate layer} & \\textbf{Output feature dimension$^{*}$} \\\\\n            \\midrule\n            $\\text{CLIP}_{\\text{ViT16}}$   & base & 512 \\\\\n             $\\text{CLIP}_{\\text{ViT32}}$ & no-proj & 768 \\\\\n             \\hline\n             % \\hline\n            \\multirow{5}{*}{\\makecell[c]{ResNet50\\\\(ResNet101)}} & base & 1024 (512) \\\\\n             & layer1 & [256, 56, 56] $\\rightarrow$ 1024 \\\\\n             & layer2 & [512, 28, 28] $\\rightarrow$ 1024 \\\\\n             & layer3 & [1024, 14, 14] $\\rightarrow$ 1024 \\\\\n             & layer4 & [2048, 7, 7] $\\rightarrow$ 1024 \\\\\n             % \\hline\n             \\hline\n            % \\multirow{5}{*}{ResNet101} & base & 1024 \\\\\n            %  & layer1 & 1024 \\\\\n            %  & layer2 & 1024 \\\\\n            %  & layer3 & 1024 \\\\\n            %  & layer4 & 1024 \\\\\n            %  \\hline\n            %  \\hline\n            % \\multirow{2}{*}{MobileNetV2} & base & 1000 \\\\\n            %   & layer1 & [32, 112, 112] $\\rightarrow$ 1024 \\\\\n            % \\hline\n            % \\hline\n            % \\multirow{2}{*}{MobileNetV3} & base & 1000 \\\\\n            %   & layer1 & [16, 112, 112] $\\rightarrow$ 1024 \\\\\n            MobileNetV2 & \\multirow{2}{*}{base} & \\multirow{2}{*}{1000} \\\\\n            MobileNetV3 & & \\\\\n            \\bottomrule\n        \\end{tabular}\n        \\begin{tablenotes}\n            \\item \\hspace{-1.2em}$^{*}$ {We use a ResNet-based projection module to transform those intermediate features retaining spatial dimensions (e.g., [32, 112, 112]) into a unified vectorized feature space (i.e., $\\mathbb{R}^{1024}$).}\n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\vspace{-1.2em}\n\\end{table}", "caption": "\\caption{Information of different victim models and their intermediate layers' shapes.}", "label": "\\label{tab:victim_info}", "tabular": "\\begin{tabular}{ccc}\n            \\toprule\n            \\textbf{Victim model} & \\textbf{Intermediate layer} & \\textbf{Output feature dimension$^{*}$} \\\\\n            \\midrule\n            $\\text{CLIP}_{\\text{ViT16}}$   & base & 512 \\\\\n             $\\text{CLIP}_{\\text{ViT32}}$ & no-proj & 768 \\\\\n             \\hline\n             % \\hline\n            \\multirow{5}{*}{\\makecell[c]{ResNet50\\\\(ResNet101)}} & base & 1024 (512) \\\\\n             & layer1 & [256, 56, 56] $\\rightarrow$ 1024 \\\\\n             & layer2 & [512, 28, 28] $\\rightarrow$ 1024 \\\\\n             & layer3 & [1024, 14, 14] $\\rightarrow$ 1024 \\\\\n             & layer4 & [2048, 7, 7] $\\rightarrow$ 1024 \\\\\n             % \\hline\n             \\hline\n            % \\multirow{5}{*}{ResNet101} & base & 1024 \\\\\n            %  & layer1 & 1024 \\\\\n            %  & layer2 & 1024 \\\\\n            %  & layer3 & 1024 \\\\\n            %  & layer4 & 1024 \\\\\n            %  \\hline\n            %  \\hline\n            % \\multirow{2}{*}{MobileNetV2} & base & 1000 \\\\\n            %   & layer1 & [32, 112, 112] $\\rightarrow$ 1024 \\\\\n            % \\hline\n            % \\hline\n            % \\multirow{2}{*}{MobileNetV3} & base & 1000 \\\\\n            %   & layer1 & [16, 112, 112] $\\rightarrow$ 1024 \\\\\n            MobileNetV2 & \\multirow{2}{*}{base} & \\multirow{2}{*}{1000} \\\\\n            MobileNetV3 & & \\\\\n            \\bottomrule\n        \\end{tabular}", "subtables": []}, {"original": "\\begin{threeparttable}\n        % \\fontsize{6.5}{8}\\selectfont\n        \\sizefive\n        \\setlength{\\tabcolsep}{4pt}\n        \\renewcommand{\\arraystretch}{1.05}\n        \\begin{tabular}{ccc}\n            \\toprule\n            \\textbf{Victim model} & \\textbf{Intermediate layer} & \\textbf{Output feature dimension$^{*}$} \\\\\n            \\midrule\n            $\\text{CLIP}_{\\text{ViT16}}$   & base & 512 \\\\\n             $\\text{CLIP}_{\\text{ViT32}}$ & no-proj & 768 \\\\\n             \\hline\n             % \\hline\n            \\multirow{5}{*}{\\makecell[c]{ResNet50\\\\(ResNet101)}} & base & 1024 (512) \\\\\n             & layer1 & [256, 56, 56] $\\rightarrow$ 1024 \\\\\n             & layer2 & [512, 28, 28] $\\rightarrow$ 1024 \\\\\n             & layer3 & [1024, 14, 14] $\\rightarrow$ 1024 \\\\\n             & layer4 & [2048, 7, 7] $\\rightarrow$ 1024 \\\\\n             % \\hline\n             \\hline\n            % \\multirow{5}{*}{ResNet101} & base & 1024 \\\\\n            %  & layer1 & 1024 \\\\\n            %  & layer2 & 1024 \\\\\n            %  & layer3 & 1024 \\\\\n            %  & layer4 & 1024 \\\\\n            %  \\hline\n            %  \\hline\n            % \\multirow{2}{*}{MobileNetV2} & base & 1000 \\\\\n            %   & layer1 & [32, 112, 112] $\\rightarrow$ 1024 \\\\\n            % \\hline\n            % \\hline\n            % \\multirow{2}{*}{MobileNetV3} & base & 1000 \\\\\n            %   & layer1 & [16, 112, 112] $\\rightarrow$ 1024 \\\\\n            MobileNetV2 & \\multirow{2}{*}{base} & \\multirow{2}{*}{1000} \\\\\n            MobileNetV3 & & \\\\\n            \\bottomrule\n        \\end{tabular}\n        \\begin{tablenotes}\n            \\item \\hspace{-1.2em}$^{*}$ {We use a ResNet-based projection module to transform those intermediate features retaining spatial dimensions (e.g., [32, 112, 112]) into a unified vectorized feature space (i.e., $\\mathbb{R}^{1024}$).}\n        \\end{tablenotes}\n    \\end{threeparttable}", "caption": "", "label": null, "tabular": "\\begin{tabular}{ccc}\n            \\toprule\n            \\textbf{Victim model} & \\textbf{Intermediate layer} & \\textbf{Output feature dimension$^{*}$} \\\\\n            \\midrule\n            $\\text{CLIP}_{\\text{ViT16}}$   & base & 512 \\\\\n             $\\text{CLIP}_{\\text{ViT32}}$ & no-proj & 768 \\\\\n             \\hline\n             % \\hline\n            \\multirow{5}{*}{\\makecell[c]{ResNet50\\\\(ResNet101)}} & base & 1024 (512) \\\\\n             & layer1 & [256, 56, 56] $\\rightarrow$ 1024 \\\\\n             & layer2 & [512, 28, 28] $\\rightarrow$ 1024 \\\\\n             & layer3 & [1024, 14, 14] $\\rightarrow$ 1024 \\\\\n             & layer4 & [2048, 7, 7] $\\rightarrow$ 1024 \\\\\n             % \\hline\n             \\hline\n            % \\multirow{5}{*}{ResNet101} & base & 1024 \\\\\n            %  & layer1 & 1024 \\\\\n            %  & layer2 & 1024 \\\\\n            %  & layer3 & 1024 \\\\\n            %  & layer4 & 1024 \\\\\n            %  \\hline\n            %  \\hline\n            % \\multirow{2}{*}{MobileNetV2} & base & 1000 \\\\\n            %   & layer1 & [32, 112, 112] $\\rightarrow$ 1024 \\\\\n            % \\hline\n            % \\hline\n            % \\multirow{2}{*}{MobileNetV3} & base & 1000 \\\\\n            %   & layer1 & [16, 112, 112] $\\rightarrow$ 1024 \\\\\n            MobileNetV2 & \\multirow{2}{*}{base} & \\multirow{2}{*}{1000} \\\\\n            MobileNetV3 & & \\\\\n            \\bottomrule\n        \\end{tabular}", "subtables": []}, {"original": "\\begin{tablenotes}\n            \\item \\hspace{-1.2em}$^{*}$ {We use a ResNet-based projection module to transform those intermediate features retaining spatial dimensions (e.g., [32, 112, 112]) into a unified vectorized feature space (i.e., $\\mathbb{R}^{1024}$).}\n        \\end{tablenotes}", "caption": "", "label": null, "tabular": "", "subtables": []}, {"original": "\\begin{table*}\n    \\centering\n    \\caption{Experimental results of \\textsc{CapRecover} attacking different victim models on three datasets.}\n    \\label{tab:exp_res_coco2017}\n    \\vspace{-1.0em}\n    \\begin{threeparttable}\n        \\sizefive\n        \\setlength{\\tabcolsep}{4pt}\n        \\renewcommand{\\arraystretch}{1.05}\n        \\begin{tabular}{ccccccccccc}\n            \\toprule\n            \\textbf{Dataset} & \\textbf{Victim model}$^{*}$ & \\textbf{BLEU-1} & \\textbf{BLEU-2} & \\textbf{BLEU-3} & \\textbf{BLEU-4} & \\textbf{METEOR} & \\textbf{ROUGE\\_L} & \\textbf{CIDEr} & \\textbf{SPICE} & \\makecell[c]{\\textbf{Cosine Similarity (\\%)}$^{\\ddagger}$} \\\\\n            \\midrule\n            \\multirow{6}{*}{COCO2017} & $\\text{CLIP}_{\\text{ViT16}}$    & 0.72        & 0.55        & 0.41  & 0.30 & 0.26 & 0.53 & 0.99 & 0.19 & 84.38       \\\\\n            & $\\text{CLIP}_{\\text{ViT32}}$     & 0.70        & 0.53        & 0.39  & 0.29 & 0.26 & 0.53 & 0.95 & 0.19 & 80.38       \\\\\n            & RN50     & 0.70        & 0.52        & 0.38  & 0.28 & 0.25 & 0.52 & 0.90 & 0.18 & 76.84       \\\\\n            & RN101     & 0.70        & 0.52        & 0.39  & 0.28 & 0.25 & 0.53 & 0.93 & 0.18 & 79.98       \\\\\n            & MNV2     & 0.39        & 0.18        & 0.10  & 0.06 & 0.11 & 0.31 & 0.09 & 0.03 & ~0.44     \\\\\n            & MNV3     & 0.40        & 0.19        & 0.10  & 0.08 & 0.11 & 0.31 & 0.10 & 0.03 & ~2.74       \\\\\n            \\hline\n            \\hline\n            \\multirow{6}{*}{Flickr8K} & $\\text{CLIP}_{\\text{ViT16}}$    & 0.30        & 0.15        & 0.08  & 0.05 & 0.13 & 0.27 & 0.54 & 0.19 & 22.40         \\\\\n            & $\\text{CLIP}_{\\text{ViT32}}$     & 0.29        & 0.15        & 0.08  & 0.05 & 0.12 & 0.26 & 0.48 & 0.17 & 18.40        \\\\\n            & RN50     & 0.28        & 0.14        & 0.08  & 0.04 & 0.12 & 0.25 & 0.46 & 0.17 & 16.50        \\\\\n            & RN101    & 0.28        & 0.14        & 0.08  & 0.05 & 0.12 & 0.25 & 0.47 & 0.17 & 18.00       \\\\\n            & MNV2     & 0.20        & 0.08        & 0.04  & 0.02 & 0.07 & 0.17 & 0.16 & 0.06 & ~1.20        \\\\\n            & MNV3     & 0.21        & 0.08        & 0.04  & 0.02 & 0.07 & 0.17 & 0.17 & 0.06 & ~1.80        \\\\\n            \\hline\n            \\hline\n            \\multirow{6}{*}{ImageNet-1K} & $\\text{CLIP}_{\\text{ViT16}}$     & 0.45        & 0.30        & 0.21  & 0.15 & 0.18 & 0.41 & 1.18 & 0.2 & 40.78       \\\\\n            & $\\text{CLIP}_{\\text{ViT32}}$     & 0.44        & 0.29        & 0.20  & 0.14 & 0.18 & 0.40 & 1.08 & 0.19 & 36.11        \\\\\n            & RN50     & 0.42        & 0.27        & 0.18  & 0.13 & 0.16 & 0.38 & 0.93 & 0.16 & 27.00       \\\\\n            & RN101     & 0.42        & 0.27        & 0.18  & 0.13 & 0.16 & 0.38 & 0.95 & 0.16 & 27.78      \\\\\n            & MNV2     & 0.29        & 0.15        & 0.07  & 0.03 & 0.09 & 0.26 & 0.18 & 0.03 & ~1.67      \\\\\n            & MNV3     & 0.29        & 0.14        & 0.07  & 0.04 & 0.09 & 0.26 & 0.22 & 0.04 & ~6.11    \\\\\n            \\bottomrule\n        \\end{tabular}\n        \\begin{tablenotes}\n            \\item \\hspace{-1.2em}$^{*}$ {``RN50'' (``RN101'') denotes ResNet50 (ResNet101) and ``MNV2'' (MNV3) denotes MobileNetV2 (MobileNetV3). }\n            % Here we utilize the final output image features of the corresponding model as intermediate features. The experimental results for the middle layer of different models are shown in Table \\ref{tab:res_model_middle_layers}.}\n            \\item \\hspace{-1.2em}$^{\\ddagger}$ {We calculate the proportion of cosine similarities that are greater than a predefined threshold, which is empirically set to 0.7 in our paper.}\n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\vspace{-1.1em}\n\\end{table*}", "caption": "\\caption{Experimental results of \\textsc{CapRecover} attacking different victim models on three datasets.}", "label": "\\label{tab:exp_res_coco2017}", "tabular": "\\begin{tabular}{ccccccccccc}\n            \\toprule\n            \\textbf{Dataset} & \\textbf{Victim model}$^{*}$ & \\textbf{BLEU-1} & \\textbf{BLEU-2} & \\textbf{BLEU-3} & \\textbf{BLEU-4} & \\textbf{METEOR} & \\textbf{ROUGE\\_L} & \\textbf{CIDEr} & \\textbf{SPICE} & \\makecell[c]{\\textbf{Cosine Similarity (\\%)}$^{\\ddagger}$} \\\\\n            \\midrule\n            \\multirow{6}{*}{COCO2017} & $\\text{CLIP}_{\\text{ViT16}}$    & 0.72        & 0.55        & 0.41  & 0.30 & 0.26 & 0.53 & 0.99 & 0.19 & 84.38       \\\\\n            & $\\text{CLIP}_{\\text{ViT32}}$     & 0.70        & 0.53        & 0.39  & 0.29 & 0.26 & 0.53 & 0.95 & 0.19 & 80.38       \\\\\n            & RN50     & 0.70        & 0.52        & 0.38  & 0.28 & 0.25 & 0.52 & 0.90 & 0.18 & 76.84       \\\\\n            & RN101     & 0.70        & 0.52        & 0.39  & 0.28 & 0.25 & 0.53 & 0.93 & 0.18 & 79.98       \\\\\n            & MNV2     & 0.39        & 0.18        & 0.10  & 0.06 & 0.11 & 0.31 & 0.09 & 0.03 & ~0.44     \\\\\n            & MNV3     & 0.40        & 0.19        & 0.10  & 0.08 & 0.11 & 0.31 & 0.10 & 0.03 & ~2.74       \\\\\n            \\hline\n            \\hline\n            \\multirow{6}{*}{Flickr8K} & $\\text{CLIP}_{\\text{ViT16}}$    & 0.30        & 0.15        & 0.08  & 0.05 & 0.13 & 0.27 & 0.54 & 0.19 & 22.40         \\\\\n            & $\\text{CLIP}_{\\text{ViT32}}$     & 0.29        & 0.15        & 0.08  & 0.05 & 0.12 & 0.26 & 0.48 & 0.17 & 18.40        \\\\\n            & RN50     & 0.28        & 0.14        & 0.08  & 0.04 & 0.12 & 0.25 & 0.46 & 0.17 & 16.50        \\\\\n            & RN101    & 0.28        & 0.14        & 0.08  & 0.05 & 0.12 & 0.25 & 0.47 & 0.17 & 18.00       \\\\\n            & MNV2     & 0.20        & 0.08        & 0.04  & 0.02 & 0.07 & 0.17 & 0.16 & 0.06 & ~1.20        \\\\\n            & MNV3     & 0.21        & 0.08        & 0.04  & 0.02 & 0.07 & 0.17 & 0.17 & 0.06 & ~1.80        \\\\\n            \\hline\n            \\hline\n            \\multirow{6}{*}{ImageNet-1K} & $\\text{CLIP}_{\\text{ViT16}}$     & 0.45        & 0.30        & 0.21  & 0.15 & 0.18 & 0.41 & 1.18 & 0.2 & 40.78       \\\\\n            & $\\text{CLIP}_{\\text{ViT32}}$     & 0.44        & 0.29        & 0.20  & 0.14 & 0.18 & 0.40 & 1.08 & 0.19 & 36.11        \\\\\n            & RN50     & 0.42        & 0.27        & 0.18  & 0.13 & 0.16 & 0.38 & 0.93 & 0.16 & 27.00       \\\\\n            & RN101     & 0.42        & 0.27        & 0.18  & 0.13 & 0.16 & 0.38 & 0.95 & 0.16 & 27.78      \\\\\n            & MNV2     & 0.29        & 0.15        & 0.07  & 0.03 & 0.09 & 0.26 & 0.18 & 0.03 & ~1.67      \\\\\n            & MNV3     & 0.29        & 0.14        & 0.07  & 0.04 & 0.09 & 0.26 & 0.22 & 0.04 & ~6.11    \\\\\n            \\bottomrule\n        \\end{tabular}", "subtables": []}, {"original": "\\begin{threeparttable}\n        \\sizefive\n        \\setlength{\\tabcolsep}{4pt}\n        \\renewcommand{\\arraystretch}{1.05}\n        \\begin{tabular}{ccccccccccc}\n            \\toprule\n            \\textbf{Dataset} & \\textbf{Victim model}$^{*}$ & \\textbf{BLEU-1} & \\textbf{BLEU-2} & \\textbf{BLEU-3} & \\textbf{BLEU-4} & \\textbf{METEOR} & \\textbf{ROUGE\\_L} & \\textbf{CIDEr} & \\textbf{SPICE} & \\makecell[c]{\\textbf{Cosine Similarity (\\%)}$^{\\ddagger}$} \\\\\n            \\midrule\n            \\multirow{6}{*}{COCO2017} & $\\text{CLIP}_{\\text{ViT16}}$    & 0.72        & 0.55        & 0.41  & 0.30 & 0.26 & 0.53 & 0.99 & 0.19 & 84.38       \\\\\n            & $\\text{CLIP}_{\\text{ViT32}}$     & 0.70        & 0.53        & 0.39  & 0.29 & 0.26 & 0.53 & 0.95 & 0.19 & 80.38       \\\\\n            & RN50     & 0.70        & 0.52        & 0.38  & 0.28 & 0.25 & 0.52 & 0.90 & 0.18 & 76.84       \\\\\n            & RN101     & 0.70        & 0.52        & 0.39  & 0.28 & 0.25 & 0.53 & 0.93 & 0.18 & 79.98       \\\\\n            & MNV2     & 0.39        & 0.18        & 0.10  & 0.06 & 0.11 & 0.31 & 0.09 & 0.03 & ~0.44     \\\\\n            & MNV3     & 0.40        & 0.19        & 0.10  & 0.08 & 0.11 & 0.31 & 0.10 & 0.03 & ~2.74       \\\\\n            \\hline\n            \\hline\n            \\multirow{6}{*}{Flickr8K} & $\\text{CLIP}_{\\text{ViT16}}$    & 0.30        & 0.15        & 0.08  & 0.05 & 0.13 & 0.27 & 0.54 & 0.19 & 22.40         \\\\\n            & $\\text{CLIP}_{\\text{ViT32}}$     & 0.29        & 0.15        & 0.08  & 0.05 & 0.12 & 0.26 & 0.48 & 0.17 & 18.40        \\\\\n            & RN50     & 0.28        & 0.14        & 0.08  & 0.04 & 0.12 & 0.25 & 0.46 & 0.17 & 16.50        \\\\\n            & RN101    & 0.28        & 0.14        & 0.08  & 0.05 & 0.12 & 0.25 & 0.47 & 0.17 & 18.00       \\\\\n            & MNV2     & 0.20        & 0.08        & 0.04  & 0.02 & 0.07 & 0.17 & 0.16 & 0.06 & ~1.20        \\\\\n            & MNV3     & 0.21        & 0.08        & 0.04  & 0.02 & 0.07 & 0.17 & 0.17 & 0.06 & ~1.80        \\\\\n            \\hline\n            \\hline\n            \\multirow{6}{*}{ImageNet-1K} & $\\text{CLIP}_{\\text{ViT16}}$     & 0.45        & 0.30        & 0.21  & 0.15 & 0.18 & 0.41 & 1.18 & 0.2 & 40.78       \\\\\n            & $\\text{CLIP}_{\\text{ViT32}}$     & 0.44        & 0.29        & 0.20  & 0.14 & 0.18 & 0.40 & 1.08 & 0.19 & 36.11        \\\\\n            & RN50     & 0.42        & 0.27        & 0.18  & 0.13 & 0.16 & 0.38 & 0.93 & 0.16 & 27.00       \\\\\n            & RN101     & 0.42        & 0.27        & 0.18  & 0.13 & 0.16 & 0.38 & 0.95 & 0.16 & 27.78      \\\\\n            & MNV2     & 0.29        & 0.15        & 0.07  & 0.03 & 0.09 & 0.26 & 0.18 & 0.03 & ~1.67      \\\\\n            & MNV3     & 0.29        & 0.14        & 0.07  & 0.04 & 0.09 & 0.26 & 0.22 & 0.04 & ~6.11    \\\\\n            \\bottomrule\n        \\end{tabular}\n        \\begin{tablenotes}\n            \\item \\hspace{-1.2em}$^{*}$ {``RN50'' (``RN101'') denotes ResNet50 (ResNet101) and ``MNV2'' (MNV3) denotes MobileNetV2 (MobileNetV3). }\n            % Here we utilize the final output image features of the corresponding model as intermediate features. The experimental results for the middle layer of different models are shown in Table \\ref{tab:res_model_middle_layers}.}\n            \\item \\hspace{-1.2em}$^{\\ddagger}$ {We calculate the proportion of cosine similarities that are greater than a predefined threshold, which is empirically set to 0.7 in our paper.}\n        \\end{tablenotes}\n    \\end{threeparttable}", "caption": "", "label": null, "tabular": "\\begin{tabular}{ccccccccccc}\n            \\toprule\n            \\textbf{Dataset} & \\textbf{Victim model}$^{*}$ & \\textbf{BLEU-1} & \\textbf{BLEU-2} & \\textbf{BLEU-3} & \\textbf{BLEU-4} & \\textbf{METEOR} & \\textbf{ROUGE\\_L} & \\textbf{CIDEr} & \\textbf{SPICE} & \\makecell[c]{\\textbf{Cosine Similarity (\\%)}$^{\\ddagger}$} \\\\\n            \\midrule\n            \\multirow{6}{*}{COCO2017} & $\\text{CLIP}_{\\text{ViT16}}$    & 0.72        & 0.55        & 0.41  & 0.30 & 0.26 & 0.53 & 0.99 & 0.19 & 84.38       \\\\\n            & $\\text{CLIP}_{\\text{ViT32}}$     & 0.70        & 0.53        & 0.39  & 0.29 & 0.26 & 0.53 & 0.95 & 0.19 & 80.38       \\\\\n            & RN50     & 0.70        & 0.52        & 0.38  & 0.28 & 0.25 & 0.52 & 0.90 & 0.18 & 76.84       \\\\\n            & RN101     & 0.70        & 0.52        & 0.39  & 0.28 & 0.25 & 0.53 & 0.93 & 0.18 & 79.98       \\\\\n            & MNV2     & 0.39        & 0.18        & 0.10  & 0.06 & 0.11 & 0.31 & 0.09 & 0.03 & ~0.44     \\\\\n            & MNV3     & 0.40        & 0.19        & 0.10  & 0.08 & 0.11 & 0.31 & 0.10 & 0.03 & ~2.74       \\\\\n            \\hline\n            \\hline\n            \\multirow{6}{*}{Flickr8K} & $\\text{CLIP}_{\\text{ViT16}}$    & 0.30        & 0.15        & 0.08  & 0.05 & 0.13 & 0.27 & 0.54 & 0.19 & 22.40         \\\\\n            & $\\text{CLIP}_{\\text{ViT32}}$     & 0.29        & 0.15        & 0.08  & 0.05 & 0.12 & 0.26 & 0.48 & 0.17 & 18.40        \\\\\n            & RN50     & 0.28        & 0.14        & 0.08  & 0.04 & 0.12 & 0.25 & 0.46 & 0.17 & 16.50        \\\\\n            & RN101    & 0.28        & 0.14        & 0.08  & 0.05 & 0.12 & 0.25 & 0.47 & 0.17 & 18.00       \\\\\n            & MNV2     & 0.20        & 0.08        & 0.04  & 0.02 & 0.07 & 0.17 & 0.16 & 0.06 & ~1.20        \\\\\n            & MNV3     & 0.21        & 0.08        & 0.04  & 0.02 & 0.07 & 0.17 & 0.17 & 0.06 & ~1.80        \\\\\n            \\hline\n            \\hline\n            \\multirow{6}{*}{ImageNet-1K} & $\\text{CLIP}_{\\text{ViT16}}$     & 0.45        & 0.30        & 0.21  & 0.15 & 0.18 & 0.41 & 1.18 & 0.2 & 40.78       \\\\\n            & $\\text{CLIP}_{\\text{ViT32}}$     & 0.44        & 0.29        & 0.20  & 0.14 & 0.18 & 0.40 & 1.08 & 0.19 & 36.11        \\\\\n            & RN50     & 0.42        & 0.27        & 0.18  & 0.13 & 0.16 & 0.38 & 0.93 & 0.16 & 27.00       \\\\\n            & RN101     & 0.42        & 0.27        & 0.18  & 0.13 & 0.16 & 0.38 & 0.95 & 0.16 & 27.78      \\\\\n            & MNV2     & 0.29        & 0.15        & 0.07  & 0.03 & 0.09 & 0.26 & 0.18 & 0.03 & ~1.67      \\\\\n            & MNV3     & 0.29        & 0.14        & 0.07  & 0.04 & 0.09 & 0.26 & 0.22 & 0.04 & ~6.11    \\\\\n            \\bottomrule\n        \\end{tabular}", "subtables": []}, {"original": "\\begin{tablenotes}\n            \\item \\hspace{-1.2em}$^{*}$ {``RN50'' (``RN101'') denotes ResNet50 (ResNet101) and ``MNV2'' (MNV3) denotes MobileNetV2 (MobileNetV3). }\n            % Here we utilize the final output image features of the corresponding model as intermediate features. The experimental results for the middle layer of different models are shown in Table \\ref{tab:res_model_middle_layers}.}\n            \\item \\hspace{-1.2em}$^{\\ddagger}$ {We calculate the proportion of cosine similarities that are greater than a predefined threshold, which is empirically set to 0.7 in our paper.}\n        \\end{tablenotes}", "caption": "", "label": null, "tabular": "", "subtables": []}, {"original": "\\begin{table}[t]\n    \\centering\n    \\caption{Comparison of experimental results on ResNet50 and $\\text{CLIP}_{\\text{ViT16}}$ using different middle layers.}\n    \\label{tab:exp_rn50_mid_layer}\n    \\vspace{-1.0em}\n    \\begin{threeparttable}\n        \\sizefive\n        \\setlength{\\tabcolsep}{4pt}\n        \\renewcommand{\\arraystretch}{1.05}\n        \\begin{tabular}{ccccc}\n            \\toprule\n            Victim model & Middle layer & BLEU-1 & CIDEr & Cosine Similarity (\\%) \\\\\n            \\midrule\n            \\multirow{5}{*}{ResNet50} & layer1       & 0.24        & 0.19        & ~0.00         \\\\\n             & layer2       & 0.51        & 0.31        & 43.76          \\\\\n             & layer3       & 0.58        & 0.55        & 31.42          \\\\\n             & layer4       & 0.62        & 0.68        & 85.64          \\\\\n             & base         & 0.70        & 0.90        & 90.52       \\\\\n             \\hline\n             \\hline\n             \\multirow{2}{*}{$\\text{CLIP}_{\\text{ViT16}}$} & no-proj & 0.69 & 0.90 & 93.04 \\\\\n              & base & 0.72 & 0.99 & 94.76 \\\\\n            \\bottomrule\n        \\end{tabular}\n    \\end{threeparttable}\n    % \\vspace{-1.0em}\n\\end{table}", "caption": "\\caption{Comparison of experimental results on ResNet50 and $\\text{CLIP}_{\\text{ViT16}}$ using different middle layers.}", "label": "\\label{tab:exp_rn50_mid_layer}", "tabular": "\\begin{tabular}{ccccc}\n            \\toprule\n            Victim model & Middle layer & BLEU-1 & CIDEr & Cosine Similarity (\\%) \\\\\n            \\midrule\n            \\multirow{5}{*}{ResNet50} & layer1       & 0.24        & 0.19        & ~0.00         \\\\\n             & layer2       & 0.51        & 0.31        & 43.76          \\\\\n             & layer3       & 0.58        & 0.55        & 31.42          \\\\\n             & layer4       & 0.62        & 0.68        & 85.64          \\\\\n             & base         & 0.70        & 0.90        & 90.52       \\\\\n             \\hline\n             \\hline\n             \\multirow{2}{*}{$\\text{CLIP}_{\\text{ViT16}}$} & no-proj & 0.69 & 0.90 & 93.04 \\\\\n              & base & 0.72 & 0.99 & 94.76 \\\\\n            \\bottomrule\n        \\end{tabular}", "subtables": []}, {"original": "\\begin{threeparttable}\n        \\sizefive\n        \\setlength{\\tabcolsep}{4pt}\n        \\renewcommand{\\arraystretch}{1.05}\n        \\begin{tabular}{ccccc}\n            \\toprule\n            Victim model & Middle layer & BLEU-1 & CIDEr & Cosine Similarity (\\%) \\\\\n            \\midrule\n            \\multirow{5}{*}{ResNet50} & layer1       & 0.24        & 0.19        & ~0.00         \\\\\n             & layer2       & 0.51        & 0.31        & 43.76          \\\\\n             & layer3       & 0.58        & 0.55        & 31.42          \\\\\n             & layer4       & 0.62        & 0.68        & 85.64          \\\\\n             & base         & 0.70        & 0.90        & 90.52       \\\\\n             \\hline\n             \\hline\n             \\multirow{2}{*}{$\\text{CLIP}_{\\text{ViT16}}$} & no-proj & 0.69 & 0.90 & 93.04 \\\\\n              & base & 0.72 & 0.99 & 94.76 \\\\\n            \\bottomrule\n        \\end{tabular}\n    \\end{threeparttable}", "caption": "", "label": null, "tabular": "\\begin{tabular}{ccccc}\n            \\toprule\n            Victim model & Middle layer & BLEU-1 & CIDEr & Cosine Similarity (\\%) \\\\\n            \\midrule\n            \\multirow{5}{*}{ResNet50} & layer1       & 0.24        & 0.19        & ~0.00         \\\\\n             & layer2       & 0.51        & 0.31        & 43.76          \\\\\n             & layer3       & 0.58        & 0.55        & 31.42          \\\\\n             & layer4       & 0.62        & 0.68        & 85.64          \\\\\n             & base         & 0.70        & 0.90        & 90.52       \\\\\n             \\hline\n             \\hline\n             \\multirow{2}{*}{$\\text{CLIP}_{\\text{ViT16}}$} & no-proj & 0.69 & 0.90 & 93.04 \\\\\n              & base & 0.72 & 0.99 & 94.76 \\\\\n            \\bottomrule\n        \\end{tabular}", "subtables": []}, {"original": "\\begin{table}[t]\n    \\centering\n    \\caption{Experimental results of image label recovery on CIFAR-10 and TinyImageNet datasets.}\n    \\label{tab:exp_overall_res_on_dsts}\n    \\vspace{-1.0em}\n    \\begin{threeparttable}\n        \\sizefive\n        \\setlength{\\tabcolsep}{4pt}\n        \\renewcommand{\\arraystretch}{1.10}\n        \\begin{tabular}{cccc}\n            \\toprule\n            Victim model & datasets & Top-1 Accuracy (\\%) & Top-5 Accuracy (\\%) \\\\\n            \\midrule\n            \\multirow{2}{*}{ResNet50} & CIFAR-10       & 83.35   & 99.55    \\\\\n             & TinyImageNet       & 60.13       & 83.79  \\\\\n             \\hline\n             \\multirow{2}{*}{$\\text{CLIP}_{\\text{ViT32}}$} & CIFAR-10  & 92.71   & 99.82 \\\\\n              & TinyImageNet & 72.62 & 91.60  \\\\\n            \\bottomrule\n        \\end{tabular}\n    \\end{threeparttable}\n    \\vspace{-1.0em}\n\\end{table}", "caption": "\\caption{Experimental results of image label recovery on CIFAR-10 and TinyImageNet datasets.}", "label": "\\label{tab:exp_overall_res_on_dsts}", "tabular": "\\begin{tabular}{cccc}\n            \\toprule\n            Victim model & datasets & Top-1 Accuracy (\\%) & Top-5 Accuracy (\\%) \\\\\n            \\midrule\n            \\multirow{2}{*}{ResNet50} & CIFAR-10       & 83.35   & 99.55    \\\\\n             & TinyImageNet       & 60.13       & 83.79  \\\\\n             \\hline\n             \\multirow{2}{*}{$\\text{CLIP}_{\\text{ViT32}}$} & CIFAR-10  & 92.71   & 99.82 \\\\\n              & TinyImageNet & 72.62 & 91.60  \\\\\n            \\bottomrule\n        \\end{tabular}", "subtables": []}, {"original": "\\begin{threeparttable}\n        \\sizefive\n        \\setlength{\\tabcolsep}{4pt}\n        \\renewcommand{\\arraystretch}{1.10}\n        \\begin{tabular}{cccc}\n            \\toprule\n            Victim model & datasets & Top-1 Accuracy (\\%) & Top-5 Accuracy (\\%) \\\\\n            \\midrule\n            \\multirow{2}{*}{ResNet50} & CIFAR-10       & 83.35   & 99.55    \\\\\n             & TinyImageNet       & 60.13       & 83.79  \\\\\n             \\hline\n             \\multirow{2}{*}{$\\text{CLIP}_{\\text{ViT32}}$} & CIFAR-10  & 92.71   & 99.82 \\\\\n              & TinyImageNet & 72.62 & 91.60  \\\\\n            \\bottomrule\n        \\end{tabular}\n    \\end{threeparttable}", "caption": "", "label": null, "tabular": "\\begin{tabular}{cccc}\n            \\toprule\n            Victim model & datasets & Top-1 Accuracy (\\%) & Top-5 Accuracy (\\%) \\\\\n            \\midrule\n            \\multirow{2}{*}{ResNet50} & CIFAR-10       & 83.35   & 99.55    \\\\\n             & TinyImageNet       & 60.13       & 83.79  \\\\\n             \\hline\n             \\multirow{2}{*}{$\\text{CLIP}_{\\text{ViT32}}$} & CIFAR-10  & 92.71   & 99.82 \\\\\n              & TinyImageNet & 72.62 & 91.60  \\\\\n            \\bottomrule\n        \\end{tabular}", "subtables": []}, {"original": "\\begin{table}[t]\n    \\centering\n    \\caption{Results of image label recovery on CIFAR-10.}\n    \\label{tab:exp_img_classification}\n    \\vspace{-1.0em}\n    \\begin{threeparttable}\n        \\sizefive\n        \\setlength{\\tabcolsep}{4pt}\n        \\renewcommand{\\arraystretch}{1.05}\n        \\begin{tabular}{cccc}\n            \\toprule\n            Class & Precision & Recall & F1-Score  \\\\\n            \\midrule\n            Airplane    & 0.93   & 0.96  & 0.95 \\\\\n            Automobile  & 0.96   & 0.97  & 0.97 \\\\\n            Bird        & 0.91   & 0.90  & 0.91 \\\\\n            Cat         & 0.85   & 0.85  & 0.85 \\\\\n            Deer        & 0.91   & 0.93  & 0.92 \\\\\n            Dog         & 0.88   & 0.87  & 0.87 \\\\\n            Frog        & 0.92   & 0.94  & 0.93 \\\\\n            Horse       & 0.97   & 0.94  & 0.96 \\\\\n            Ship        & 0.96   & 0.96  & 0.96 \\\\\n            Truck       & 0.97   & 0.96  & 0.96 \\\\\n            \\bottomrule\n        \\end{tabular}\n    \\end{threeparttable}\n    \\vspace{-1.0em}\n\\end{table}", "caption": "\\caption{Results of image label recovery on CIFAR-10.}", "label": "\\label{tab:exp_img_classification}", "tabular": "\\begin{tabular}{cccc}\n            \\toprule\n            Class & Precision & Recall & F1-Score  \\\\\n            \\midrule\n            Airplane    & 0.93   & 0.96  & 0.95 \\\\\n            Automobile  & 0.96   & 0.97  & 0.97 \\\\\n            Bird        & 0.91   & 0.90  & 0.91 \\\\\n            Cat         & 0.85   & 0.85  & 0.85 \\\\\n            Deer        & 0.91   & 0.93  & 0.92 \\\\\n            Dog         & 0.88   & 0.87  & 0.87 \\\\\n            Frog        & 0.92   & 0.94  & 0.93 \\\\\n            Horse       & 0.97   & 0.94  & 0.96 \\\\\n            Ship        & 0.96   & 0.96  & 0.96 \\\\\n            Truck       & 0.97   & 0.96  & 0.96 \\\\\n            \\bottomrule\n        \\end{tabular}", "subtables": []}, {"original": "\\begin{threeparttable}\n        \\sizefive\n        \\setlength{\\tabcolsep}{4pt}\n        \\renewcommand{\\arraystretch}{1.05}\n        \\begin{tabular}{cccc}\n            \\toprule\n            Class & Precision & Recall & F1-Score  \\\\\n            \\midrule\n            Airplane    & 0.93   & 0.96  & 0.95 \\\\\n            Automobile  & 0.96   & 0.97  & 0.97 \\\\\n            Bird        & 0.91   & 0.90  & 0.91 \\\\\n            Cat         & 0.85   & 0.85  & 0.85 \\\\\n            Deer        & 0.91   & 0.93  & 0.92 \\\\\n            Dog         & 0.88   & 0.87  & 0.87 \\\\\n            Frog        & 0.92   & 0.94  & 0.93 \\\\\n            Horse       & 0.97   & 0.94  & 0.96 \\\\\n            Ship        & 0.96   & 0.96  & 0.96 \\\\\n            Truck       & 0.97   & 0.96  & 0.96 \\\\\n            \\bottomrule\n        \\end{tabular}\n    \\end{threeparttable}", "caption": "", "label": null, "tabular": "\\begin{tabular}{cccc}\n            \\toprule\n            Class & Precision & Recall & F1-Score  \\\\\n            \\midrule\n            Airplane    & 0.93   & 0.96  & 0.95 \\\\\n            Automobile  & 0.96   & 0.97  & 0.97 \\\\\n            Bird        & 0.91   & 0.90  & 0.91 \\\\\n            Cat         & 0.85   & 0.85  & 0.85 \\\\\n            Deer        & 0.91   & 0.93  & 0.92 \\\\\n            Dog         & 0.88   & 0.87  & 0.87 \\\\\n            Frog        & 0.92   & 0.94  & 0.93 \\\\\n            Horse       & 0.97   & 0.94  & 0.96 \\\\\n            Ship        & 0.96   & 0.96  & 0.96 \\\\\n            Truck       & 0.97   & 0.96  & 0.96 \\\\\n            \\bottomrule\n        \\end{tabular}", "subtables": []}, {"original": "\\begin{table}[t]\n    \\centering\n    \\caption{Evaluation results of \\textsc{CapRecover} attacking ResNet50 with/without additional noise.}\n    \\label{tab:exp_add_noise}\n    \\vspace{-1.0em}\n    \\begin{threeparttable}\n        \\sizefive\n        \\setlength{\\tabcolsep}{7pt}\n        \\renewcommand{\\arraystretch}{1.2}\n        \\begin{tabular}{cccccc}\n            \\toprule\n            Dataset & Middle layer & layer1 & layer2 & layer3 & layer4 \\\\\n            \\midrule\n            \\multirow{2}{*}{COCO2017} & w/o noise & 0.24 & 0.51       & 0.58        & 0.62         \\\\\n            & w/ noise & 0.49 & 0.03       & 0.02        & 0.05    \\\\\n            \\bottomrule\n        \\end{tabular}\n    \\end{threeparttable}\n    \\vspace{-1.0em}\n\\end{table}", "caption": "\\caption{Evaluation results of \\textsc{CapRecover} attacking ResNet50 with/without additional noise.}", "label": "\\label{tab:exp_add_noise}", "tabular": "\\begin{tabular}{cccccc}\n            \\toprule\n            Dataset & Middle layer & layer1 & layer2 & layer3 & layer4 \\\\\n            \\midrule\n            \\multirow{2}{*}{COCO2017} & w/o noise & 0.24 & 0.51       & 0.58        & 0.62         \\\\\n            & w/ noise & 0.49 & 0.03       & 0.02        & 0.05    \\\\\n            \\bottomrule\n        \\end{tabular}", "subtables": []}, {"original": "\\begin{threeparttable}\n        \\sizefive\n        \\setlength{\\tabcolsep}{7pt}\n        \\renewcommand{\\arraystretch}{1.2}\n        \\begin{tabular}{cccccc}\n            \\toprule\n            Dataset & Middle layer & layer1 & layer2 & layer3 & layer4 \\\\\n            \\midrule\n            \\multirow{2}{*}{COCO2017} & w/o noise & 0.24 & 0.51       & 0.58        & 0.62         \\\\\n            & w/ noise & 0.49 & 0.03       & 0.02        & 0.05    \\\\\n            \\bottomrule\n        \\end{tabular}\n    \\end{threeparttable}", "caption": "", "label": null, "tabular": "\\begin{tabular}{cccccc}\n            \\toprule\n            Dataset & Middle layer & layer1 & layer2 & layer3 & layer4 \\\\\n            \\midrule\n            \\multirow{2}{*}{COCO2017} & w/o noise & 0.24 & 0.51       & 0.58        & 0.62         \\\\\n            & w/ noise & 0.49 & 0.03       & 0.02        & 0.05    \\\\\n            \\bottomrule\n        \\end{tabular}", "subtables": []}], "figure": [{"original": "\\begin{figure*}[thbp]\n    \\centering\n    \\includegraphics[width = 0.88\\textwidth]{figs/illustration_v4.pdf}\n    \\vspace{-0.9em}\n    \\caption{Illustration of the cross-modality feature inversion attack scenario. In the depicted attack scenario, the adversary steals the intermediate image features from the visual model. Leveraging these stolen features, the adversary employs \\textsc{CapRecover} to reconstruct the image caption/label, potentially revealing sensitive or private information.}\n    \\label{fig:illustration}\n    \\vspace{-0.5em}\n\\end{figure*}", "caption": "\\caption{Illustration of the cross-modality feature inversion attack scenario. In the depicted attack scenario, the adversary steals the intermediate image features from the visual model. Leveraging these stolen features, the adversary employs \\textsc{CapRecover} to reconstruct the image caption/label, potentially revealing sensitive or private information.}", "label": "\\label{fig:illustration}", "subfigures": [], "figure_paths": ["figs/illustration_v4.pdf"]}, {"original": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width = 0.93\\linewidth]{figs/model_overview_v5.pdf}\n    \\vspace{-0.8em}\n    \\caption{Overview of \\textsc{CapRecover}. \\textsc{CapRecover} mainly consists of: (1) Feature projection module, (2) Feature-text alignment module, and (3) Caption generation module. We freeze the language model and optimize other modules.}\n    \\label{fig:model_overview}\n    \\vspace{-0.8em}\n\\end{figure}", "caption": "\\caption{Overview of \\textsc{CapRecover}. \\textsc{CapRecover} mainly consists of: (1) Feature projection module, (2) Feature-text alignment module, and (3) Caption generation module. We freeze the language model and optimize other modules.}", "label": "\\label{fig:model_overview}", "subfigures": [], "figure_paths": ["figs/model_overview_v5.pdf"]}, {"original": "\\begin{figure*}[t]\n    \\hspace{-1.5em}\n    \\begin{tabular}{cc}\n        \n        \\subfigure[{Evaluation on COCO2017 dataset.}]{\n            \\begin{minipage}[t]{0.31\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_coco2017}\n                \\includegraphics[width=0.89\\textwidth]{./figs/plot_cosine_similarity_pdf_distribution_blip2-opt-2.7b_COCO2017_all_victim_models.pdf}\n        \\end{minipage}}\n\n        \\subfigure[{Evaluation on Flickr8K dataset.}]{\n            \\begin{minipage}[t]{0.31\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_flickr8k}\n                \\includegraphics[width=0.89\\textwidth]{./figs/plot_cosine_similarity_pdf_distribution_blip2-opt-2.7b_flickr8k_all_victim_models.pdf}\n        \\end{minipage}}\n\n        \\subfigure[{Evaluation on ImageNet-1K dataset.}]{\n            \\begin{minipage}[t]{0.31\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_imagenet1k}\n                \\includegraphics[width=0.89\\textwidth]{./figs/plot_cosine_similarity_pdf_distribution_blip2-opt-2.7b_imagenet_all_victim_models.pdf}\n        \\end{minipage}\n        }\n    \\end{tabular}\n    \\vspace{-1.3em}\n\n    \\caption{\n        Distribution of cosine similarities across three datasets. We use intermediate features extracted from the final layer of the victim model to train \\textsc{CapRecover}. We analyze how other intermediate layers' features impact performance in Sec.~\\ref{subsec:further_study_analysis}.\n    }\n\t\\label{fig:cosine_similarity_on_three_dst}\n    \\vspace{-1.0em}\n\\end{figure*}", "caption": "\\caption{\n        Distribution of cosine similarities across three datasets. We use intermediate features extracted from the final layer of the victim model to train \\textsc{CapRecover}. We analyze how other intermediate layers' features impact performance in Sec.~\\ref{subsec:further_study_analysis}.\n    }", "label": "\\label{fig:cosine_similarity_on_three_dst}", "subfigures": [], "figure_paths": ["./figs/plot_cosine_similarity_pdf_distribution_blip2-opt-2.7b_COCO2017_all_victim_models.pdf", "./figs/plot_cosine_similarity_pdf_distribution_blip2-opt-2.7b_flickr8k_all_victim_models.pdf", "./figs/plot_cosine_similarity_pdf_distribution_blip2-opt-2.7b_imagenet_all_victim_models.pdf"]}, {"original": "\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width = 0.91\\linewidth]{figs/rn50_heatmap_v4.pdf}\n    \\vspace{-0.8em}\n    \\caption{Example of visualizing the heatmaps of ResNet50's different middle layers. Below each figure is the generated/ground truth caption. These figures demonstrate that the shallow layer (e.g., RN50-Layer1) pays more attention to edges and local features, while the deeper the layer (e.g., RN50-Layer4), the more attention is paid to the more semantic areas in the image.}\n    \\label{fig:rn50_heatmap}\n    \\vspace{-1.0em}\n\\end{figure*}", "caption": "\\caption{Example of visualizing the heatmaps of ResNet50's different middle layers. Below each figure is the generated/ground truth caption. These figures demonstrate that the shallow layer (e.g., RN50-Layer1) pays more attention to edges and local features, while the deeper the layer (e.g., RN50-Layer4), the more attention is paid to the more semantic areas in the image.}", "label": "\\label{fig:rn50_heatmap}", "subfigures": [], "figure_paths": ["figs/rn50_heatmap_v4.pdf"]}, {"original": "\\begin{figure*}[t]\n    \\hspace{-1.3em}\n    \\begin{tabular}{ccccc}\n    \n        \\subfigure[{ResNet50-layer1.}]{\n            \\begin{minipage}[t]{0.19\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_rn50_layer1}\n                \\includegraphics[width=0.97\\textwidth]{./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-layer1.pdf}\n        \\end{minipage}}\n\n        \\subfigure[{ResNet50-layer2.}]{\n            \\begin{minipage}[t]{0.19\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_rn50_layer2}\n                \\includegraphics[width=0.97\\textwidth]{./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-layer2.pdf}\n        \\end{minipage}}\n\n        \\subfigure[{ResNet50-layer3.}]{\n            \\begin{minipage}[t]{0.19\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_rn50_layer3}\n                \\includegraphics[width=0.97\\textwidth]{./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-layer3.pdf}\n        \\end{minipage}}\n\n        \\subfigure[{ResNet50-layer4.}]{\n            \\begin{minipage}[t]{0.19\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_rn50_layer4}\n                \\includegraphics[width=0.97\\textwidth]{./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-layer4.pdf}\n        \\end{minipage}}\n\n        \\subfigure[{ResNet50-base.}]{\n            \\begin{minipage}[t]{0.19\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_rn50_base}\n                \\includegraphics[width=0.97\\textwidth]{./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-base.pdf}\n        \\end{minipage}}\n        \n    \\end{tabular}\n    \\vspace{-1.1em}\n\n    \\caption{\n        Evaluate the distributions of cosine similarity on the COCO2017 dataset. We train \\textsc{CapRecover} using the intermediate image features produced by their final linear projection layers. We further discuss how other middle layers' intermediate features affect \\textsc{CapRecover}'s performance in Sec. \\ref{subsec:further_study_analysis}.\n    }\n\t\\label{fig:bleu1_on_rn50_coco}\n    \\vspace{-0.8em}\n\\end{figure*}", "caption": "\\caption{\n        Evaluate the distributions of cosine similarity on the COCO2017 dataset. We train \\textsc{CapRecover} using the intermediate image features produced by their final linear projection layers. We further discuss how other middle layers' intermediate features affect \\textsc{CapRecover}'s performance in Sec. \\ref{subsec:further_study_analysis}.\n    }", "label": "\\label{fig:bleu1_on_rn50_coco}", "subfigures": [], "figure_paths": ["./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-layer1.pdf", "./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-layer2.pdf", "./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-layer3.pdf", "./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-layer4.pdf", "./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-base.pdf"]}, {"original": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width = 0.85\\linewidth]{figs/plot_cosine_similarity_distribution_blip2-opt-2.7b_RN50_mean_all_leaked_feature_layers.pdf}\n    \\vspace{-0.8em}\n    \\caption{Embedding Cosine Similarity distributions for different middle layers of ResNet50. The results indicate that \\textsc{CapRecover} employs the deep layers and may perform better compared to the shallow layers.}\n    \\label{fig:exp_resnet50_cosine_similarity_middle_layers}\n    \\vspace{-0.5em}\n\\end{figure}", "caption": "\\caption{Embedding Cosine Similarity distributions for different middle layers of ResNet50. The results indicate that \\textsc{CapRecover} employs the deep layers and may perform better compared to the shallow layers.}", "label": "\\label{fig:exp_resnet50_cosine_similarity_middle_layers}", "subfigures": [], "figure_paths": ["figs/plot_cosine_similarity_distribution_blip2-opt-2.7b_RN50_mean_all_leaked_feature_layers.pdf"]}, {"original": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width = 0.90\\linewidth]{figs/confusion_matrix_cifar10_vit32.png}\n    \\vspace{-0.8em}\n    \\caption{Confusion matrix of prediction results on CIFAR-10 test set. This matrix illustrates that \\textsc{CapRecover} can accurately reconstruct the image classes.}\n    \\label{fig:exp_img_classification_confusion_matrix}\n    \\vspace{-0.8em}\n\\end{figure}", "caption": "\\caption{Confusion matrix of prediction results on CIFAR-10 test set. This matrix illustrates that \\textsc{CapRecover} can accurately reconstruct the image classes.}", "label": "\\label{fig:exp_img_classification_confusion_matrix}", "subfigures": [], "figure_paths": ["figs/confusion_matrix_cifar10_vit32.png"]}], "equations": ["\\begin{equation}\n    \\arg\\min\\limits_{\\theta} \\; \\mathcal{L}(T'_{\\text{cap}}, T_{\\text{cap}}), \\label{eq:caption_obj}\n\\end{equation}", "\\begin{equation}\n    \\arg\\min\\limits_{\\theta} \\; \\mathcal{L}_{\\text{cls}}(y'_{\\text{cls}}, y_{\\text{cls}}). \\label{eq:label_obj}\n\\end{equation}", "\\begin{equation}\n\\mathbf{F}_i^{proj} = \\mathbf{W}_p \\mathbf{F}_i + \\mathbf{b}_p, \\quad \\mathbf{W}_p \\in \\mathbb{R}^{d' \\times d}, \\quad \\mathbf{b}_p \\in \\mathbb{R}^{d'}\n\\end{equation}", "\\begin{equation}\n\\mathbf{F}_i^{proj} = \\mathbf{W}_p \\cdot g(\\mathbf{F}_i) +\\mathbf{b}_p, \\quad g: \\mathbb{R}^{C \\times H \\times W} \\to \\mathbb{R}^{d'}\n\\end{equation}", "\\begin{equation}\n\\mathbf{Z}_i = \\text{Q-Former}(\\mathbf{Q}, \\mathbf{F}_i^{proj}, \\mathbf{T}_i),\n\\end{equation}", "\\begin{equation}\n\\mathbf{E}_i = \\mathbf{W}_l \\mathbf{Z}_i, \\quad \\mathbf{W}_l \\in \\mathbb{R}^{d_{LM} \\times d''}\n\\end{equation}", "\\begin{equation}\nP(C_i \\mid \\mathbf{E}_i, \\mathbf{T}_i) = \\prod_{t=1}^{T} P(c_{i,t} \\mid c_{i,<t}, \\mathbf{E}_i, \\mathbf{T}_i),\n\\end{equation}", "\\begin{equation}\n\\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^T \\log P(c_{i,t}^{*} \\mid c_{i,<t}^{*}, \\mathbf{E}_i, \\mathbf{T}_i)\n\\end{equation}"], "algorithm": [], "sections": {"Introduction": {"content": "\n\\label{sec:introduction}\n\n\\par\\noindent The rapid advancement of Vision-Language Models (VLMs) has fundamentally reshaped the landscape of multimodal AI, positioning these models as the cornerstone of modern user-facing assistants. Unlike traditional Large Language Models (LLMs), VLMs seamlessly integrate image understanding with natural language processing, enabling comprehensive interpretations of real-world data. By harnessing vast amounts of textual and visual information, VLMs have achieved impressive results in tasks such as image captioning (e.g., GPT-4o \\cite{openai_gpt4o}), text-to-image generation (e.g., Stable Diffusion \\cite{stable_diffusion}), and optical character recognition. The success of architectures like CLIP \\cite{clip} and BLIP2 \\cite{blip2} underscores their potential to drive significant innovations in both research and practical applications.\n\n\n\n\n\n\nDespite these advances, VLMs are not without vulnerabilities. Recent research has predominantly focused on security threats such as prompt jailbreaking\u2014where attackers manipulate models to produce harmful outputs\u2014and prompt-stealing attacks that extract sensitive user prompts from generated images \\cite{gong2023figstep,luo2024jailbreakv28k,shayegani2023plug,2024_usenix_prompt_stealing_attack}. However, one critical dimension remains underexplored: the leakage of sensitive information through intermediate feature representations. As illustrated in Figure \\ref{fig:illustration}, an adversary who gains access to intermediate image features from the victim model (for instance, from a local device) could reconstruct the original image caption, potentially exposing private user data. This issue is especially relevant in the split DNN computing paradigm~\\cite{mudvari2024splitllm,zhang2024edgeshard,he2024large,lu2024merge}, where a large model is divided into multiple blocks tailored to the computational capabilities of edge devices. In this setup, user data is initially processed on the edge device using the first layers of the model, and intermediate results are transmitted to a remote server for processing by later layers. This data transfer poses a risk, as it can be intercepted and exploited by attackers to reconstruct user inputs. Consequently, understanding and mitigating the leakage of intermediate image features is imperative for protecting user data and maintaining the integrity of VLM-driven services.\n\n\n\\begin{figure*}[thbp]\n    \\centering\n    \\includegraphics[width = 0.88\\textwidth]{figs/illustration_v4.pdf}\n    \\vspace{-0.9em}\n    \\caption{Illustration of the cross-modality feature inversion attack scenario. In the depicted attack scenario, the adversary steals the intermediate image features from the visual model. Leveraging these stolen features, the adversary employs \\textsc{CapRecover} to reconstruct the image caption/label, potentially revealing sensitive or private information.}\n    \\label{fig:illustration}\n    \\vspace{-0.5em}\n\\end{figure*}\n    \\centering\n    \\includegraphics[width = 0.88\\textwidth]{figs/illustration_v4.pdf}\n    \\vspace{-0.9em}\n    \\caption{Illustration of the cross-modality feature inversion attack scenario. In the depicted attack scenario, the adversary steals the intermediate image features from the visual model. Leveraging these stolen features, the adversary employs \\textsc{CapRecover} to reconstruct the image caption/label, potentially revealing sensitive or private information.}\n    \\label{fig:illustration}\n    \\vspace{-0.5em}\n\n\n\n\n\nPrior work \\cite{xu2024stealthy, zhu2025passiveinfer, he2019model} has explored reconstructing images from intermediate features to further infer their semantic content. However, these methods are indirect and often suffer from preserving fine-grained semantics with poor visual fidelity (e.g., blurriness, missing textures), which could consequently limit their performance. Moreover, some attackers may primarily focus on the semantic meaning in the target image, e.g., what is happening and who is involved. This raises a critical yet underexplored question: \\emph{Is it possible\u2014and potentially more effective\u2014for an attacker to directly recover high-level semantic information, such as image labels or captions, from intermediate features, without reconstructing the image at all?} This new form of \\textit{feature inversion attack} shifts the adversary's focus from pixel-level recovery to semantic reconstruction, and more directly threatens user privacy in practical scenarios.\n\n\n\n\n\n\\subsection{Our work and contributions}\n\\label{subsec:intro_our_work_contri}\n\n\n\nIn this paper, we take a fundamentally different approach: instead of reconstructing the image, we \\emph{directly recover/reconstruct the image's semantic content} from the leaked intermediate image features. We introduce \\textsc{CapRecover}, a generic cross-modality feature inversion framework that exposes a critical vulnerability in VLMs: the capacity to reconstruct textual descriptions from intermediate image features. \\textsc{CapRecover} bridges intermediate visual features with a pre-trained language model, bypassing image reconstruction entirely. By learning a lightweight projection layer between the vision and language domains, \\textsc{CapRecover} enables accurate and fluent semantic recovery from commonly used encoders such as ResNet and ViT.\n\nTo understand the privacy implications of this attack, we consider a threat model where the attacker passively observes the intermediate visual features sent from a user's device to the cloud in a split-VLM pipeline. The attacker has no access to the original image or the language module of the VLM, and aims to infer semantic content directly from the encoder output. While a full description is provided in Sec. \\ref{sec:threat_model}, we note here that this threat model aligns with realistic deployment scenarios in edge-cloud systems and highlights a previously underestimated attack surface.\n\n\n\n\n\n\n\n\n\n\nWe evaluate \\textsc{CapRecover} on multiple datasets and VLM architectures across two key tasks: image classification and image captioning. Our experiments show that \\textsc{CapRecover} can recover labels and captions with high fidelity\u2014even without reconstructing a single pixel. We further analyze how semantic leakage varies across encoder depths and propose a simple, training-free defense mechanism that reduces leakage via reversible noise injection.\n\n\nWe summarize our contributions as follows:\n\n\\begin{itemize}\n    % \\item \\textbf{A generic adversarial framework.} We introduce \\textsc{CapRecover}, the first generic adversarial framework that reconstructs semantic information (labels or captions) directly from visual encoder features, without reconstructing images. By utilizing a feature-text alignment mechanism, our model effectively recovers the target image's semantic information, even when explicit textual cues are unavailable.\n    % % image captions by exploiting intermediate image features from the victim visual model. By combining a feature projection module with a feature-text alignment mechanism, \\textsc{CapRecover} effectively leverages latent visual representations to generate accurate and coherent captions, even when explicit textual cues are unavailable.\n    \n    % \\item \\textbf{Extensive evaluation and analysis.} We demonstrate the effectiveness of \\textsc{CapRecover} across image classification and image captioning tasks with multiple widely used datasets and victim models. \\textsc{CapRecover} achieves 92.71\\% label recovery accuracy on CIFAR-10 and ROUGE-L scores up to 0.52 for reconstructing image captions on COCO2017 dataset.\n    \n    % % three widely used datasets and multiple victim models. We employ several evaluation metrics to assess its caption reconstruction capabilities. Our experimental results show that \\textsc{CapRecover} successfully recovers key information from the target image\u2019s captions, highlighting its robustness in real-world scenarios.\n\n    \\item \\textbf{A general adversarial framework.} We propose \\textsc{CapRecover}, the first generic cross-modality feature inversion framework that directly reconstructs semantic information from the leaked intermediate image features, without requiring pixel-level image reconstruction. By leveraging a feature-to-text alignment mechanism, \\textsc{CapRecover} effectively recovers image labels and captions even in the absence of explicit textual outputs.\n\n    \\item \\textbf{Extensive evaluation and analysis.} We evaluate \\textsc{CapRecover} on both image classification and image captioning tasks using widely adopted datasets and victim models. \\textsc{CapRecover} achieves up to 92.71\\% Top-1 accuracy on CIFAR-10 for label recovery and a ROUGE-L score of 0.52 on COCO2017 for caption reconstruction.\n\n\n    \\item \\textbf{An effective protection approach.} We propose a straightforward yet effective protection approach: Add random noise to the output of each layer in the victim model and remove this noise in the subsequent layer. Our approach only needs a small noise cost without any additional training cost, which can effectively mitigate the risks of sensitive information leakage from the intermediate image features.\n\n\\end{itemize}\\begin{itemize}\n    % \\item \\textbf{A generic adversarial framework.} We introduce \\textsc{CapRecover}, the first generic adversarial framework that reconstructs semantic information (labels or captions) directly from visual encoder features, without reconstructing images. By utilizing a feature-text alignment mechanism, our model effectively recovers the target image's semantic information, even when explicit textual cues are unavailable.\n    % % image captions by exploiting intermediate image features from the victim visual model. By combining a feature projection module with a feature-text alignment mechanism, \\textsc{CapRecover} effectively leverages latent visual representations to generate accurate and coherent captions, even when explicit textual cues are unavailable.\n    \n    % \\item \\textbf{Extensive evaluation and analysis.} We demonstrate the effectiveness of \\textsc{CapRecover} across image classification and image captioning tasks with multiple widely used datasets and victim models. \\textsc{CapRecover} achieves 92.71\\% label recovery accuracy on CIFAR-10 and ROUGE-L scores up to 0.52 for reconstructing image captions on COCO2017 dataset.\n    \n    % % three widely used datasets and multiple victim models. We employ several evaluation metrics to assess its caption reconstruction capabilities. Our experimental results show that \\textsc{CapRecover} successfully recovers key information from the target image\u2019s captions, highlighting its robustness in real-world scenarios.\n\n    \\item \\textbf{A general adversarial framework.} We propose \\textsc{CapRecover}, the first generic cross-modality feature inversion framework that directly reconstructs semantic information from the leaked intermediate image features, without requiring pixel-level image reconstruction. By leveraging a feature-to-text alignment mechanism, \\textsc{CapRecover} effectively recovers image labels and captions even in the absence of explicit textual outputs.\n\n    \\item \\textbf{Extensive evaluation and analysis.} We evaluate \\textsc{CapRecover} on both image classification and image captioning tasks using widely adopted datasets and victim models. \\textsc{CapRecover} achieves up to 92.71\\% Top-1 accuracy on CIFAR-10 for label recovery and a ROUGE-L score of 0.52 on COCO2017 for caption reconstruction.\n\n\n    \\item \\textbf{An effective protection approach.} We propose a straightforward yet effective protection approach: Add random noise to the output of each layer in the victim model and remove this noise in the subsequent layer. Our approach only needs a small noise cost without any additional training cost, which can effectively mitigate the risks of sensitive information leakage from the intermediate image features.\n\n\\end{itemize}\n    \n\n    \\item \\textbf{A general adversarial framework.} We propose \\textsc{CapRecover}, the first generic cross-modality feature inversion framework that directly reconstructs semantic information from the leaked intermediate image features, without requiring pixel-level image reconstruction. By leveraging a feature-to-text alignment mechanism, \\textsc{CapRecover} effectively recovers image labels and captions even in the absence of explicit textual outputs.\n\n    \\item \\textbf{Extensive evaluation and analysis.} We evaluate \\textsc{CapRecover} on both image classification and image captioning tasks using widely adopted datasets and victim models. \\textsc{CapRecover} achieves up to 92.71\\% Top-1 accuracy on CIFAR-10 for label recovery and a ROUGE-L score of 0.52 on COCO2017 for caption reconstruction.\n\n\n    \\item \\textbf{An effective protection approach.} We propose a straightforward yet effective protection approach: Add random noise to the output of each layer in the victim model and remove this noise in the subsequent layer. Our approach only needs a small noise cost without any additional training cost, which can effectively mitigate the risks of sensitive information leakage from the intermediate image features.\n\n\n\n\n\n\n\n\n\n\n\n\n", "appendix": false}, "Threat Model": {"content": "\n\\label{sec:threat_model}\n\nWe consider a cross-modality feature inversion attack scenario where an adversary aims to reconstruct/recover the semantic description/label corresponding to a given image by exploiting the \\textit{intermediate image features} $\\mathbf{F}$\\mathbf{F} produced by the victim visual encoder $\\mathcal{V}_{image}$\\mathcal{V}_{image}image. We assume a reasonable deployment situation consistent with practical deployment \\cite{he2024large, lu2024merge} where VLMs are deployed in user-facing applications or on edge devices, which commonly keep raw images and final captions locally private, yet may expose intermediate features (e.g., when features are transmitted to a cloud service or temporarily stored in device memory).\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width = 0.93\\linewidth]{figs/model_overview_v5.pdf}\n    \\vspace{-0.8em}\n    \\caption{Overview of \\textsc{CapRecover}. \\textsc{CapRecover} mainly consists of: (1) Feature projection module, (2) Feature-text alignment module, and (3) Caption generation module. We freeze the language model and optimize other modules.}\n    \\label{fig:model_overview}\n    \\vspace{-0.8em}\n\\end{figure}\n    \\centering\n    \\includegraphics[width = 0.93\\linewidth]{figs/model_overview_v5.pdf}\n    \\vspace{-0.8em}\n    \\caption{Overview of \\textsc{CapRecover}. \\textsc{CapRecover} mainly consists of: (1) Feature projection module, (2) Feature-text alignment module, and (3) Caption generation module. We freeze the language model and optimize other modules.}\n    \\label{fig:model_overview}\n    \\vspace{-0.8em}\n\n\n\n\\begin{table}[t]\n    \\centering\n    \\caption{Datasets Used in this paper.}\n    \\label{tab:dsts_used}\n    \\vspace{-1.0em}\n    \\begin{threeparttable}\n    \n    % \\fontsize{6.5}{8}\\selectfont % \u8c03\u6574\u5b57\u4f53\u5927\u5c0f\n    \\sizefive\n    \\setlength{\\tabcolsep}{4pt} % \u8c03\u6574\u5217\u95f4\u8ddd\n    \\renewcommand{\\arraystretch}{1.1} % \u8c03\u6574\u884c\u95f4\u8ddd\n    \\begin{tabular}{crrrr}\n    \\toprule\n    \\textbf{Dataset}{$^\\dagger$} & \\makecell[c]{\\textbf{Training size}} & \\makecell[c]{\\textbf{Sample size}} & \\makecell[c]{\\textbf{Test size}} & \\makecell[c]{\\textbf{Sample size}} \\\\ \n    \\midrule\n    COCO2017 & 118,287 & 30,000 & 5,000 & 5,000 \\\\\n    Flickr8K & 6,000 & 6,000 & 1,000 & 1,000 \\\\\n    ImageNet-1K & 1,281,167 & 12,000 & 50,000 & 900 \\\\\n    \\hline\n    CIFAR-10 & 50,000 & 50,000 & 10,000 & 10,000 \\\\\n    TinyImageNet & 100,000 & 100,000 & 10,000 & 10,000 \\\\\n    \\bottomrule\n    \\end{tabular}\n    \\begin{tablenotes}\n        \\item \\hspace{-1.2em}$^{\\dagger}$ {We use CIFAR-10 and TinyImageNet datasets for image label recovery and COCO2017, Flickr8K and ImageNet-1K datasets for image caption reconstruction, respectively.}\n    \\end{tablenotes}\n    \\end{threeparttable}\n    \\vspace{-0.9em}\n\\end{table}\n    \\centering\n    \\caption{Datasets Used in this paper.}\n    \\label{tab:dsts_used}\n    \\vspace{-1.0em}\n    \n    \n    \\sizefive\n    \\setlength{\\tabcolsep}{4pt} \\renewcommand{\\arraystretch}{1.1} \n    \\toprule\n    \\textbf{Dataset}{$^\\dagger$}$^\\dagger$^\\dagger & \\makecell[c]{\\textbf{Training size}}\\textbf{Training size} & \\makecell[c]{\\textbf{Sample size}}\\textbf{Sample size} & \\makecell[c]{\\textbf{Test size}}\\textbf{Test size} & \\makecell[c]{\\textbf{Sample size}}\\textbf{Sample size} \\\\ \n    \\midrule\n    COCO2017 & 118,287 & 30,000 & 5,000 & 5,000 \\\\\n    Flickr8K & 6,000 & 6,000 & 1,000 & 1,000 \\\\\n    ImageNet-1K & 1,281,167 & 12,000 & 50,000 & 900 \\\\\n    \\hline\n    CIFAR-10 & 50,000 & 50,000 & 10,000 & 10,000 \\\\\n    TinyImageNet & 100,000 & 100,000 & 10,000 & 10,000 \\\\\n    \\bottomrule\n    \n    \n        \\item \\hspace{-1.2em}$^{\\dagger}$^{\\dagger}\\dagger {We use CIFAR-10 and TinyImageNet datasets for image label recovery and COCO2017, Flickr8K and ImageNet-1K datasets for image caption reconstruction, respectively.}We use CIFAR-10 and TinyImageNet datasets for image label recovery and COCO2017, Flickr8K and ImageNet-1K datasets for image caption reconstruction, respectively.\n    \n    \n    \\vspace{-0.9em}\n\n\n\n\\subsection{Adversary's Capabilities and Access.} \n\\label{subsec:adv_cap}\n\n\n\nWe assume the adversary can intercept or obtain the victim model\u2019s intermediate visual representations $\\mathbf{F}$\\mathbf{F} but \\emph{can not} directly access to the original input image $I$I or its corresponding semantic description (e.g., the ground truth caption $T_{cap}$T_{cap}cap and image label $y_{cls}$y_{cls}cls). This can occur in practical scenarios where:\n\\begin{itemize}[leftmargin=1.0em]\n    \\item The adversary intercepts intermediate features transmitted from an edge device to a cloud server responsible for caption or label generation;\n    \\item A malicious insider or malware on the user\u2019s device extracts intermediate features from memory.\n\\end{itemize}\\begin{itemize}[leftmargin=1.0em]\n    \\item The adversary intercepts intermediate features transmitted from an edge device to a cloud server responsible for caption or label generation;\n    \\item A malicious insider or malware on the user\u2019s device extracts intermediate features from memory.\n\\end{itemize}\n    \\item The adversary intercepts intermediate features transmitted from an edge device to a cloud server responsible for caption or label generation;\n    \\item A malicious insider or malware on the user\u2019s device extracts intermediate features from memory.\n\nWe assume that the attacker knows the architecture of the victim\u2019s visual encoder (e.g., ResNet50, ViT), including the position of intermediate layers used for downstream tasks. The attacker may also leverage auxiliary resources such as publicly available pretrained models to assist in decoding the extracted features. This setup aligns with a standard \\textit{white-box} or \\textit{gray-box} threat model.\n\n\n\n\n\\subsection{Adversary's Objective.} \n\\label{subsec:adv_obj}\n\n\n\n\n\n\nThe adversary's goal is to exploit the intermediate image features $\\mathbf{F}$\\mathbf{F} produced by the victim visual encoder ($\\mathbf{F} = \\mathcal{V}_{Image}(I)$\\mathbf{F} = \\mathcal{V}_{Image}Image(I)) to reconstruct high-level semantic information. In this paper, we mainly consider two forms of semantic targets:\n\\begin{itemize}[leftmargin=1.0em]\n    \\item \\textbf{Caption Reconstruction:} The attacker trains a cross-modality inversion attack model $\\mathcal{A}_\\theta$ to generate a textual caption $T'_{\\text{cap}} = \\mathcal{A}_\\theta(\\mathbf{F})$ that approximates the ground-truth caption $T_{\\text{cap}}$;\n    \\item \\textbf{Label Recovery:} The attacker trains $\\mathcal{A}_\\theta$ as a classifier to predict the image label $y'_{\\text{cls}} = \\mathcal{A}_\\theta(\\mathbf{F})$ matching the true label $y_{\\text{cls}}$.\n\\end{itemize}\\begin{itemize}[leftmargin=1.0em]\n    \\item \\textbf{Caption Reconstruction:} The attacker trains a cross-modality inversion attack model $\\mathcal{A}_\\theta$ to generate a textual caption $T'_{\\text{cap}} = \\mathcal{A}_\\theta(\\mathbf{F})$ that approximates the ground-truth caption $T_{\\text{cap}}$;\n    \\item \\textbf{Label Recovery:} The attacker trains $\\mathcal{A}_\\theta$ as a classifier to predict the image label $y'_{\\text{cls}} = \\mathcal{A}_\\theta(\\mathbf{F})$ matching the true label $y_{\\text{cls}}$.\n\\end{itemize}\n    \\item \\textbf{Caption Reconstruction:} The attacker trains a cross-modality inversion attack model $\\mathcal{A}_\\theta$\\mathcal{A}_\\theta to generate a textual caption $T'_{\\text{cap}} = \\mathcal{A}_\\theta(\\mathbf{F})$T'_{\\text{cap}}\\text{cap} = \\mathcal{A}_\\theta(\\mathbf{F}) that approximates the ground-truth caption $T_{\\text{cap}}$T_{\\text{cap}}\\text{cap};\n    \\item \\textbf{Label Recovery:} The attacker trains $\\mathcal{A}_\\theta$\\mathcal{A}_\\theta as a classifier to predict the image label $y'_{\\text{cls}} = \\mathcal{A}_\\theta(\\mathbf{F})$y'_{\\text{cls}}\\text{cls} = \\mathcal{A}_\\theta(\\mathbf{F}) matching the true label $y_{\\text{cls}}$y_{\\text{cls}}\\text{cls}.\n\n\nFor caption reconstruction, the attacker minimizes a semantic loss between the generated and reference captions:\n\\begin{equation}\n    \\arg\\min\\limits_{\\theta} \\; \\mathcal{L}(T'_{\\text{cap}}, T_{\\text{cap}}), \\label{eq:caption_obj}\n\\end{equation}\\begin{equation}\n    \\arg\\min\\limits_{\\theta} \\; \\mathcal{L}(T'_{\\text{cap}}, T_{\\text{cap}}), \\label{eq:caption_obj}\n\\end{equation}\n    \\arg\\min\\limits_{\\theta}\\theta \\; \\mathcal{L}(T'_{\\text{cap}}\\text{cap}, T_{\\text{cap}}\\text{cap}), \\label{eq:caption_obj}\n\nwhere $\\mathcal{L}(\\cdot,\\cdot)$\\mathcal{L}(\\cdot,\\cdot) is a semantic loss function (e.g., based on token-level or embedding-level similarity; see Sec.~\\ref{sec:method} for details).\n\nFor label recovery, the objective reduces to a standard classification loss:\n\\begin{equation}\n    \\arg\\min\\limits_{\\theta} \\; \\mathcal{L}_{\\text{cls}}(y'_{\\text{cls}}, y_{\\text{cls}}). \\label{eq:label_obj}\n\\end{equation}\\begin{equation}\n    \\arg\\min\\limits_{\\theta} \\; \\mathcal{L}_{\\text{cls}}(y'_{\\text{cls}}, y_{\\text{cls}}). \\label{eq:label_obj}\n\\end{equation}\n    \\arg\\min\\limits_{\\theta}\\theta \\; \\mathcal{L}_{\\text{cls}}\\text{cls}(y'_{\\text{cls}}\\text{cls}, y_{\\text{cls}}\\text{cls}). \\label{eq:label_obj}\n\n\nA successful attack implies that even without accessing raw pixels, the intermediate image features alone are sufficient to compromise user privacy by revealing semantic-level information.\n\n\n\n\n", "appendix": false}, "Methodology of ": {"content": "\n\\label{sec:method}\n\n\\par\\noindent In this section, we introduce the overview of \\textsc{CapRecover}. During training, \\textsc{CapRecover} aligns these intermediate image features with the corresponding ground truth captions/labels, effectively learning the mapping between visual representations and textual descriptions. During inference, \\textsc{CapRecover} relies exclusively on the intermediate features to generate the image caption. While we describe our model primarily in the context of image caption reconstruction, the overall framework applies equally to image label recovery with only minor task-specific adaptations.\n\n\n\n\n\n\\subsection{Overview of \\textsc{CapRecover}}\n\\label{subsec:overview_model}\n\n\\par\\noindent As shown in Figure \\ref{fig:model_overview}, \\textsc{CapRecover} is composed of three primary modules: (1) Feature Projection Module, (2) Feature-Text Alignment Module, and (3) Caption Generation Module.\n\n\n\n\\begin{table}[t]\n    \\centering\n    \\caption{Information of different victim models and their intermediate layers' shapes.}\n    \\label{tab:victim_info}\n    \\vspace{-1.0em} \n    \\begin{threeparttable}\n        % \\fontsize{6.5}{8}\\selectfont\n        \\sizefive\n        \\setlength{\\tabcolsep}{4pt}\n        \\renewcommand{\\arraystretch}{1.05}\n        \\begin{tabular}{ccc}\n            \\toprule\n            \\textbf{Victim model} & \\textbf{Intermediate layer} & \\textbf{Output feature dimension$^{*}$} \\\\\n            \\midrule\n            $\\text{CLIP}_{\\text{ViT16}}$   & base & 512 \\\\\n             $\\text{CLIP}_{\\text{ViT32}}$ & no-proj & 768 \\\\\n             \\hline\n             % \\hline\n            \\multirow{5}{*}{\\makecell[c]{ResNet50\\\\(ResNet101)}} & base & 1024 (512) \\\\\n             & layer1 & [256, 56, 56] $\\rightarrow$ 1024 \\\\\n             & layer2 & [512, 28, 28] $\\rightarrow$ 1024 \\\\\n             & layer3 & [1024, 14, 14] $\\rightarrow$ 1024 \\\\\n             & layer4 & [2048, 7, 7] $\\rightarrow$ 1024 \\\\\n             % \\hline\n             \\hline\n            % \\multirow{5}{*}{ResNet101} & base & 1024 \\\\\n            %  & layer1 & 1024 \\\\\n            %  & layer2 & 1024 \\\\\n            %  & layer3 & 1024 \\\\\n            %  & layer4 & 1024 \\\\\n            %  \\hline\n            %  \\hline\n            % \\multirow{2}{*}{MobileNetV2} & base & 1000 \\\\\n            %   & layer1 & [32, 112, 112] $\\rightarrow$ 1024 \\\\\n            % \\hline\n            % \\hline\n            % \\multirow{2}{*}{MobileNetV3} & base & 1000 \\\\\n            %   & layer1 & [16, 112, 112] $\\rightarrow$ 1024 \\\\\n            MobileNetV2 & \\multirow{2}{*}{base} & \\multirow{2}{*}{1000} \\\\\n            MobileNetV3 & & \\\\\n            \\bottomrule\n        \\end{tabular}\n        \\begin{tablenotes}\n            \\item \\hspace{-1.2em}$^{*}$ {We use a ResNet-based projection module to transform those intermediate features retaining spatial dimensions (e.g., [32, 112, 112]) into a unified vectorized feature space (i.e., $\\mathbb{R}^{1024}$).}\n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\vspace{-1.2em}\n\\end{table}\n    \\centering\n    \\caption{Information of different victim models and their intermediate layers' shapes.}\n    \\label{tab:victim_info}\n    \\vspace{-1.0em} \n    \n        \\sizefive\n        \\setlength{\\tabcolsep}{4pt}\n        \\renewcommand{\\arraystretch}{1.05}\n        \n            \\toprule\n            \\textbf{Victim model} & \\textbf{Intermediate layer} & \\textbf{Output feature dimension$^{*}$} \\\\\n            \\midrule\n            $\\text{CLIP}_{\\text{ViT16}}$\\text{CLIP}_{\\text{ViT16}}\\text{ViT16}   & base & 512 \\\\\n             $\\text{CLIP}_{\\text{ViT32}}$\\text{CLIP}_{\\text{ViT32}}\\text{ViT32} & no-proj & 768 \\\\\n             \\hline\n             \\multirow{5}5{*}*{\\makecell[c]{ResNet50\\\\(ResNet101)}}\\makecell[c]{ResNet50\\\\(ResNet101)}ResNet50\\\\(ResNet101) & base & 1024 (512) \\\\\n             & layer1 & [256, 56, 56] $\\rightarrow$\\rightarrow 1024 \\\\\n             & layer2 & [512, 28, 28] $\\rightarrow$\\rightarrow 1024 \\\\\n             & layer3 & [1024, 14, 14] $\\rightarrow$\\rightarrow 1024 \\\\\n             & layer4 & [2048, 7, 7] $\\rightarrow$\\rightarrow 1024 \\\\\n             \\hline\n            MobileNetV2 & \\multirow{2}2{*}*{base}base & \\multirow{2}2{*}*{1000}1000 \\\\\n            MobileNetV3 & & \\\\\n            \\bottomrule\n        \n        \n            \\item \\hspace{-1.2em}$^{*}$^{*}* {We use a ResNet-based projection module to transform those intermediate features retaining spatial dimensions (e.g., [32, 112, 112]) into a unified vectorized feature space (i.e., $\\mathbb{R}^{1024}$).}We use a ResNet-based projection module to transform those intermediate features retaining spatial dimensions (e.g., [32, 112, 112]) into a unified vectorized feature space (i.e., $\\mathbb{R}^{1024}$\\mathbb{R}^{1024}1024).\n        \n    \n    \\vspace{-1.2em}\n\n\n\n\n\n\\subsubsection{Feature Projection Module}\n\\label{subsec:caprecover_feature_projection}\n\n\\textsc{CapRecover} maps the victim model\u2019s intermediate features into a dimensionally fixed (e.g., 1024) feature space via a projection layer. For example, given an input image $ I_i $ I_i , the victim model's middle layers produce intermediate features $ \\mathbf{F}_i $ \\mathbf{F}_i . When these features are already in vector form, i.e., $\\mathbf{F}_i \\in \\mathbb{R}^d $\\mathbf{F}_i \\in \\mathbb{R}^d , we can simply apply a simple linear projection for $\\mathbf{F}_{i}$\\mathbf{F}_{i}i, which is\n\\begin{equation}\n\\mathbf{F}_i^{proj} = \\mathbf{W}_p \\mathbf{F}_i + \\mathbf{b}_p, \\quad \\mathbf{W}_p \\in \\mathbb{R}^{d' \\times d}, \\quad \\mathbf{b}_p \\in \\mathbb{R}^{d'}\n\\end{equation}\\begin{equation}\n\\mathbf{F}_i^{proj} = \\mathbf{W}_p \\mathbf{F}_i + \\mathbf{b}_p, \\quad \\mathbf{W}_p \\in \\mathbb{R}^{d' \\times d}, \\quad \\mathbf{b}_p \\in \\mathbb{R}^{d'}\n\\end{equation}\n\\mathbf{F}_i^{proj}proj = \\mathbf{W}_p \\mathbf{F}_i + \\mathbf{b}_p, \\quad \\mathbf{W}_p \\in \\mathbb{R}^{d' \\times d}d' \\times d, \\quad \\mathbf{b}_p \\in \\mathbb{R}^{d'}d'\n\nwhere $ \\mathbf{F}_i^{proj} \\in \\mathbb{R}^{d'} $ \\mathbf{F}_i^{proj}proj \\in \\mathbb{R}^{d'}d'  is the projected feature. $d^{'}$d^{'}' is the dimension of the projected feature space. $\\textbf{W}_p$\\textbf{W}_p and $\\mathbf{b}_p$\\mathbf{b}_p are learnable parameters of the feature projection layer.\n\nIn cases where the intermediate outputs retain spatial dimensions (i.e., $\\mathbf{F}_i \\in \\mathbb{R}^{C \\times H \\times W}$\\mathbf{F}_i \\in \\mathbb{R}^{C \\times H \\times W}C \\times H \\times W with $C$C, $H$H and $W$W denoting the channel, height, and width, respectively), \\textsc{CapRecover} first employs a ResNet-based projection module $g(\\cdot)$g(\\cdot) to convert these spatial features into a vectorized form. The transformation is then expressed as:\n\\begin{equation}\n\\mathbf{F}_i^{proj} = \\mathbf{W}_p \\cdot g(\\mathbf{F}_i) +\\mathbf{b}_p, \\quad g: \\mathbb{R}^{C \\times H \\times W} \\to \\mathbb{R}^{d'}\n\\end{equation}\\begin{equation}\n\\mathbf{F}_i^{proj} = \\mathbf{W}_p \\cdot g(\\mathbf{F}_i) +\\mathbf{b}_p, \\quad g: \\mathbb{R}^{C \\times H \\times W} \\to \\mathbb{R}^{d'}\n\\end{equation}\n\\mathbf{F}_i^{proj}proj = \\mathbf{W}_p \\cdot g(\\mathbf{F}_i) +\\mathbf{b}_p, \\quad g: \\mathbb{R}^{C \\times H \\times W}C \\times H \\times W \\to \\mathbb{R}^{d'}d'\n\nwhere $\\mathbf{W}_p$\\mathbf{W}_p and $\\mathbf{b}_p$\\mathbf{b}_p are learnable parameters of  $g(\\cdot)$g(\\cdot). This additional projection module ensures that \\textsc{CapRecover} can consistently process intermediate features from various victim models and different network layers by mapping them into a unified feature space.\n\n\n\\subsubsection{Feature-Text Alignment Module}\n\\label{subsec:caprecover_feature_text_alignment}\n\n\n\n\\textsc{CapRecover} employs an alignment module (for image features and captions) to establish a semantic correspondence between the projected intermediate image features and the ground truth caption. Specifically, while training our \\textsc{CapRecover}, the alignment module first tokenizes and embeds the ground truth caption for each image, resulting in a sequence of text embeddings $\\mathbf{T}_i$\\mathbf{T}_i. Second, to fuse these textual cues with the visual information, \\textsc{CapRecover} further employs a Q-Former model that leverages $K$K trainable query tokens $\\mathbf{Q} \\in \\mathbb{R}^{K \\times d'}$\\mathbf{Q} \\in \\mathbb{R}^{K \\times d'}K \\times d'.\n\nThe Q-Former performs cross-modal attention by interacting with the projected features $\\mathbf{F}_i^{proj}$\\mathbf{F}_i^{proj}proj and the text embedding $\\mathbf{T}_i$\\mathbf{T}_i, producing enriched embeddings $ \\mathbf{Z}_i $ \\mathbf{Z}_i  that capture the alignment between visual and textual modalities, i.e.,\n\\begin{equation}\n\\mathbf{Z}_i = \\text{Q-Former}(\\mathbf{Q}, \\mathbf{F}_i^{proj}, \\mathbf{T}_i),\n\\end{equation}\\begin{equation}\n\\mathbf{Z}_i = \\text{Q-Former}(\\mathbf{Q}, \\mathbf{F}_i^{proj}, \\mathbf{T}_i),\n\\end{equation}\n\\mathbf{Z}_i = \\text{Q-Former}(\\mathbf{Q}, \\mathbf{F}_i^{proj}proj, \\mathbf{T}_i),\n\nwhere $\\mathbf{Z}_i \\in \\mathbb{R}^{K \\times d''}$\\mathbf{Z}_i \\in \\mathbb{R}^{K \\times d''}K \\times d'' and $d''$d'' is the hidden size of the Q-Former. $\\mathbf{Z}_i$\\mathbf{Z}_i is further projected to match the input space of the language model:\n\\begin{equation}\n\\mathbf{E}_i = \\mathbf{W}_l \\mathbf{Z}_i, \\quad \\mathbf{W}_l \\in \\mathbb{R}^{d_{LM} \\times d''}\n\\end{equation}\\begin{equation}\n\\mathbf{E}_i = \\mathbf{W}_l \\mathbf{Z}_i, \\quad \\mathbf{W}_l \\in \\mathbb{R}^{d_{LM} \\times d''}\n\\end{equation}\n\\mathbf{E}_i = \\mathbf{W}_l \\mathbf{Z}_i, \\quad \\mathbf{W}_l \\in \\mathbb{R}^{d_{LM} \\times d''}d_{LM}LM \\times d''\n\nwhere $ \\mathbf{E}_i \\in \\mathbb{R}^{K \\times d_{LM}} $ \\mathbf{E}_i \\in \\mathbb{R}^{K \\times d_{LM}}K \\times d_{LM}LM  serves as input to the language model.\nAt inference time, when the ground truth caption is not available, the Feature-Text Alignment module relies solely on the projected image features $\\mathbf{F}_i^{proj}$\\mathbf{F}_i^{proj}proj to generate the enriched embeddings $\\mathbf{Z}_i$\\mathbf{Z}_i. These embeddings are subsequently forwarded to the caption generation module, completing the reconstruction pipeline.\n\n\n\n\n\n\\begin{table*}\n    \\centering\n    \\caption{Experimental results of \\textsc{CapRecover} attacking different victim models on three datasets.}\n    \\label{tab:exp_res_coco2017}\n    \\vspace{-1.0em}\n    \\begin{threeparttable}\n        \\sizefive\n        \\setlength{\\tabcolsep}{4pt}\n        \\renewcommand{\\arraystretch}{1.05}\n        \\begin{tabular}{ccccccccccc}\n            \\toprule\n            \\textbf{Dataset} & \\textbf{Victim model}$^{*}$ & \\textbf{BLEU-1} & \\textbf{BLEU-2} & \\textbf{BLEU-3} & \\textbf{BLEU-4} & \\textbf{METEOR} & \\textbf{ROUGE\\_L} & \\textbf{CIDEr} & \\textbf{SPICE} & \\makecell[c]{\\textbf{Cosine Similarity (\\%)}$^{\\ddagger}$} \\\\\n            \\midrule\n            \\multirow{6}{*}{COCO2017} & $\\text{CLIP}_{\\text{ViT16}}$    & 0.72        & 0.55        & 0.41  & 0.30 & 0.26 & 0.53 & 0.99 & 0.19 & 84.38       \\\\\n            & $\\text{CLIP}_{\\text{ViT32}}$     & 0.70        & 0.53        & 0.39  & 0.29 & 0.26 & 0.53 & 0.95 & 0.19 & 80.38       \\\\\n            & RN50     & 0.70        & 0.52        & 0.38  & 0.28 & 0.25 & 0.52 & 0.90 & 0.18 & 76.84       \\\\\n            & RN101     & 0.70        & 0.52        & 0.39  & 0.28 & 0.25 & 0.53 & 0.93 & 0.18 & 79.98       \\\\\n            & MNV2     & 0.39        & 0.18        & 0.10  & 0.06 & 0.11 & 0.31 & 0.09 & 0.03 & ~0.44     \\\\\n            & MNV3     & 0.40        & 0.19        & 0.10  & 0.08 & 0.11 & 0.31 & 0.10 & 0.03 & ~2.74       \\\\\n            \\hline\n            \\hline\n            \\multirow{6}{*}{Flickr8K} & $\\text{CLIP}_{\\text{ViT16}}$    & 0.30        & 0.15        & 0.08  & 0.05 & 0.13 & 0.27 & 0.54 & 0.19 & 22.40         \\\\\n            & $\\text{CLIP}_{\\text{ViT32}}$     & 0.29        & 0.15        & 0.08  & 0.05 & 0.12 & 0.26 & 0.48 & 0.17 & 18.40        \\\\\n            & RN50     & 0.28        & 0.14        & 0.08  & 0.04 & 0.12 & 0.25 & 0.46 & 0.17 & 16.50        \\\\\n            & RN101    & 0.28        & 0.14        & 0.08  & 0.05 & 0.12 & 0.25 & 0.47 & 0.17 & 18.00       \\\\\n            & MNV2     & 0.20        & 0.08        & 0.04  & 0.02 & 0.07 & 0.17 & 0.16 & 0.06 & ~1.20        \\\\\n            & MNV3     & 0.21        & 0.08        & 0.04  & 0.02 & 0.07 & 0.17 & 0.17 & 0.06 & ~1.80        \\\\\n            \\hline\n            \\hline\n            \\multirow{6}{*}{ImageNet-1K} & $\\text{CLIP}_{\\text{ViT16}}$     & 0.45        & 0.30        & 0.21  & 0.15 & 0.18 & 0.41 & 1.18 & 0.2 & 40.78       \\\\\n            & $\\text{CLIP}_{\\text{ViT32}}$     & 0.44        & 0.29        & 0.20  & 0.14 & 0.18 & 0.40 & 1.08 & 0.19 & 36.11        \\\\\n            & RN50     & 0.42        & 0.27        & 0.18  & 0.13 & 0.16 & 0.38 & 0.93 & 0.16 & 27.00       \\\\\n            & RN101     & 0.42        & 0.27        & 0.18  & 0.13 & 0.16 & 0.38 & 0.95 & 0.16 & 27.78      \\\\\n            & MNV2     & 0.29        & 0.15        & 0.07  & 0.03 & 0.09 & 0.26 & 0.18 & 0.03 & ~1.67      \\\\\n            & MNV3     & 0.29        & 0.14        & 0.07  & 0.04 & 0.09 & 0.26 & 0.22 & 0.04 & ~6.11    \\\\\n            \\bottomrule\n        \\end{tabular}\n        \\begin{tablenotes}\n            \\item \\hspace{-1.2em}$^{*}$ {``RN50'' (``RN101'') denotes ResNet50 (ResNet101) and ``MNV2'' (MNV3) denotes MobileNetV2 (MobileNetV3). }\n            % Here we utilize the final output image features of the corresponding model as intermediate features. The experimental results for the middle layer of different models are shown in Table \\ref{tab:res_model_middle_layers}.}\n            \\item \\hspace{-1.2em}$^{\\ddagger}$ {We calculate the proportion of cosine similarities that are greater than a predefined threshold, which is empirically set to 0.7 in our paper.}\n        \\end{tablenotes}\n    \\end{threeparttable}\n    \\vspace{-1.1em}\n\\end{table*}\n    \\centering\n    \\caption{Experimental results of \\textsc{CapRecover} attacking different victim models on three datasets.}\n    \\label{tab:exp_res_coco2017}\n    \\vspace{-1.0em}\n    \n        \\sizefive\n        \\setlength{\\tabcolsep}{4pt}\n        \\renewcommand{\\arraystretch}{1.05}\n        \n            \\toprule\n            \\textbf{Dataset} & \\textbf{Victim model}$^{*}$^{*}* & \\textbf{BLEU-1} & \\textbf{BLEU-2} & \\textbf{BLEU-3} & \\textbf{BLEU-4} & \\textbf{METEOR} & \\textbf{ROUGE\\_L} & \\textbf{CIDEr} & \\textbf{SPICE} & \\makecell[c]{\\textbf{Cosine Similarity (\\%)}$^{\\ddagger}$}\\textbf{Cosine Similarity (\\%)}$^{\\ddagger}$^{\\ddagger}\\ddagger \\\\\n            \\midrule\n            \\multirow{6}6{*}*{COCO2017}COCO2017 & $\\text{CLIP}_{\\text{ViT16}}$\\text{CLIP}_{\\text{ViT16}}\\text{ViT16}    & 0.72        & 0.55        & 0.41  & 0.30 & 0.26 & 0.53 & 0.99 & 0.19 & 84.38       \\\\\n            & $\\text{CLIP}_{\\text{ViT32}}$\\text{CLIP}_{\\text{ViT32}}\\text{ViT32}     & 0.70        & 0.53        & 0.39  & 0.29 & 0.26 & 0.53 & 0.95 & 0.19 & 80.38       \\\\\n            & RN50     & 0.70        & 0.52        & 0.38  & 0.28 & 0.25 & 0.52 & 0.90 & 0.18 & 76.84       \\\\\n            & RN101     & 0.70        & 0.52        & 0.39  & 0.28 & 0.25 & 0.53 & 0.93 & 0.18 & 79.98       \\\\\n            & MNV2     & 0.39        & 0.18        & 0.10  & 0.06 & 0.11 & 0.31 & 0.09 & 0.03 & ~0.44     \\\\\n            & MNV3     & 0.40        & 0.19        & 0.10  & 0.08 & 0.11 & 0.31 & 0.10 & 0.03 & ~2.74       \\\\\n            \\hline\n            \\hline\n            \\multirow{6}6{*}*{Flickr8K}Flickr8K & $\\text{CLIP}_{\\text{ViT16}}$\\text{CLIP}_{\\text{ViT16}}\\text{ViT16}    & 0.30        & 0.15        & 0.08  & 0.05 & 0.13 & 0.27 & 0.54 & 0.19 & 22.40         \\\\\n            & $\\text{CLIP}_{\\text{ViT32}}$\\text{CLIP}_{\\text{ViT32}}\\text{ViT32}     & 0.29        & 0.15        & 0.08  & 0.05 & 0.12 & 0.26 & 0.48 & 0.17 & 18.40        \\\\\n            & RN50     & 0.28        & 0.14        & 0.08  & 0.04 & 0.12 & 0.25 & 0.46 & 0.17 & 16.50        \\\\\n            & RN101    & 0.28        & 0.14        & 0.08  & 0.05 & 0.12 & 0.25 & 0.47 & 0.17 & 18.00       \\\\\n            & MNV2     & 0.20        & 0.08        & 0.04  & 0.02 & 0.07 & 0.17 & 0.16 & 0.06 & ~1.20        \\\\\n            & MNV3     & 0.21        & 0.08        & 0.04  & 0.02 & 0.07 & 0.17 & 0.17 & 0.06 & ~1.80        \\\\\n            \\hline\n            \\hline\n            \\multirow{6}6{*}*{ImageNet-1K}ImageNet-1K & $\\text{CLIP}_{\\text{ViT16}}$\\text{CLIP}_{\\text{ViT16}}\\text{ViT16}     & 0.45        & 0.30        & 0.21  & 0.15 & 0.18 & 0.41 & 1.18 & 0.2 & 40.78       \\\\\n            & $\\text{CLIP}_{\\text{ViT32}}$\\text{CLIP}_{\\text{ViT32}}\\text{ViT32}     & 0.44        & 0.29        & 0.20  & 0.14 & 0.18 & 0.40 & 1.08 & 0.19 & 36.11        \\\\\n            & RN50     & 0.42        & 0.27        & 0.18  & 0.13 & 0.16 & 0.38 & 0.93 & 0.16 & 27.00       \\\\\n            & RN101     & 0.42        & 0.27        & 0.18  & 0.13 & 0.16 & 0.38 & 0.95 & 0.16 & 27.78      \\\\\n            & MNV2     & 0.29        & 0.15        & 0.07  & 0.03 & 0.09 & 0.26 & 0.18 & 0.03 & ~1.67      \\\\\n            & MNV3     & 0.29        & 0.14        & 0.07  & 0.04 & 0.09 & 0.26 & 0.22 & 0.04 & ~6.11    \\\\\n            \\bottomrule\n        \n        \n            \\item \\hspace{-1.2em}$^{*}$^{*}* {``RN50'' (``RN101'') denotes ResNet50 (ResNet101) and ``MNV2'' (MNV3) denotes MobileNetV2 (MobileNetV3). }``RN50'' (``RN101'') denotes ResNet50 (ResNet101) and ``MNV2'' (MNV3) denotes MobileNetV2 (MobileNetV3). \n            \\item \\hspace{-1.2em}$^{\\ddagger}$^{\\ddagger}\\ddagger {We calculate the proportion of cosine similarities that are greater than a predefined threshold, which is empirically set to 0.7 in our paper.}We calculate the proportion of cosine similarities that are greater than a predefined threshold, which is empirically set to 0.7 in our paper.\n        \n    \n    \\vspace{-1.1em}\n\n\n\\subsubsection{Caption Generation}\n\\label{subsec:caprecover_caption_generation}\n\nBy using these outputs from the Feature-Text Alignment Module, \\textsc{CapRecover} further employs a large language model (LLM) to interpret the compressed image representations and generate the final caption that accurately describes the image's semantic content. Specifically, the LLM processes the input embeddings $\\mathbf{E}_i$\\mathbf{E}_i and, if provided, extra text input embeddings $\\mathbf{T}_i$\\mathbf{T}_i (e.g., prompts), to generate the caption $C_i$C_i for the image $I_i$I_i. This caption generation process is modeled autoregressively as follows:\n\\begin{equation}\nP(C_i \\mid \\mathbf{E}_i, \\mathbf{T}_i) = \\prod_{t=1}^{T} P(c_{i,t} \\mid c_{i,<t}, \\mathbf{E}_i, \\mathbf{T}_i),\n\\end{equation}\\begin{equation}\nP(C_i \\mid \\mathbf{E}_i, \\mathbf{T}_i) = \\prod_{t=1}^{T} P(c_{i,t} \\mid c_{i,<t}, \\mathbf{E}_i, \\mathbf{T}_i),\n\\end{equation}\nP(C_i \\mid \\mathbf{E}_i, \\mathbf{T}_i) = \\prod_{t=1}t=1^{T}T P(c_{i,t}i,t \\mid c_{i,<t}i,<t, \\mathbf{E}_i, \\mathbf{T}_i),\n\nwhere $c_{i,t}$c_{i,t}i,t denotes the token generated at time step $t$t and $c_{i,<t}$c_{i,<t}i,<t represents all preceding tokens before time step $t$t.\n\n\n\n\n\n\\subsection{Model Training Objective and Settings}\n\\label{subsec:model_obj}\n\n\n\\par\\noindent To train the model, we minimize the cross-entropy loss between the generated caption $ C_i $ C_i  and the ground-truth caption $ C_i^{*} $ C_i^{*}* :\n\\begin{equation}\n\\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^T \\log P(c_{i,t}^{*} \\mid c_{i,<t}^{*}, \\mathbf{E}_i, \\mathbf{T}_i)\n\\end{equation}\\begin{equation}\n\\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=1}^T \\log P(c_{i,t}^{*} \\mid c_{i,<t}^{*}, \\mathbf{E}_i, \\mathbf{T}_i)\n\\end{equation}\n\\mathcal{L} = - \\frac{1}{N} \\sum_{i=1}i=1^N \\sum_{t=1}t=1^T \\log P(c_{i,t}i,t^{*}* \\mid c_{i,<t}i,<t^{*}*, \\mathbf{E}_i, \\mathbf{T}_i)\n\nwhere $ N $ N  is the batch size, $ T $ T  is the caption length, and $ c_{i,t}^{*} $ c_{i,t}i,t^{*}*  is the ground-truth.\n\n\n\n\\label{subsec:model_setting}\n\n\n\n\n\n\n\nThe feature projection module in \\textsc{CapRecover} is initialized with a random distribution, while employing a pre-trained Q-Former model for the feature-text alignment and a pre-trained OPT model for language generation. To focus the training on aligning the visual features with the corresponding textual information, we freeze the parameters of the language model and update only those in the feature projection and feature-text alignment modules.\n\n\\textsc{CapRecover} is trained for six epochs with a learning rate of $5e-5$5e-5. The training batch size is configured to 16, while the testing batch size is set at 8. All experiments are conducted on a cloud server equipped with a single NVIDIA RTX 4090 (24 GB memory).\n\n\n\n\n\n\n", "appendix": false}, "Experiments on Caption Reconstruction": {"content": "\n\\label{sec:experiments}\n\n\\subsection{Experimental Settings}\n\\label{subsec:settings}\n\n\\subsubsection{Datasets}\n\\label{subsec:exp_dst}\n\n\n\\begin{figure*}[t]\n    \\hspace{-1.5em}\n    \\begin{tabular}{cc}\n        \n        \\subfigure[{Evaluation on COCO2017 dataset.}]{\n            \\begin{minipage}[t]{0.31\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_coco2017}\n                \\includegraphics[width=0.89\\textwidth]{./figs/plot_cosine_similarity_pdf_distribution_blip2-opt-2.7b_COCO2017_all_victim_models.pdf}\n        \\end{minipage}}\n\n        \\subfigure[{Evaluation on Flickr8K dataset.}]{\n            \\begin{minipage}[t]{0.31\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_flickr8k}\n                \\includegraphics[width=0.89\\textwidth]{./figs/plot_cosine_similarity_pdf_distribution_blip2-opt-2.7b_flickr8k_all_victim_models.pdf}\n        \\end{minipage}}\n\n        \\subfigure[{Evaluation on ImageNet-1K dataset.}]{\n            \\begin{minipage}[t]{0.31\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_imagenet1k}\n                \\includegraphics[width=0.89\\textwidth]{./figs/plot_cosine_similarity_pdf_distribution_blip2-opt-2.7b_imagenet_all_victim_models.pdf}\n        \\end{minipage}\n        }\n    \\end{tabular}\n    \\vspace{-1.3em}\n\n    \\caption{\n        Distribution of cosine similarities across three datasets. We use intermediate features extracted from the final layer of the victim model to train \\textsc{CapRecover}. We analyze how other intermediate layers' features impact performance in Sec.~\\ref{subsec:further_study_analysis}.\n    }\n\t\\label{fig:cosine_similarity_on_three_dst}\n    \\vspace{-1.0em}\n\\end{figure*}\n    \\hspace{-1.5em}\n    \n        \n        \\subfigure[{Evaluation on COCO2017 dataset.}Evaluation on COCO2017 dataset.]{\n            \\begin{minipage}[t]{0.31\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_coco2017}\n                \\includegraphics[width=0.89\\textwidth]{./figs/plot_cosine_similarity_pdf_distribution_blip2-opt-2.7b_COCO2017_all_victim_models.pdf}\n        \\end{minipage}}\n            [t]{0.31\\textwidth}0.31\\textwidth\n                \\centering\n                \\label{fig:bleu1_on_coco2017}\n                \\includegraphics[width=0.89\\textwidth]{./figs/plot_cosine_similarity_pdf_distribution_blip2-opt-2.7b_COCO2017_all_victim_models.pdf}\n        \n\n        \\subfigure[{Evaluation on Flickr8K dataset.}Evaluation on Flickr8K dataset.]{\n            \\begin{minipage}[t]{0.31\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_flickr8k}\n                \\includegraphics[width=0.89\\textwidth]{./figs/plot_cosine_similarity_pdf_distribution_blip2-opt-2.7b_flickr8k_all_victim_models.pdf}\n        \\end{minipage}}\n            [t]{0.31\\textwidth}0.31\\textwidth\n                \\centering\n                \\label{fig:bleu1_on_flickr8k}\n                \\includegraphics[width=0.89\\textwidth]{./figs/plot_cosine_similarity_pdf_distribution_blip2-opt-2.7b_flickr8k_all_victim_models.pdf}\n        \n\n        \\subfigure[{Evaluation on ImageNet-1K dataset.}Evaluation on ImageNet-1K dataset.]{\n            \\begin{minipage}[t]{0.31\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_imagenet1k}\n                \\includegraphics[width=0.89\\textwidth]{./figs/plot_cosine_similarity_pdf_distribution_blip2-opt-2.7b_imagenet_all_victim_models.pdf}\n        \\end{minipage}\n        }\n            [t]{0.31\\textwidth}0.31\\textwidth\n                \\centering\n                \\label{fig:bleu1_on_imagenet1k}\n                \\includegraphics[width=0.89\\textwidth]{./figs/plot_cosine_similarity_pdf_distribution_blip2-opt-2.7b_imagenet_all_victim_models.pdf}\n        \n        \n    \n    \\vspace{-1.3em}\n\n    \\caption{\n        Distribution of cosine similarities across three datasets. We use intermediate features extracted from the final layer of the victim model to train \\textsc{CapRecover}. We analyze how other intermediate layers' features impact performance in Sec.~\\ref{subsec:further_study_analysis}.\n    }\n\t\\label{fig:cosine_similarity_on_three_dst}\n    \\vspace{-1.0em}\n\n\nTo comprehensively evaluate the effectiveness of \\textsc{CapRecover}, we adopt three widely-used datasets: COCO2017 \\cite{lin2014microsoft}, Flickr8K \\cite{dst_flickr8k}, and ImageNet-1K \\cite{dst_imagenet}. For the ImageNet-1K dataset, we use Qwen2.5 \\cite{yang2024qwen2} to generate captions for the images. We employ generated captions for ImageNet-1K due to: (1) the original ImageNet-1K dataset does not provide human-annotated captions, and (2) recent research \\cite{nguyen2023improving, lei2023image} demonstrates the effectiveness and semantic accuracy of captions generated by advanced VLMs. We will clarify this in the revised version. Given the large size of the original ImageNet-1K dataset, we randomly sample 12,000 images for the training set and 1,000 images for the test set. More details about these datasets are provided in Table \\ref{tab:dsts_used}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Victim Models}\n\\label{subsec:model_layer}\nWe focus on three widely adopted visual models commonly utilized in Vision-Language Models (VLMs) and deployed on edge devices: Vision Transformer \\cite{dosovitskiy2020image} (ViT), ResNet \\cite{he2016deep}, and MobileNet (MobileNetV2 \\cite{sandler2018mobilenetv2} and MobileNetV3 \\cite{howard2019searching}). In practice, ResNet (e.g., ResNet50 and ResNet101) serves as the image encoder in VLMs like CLIP \\cite{clip} and UPL \\cite{huang2022unsupervised}. ViT (e.g., ViT-16B and ViT-32B) serves as the visual module in VLMs such as CLIP \\cite{clip} and LlaVa \\cite{liu2023llava}. As a lightweight convolutional neural network optimized for mobile applications, MobileNet is widely deployed on edge devices like mobile phones, offering efficient performance for on-device inference.\n\nWe analyze both the final output of the victim model (referred to as the ``base'' output) and the intermediate output before the final linear projection layer (denoted as ``no-proj'' for ViT-based models and ``layer4'' for ResNet-based models). Additionally, we examine the impact of different middle layers within the victim models (e.g., ``layer1''$\\thicksim$\\thicksim``layer4'' in ResNet50) on reconstruction performance.\n\n\n\n\n\n\n\n\n\n\\subsubsection{Evaluation Metrics}\n\\label{subsec:metrics}\n\n\n\n\n\n\nWe evaluate \\textsc{CapRecover}'s performance using two main categories of metrics: standard metrics and semantic similarity metrics based on cosine similarity.\n\nCommon Metrics:\nWe adopt widely used evaluation measures, including: BLEU-1$\\thicksim$\\thicksimBLEU-4, METEOR, ROUGE\\_L, CIDEr, and SPICE, to assess the quality of the generated captions. These metrics quantify how closely the generated captions match the ground truth captions in terms of lexical overlap. We primarily rely on ROUGE-L as our main metric, since it captures structural alignment and semantic completeness more effectively than n-gram-based scores. We consider ROUGE-L scores above 0.3 as indicative of moderate attack success, reflecting partial semantic recovery, and scores above 0.5 as indicative of successful attacks that capture most of the key semantic content.\n\nEmbedding-Based Cosine Similarity:\nIn addition to the common metrics, we use a pre-trained embedding model \\cite{yang2024qwen2} to project both the generated and ground truth captions into a shared semantic space. We then compute the cosine similarity between these embeddings to measure the semantic alignment between the captions.  We interpret similarity values above 0.7 as successful attacks.\n\nNote that as far as we know, our work mainly focuses on the direct recovery of image captions or labels from leaked intermediate features rather than reconstructing images first. To our knowledge, no prior studies address this specific problem.\n\n\n\n\n\n\n\n\\subsection{Experimental Results}\n\\label{subsec:exp_res}\n\n\n\n\n\n\n\n\n\n\n\\subsubsection{Overall results}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe evaluate the performance of \\textsc{CapRecover} on six victim models across three benchmark datasets: COCO2017, Flickr8K, and ImageNet-1K. As shown in Table~\\ref{tab:exp_res_coco2017}, \\textsc{CapRecover} achieves the strongest results on the COCO2017 dataset. For example, when attacking $\\text{CLIP}_{\\text{ViT16}}$\\text{CLIP}_{\\text{ViT16}}\\text{ViT16}, the model achieves a BLEU-1 score of 0.72 and a ROUGE-L score of 0.53, with 84.38\\% of generated captions exceeding a cosine similarity threshold of 0.7\u2014indicating strong semantic and structural alignment with the ground truth. Similar performance is observed for other ViT- and ResNet-based victim models, with ROUGE-L scores consistently around 0.52\u20130.53, which we consider indicative of successful semantic inversion.\n\n\n\n\n\n\\begin{figure*}[t]\n    \\centering\n    \\includegraphics[width = 0.91\\linewidth]{figs/rn50_heatmap_v4.pdf}\n    \\vspace{-0.8em}\n    \\caption{Example of visualizing the heatmaps of ResNet50's different middle layers. Below each figure is the generated/ground truth caption. These figures demonstrate that the shallow layer (e.g., RN50-Layer1) pays more attention to edges and local features, while the deeper the layer (e.g., RN50-Layer4), the more attention is paid to the more semantic areas in the image.}\n    \\label{fig:rn50_heatmap}\n    \\vspace{-1.0em}\n\\end{figure*}\n    \\centering\n    \\includegraphics[width = 0.91\\linewidth]{figs/rn50_heatmap_v4.pdf}\n    \\vspace{-0.8em}\n    \\caption{Example of visualizing the heatmaps of ResNet50's different middle layers. Below each figure is the generated/ground truth caption. These figures demonstrate that the shallow layer (e.g., RN50-Layer1) pays more attention to edges and local features, while the deeper the layer (e.g., RN50-Layer4), the more attention is paid to the more semantic areas in the image.}\n    \\label{fig:rn50_heatmap}\n    \\vspace{-1.0em}\n\n\n\n\\begin{figure*}[t]\n    \\hspace{-1.3em}\n    \\begin{tabular}{ccccc}\n    \n        \\subfigure[{ResNet50-layer1.}]{\n            \\begin{minipage}[t]{0.19\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_rn50_layer1}\n                \\includegraphics[width=0.97\\textwidth]{./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-layer1.pdf}\n        \\end{minipage}}\n\n        \\subfigure[{ResNet50-layer2.}]{\n            \\begin{minipage}[t]{0.19\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_rn50_layer2}\n                \\includegraphics[width=0.97\\textwidth]{./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-layer2.pdf}\n        \\end{minipage}}\n\n        \\subfigure[{ResNet50-layer3.}]{\n            \\begin{minipage}[t]{0.19\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_rn50_layer3}\n                \\includegraphics[width=0.97\\textwidth]{./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-layer3.pdf}\n        \\end{minipage}}\n\n        \\subfigure[{ResNet50-layer4.}]{\n            \\begin{minipage}[t]{0.19\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_rn50_layer4}\n                \\includegraphics[width=0.97\\textwidth]{./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-layer4.pdf}\n        \\end{minipage}}\n\n        \\subfigure[{ResNet50-base.}]{\n            \\begin{minipage}[t]{0.19\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_rn50_base}\n                \\includegraphics[width=0.97\\textwidth]{./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-base.pdf}\n        \\end{minipage}}\n        \n    \\end{tabular}\n    \\vspace{-1.1em}\n\n    \\caption{\n        Evaluate the distributions of cosine similarity on the COCO2017 dataset. We train \\textsc{CapRecover} using the intermediate image features produced by their final linear projection layers. We further discuss how other middle layers' intermediate features affect \\textsc{CapRecover}'s performance in Sec. \\ref{subsec:further_study_analysis}.\n    }\n\t\\label{fig:bleu1_on_rn50_coco}\n    \\vspace{-0.8em}\n\\end{figure*}\n    \\hspace{-1.3em}\n    \n    \n        \\subfigure[{ResNet50-layer1.}ResNet50-layer1.]{\n            \\begin{minipage}[t]{0.19\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_rn50_layer1}\n                \\includegraphics[width=0.97\\textwidth]{./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-layer1.pdf}\n        \\end{minipage}}\n            [t]{0.19\\textwidth}0.19\\textwidth\n                \\centering\n                \\label{fig:bleu1_on_rn50_layer1}\n                \\includegraphics[width=0.97\\textwidth]{./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-layer1.pdf}\n        \n\n        \\subfigure[{ResNet50-layer2.}ResNet50-layer2.]{\n            \\begin{minipage}[t]{0.19\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_rn50_layer2}\n                \\includegraphics[width=0.97\\textwidth]{./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-layer2.pdf}\n        \\end{minipage}}\n            [t]{0.19\\textwidth}0.19\\textwidth\n                \\centering\n                \\label{fig:bleu1_on_rn50_layer2}\n                \\includegraphics[width=0.97\\textwidth]{./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-layer2.pdf}\n        \n\n        \\subfigure[{ResNet50-layer3.}ResNet50-layer3.]{\n            \\begin{minipage}[t]{0.19\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_rn50_layer3}\n                \\includegraphics[width=0.97\\textwidth]{./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-layer3.pdf}\n        \\end{minipage}}\n            [t]{0.19\\textwidth}0.19\\textwidth\n                \\centering\n                \\label{fig:bleu1_on_rn50_layer3}\n                \\includegraphics[width=0.97\\textwidth]{./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-layer3.pdf}\n        \n\n        \\subfigure[{ResNet50-layer4.}ResNet50-layer4.]{\n            \\begin{minipage}[t]{0.19\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_rn50_layer4}\n                \\includegraphics[width=0.97\\textwidth]{./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-layer4.pdf}\n        \\end{minipage}}\n            [t]{0.19\\textwidth}0.19\\textwidth\n                \\centering\n                \\label{fig:bleu1_on_rn50_layer4}\n                \\includegraphics[width=0.97\\textwidth]{./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-layer4.pdf}\n        \n\n        \\subfigure[{ResNet50-base.}ResNet50-base.]{\n            \\begin{minipage}[t]{0.19\\textwidth}\n                \\centering\n                \\label{fig:bleu1_on_rn50_base}\n                \\includegraphics[width=0.97\\textwidth]{./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-base.pdf}\n        \\end{minipage}}\n            [t]{0.19\\textwidth}0.19\\textwidth\n                \\centering\n                \\label{fig:bleu1_on_rn50_base}\n                \\includegraphics[width=0.97\\textwidth]{./figs/Bleu_1_COCO2017_blip2-opt-2.7b_RN50_resnet-base.pdf}\n        \n        \n    \n    \\vspace{-1.1em}\n\n    \\caption{\n        Evaluate the distributions of cosine similarity on the COCO2017 dataset. We train \\textsc{CapRecover} using the intermediate image features produced by their final linear projection layers. We further discuss how other middle layers' intermediate features affect \\textsc{CapRecover}'s performance in Sec. \\ref{subsec:further_study_analysis}.\n    }\n\t\\label{fig:bleu1_on_rn50_coco}\n    \\vspace{-0.8em}\n\n\n\nIn contrast, performance on the Flickr8K dataset is substantially lower across all models. For instance, CLIP\\textsubscript{ViT16}ViT16 yields a BLEU-1 score of only 0.30 and a ROUGE-L of 0.27, and the proportion of cosine similarities exceeding 0.7 drops to 22.40\\%. This degradation is largely due to the small size of Flickr8K (8,000 images) and the nature of its captions, which are shorter, less diverse, and often semantically sparse. Such properties limit the model\u2019s ability to learn rich visual-to-text mappings and result in lower alignment on both lexical and structural metrics.\n\nResults on ImageNet-1K fall between the two extremes. Despite being a classification dataset without explicit captions, ViT- and ResNet-based models still enable moderate recovery: CLIP\\textsubscript{ViT16}ViT16 yields a BLEU-1 of 0.45, ROUGE-L of 0.41, and cosine similarity over 40\\%. These results suggest that classification-pretrained encoders implicitly retain a significant amount of caption-relevant semantic information in their intermediate features.\n\nAcross all three datasets, MobileNetV2 and MobileNetV3 consistently show the weakest performance. For example, on COCO2017, their ROUGE-L scores are only 0.31 and their cosine similarities barely exceed 3\\%, indicating poor semantic preservation. We attribute this to the lightweight, efficiency-oriented design of MobileNet models. MobileNet utilizes depthwise separable convolutions and aggressive dimensionality reduction strategies designed for efficiency, which reduce model complexity but significantly compromise the model\u2019s ability to capture detailed and high-level semantic features \\cite{li2022efficientformer,li2023rethinking,vasu2023fastvit}. This architectural limitation inherently leads to less detailed intermediate features, explaining the lower effectiveness of CapRecover on MobileNet. We will clarify this in the revised version.\nA more detailed analysis of layer-wise performance is provided in Section~\\ref{subsec:further_study_analysis}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\vspace{-0.5em}\n\\subsection{Further Study on Middle Layers}\n\\label{subsec:further_study_analysis}\n\n\n\\par\\noindent As shown in Table \\ref{tab:exp_rn50_mid_layer}, we employ $\\text{CLIP}_{\\text{ViT16}}$\\text{CLIP}_{\\text{ViT16}}\\text{ViT16} and ResNet50 as victim models to investigate how intermediate image features from different layers impact caption reconstruction. Our analysis reveals that features extracted from shallow layers contribute minimally to caption reconstruction because they primarily capture low-level visual characteristics, such as edges and textures. \n\nIn contrast, the intermediate image features from deeper layers significantly enhance \\textsc{CapRecover}\u2019s performance, as evidenced by higher BLEU-1 scores compared to those obtained from shallow layers. This finding suggests that as convolutional layers deepen, they capture more specific and meaningful semantic information from the image. Figure \\ref{fig:bleu1_on_rn50_coco} illustrates this trend by showing that the BLEU-1 score distribution for captions generated by \\textsc{CapRecover} on the COCO2017 dataset shifts to the right as layer depth increases, reflecting improved overall prediction accuracy.\n\n\nFurthermore, Figure \\ref{fig:rn50_heatmap} visualizes heat maps of different convolutional layers in ResNet50 for a target image. When \\textsc{CapRecover} uses features from a shallow layer (e.g., ResNet50-layer1), which captures basic semantics such as the edges of a mountain or human, the generated captions are less accurate and may even meaningless. However, as the intermediate features come from deeper layers, \\textsc{CapRecover} gradually captures more relevant information from the image (such as ``snow'', ``skiing'' and ``man''). When utilizing features from a deep middle layer (e.g., ResNet50-layer4), the generated caption closely approximates the ground truth caption.\n\n\n\n\n\n\n\n\n\n\n\n", "appendix": false}, "Experiments on Label Recovery": {"content": "\n\\label{sec:exp_img_classification}\n\n\nIn this section, we explore extending \\textsc{CapRecover} to additional Vision-Language Model (VLM) application scenarios\u2014specifically, \\textit{image label recovery}. While our method is primarily introduced in the context of image caption reconstruction, the underlying architecture and attack strategy are general and easily transferable. To adapt \\textsc{CapRecover} (as shown in Figure \\ref{fig:model_overview}) for classification reconstruction tasks, we replace the original large language model (LLM) component with a standard linear classifier, as our preliminary experiments indicate that employing a simple classifier is enough to achieve high Top-1 accuracy.\n\n\n\\subsection{Experimental Settings}\n\\label{subsec:exp_set_img_classification}\n\n\n\n\\subsubsection{Dataset} As shown in Table \\ref{tab:dsts_used}, we evaluate \\textsc{CapRecover} on the CIFAR-10 and TinyImageNet datasets. CIFAR-10 consists of 60,000 images evenly distributed across 10 classes, with 50,000 images used for training and 10,000 for testing. TinyImageNet contains 200 classes with 500 training images and 50 validation images per class (a total of 100,000 images). Due to the increased dataset complexity and class granularity, TinyImageNet serves as a challenging benchmark for label recovery attacks.\n\n\\subsubsection{Victim models} Similar to the setting in Sec. \\ref{subsec:model_layer}, the victim models we selected are ResNet-50 (i.e., RN50) and a CLIP variant with a ViT-B/32 backbone (i.e., $\\text{CLIP}_{\\text{ViT16}}$\\text{CLIP}_{\\text{ViT16}}\\text{ViT16}), from which intermediate visual features are extracted as inputs to our \\textsc{CapRecover}. We use the final output of these victim models (i.e., ``base'' output) as the intermediate features.\n\n\\subsubsection{Model settings} We train the label recovery model using the Adam optimizer with a learning rate of $5 \\times 10^{-4}$5 \\times 10^{-4}-4, a batch size of 64 for training and 16 for evaluation. The model is trained for 5 epochs in total. All experiments are conducted using standard cross-entropy loss for classification.\n\n\n\n\\begin{table}[t]\n    \\centering\n    \\caption{Comparison of experimental results on ResNet50 and $\\text{CLIP}_{\\text{ViT16}}$ using different middle layers.}\n    \\label{tab:exp_rn50_mid_layer}\n    \\vspace{-1.0em}\n    \\begin{threeparttable}\n        \\sizefive\n        \\setlength{\\tabcolsep}{4pt}\n        \\renewcommand{\\arraystretch}{1.05}\n        \\begin{tabular}{ccccc}\n            \\toprule\n            Victim model & Middle layer & BLEU-1 & CIDEr & Cosine Similarity (\\%) \\\\\n            \\midrule\n            \\multirow{5}{*}{ResNet50} & layer1       & 0.24        & 0.19        & ~0.00         \\\\\n             & layer2       & 0.51        & 0.31        & 43.76          \\\\\n             & layer3       & 0.58        & 0.55        & 31.42          \\\\\n             & layer4       & 0.62        & 0.68        & 85.64          \\\\\n             & base         & 0.70        & 0.90        & 90.52       \\\\\n             \\hline\n             \\hline\n             \\multirow{2}{*}{$\\text{CLIP}_{\\text{ViT16}}$} & no-proj & 0.69 & 0.90 & 93.04 \\\\\n              & base & 0.72 & 0.99 & 94.76 \\\\\n            \\bottomrule\n        \\end{tabular}\n    \\end{threeparttable}\n    % \\vspace{-1.0em}\n\\end{table}\n    \\centering\n    \\caption{Comparison of experimental results on ResNet50 and $\\text{CLIP}_{\\text{ViT16}}$ using different middle layers.}\n    \\label{tab:exp_rn50_mid_layer}\n    \\vspace{-1.0em}\n    \n        \\sizefive\n        \\setlength{\\tabcolsep}{4pt}\n        \\renewcommand{\\arraystretch}{1.05}\n        \n            \\toprule\n            Victim model & Middle layer & BLEU-1 & CIDEr & Cosine Similarity (\\%) \\\\\n            \\midrule\n            \\multirow{5}5{*}*{ResNet50}ResNet50 & layer1       & 0.24        & 0.19        & ~0.00         \\\\\n             & layer2       & 0.51        & 0.31        & 43.76          \\\\\n             & layer3       & 0.58        & 0.55        & 31.42          \\\\\n             & layer4       & 0.62        & 0.68        & 85.64          \\\\\n             & base         & 0.70        & 0.90        & 90.52       \\\\\n             \\hline\n             \\hline\n             \\multirow{2}2{*}*{$\\text{CLIP}_{\\text{ViT16}}$}$\\text{CLIP}_{\\text{ViT16}}$\\text{CLIP}_{\\text{ViT16}}\\text{ViT16} & no-proj & 0.69 & 0.90 & 93.04 \\\\\n              & base & 0.72 & 0.99 & 94.76 \\\\\n            \\bottomrule\n        \n    \n    \n\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width = 0.85\\linewidth]{figs/plot_cosine_similarity_distribution_blip2-opt-2.7b_RN50_mean_all_leaked_feature_layers.pdf}\n    \\vspace{-0.8em}\n    \\caption{Embedding Cosine Similarity distributions for different middle layers of ResNet50. The results indicate that \\textsc{CapRecover} employs the deep layers and may perform better compared to the shallow layers.}\n    \\label{fig:exp_resnet50_cosine_similarity_middle_layers}\n    \\vspace{-0.5em}\n\\end{figure}\n    \\centering\n    \\includegraphics[width = 0.85\\linewidth]{figs/plot_cosine_similarity_distribution_blip2-opt-2.7b_RN50_mean_all_leaked_feature_layers.pdf}\n    \\vspace{-0.8em}\n    \\caption{Embedding Cosine Similarity distributions for different middle layers of ResNet50. The results indicate that \\textsc{CapRecover} employs the deep layers and may perform better compared to the shallow layers.}\n    \\label{fig:exp_resnet50_cosine_similarity_middle_layers}\n    \\vspace{-0.5em}\n\n\n\n\\subsection{Overall Experimental Results}\n\\label{subsec:exp_res_img_classification}\n\n\n\nTable \\ref{tab:exp_overall_res_on_dsts} shows the consistently strong performance of \\textsc{CapRecover} on image label recovery tasks across two widely-used datasets (CIFAR-10 and TinyImageNet) and two victim models (ResNet50 and CLIP\\textsubscript{ViT32}ViT32). Specifically, \\textsc{CapRecover} achieves particularly high Top-1 and Top-5 accuracy scores on CIFAR-10 dataset. Notably, when attacking CLIP\\textsubscript{ViT32}ViT32, \\textsc{CapRecover} achieves a Top-1 accuracy of 92.71\\% and Top-5 accuracy of 99.82\\%, indicating near-perfect recovery of class labels.\n\nWhile testing the TinyImageNet dataset, due to its larger number of classes and higher semantic complexity, \\textsc{CapRecover} achieves lower accuracy scores compared to the results on CIFAR-10. However, even under this challenging setting, \\textsc{CapRecover} still achieves 72.62\\% Top-1 accuracy and 91.60\\% Top-5 accuracy when targeting CLIP\\textsubscript{ViT32}ViT32, demonstrating the model\u2019s strong generalization ability across both datasets and victim architectures. In comparison, attacks on ResNet50 yield lower performance across both datasets, suggesting that visual representations from CLIP-based encoders are more vulnerable to semantic leakage.\n\n\n\\begin{table}[t]\n    \\centering\n    \\caption{Experimental results of image label recovery on CIFAR-10 and TinyImageNet datasets.}\n    \\label{tab:exp_overall_res_on_dsts}\n    \\vspace{-1.0em}\n    \\begin{threeparttable}\n        \\sizefive\n        \\setlength{\\tabcolsep}{4pt}\n        \\renewcommand{\\arraystretch}{1.10}\n        \\begin{tabular}{cccc}\n            \\toprule\n            Victim model & datasets & Top-1 Accuracy (\\%) & Top-5 Accuracy (\\%) \\\\\n            \\midrule\n            \\multirow{2}{*}{ResNet50} & CIFAR-10       & 83.35   & 99.55    \\\\\n             & TinyImageNet       & 60.13       & 83.79  \\\\\n             \\hline\n             \\multirow{2}{*}{$\\text{CLIP}_{\\text{ViT32}}$} & CIFAR-10  & 92.71   & 99.82 \\\\\n              & TinyImageNet & 72.62 & 91.60  \\\\\n            \\bottomrule\n        \\end{tabular}\n    \\end{threeparttable}\n    \\vspace{-1.0em}\n\\end{table}\n    \\centering\n    \\caption{Experimental results of image label recovery on CIFAR-10 and TinyImageNet datasets.}\n    \\label{tab:exp_overall_res_on_dsts}\n    \\vspace{-1.0em}\n    \n        \\sizefive\n        \\setlength{\\tabcolsep}{4pt}\n        \\renewcommand{\\arraystretch}{1.10}\n        \n            \\toprule\n            Victim model & datasets & Top-1 Accuracy (\\%) & Top-5 Accuracy (\\%) \\\\\n            \\midrule\n            \\multirow{2}2{*}*{ResNet50}ResNet50 & CIFAR-10       & 83.35   & 99.55    \\\\\n             & TinyImageNet       & 60.13       & 83.79  \\\\\n             \\hline\n             \\multirow{2}2{*}*{$\\text{CLIP}_{\\text{ViT32}}$}$\\text{CLIP}_{\\text{ViT32}}$\\text{CLIP}_{\\text{ViT32}}\\text{ViT32} & CIFAR-10  & 92.71   & 99.82 \\\\\n              & TinyImageNet & 72.62 & 91.60  \\\\\n            \\bottomrule\n        \n    \n    \\vspace{-1.0em}\n\n\n\\begin{table}[t]\n    \\centering\n    \\caption{Results of image label recovery on CIFAR-10.}\n    \\label{tab:exp_img_classification}\n    \\vspace{-1.0em}\n    \\begin{threeparttable}\n        \\sizefive\n        \\setlength{\\tabcolsep}{4pt}\n        \\renewcommand{\\arraystretch}{1.05}\n        \\begin{tabular}{cccc}\n            \\toprule\n            Class & Precision & Recall & F1-Score  \\\\\n            \\midrule\n            Airplane    & 0.93   & 0.96  & 0.95 \\\\\n            Automobile  & 0.96   & 0.97  & 0.97 \\\\\n            Bird        & 0.91   & 0.90  & 0.91 \\\\\n            Cat         & 0.85   & 0.85  & 0.85 \\\\\n            Deer        & 0.91   & 0.93  & 0.92 \\\\\n            Dog         & 0.88   & 0.87  & 0.87 \\\\\n            Frog        & 0.92   & 0.94  & 0.93 \\\\\n            Horse       & 0.97   & 0.94  & 0.96 \\\\\n            Ship        & 0.96   & 0.96  & 0.96 \\\\\n            Truck       & 0.97   & 0.96  & 0.96 \\\\\n            \\bottomrule\n        \\end{tabular}\n    \\end{threeparttable}\n    \\vspace{-1.0em}\n\\end{table}\n    \\centering\n    \\caption{Results of image label recovery on CIFAR-10.}\n    \\label{tab:exp_img_classification}\n    \\vspace{-1.0em}\n    \n        \\sizefive\n        \\setlength{\\tabcolsep}{4pt}\n        \\renewcommand{\\arraystretch}{1.05}\n        \n            \\toprule\n            Class & Precision & Recall & F1-Score  \\\\\n            \\midrule\n            Airplane    & 0.93   & 0.96  & 0.95 \\\\\n            Automobile  & 0.96   & 0.97  & 0.97 \\\\\n            Bird        & 0.91   & 0.90  & 0.91 \\\\\n            Cat         & 0.85   & 0.85  & 0.85 \\\\\n            Deer        & 0.91   & 0.93  & 0.92 \\\\\n            Dog         & 0.88   & 0.87  & 0.87 \\\\\n            Frog        & 0.92   & 0.94  & 0.93 \\\\\n            Horse       & 0.97   & 0.94  & 0.96 \\\\\n            Ship        & 0.96   & 0.96  & 0.96 \\\\\n            Truck       & 0.97   & 0.96  & 0.96 \\\\\n            \\bottomrule\n        \n    \n    \\vspace{-1.0em}\n\n\n\\subsection{Further Analysis on CIFAR-10}\nGiven the large number of categories in the TinyImageNet dataset, which makes detailed per-class analysis less tractable, we focus our in-depth experimental analysis on the CIFAR-10 dataset. Based on the experimental results presented in Figure \\ref{fig:exp_img_classification_confusion_matrix} and Table \\ref{tab:exp_img_classification}, we observe that \\textsc{CapRecover} demonstrates strong performance in reconstructing the classification labels of objects from intermediate image features on the CIFAR10 test set. The confusion matrix (Figure \\ref{fig:exp_img_classification_confusion_matrix}) illustrates clear and distinct diagonal patterns, indicating that predictions generally align closely with true labels. Most object categories such as ``Automobile'', ``Ship'', and ``Truck'' achieve very high correct prediction counts, approaching nearly perfect reconstruction accuracy.\n\nFurther quantitative analysis in Table \\ref{tab:exp_img_classification} confirms these observations. \\textsc{CapRecover} achieves consistently high precision, recall, and F1-scores across all ten classes, with scores predominantly above 0.90. The ``Truck'' and ``Ship'' categories achieve particularly high scores (F1-scores of 0.96), demonstrating especially robust performance. Even categories with slightly lower performance, such as ``Cat'' (F1-score of 0.85), remain sufficiently accurate to confirm the model's effectiveness.\n\nOverall, these experimental results indicate that \\textsc{CapRecover} effectively reconstructs object classifications from intermediate features, highlighting its potential to successfully exploit feature leakage vulnerabilities in image recognition scenarios.\n\n\n\n\n", "appendix": false}, "Discussion on Potential Defense Mechanisms": {"content": "\n\\label{sec:protection_approach}\n\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width = 0.90\\linewidth]{figs/confusion_matrix_cifar10_vit32.png}\n    \\vspace{-0.8em}\n    \\caption{Confusion matrix of prediction results on CIFAR-10 test set. This matrix illustrates that \\textsc{CapRecover} can accurately reconstruct the image classes.}\n    \\label{fig:exp_img_classification_confusion_matrix}\n    \\vspace{-0.8em}\n\\end{figure}\n    \\centering\n    \\includegraphics[width = 0.90\\linewidth]{figs/confusion_matrix_cifar10_vit32.png}\n    \\vspace{-0.8em}\n    \\caption{Confusion matrix of prediction results on CIFAR-10 test set. This matrix illustrates that \\textsc{CapRecover} can accurately reconstruct the image classes.}\n    \\label{fig:exp_img_classification_confusion_matrix}\n    \\vspace{-0.8em}\n\n\n\\subsection{Noise-Based Feature Obfuscation}\n\n\n\n\n\n\\begin{table}[t]\n    \\centering\n    \\caption{Evaluation results of \\textsc{CapRecover} attacking ResNet50 with/without additional noise.}\n    \\label{tab:exp_add_noise}\n    \\vspace{-1.0em}\n    \\begin{threeparttable}\n        \\sizefive\n        \\setlength{\\tabcolsep}{7pt}\n        \\renewcommand{\\arraystretch}{1.2}\n        \\begin{tabular}{cccccc}\n            \\toprule\n            Dataset & Middle layer & layer1 & layer2 & layer3 & layer4 \\\\\n            \\midrule\n            \\multirow{2}{*}{COCO2017} & w/o noise & 0.24 & 0.51       & 0.58        & 0.62         \\\\\n            & w/ noise & 0.49 & 0.03       & 0.02        & 0.05    \\\\\n            \\bottomrule\n        \\end{tabular}\n    \\end{threeparttable}\n    \\vspace{-1.0em}\n\\end{table}\n    \\centering\n    \\caption{Evaluation results of \\textsc{CapRecover} attacking ResNet50 with/without additional noise.}\n    \\label{tab:exp_add_noise}\n    \\vspace{-1.0em}\n    \n        \\sizefive\n        \\setlength{\\tabcolsep}{7pt}\n        \\renewcommand{\\arraystretch}{1.2}\n        \n            \\toprule\n            Dataset & Middle layer & layer1 & layer2 & layer3 & layer4 \\\\\n            \\midrule\n            \\multirow{2}2{*}*{COCO2017}COCO2017 & w/o noise & 0.24 & 0.51       & 0.58        & 0.62         \\\\\n            & w/ noise & 0.49 & 0.03       & 0.02        & 0.05    \\\\\n            \\bottomrule\n        \n    \n    \\vspace{-1.0em}\n\n\nTo protect intermediate representations in split DNN deployments, we propose a lightweight noise-based defense mechanism that introduces random Gaussian noise into the intermediate features during inference. While this technique effectively degrades feature inversion attacks, its practical feasibility hinges on low communication and computation overhead, especially in edge\u2013cloud settings.\n\n\\textbf{Local-only noise handling.}  \nTo ensure deployment efficiency, our design ensures that both the injection and noise removal are performed entirely on the client-side (i.e., the edge device). Specifically, for any intermediate feature $F^{(i)}$F^{(i)}(i), the edge device generates a random noise vector $\\epsilon^{(i)} \\sim \\mathcal{N}(0, \\sigma^2)$\\epsilon^{(i)}(i) \\sim \\mathcal{N}(0, \\sigma^2), computes the obfuscated representation $\\tilde{F}^{(i)} = F^{(i)} + \\epsilon^{(i)}$\\tilde{F}^{(i)}(i) = F^{(i)}(i) + \\epsilon^{(i)}(i), and then removes the noise in the subsequent layer before transmitting the result to the cloud:\n\\[\nF^{(i+1)} = g(\\tilde{F}^{(i)} - \\epsilon^{(i)}) = g(F^{(i)}).\n\\]\nF^{(i+1)}(i+1) = g(\\tilde{F}^{(i)}(i) - \\epsilon^{(i)}(i)) = g(F^{(i)}(i)).\n\nThe noise is neither stored nor transmitted\u2014thus, incurring \\textit{no additional communication cost} and avoiding the need for synchronization with the cloud.\n\n\\textbf{Negligible computational overhead.}  \nThe only extra computation required is sampling from a standard Gaussian distribution and applying addition/subtraction operations\u2014both of which are lightweight and can be efficiently executed on modern edge hardware (e.g., CPUs or NPUs). In our measurements, the time overhead introduced per inference was negligible ($<1\\%$<1\\% relative increase), making this defense practical for real-time applications.\n\n\\textbf{Security benefit.}  \nBy ensuring that the intermediate features transmitted to the server are never raw (i.e., always processed), attackers who intercept these representations cannot reconstruct accurate semantic content without knowledge of the locally generated $\\epsilon^{(i)}$\\epsilon^{(i)}(i). Moreover, since the noise is regenerated for each image, even partial leaks from one instance do not compromise others.\n\nOverall, his defense achieves a strong trade-off between privacy protection and deployment practicality. It requires no retraining, with no changes to final predictions. It can be readily integrated into edge-side inference pipelines with minimal modification.\n\n\\subsection{Potential for Homomorphic Encryption}\n\n\nHomomorphic Encryption (HE) \\cite{wiki_def_he} represents a promising cryptographic approach to mitigating privacy risks associated with Vision-Language Models (VLMs). In typical VLM deployments, intermediate image features generated by the visual encoder are frequently transmitted between client devices and remote servers, creating opportunities for attackers to intercept and exploit these representations to reconstruct sensitive textual information, such as image captions. By encrypting these intermediate features homomorphically, HE enables operations to be conducted directly on encrypted data without revealing the underlying plaintext features, thereby significantly reducing the risk of privacy leakage.\n\nThe primary advantage of employing HE in VLM scenarios lies in its capability to ensure data confidentiality even when intermediate features are intercepted during transmission or while temporarily stored. Attackers accessing encrypted features would find it computationally infeasible to derive meaningful information without the appropriate decryption keys, effectively safeguarding sensitive textual descriptions embedded within the features.\n\nThere are practical precedents demonstrating the feasibility of HE in protecting model parameters and gradients within federated learning scenarios. For example, NVIDIA researchers successfully integrated homomorphic encryption with XGBoost \\cite{xu2025secure}, employing CUDA acceleration to achieve efficient privacy-preserving federated learning. Such cases provide encouraging evidence that similar strategies could protect privacy from intermediate features, thus securing textual information from feature inversion attacks.\n\n\n", "appendix": false}, "Conclusion": {"content": "\n\\label{sec:conclusion}\n\n\n\n\\par\\noindent In this paper, we focus on the cross-modality feature inversion attack, proposing \\textsc{CapRecover}, a generic framework that reconstructs image captions and classification labels directly from leaked intermediate image features. By leveraging a feature projection module and a feature-to-text alignment mechanism, \\textsc{CapRecover} effectively recovers semantic information\u2014even when using features from the final linear projection layer of the visual encoder. Our extensive experiments demonstrate \\textsc{CapRecover}'s effectiveness across multiple datasets and models. Furthermore, we propose an effective protection approach without additional training costs, thereby efficiently preventing attackers from reconstructing sensitive image information. \n\n\n\n\n\n\n\\bibliographystyle{ACM-Reference-Format}ACM-Reference-Format\n\\balance\n\\bibliography{ref}\n\n\n\n\n\n\n\n", "appendix": false}}, "categories": ["cs.CV", "cs.AI"], "published": "2025-07-30 16:42:02+00:00", "primary_category": "cs.CV", "summary": "As Vision-Language Models (VLMs) are increasingly deployed in split-DNN\nconfigurations--with visual encoders (e.g., ResNet, ViT) operating on user\ndevices and sending intermediate features to the cloud--there is a growing\nprivacy risk from semantic information leakage. Existing approaches to\nreconstructing images from these intermediate features often result in blurry,\nsemantically ambiguous images. To directly address semantic leakage, we propose\nCapRecover, a cross-modality inversion framework that recovers high-level\nsemantic content, such as labels or captions, directly from intermediate\nfeatures without image reconstruction.\n  We evaluate CapRecover on multiple datasets and victim models, demonstrating\nstrong performance in semantic recovery. Specifically, CapRecover achieves up\nto 92.71% Top-1 label accuracy on CIFAR-10 and generates fluent captions from\nResNet50 features on COCO2017 with ROUGE-L scores up to 0.52. Our analysis\nfurther reveals that deeper convolutional layers encode significantly more\nsemantic information compared to shallow layers. To mitigate semantic leakage,\nwe introduce a simple yet effective protection method: adding random noise to\nintermediate features at each layer and removing the noise in the next layer.\nExperimental results show that this approach prevents semantic leakage without\nadditional training costs."}