{"title": "Artificial Intelligence in Creative Industries: Advances Prior to 2025", "author": "Nantheera Anantrasirichai", "abstract": "\\begin{abstract}\n   The rapid advancements in artificial intelligence (AI), particularly in generative AI and large language models (LLMs), have profoundly impacted the creative industries, enabling more innovative content creation, enhancing workflows, and democratizing access to creative tools. This paper explores these  technological shifts, with particular focus on how those that have emerged since our previous review in 2022 have expanded creative opportunities and improved efficiency. These technological advancements have enhanced the capabilities of text-to-image, text-to-video, and multimodal generation technologies. In particular, key breakthroughs in LLMs have established new benchmarks in conversational AI, while advancements in image generators have revolutionized content creation. We also discuss the integration of AI into post-production workflows, which has significantly accelerated and improved traditional processes. Once content has been created, it must be delivered to its audiences  the media industry is facing the demands of increased communication traffic due to creative content.  We therefore include a discussion of how AI is beginning to transform the way we represent and compress media content. We highlight the trend toward unified AI frameworks capable of addressing and integrating multiple creative tasks, and we underscore the importance of human insight to drive the creative process and oversight to mitigate AI-generated inaccuracies. Finally, we explore AI's future potential in the creative sector, stressing the need to navigate emerging challenges and to maximize its benefits while addressing the associated risks.\n\n\\end{abstract}", "citations": {}, "refs": [], "table": [{"original": "\\begin{table}\n\\caption{Creative applications and corresponding AI-based methods mentioned in this paper}\n%\\tiny\n %\\hskip-5.0cm\n %\\begin{tabular}{p{1cm}p{1.4cm}|p{4cm}p{4cm}p{4cm}p{4cm}}\n \\resizebox{\\linewidth}{!}{\\begin{tabular}{lr|lll}\n \\\\\n \\toprule\n\\multicolumn{2}{c}{\\multirow{2}{*}{Application}} & \\multicolumn{3}{|c}{Technology} \\\\ \\cmidrule{3-5}\n& & Trans./Attn.$^1$ & Diffusion model$^2$  & INR\\\\\n\\midrule\n{\\bf Creation} & Text & \\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}  & \\\\\n& audio/music & \\cite{Alayrac:Flamingo:2022, Huang:GenerSpeech:2022} & \\cite{Li:Diffusion:2022, Yang:Diffsound:2023, Evans:stable:2025} & \\\\\n& Image &  \\cite{Alayrac:Flamingo:2022,esser:scaling:2024} & \\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024, Feng_Ma_2025, Liu_Ma_2025} & \\\\\n% ---------------------------------------------------------------\n& Animation/video & \\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024} & \\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024, wang2025lavie, wu2025customcrafter} &\\\\\n% ---------------------------------------------------------------\n & 3D/AR/VR & \\cite{yang:Holodeck:2024} & \\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024} & \\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear} \\\\\n% ---------------------------------------------------------------\n\\midrule\n{\\bf Information} & Text categorization & \\cite{sun:text:2023, shi:chatgraph:2023, Hou:promptboosting:2023, AI2025125952} \\\\\n% ---------------------------------------------------------------\n{\\bf Analysis} & Film analysis & \\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}  \\\\\n % ---------------------------------------------------------------\n & Content retrieval & \\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning} & \\cite{Jin:DiffusionRet:2023} \\\\\n % ---------------------------------------------------------------\n & Intelligent assistants & \\cite{King:Sasha:2024} \\\\\n % ---------------------------------------------------------------\n \\midrule\n{\\bf Content} & Enhancement  & \\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024} & \\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight} & \\cite{Yang:Implicit:2023} \\\\\n{\\bf  Enhancement  } & Style transfer & \\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR} & \\cite{Zhang:Inversion:2023,Chai:StableVideo:2023} & \\cite{Moon:generalizable:2023,Kim:Controllable:2024} \\\\\n% ---------------------------------------------------------------\n {\\bf  and Post} & Super-resolution & \\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024,wang2025seedvr} & \\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023, cao2025zero, wang2025seedvr} & \\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}\\\\\n% ---------------------------------------------------------------\n{\\bf Production} & {Restoration} & \\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025} & \\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023, cao2025zero, feng2025residual} & \\cite{Jiang:NeRT:2023} \\\\\n  % ---------------------------------------------------------------\n & Inpainting & \\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024} & \\cite{Moliner:solving:2023, Fei:Generative:2023} & \\\\\n % ---------------------------------------------------------------\n& Fusion & \\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023, LI2024102147} & \\cite{Zhao:DDFM:2023} & \\\\\n & Editing/VFX & \\cite{Shi:Motion-I2V:2024} & \\cite{Shi:Motion-I2V:2024,guo2024liveportrait} \\\\\n \\midrule\n % ---------------------------------------------------------------\n{\\bf Information} & Segmentation & \\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2,Zhang:DVISp:2025} & \\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024} & \\cite{Gong:Continuous:2023, Cen:Segment:2023} \\\\\n % ---------------------------------------------------------------\n{\\bf  Extraction} & Recognition &  \\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12} & \\cite{Li:Your:2023,Chen:DiffusionDet:2023, Zhang:DiffAD:2025, WU2025102965}\\\\\n % ---------------------------------------------------------------\n{\\bf  and}  & Tracking &  \\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring} & \\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024} & \\cite{Jung:AnyFlow:2023}\\\\\n % ---------------------------------------------------------------\n{\\bf  Understanding}  & 3D Reconstruction  & \\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024, LIU:DSEM:2025} & \\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024} & \\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}$^\\dag$  \\\\\n\\midrule\n\n% ---------------------------------------------------------------\n{\\bf Compression} & Image$^\\ast$ & \\cite{zhu2022transformer,zou2022devil,liu2023learned}& \\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual} & \\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}\\\\\n& Video& \\cite{xiang2022mimt,mentzer2022vct}& \\cite{li2024extreme}&\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}\\\\\n&Audio$^\\ast$ & & &\\\\\n\\midrule\n% ---------------------------------------------------------------\n{\\bf Quailty}  &  Image$^\\ast$ & \\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}& &\\\\\n\\textbf{Assessment}& Video$^\\ast$ & \\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}& & \\\\\n\\bottomrule\n\\multicolumn{5}{l}{$^1$ Trans./Attn. include transformers, mamba and CNN-based architectures that use attention module.} \\\\\n\\multicolumn{5}{l}{$^2$ Some diffusion models employ the transformer in their denoising autoencoders.}\n \\end{tabular}}\n \\footnotesize\n $^\\dag$ \nThese methods are based on explicit neural representations. \\\\\n$^*$ It is noted that for some compression and quality assessment tasks, there are other dominant network architectures in existing works. For example, LLMs have been used for image and audio compression, and visual quality assessment. Many neural audio codecs are also based on VQ-VAE models.\n \n\\label{tab:gather}\n\\end{table}", "caption": "\\caption{Creative applications and corresponding AI-based methods mentioned in this paper}", "label": "\\label{tab:gather}", "tabular": "\\begin{tabular}{lr|lll}\n \\\\\n \\toprule\n\\multicolumn{2}{c}{\\multirow{2}{*}{Application}} & \\multicolumn{3}{|c}{Technology} \\\\ \\cmidrule{3-5}\n& & Trans./Attn.$^1$ & Diffusion model$^2$  & INR\\\\\n\\midrule\n{\\bf Creation} & Text & \\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}  & \\\\\n& audio/music & \\cite{Alayrac:Flamingo:2022, Huang:GenerSpeech:2022} & \\cite{Li:Diffusion:2022, Yang:Diffsound:2023, Evans:stable:2025} & \\\\\n& Image &  \\cite{Alayrac:Flamingo:2022,esser:scaling:2024} & \\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024, Feng_Ma_2025, Liu_Ma_2025} & \\\\\n% ---------------------------------------------------------------\n& Animation/video & \\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024} & \\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024, wang2025lavie, wu2025customcrafter} &\\\\\n% ---------------------------------------------------------------\n & 3D/AR/VR & \\cite{yang:Holodeck:2024} & \\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024} & \\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear} \\\\\n% ---------------------------------------------------------------\n\\midrule\n{\\bf Information} & Text categorization & \\cite{sun:text:2023, shi:chatgraph:2023, Hou:promptboosting:2023, AI2025125952} \\\\\n% ---------------------------------------------------------------\n{\\bf Analysis} & Film analysis & \\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}  \\\\\n % ---------------------------------------------------------------\n & Content retrieval & \\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning} & \\cite{Jin:DiffusionRet:2023} \\\\\n % ---------------------------------------------------------------\n & Intelligent assistants & \\cite{King:Sasha:2024} \\\\\n % ---------------------------------------------------------------\n \\midrule\n{\\bf Content} & Enhancement  & \\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024} & \\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight} & \\cite{Yang:Implicit:2023} \\\\\n{\\bf  Enhancement  } & Style transfer & \\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR} & \\cite{Zhang:Inversion:2023,Chai:StableVideo:2023} & \\cite{Moon:generalizable:2023,Kim:Controllable:2024} \\\\\n% ---------------------------------------------------------------\n {\\bf  and Post} & Super-resolution & \\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024,wang2025seedvr} & \\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023, cao2025zero, wang2025seedvr} & \\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}\\\\\n% ---------------------------------------------------------------\n{\\bf Production} & {Restoration} & \\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025} & \\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023, cao2025zero, feng2025residual} & \\cite{Jiang:NeRT:2023} \\\\\n  % ---------------------------------------------------------------\n & Inpainting & \\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024} & \\cite{Moliner:solving:2023, Fei:Generative:2023} & \\\\\n % ---------------------------------------------------------------\n& Fusion & \\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023, LI2024102147} & \\cite{Zhao:DDFM:2023} & \\\\\n & Editing/VFX & \\cite{Shi:Motion-I2V:2024} & \\cite{Shi:Motion-I2V:2024,guo2024liveportrait} \\\\\n \\midrule\n % ---------------------------------------------------------------\n{\\bf Information} & Segmentation & \\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2,Zhang:DVISp:2025} & \\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024} & \\cite{Gong:Continuous:2023, Cen:Segment:2023} \\\\\n % ---------------------------------------------------------------\n{\\bf  Extraction} & Recognition &  \\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12} & \\cite{Li:Your:2023,Chen:DiffusionDet:2023, Zhang:DiffAD:2025, WU2025102965}\\\\\n % ---------------------------------------------------------------\n{\\bf  and}  & Tracking &  \\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring} & \\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024} & \\cite{Jung:AnyFlow:2023}\\\\\n % ---------------------------------------------------------------\n{\\bf  Understanding}  & 3D Reconstruction  & \\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024, LIU:DSEM:2025} & \\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024} & \\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}$^\\dag$  \\\\\n\\midrule\n\n% ---------------------------------------------------------------\n{\\bf Compression} & Image$^\\ast$ & \\cite{zhu2022transformer,zou2022devil,liu2023learned}& \\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual} & \\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}\\\\\n& Video& \\cite{xiang2022mimt,mentzer2022vct}& \\cite{li2024extreme}&\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}\\\\\n&Audio$^\\ast$ & & &\\\\\n\\midrule\n% ---------------------------------------------------------------\n{\\bf Quailty}  &  Image$^\\ast$ & \\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}& &\\\\\n\\textbf{Assessment}& Video$^\\ast$ & \\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}& & \\\\\n\\bottomrule\n\\multicolumn{5}{l}{$^1$ Trans./Attn. include transformers, mamba and CNN-based architectures that use attention module.} \\\\\n\\multicolumn{5}{l}{$^2$ Some diffusion models employ the transformer in their denoising autoencoders.}\n \\end{tabular}", "subtables": []}], "figure": [{"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/generativemodel.png}\n    \\caption{Generative AI. (a) Transformer architecture \\cite{Vaswani:attention:2017}. (b) The top row represents the diffusion process and the bottom row represents the generation process of the new image \\cite{Yang:diffusion:2023}. (c) Latent Diffusion Models (LDM) \\cite{Rombach:LDM:2022}. }\n    \\label{fig:generativemodel}\n\\end{figure}", "caption": "\\caption{Generative AI. (a) Transformer architecture \\cite{Vaswani:attention:2017}. (b) The top row represents the diffusion process and the bottom row represents the generation process of the new image \\cite{Yang:diffusion:2023}. (c) Latent Diffusion Models (LDM) \\cite{Rombach:LDM:2022}. }", "label": "\\label{fig:generativemodel}", "subfigures": [], "figure_paths": ["figures/generativemodel.png"]}, {"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/FLASK_LLM_and_history.png}\n    \\caption{(a) Timeline of large language models. (b) Performance comparison evaluated by FLASK \\cite{Ye:FLASK:2024}.}\n    \\label{fig:FLASK_LLM_and_history}\n\\end{figure}", "caption": "\\caption{(a) Timeline of large language models. (b) Performance comparison evaluated by FLASK \\cite{Ye:FLASK:2024}.}", "label": "\\label{fig:FLASK_LLM_and_history}", "subfigures": [], "figure_paths": ["figures/FLASK_LLM_and_history.png"]}, {"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/LLMGround.jpg}\n    \\caption{Text-to-image generation (generated on 27 November 2024). (a) text-to-image generation by Ideogram v1, DALL\u00b7E 3, Photoshop 2025, and sdxy-turbo by Nvidia. (b) The top-row images were generated by DALL\u00b7E in ChatGPT 4. The bottom-row images are generated by LLM-grounded Diffusion \\cite{Lian:LLMG:2024}. }\n    \\label{fig:LLMGround}\n\\end{figure}", "caption": "\\caption{Text-to-image generation (generated on 27 November 2024). (a) text-to-image generation by Ideogram v1, DALL\u00b7E 3, Photoshop 2025, and sdxy-turbo by Nvidia. (b) The top-row images were generated by DALL\u00b7E in ChatGPT 4. The bottom-row images are generated by LLM-grounded Diffusion \\cite{Lian:LLMG:2024}. }", "label": "\\label{fig:LLMGround}", "subfigures": [], "figure_paths": ["figures/LLMGround.jpg"]}, {"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/Deepmotion_Vasa.png}\n    \\caption{(Left) Video-to-3D post animation by DeepMotion. (Right) Image and audio to video by VASA-1 \\cite{xu:VASA-1:2024}}\n    \\label{fig:Deepmotion_Vasa}\n\\end{figure}", "caption": "\\caption{(Left) Video-to-3D post animation by DeepMotion. (Right) Image and audio to video by VASA-1 \\cite{xu:VASA-1:2024}}", "label": "\\label{fig:Deepmotion_Vasa}", "subfigures": [], "figure_paths": ["figures/Deepmotion_Vasa.png"]}, {"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/SR_results.png}\n    \\caption{(Left) Examples of SR ($\\times$4) using generative model. (Right) Real-time portrait editing with FacePoke.} \n    \\label{fig:SR}\n\\end{figure}", "caption": "\\caption{(Left) Examples of SR ($\\times$4) using generative model. (Right) Real-time portrait editing with FacePoke.}", "label": "\\label{fig:SR}", "subfigures": [], "figure_paths": ["figures/SR_results.png"]}, {"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation.png}\n    \\caption{(a) Tasks in Object-centric understanding defined by Yan et al. \\cite{Yan:Universal:2023} (REC:,  Referring Expression Comprehension, RES: Referring Expression Segmentation, VOS:  Video Object Segmentation, RVOS: Referring Video Object Segmentation, MOT: Multiple Object Tracking, MOTS: Multi-Object Tracking and Segmentation, VIS: Video Instance Segmentation, SOT: Single Object Tracking. (b) Current high-quality segmentation \\cite{Ke:SAM-HQ:2023}.}\n    \\label{fig:segmentation}\n\\end{figure}", "caption": "\\caption{(a) Tasks in Object-centric understanding defined by Yan et al. \\cite{Yan:Universal:2023} (REC:,  Referring Expression Comprehension, RES: Referring Expression Segmentation, VOS:  Video Object Segmentation, RVOS: Referring Video Object Segmentation, MOT: Multiple Object Tracking, MOTS: Multi-Object Tracking and Segmentation, VIS: Video Instance Segmentation, SOT: Single Object Tracking. (b) Current high-quality segmentation \\cite{Ke:SAM-HQ:2023}.}", "label": "\\label{fig:segmentation}", "subfigures": [], "figure_paths": ["figures/segmentation.png"]}, {"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/3Drepresentation.png}\n    \\caption{3D representation. (a) Neural Radiance Fields (NeRFs) \\cite{Mildenhall:NeRF:2020}. (b) Gaussian Splatting \\cite{kerbl:3Dgaussians:2023}. (c) Example scenes of VR-GS system for 3D content interaction in VR \\cite{jiang:vrgs:2024}. }\n    \\label{fig:3Drepresentation}\n\\end{figure}", "caption": "\\caption{3D representation. (a) Neural Radiance Fields (NeRFs) \\cite{Mildenhall:NeRF:2020}. (b) Gaussian Splatting \\cite{kerbl:3Dgaussians:2023}. (c) Example scenes of VR-GS system for 3D content interaction in VR \\cite{jiang:vrgs:2024}. }", "label": "\\label{fig:3Drepresentation}", "subfigures": [], "figure_paths": ["figures/3Drepresentation.png"]}], "equations": ["\\begin{equation}\n    \\text{Attention}(Q, K, V ) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) V,\n    \\label{eqn:attention}\n\\end{equation}", "\\begin{equation}\n\\begin{split}\n        \\text{MultiHead}(Q, K, V ) &= \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O,\\\\\n        \\text{head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V ). \n\\end{split}\n\\label{eqn:MultiHead}\n\\end{equation}", "\\begin{equation}\nF(x, \\Phi, \\nabla_x \\Phi, \\nabla_x^2 \\Phi, \\dots) = 0, \\quad \\Phi : x \\mapsto \\Phi(x).\n\\label{eqn:inr}\n\\end{equation}"], "algorithm": [], "sections": {"Acknowledgements": {"content": "\n\nThis work has been funded by the UKRI MyWorld Strength in Places Programme (SIPF00006/1).\n\n\n\\newpage\n\n\\tableofcontents\n\n\\newpage\n\n", "appendix": false}, "Introduction": {"content": "\n\nThe influence of artificial intelligence (AI) has grown dramatically over the past few years, particularly due to the rise of generative AI and large language models (LLMs). These advancements are widely regarded as beneficial by many countries, creating significant opportunities for growth (e.g. as outlined in the UK, by the Authority of the House of Lords \\cite{UK:Large:2024}). These advances have also had significant direct and indirect impacts on the creative industries, influencing the direction of their growth. Generative AI, for instance, primarily focuses on generating new data that is not identical to the training data yet shares similarities with it. However, the cardinality of the training data can be huge,  larger than what any individual human has ever encountered. The resulting output may therefore act as a new source of insipration. \n\nAI tools also provide opportunities for a wider range of users to work more efficiently and  effectively,  with even greater creativity. Moreover, these new technologies not only influence creators,  but they also enable new ways for audiences to experience art and culture \\cite{Jeary2024}.\n\nA major breakthrough in generative AI has been led by OpenAI\\footnote{\\url{https://openai.com/}}, an AI research and deployment company, with their introduction of Generative Pre-trained Transformer (GPT) models for LLMs. LLMs are specifically designed to understand and generate human language. They are characterized by their vast size in terms of parameters and the amount of training data used to create them. This breakthrough was particularly impactful when the company released ChatGPT in 2022, which was fine-tuned from a model in the GPT-3.5 series. ChatGPT is a conversational model that includes advanced safety features that mitigate the generation of inappropriate content. Several other LLM platforms were also developed contemporaneously, such as LaMDA and PaLM by Google AI, Ernie Bot by Baidu, and BLOOM by BigScience. Additionally, Anthropic launched Claude, the LLM trained specifically to be harmless and honest, leveraging reinforcement learning from human feedback (RLHF) - a technique used to train AI systems to appear more human \\cite{Bai:Train:2021}. Nonetheless, ChatGPT stands out as the most renowned, thanks to its quick and efficient responses, and notably its public accessibility, being available for free. \n\nAnother breakthrough in 2022 was in the area of text-to-image models. OpenAI achieved a significant milestone with DALL\u00b7E 2, producing impressive artworks and photorealistic images despite its limited language understanding. Midjourney by Midjourney, Inc., another well-known text-to-image generator, supports higher resolution images, up to 4096$\\times$\\times4096 pixels. Stable Diffusion by Stability AI, for which the code and model weights are publicly available\\footnote{\\url{https://github.com/Stability-AI/stablediffusion}}, allows developers and artists to further adapt AI to suit their own specific applications.\n\nThe next breakthrough happened in 2023 when OpenAI unveiled GPT-4, a significantly larger model with estimated 1.8 trillion  parameters and improved performance compared to its predecessors \\cite{openai:gpt4:2023}. However, this still represents less than 1\\% of the human brain\u2019s approximately 600 trillion synaptic connections\\footnote{\\url{https://www.rsb.org.uk/biologist-features/ai-versus-the-brain}}. GPT-4 is a multimodal large language model that can generate responses to both text and images. It incorporates DALL\u00b7E 3, enabling it to comprehend a much broader range of nuances and details than earlier versions. In March 2024, Claude 3 Opus by Anthropic was released, boasting multimodal capabilities in generating images, tables, graphs and diagrams. Moreover, Anthropic claims that Claude 3 Opus outperforms GPT-4 in generating human-like dialog and contextually aware responses. These rapid advances have, in turn,  led the creative industries to face significant challenges. For example, DMG Media, the Financial Times, and Guardian Media Group have highlighted concerns about the potential impact on print journalism, particularly if AI tools reduce the need for users to click through to news websites, affecting advertising and subscription revenues \\cite{UK:Large:2024}. There is also concern about `AI-generated slop'\u2014low-quality, mass-produced content created by AI that often lacks coherence or originality\\footnote{\\url{https://reutersinstitute.politics.ox.ac.uk/news/ai-generated-slop-quietly-conquering-internet-it-threat-journalism-or-problem-will-fix-itself}}. It is typically used for spam, search engines, or clickbait, and is criticized for cluttering the internet and undermining genuine human-created content.\n\nThe generation of videos is significantly more challenging for AI than generating images. In February 2024, Google announced Gemini 1.5 which had the capability to process approximately 8 times more data than GPT-4, opening opportunities for  video  and audio processing \\footnote{\\url{https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/}}. In the same month, OpenAI provided its first preview of Sora,  a model capable of generating impressive realistic videos up to 1 minute long. Based on the videos released by OpenAI, Sora appears to outperform other text-to-video models. Sora is currently available to ChatGPT subscribers.  A month later, Gemini 1.5 announced its support for native audio understanding in 180$+$+ countries.  With the emergence of these tools, together with the prospect of further advances, it is clear that video content creation will be a major beneficiary. This will further open up the media landscape for creativity and provide more opportunities for diverse storytellers, while also reducing production time. A recent example is the AI-generated Christmas commercial by Coca-Cola\\footnote{\\url{https://www.youtube.com/watch?v=4RSTupbfGog}}. Such advertisements overcome the limitations of current technologies by using very short videos with rapid scene transitions, ensuring that any artifacts, such as unnatural fingers, are less apparent.\n\nFor the case of post-production workflows, generative AI may not have a direct impact, but the neural networks originally proposed for generative AI have been widely adapted to serve this purpose. This has led to significant improvements in both output quality and computational speed. Moreover, there is  a noticeable trend  towards adopting a unified framework rather than addressing individual tasks, as it better reflects real-world scenarios. For instance, natural history filmmaking involves challenging acquisition environments and high production standards. Filming often takes place in low light conditions, in the presence of heat haze, underwater or in adverse weather conditions. This often results in increased noise levels, focus issues, low contrast, color balance problems, and blurriness in the footage. In such cases, Unified models can offer advantages in generalizing to diverse tasks and providing flexibility. Take Painter by BAAI Vision \\cite{Wang:Painter:2023} as an example, which employs an image pair as a task prompt (similar to a text prompt in LLMs), their model transfers the input image to produce a similar output as the task prompt, enabling it to undertake various tasks such as segmentation, low-light enhancement or rain removal.\n\nWhile generative AI can facilitate and accelerate the creation and post-processing of digital media, there is an equivalent need to transmit or stream it efficiently to users. Although AI-based solutions have been proposed both for the enhancement of conventional video coding tools and for new compression frameworks, they are yet to be  adopted in practical applications due to hardware constraints, complexity issues and a lack of standardization. Despite this,  the latest learning-based video codecs have already demonstrated their potential to compete with conventional standardized video codecs and are being actively investigated in various standards bodies such as MPEG and AOM.\n\nFurthermore, in recent years,  AI has also impacted our ability to assess and monitor the perceptual quality of visual media. Advances have included new model architectures based on different attention mechanisms and the application of LLMs, which evidently improve model generalization. New training methodologies have also been proposed based on weakly/unsupervised learning, which address issues associated with the limited availability of labeled training content.\n\n\nOne of the exciting aspects of using LLMs in the creative sector is that `The human in the loop' \\cite{chung:human-loop:2021} is simplified through text prompts, with sophisticated, multilingual language capabilities enabling artists to convey complex emotions and narratives. This is important because generative AI does produce mistakes, known as hallucinations. Human oversight is thus essential to correct this through reinforcement learning with feedback \\cite{Wu:brief:2023}.\n\n\n\nIn this paper, the objective is to reveal to the reader, the latest technology advancements that have emerged since our previous review paper on AI in the creative industries (published in 2022)  \\cite{Anantrasirichai:AI:2022}. Compared to this earlier paper, which was written when most AI technologies were used as support tools, this updated review describes thesignificant disruptive shifts that have emerged over the past 3-4 years driven by generative AI and other  recent AI-based technologies.  Similar to \\cite{Anantrasirichai:AI:2022}, we first provide a high-level overview of current advanced AI technologies (Section \\ref{sec:overview}), followed by a selection of key creative domain applications (Section \\ref{sec:existing}) where current AI technologies are changing creative practice. Finally, we discuss the  challenges and the future potential of AI associated with the creative industries (Section \\ref{sec:discussion}).\n\n\n", "appendix": false}, "Current Advanced AI Technologies": {"content": "\n\\label{sec:overview}\n\nThis paper provides a review of AI in the creative industries,  building on our previous publication in 2022 \\cite{Anantrasirichai:AI:2022}. The reader is referred to that work for an introduction to AI, basic neurons, convolutional neural networks (CNNs), generative adversarial networks (GANs), recurrent neural networks (RNNs) and deep reinforcement learning (DRL). In this paper,  we emphasize four key technologies that have grown in importance since 2022 that have had a significant impact on the creative industries. These are Transformers, Large language models (LLMs), Diffusion Models (DMs), and Implicit Neural Representations (INRs). It is important to note that, while these newer technologies are gaining prominence, those from previous generations remain in widespread use, often in conjunction with the newer ones. For instance, CNNs complement transformers since CNNs effectively capture local features and semantic meaning, while the attention mechanism in transformers capture global dependencies.\n\nOne important class within AI that has become dominant since our previous review comprises Foundation models (FMs). These were described by The Stanford Institute for Human-Centered Artificial Intelligence in 2021 \\cite{Bommasani2021FoundationModels} as ``any model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks\". Foundation models have been enabled by rapid advances in AI-oriented computing power and have been underpinned the emergence and success of Large Language Models, particularly following the launch of ChatGPT by OpenAI in  2022. ChatGPT has become the fastest-growing consumer software application in history\\footnote{\\url{https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/}}.\n\nThese technologies are expanded on below.\n\n\n\n\\subsection{Transformers}\n\\label{ssec:transformers}\n\nIn 2017, Google AI introduced the concept of  `Transformer' architectures in their publication `Attention Is All You Need' \\cite{Vaswani:attention:2017}. This work has since, been instrumental in the development and success  of large language models alongside many other applications, including vision understanding \\cite{Dosovitskiy:image:2021}, and multiple modality learning (e.g., Gato \\cite{Reed:Generalist:2022}). \n\nBefore the advent of transformers, natural language processing (NLP) was performed using recurrent neural networks (RNNs), processing data sequences sequentially. In contrast, the ability of transformers to capture long-range dependencies through self-attention mechanisms that extend across all words in the sequence, meant that the importance of different words could be established globally,  understanding relationships regardless of their positions. This context-aware representation enables parallel processing of the entire sequence, making the transformers computationally efficient. A set of several attention layers running in parallel is called Multi-Head Attention. \n\nThe Transformer architecture, shown in Fig. \\ref{fig:generativemodel} (a), comprises Encoder and Decoder sections, similar to many CNN-based generators. However the encoder is now a stack of identical layers, concatenating a multi-head self-attention mechanism and a fully connected feed-forward network. The decoder is also a stack of identical layers, in which each layer has additional sub-layer to perform multi-head attention over the output of the encoder stack.\n\nMathematically, the attention function is computed from inputs: query $Q$Q, keys $K$K, and values $V$V. The matrix of outputs of  attention function is\n\\begin{equation}\n    \\text{Attention}(Q, K, V ) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) V,\n    \\label{eqn:attention}\n\\end{equation}\\begin{equation}\n    \\text{Attention}(Q, K, V ) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) V,\n    \\label{eqn:attention}\n\\end{equation}\n    \\text{Attention}(Q, K, V ) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) V,\n    \\label{eqn:attention}\n\n\\noindent where $d_k$d_k is a  dimension of $K$K. The term ${QK^T}${QK^T}QK^T is Dot-Product Attention, which yields a high similarity value when the two words are closely related.  If $Q$Q and $K$K are from the same sentence, Eq.~\\ref{eqn:attention} refers to self-attention, but if $Q$Q and $K$K are from different sentences, it is referred to as cross-attention. Within the network, multi-head attention is actually employed to concurrently process attention and enable the model to collectively focus on information from distinct representation subspaces at various positions through the learnable parameters $W$Ws.\n\\begin{equation}\n\\begin{split}\n        \\text{MultiHead}(Q, K, V ) &= \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O,\\\\\n        \\text{head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V ). \n\\end{split}\n\\label{eqn:MultiHead}\n\\end{equation}\\begin{equation}\n\\begin{split}\n        \\text{MultiHead}(Q, K, V ) &= \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O,\\\\\n        \\text{head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V ). \n\\end{split}\n\\label{eqn:MultiHead}\n\\end{equation}\n\n        \\text{MultiHead}(Q, K, V ) &= \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O,\\\\\n        \\text{head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V ). \n\n\\label{eqn:MultiHead}\n\nIt should be noted that attention modules are not solely used in transformers, but have also been successfully integrated into other deep learning architectures such as CNNs, used for image classification \\cite{Li:HAM:2022}, object detection \\cite{Woo:CBAM:2018}, and other computer vision tasks \\cite{Guo:Attention:2022}.\n\nIn 2020, the first successful training of a transformer encoder for image recognition was published \\cite{Dosovitskiy:image:2021}, reeferred to as a Vision Transformer (ViT). The ViT decomposes an input image into patches, similar to words in a sentence, and processes them through multi-head attention. Additionally, a Multilayer Perceptron (MLP) is employed as the feedforward network.  In later work Microsoft introduced  a hierarchical division of image inputs and a shifted window approach in their Swin Transformer \\cite{Liu:Swin:2021}. This was reported to outperform ViT by 2.4\\% in ImageNet-22K classification (21,841 different categories). Its version 2 \\cite{Liu:Swinv2:2022} applied a cosine function in the attention module. enabling the scaling of capacity and resolution. More detail on transformer-based object detection is discussed in Section \\ref{sssec:recog}. To date, Swin Transformers have been widely adopted in a range of applications including image restoration \\cite{Fan:SUNet:2022}. \n\n\n\nComprehensive surveys on the use of transformers for image and video processing can be found in \\cite{Khan:Transformers:2022} and \\cite{Selva:video:2023}, respectively.\n \nTransformers have been widely used and offer better performance across many tasks. One reason for this widespread adoption has been the availability of open-source Transformer libraries such as Hugging   Face\\footnote{\\url{https://huggingface.co/}}, a platform that assists developers to build applications for tasks including computer vision, NLP, audio, tabular data, multimodal tasks, and reinforcement learning. The platform also provides access to model zoo \\footnote{Such as \\url{https://modelzoo.co/} and \\url{https://pytorch.org/serve/model_zoo.html}} pretrained networks and datasets. \n\nIn recent years state space models \\cite{gu2023mamba, zhu2024vision}, commonly known as `Mamba' have emerged. These are  a linear variant of Transformersdistinguished by their linear complexity in attention modeling. They are acknowledged to offer an equivalent or better performance than traditional Transformers, while demanding fewer computational resources and less memory.\n\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/generativemodel.png}\n    \\caption{Generative AI. (a) Transformer architecture \\cite{Vaswani:attention:2017}. (b) The top row represents the diffusion process and the bottom row represents the generation process of the new image \\cite{Yang:diffusion:2023}. (c) Latent Diffusion Models (LDM) \\cite{Rombach:LDM:2022}. }\n    \\label{fig:generativemodel}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/generativemodel.png}\n    \\caption{Generative AI. (a) Transformer architecture \\cite{Vaswani:attention:2017}. (b) The top row represents the diffusion process and the bottom row represents the generation process of the new image \\cite{Yang:diffusion:2023}. (c) Latent Diffusion Models (LDM) \\cite{Rombach:LDM:2022}. }\n    \\label{fig:generativemodel}\n\n\n\\subsection{Large language models}\n\\label{ssec:llms}\n\nLLMs are based on transformer models using self-attention mechanisms as their core modules. Training LLMs comprises two steps: i) pre-training in an unsupervised learning manner, and ii) fine-tuning to a specific task or prompt-tuning for better user inputs. The models are first `pre-trained' with a large amount of unlabelled text data to learn the meaning of words, and the relationships between those words, before using it to adapt to a downstream task. This is why OpenAI refers to their model as a Generative Pre-trained Transformer (GPT). \n\nFine-tuning involves training the model on new datasets. The drawback is however that these data need to be large enough to ensure generalization to  new tasks. Prompt-tuning and prompt engineering are relatively new disciplines for developing and optimizing prompts to efficiently use language models. Prompts guide the way AI models interpret and respond to user queries. Prompt engineering is the process of structuring text or phrasing that guide the model towards generating the desired output. This relies heavily on trial and error, and an understanding of how the model responds. \nPrompt-tuning, on the other hand, involves training a small set of parameters before utilizing the LLM, thus requiring a relatively small amount of new data. This approach essentially converts text inputs into task-specific virtual inputs, referred to as tokens, while the pre-trained LLM remains unchanged \\cite{Lester:power:2021}.  The main drawback of prompt-tuning is lack of interpretability. This paradigm has however extended to other domains, such as visual prompt tuning \\cite{Jia:VPT:2022}. For a comprehensive survey of LLMs, please refer to \\cite{zhao:survey:2023}.\n\nTo date, there are many LLM platforms as shown in Fig. \\ref{fig:FLASK_LLM_and_history}. Fig. \\ref{fig:FLASK_LLM_and_history}(a) shows their timeline. Many surveys and evaluations of LLMs are also available \\cite{zhao:survey:2023,Chang:Survey:2024,YAO:Survey:2024}. These include FLASK (Fine-grained Language Model Evaluation based on Alignment SKill Sets) \\cite{Ye:FLASK:2024} which evaluates LLMs based on 12 fine-grained skills for comprehensive language model evaluation: logical correctness, logical robustness, logical efficiency, factuality, commonsense understanding, comprehension, insightfulness, completeness, metacognition, conciseness, readability, and harmlessness. Evaluation results from FLASK are shown in Fig. \\ref{fig:FLASK_LLM_and_history} (b).\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/FLASK_LLM_and_history.png}\n    \\caption{(a) Timeline of large language models. (b) Performance comparison evaluated by FLASK \\cite{Ye:FLASK:2024}.}\n    \\label{fig:FLASK_LLM_and_history}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/FLASK_LLM_and_history.png}\n    \\caption{(a) Timeline of large language models. (b) Performance comparison evaluated by FLASK \\cite{Ye:FLASK:2024}.}\n    \\label{fig:FLASK_LLM_and_history}\n\n\n\n\n\n\\subsection{Diffusion Models}\n\\label{ssec:DMs}\n\nA generative model, in the context of AI, exploits machine learning to learn a probability distribution of the training data to generate new data samples. The very first models were based on Autoencoders (AEs) that learn to encode input data into a lower-dimensional representation (latent space) and then decode it back to its original form. A specific type of AE, a variational autoencoders (VAE) \\cite{Kingma:auto:2014}, learns the latent space as statistical parameters of probabilistic distributions, leading to significant improvement of the generated results. Concurrently,  Goodfellow et al. \\cite{Goodfellow:GAN:2014} introduced an alternative architecture known as a Generative Adversarial Network (GAN). GANs comprise two competing AI modules: a generator, which creates a sample, and a discriminator, which determines whether the received sample is real or generated. When comparing VAEs to GANs, VAEs exhibit greater stability during training, whereas GANs excel at producing realistic images. More details about AEs and GANs for creative technologies can be found in our previous review \\cite{Anantrasirichai:AI:2022}.\n\nAn important factor in driving the rapid growth of generative AI has been  the development of diffusion probabilistic models (referred to as diffusion models (DMs)). The first DM was introduced in 2015 by Sohl-Dickstein et al. \\cite{Dickstein:Deep:2015}, using Nonequilibrium Thermodynamics. However, it took a further 5 years for DMs to generate desirable results: the era of DMs began with Denoising Diffusion Probabilistic Models (DDPMs) proposed by Ho et al. \\cite{Ho:DDPM:2020} in 2020 and Score-based diffusion models proposed by Song et al. \\cite{Song:Score:2021} in 2021. These involve a simplified process using a denoising autoencoder to approximate Bayesian inference. In brief, the models leverage a diffusion process to learn a probability distribution of the input data. As the name suggests, the data is diffused by gradually adding noise at each iteration step as shown in Fig. \\ref{fig:generativemodel} (b). A deep neural network (DNN) is then trained to remove this noise, called the denoising process or reverse process. Consequently, the trained model uses random noise to generate data with characteristics similar to those of the training samples. Comparing to GANs, DMs provide higher diversity samples \\cite{Dhariwal:Diffusion:2021} and a training process that is much more stable and does not suffer from mode collapse. DMs are however computationally intensive and require longer training times compared to GANs. The complexity can significantly reduced by training the DMs in latent space. Latent Diffusion Models (LDM) \\cite{Rombach:LDM:2022} use pretrained networks to convert images to feature maps, and perform training on a low-dimensional space. The diagram of LDM is shown in Fig. \\ref{fig:generativemodel} (c). \n\nGenerating a synthesized sample at random might not be particularly useful, especially for creative industry applications. Therefore, conditional diffusion models have been proposed, supporting a wide range of applications such as text-to-sound, text-to-images, and image-to-videos. For DMs, the conditional distributions are modelled using a conditional denoising autoencoder. Classifier guidance was introduced in \\cite{Dhariwal:Diffusion:2021} to improve the generation of images of a desired class. For example, when we provide the model with information, such as `a flower', the DM will synthesize a variety of flower images, as the word `flower' guides the model toward the latent distribution that is formed by various images of flowers. The work in \\cite{Choi:ILVR:2021} simply refines the latent space of well-trained unconditional DDPM so that the higher-level semantics of the synthetic samples are similar to the reference (conditioning).\nThe LDM \\cite{Rombach:LDM:2022} offers more flexible conditional image generators by adding cross-attention layer (referred to Transformers in Section \\ref{ssec:transformers}) to the denoising autoencoder. A survey on the methods and applications of DMs prior to 2024 can be found in \\cite{Cao:survey:2024}.\n\n\n\n\\subsection{Implicit Neural Representations}\n\nImplicit Neural Representations (INR), also called neural fields, neural implicits or coordinate-based neural networks, represent input content implicitly through learned functions $F$F, as shown in Eq.~\\ref{eqn:inr}. They can be considered as fields $x$x (represented by a scalar, vector, or a tensor with a value, such as magnetic field in physics) that are fully or partially parameterized by a neural network $\\Phi$\\Phi, typically an MLP \\cite{xie2022neural}.\n\\begin{equation}\nF(x, \\Phi, \\nabla_x \\Phi, \\nabla_x^2 \\Phi, \\dots) = 0, \\quad \\Phi : x \\mapsto \\Phi(x).\n\\label{eqn:inr}\n\\end{equation}\\begin{equation}\nF(x, \\Phi, \\nabla_x \\Phi, \\nabla_x^2 \\Phi, \\dots) = 0, \\quad \\Phi : x \\mapsto \\Phi(x).\n\\label{eqn:inr}\n\\end{equation}\nF(x, \\Phi, \\nabla_x \\Phi, \\nabla_x^2 \\Phi, \\dots) = 0, \\quad \\Phi : x \\mapsto \\Phi(x).\n\\label{eqn:inr}\n\n\n Although this concept appears complex,  the process is actually very straightforward. For example, in the case of an image, the coordinates of each pixel $(x, y)$(x, y) contain color information $(r, g, b)$(r, g, b). The INR inputs $(x, y)$(x, y) to the MLP and learns to provide the output $(r, g, b)$(r, g, b). The weights and biases of the MLP now represent such an image. Usually, the number of parameters of the MLP is smaller than the total number of pixels multiplied by 3, accounting for the 3 color channels. Hence, one of its emerging applications is in data compression \\cite{kwan2024hinerv}. Moreover, the INR can handle complex and high-dimensional data efficiently, attracting attention for visual computing applications such as  3D scene reconstruction.\n\nTraditional MLPs employ ReLU (rectified linear unit) for non-linear activation due to its simplicity. However, Sitzmann et al. \\cite{sitzmann:siren:2020} demonstrated that using periodic functions, such as sinusoids, are more suitable for representing complex natural signals, offering a better fit to the first- and second-order derivatives of the signals. However, this activation can cause ringing artifacts. Saragadam et al. instead proposed using complex Gabor wavelets \\cite{Saragadam:wire:2023}, which learn to represent high frequencies better and simultaneously are robust to noise.\n\nOne of the fastest-growing areas that exploits INRs is \\textbf{Neural Radiance Fields (NeRF)}, evidenced by 57 papers presented at CVPR, the largest annual conference in computer vision, in 2022 growing to 175 papers in 2023\\footnote{\\url{https://markboss.me/post/nerf_at_cvpr23/}}, before dropping to 71 in 2024, largely due to competition from 3D Gaussian Splatting\\footnote{\\url{https://github.com/Yubel426/NeRF-3DGS-at-CVPR-2024}}. First introduced in 2020 by Mildenhall et al. \\cite{Mildenhall:NeRF:2020}, NeRF is a form of neural rendering, a subset of generative AI, that generates novel views of a scene based on a partial set of 2D images. It achieves this by learning a mapping from 3D spatial coordinates and view directions $(x,y,z,\\theta,\\phi)$(x,y,z,\\theta,\\phi) to colors and density $(r,g,b,\\sigma)$(r,g,b,\\sigma). This implicit representation allows NeRF to handle complex scenes with varying geometry and appearance,  resulting in highly realistic renderings that include accurate lighting, shadows, and reflections. More detail can be found in Section \\ref{sssec:nerf}.\n\n\n\n\n\\begin{table}\n\\caption{Creative applications and corresponding AI-based methods mentioned in this paper}\n%\\tiny\n %\\hskip-5.0cm\n %\\begin{tabular}{p{1cm}p{1.4cm}|p{4cm}p{4cm}p{4cm}p{4cm}}\n \\resizebox{\\linewidth}{!}{\\begin{tabular}{lr|lll}\n \\\\\n \\toprule\n\\multicolumn{2}{c}{\\multirow{2}{*}{Application}} & \\multicolumn{3}{|c}{Technology} \\\\ \\cmidrule{3-5}\n& & Trans./Attn.$^1$ & Diffusion model$^2$  & INR\\\\\n\\midrule\n{\\bf Creation} & Text & \\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}  & \\\\\n& audio/music & \\cite{Alayrac:Flamingo:2022, Huang:GenerSpeech:2022} & \\cite{Li:Diffusion:2022, Yang:Diffsound:2023, Evans:stable:2025} & \\\\\n& Image &  \\cite{Alayrac:Flamingo:2022,esser:scaling:2024} & \\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024, Feng_Ma_2025, Liu_Ma_2025} & \\\\\n% ---------------------------------------------------------------\n& Animation/video & \\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024} & \\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024, wang2025lavie, wu2025customcrafter} &\\\\\n% ---------------------------------------------------------------\n & 3D/AR/VR & \\cite{yang:Holodeck:2024} & \\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024} & \\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear} \\\\\n% ---------------------------------------------------------------\n\\midrule\n{\\bf Information} & Text categorization & \\cite{sun:text:2023, shi:chatgraph:2023, Hou:promptboosting:2023, AI2025125952} \\\\\n% ---------------------------------------------------------------\n{\\bf Analysis} & Film analysis & \\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}  \\\\\n % ---------------------------------------------------------------\n & Content retrieval & \\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning} & \\cite{Jin:DiffusionRet:2023} \\\\\n % ---------------------------------------------------------------\n & Intelligent assistants & \\cite{King:Sasha:2024} \\\\\n % ---------------------------------------------------------------\n \\midrule\n{\\bf Content} & Enhancement  & \\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024} & \\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight} & \\cite{Yang:Implicit:2023} \\\\\n{\\bf  Enhancement  } & Style transfer & \\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR} & \\cite{Zhang:Inversion:2023,Chai:StableVideo:2023} & \\cite{Moon:generalizable:2023,Kim:Controllable:2024} \\\\\n% ---------------------------------------------------------------\n {\\bf  and Post} & Super-resolution & \\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024,wang2025seedvr} & \\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023, cao2025zero, wang2025seedvr} & \\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}\\\\\n% ---------------------------------------------------------------\n{\\bf Production} & {Restoration} & \\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025} & \\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023, cao2025zero, feng2025residual} & \\cite{Jiang:NeRT:2023} \\\\\n  % ---------------------------------------------------------------\n & Inpainting & \\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024} & \\cite{Moliner:solving:2023, Fei:Generative:2023} & \\\\\n % ---------------------------------------------------------------\n& Fusion & \\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023, LI2024102147} & \\cite{Zhao:DDFM:2023} & \\\\\n & Editing/VFX & \\cite{Shi:Motion-I2V:2024} & \\cite{Shi:Motion-I2V:2024,guo2024liveportrait} \\\\\n \\midrule\n % ---------------------------------------------------------------\n{\\bf Information} & Segmentation & \\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2,Zhang:DVISp:2025} & \\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024} & \\cite{Gong:Continuous:2023, Cen:Segment:2023} \\\\\n % ---------------------------------------------------------------\n{\\bf  Extraction} & Recognition &  \\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12} & \\cite{Li:Your:2023,Chen:DiffusionDet:2023, Zhang:DiffAD:2025, WU2025102965}\\\\\n % ---------------------------------------------------------------\n{\\bf  and}  & Tracking &  \\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring} & \\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024} & \\cite{Jung:AnyFlow:2023}\\\\\n % ---------------------------------------------------------------\n{\\bf  Understanding}  & 3D Reconstruction  & \\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024, LIU:DSEM:2025} & \\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024} & \\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}$^\\dag$  \\\\\n\\midrule\n\n% ---------------------------------------------------------------\n{\\bf Compression} & Image$^\\ast$ & \\cite{zhu2022transformer,zou2022devil,liu2023learned}& \\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual} & \\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}\\\\\n& Video& \\cite{xiang2022mimt,mentzer2022vct}& \\cite{li2024extreme}&\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}\\\\\n&Audio$^\\ast$ & & &\\\\\n\\midrule\n% ---------------------------------------------------------------\n{\\bf Quailty}  &  Image$^\\ast$ & \\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}& &\\\\\n\\textbf{Assessment}& Video$^\\ast$ & \\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}& & \\\\\n\\bottomrule\n\\multicolumn{5}{l}{$^1$ Trans./Attn. include transformers, mamba and CNN-based architectures that use attention module.} \\\\\n\\multicolumn{5}{l}{$^2$ Some diffusion models employ the transformer in their denoising autoencoders.}\n \\end{tabular}}\n \\footnotesize\n $^\\dag$ \nThese methods are based on explicit neural representations. \\\\\n$^*$ It is noted that for some compression and quality assessment tasks, there are other dominant network architectures in existing works. For example, LLMs have been used for image and audio compression, and visual quality assessment. Many neural audio codecs are also based on VQ-VAE models.\n \n\\label{tab:gather}\n\\end{table}\n\\caption{Creative applications and corresponding AI-based methods mentioned in this paper}\n\\resizebox{\\linewidth}\\linewidth{!}!{\\begin{tabular}{lr|lll}\n \\\\\n \\toprule\n\\multicolumn{2}{c}{\\multirow{2}{*}{Application}} & \\multicolumn{3}{|c}{Technology} \\\\ \\cmidrule{3-5}\n& & Trans./Attn.$^1$ & Diffusion model$^2$  & INR\\\\\n\\midrule\n{\\bf Creation} & Text & \\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}  & \\\\\n& audio/music & \\cite{Alayrac:Flamingo:2022, Huang:GenerSpeech:2022} & \\cite{Li:Diffusion:2022, Yang:Diffsound:2023, Evans:stable:2025} & \\\\\n& Image &  \\cite{Alayrac:Flamingo:2022,esser:scaling:2024} & \\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024, Feng_Ma_2025, Liu_Ma_2025} & \\\\\n% ---------------------------------------------------------------\n& Animation/video & \\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024} & \\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024, wang2025lavie, wu2025customcrafter} &\\\\\n% ---------------------------------------------------------------\n & 3D/AR/VR & \\cite{yang:Holodeck:2024} & \\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024} & \\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear} \\\\\n% ---------------------------------------------------------------\n\\midrule\n{\\bf Information} & Text categorization & \\cite{sun:text:2023, shi:chatgraph:2023, Hou:promptboosting:2023, AI2025125952} \\\\\n% ---------------------------------------------------------------\n{\\bf Analysis} & Film analysis & \\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}  \\\\\n % ---------------------------------------------------------------\n & Content retrieval & \\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning} & \\cite{Jin:DiffusionRet:2023} \\\\\n % ---------------------------------------------------------------\n & Intelligent assistants & \\cite{King:Sasha:2024} \\\\\n % ---------------------------------------------------------------\n \\midrule\n{\\bf Content} & Enhancement  & \\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024} & \\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight} & \\cite{Yang:Implicit:2023} \\\\\n{\\bf  Enhancement  } & Style transfer & \\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR} & \\cite{Zhang:Inversion:2023,Chai:StableVideo:2023} & \\cite{Moon:generalizable:2023,Kim:Controllable:2024} \\\\\n% ---------------------------------------------------------------\n {\\bf  and Post} & Super-resolution & \\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024,wang2025seedvr} & \\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023, cao2025zero, wang2025seedvr} & \\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}\\\\\n% ---------------------------------------------------------------\n{\\bf Production} & {Restoration} & \\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025} & \\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023, cao2025zero, feng2025residual} & \\cite{Jiang:NeRT:2023} \\\\\n  % ---------------------------------------------------------------\n & Inpainting & \\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024} & \\cite{Moliner:solving:2023, Fei:Generative:2023} & \\\\\n % ---------------------------------------------------------------\n& Fusion & \\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023, LI2024102147} & \\cite{Zhao:DDFM:2023} & \\\\\n & Editing/VFX & \\cite{Shi:Motion-I2V:2024} & \\cite{Shi:Motion-I2V:2024,guo2024liveportrait} \\\\\n \\midrule\n % ---------------------------------------------------------------\n{\\bf Information} & Segmentation & \\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2,Zhang:DVISp:2025} & \\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024} & \\cite{Gong:Continuous:2023, Cen:Segment:2023} \\\\\n % ---------------------------------------------------------------\n{\\bf  Extraction} & Recognition &  \\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12} & \\cite{Li:Your:2023,Chen:DiffusionDet:2023, Zhang:DiffAD:2025, WU2025102965}\\\\\n % ---------------------------------------------------------------\n{\\bf  and}  & Tracking &  \\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring} & \\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024} & \\cite{Jung:AnyFlow:2023}\\\\\n % ---------------------------------------------------------------\n{\\bf  Understanding}  & 3D Reconstruction  & \\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024, LIU:DSEM:2025} & \\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024} & \\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}$^\\dag$  \\\\\n\\midrule\n\n% ---------------------------------------------------------------\n{\\bf Compression} & Image$^\\ast$ & \\cite{zhu2022transformer,zou2022devil,liu2023learned}& \\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual} & \\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}\\\\\n& Video& \\cite{xiang2022mimt,mentzer2022vct}& \\cite{li2024extreme}&\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}\\\\\n&Audio$^\\ast$ & & &\\\\\n\\midrule\n% ---------------------------------------------------------------\n{\\bf Quailty}  &  Image$^\\ast$ & \\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}& &\\\\\n\\textbf{Assessment}& Video$^\\ast$ & \\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}& & \\\\\n\\bottomrule\n\\multicolumn{5}{l}{$^1$ Trans./Attn. include transformers, mamba and CNN-based architectures that use attention module.} \\\\\n\\multicolumn{5}{l}{$^2$ Some diffusion models employ the transformer in their denoising autoencoders.}\n \\end{tabular}}\n \\\\\n \\toprule\n\\multicolumn{2}2{c}c{\\multirow{2}{*}{Application}}\\multirow{2}2{*}*{Application}Application & \\multicolumn{3}3{|c}|c{Technology}Technology \\\\ \\cmidrule{3-5}3-5\n& & Trans./Attn.$^1$^1 & Diffusion model$^2$^2  & INR\\\\\n\\midrule\n{\\bf Creation}\\bf Creation & Text & \\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}  & \\\\\n& audio/music & \\cite{Alayrac:Flamingo:2022, Huang:GenerSpeech:2022} & \\cite{Li:Diffusion:2022, Yang:Diffsound:2023, Evans:stable:2025} & \\\\\n& Image &  \\cite{Alayrac:Flamingo:2022,esser:scaling:2024} & \\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024, Feng_Ma_2025, Liu_Ma_2025} & \\\\\n& Animation/video & \\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024} & \\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024, wang2025lavie, wu2025customcrafter} &\\\\\n& 3D/AR/VR & \\cite{yang:Holodeck:2024} & \\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024} & \\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear} \\\\\n\\midrule\n{\\bf Information}\\bf Information & Text categorization & \\cite{sun:text:2023, shi:chatgraph:2023, Hou:promptboosting:2023, AI2025125952} \\\\\n{\\bf Analysis}\\bf Analysis & Film analysis & \\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}  \\\\\n & Content retrieval & \\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning} & \\cite{Jin:DiffusionRet:2023} \\\\\n & Intelligent assistants & \\cite{King:Sasha:2024} \\\\\n \\midrule\n{\\bf Content}\\bf Content & Enhancement  & \\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024} & \\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight} & \\cite{Yang:Implicit:2023} \\\\\n{\\bf  Enhancement  }\\bf  Enhancement   & Style transfer & \\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR} & \\cite{Zhang:Inversion:2023,Chai:StableVideo:2023} & \\cite{Moon:generalizable:2023,Kim:Controllable:2024} \\\\\n{\\bf  and Post}\\bf  and Post & Super-resolution & \\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024,wang2025seedvr} & \\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023, cao2025zero, wang2025seedvr} & \\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}\\\\\n{\\bf Production}\\bf Production & {Restoration}Restoration & \\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025} & \\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023, cao2025zero, feng2025residual} & \\cite{Jiang:NeRT:2023} \\\\\n  & Inpainting & \\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024} & \\cite{Moliner:solving:2023, Fei:Generative:2023} & \\\\\n & Fusion & \\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023, LI2024102147} & \\cite{Zhao:DDFM:2023} & \\\\\n & Editing/VFX & \\cite{Shi:Motion-I2V:2024} & \\cite{Shi:Motion-I2V:2024,guo2024liveportrait} \\\\\n \\midrule\n {\\bf Information}\\bf Information & Segmentation & \\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2,Zhang:DVISp:2025} & \\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024} & \\cite{Gong:Continuous:2023, Cen:Segment:2023} \\\\\n {\\bf  Extraction}\\bf  Extraction & Recognition &  \\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12} & \\cite{Li:Your:2023,Chen:DiffusionDet:2023, Zhang:DiffAD:2025, WU2025102965}\\\\\n {\\bf  and}\\bf  and  & Tracking &  \\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring} & \\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024} & \\cite{Jung:AnyFlow:2023}\\\\\n {\\bf  Understanding}\\bf  Understanding  & 3D Reconstruction  & \\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024, LIU:DSEM:2025} & \\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024} & \\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}$^\\dag$^\\dag  \\\\\n\\midrule\n\n{\\bf Compression}\\bf Compression & Image$^\\ast$^\\ast & \\cite{zhu2022transformer,zou2022devil,liu2023learned}& \\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual} & \\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}\\\\\n& Video& \\cite{xiang2022mimt,mentzer2022vct}& \\cite{li2024extreme}&\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}\\\\\n&Audio$^\\ast$^\\ast & & &\\\\\n\\midrule\n{\\bf Quailty}\\bf Quailty  &  Image$^\\ast$^\\ast & \\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}& &\\\\\n\\textbf{Assessment}& Video$^\\ast$^\\ast & \\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}& & \\\\\n\\bottomrule\n\\multicolumn{5}5{l}l{$^1$ Trans./Attn. include transformers, mamba and CNN-based architectures that use attention module.}$^1$^1 Trans./Attn. include transformers, mamba and CNN-based architectures that use attention module. \\\\\n\\multicolumn{5}5{l}l{$^2$ Some diffusion models employ the transformer in their denoising autoencoders.}$^2$^2 Some diffusion models employ the transformer in their denoising autoencoders.\n \n \\footnotesize\n $^\\dag$^\\dag \nThese methods are based on explicit neural representations. \\\\\n$^*$^* It is noted that for some compression and quality assessment tasks, there are other dominant network architectures in existing works. For example, LLMs have been used for image and audio compression, and visual quality assessment. Many neural audio codecs are also based on VQ-VAE models.\n \n\\label{tab:gather}\n\n ", "appendix": false}, "Advanced AI for the creative industries": {"content": "\n\\label{sec:existing}\n\nSimilarly to our previous (2021) review of AI for the creative industries~\\cite{Anantrasirichai:AI:2022}, Table \\ref{tab:gather} categorizes applications and corresponding AI-based solutions. These areas are explored in more detail below.\n\n\\subsection{Content creation}\n\nContent creation is a fundamental activity of artists and designers and the term `\\textit{AI art}' refers to artforms created with the assistance of an AI algorithms or entirely by an AI system. This can refer to various digital forms including images, texts, audio, and videos. The roots of AI art can be traced back to the 20th century, exemplified by AARON, a computer program initiated in 1972 to autonomously produce paintings and drawings \\cite{encyclopedia_ai_v1}. The practicality of AI art has been enhanced  with advancements in deep learning, particularly GANs from 2014 and, more recently, transformers, DMs and INRs. \n\n\\subsubsection{Text generation, script and journalism}\n\nIn the era of LLMs, AI writing tools have been widely used to assist various writing tasks, including generation written articles, blog posts, essays, and reports. These tools go beyond mere grammar and spelling checks; they boast advancements enabling them to analyze the style and tone of written material, adding images, videos and tables, offering suggestions to enhance clarity, coherence, and overall readability \\cite{ippolito:creative:2022}. Moreover, AI tools extend their utility beyond content generation by automating tasks like keyword generation, meta tags, and descriptions, thereby increasing search rankings using search engine optimization (SEO). Additionally, they support the process of publishing across multiple online platforms. Transformers have been used to generate image captions by combining information from the images with a word prefix or questions \\cite{Wang:SIMVLM:2022}.\n\nAI script generators serve as beneficial aids for writers, filmmakers, and game developers, offering inspiration, idea generation, and assistance in crafting entire scripts \\cite{Jeary2024, Azzarelli:Reviewing:2024}. Human-AI brainstorming is helpful and saves time \\cite{guo:exploring:2024}. Presently, there are numerous software and websites providing both free and paid script generation services. However, many of these tools are still constrained when it comes to longform creative writing. Dramatron, developed by Google \\cite{Mirowski:cowriting:2023}, introduces hierarchical language generation, enabling the creation of cohesive scripts and screenplays spanning long ranges. This includes elements such as titles, characters, story beats, location descriptions, and dialogue.\n\nAs discussed earlier, chatbots are now powered by LLMs, effectively simulating human conversation. These fundamental LLMs are specialized for specific tasks. For instance journalist AI and blog AI writers\\footnote{For example, see \\url{https://tryjournalist.com/}} generate content with layouts suitable for print or online publication. Additionally,  AI tools exist that are designed to detect AI-generated content (e.g., for checking for copyright), AI-writing styles, content originality and to ensure the naturalness and flow of articles. Undoubtedly, generative AI is reshaping the way artists and journalists operate. For an in-depth exploration of the impact and implications of these technological advancements on news organizations, refer to the survey conducted by Beckett et al. \\cite{Beckett:Generating:2023}.\n\nGenerating text and scripts automatically can also be done through image and video inputs without text prompts (e.g., image captioning \\cite{Stefanini:From:2023}) and with text prompts. These approaches are referred to as Vision Language Models (VLMs):  multimodal models that learn from images and text. The most common and prominent models often consist of an image encoder, an embedding projector to align image and text representation, often via a dense neural network, and a text decoder stacked in this order. The most well-known technique is Contrastive Language-Image Pre-training (CLIP) \\cite{radford2021learning}. More recent work in \\cite{wei2024vary} scales up the vision vocabulary by incorporating new image features into the existing CLIP model, resulting in improved content understanding. A comprehensive survey of VLMs for vision tasks can be found in \\cite{Zhang:vision:2024}.\n\n\\subsubsection{Audio and music generation}\n\\label{sssec:musicgen}\n\nSimilar to language models, AI-based music generation has rapidly advanced due to unsupervised learning on large datasets and the use of transformers (see Section \\ref{ssec:llms}). Examples of such systems include MuseNet\\footnote{\\url{https://openai.com/research/musenet}}, Magenta Studio\\footnote{\\url{https://magenta.tensorflow.org/studio}}, and Musicfy\\footnote{\\url{https://musicfy.lol/}}. These tools assist in music composition by learning complex musical patterns, predicting the next word or music note in a sequence, and mixing specified instruments. Moreover, AI tools can convert one type of sound into another, such as from whistling to violin or from flute to saxophone\\footnote{See an example by Ummet Ozcan at \\url{https://www.youtube.com/watch?v=lI1LCfTx2lI}}. This capability is invaluable for artists who may not be proficient in playing all the instruments they wish to incorporate, saving both time and costs. In  2024, Suno has released a model capable of producing radio-quality music that can be created in 2 minutes\\footnote{\\url{https://www.suno.ai/blog/v3}}. Later, Udio \\footnote{\\url{https://www.udio.com/}}, was launched. This offers a prompt to create lyrics and music with a maximum duration of 90 seconds, and also appears to have, at least some, awareness of copyright.\n\nAI voice software changes vocalizations from one person to another, for example enabling users to train the model to convert other people's voices into their own, e.g. lalals\\footnote{\\url{https://lalals.com/}}, Kits\\footnote{\\url{https://www.kits.ai/}}, Media.io\\footnote{\\url{https://www.media.io/online-voice-changer.html}}, etc. Certain software, such as Voice.ai\\footnote{\\url{https://voice.ai/}}, even offers real-time voice changing capabilities. The technologies behind this uses a transformer to learn voice features and patterns in mel-spectrogram form. For example, the framework proposed in \\cite{Yang:Diffsound:2023} uses a DM-based method with a transformer backbone to turn text input into a mel-spectrogram using the vector quantized variational autoencoder (VQ-VAE) \\cite{Oord:Neural:2017}. Next, this mel-spectrogram is transformed into a sound wave.  Unlike a regular spectrogram, the mel-spectrogram is based on the mel-frequency scale, which offers higher resolution for lower frequencies. Voice style transfer often uses zero-shot learning (a model is trained to recognize classes or categories that it has never encountered during training) \\cite{Huang:GenerSpeech:2022} or few-shot learning (a model trained with only one or a few examples per class) \\cite{Wang:One:2022}. Stable Audio Open \\cite{Evans:stable:2025} introduces a text-conditioned generative model for non-speech audio, trained on Creative Commons licensed data, capable of producing state-of-the-art 44.1kHz stereo audio.\n\nAnother emerging AI technology application is in the field of spatial audio. In 2022, Apple Music revealed that, in just over a year, more than 80\\% of its worldwide subscribers were enjoying the spatial audio experience, with monthly plays in spatial audio increasing by over 1,000\\%\\footnote{\\url{https://www.apple.com/uk/newsroom/2023/01/apple-celebrates-a-groundbreaking-year-in-entertainment/}}. With head tracking, this technology significantly enhances the immersive experience. Masterchannel has launched SpatialAI\\footnote{\\url{https://platform.masterchannel.ai/spatial}}, claiming it to be the world's first spatial mastering AI. This processes audio files and returns an optimized track for streaming platforms, along with an individually optimized stereo version for traditional distribution. All these advancements leverage transformer-based technologies.\n\n\\subsubsection{Image generation}\n\nAs described in Section \\ref{ssec:DMs}, recent advances in AI technologies for image generation are based on Diffusion Models (DMs). Well-known and highly competitive text-to-image models include Stable Diffusion\\footnote{\\url{https://stability.ai/stable-image}}, Midjourney\\footnote{\\url{https://www.midjourney.com/home}}, DALL\u00b7E\\footnote{\\url{https://openai.com/dall-e-3}}, and Ideogram\\footnote{\\url{https://ideogram.ai/}}. Released in March 2024, the latest version of Stable Diffusion (SD3), has been reported to outperform state-of-the-art text-to-image generation systems such as DALL\u00b7E 3 (released August 2023) \\cite{esser:scaling:2024}, Midjourney v6 (released December 2023), and Ideogram v1 (released February 2024) in terms of typography and prompt adherence, based on human preference evaluations. These open-source tools are built on a Multimodal Diffusion Transformer (MM-DiT) architecture, which integrates attention from both text and images. LLM4GEN \\cite{Liu_Ma_2025} fuses features from LLM and CLIP models to enhance the semantic understanding in text-to-image diffusion models, enabling them to better handle complex and dense prompts involving multiple objects. Examples of text-to-image generation are shown in Fig. \\ref{fig:LLMGround} (a) comparing the performance of four models, i.e. Ideogram v1, DALL\u00b7E 3, Photoshop 2025, and sdxy-turbo by Nvidia. It is clear that hands are one of the most difficult features to generate, e.g., one hand has six fingers.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/LLMGround.jpg}\n    \\caption{Text-to-image generation (generated on 27 November 2024). (a) text-to-image generation by Ideogram v1, DALL\u00b7E 3, Photoshop 2025, and sdxy-turbo by Nvidia. (b) The top-row images were generated by DALL\u00b7E in ChatGPT 4. The bottom-row images are generated by LLM-grounded Diffusion \\cite{Lian:LLMG:2024}. }\n    \\label{fig:LLMGround}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/LLMGround.jpg}\n    \\caption{Text-to-image generation (generated on 27 November 2024). (a) text-to-image generation by Ideogram v1, DALL\u00b7E 3, Photoshop 2025, and sdxy-turbo by Nvidia. (b) The top-row images were generated by DALL\u00b7E in ChatGPT 4. The bottom-row images are generated by LLM-grounded Diffusion \\cite{Lian:LLMG:2024}. }\n    \\label{fig:LLMGround}\n\n\n\nDALL\u00b7E 3, available on ChatGPT 4, also provides an inpainting tool, allowing the user to manually select the area to edit. However, as of April 2024, its performance is still limited. As illustrated in Fig. \\ref{fig:LLMGround} (b), the selected area is the white car, and with the follow-up request to change the white car to the red car, DALL\u00b7E 3 generates correctly. However, if asked to replace it with a bicycle, it does not work. LLM-grounded Diffusion \\cite{Lian:LLMG:2024} was the first to introduce a framework that allows multiple rounds of user requests without the need for manual selection on the image. This is achieved by generating layout-grounded images, first using stable diffusion and then masking the latent variables as priors for the next round of generation\\footnote{Images in Fig. \\ref{fig:LLMGround} (b) were generated using their demo: https://huggingface.co/spaces/longlian/llm-grounded-diffusion.}. Since then, text-driven image editing has seen significant improvements in quality, with most recent approaches adopting Diffusion Transformer architectures \\cite{Feng_Ma_2025, Huang:Diff:2025}. \n\nSimilar to DALL\u00b7E 3, Photoshop features a Generative Fill tool\\footnote{\\url{https://www.adobe.com/th_en/products/photoshop/generative-fill.html}} designed to generate new images or assist with photo editing. It accepts a text prompt and provides several generation choices. After defining the editing area, users can remove and add new objects (more inpainting tasks are discussed in Section \\ref{ssec:inpaiting}), transfer to new styles, and expand content within images. Recently, Brooks et al. introduced InstructPix2Pix \\cite{Brooks:InstructPix2Pix:2023},  a conditional diffusion model that generates image editing examples without predefined editing areas. By combining GPT-3 and Stable Diffusion, the model effectively captures and matches the semantic meaning of the content in both text and image. Sometimes, style and context are not easy to describe in words. Textual Inversion \\cite{gal:Image:2023} personalizes large pre-trained text-to-image diffusion models based on specific objects and styles, using 3-5 images of a user-provided concept. ByteDance announced Hyper-SD \\cite{ren:hypersd:2024} which proposed trajectory segmented consistency distillation and provides real-time high-resolution image generation from drawing with a control text prompt. \n\n\n\n\\subsubsection{Video generation and animation} \n\\label{sssec:videogen}\n\nDespite the success of text-to-image generation, text-to-video generation has not advanced at the same pace, only starting to grow more rapidly in 2024,  largely due to its computational expense and content complexity. Several major companies and private platforms have however now released offerings, including Gemini 1.5 by Google, Make-A-Video by Meta, and Sora by OpenAI. Make-A-Video \\cite{singer:Make:2023}, through a spatiotemporally factorized diffusion model, leverages joint text-image priors and super-resolution in space and time. Some results however contain  flickering artifacts\\footnote{\\url{https://makeavideo.studio/}}. Gen-2 by Runway\\footnote{\\url{https://research.runwayml.com/gen2}} offers both text- and image-to-video and can generate a smooth 4-sec video. In April 2024, Adobe Premier Pro announced their integration of generative AI tools for video extension with third-party models by OpenAI, Runway and Pika Labs\\footnote{\\url{https://www.adobe.com/products/premiere/ai-video-editing.html}}. This new update also includes a contextual-selection tool, inpainting for object removal, and object addition to the defined areas in the videos with a text prompt. \n\nText-to-video technologies, combined with AI voice, have been tested not only by artists or producers but also by a wider audience. Results from these tests, such as automatically turning scripts into movie trailers and music videos, have been widely shared on public online platforms\\footnote{\\url{https://twitter.com/minchoi/status/1775907105813217398}}. However, scene composition and transitions still require further editing to align with producers' needs\\footnote{See an example by Curious Refuge at \\url{https://www.youtube.com/watch?v=fJQbP34GoHQ}}.\nIn April 2024, Microsoft introduced VASA-1 \\cite{xu:VASA-1:2024}, which turns a single static image and a speech audio clip into a video clip of realistic talking faces mimicking human facial expressions and head movements, as shown in Fig. \\ref{fig:Deepmotion_Vasa} (right). The overall quality of the generated videos is better than VLOGGER by Google \\cite{corona:vlogger:2024}, which is based on similar technology -- diffusion models. However, VLOGGER also offers movement of the upper body and hand gestures. Recently, ByteDance introduced an audio-driven interactive head generation \\cite{Zhu:INFP:2024} that offers listening and speaking states during multi-turn conversations. This framework is based on a conditional diffusion transformer\n.\nThe main technologies underpinning text-to-video and image-to-video tasks are based on DMs with a combination of 3D convolutions (or separately spatial and temporal convolutions), and spatial and temporal attention modules \\cite{wang:modelscope:2023}. Tune-A-Video \\cite{wu:tune:2023} modifies the style of an input video using a text prompt. The method leverages pretrained text-to-image models and introduces attention tuning to ensure temporal consistency. Early video generation methods often exhibit flickering, as observed in the CVPR2023 competition on text-guided video editing, where all results suffered from temporal inconsistency. Dreamix \\cite{molad:dreamix:2023} videos do not have this issue, but they are very blurry. As an example of a transformer-based approach, CogVideo \\cite{hong:cogvideo:2023} employs VQ-VAE to convert input frames to tokens, which are then fused with text tokens to produce a new video. Phenaki \\cite{villegas:phenaki:2023} exploits transformers to generate variable length videos, but the quality is lower than those based on DMs. Evaluations of these methods can be found in \\cite{Liu:FETV:2023}. More recent work has applied spatiotemporal layers to model temporal dynamics \\cite{Gupta:Photorealistic:2024}. The transformer blocks have been redesigned for latent video diffusion modeling with window-restricted spatial and spatiotemporal attention. LaVie \\cite{wang2025lavie} demonstrates that simple temporal self-attention mechanisms, when combined with rotary positional encoding, are sufficient to capture the temporal correlations inherent in video data. The image-to-video generation process is analogous to text-to-video methods, but it conditions diffusion models on images rather than text. Some approaches further enhance generation by incorporating both textual descriptions (to guide motion) and images (to define objects and scenes) as inputs \\cite{wu2025customcrafter}.\nMany more free and commercial tools for video generation are now emerging. These include Veo 3 by Google DeepMind\\footnote{https://deepmind.google/models/veo/}, Kling AI\\footnote{https://www.klingai.com/}, Pika 2.2\\footnote{https://pikartai.com/pika-2-2/}, Hailuo AI\\footnote{https://hailuoai.video/}, etc. Though not perfect, the generated videos are close to reality (visit their websites for showcase examples).\n\nGenerating characters with human posture and motion from text prompts has also become popular. Make-An-Animation \\cite{Azadi:Make:2023} trains on image-text datasets and fine-tunes on motion capture data, adding additional layers to model the temporal dimension. Animate Anyone by Alibaba Group \\cite{Hu_2024_CVPR} inputs a real photo or anime of a person with a sequence of guided poses. The results are significantly better than existing techniques, including Disco \\cite{wang:disco:2024} and Bidirectionally Deformable Motion Modulation (BDMM) \\cite{Yu:Bidirectionally:2023}. They also suggest using Animate Anyone  with Outfit Anyone\\footnote{\\url{https://humanaigc.github.io/outfit-anyone/}} to produce a character with a reference outfit.\n\nViggle\\footnote{\\url{https://viggle.ai/}} claims to be the first video-3D foundation model embodying an actual understanding of physics. It combines a character and a text prompt about motion to generate character animation. Available AI tools for 3D on the market include DeepMotion\\footnote{\\url{https://www.deepmotion.com/}} that offers text-to-3D post animation and video-to-3D post animation, shown in Fig. \\ref{fig:Deepmotion_Vasa} (left). The later function can track multiple people from real video and generates replicated characters with the same motions.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/Deepmotion_Vasa.png}\n    \\caption{(Left) Video-to-3D post animation by DeepMotion. (Right) Image and audio to video by VASA-1 \\cite{xu:VASA-1:2024}}\n    \\label{fig:Deepmotion_Vasa}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/Deepmotion_Vasa.png}\n    \\caption{(Left) Video-to-3D post animation by DeepMotion. (Right) Image and audio to video by VASA-1 \\cite{xu:VASA-1:2024}}\n    \\label{fig:Deepmotion_Vasa}\n\n\n\\subsubsection{Augmented, virtual and mixed reality, and 3D content}\n\nWhile the benefits of LLMs in Augmented Reality (AR) directly target educational purposes, enhance cognitive support, and facilitate communication \\cite{XU2025103402}, mixed reality (MR) has once again become exciting since the release of the Apple Vision Pro in February 2024. This demonstrated the potential of MR experiences by merging real-world environments with computer-generated ones. Thanks to the rapid growth of AI-based 3D representation (see Section \\ref{ssec:3Dreconstruct}), the generation of AR/VR/MR content has advanced significantly. Real-time rendering with immersive interaction has improved, and real scenes can now be generated avoiding uncanny valley effects. There has also been an attempt to use autoregressive and generative models to estimate lighting, achieving a visually coherent environment between virtual and physical spaces in AR \\cite{zhao2024clear}.\n\nSimilar to other content generation tools, LLMs have been influenced on immersive technologies, including text-to-3D and image-to-3D. Exciting examples include\nHolodeck \\cite{yang:Holodeck:2024}, which automatically generates 3D embodied environments via text-prompt interactions with a large language model (GPT-4). 3D objects are gathered from Objaverse \\cite{deitke:Objaverse:2023}, a dataset with 800K+ annotated 3D objects. RealFusion \\cite{Melas:RealFusion:2023}, a single image to 3D object generator, merges 2D diffusion models with NeRF, improving Instant-NGP \\cite{mueller:instant:2022}, which provides an API for VR controls. NeuralLift-360 \\cite{Xu:NeuralLift:2023} also uses diffusion models to generate priors for novel view synthesis. Magic123 \\cite{Qian:Magic123:2024} is the latest image-to-3D tool that uses  2D and 3D priors simultaneously  to produce high-quality high-resolution 3D geometry and textures. DreamGaussian \\cite{tang:dreamgaussian:2024} offers text-to-3D and image-to-3D by adapting 3D Gaussian splatting (more in Section \\ref{sssec:3DGS}) into generative settings using a diffusion prior. This generates photo-realistic 3D assets with explicit mesh and texture maps within only 2 minutes. DreamGaussian4D \\cite{ren:dreamgaussian4d:2023} employs image-to-video diffusion and a 4D Gaussian Splatting representation to generate an image-to-4D model. The results are not very sharp, but they can be further edited with Blender. \n\nIn July 2024, Shutterstock launched its Generative 3D service in commercial beta, powered by NVIDIA Edify, a multimodal generative AI architecture. This service enables creators to rapidly prototype 3D assets and generate 360-degree HDRi backgrounds to light scenes using text or image prompts. In conjunction with OpenUSD, the created scenes can be rendered into 2D images and used as input for AI-powered image generators, allowing for the production of precise, brand-accurate visuals.\n\n\n\n\n\n\n\n\\subsection{Information analysis}\n\n\\subsubsection{Text categorization}\n\nApplications of text categorization include detecting spam emails, automating customer support, monitoring social media for harmful content, etc. At its core, text categorization involves assigning predefined labels to text documents, which can be anything from a tweet to a lengthy article. LLMs are particularly well-suited for this task due to their ability to comprehend complex and nuanced language. One of the main advantages of using LLMs in text categorization is their transfer learning capability. Models can be pre-trained on a large amount of text and then fine-tuned on a smaller, task-specific dataset, with or without further post-processing technique. For example, CARP \\cite{sun:text:2023} applies kNN to integrate diagnostic reasoning process for final decision. ChatGraph, proposed by Shi et al. \\cite{shi:chatgraph:2023}, utilizes ChatGPT to refine text documents. It uses a knowledge graph, extracted using another specific defined prompt, and finally, a linear model is trained on the text graph for classification. Multiple learners are also used to enhance the performances \\cite{Hou:promptboosting:2023,AI2025125952}.\n\n\\subsubsection{Advertisements and film analysis}\n\nNot only does AI assist in generating ideas and content, but it can also aid creators in effectively matching content to their audiences, particularly on an individual level \\cite{feizi:Online:2023}. This effectively helps in advertising personalization\u2014eMarketer\\footnote{\\url{https://www.emarketer.com/content/spotlight-marketing-personalization}} reported that nearly nine out of ten consumers are comfortable with their browsing history being utilized to create personalized ads. In contrast to outdated syntax-style searches, advanced LLM tools can comprehensively grasp user intent behind each search through conversation prompts, providing advertisers with a high level of granularity.\n\nCurrent advances in generative AI would greatly benefit sentiment analysis, also known as opinion mining, where opinions are gather from social media, articles, customer feedback, and corporate communication and are analysed to understand emotion of the owners. This is a potential tool for filmmakers and studios, enabling the creation of effective and targeted marketing campaigns. By analyzing viewer emotions and opinions, AI can provide valuable insights into audience preferences, aiding in the optimization of film marketing strategies. Sentiment analysis with modern generative AI produce more accurate results. Technically, LLMs learn complex patterns and relationships in text data for sentiment classification \\cite{Mao:Biases:2023, krugmann:sentiment:2024}. SiEBERT \\cite{Hartmann:More:2023} provides pre-trained model with open-source scripts to be fine-tuned to further improve accuracy for novel applications. Cinema Multiverse Lounge \\cite{Ryu:Cinema:2025}, a multi-agent conversational system, allows users to interact with LLM-driven agents, each embodying a distinct film-related target users.\n\n\\subsubsection{Content retrieval and recommendation services}\n\nGenerative retrieval (GR) was pioneered by Metzler et al. \\cite{Metzler:Rethinking:2021}. Unlike traditional retrieval, which adheres to the ``index-retrieve-then-rank\" paradigm, the GR paradigm employs a single model to obtain results from query input. The model generally involve deep-learning based transformers, generating output token-by-token. More recent work in \\cite{li2024learning} introduces learning-to-rank training to enhance the performance system up to 30\\%.\nGR has several advantages including substituting the bulky external index with an internal index (i.e., model parameters), significantly reducing memory usage, and enabling optimization during end-to-end model training towards a universal objective for information retrieval tasks. Conversational question answering techniques have been integrated to enhance the document retrieval \\cite{li:unigen:2024}. \n\nWhen retrieving visual content, recent work exploits generative models to enhance content-based model search \\cite{Lu:content:2023}. These models decode the text, image, or video query into samples of possible outputs, which are then used to learn statistics for better matching between the query and output candidates. DMs are also employed for visual retrieval tasks, where they learn joint data distributions between text queries and video candidates \\cite{Jin:DiffusionRet:2023}. A comprehensive survey on Generative Information Retrieval is available in \\cite{Li:From:2025}.\n\n\n\nWhile the retrieval task involves users directly defining a specific query input, recommendation services operate by retrieving content based on previous usage patterns. Essentially, a recommendation engine is a system that suggests products, services, or information to users through data analysis. Research in \\cite{CHUA:AI:2023} has reported a positive association between buyers' attitudes toward AI and their behavioral intention to accept AI-based recommendations, with potential for further growth. Notable examples include the recommendation framework developed by Google \\cite{Rajput:recommender:2023}, which utilizes GR. This framework assigns Semantic IDs to each item and trains a retrieval model to predict the Semantic ID of an item that a given user may engage with. A report by Aggarwal et al.~\\cite{aggarwal2025evolution} states that the recommendation accuracy of recommendation services has increased from 45.0\\% to 91.5\\% with the integration of generative AI.\n\n\\subsubsection{Intelligent assistants}\n\nIntelligent assistants refer to software programs or applications that use AI and NLP to interact with users and provide helpful responses or perform tasks. These assistants can range from simple chatbots to sophisticated virtual agents capable of understanding and responding to complex queries. They're designed to assist users in various tasks, from answering questions and providing information to scheduling appointments and controlling smart home devices.\n\nCurrent LLMs obviously enhance the performance of intelligent assistants, designed to understand complex inquiries and generate more natural conversational responses, such as Sasha \\cite{King:Sasha:2024}. Generative AI can also be used to enhance the performance of human customer support agents, aiding in search and summarization, as discussed in the previous section. Brynjolfsson et al. \\cite{brynjolfsson:generative:2023} examined the implementation of a generative AI tool designed to offer conversational guidance to customer support agents. Their research revealed that AI assistance significantly enhances problem resolution and customer satisfaction. Furthermore, they observed that AI recommendations prompt low-skill workers to adopt communication styles akin to those of high-skill workers. AI-based intelligent assistants may currently be more focused on educational purposes, but they can clearly help artists write more efficiently \\cite{Lee:design:2024} or assist in customizing personal requirements \\cite{sajja2024ai}. The performance of personalized assistants can be enhanced with domain-specific knowledge to provide more in-depth responses to users \\cite{jiang2025domain}.\n \n\\subsection{Content enhancement and post production workflows}\n\n\\subsubsection{Enhancement}\n\nIn our previous review paper \\cite{Anantrasirichai:AI:2022}, we discussed AI technologies for contrast enhancement and colorization as separate topics, as methods were developed specifically for each task. However, in recent years, there has been a shift towards addressing more complex issues, such as those encountered in low-light environments and underwater scenarios. These real-world situations often involve a combination of challenges, including low contrast, color imbalance, and noise.\n\nIn low-light conditions, scenes often exhibit low contrast, leading to focusing difficulties or the need for long exposures, which can result in blurred images and videos. To address this, LEDNet \\cite{Zhou:LEDNet:2022} has introduced a synthetic dataset for such scenarios and incorporated a learnable non-linear activation function within the network to enhance feature intensities. Meanwhile, SNR-Aware \\cite{Xu:SNR:2022} estimates spatial-varying Signal-to-Noise Ratio (SNR) maps and proposes local and global learning branches using ResNet and transformer architectures, respectively. NeRCo \\cite{Yang:Implicit:2023} address low-light problem with INR, which unifies the diverse degradation factors of real-world scenes with a controllable fitting function.  Diffusion models (DMs) have also become popular choices for low-light image enhancement \\cite{HOU:Global:2023,Yi:Diff:2023,Jiang:Low:2023}. Diff-Retinex \\cite{Yi:Diff:2023} formulates the low-light image enhancement problem into Retinex decomposition, and employs multi-path generative diffusion networks to reconstruct the normal-light Retinex probability distribution. A recent state-of-the-art approach presented in \\cite{Jiang:Low:2023} decomposes images into high and low frequencies using wavelet transform. High frequencies are enhanced using a transformer-based pipeline, while the low frequencies undergo a diffusion process. This method achieves nearly 2.8dB improvement over the state-of-the-art transformer-based approach, e.g. LLFormer \\cite{Wang:Ultra:2023}, and significantly better than  INR-based method, NeRCo \\cite{Yang:Implicit:2023}, on a real low-light image benchmarking dataset. The technique has been extended for video enhancement in \\cite{lin2024lowlight}. The output of the enhancement typically depends on user preferences. This has been viewed as a one-to-many inverse problem, with attempts to solve it using Bayesian approaches. For example, a Bayesian Enhancement Model (BEM) \\cite{huang2025bayesian} incorporates Bayesian Neural Networks (BNNs) to capture data uncertainty and produce diverse outputs. The method can be used with Transformers or Mamba as the architecture backbone. \n\nRegarding video enhancement, transformer and DMs are still in their early stages. STA-SUNet \\cite{Lin:SPATIO:2024} has demonstrated that using transformers for low-light video enhancement outperforms CNN-based methods \\cite{anantrasirichai:BVI:2024}. The recent Mamba-based network \\cite{huang2025bvi} also demonstrates promising results, outperforming STA-SUNet by more than 2 dB in PSNR. It is important to note that low-light enhancement is subjective. While most training datasets use normal lighting conditions as ground truth \\cite{Lin:BVI-RLV:2024}, the enhanced images and videos may alter the mood and tone of the content. Therefore, the tools for creative industries should be adjustable, not only for entire images and videos but also adaptive to specific areas and content. For instance, CLE Diffusion \\cite{Yin:CLE:2023} enables user-friendly editing of lighting with fine-grained regional controllability. \n\nRecent efforts have focused on enhancing User-Generated Content (UGC) videos\u2014authentic recordings created by individuals rather than brands, often showcasing real experiences with products or services. The winning solution of the NTIRE 2025 Challenge on UGC Video Enhancement \\cite{safonov2025ntire} implemented a pipeline of four sequential modules: color enhancement, denoising, BasicVSR++ restoration \\cite{Chan:BasicVSR:2022}, and SwinIR \\cite{Liang:SwinIR:2021}. This method achieved a 17\\% higher subjective score than the second-place entry, which used a two-stage framework, highlighting a notable improvement in perceived visual quality.\n\n\\subsubsection{Style transfer}\n\nStyle transfer in AI art refers to a technique where the artistic style of one image (or video) is applied to another image (or video) while preserving the content of the latter. Style transfer has numerous applications in art, design, and image editing, allowing artists and designers to create unique and visually appealing compositions by blending different artistic styles with existing images (or videos). The applications also include image-to-image and sequence-to-sequence translations.\n\nStyTr2 \\cite{Deng:StyTr2:2022} is the first transformer-based method for style transfer, applying content as a query and style as a key of attention. InST \\cite{Zhang:Inversion:2023} utilizes Stable Diffusion Models as the generative backbone and introduces an attention-based textual inversion module to learn the description of the content. StableVideo \\cite{Chai:StableVideo:2023} uses a text prompt to describe the desired appearance of the output, transforming the input video to have a new look based on a diffusion model. For instance, a video of a white car driving in summer can be altered to show a red car driving in winter. A large pre-trained DM is employed in \\cite{Chung_2024_CVPR}, where the style is injected to manipulate the self-attention of the decoder. To deal with the disharmonious color, they propose an adaptive instance normalization. A survey of style transfer using transformers and diffusion models can be found in \\cite{ZHOU:Bridging:2025}. Implicit Neural Representations (INRs) are less commonly used in style transfer tasks due to the difficulty of modeling the cross-representation between style and content. Moon et al. \\cite{Moon:generalizable:2023} combined INRs with vision transformers for generalizable style transfer; however, the results remain limited in quality. In contrast, the method proposed by Kim et al. \\cite{Kim:Controllable:2024} uses multilayer perceptrons (MLPs) to map image coordinates to the colors of the stylized output, guided by features extracted from both the content and style inputs to allow controllability.\n\n\n\\subsubsection{Upscaling imagery: super-resolution (SR)}\n\nImpressive super-resolution (SR) results from transformer and diffusion models have been published extensively in the past few years. Originally, SR methods were developed using multiple low-resolution (LS) images, as different features in each image are combined to construct an enhanced one. However, these methods are not practical, as in most cases only one LS image is available. Hence, more methods have been developed for single image super-resolution (SISR).\n\nThe first use of a transformer, called ESRT, was for capturing long-term dependencies, such as repeating patterns in buildings. This was done in the feature domain extracted by a lightweight CNN module \\cite{Lu:Transformer:2022}, outperforming those that use only CNNs. Since then, most SISR methods have been based on transformers. The Hybrid Attention Transformer (HAT) \\cite{Chen:Activating:2023} was introduced, which improves the SR quality over ESRT by more than 2dB when upscaling 2$\\times$\\times-4$\\times$\\times. However, the NTIRE 2023 Real-Time Super-Resolution Challenge \\cite{Conde:Efficient:2023} showed that the winner, Bicubic++ \\cite{Bilecen:Bicubic:2023}, uses only convolutional layers and achieves the fastest speed at 1.17ms in upscaling 720p to 4K images. This method is significantly faster than any of the participants in the NTIRE 2025 Challenge \\cite{chen2025ntire}, where Transformer-based architectures continue to dominate as the mainstream approach.\n\nFor DMs, SR3 by Google \\cite{Saharia:image:2023} has produced truly impressive results. It operates by learning to transform a standard normal distribution into an empirical data distribution through a sequence of refinement steps, interpolating in a cascaded manner\u2014upscaling 4$\\times$\\times at a time. Later, IDM \\cite{Gao:Implicit:2023} combines INR with a U-Net denoising model in the reverse process of the DM. It is crucial to emphasize again that DMs are generative models. The SR results are generated based on the statistics we provide to the model during training (LR training samples). This is not for a restoration task, but rather for synthetic generation. A survey in SISR using DMs can be found in \\cite{moser:diffusion:2024}.\n\nFor video SR, numerous methods have emerged as part of a unified enhancement framework, as discussed in the previous section. One of the pioneering works to incorporate transformers specifically for video SR tasks is the Trajectory-aware Transformer for Video Super-Resolution (TTVSR) \\cite{Liu:Learning:2022}. Although the results are slightly inferior to those of BasicVSR++ \\cite{Chan:BasicVSR:2022}, which employs CNN and was introduced around the same time, both methods significantly enhance detail and sharpness compared to previous approaches, albeit not in real time. To address this limitation, the Deformable Attention Pyramid \\cite{Fuoli:Fast:2023} has been introduced, offering slightly lower quality but a speed-up of over 3$\\times$\\times. Recently, Adobe announced their VideoGigaGAN \\cite{xu:videogigagan:2024}, which can perform 8$\\times$\\times upsampling. This is achieved by adding flow estimation and temporal self-attention to the GigaGAN upsampler \\cite{kang:gigagan:2023}, which is primarily used for image SR, and text-to-image synthesis. Cao et al. \\cite{cao2025zero} introduce a zero-shot video super-resolution framework that leverages a pre-trained image diffusion model, and replaces the spatial self-attention layer with a novel short-long-range (SLR) temporal attention layer. Recently, SeedVR integrated text information (captions) into a Diffusion Transformer (DiT) model, achieving state-of-the-art performance in video super-resolution.\n\nCompared to traditional upscaling methods, generative AI can add details that did not exist in the original input image. These methods excel at generating high-quality natural images and structures, such as buildings, which are commonly included in training datasets. However, the process can be slow and may produce unpredictable results if the input image has very low resolution or contains content rarely seen in natural images. As shown in Fig. \\ref{fig:SR} (left), generative AI fails to upscale the knitting texture areas, instead generating lines more commonly found in typical images. While AI methods produce sharper edges, they perform less effectively on text.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/SR_results.png}\n    \\caption{(Left) Examples of SR ($\\times$4) using generative model. (Right) Real-time portrait editing with FacePoke.} \n    \\label{fig:SR}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/SR_results.png}\n    \\caption{(Left) Examples of SR ($\\times$4) using generative model. (Right) Real-time portrait editing with FacePoke.} \n    \\label{fig:SR}\n\n\n\\subsubsection{Restoration}\n\nIn our previous review paper \\cite{Anantrasirichai:AI:2022}, we categorized the work on restoration into several different types of distortions, including deblurring, denoising, dehazing, and mitigating atmospheric turbulence. Recent work however uses a unified network architecture to address these as inverse problems $y = hx + n$y = hx + n, where $x$x and $y$y are the ideal and observed data, respectively. $h$h is a degradation function, such as blur, and $n$n is additive noise. Often the super-resolution task is also considered as an inverse problem, meaning $h$h includes downsampling process. Note that although designed as a single network, the model is trained with each distorted dataset separately. \n\nThe pioneering transformer-based method for image restoration, SwinIR \\cite{Liang:SwinIR:2021}, employs several concatenated Swin Transformer blocks \\cite{Liu:Swin:2021}. SwinIR surpasses state-of-the-art CNN-based methods proposed up to the year 2021 in super-resolution and denoising tasks. The model is smaller and reconstructs fine details more effectively.\nOther two popular approaches that emerged in the same timeframe are Uformer \\cite{Wang:Uformer:2022} and Restormer \\cite{Zamir:Restormer:2022}. Both incorporate Transformer blocks into hierarchical encoder-decoder networks, employing skip connections similar to those in U-Net. Their objective was to restore noisy images, sharpen blurry images, and remove rain. The networks focused on predicting the residual $R$R and obtaining the restored image $\\hat{x}$\\hat{x} through $\\hat{x} = y + R$\\hat{x} = y + R. While their performance is very similar, Restormer has half the parameters of Uformer. More recent, GRL by Li et al. \\cite{li:GRL:2023} exploits a hierarchy of features in a global, regional, and local range using different ways to compute self-attentions as an image often show similarity within itself in different scales and areas. GRL outperforms SwinIR and Restormer. Additionally, Fei et al. introduced the Generative Diffusion Prior \\cite{Fei:Generative:2023} for unsupervised learning, aiming to model posterior distributions for image restoration and enhancement. VmambaIR \\cite{Shi:VmambaIR:2025} incorporates Mamba blocks into the U-Net architecture, achieving superior performance compared to SwinIR and Restormer in both visual quality and model size.\n\nFor video restoration, the general framework comprises frame alignment, feature fusion and reconstruction. The process could be similar to image restoration but input multiple frames and run through the sequences in sliding window manner to exploit temporal information of a number of consecutive frames. \nRecently, Video Restoration Transformer (VRT) \\cite{Liang:VRT:2024} and its improved version with recurrent process (RVRT) \\cite{liang:RVRT:2022}, have emerged as the state of the arts for video super-resolution, deblurring, denoising, and frame interpolation. This method introduces temporal reciprocal self-attention in the transformer architecture and parallel warping using MLP. These innovations enable parallel computation and outperform the previous state-of-the-art methods by up to 2.16dB on benchmark datasets. FMA-Net \\cite{Youk:FMA:2024} proposed multi-attention for joint Video super-resolution and deblurring, achieving fast runtime with nearly 40\\% improvement over RVRT, and the restored quality was reported better by up to 3\\%. \n\nFor audio restoration, most software discussed in Section \\ref{sssec:musicgen} offers tools for enhancing audio quality, such as eliminating background noise, echo, microphone rumble, and occasionally room reverberation, which have been well-established even before the advent of deep learning. There have been efforts to utilize AI for learning global contextual information to aid in the removal of unwanted sounds, leading to better final quality \\cite{Yu:DBT:2022}. The latest advancements in this domain are primarily focused on addressing issues where significant portions of the audio data are missing. For instance, Moliner et al. \\cite{Moliner:solving:2023} tackle problems such as audio bandwidth extension, inpainting, and declipping by treating them as inverse problems using a diffusion model. For a comprehensive survey on the use of diffusion models in restoration tasks, refer to \\cite{10902142}. \n\nThe following methods have been proposed for specific problems, but ideally, they should be adaptable for other tasks, even though they may not perform as well as they do for the original task.\n \ni) \\textbf{Deblurring}: A lightweight deep CNN model was recently proposed in \\cite{Pan:Deep:2023}, where a new discriminative temporal feature fusion has been introduced to select the most useful spatial and temporal features from adjacent frames. Feature propagation along the video is done in the wavelet domain. The deblurring performance is comparable to RVRT \\cite{liang:RVRT:2022}, but it is 5 times faster. DaBiT \\cite{Morris:DaBiT:2024} mitigates focal blur content with depth information and applies SR for further enhancing fine details.\nNote that not only in software, but AI technologies have also been integrated into hardware. This includes autofocus, which is crucial for capturing sharp images of subjects, especially in dynamic environments where manual adjustments are impractical due to rapid movement. AI-driven autofocus methods have emerged, often tailored for specific camera hardware. For instance, Choi et al. proposed an autofocus model optimized for dual-pixel Canon cameras \\cite{Choi:Exploring:2023}. Additionally, Yang et al. investigated the correlation between language input and blur map estimation, utilizing semantic cues to enhance autofocus performance \\cite{yang:ldp:2023}. Remarkably, their model achieves comparable results to previous state-of-the-art methods while being more lightweight \\cite{Yang:K3DN:2023}. Autofocus could be used in conjunction with real-time object tracking (see Section \\ref{sssec:tracking}) to produce desirable sharpness for moving objects in the video. Recently, Feng et al. \\cite{feng2025residual} proposed a novel residual diffusion deblurring framework that integrates a conditional diffusion model guided by a defocus map and incorporates residual learning into the single-image defocus deblurring process.\n\nii) \\textbf{Denoising}: SUNet \\cite{Fan:SUNet:2022} applies Swin transformer blocks combined in a UNet-like architecture. Denoising with diffusion models (DMs) \\cite{yang:realworld:2023} has been proposed by diffusing with estimated noise that is closer to real-world noise rather than Gaussian noise, achieving better performance than SwinIR \\cite{Liang:SwinIR:2021} and Uformer \\cite{Wang:Uformer:2022}. INR with complex Gabor wavelets as activation functions show promising denoising results \\cite{Saragadam:wire:2023}. The NTIRE 2025 Image Denoising Challenge \\cite{sun2025tenth} revealed that the top-performing methods combined transformer-based and convolutional network architectures. Similarly, recent advances in video denoising also adopt a hybrid approach that integrates both architectures \\cite{Jin:Masked:2025,  Yue:RViDeformer:2025}.\n\niii) \\textbf{Dehazing}: Vision transformers for single image dehazing was proposed in DehazeFormer \\cite{Song:vision:2023}. Similar to SUNet, it is UNet-like architecture, but introduced Rescale Layer Normalization for better suit on improving contrast. The Fast Fourier Transform (FFT) has been employed in \\cite{fang2025guided} due to the phase spectrum conveying more structural detail than the amplitude spectrum and demonstrating greater robustness to contrast distortion and noise. Then cross-attention between the RGB and YCbCr color spaces is applied. This approach achieves nearly 5 dB higher PSNR than DehazeFormer on a real-world smoke dataset.  For video dehazing, Xu et al. \\cite{Xu:Video:2023} introduced a recurrent multi-range scene radiance recovery module with the space-time deformable attention. They also employs physics prior to inform haze attenuation. This method outperforms DehazeFormer by approximately 1dB.\n\niv) \\textbf{Mitigating atmospheric turbulence}: Similar to dehazing, physics-inspired models have been widely developed to remove turbulence distortion \\cite{Jaiswal:Physics:2023,Jiang:NeRT:2023}, while complex-valued CNNs have been proposed to exploit phase information \\cite{Atmospheric:2023}. There was also an attempt to use instance normalization (INR) to address this issue, providing solutions for tile and blur correction \\cite{Jiang:NeRT:2023}. However, diffusion models have shown superior performance on single-image restoration tasks \\cite{Nair:AT-DDPM:2023}, while transformer-based methods remained the state-of-the-art for video restoration \\cite{Zhang:Image:2024, zou2024deturb}. More recently, Mamba-based architectures have demonstrated their effectiveness in both visual quality and model efficiency \\cite{hill2025mamat}. A recent review can be found in \\cite{Hill2025}.\n\n\\subsubsection{Inpainting}\n\\label{ssec:inpaiting}\n\nVisual inpainting is the process of filling in lost or damaged parts of an image or video. CNNs and GANs have already achieved impressive results (see our previous review paper \\cite{Anantrasirichai:AI:2022}). Recent work has focused more on editing rather than simply filling in the missing areas. This means users can now mask large areas of an image, and AI tools generate multiple results for users to choose from, a technique known as pluralistic inpainting \\cite{zheng:pluralistic:2019}. Some notable methods include the following: Mask-Aware Transformer (MAT) \\cite{Li:MAT:2022} offers several outputs to fill a large missing area, consisting of a convolutional head, a transformer body, and a convolutional tail for reconstruction, along with a Conv-U-Net for refinement. PUT \\cite{Liu:Reduce:2022} proposes a patch-based vector VQ-VAE and unquantized Transformer to minimize information loss. Spa-former \\cite{HUANG:Sparse:2024} employs a UNet-like architecture, where each level performs transformer with sparse self-attention to remove coefficients with low or no correlation, leading to memory reduction, while improving result quality by up to 5\\% compared to PUT.\n\nVideo inpainting presents greater complexity compared to image inpainting, despite the abundance of information available in an image sequence. The process typically involves tracking masks across frames, estimating optical flow, and ensuring temporal consistency.  The current state-of-the-art methods include DLFormer \\cite{Ren:DLFormer:2022} and  ProPainter \\cite{Zhou:ProPainter:2023}. DLFormer conducts inpainting in latent space and utilizes discrete codes for video representation. On the other hand, ProPainter employs flow-based deformable alignment to enhance robustness to occlusion and inaccurate flow completion. The method excels in filling complete and rich textures, achieving a speed of 12 fps for full HD video. Video inpainting is also used for dubbing. DINet \\cite{Zhang:DINet:2023} replaces the mouth area to synchronize with a new language being spoken.\n\nA comprehensive survey of learning-based image and video inpainting, covering approaches such as CNNs, VAEs, GANs, transformers, and diffusion models, can be found in \\cite{quan:deep:2024}. Additionally, Elharrouss et al. \\cite{elharrouss2025transformer} provide an in-depth review of the current challenges and future directions specific to transformer-based inpainting techniques.\n\n\\subsubsection{Image Fusion}\n\\label{sssec:fusion}\n\nImage fusion is the process of  merging multiple images from either the same source (such as varying focal points or exposures) or different modalities (e.g. visible and infrared cameras) into a single image. This process integrates complementary information from the various images to enhance overall quality, improve interpretation, and increase the usability of the final image.\n\nTransformers and CNNs have been combined to extract global and local information, respectively. Most methods use CNNs for feature extraction, with transformers operating in the latent space \\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023}. Notable methods include SwinFusion \\cite{Ma:SwinFusion:2022}, which utilizes a self-attention-based intra-domain fusion unit and a cross-attention-based inter-domain fusion unit to achieve multi-modal and digital photography image fusion. Transformer-based image fusion has also been applied to downstream tasks like segmentation \\cite{Liu:Multi:2023}, achieving superior results by leveraging the additional information. Self-attention blocks are employed to enhance intra-feature representations, while the cross-attention mechanism integrates inter-feature information to improve the quality of the fused output \\cite{LI2024102147}.\n\nDDFM, the first diffusion model-based image fusion method, estimates noise in the reverse process by combining multiple inputs \\cite{Zhao:DDFM:2023}. The expectation-maximization (EM) algorithm is integrated to estimate the noise distribution parameters, resulting in sharper images compared to traditional DDPM. For an in-depth review, the reader is referred to recent work in \\cite{Karim:Current:2023, Zhang:Visible:2023}.\n\n\\subsubsection{Editing and Visual Special Effects (VFX)}\n\nEditing or modifying specific areas of an image is much easier with DM technologies, particularly for headshot photos, such as targeting the eyes and mouth on the face \\cite{guo2024liveportrait}. This capability has been extended to video generation (see Section \\ref{sssec:videogen}). Fig. \\ref{fig:SR} shows an example of the online tool, FacePoke\\footnote{\\url{https://huggingface.co/spaces/jbilcke-hf/FacePoke}}, which allows users to move the head and modify the shapes of the eyes and mouth in real time. Motion-I2V \\cite{Shi:Motion-I2V:2024} provides motion blur and motion drag tools to control specific areas of an image to add motion. The method is based on a diffusion-based motion field predictor and motion-augmented temporal attention.\n\nVFX aims to create and/or manipulate imagery outside the context of a live-action shot in filmmaking and video production. When adding objects, scenes, and effects into traditional photographic videos, generative AI has obviously become an important tool, but some manual operations are still required. For example, in After Effects (EA)\\footnote{\\url{https://www.adobe.com/uk/products/aftereffects.html}}, the user selects the area where the object will be added and uses text prompts to describe such object. Subsequently, with the current EA version, the user will need to apply motion tracking so the generated object is moved accordingly.\n\nAI technologies can upscale, enhance, and restore low-quality or old footage. For example, standard definition videos can be converted to high definition or even 4K quality without traditional manual remastering processes. This is particularly useful for remastering old movies or enhancing visual details in scenes. Generative AI has also simplified and accelerated automated processes, such as rotoscoping \\cite{Tous:Lester:2024}, an animation technique where animators trace over motion picture footage frame by frame to create realistic action. AI models can accurately detect and segment objects and characters in video frames, significantly speeding up the post-production process. Additionally, AI can assist the rapid creation of 3D models from 2D images generating realistic animations with minimal input data, facilitating complex human motions and synchronized facial expressions to voiceovers. One restriction is that current technologies still cannot yet generate full 4K accurate visual effects.\n\n\n\n\n\n\\subsection{Information Extraction and Understanding}\n\\label{ssec:infoextract}\n\nAI plays a crucial role in automating and optimizing the process of information extraction and understanding, enabling organizations to derive actionable insights from large and diverse data. Yan et al. \\cite{Yan:Universal:2023} have categorized information extraction tasks based on the Format-Time-Reference space, as illustrated in Fig. \\ref{fig:segmentation} (a), where object detection and video object segmentation (VOS) are considered to be the simplest and the most complex tasks, respectively. Recent advancements in this field draw significant inspiration from LLMs. These advancements include the utilization of prompts as conditional inputs for acquiring information. Moreover, following the pipeline approach used in LLMs, there is a growing trend towards leveraging very large datasets to pre-train models before fine-tuning them for specific downstream tasks. For instance, Meta AI \\cite{Oquab:DINOv2:2024} has introduced DINOv2, aimed at enriching information about visual content through self-supervised learning. This model was trained with 142 million carefully selected images, employing the ViT architecture. Google have introduced VideoPrism \\cite{zhao:videoprism:2024}, a tool for scene understanding including classification, localization, retrieval, captioning, and question answering (QA). The model was trained on an extensive and diverse dataset consisting of 36 million high-quality video-text pairs and 582 million video clips accompanied by noisy or machine-generated parallel text.\n\n\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation.png}\n    \\caption{(a) Tasks in Object-centric understanding defined by Yan et al. \\cite{Yan:Universal:2023} (REC:,  Referring Expression Comprehension, RES: Referring Expression Segmentation, VOS:  Video Object Segmentation, RVOS: Referring Video Object Segmentation, MOT: Multiple Object Tracking, MOTS: Multi-Object Tracking and Segmentation, VIS: Video Instance Segmentation, SOT: Single Object Tracking. (b) Current high-quality segmentation \\cite{Ke:SAM-HQ:2023}.}\n    \\label{fig:segmentation}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation.png}\n    \\caption{(a) Tasks in Object-centric understanding defined by Yan et al. \\cite{Yan:Universal:2023} (REC:,  Referring Expression Comprehension, RES: Referring Expression Segmentation, VOS:  Video Object Segmentation, RVOS: Referring Video Object Segmentation, MOT: Multiple Object Tracking, MOTS: Multi-Object Tracking and Segmentation, VIS: Video Instance Segmentation, SOT: Single Object Tracking. (b) Current high-quality segmentation \\cite{Ke:SAM-HQ:2023}.}\n    \\label{fig:segmentation}\n\n\n\\subsubsection{Segmentation}\n\\label{sssec:seg}\n\nThe need for segmentation has grown dramatically in the past few years, given its central role in visual perception. Many segmentation methods now integrate an input prompt for users to define their preferred output appearances, such as pixel-wise segmentation, bounding boxes around objects, or segmented areas of interest. Most of these methods utilize transformer architectures \\cite{Bowen:Marked:2022}. Among them, Segment Anything (SAM) by Meta AI \\cite{Kirillov:SAM:2023} stands out as a pioneer in promptable segmentation approaches. This method computes masks in real-time and has been trained with over 1 billion masks across 11 million images, facilitating transferability from zero-shot to new image distributions and tasks. HQ-SAM \\cite{Ke:SAM-HQ:2023} enhances SAM by incorporating global-local feature fusion, leading to high-quality mask predictions. SegGPT \\cite{Wang:SegGPT:2023} proposed context ensemble strategies and allows users to tune a prompt for a specific dataset, scene, or even a person, while SEEM \\cite{Zou:Segment:2023} provides a completely promptable and interactive segmentation interface. More recently, SAM2 \\cite{ravi2024sam2} introduced support for real-time video segmentation. It is a unified model trained on a larger dataset than SAM. Interactive tools enable users to mark areas of interest and specify regions to exclude from the segmentation map. Zhou et al. propose an audio-visual segmentation (AVS) to generate pixel-level segmentation masks for sounding objects in audible videos. DVIS++ by \\cite{Zhang:DVISp:2025} introduces a universal video segmentation framework capable of producing instance, semantic, and panoptic segmentation outputs. This transformer-based architecture comprises a segmentor, tracker, and refinement module, achieving state-of-the-art performance across several video segmentation benchmarks.\n\nWith DMs tehchnologies, Baranchuk et al. \\cite{Baranchuk:label:2022} have investigated semantic representation, and found DMs outperform other few-shot learning approaches. DiffuMask \\cite{Wu:DiffuMask:2023} automatically generate image and pixel-level semantic annotation using pre-trained Stable Diffusion with input as a text prompt. It has been proven that using these synthetic data improve segmentation accuracy. Currently, the state-of-the-art panoptic segmentation is the method developed by Nvdia, which is based on text-to-image DMs \\cite{Xu:Open:2023}, outperforming the previous methods by up to 7.6\\%.\n\nApplying INRs to segmentation is more popular in the medical domain, as the specific signals used, such as computed tomography (CT) and magnetic resonance imaging (MRI), can be formulated as continuous functions. In creative technologies, unsupervised domain adaptation (UDA) and INRs are used for continuous rectification function modeling in \\cite{Gong:Continuous:2023}, achieving superior segmentation results in night vision. Recently, this work has been integrated with a non-local means block in \\cite{Lin:Feature:2024}  showning a significant improvement for instant segmentation in low-light scenes.\n\n3D segmentation is also crucial for scene manipulation. In radiance fields, earlier segmentation methods required additional modules such as using k-means clustering to separate objects from the background \\cite{Goel:Interactive:2023}. However, the recent SA3D approach \\cite{Cen:Segment:2023} segments 3D objects using NeRFs as the structural prior. SA3D operates by taking a trained NeRF and a set of prompts from a single view, then performing an iterative procedure. This involves rendering novel 2D views, self-prompting SAM for 2D segmentation, and projecting the segmentation back onto 3D mask grids. A comprehensive survey of 3D segmentation in computer vision can be found in \\cite{HE2025102722}.\n\n\\subsubsection{Detection and recognition}\n\\label{sssec:recog}\n\nIntroduced in 2020, DETR by Facebook AI \\cite{Carion:DERT:2020} was one of the first to adopt a transformer architecture for object detection. The approach achieves comparable results to an optimized Faster R-CNN \\cite{Ren:Faster:2027}, introduced in 2015. Deformable convolution has alson been used, (Deformable DETR \\cite {Zhu:Deformable:2021}), resulting in training faster with approximately 5\\% accuracy improvement.\nA survey until 2022  \\cite{Zou:object:2023} reported that Deformable DETR and Swin Transformers \\cite{Liu:Swin:2021} outperform pure CNN-based YOLOv4 \\cite{bochkovskiy2020yolov4}. \nSwinV2 improves the first version by replacing original dot product attention with scaled cosine attention, improving accuracy by approximately 5\\%. Later, RT-DETR \\cite{lv2:detrs:2024} improved inference speed by decoupling the intra-scale interaction and cross-scale fusion of features with different scales. RT-DETR is 25\\% faster than YOLOv8\\footnote{\\url{https://github.com/ultralytics/ultralytics}} with 6\\% improvement on MS COCO Object Detection dataset. Recently, YOLOv10 \\cite{wang:yolov10:2024} has been released. YOLOv10 further improves the speed of detection approximately by 30\\% over RT-DETR with the same accuracy. A review of transformer-based methods for object detection can be found in \\cite{Li:Transformer:2023, KHEDDAR2025103347}. Recently, YOLO12 \\cite{tian2025yolov12} introduced an attention-centric architecture, achieving a 2.1\\% and 1.2\\% mAP improvement over YOLOv10-N and YOLOv11-N respectively, with only a slight decrease in speed.\n\nTo detect 3D objects, the transformer-based method MonoDTR \\cite{Huang:MonoDTR:2022} incorporates depth estimation from a single 2D image \\cite{Yang:depthanything:2024} to predict 3D bounding boxes. More 3D object detection methods have been developed for autonomous driving \\cite{10637966}; however, these approaches can also be adapted for AR and VR applications \\cite{im2025gate3d}.\n\nWhile DMs are primarily used to generate synthetic datasets \\cite{Wu:datasetDM:2023, Fang:Data:2024}, they have also been demonstrated to function as zero-shot classifiers by Li et al. \\cite{Li:Your:2023}. DMs are also of interest for detection tasks, Although feature extractors are still predominantly based on CNNs, such as ResNet, or Transformers (like Swin). DiffusionDet \\cite{Chen:DiffusionDet:2023} formulates object detection as a denoising diffusion process from noisy boxes to object boxes, reporting performance that surpasses DETR. DMs have also been employed for anomaly detection \\cite{Zhang:DiffAD:2025, WU2025102965}, functioning similarly to zero-shot classifiers.\n\n\n\\subsubsection{Tracking}\n\\label{sssec:tracking}\n\nObject tracking stands out as one of the tasks that greatly benefits from transformers since \\textit{attention} is needed in both space and time. An experimental survey cited in \\cite{Kugarajeevan:Transformers:2023} reveals that transformer-based methods consistently rank at the top of the leaderboard across various datasets. In the Visual Object Tracking (VOT) challenges of 2023\\footnote{https://eu.aihub.ml/competitions/201\\#results}, all of the top-10 employed transformer-based methodologies. The highest-performing approach achieved a 10\\% improvement in tracking quality compared to the winner in 2020. The current state-of-the-art for single-object tracking\\footnote{https://paperswithcode.com/sota/visual-object-tracking-on-lasot}, however, is based on cross-attention and Mamba \\cite{kang2025exploring}.\n\nThe first three tracking-by-attention approaches are TrackFormer \\cite{Meinhardt:TrackFormer:2022}, MixFormer \\cite{cui:mixformer:2022}, and ToMP \\cite{Mayer:Transforming:2022}. TrackFormer extracts visual features using a CNN-based encoder, which are then tracked using a vanilla transformer \\cite{Vaswani:attention:2017} in a frame sequence, while MixFormer introduces cross-attention between the target and search regions. ToMP tracks the objects using prediction aspects. Many more methods have been proposed, including SeqTrack \\cite{Chen:SeqTrack:2023} and Track Anything Model (TAM) \\cite{yang:track:2023}. SeqTrack extracts visual features with a bidirectional transformer, while the decoder generates a sequence of bounding box values autoregressively with a causal transformer. TAM combines SAM \\cite{Kirillov:SAM:2023} and XMem \\cite{cheng:xmem:2022}, offering tracking and segmentation performance on the human-selected target. However, the masked area is still not very sharp, and there is a subtle degree of temporal inconsistency. MOTRv2 \\cite{Zhang:MOTRv2:2023} combines YOLOX \\cite{ge:yolox:2021} for object recognition and MOTR \\cite{zeng:motr:2022} for tracking, outperforming TrackFormer by 20\\%. Additionally, some methods have been specifically proposed for challenging environments, such as low light \\cite{Yi:Comprehensive:2024} and small objects, as seen in AnyFlow \\cite{Jung:AnyFlow:2023}. The latter exploits INR to upsample a continuous coordinate-based flow map, similar to SISR technique proposed in \\cite{Chen:Learning:2021}.\n\nSimilarly to detection tasks, DMs for tracking tasks are used as downstream processes by concatenating the diffusion head to the feature extraction backbone. However, a spatial-temporal fusion module has been added to the diffusion head to exploit temporal video features\\cite{Luo:DiffusionTrack:2024}. DiffusionTrack \\cite{Xie:DiffusionTrack:2024} localizes the target in a progressive diffusion manner, which is claimed to better handle challenging scenarios. The method in \\cite{Zhang:DiffusionTracker:2024} exploits spatial-temporal weighting to suppress the probability of the tracker changing the target to the distractors. It, however, reports under-performance compared to  MixFormer.\n\n\n\n\\subsection{3D Reconstruction and Rendering}\n\\label{ssec:3Dreconstruct}\n\nBridging the gap between digital and physical realms, 3D reconstruction and rendering are integral to various creative technologies.  In film and animation, they enable the creation of detailed digital models that blend seamlessly with live-action footage. Video games and digital twins leverage these technologies for dynamic environmental rendering. VR and AR use 3D reconstruction to create immersive and interactive experiences, with AR integrating digital content into real-world contexts. With recent AI technologies, 3D reconstruction and rendering have become faster and closer to reality. In particular, neural radiance fields and Gaussian Splatting enable artists and film producers to create shots that cannot be one in the real shooting environments.\n\n\\subsubsection{Depth Estimation}\n\\label{sssec:depth}\n\nAccurate depth information (alongside texture data) is typically required to construct 3D models. Depth sensors, such as lidar (Light Detection and Ranging) and structured-light 3D scanners, can be used for this purpose, but their applications are often limited by distance and cost. Consequently, vision-based sensors have become widely used. These sensors utilize two or more cameras to simulate human binocular vision or employ a single camera to capture images from different locations.\n\nAs deep learning can capture monocular cues such as object size, texture gradients, and perspective, depth estimation from a single image can produce accurate results. There have been attempts to use transformers, such as \\cite{Zhang:Lite:2023} and \\cite{Chen:Vision:2023}, and diffusion models, such as \\cite{Ji:DDP:2023} and \\cite{Ke:Repurposing:2024}. Amongst these, Depth Anything v2 \\cite{Yang:depthanythingv2:2024} has become a state-of-the-art monocular depth estimation method. It is built on the previous version \\cite{Yang:depthanything:2024}, jointly trained on large-scale labeled and unlabeled images and uses semantic priors from pretrained encoders. Depth Anything v2 significantly outperforms V1 in fine-grained details and robustness by using synthetic images and pseudo-labeled real images, as well as by extracting intermediate features from DINOv2 \\cite{Oquab:DINOv2:2024}, which is trained with vision transformers. One of the notable capabilities of Depth Anything v2 is its ability to predict depth of transparent and reflective surfaces.\n\n\\subsubsection{Neural Radiance Fields}\n\\label{sssec:nerf}\n\nNeural Radiance Fields (NeRFs), introduced in \\cite{Mildenhall:NeRF:2020}, have demonstrated the ability to learn a 3D scene from a smaller number of images captured from various viewpoints, as opposed to photogrammetry. They excel in neural rendering, particularly in view-dependent novel view synthesis, and have effectively tackled several challenges associated with automated 3D capture \\cite{xie2022neural}, such as accurately representing the reflectance properties of the scene. NeRFs offer high-resolution photo-realistic novel views and flexibility in postprocessing. They have hence gained significant attention in cinematography \\cite{Azzarelli2024}, as they offer reduced time and cost, particularly for outdoor shooting.\n\nIn the NeRF process (see Fig. \\ref{fig:3Drepresentation} (a)), the camera positions and orientations are typically estimated from a series of 2D images using techniques like feature-mapping and Structure-from-Motion (SfM), as demonstrated in \\cite{schoenberger:sfm:2016}. Leveraging INR, each image (or camera pose) is mapped into camera rays that traverse the scene, generating 3D points with directional radiance (towards the camera). These points are then processed by an MLP to predict volume density and emitted radiance. Subsequently, volume rendering techniques are employed to generate an image, which is compared with the original via loss calculation. The MLP iteratively refines the model by minimizing this loss.\n\nSince their introduction, there have been many variants of NeRFs aimed at improving their performance. Mip-NeRF360 \\cite{Barron:Mip-NeRF360:2022} proposed unbounded anti-aliased technique achieving full 360 degree content. Google Research\n\\cite{Mildenhall:NeRFDark:2022} trains NeRF from noisy RAW images captured in the dark scene, allowing changing viewpoint, focus, exposure, and tone mapping simultaneously. With segmentation techniques significantly advanced (see Section \\ref{sssec:seg}), there have been integrations utilizing semantic segmentation to enhance 3D representation \\cite{Guo:neural:2022}. DSEM-NeR \\cite{LIU:DSEM:2025} integrates the pretrained CLIP model to extract multimodal features\u2014including color, depth, and semantics\u2014from multi-view 2D images, thereby enhancing the reconstruction quality of complex scenes.\n\nWhile the rendering quality of NeRF is very good, training and rendering times remain extremely high. The Instant-NGP tool developed by Nvidia \\cite{mueller:instant:2022} enables real-time training of NeRFs by bypassing sampling in empty spaces and dense areas, and by incorporating multi-resolution hash encoding techniques. These advancements substantially reduce the computational burden associated with representing high-resolution image features -- training times have been reduced from hours to just a few seconds. Moreover, it offers VR controls for immersive 3D rendering experiences using OpenXR\\footnote{An open-source, royalty-free standard for access to virtual reality and augmented reality platforms and devices. \\url{https://www.khronos.org/openxr/}}. This allows users to navigate scenes, manipulate objects, and interact with the environment directly through VR headsets. Diffusion models are integrated to regularize NeRF reconstructions \\cite{wynn:diffusionerf:2023}, resulting in smoother depth continuity and clearer edges where depth discontinuities occur.\n\nThe initial application of NeRFs to dynamic scenes was undertaken by Pumarola et al. \\cite{pumarola:DNeRF:2020}, known as D-NeRF.\nHowever, the current leading method for generating high-quality novel views of real dynamic scenes is TiNeuVox \\cite{Fang:Fast:2022}.  It enhances temporal information by interpolating voxel features before feeding them into the radiance network to estimate density and color, similar to ordinary NeRF. DynVideo-E~\\cite{Liu_2024_CVPR} adds an MLP to predict motion fields but focuses on human-centric content. PaReNeRF~\\cite{Tang_2024_CVPR} addresses large-scale dynamic scenes using patch-based sampling. The main drawback of these methods is the large model size and/or long training time. Therefore, $K$K-planes \\cite{Fridovich:kplanes:2023} propose a simple planar factorization for volumetric rendering, achieving low memory usage (1000$\\times$\\times compression over a full 4D grid). Wavelet transform are employed in \\cite{azzarelli:waveplanes:2023} to further reduce model size. KFD-NeRF~\\cite{zhan2024kfd} incorporates a Kalman filter-guided deformation field for more accurate motion estimation.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/3Drepresentation.png}\n    \\caption{3D representation. (a) Neural Radiance Fields (NeRFs) \\cite{Mildenhall:NeRF:2020}. (b) Gaussian Splatting \\cite{kerbl:3Dgaussians:2023}. (c) Example scenes of VR-GS system for 3D content interaction in VR \\cite{jiang:vrgs:2024}. }\n    \\label{fig:3Drepresentation}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/3Drepresentation.png}\n    \\caption{3D representation. (a) Neural Radiance Fields (NeRFs) \\cite{Mildenhall:NeRF:2020}. (b) Gaussian Splatting \\cite{kerbl:3Dgaussians:2023}. (c) Example scenes of VR-GS system for 3D content interaction in VR \\cite{jiang:vrgs:2024}. }\n    \\label{fig:3Drepresentation}\n\n\n\\subsubsection{3D Gaussian Splatting}\n\\label{sssec:3DGS}\n\nThe main issue with NeRFs as a method to generate high-quality novel views is training time, which can exceed a day for high-resolution content on a single RTX 3090 GPU~\\cite{9879447}. 3D Gaussian Splatting (3D-GS) \\cite{kerbl:3Dgaussians:2023} has been introduced to address this, using anisotropic 3D Gaussians to form a high-quality, unstructured representation of radiance fields. The process estimates a sparse point cloud through SfM. Each point possesses 3D Gaussian properties, such as position, covariance matrix, opacity, and spherical harmonics coefficients representing colors. The optimization of these parameters is interleaved with steps that control the density of the Gaussians to better represent the scene, as shown in Fig. \\ref{fig:3Drepresentation} (b). A survey of 3D-GS can be found in \\cite{10521791}.\n\nIn contrast to traditional NeRFs based on implicit scene representations, 3D-GS provides an explicit representation that can be seamlessly integrated with post-processing manipulations, such as animating and editing. VR-GS \\cite{jiang:vrgs:2024} offers intuitive and interactive physics-based game-play with deformable virtual objects and realistic environments represented with 3D-GS. The example scenes are shown in Fig.~\\ref{fig:3Drepresentation} (c). Physics-inspired approaches are also integrated to improve 3D modeling in different media, such as 3D underwater scenes \\cite{Wang2025}.\n\nFor dynamic scenes, 4D Gaussian Splatting (4D-GS) \\cite{wu:4dgaussians:2024} introduces a Gaussian deformation field for motion and shape. It exploits a multi-resolution encoding method, achieving real-time rendering of up to 82 fps at a resolution of 800$\\times$\\times800 pixels on an RTX 3090 GPU. Instead of developing in 4D, CoGS \\cite{Yu:CoGS:2024} exploits 3D-GS by integrating control mechanisms in separate regions to learn individual temporal dimensions. SC-GS \\cite{Huang:SCGS:2024} extracts sparse control points and uses an MLP to predict time-varying 6 DoF transformations. While the results show better visual quality than 4D-GS and CoGS, the performance heavily relies on camera pose estimation. Kong et al. \\cite{kong2025efficient} represent dynamic scenes using sparse, time-variant attribute modeling with a deformable MLP, while efficiently filtering out anchors corresponding to static regions. Their model achieves fast rendering speeds of over 110 FPS at a resolution of 960$\\times$\\times540\u2014nearly 10 times faster than SC-GS\u2014and delivers a 1 dB improvement in PSNR.\n\nLUMA AI\\footnote{\\url{https://lumalabs.ai/interactive-scenes}} and \nPolycam\\footnote{\\url{https://poly.cam/captures}} offer free tools for Gaussian splatting and photogrammetry creation for non-commercial use. The 3D objects created can be experienced with VR headsets for more immersive 3D and further used or developed in other applications. However, these tools have limitations in handling dynamic scenes due to occlusions, sparse observations per timestamp, and object reappearances over time. Rendering dynamic avatars can produce higher quality results by incorporating additional information. For example, EVA \\cite{junkawitsch2025eva} disentangles the 3D Gaussian appearance into skeletal motion, facial expressions, body movements, and skin. These components are then splatted to render the final photorealistic image.\n\n\\subsubsection{Digital Twins}\n\\label{sssec:digital_twins}\n\nA digital twin is a virtual replica of a physical object, system, or process, continuously updated with real-time data for purposes such as simulation, testing, monitoring, and maintenance. This technology is increasingly adopted across various applications within the creative industries. For example, in product design and branding, it enables immediate observation of how a design performs in various contexts, facilitating the development of user-friendly products. Unilever reported that integrating digital product twins with 3D technologies, such as NVIDIA Omniverse\\footnote{\\url{https://www.nvidia.com/en-gb/omniverse/}}, enabled the creation of product imagery twice as fast and 50\\% more cost-effective\\footnote{\\url{https://www.unilever.com/news/press-and-media/press-releases/2025/unilever-reinvents-product-shoots-with-digital-twins-and-ai/}}. Digital twins also allow consumers to explore products or spaces virtually, simulating real-world interactions. \n\nAccenture plc, a global professional services company, collaborated with Walt Disney Studios to develop digital twin technologies aimed at transforming the filmmaking process\\footnote{\\url{https://www.accenture.com/mx-es/case-studies/communications-media/empowering-film-creatives-digital-twins}}. Their goal is to generate remotely accessible 3D models, enabling virtual exploration of potential shooting locations without requiring physical visits. The Virtual StudioLAB provides a digital replica created using 360-degree imagery and 3D modeling. These innovations have streamlined pre-production workflows for major productions from Marvel Studios and 20th Century Studios.\n\nDigital representations such as avatars, proxies, and digital twins are increasingly being explored in artistic contexts, particularly in relation to identity, presence, and embodiment in virtual environments. The Tate Modern\u2019s film programme Avatars, Proxies and Digital Twins (Feb\u2013May 2025) investigated these themes through curated audiovisual works, offering critical reflections on digital personhood. By engaging with diverse narrative forms, the programme highlighted the sociocultural implications of digital self-representation, prompting discourse on authenticity, agency, and the role of immersive media in shaping future human\u2013machine interaction.\n\n\\subsection{Data Compression}\n\\label{ssec:compression}\n\nData compression plays an important role in the delivery of creative content to audiences, effectively reducing memory and bandwidth requirements during signal storage and transmission \\cite{Bull:intelligent:2021}. Although coding methods based on conventional signal processing theories are still widely employed in most standards and application scenarios, learning-based solutions have emerged in research, showing great potential to achieve competitive performance in recent years. This subsection provides a brief overview of the recent advances in image, video, and audio compression, in particular focusing on the approaches proposed after 2021.\n\n\\subsubsection{Image Compression}\n\nSince the first neural image codec \\cite{balle2016density} was proposed in 2016, numerous learning-based image compression methods have been developed, with significant performance improvements reported \\cite{balle2018variational,cheng2020learned}. Driven by the latest advances in neural network architectures, neural image codecs now outperform standard image codecs. Instead of using CNNs as the basic network structure, transformer-based architectures have become popular, offering the potential for  better compression efficiency.  Notable examples include SwinT-ChARM \\cite{zhu2022transformer}, STF \\cite{zou2022devil} and LIC-TCM \\cite{liu2023learned}. SwinT-ChARM \\cite{zhu2022transformer} employs Swin transformers for non-linear transforms and outperforms the latest standard image codec, the Versatile Video Coding (VVC) Test Model (VTM, All Intra). STF \\cite{zou2022devil} is based on a symmetrical transformer framework containing absolute transformer blocks in both the down-sampling encoder and the up-sampling decoder, which also shows improved rate-quality performance over VTM. LIC-TCM \\cite{liu2023learned} exploits the local modeling ability of CNN and the non-local modeling performance of transformers, and proposes a parallel transformer-CNN mixture block. This new network structure, together with a channel-wise entropy model based on attention modules using Swin transformers, contributes to the superior performance of STF, with a more than 10\\% bitrate saving over VTM. \n\nAn alternative approach to learned image coding is based on advanced generative models. Early works \\cite{agustsson2019generative,mentzer2020high} employed GANs to generate more photo realistic results with improved visual quality. Although these models fail to outperform conventional, CNN-based or transformer-based approaches, when distortion-based quality metrics, e.g., PSNR, are used for performance evaluation, they have been reported to perform well when perceptual quality models, such as MS-SSIM \\cite{Bovik_MSSSIM} and VMAF \\cite{VMAFblog}, or subjective tests are employed to measure perceived video quality. More recently, diffusion models have been applied in image compression to allow realistic reconstruction at ultra-low bitrates \\cite{careil2023towards} achieving competitive performance compared to GAN-based models \\cite{yang2024lossy}. However, it should be noted that some of these generative models aim to generate (or synthesize) images with ``perfect realism'' rather than reconstruct results which are most similar to the original content. Notable work in this category includes image codecs using score-based generative models \\cite{hoogeboom2023high} and the diffusion-based residual augmentation codec (DIRAC) \\cite{ghouse2023residual}. Moreover, another type of generative model based on INR has been employed for image compression; this learns a mapping between the spatial coordinates and the respective pixel values for the input image. The learned INR model is then compressed through parameter quantization and model compression to minimize the required bitrate. Notable INR-based image codecs include COIN/COIN++ \\cite{dupont2021coin,dupontcoin++} and \\cite{strumpler2022implicit} that combine SIREN networks \\cite{sitzmann2020implicit} with positional encoding.   \n\nIn order to evaluate and compare neural image codecs under fair test conditions, public grand challenges have been increasingly run,  typically associated with international conferences. One of the most well-known of these is the Challenge on Learned Image Compression (CLIC) \\cite{clic}. In its latest competition, the best performing learned image codec \\cite{li2024semantic}, which is based on a GAN-enhanced Vector Quantized Variational AutoEncoder (VQ-VAE) framework, offered up to 0.6dB PSNR gain over VTM (version 22.2, All Intra) at similar bitrates; this codec is based on an autoencoder architecture with latent refinement and perceptual losses. \n\nTo support the deployment of neural image codecs, the International Organization for Standardization (ISO)/International Electrotechnical Commission(IEC) has developed a royalty-free learned image coding standard, denoted as JPEG AI \\cite{ascenso2023jpeg}, which aims to offer significant performance improvement over existing standards for both human and machine vision tasks. The Call for Proposals of JPEG AI was published in 2022, while the Working Draft and the Committee Draft outlining its core coding system were released in 2023 \\cite{JPEGAIN100634}, with its first version published in October 2024 \\cite{JPEGAIN100634}. JPEG AI follows the same framework (the auto-encoder structure) as most existing neural image codecs, and its test model JPEG AI VM (version 4.3) has been reported to achieve up to 28.5\\% coding gains over VVC VTM (All Intra mode) \\cite{JPEGAIM101081}.\n\n\\subsubsection{Video Compression}\n\nCompared to image coding, the compression of video content is a much more challenging task, particularly for immersive video formats and diverse content types. Although video coding standards including H.264/AVC (Advanced Video Coding), H.265/HEVC (High Efficiency Video Coding) and H.266/VVC (Versatile Video Coding) are still predominant in real-world applications, learning-based video coding has advanced dramatically in the past five years, with new deep learning enhanced conventional coding tools and end-to-end optimized neural video coding frameworks proposed.    \n\ni) \\textbf{The enhancement of conventional coding tools} focuses on employing deep learning techniques to improve the performance of one (or multiple) coding modules in a standard-applicant codec. These modules include intra prediction \\cite{li2021deepqtmt}, inter prediction \\cite{jin2021deep}, in-loop filtering \\cite{feng2024low}, post filtering \\cite{zhang2023wcdann} and resolution re-sampling \\cite{wang2023compression}. To facilitate efficient integration, the MPEG Joint Video Experts Team (JVET)  built a test model in 2022 based on VTM 11, named Neural Network-based Video Coding (NNVC) \\cite{li2023designs}, with its latest version NNVC-7.1 containing two major learning-based coding tools, neural-network based intra prediction and in-loop filtering, which has achieved an up to 13\\% coding gain over VTM 11 (Random Access mode) \\cite{JVET-AG0014}. However, this learning-based codec requires much higher computational complexity (up to 477 kMACs/pixel) and high-spec GPU support compared to conventional codecs. Meanwhile, members of the Alliance of Open Media (AOM) have also developed multiple CNN-based coding tools for the next generation of video coding standard beyond AV1. The latest proposals focus on the trade-off between performance and complexity, with one of them based on inloop filtering and super-resolution, which achieves an average BD-rate saving of 3.9\\% (in PSNR) over AVM, the test model of AV2, but only requires a much lower computational complexity (below 1.5kMACs/pixel) \\cite{joshi2023switchable}. More recently, research has been conducted to further improve the performance of these learning-based coding tools utilizing more advanced network architectures, including ViTs \\cite{kathariya2023joint}, and diffusion models \\cite{li2024extreme}. There are also investigations on applying preprocessing before compression \\cite{chadha2021deep,tan2024joint}, where the training of the deep preprocessors is based on proxy video codecs and/or rate-distortion loss functions to simulate the behavior of conventional video coding algorithms.\n\nii) \\textbf{End-to-end optimized neural video codecs.} Alongside the enhancement of coding tools in conventional video codecs, more recent research activities have focused on using neural networks to implement the whole coding workflow, enabling data-driven end-to-end optimization. The performance of these neural video codecs has advanced significantly in the last five years, since the first attempt, DVC \\cite{lu2019dvc}, was published. DVC  matched the performance of a fast implementation of H.264 (x264). However currently, learned video coding algorithms (e.g., DCVC-FM \\cite{li2024neural} and DCVC-LCG \\cite{Qi2024longterm}) are able to compete or even outperform the state-of-the-art standard codecs, such as VVC VTM under certain coding configurations. These learning-based methods often focus on enhancement from different perspectives, including feature space conditional coding (e.g., FVC~\\cite{hu2021fvc} and DCVC \\cite{li2021deep}), instance adaptation ~\\cite{khani2021efficient,yang2024parameter}, and motion estimation (e.g., DCVC-DC~\\cite{li2023neural}). New architectures have also been proposed such as CANF-VC~\\cite{ho2022canf} based on a video generative model, MTMT \\cite{xiang2022mimt} using a masked image modeling transformer-based entropy model and VCT \\cite{mentzer2022vct} based on a video compression transformer. It is noted that although promising coding performance has been achieved in the aforementioned works, these neural video codecs (in particular those based on autoencoder backbones) are typically associated with high computational complexity (especially in the decoder), which constrains their deployment for practical applications. To address this issue, researchers attempted to achieve complexity reduction while maintaining the coding performance through model pruning and knowledge distillation~\\cite{guo2023evc,peng2024accelerating}. \n\nIt should be noted that the neural codecs mentioned above are typically trained offline with diverse video content \\cite{nawala2024bvi}, and deployed online for inference. In this case, model generalization becomes important, and this is why these codecs often have a large model capacity, resulting in large model sizes and slow inference runtime. Inspired by recent advances in implicit neural representations (INR), a new type of video codec has emerged that employs INR models to ``represent'' the video  by learning a coordinate-based mapping and compressing the network parameters for transmission. This approach converts a video coding problem into a model compression task, which allows the use of a much smaller network to ``overfit'' the input video, with the real potential for fast decoding. Existing implicit neural video representation (NeRV) models can be classified into index-based and content-based methods. The former takes frame~\\cite{chen2021nerv}, patch~\\cite{bai2023ps} or disentangled spatial/grid coordinates~\\cite{li2022nerv} as model input, while content-based approaches~\\cite{kwan2024hinerv,kim2024c3,leguay2024cool} have content-specific embedding as inputs. Currently, one of the best INR-based video codecs \\cite{kwan2024nvrc} has already achieved a performance similar to that of VVC VTM (RA), but with a much lower decoding complexity compared to autoencoder-based neural codecs. Some of these models have also been applied to volumetric video content~\\cite{ruan2024point,kwan2024immersive}, demonstrating their potential to compete with standard and other learning-based methods. However, it should be noted that the training of most NeRV models is based on an entire video sequence or even datasets; this results in a high system delay and does not meet the requirement of many low latency video streaming or real-time applications. To address this limitation, significant advances  have been made~\\cite{gao2024pnvc} towards more practical INR-based video compression (such as the Low Delay and Random Access modes in VVC VTM \\cite{bossen2023vtmctc}) by combining pre-training and online model overfitting.\n\nSimilarly to image compression, international grand challenges are used to compare neural video compression methods, with notable venues including the NN-based Video Coding Grand Challenge associated with The IEEE International Symposium on Circuits and Systems (ISCAS) and the Challenge on Learned Image Compression (CLIC, video coding track) with IEEE/CVF CVPR and Data Compression Conference (in 2024). The best performer in ISCAS 2024 NN-based Video Coding Grand Challenge offers an overall 55\\% BD-rate saving over HEVC Test Model HM \n \\cite{iscas2024}, while the winner of the CLIC (video coding track) in 2024, a neural-network enhanced ECM codec \\cite{zhao2024neural} with a CNN-based in-loop filter, shows a more than 2dB (in PSNR) gain compared to VTM (RA) at the same bitrates. \n\n\\subsubsection{Audio Compression}\n\nSimilarly to images and videos, learning-based solutions have also been researched to compress audio signals, and most neural audio codecs are based on VQ-VAE \\cite{Oord:Neural:2017}. SoundStream \\cite{zeghidour2021soundstream} is one of such models, which can encode audio content at various bitrates. It is based on a residual vector quantizer (RVQ) which trades off between rate, distortion, and complexity. This work has been further enhanced with a multi-scale spectrogram adversary and a loss balancer mechanism, resulting in improved rate-distortion performance. A more advanced universal model has been further developed \\cite{kumar2024high} based on improved adversarial and reconstruction losses, which can compress different types of audio. RVQ has also been extended from a single scale to multiple scales \\cite{siuzdak2024snac}, which performs hierarchical quantization at variable frame rates. \n\nMore recently, researchers have started to exploit the use of LLMs for audio compression, leveraging the audio generation/synthesis abilities of generative models. UniAudio 1.5 \\cite{yang2024uniaudio} is one of such attempts, which converts an audio into the textural space, which can be represented by a pre-trained LLM that shares a similar backbone of UniAudio \\cite{yang2023uniaudio}, a universal audio foundation model. LFSC is another neural audio codec based on LLMs, which achieved fast LLM training and inference through finite scalar quantization and adversarial training. \n\n\\subsection{Visual Quality Assessment}\n\\label{ssec:asssessment}\n\nAssessing the quality of visual signals remains an important and challenging task for many image and video processing applications. While subjective tests involving human participants remain the gold standard, objective quality models are frequently used  because of their time and cost efficiency. These quality assessment methods are typically used to evaluate the performance of different visual processing approaches, and they can also be converted to loss functions, which are employed for optimizing learning-based processing models.  \n\nIn recent years, quality assessment methods have  been enhanced using deep learning techniques. The resulting learning-based quality models can quickly adapt to a specific type of content, leading to better performance compared to conventional, hand-crafted quality metrics. This section provides a brief summary of existing works in this research area, and highlights the main challenges which should be addressed in the near future. A more comprehensive overview of the image and video quality assessment literature can be found in \\cite{zhai2020perceptual,zheng2024video,zhang2024quality}.\n\n\\subsubsection{Quality assessment models}\n\nImage and video quality assessment methods can be classified into two primary categories according to the availability of the corresponding reference content to the distorted test version: full-reference and no-reference models\\footnote{Reduced-reference quality metrics do existing in the literature, but the research in this field is less active in recent years.}. Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants \\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}), distortion \\cite{chandler2007vsnr,larson2010most,STMAD}, and artifacts \\cite{ou2010perceptual,zhu2014no,zhang2015perception}. In many cases, the extracted features are further processed by models that simulate texture masking \\cite{helmholtz1896handbook}, contrast sensitivity \\cite{kelly1977visual}, and saliency \\cite{itti2001computational}. These hand-crafted quality models have also been combined with features within a regression-based framework in order to achieve more accurate prediction performance - VMAF is one such example \\cite{VMAFblog}. When neural networks are involved for feature extraction, they are trained to capture information which can directly contribute to quality prediction through an end-to-end optimization strategy. Initially, convolutional neural networks were typically used, with notable examples such as DeepQA \\cite{kim2017deep}, LPIPS \\cite{zhang2018unreasonable} and CONTRIQUE \\cite{madhusudana2022image} for image quality assessment, and TLVQA \\cite{korhonen2019two}, C3DVQA \\cite{xu2020c3dvqa} and DeepVQA \\cite{kim2018deep} for video quality assessment. Recent works have been reported to achieve better performance when Vision Transformers (ViTs) (or similar variants) are employed due to the effectiveness of their self-attention mechanism. Important works in this class include IQT \\cite{cheon2021perceptual}, TRes \\cite{golestaneh2022no}, SaTQA \\cite{shi2024transformer}, FastVQA \\cite{wu2022fast} and RankDVQA \\cite{feng2024rankdvqa}. The former has been further extended as DOVER \\cite{wu2023exploringvideo} and COVER \\cite{he2024cover} when aesthetic and/or semantic aspects in the content are taken into account. \n\nMore recently, inspired by the success of large language models (LLMs) \\cite{openai:gpt4:2023,touvron2023llama} in other machine learning tasks, these have been utilized in image and video quality assessment,  demonstrating significant potential to achieve better model generalization. Q-Bench \\cite{wu2024qbench} is one of the first attempts that employs multimodal large language models to predict the perceptual quality of images based on prompt-driven evaluation. It queries the LLMs to provide information related to the final quality rating of the input image and the quality description. This has been further extended for video quality assessment tasks in Q-Align \\cite{wu2024qalign}. Other notable works include X-iqe \\cite{chen2023x} that performs the quality prompt in a multi-iteration manner focusing on both image fidelity and aesthetics. Prompt-based approaches have also been proposed for differentiating the quality difference between multiple images, such as 2AFC-LMMs \\cite{zhu20242afc} based on a two-alternative forced choice prompt and MAP (maximum a posteriori) estimation. Moreover, recent research works also focus on using pre-trained vision-language models, such as CLIP \\cite{radford2021learning}, which align better image and text modalities. Important examples in this class for image quality assessment include ZEN-IQA \\cite{miyata2024zen}, QA-CLIP \\cite{pan2023quality} and PromptIQA \\cite{chen2025promptiqa}. Similar works have also been proposed for video quality assessment, such as BVQI \\cite{wu2023exploring,wu2023towards} and COVER \\cite{he2024cover}.\n\n\n\nTo support the training and validation of learning-based quality assessment models, image or video databases containing ground-truth subjective quality scores are typically employed. Commonly used image quality databases include LIVE \\cite{sheikh2006astatistical}, CSIQ \\cite{larson2010most}, TID2013 \\cite{ponomarenko2013color}, PieAPP and PIPAL, while video quality databases such as LIVE-VQA~\\cite{seshadrinathan2010study}, KoNViD-1K~\\cite{hosu2017konstanz}, YouTube UGC~\\cite{wang2019youtube} and LIVE-VQC~\\cite{sinno2018large} are typically employed for benchmarking in the literature. There are also databases developed that investigate the impact of specific video formats and/or artifacts, such as LIVE-YT-HFR  \\cite{madhusudana2021subjective} focusing on frame rates, VSR-QAD \\cite{zhou2024database} on spatial resolution (or super-resolution artifacts), BAND-2k \\cite{chen2024band2k} on banding artifacts and Maxwell \\cite{wu2023towards}/BVI-Artifact \\cite{feng2024bvi} containing multiple artifacts commonly produced in video streaming. Based on these databases, many learning-based quality assessment models are trained to minimize the difference (L1 or L2 norm) between predicted quality indices and subjective scores. However, due to the limited number of ground-truth quality labels associated with these databases and the resourcing-costing nature for collecting subjective data through human participants involved in psychophysical experiments, this type of training methodology cannot offer satisfactory performance, in particular when the model capacity is large. Moreover, since the experimental settings and conditions used for quality labeling are different in these databases, intra-database cross-validation is always required due to the limited model generalization and potential overfitting problems. \n\nTo address these issues, various proxy quality metrics have been used to label images and videos, which avoid expensive subjective tests and enable the generation of a large amount of training material with pseudo-ground-truth quality annotations. To further improve the reliability of quality labels, instead of learning the absolute values of the quality labels, ranking-inspired training strategies have been developed, which focus on improving the monotonicity characteristics of quality. Important examples based on these weakly supervised training methodologies include RankIQA \\cite{liu2017rankiqa} and UNIQUE \\cite{zhang2021uncertainty} for the image quality assessment task, and VFIPS \\cite{hou2022perceptual} and RankDVQA \\cite{feng2024rankdvqa} for video quality assessment. Moreover, different self-supervised learning approaches have also been employed, which transform quality labeling to an auxiliary task. For example, CONTRIQUE \\cite{madhusudana2022image} learns relevant features from an unannotated image database based on the prediction of distortion types and degrees through contrastive learning. This method has been further applied to video quality assessment, resulting in a contrastive video quality estimator, CONVIQT \\cite{madhusudana2023conviqt}. More recently, quality-aware contrastive loss has been designed in \\cite{zhao2023quality,peng2024rmt} to stabilize the learning process.\n\n\\subsubsection{Performance and main challenges}\n\nDue to the lack of standard test conditions and limited model generalization within many existing image and video quality assessment models, deep compression methods are typically trained and benchmarked using different databases in conjunction with intra-database cross-validation. This can result in inconsistent evaluation results and conclusions. To enable a fair and meaningful comparison, various challenges and contests have been held for visual quality assessment. The Sixth Challenge on Learned Image Compression (CLIC) \\cite{clic} associated with the Data Compression Conference 2024 is one of the latest examples which includes two quality assessment tracks for image and video compression. The best performer in the video quality assessment track achieves a Spearman Ranking Correlation Coefficient value of 0.825 \\cite{feng2024rankdvqa}, which is based on a ranking-inspired training methodology. Other notable challenges include the IEEE/CVF WACV 2023 HDR VQA Grand Challenge and the Video Super-Resolution Quality Assessment Challenge in ECCV 2024, which focus on high dynamic range and super-resolved content, respectively. \n\nAlthough significant progress has been made in the past few years in visual quality assessment, including new models and training methodology, challenges remain, including limited model generalization and high computational complexity. \n\nAnother important use of quality metrics is as embedded loss functions for image and video processing optimization. This requires further capability and robustness, alongside complexity reduction, all topics to be addressed in future work. \n\n", "appendix": false}, "Closing Thoughts and Future of AI in Creativity": {"content": "\n\\label{sec:discussion}\n\nThis paper has presented a comprehensive review of current AI technologies and their applications that have emerged in recent years. Generative methods have driven a rapid growth in AI usage, particularly in the creative sector, significantly advancing the state of the art across various creative applications such as content creation, information extraction and analysis, content enhancement and data compression.\n\nThrough these applications, generative AI has not only broadened creative possibilities but has also reduced the manual effort and time traditionally associated with the production pipeline, allowing for greater creative experimentation and quicker production cycles. As this technology advances, it promises to unlock even more sophisticated capabilities in the creative industry. However, creative technologists, artists and other users must adapt,  learn to use, and build these tools effectively and safely.\n\n\n\\subsection{Challenges for AI in the Creative Sector}\n\nOne of the primary challenges for artists engaging with modern generative AI and LLMs is the lack of consistent, controllable output. These models operate via stochastic sampling from high-dimensional latent spaces, meaning that identical prompts can yield different results across runs. This unpredictability makes it difficult for artists to achieve and iterate toward a precise creative vision. Although prompt engineering has emerged as a technique to guide model behavior, it requires technical knowledge and iterative refinement, which may not align with the intuitive or exploratory approaches common in artistic practice. \n\nMoreover, there is a fundamental tension between the structured nature of current AI pipelines and the nonlinear, often improvisational workflows of creative disciplines. Many generative tools were originally designed for tasks like software development, content automation, or optimization \\cite{zhong:LDB:2024}, and are ill-suited for open-ended, exploratory creation. Artists typically work in cycles of ideation, experimentation, and revision\u2014processes that demand fluid, real-time interaction and control, which existing AI systems struggle to support. These limitations point to a gap in current AI design: a need for systems that not only generate high-quality content but also adapt to the iterative, interpretive nature of artistic production. One possible approach addressed to these challenges is a shift toward top-down creative workflows, where artists define high-level concepts, themes, or goals via text prompts before refining specific outputs. This approach helps align AI-generated results with artistic intent, offering a degree of control over inherently stochastic systems.\n\nSpeaking at the World Government Summit in Dubai in 2024,\\footnote{\\url{https://blogs.nvidia.com/blog/world-governments-summit/}} NVIDIA CEO Jensen Huang argued that with rapid advancements in AI, learning to code may become less essential for newcomers to the tech sector. He envisioned a future where traditional programming could be replaced by more intuitive AI-driven tools, thereby automating complex tasks and enhancing productivity\u2014particularly for artists without coding expertise. While this perspective remains debated, it highlights the potential for AI to become more accessible within creative fields. However, AI-assisted coding tools are insufficient for creative practitioners, as artistic workflows tend to be unstructured and rely on domain-specific data. Artists must hence turn to techniques such as fine-tuning pre-trained models, few-shot learning, or domain adaptation\u2014methods that are powerful yet typically inaccessible without machine learning expertise.\n\nThere is also broader concerns persist regarding the long-term impact of AI on the creative industries, particularly with the potential emergence of artificial general intelligence (AGI). Envisioned by organizations like OpenAI, DeepMind, and Anthropic, AGI could surpass human cognitive abilities, raising ethical and existential questions about the role of human agency in artistic expression.\n\n\n\\subsection{Ethical Issues, Fakes and Bias}\n\n\n\nAdvancements in generative AI, exemplified by models like Sora and Gemini 1.5 Pro, provoke ethical concerns and societal implications. These models, capable of generating highly realistic content, escalate the risk of misuse, through malicious deepfakes and misinformation. We are now in a situation where AI results transcend the uncanny valley, further complicating matters and challenging perceptions of authenticity. For example, the artist Miles Astray demonstrated that even authentic photographs could be mistaken for AI-generated images. His real photograph `F L A M I N G O N E' won both the jury\u2019s award and the people\u2019s choice award in the AI category of the 1839 Awards. His aim wasto highlight the ethical dilemmas inherent in AI, suggesting that the benefits of discussing AI's ethical implications could surpass the ethical concerns related to viewer deception\\footnote{\\url{https://www.milesastray.com/news/newsflash-reclaiming-the-brain}}.\n\nWhile democratizing AI tools no-doubt presents opportunities to transform creative processes and workflows, it also necessitates robust regulatory frameworks to safeguard privacy and ownership. For example, deepfake technologies stimulate  significant concerns about the spread of misinformation and other malicious uses. Efforts to detect and identify increasingly realistic deepfakes are thus as important as the generative methods used to produce them. These must however be accompanied by increased media literacy, and policies that address the ethical and legal implications.\n\nDiversity and representation is a key issue when using AI tools. Unified Concept Editing \\cite{Gandikota:Unified:2024} has been proposed as a basis for image generation in digital mediums. This aims to ensure the production of safe content with diverse representation, reducing gender and racial biases. Hallucination in generative AI (the production of outputs that are not faithful representations of reality but instead contain imagined or unrealistic elements) are a further cause of concern. These undermine trust in AI processes and can be due to limitations in the training data, biases in the model architecture or imperfections in the optimization process. Hallucinations associated with LLMs are one of the issues highlighted by the UK Government  \\cite{UK:Large:2024}, alongside bias, regurgitation of private data, difficulties with multi-step tasks and challenges in interpreting black-box processes.\n\nGovernments across the world  are increasingly expressing concerns about the challenges and uncertainties that generative AI technologies pose to rights holders and human creativity \\cite{Jeary2024}. Generative AI presents substantial legal challenges, including the copyright status of AI-generated work and the intellectual property and copyright implications of the datasets used in training AI models. Viewpoints on this issue do however differ. For example, the track ``Heart on My Sleeve,\" penned by an (as yet unidentified) human author, featured AI-generated vocals that replicated the voices of Drake and The Weeknd. Released independently on April 4, 2023, it was accessible via streaming platforms including Apple Music, Spotify, and YouTube. The song quickly became viral, accumulating over 20 million views across all platforms\\footnote{\\url{https://www.nbcnews.com/pop-culture/viral-ai-powered-drake-weeknd-song-removed-streaming-services-rcna80098}}, prior to its removal by Universal Music Group, Drake's recording label. In contrast, Canadian artist Grimes has extended an invitation to musicians to emulate her voice via AI for the creation of new musical pieces, stipulating that the lyrics should not be harmful. She has advocated for the democratization of art and the abolition of copyright\\footnote{\\url{https://www.bbc.com/news/entertainment-arts-65385382}}. Additionally, Grimes has employed AI to design visual content for her LED backdrop at Coachella in 2024.\n\nFinally, the rapid development of AI technologies has also raised concerns about job displacement and the balance between automation and human participation in creative processes. Ensuring that AI augments, rather than undermines, human effort poses a significant challenge for developers and policymakers. \n\n\n\n\\subsection{The future of AI technologies}\n\nSeveral key technological issues remain which need to be addressed if AI is to deliver its full potential. These in particular relate to training data, computational complexity and their depth of reasoning or planning, and are discussed below.\n\nA substantial amount of data is essential for training AI models in order to achieve high performance and good generalisation. Major companies such as Google, Meta, and NVIDIA, with their respective models: BERT, Segment Anything, and Canvas, dominate this space, benefiting from leveraged resources to gather data and process it to train sophisticated models. However,  in November 2024, Bloomberg reported that OpenAI, Anthropic, and Google are all experiencing relatively slow growth in the performance of their AI models, with one of the key challenges being training data\\footnote{\\url{https://www.bloomberg.com/news/articles/2024-11-13/openai-google-and-anthropic-are-struggling-to-build-more-advanced-ai}}.\n\nLLMs excel in applications involving complex tasks, advanced reasoning, data analysis, and understanding context. \nHowever, these models typically require high computational resources or cloud computing for development, operation and fine-tuning. A new trend emerging in alongside LLMs is the development of Small language models (SLMs), such as Phi-3 by Microsoft\\footnote{\\url{https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/}}. SLMs offer promising solutions for regulated industries and sectors encountering scenarios where high-quality results are essential while keeping data 'on-site'. Their potential is particularly relevant when deploying more capable SLMs on smartphones and other mobile devices, allowing them to operate `at the edge' without relying on cloud connectivity. Recent highly successful platforms, such as  DeepSeek-V3 \\cite{deepseekv3} and Qwen2.5-Max \\cite{qwen25}, are based on Mixture-of-Experts (MoE) models, which tackle complex problems by dividing them into simpler sub-tasks, each handled by a specialized ``expert.\"\n\nDespite the evident advancements in AI, current models still struggle with tasks requiring planning or deep reasoning and are prone to errors when encountering unexpected data. This, in turn, reduces the confidence of users and trust in the results.  AI algorithms can learn through reinforcement learning, but this process often identifies the best outcome as an anomaly rather than the norm. Yann LeCun, Professor at NYU and Chief AI Scientist at Meta, noted that while LLMs show a degree of comprehension in processing and generating text, their understanding lacks depth, often leading to results that defy common sense\\footnote{\\url{https://twitter.com/ylecun/status/1728496457601183865}}. He advocates for self-supervised learning as a pivotal future direction for AI, emphasizing its potential to derive insights from unlabeled data. Concurrently, Andrew Ng, Adjunct Professor at Stanford University and Founder of DeepLearning.AI, sees iterative AI agentic workflows\\footnote{\\url{https://www.youtube.com/watch?v=sal78ACtGTc}} as a key advancement for enhancing AI tool capabilities through an interactive approach by AI agents. These workflows involve autonomous agents that interactively learn from experience, understand natural language, and execute tasks on behalf of users.\n\nThe increasing openness of code and datasets is seen by many as a catalyst for accelerating AI advancements, with major firms like Microsoft, Google, and Meta supporting open access technologies. However, this openness also introduces security risks, necessitating new regulatory measures to monitor models post-release, to standardize documentation, and to assess the safety of  software code and training data disclosure.\n\nFinally, as stated in \\cite{Jeary2024}, the rapid advancement of AI technologies has revolutionized cultural experiences, often referred to as `CreaTech'\u2014the convergence of the creative and digital sectors \\cite{CreativeIndustriesCouncil2021}. Such innovations not only reshape how people engage with art and creative work (e.g., through AR/VR/MR) but also drive the evolution of the technologies themselves.\n\n\n\n\n\\bibliographystyle{IEEEbib}IEEEbib\n\\bibliography{literature_review}\n\n", "appendix": false}}, "categories": ["cs.AI"], "published": "2025-01-06 02:46:33+00:00", "primary_category": "cs.AI", "summary": "The rapid advancements in artificial intelligence (AI), particularly in\ngenerative AI and large language models (LLMs), have profoundly impacted the\ncreative industries, enabling more innovative content creation, enhancing\nworkflows, and democratizing access to creative tools. This paper explores\nthese technological shifts, with particular focus on how those that have\nemerged since our previous review in 2022 have expanded creative opportunities\nand improved efficiency. These technological advancements have enhanced the\ncapabilities of text-to-image, text-to-video, and multimodal generation\ntechnologies. In particular, key breakthroughs in LLMs have established new\nbenchmarks in conversational AI, while advancements in image generators have\nrevolutionized content creation. We also discuss the integration of AI into\npost-production workflows, which has significantly accelerated and improved\ntraditional processes. Once content has been created, it must be delivered to\nits audiences the media industry is facing the demands of increased\ncommunication traffic due to creative content. We therefore include a\ndiscussion of how AI is beginning to transform the way we represent and\ncompress media content. We highlight the trend toward unified AI frameworks\ncapable of addressing and integrating multiple creative tasks, and we\nunderscore the importance of human insight to drive the creative process and\noversight to mitigate AI-generated inaccuracies. Finally, we explore AI's\nfuture potential in the creative sector, stressing the need to navigate\nemerging challenges and to maximize its benefits while addressing the\nassociated risks."}