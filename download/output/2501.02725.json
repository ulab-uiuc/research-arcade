{"title": "Artificial Intelligence in Creative Industries: Advances Prior to 2025", "author": "Nantheera Anantrasirichai", "abstract": "\\begin{abstract}\n   The rapid advancements in artificial intelligence (AI), particularly in generative AI and large language models (LLMs), have profoundly impacted the creative industries, enabling more innovative content creation, enhancing workflows, and democratizing access to creative tools. This paper explores these  technological shifts, with particular focus on how those that have emerged since our previous review in 2022 have expanded creative opportunities and improved efficiency. These technological advancements have enhanced the capabilities of text-to-image, text-to-video, and multimodal generation technologies. In particular, key breakthroughs in LLMs have established new benchmarks in conversational AI, while advancements in image generators have revolutionized content creation. We also discuss the integration of AI into post-production workflows, which has significantly accelerated and improved traditional processes. Once content has been created, it must be delivered to its audiences  the media industry is facing the demands of increased communication traffic due to creative content.  We therefore include a discussion of how AI is beginning to transform the way we represent and compress media content. We highlight the trend toward unified AI frameworks capable of addressing and integrating multiple creative tasks, and we underscore the importance of human insight to drive the creative process and oversight to mitigate AI-generated inaccuracies. Finally, we explore AI's future potential in the creative sector, stressing the need to navigate emerging challenges and to maximize its benefits while addressing the associated risks.\n\n\\end{abstract}", "citations": {"UK:Large:2024": {"bib_key": "UK:Large:2024", "bib_title": "Large language models and generative {AI}", "bib_author ": "{Communications", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "as outlined in the UK, by the Authority of the House of Lords\\cite{UK:Large:2024}", "next_context": ")."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "For example, DMG Media, the Financial Times, and Guardian Media Group have highlighted concerns about the potential impact on print journalism, particularly if AI tools reduce the need for users to click through to news websites, affecting advertising and subscription revenues\\cite{UK:Large:2024}", "next_context": "."}, {"section": "Closing Thoughts and Future of AI in Creativity", "subsection": "Ethical Issues, Fakes and Bias", "subsubsection": null, "prev_context": "Hallucinations associated with LLMs are one of the issues highlighted by the UK Government\\cite{UK:Large:2024}", "next_context": ", alongside bias, regurgitation of private data, difficulties with multi-step tasks and challenges in interpreting black-box processes."}], "importance_score": 3.0}, "Jeary2024": {"bib_key": "Jeary2024", "bib_title": "Artificial intelligence and new technology in creative industries", "bib_author ": "Lois Jeary", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Moreover, these new technologies not only influence creators,  but they also enable new ways for audiences to experience art and culture\\cite{Jeary2024}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "AI script generators serve as beneficial aids for writers, filmmakers, and game developers, offering inspiration, idea generation, and assistance in crafting entire scripts\\cite{Jeary2024, Azzarelli:Reviewing:2024}", "next_context": "."}, {"section": "Closing Thoughts and Future of AI in Creativity", "subsection": "Ethical Issues, Fakes and Bias", "subsubsection": null, "prev_context": "Governments across the world  are increasingly expressing concerns about the challenges and uncertainties that generative AI technologies pose to rights holders and human creativity\\cite{Jeary2024}", "next_context": "."}, {"section": "Closing Thoughts and Future of AI in Creativity", "subsection": "The future of AI technologies", "subsubsection": null, "prev_context": "Finally, as stated in\\cite{Jeary2024}", "next_context": ", the rapid advancement of AI technologies has revolutionized cultural experiences, often referred to as `CreaTech'\u2014the convergence of the creative and digital sectors\\cite{CreativeIndustriesCouncil2021}."}], "importance_score": 3.5}, "Bai:Train:2021": {"bib_key": "Bai:Train:2021", "bib_title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback", "bib_author ": "Bai, Yuntao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "\\cite{Bai:Train:2021}", "next_context": "."}], "importance_score": 1.0}, "openai:gpt4:2023": {"bib_key": "openai:gpt4:2023", "bib_title": "{GPT-4} Technical Report", "bib_author ": "{OpenAI}", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The next breakthrough happened in 2023 when OpenAI unveiled GPT-4, a significantly larger model with estimated 1.8 trillion  parameters and improved performance compared to its predecessors\\cite{openai:gpt4:2023}", "next_context": "."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "More recently, inspired by the success of large language models (LLMs)\\cite{openai:gpt4:2023,touvron2023llama}", "next_context": "in other machine learning tasks, these have been utilized in image and video quality assessment,  demonstrating significant potential to achieve better model generalization."}], "importance_score": 1.75}, "Wang:Painter:2023": {"bib_key": "Wang:Painter:2023", "bib_title": "Images Speak in Images: A Generalist Painter for In-Context Visual Learning", "bib_author ": "Wang, Xinlong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Take Painter by BAAI Vision\\cite{Wang:Painter:2023}", "next_context": "as an example, which employs an image pair as a task prompt (similar to a text prompt in LLMs), their model transfers the input image to produce a similar output as the task prompt, enabling it to undertake various tasks such as segmentation, low-light enhancement or rain removal."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2,Zhang:DVISp:2025}", "next_context": ""}], "importance_score": 1.1611111111111112}, "chung:human-loop:2021": {"bib_key": "chung:human-loop:2021", "bib_title": "Human in the Loop for Machine Creativity", "bib_author ": "Neo Christopher Chung", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "\\cite{chung:human-loop:2021}", "next_context": "is simplified through text prompts, with sophisticated, multilingual language capabilities enabling artists to convey complex emotions and narratives."}], "importance_score": 1.0}, "Wu:brief:2023": {"bib_key": "Wu:brief:2023", "bib_title": "A Brief Overview of {ChatGPT}: The History, Status Quo and Potential Future Development", "bib_author ": "Wu, Tianyu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Human oversight is thus essential to correct this through reinforcement learning with feedback\\cite{Wu:brief:2023}", "next_context": "."}], "importance_score": 1.0}, "Anantrasirichai:AI:2022": {"bib_key": "Anantrasirichai:AI:2022", "bib_title": "Artificial intelligence in the creative industries: a review", "bib_author ": "Anantrasirichai, N.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "In this paper, the objective is to reveal to the reader, the latest technology advancements that have emerged since our previous review paper on AI in the creative industries (published in 2022)\\cite{Anantrasirichai:AI:2022}", "next_context": "."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Similar to\\cite{Anantrasirichai:AI:2022}", "next_context": ", we first provide a high-level overview of current advanced AI technologies (Section\\ref{sec:overview}), followed by a selection of key creative domain applications (Section\\ref{sec:existing}) where current AI technologies are changing creative practice."}, {"section": "Current Advanced AI Technologies", "subsection": null, "subsubsection": null, "prev_context": "This paper provides a review of AI in the creative industries,  building on our previous publication in 2022\\cite{Anantrasirichai:AI:2022}", "next_context": "."}, {"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "More details about AEs and GANs for creative technologies can be found in our previous review\\cite{Anantrasirichai:AI:2022}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": null, "subsubsection": null, "prev_context": "Similarly to our previous (2021) review of AI for the creative industries~\\cite{Anantrasirichai:AI:2022}", "next_context": ", Table\\ref{tab:gather}categorizes applications and corresponding AI-based solutions."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "In our previous review paper\\cite{Anantrasirichai:AI:2022}", "next_context": ", we discussed AI technologies for contrast enhancement and colorization as separate topics, as methods were developed specifically for each task."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "In our previous review paper\\cite{Anantrasirichai:AI:2022}", "next_context": ", we categorized the work on restoration into several different types of distortions, including deblurring, denoising, dehazing, and mitigating atmospheric turbulence."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Inpainting", "prev_context": "CNNs and GANs have already achieved impressive results (see our previous review paper\\cite{Anantrasirichai:AI:2022}", "next_context": ")."}], "importance_score": 8.0}, "Bommasani2021FoundationModels": {"bib_key": "Bommasani2021FoundationModels", "bib_title": "On the Opportunities and Risks of Foundation Models", "bib_author ": "Rishi Bommasani", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": null, "subsubsection": null, "prev_context": "These were described by The Stanford Institute for Human-Centered Artificial Intelligence in 2021\\cite{Bommasani2021FoundationModels}", "next_context": "as``any model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks\"."}], "importance_score": 1.0}, "Vaswani:attention:2017": {"bib_key": "Vaswani:attention:2017", "bib_title": "Attention is All you Need", "bib_author ": "Vaswani, Ashish", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "In 2017, Google AI introduced the concept of  `Transformer' architectures in their publication `Attention Is All You Need'\\cite{Vaswani:attention:2017}", "next_context": "."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "TrackFormer extracts visual features using a CNN-based encoder, which are then tracked using a vanilla transformer\\cite{Vaswani:attention:2017}", "next_context": "in a frame sequence, while MixFormer introduces cross-attention between the target and search regions."}], "importance_score": 2.25}, "Dosovitskiy:image:2021": {"bib_key": "Dosovitskiy:image:2021", "bib_title": "An Image is Worth 16$\\times$16 Words: {T}ransformers for Image Recognition at Scale", "bib_author ": "Alexey Dosovitskiy", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "This work has since, been instrumental in the development and success  of large language models alongside many other applications, including vision understanding\\cite{Dosovitskiy:image:2021}", "next_context": ", and multiple modality learning (e.g., Gato\\cite{Reed:Generalist:2022})."}, {"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "In 2020, the first successful training of a transformer encoder for image recognition was published\\cite{Dosovitskiy:image:2021}", "next_context": ", reeferred to as a Vision Transformer (ViT)."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12}", "next_context": ""}], "importance_score": 2.090909090909091}, "Reed:Generalist:2022": {"bib_key": "Reed:Generalist:2022", "bib_title": "A Generalist Agent", "bib_author ": "Scott Reed", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "This work has since, been instrumental in the development and success  of large language models alongside many other applications, including vision understanding\\cite{Dosovitskiy:image:2021}, and multiple modality learning (e.g., Gato\\cite{Reed:Generalist:2022}", "next_context": ")."}], "importance_score": 1.0}, "Li:HAM:2022": {"bib_key": "Li:HAM:2022", "bib_title": "{HAM: Hybrid} attention module in deep convolutional neural networks for image classification", "bib_author ": "Guoqiang Li", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "It should be noted that attention modules are not solely used in transformers, but have also been successfully integrated into other deep learning architectures such as CNNs, used for image classification\\cite{Li:HAM:2022}", "next_context": ", object detection\\cite{Woo:CBAM:2018}, and other computer vision tasks\\cite{Guo:Attention:2022}."}], "importance_score": 1.0}, "Woo:CBAM:2018": {"bib_key": "Woo:CBAM:2018", "bib_title": "{CBAM: Convolutional} Block Attention Module", "bib_author ": "Woo, Sanghyun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "It should be noted that attention modules are not solely used in transformers, but have also been successfully integrated into other deep learning architectures such as CNNs, used for image classification\\cite{Li:HAM:2022}, object detection\\cite{Woo:CBAM:2018}", "next_context": ", and other computer vision tasks\\cite{Guo:Attention:2022}."}], "importance_score": 1.0}, "Guo:Attention:2022": {"bib_key": "Guo:Attention:2022", "bib_title": "Attention mechanisms in computer vision: A survey", "bib_author ": "Guo, Ming-Hao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "It should be noted that attention modules are not solely used in transformers, but have also been successfully integrated into other deep learning architectures such as CNNs, used for image classification\\cite{Li:HAM:2022}, object detection\\cite{Woo:CBAM:2018}, and other computer vision tasks\\cite{Guo:Attention:2022}", "next_context": "."}], "importance_score": 1.0}, "Liu:Swin:2021": {"bib_key": "Liu:Swin:2021", "bib_title": "{Swin Transformer: Hierarchical} Vision Transformer using Shifted Windows", "bib_author ": "Liu, Ze", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "\\cite{Liu:Swin:2021}", "next_context": "."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "\\cite{Liang:SwinIR:2021}, employs several concatenated Swin Transformer blocks\\cite{Liu:Swin:2021}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "A survey until 2022\\cite{Zou:object:2023}reported that Deformable DETR and Swin Transformers\\cite{Liu:Swin:2021}", "next_context": "outperform pure CNN-based YOLOv4\\cite{bochkovskiy2020yolov4}."}], "importance_score": 3.090909090909091}, "Liu:Swinv2:2022": {"bib_key": "Liu:Swinv2:2022", "bib_title": "Swin Transformer V2: Scaling Up Capacity and Resolution", "bib_author ": "Liu, Ze", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "Its version 2\\cite{Liu:Swinv2:2022}", "next_context": "applied a cosine function in the attention module."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12}", "next_context": ""}], "importance_score": 1.0909090909090908}, "Fan:SUNet:2022": {"bib_key": "Fan:SUNet:2022", "bib_title": "{SUNet: swin} transformer UNet for image denoising", "bib_author ": "Fan, Chi-Mao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "To date, Swin Transformers have been widely adopted in a range of applications including image restoration\\cite{Fan:SUNet:2022}", "next_context": "."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "ii)\\textbf{Denoising}: SUNet\\cite{Fan:SUNet:2022}", "next_context": "applies Swin transformer blocks combined in a UNet-like architecture."}], "importance_score": 2.05}, "Khan:Transformers:2022": {"bib_key": "Khan:Transformers:2022", "bib_title": "Transformers in Vision: A Survey", "bib_author ": "Khan, Salman", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "Comprehensive surveys on the use of transformers for image and video processing can be found in\\cite{Khan:Transformers:2022}", "next_context": "and\\cite{Selva:video:2023}, respectively."}], "importance_score": 1.0}, "Selva:video:2023": {"bib_key": "Selva:video:2023", "bib_title": "Video Transformers: A Survey", "bib_author ": "J. Selva", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "Comprehensive surveys on the use of transformers for image and video processing can be found in\\cite{Khan:Transformers:2022}and\\cite{Selva:video:2023}", "next_context": ", respectively."}], "importance_score": 1.0}, "gu2023mamba": {"bib_key": "gu2023mamba", "bib_title": "{Mamba: Linear}-time sequence modeling with selective state spaces", "bib_author ": "Gu, Albert", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "In recent years state space models\\cite{gu2023mamba, zhu2024vision}", "next_context": ", commonly known as `Mamba' have emerged."}], "importance_score": 0.5}, "zhu2024vision": {"bib_key": "zhu2024vision", "bib_title": "Vision mamba: Efficient visual representation learning with bidirectional state space model", "bib_author ": "Zhu, Lianghui", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "In recent years state space models\\cite{gu2023mamba, zhu2024vision}", "next_context": ", commonly known as `Mamba' have emerged."}], "importance_score": 0.5}, "Lester:power:2021": {"bib_key": "Lester:power:2021", "bib_title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "bib_author ": "Lester, Brian", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Large language models", "subsubsection": null, "prev_context": "\\cite{Lester:power:2021}", "next_context": "."}], "importance_score": 1.0}, "Jia:VPT:2022": {"bib_key": "Jia:VPT:2022", "bib_title": "Visual Prompt Tuning", "bib_author ": "Jia, Menglin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Large language models", "subsubsection": null, "prev_context": "This paradigm has however extended to other domains, such as visual prompt tuning\\cite{Jia:VPT:2022}", "next_context": "."}], "importance_score": 1.0}, "zhao:survey:2023": {"bib_key": "zhao:survey:2023", "bib_title": "A Survey of Large Language Models", "bib_author ": "Zhao, Wayne Xin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Large language models", "subsubsection": null, "prev_context": "For a comprehensive survey of LLMs, please refer to\\cite{zhao:survey:2023}", "next_context": "."}, {"section": "Current Advanced AI Technologies", "subsection": "Large language models", "subsubsection": null, "prev_context": "Many surveys and evaluations of LLMs are also available\\cite{zhao:survey:2023,Chang:Survey:2024,YAO:Survey:2024}", "next_context": "."}], "importance_score": 1.3333333333333333}, "Chang:Survey:2024": {"bib_key": "Chang:Survey:2024", "bib_title": "A Survey on Evaluation of Large Language Models", "bib_author ": "Chang, Yupeng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Large language models", "subsubsection": null, "prev_context": "Many surveys and evaluations of LLMs are also available\\cite{zhao:survey:2023,Chang:Survey:2024,YAO:Survey:2024}", "next_context": "."}], "importance_score": 0.3333333333333333}, "YAO:Survey:2024": {"bib_key": "YAO:Survey:2024", "bib_title": "A Survey on Large Language Model (LLM) Security and Privacy: The Good, The Bad, and The Ugly", "bib_author ": "Yifan Yao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Large language models", "subsubsection": null, "prev_context": "Many surveys and evaluations of LLMs are also available\\cite{zhao:survey:2023,Chang:Survey:2024,YAO:Survey:2024}", "next_context": "."}], "importance_score": 0.3333333333333333}, "Ye:FLASK:2024": {"bib_key": "Ye:FLASK:2024", "bib_title": "{FLASK}: Fine-grained Language Model Evaluation based on Alignment Skill Sets", "bib_author ": "Seonghyeon Ye", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Large language models", "subsubsection": null, "prev_context": "\\cite{Ye:FLASK:2024}", "next_context": "which evaluates LLMs based on 12 fine-grained skills for comprehensive language model evaluation: logical correctness, logical robustness, logical efficiency, factuality, commonsense understanding, comprehension, insightfulness, completeness, metacognition, conciseness, readability, and harmlessness."}], "importance_score": 1.0}, "Kingma:auto:2014": {"bib_key": "Kingma:auto:2014", "bib_title": "Auto-Encoding Variational Bayes", "bib_author ": "D.P. Kingma", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "\\cite{Kingma:auto:2014}", "next_context": ", learns the latent space as statistical parameters of probabilistic distributions, leading to significant improvement of the generated results."}], "importance_score": 1.0}, "Goodfellow:GAN:2014": {"bib_key": "Goodfellow:GAN:2014", "bib_title": "Generative Adversarial Nets", "bib_author ": "Goodfellow, Ian", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "\\cite{Goodfellow:GAN:2014}", "next_context": "introduced an alternative architecture known as a Generative Adversarial Network (GAN)."}], "importance_score": 1.0}, "Dickstein:Deep:2015": {"bib_key": "Dickstein:Deep:2015", "bib_title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics", "bib_author ": "Sohl-Dickstein, Jascha", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "\\cite{Dickstein:Deep:2015}", "next_context": ", using Nonequilibrium Thermodynamics."}], "importance_score": 1.0}, "Ho:DDPM:2020": {"bib_key": "Ho:DDPM:2020", "bib_title": "Denoising Diffusion Probabilistic Models", "bib_author ": "Jonathan Ho", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "\\cite{Ho:DDPM:2020}", "next_context": "in 2020 and Score-based diffusion models proposed by Song et al."}], "importance_score": 1.0}, "Song:Score:2021": {"bib_key": "Song:Score:2021", "bib_title": "Score-Based Generative Modeling through Stochastic Differential Equations", "bib_author ": "Yang Song", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "\\cite{Song:Score:2021}", "next_context": "in 2021."}], "importance_score": 1.0}, "Dhariwal:Diffusion:2021": {"bib_key": "Dhariwal:Diffusion:2021", "bib_title": "Diffusion Models Beat {GAN}s on Image Synthesis", "bib_author ": "Prafulla Dhariwal", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "Comparing to GANs, DMs provide higher diversity samples\\cite{Dhariwal:Diffusion:2021}", "next_context": "and a training process that is much more stable and does not suffer from mode collapse."}, {"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "Classifier guidance was introduced in\\cite{Dhariwal:Diffusion:2021}", "next_context": "to improve the generation of images of a desired class."}], "importance_score": 2.0}, "Rombach:LDM:2022": {"bib_key": "Rombach:LDM:2022", "bib_title": "High-Resolution Image Synthesis with Latent Diffusion Models", "bib_author ": "Rombach, Robin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "Latent Diffusion Models (LDM)\\cite{Rombach:LDM:2022}", "next_context": "use pretrained networks to convert images to feature maps, and perform training on a low-dimensional space."}, {"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "The LDM\\cite{Rombach:LDM:2022}", "next_context": "offers more flexible conditional image generators by adding cross-attention layer (referred to Transformers in Section\\ref{ssec:transformers}) to the denoising autoencoder."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024, Feng_Ma_2025, Liu_Ma_2025}", "next_context": ""}], "importance_score": 2.111111111111111}, "Choi:ILVR:2021": {"bib_key": "Choi:ILVR:2021", "bib_title": "{ILVR: Conditioning} Method for Denoising Diffusion Probabilistic Models", "bib_author ": "Choi, Jooyoung", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "The work in\\cite{Choi:ILVR:2021}", "next_context": "simply refines the latent space of well-trained unconditional DDPM so that the higher-level semantics of the synthetic samples are similar to the reference (conditioning)."}], "importance_score": 1.0}, "Cao:survey:2024": {"bib_key": "Cao:survey:2024", "bib_title": "A Survey on Generative Diffusion Models", "bib_author ": "Cao, Hanqun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "A survey on the methods and applications of DMs prior to 2024 can be found in\\cite{Cao:survey:2024}", "next_context": "."}], "importance_score": 1.0}, "xie2022neural": {"bib_key": "xie2022neural", "bib_title": "Neural fields in visual computing and beyond", "bib_author ": "Xie, Yiheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "They can be considered as fieldsx(represented by a scalar, vector, or a tensor with a value, such as magnetic field in physics) that are fully or partially parameterized by a neural network\\Phi, typically an MLP\\cite{xie2022neural}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "They excel in neural rendering, particularly in view-dependent novel view synthesis, and have effectively tackled several challenges associated with automated 3D capture\\cite{xie2022neural}", "next_context": ", such as accurately representing the reflectance properties of the scene."}], "importance_score": 2.0}, "kwan2024hinerv": {"bib_key": "kwan2024hinerv", "bib_title": "{HiNeRV}: Video Compression with Hierarchical Encoding-based Neural Representation", "bib_author ": "Kwan, Ho Man", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "Hence, one of its emerging applications is in data compression\\cite{kwan2024hinerv}", "next_context": "."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "~\\cite{kwan2024hinerv,kim2024c3,leguay2024cool}", "next_context": "have content-specific embedding as inputs."}], "importance_score": 1.4444444444444444}, "sitzmann:siren:2020": {"bib_key": "sitzmann:siren:2020", "bib_title": "Implicit Neural Representations\nwith Periodic Activation Functions", "bib_author ": "Sitzmann, Vincent", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{sitzmann:siren:2020}", "next_context": "demonstrated that using periodic functions, such as sinusoids, are more suitable for representing complex natural signals, offering a better fit to the first- and second-order derivatives of the signals."}], "importance_score": 1.0}, "Saragadam:wire:2023": {"bib_key": "Saragadam:wire:2023", "bib_title": "WIRE: Wavelet Implicit Neural Representations", "bib_author ": "Saragadam, Vishwanath", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "instead proposed using complex Gabor wavelets\\cite{Saragadam:wire:2023}", "next_context": ", which learn to represent high frequencies better and simultaneously are robust to noise."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "INR with complex Gabor wavelets as activation functions show promising denoising results\\cite{Saragadam:wire:2023}", "next_context": "."}], "importance_score": 2.0}, "Mildenhall:NeRF:2020": {"bib_key": "Mildenhall:NeRF:2020", "bib_title": "{NeRF: Representing} Scenes as Neural Radiance Fields for View Synthesis", "bib_author ": "Mildenhall, Ben", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020}", "next_context": ", NeRF is a form of neural rendering, a subset of generative AI, that generates novel views of a scene based on a partial set of 2D images."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "Neural Radiance Fields (NeRFs), introduced in\\cite{Mildenhall:NeRF:2020}", "next_context": ", have demonstrated the ability to learn a 3D scene from a smaller number of images captured from various viewpoints, as opposed to photogrammetry."}], "importance_score": 2.0769230769230766}, "Wang:SIMVLM:2022": {"bib_key": "Wang:SIMVLM:2022", "bib_title": "{SIMVLM: Simple} Visual Language Model Pretraining with Weak Supervision", "bib_author ": "Wang, Zirui", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "\\cite{Wang:SIMVLM:2022}", "next_context": "."}], "importance_score": 1.25}, "wei2024vary": {"bib_key": "wei2024vary", "bib_title": "{Vary: Scaling} up the Vision Vocabulary for Large Vision-Language Models", "bib_author ": "Wei, Haoran", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "More recent work in\\cite{wei2024vary}", "next_context": "scales up the vision vocabulary by incorporating new image features into the existing CLIP model, resulting in improved content understanding."}], "importance_score": 1.25}, "Alayrac:Flamingo:2022": {"bib_key": "Alayrac:Flamingo:2022", "bib_title": "Flamingo: a Visual Language Model for Few-Shot Learning", "bib_author ": "Alayrac, Jean-Baptiste", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Alayrac:Flamingo:2022, Huang:GenerSpeech:2022}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Alayrac:Flamingo:2022,esser:scaling:2024}", "next_context": ""}], "importance_score": 1.0}, "Huang:GenerSpeech:2022": {"bib_key": "Huang:GenerSpeech:2022", "bib_title": "{GenerSpeech: Towards} Style Transfer for Generalizable Out-Of-Domain Text-to-Speech", "bib_author ": "Huang, Rongjie", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Alayrac:Flamingo:2022, Huang:GenerSpeech:2022}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Audio and music generation", "prev_context": "Voice style transfer often uses zero-shot learning (a model is trained to recognize classes or categories that it has never encountered during training)\\cite{Huang:GenerSpeech:2022}", "next_context": "or few-shot learning (a model trained with only one or a few examples per class)\\cite{Wang:One:2022}."}], "importance_score": 1.5}, "Li:Diffusion:2022": {"bib_key": "Li:Diffusion:2022", "bib_title": "Diffusion-LM Improves Controllable Text Generation", "bib_author ": "Li, Xiang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:Diffusion:2022, Yang:Diffsound:2023, Evans:stable:2025}", "next_context": ""}], "importance_score": 0.3333333333333333}, "Yang:Diffsound:2023": {"bib_key": "Yang:Diffsound:2023", "bib_title": "Diffsound: Discrete Diffusion Model for Text-to-Sound Generation", "bib_author ": "Yang, Dongchao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:Diffusion:2022, Yang:Diffsound:2023, Evans:stable:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Audio and music generation", "prev_context": "For example, the framework proposed in\\cite{Yang:Diffsound:2023}", "next_context": "uses a DM-based method with a transformer backbone to turn text input into a mel-spectrogram using the vector quantized variational autoencoder (VQ-VAE)\\cite{Oord:Neural:2017}."}], "importance_score": 1.3333333333333333}, "Evans:stable:2025": {"bib_key": "Evans:stable:2025", "bib_title": "Stable Audio Open", "bib_author ": "Evans, Zach", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:Diffusion:2022, Yang:Diffsound:2023, Evans:stable:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Audio and music generation", "prev_context": "Stable Audio Open\\cite{Evans:stable:2025}", "next_context": "introduces a text-conditioned generative model for non-speech audio, trained on Creative Commons licensed data, capable of producing state-of-the-art 44.1kHz stereo audio."}], "importance_score": 1.3333333333333333}, "esser:scaling:2024": {"bib_key": "esser:scaling:2024", "bib_title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "bib_author ": "Esser, Patrick", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Alayrac:Flamingo:2022,esser:scaling:2024}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024, Feng_Ma_2025, Liu_Ma_2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Image generation", "prev_context": "Released in March 2024, the latest version of Stable Diffusion (SD3), has been reported to outperform state-of-the-art text-to-image generation systems such as DALL\u00b7E 3 (released August 2023)\\cite{esser:scaling:2024}", "next_context": ", Midjourney v6 (released December 2023), and Ideogram v1 (released February 2024) in terms of typography and prompt adherence, based on human preference evaluations."}], "importance_score": 1.6111111111111112}, "Brooks:InstructPix2Pix:2023": {"bib_key": "Brooks:InstructPix2Pix:2023", "bib_title": "{InstructPix2Pix: Learning} To Follow Image Editing Instructions", "bib_author ": "Brooks, Tim", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024, Feng_Ma_2025, Liu_Ma_2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Image generation", "prev_context": "introduced InstructPix2Pix\\cite{Brooks:InstructPix2Pix:2023}", "next_context": ",  a conditional diffusion model that generates image editing examples without predefined editing areas."}], "importance_score": 1.1111111111111112}, "gal:Image:2023": {"bib_key": "gal:Image:2023", "bib_title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion", "bib_author ": "Rinon Gal", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024, Feng_Ma_2025, Liu_Ma_2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Image generation", "prev_context": "Textual Inversion\\cite{gal:Image:2023}", "next_context": "personalizes large pre-trained text-to-image diffusion models based on specific objects and styles, using 3-5 images of a user-provided concept."}], "importance_score": 1.1111111111111112}, "Gandikota:Unified:2024": {"bib_key": "Gandikota:Unified:2024", "bib_title": "Unified Concept Editing in Diffusion Models", "bib_author ": "G", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024, Feng_Ma_2025, Liu_Ma_2025}", "next_context": ""}, {"section": "Closing Thoughts and Future of AI in Creativity", "subsection": "Ethical Issues, Fakes and Bias", "subsubsection": null, "prev_context": "Unified Concept Editing\\cite{Gandikota:Unified:2024}", "next_context": "has been proposed as a basis for image generation in digital mediums."}], "importance_score": 1.1111111111111112}, "Lian:LLMG:2024": {"bib_key": "Lian:LLMG:2024", "bib_title": "{LLM}-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models", "bib_author ": "Long Lian", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024, Feng_Ma_2025, Liu_Ma_2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Image generation", "prev_context": "LLM-grounded Diffusion\\cite{Lian:LLMG:2024}", "next_context": "was the first to introduce a framework that allows multiple rounds of user requests without the need for manual selection on the image."}], "importance_score": 1.1111111111111112}, "ren:hypersd:2024": {"bib_key": "ren:hypersd:2024", "bib_title": "{Hyper-SD: Trajectory} Segmented Consistency Model for Efficient Image Synthesis", "bib_author ": "Yuxi Ren", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024, Feng_Ma_2025, Liu_Ma_2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Image generation", "prev_context": "ByteDance announced Hyper-SD\\cite{ren:hypersd:2024}", "next_context": "which proposed trajectory segmented consistency distillation and provides real-time high-resolution image generation from drawing with a control text prompt."}], "importance_score": 1.1111111111111112}, "Feng_Ma_2025": {"bib_key": "Feng_Ma_2025", "bib_title": "{DiT4Edit: Diffusion} Transformer for Image Editing", "bib_author ": "Feng, Kunyu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024, Feng_Ma_2025, Liu_Ma_2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Image generation", "prev_context": "Since then, text-driven image editing has seen significant improvements in quality, with most recent approaches adopting Diffusion Transformer architectures\\cite{Feng_Ma_2025, Huang:Diff:2025}", "next_context": "."}], "importance_score": 0.6111111111111112}, "Liu_Ma_2025": {"bib_key": "Liu_Ma_2025", "bib_title": "{LLM4GEN: Leveraging} Semantic Representation of LLMs for Text-to-Image Generation", "bib_author ": "Liu, Mushui", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024, Feng_Ma_2025, Liu_Ma_2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Image generation", "prev_context": "LLM4GEN\\cite{Liu_Ma_2025}", "next_context": "fuses features from LLM and CLIP models to enhance the semantic understanding in text-to-image diffusion models, enabling them to better handle complex and dense prompts involving multiple objects."}], "importance_score": 1.1111111111111112}, "hong:cogvideo:2023": {"bib_key": "hong:cogvideo:2023", "bib_title": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers", "bib_author ": "Wenyi Hong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "As an example of a transformer-based approach, CogVideo\\cite{hong:cogvideo:2023}", "next_context": "employs VQ-VAE to convert input frames to tokens, which are then fused with text tokens to produce a new video."}], "importance_score": 1.0909090909090908}, "villegas:phenaki:2023": {"bib_key": "villegas:phenaki:2023", "bib_title": "{Phenaki: Variable} Length Video Generation from Open Domain Textual Descriptions", "bib_author ": "Ruben Villegas", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "\\cite{villegas:phenaki:2023}", "next_context": "exploits transformers to generate variable length videos, but the quality is lower than those based on DMs."}], "importance_score": 1.0909090909090908}, "Azadi:Make:2023": {"bib_key": "Azadi:Make:2023", "bib_title": "{Make-An-Animation: Large}-Scale Text-conditional {3D} Human Motion Generation", "bib_author ": "Azadi, Samaneh", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "Make-An-Animation\\cite{Azadi:Make:2023}", "next_context": "trains on image-text datasets and fine-tunes on motion capture data, adding additional layers to model the temporal dimension."}], "importance_score": 1.0909090909090908}, "Yu:Bidirectionally:2023": {"bib_key": "Yu:Bidirectionally:2023", "bib_title": "Bidirectionally Deformable Motion Modulation For Video-based Human Pose Transfer", "bib_author ": "Yu, Wing-Yin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "\\cite{wang:disco:2024}and Bidirectionally Deformable Motion Modulation (BDMM)\\cite{Yu:Bidirectionally:2023}", "next_context": "."}], "importance_score": 1.0909090909090908}, "Liu:FETV:2023": {"bib_key": "Liu:FETV:2023", "bib_title": "{FETV: A} Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation", "bib_author ": "Liu, Yuanxin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024, wang2025lavie, wu2025customcrafter}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "Evaluations of these methods can be found in\\cite{Liu:FETV:2023}", "next_context": "."}], "importance_score": 1.202020202020202}, "wang:disco:2024": {"bib_key": "wang:disco:2024", "bib_title": "DisCo: Disentangled Control for Realistic Human Dance Generation", "bib_author ": "Wang, Tan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "\\cite{wang:disco:2024}", "next_context": "and Bidirectionally Deformable Motion Modulation (BDMM)\\cite{Yu:Bidirectionally:2023}."}], "importance_score": 1.0909090909090908}, "xu:VASA-1:2024": {"bib_key": "xu:VASA-1:2024", "bib_title": "{VASA}-1: Lifelike Audio-Driven Talking Faces Generated in Real Time", "bib_author ": "Sicheng Xu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "In April 2024, Microsoft introduced VASA-1\\cite{xu:VASA-1:2024}", "next_context": ", which turns a single static image and a speech audio clip into a video clip of realistic talking faces mimicking human facial expressions and head movements, as shown in Fig."}], "importance_score": 1.0909090909090908}, "corona:vlogger:2024": {"bib_key": "corona:vlogger:2024", "bib_title": "VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis", "bib_author ": "Corona, Enric", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "The overall quality of the generated videos is better than VLOGGER by Google\\cite{corona:vlogger:2024}", "next_context": ", which is based on similar technology--diffusion models."}], "importance_score": 1.0909090909090908}, "Gupta:Photorealistic:2024": {"bib_key": "Gupta:Photorealistic:2024", "bib_title": "Photorealistic Video Generation with Diffusion Models", "bib_author ": "Gupta, Agrim", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024, wang2025lavie, wu2025customcrafter}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "More recent work has applied spatiotemporal layers to model temporal dynamics\\cite{Gupta:Photorealistic:2024}", "next_context": "."}], "importance_score": 1.202020202020202}, "Hu_2024_CVPR": {"bib_key": "Hu_2024_CVPR", "bib_title": "{Animate Anyone: Consistent} and Controllable Image-to-Video Synthesis for Character Animation", "bib_author ": "Hu, Li", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "Animate Anyone by Alibaba Group\\cite{Hu_2024_CVPR}", "next_context": "inputs a real photo or anime of a person with a sequence of guided poses."}], "importance_score": 1.0909090909090908}, "Zhu:INFP:2024": {"bib_key": "Zhu:INFP:2024", "bib_title": "{INFP: Audio}-Driven Interactive Head Generation in Dyadic Conversations", "bib_author ": "Yongming Zhu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024, wang2025lavie, wu2025customcrafter}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "Recently, ByteDance introduced an audio-driven interactive head generation\\cite{Zhu:INFP:2024}", "next_context": "that offers listening and speaking states during multi-turn conversations."}], "importance_score": 1.202020202020202}, "singer:Make:2023": {"bib_key": "singer:Make:2023", "bib_title": "{Make-A-Video: Text}-to-Video Generation without Text-Video Data", "bib_author ": "Uriel Singer", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024, wang2025lavie, wu2025customcrafter}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "Make-A-Video\\cite{singer:Make:2023}", "next_context": ", through a spatiotemporally factorized diffusion model, leverages joint text-image priors and super-resolution in space and time."}], "importance_score": 1.1111111111111112}, "molad:dreamix:2023": {"bib_key": "molad:dreamix:2023", "bib_title": "Dreamix: Video Diffusion Models are General Video Editors", "bib_author ": "Eyal Molad", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024, wang2025lavie, wu2025customcrafter}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "\\cite{molad:dreamix:2023}", "next_context": "videos do not have this issue, but they are very blurry."}], "importance_score": 1.1111111111111112}, "wang:modelscope:2023": {"bib_key": "wang:modelscope:2023", "bib_title": "ModelScope Text-to-Video Technical Report", "bib_author ": "Jiuniu Wang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024, wang2025lavie, wu2025customcrafter}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "The main technologies underpinning text-to-video and image-to-video tasks are based on DMs with a combination of 3D convolutions (or separately spatial and temporal convolutions), and spatial and temporal attention modules\\cite{wang:modelscope:2023}", "next_context": "."}], "importance_score": 1.1111111111111112}, "wu:tune:2023": {"bib_key": "wu:tune:2023", "bib_title": "{Tune-a-video: One-shot} tuning of image diffusion models for text-to-video generation", "bib_author ": "Wu, Jay Zhangjie", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024, wang2025lavie, wu2025customcrafter}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "Tune-A-Video\\cite{wu:tune:2023}", "next_context": "modifies the style of an input video using a text prompt."}], "importance_score": 1.1111111111111112}, "wang2025lavie": {"bib_key": "wang2025lavie", "bib_title": "{LaVie: High}-Quality Video Generation with Cascaded Latent Diffusion Models", "bib_author ": "Yaohui Wang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024, wang2025lavie, wu2025customcrafter}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "LaVie\\cite{wang2025lavie}", "next_context": "demonstrates that simple temporal self-attention mechanisms, when combined with rotary positional encoding, are sufficient to capture the temporal correlations inherent in video data."}], "importance_score": 1.1111111111111112}, "wu2025customcrafter": {"bib_key": "wu2025customcrafter", "bib_title": "{CustomCrafter: Customized Video Generation with Preserving Motion and Concept Composition Abilities}", "bib_author ": "Tao Wu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024, wang2025lavie, wu2025customcrafter}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "\\cite{wu2025customcrafter}", "next_context": "."}], "importance_score": 1.1111111111111112}, "yang:Holodeck:2024": {"bib_key": "yang:Holodeck:2024", "bib_title": "{Holodeck: Language} Guided Generation of {3D} Embodied {AI} Environments", "bib_author ": "Yue Yang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{yang:Holodeck:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Augmented, virtual and mixed reality, and 3D content", "prev_context": "\\cite{yang:Holodeck:2024}", "next_context": ", which automatically generates 3D embodied environments via text-prompt interactions with a large language model (GPT-4)."}], "importance_score": 2.0}, "Xu:NeuralLift:2023": {"bib_key": "Xu:NeuralLift:2023", "bib_title": "{NeuralLift-360: Lifting} an in-the-Wild 2D Photo to A {3D} Object with 360\u00b0 Views", "bib_author ": "Xu, Dejia", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Augmented, virtual and mixed reality, and 3D content", "prev_context": "NeuralLift-360\\cite{Xu:NeuralLift:2023}", "next_context": "also uses diffusion models to generate priors for novel view synthesis."}], "importance_score": 1.25}, "Melas:RealFusion:2023": {"bib_key": "Melas:RealFusion:2023", "bib_title": "RealFusion 360$^\\circ$ Reconstruction of Any Object from a Single Image", "bib_author ": "Melas-Kyriazi, Luke", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Augmented, virtual and mixed reality, and 3D content", "prev_context": "RealFusion\\cite{Melas:RealFusion:2023}", "next_context": ", a single image to 3D object generator, merges 2D diffusion models with NeRF, improving Instant-NGP\\cite{mueller:instant:2022}, which provides an API for VR controls."}], "importance_score": 1.25}, "Qian:Magic123:2024": {"bib_key": "Qian:Magic123:2024", "bib_title": "{Magic123: One} Image to High-Quality {3D} Object Generation Using Both 2D and {3D} Diffusion Priors", "bib_author ": "Qian, Guocheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Augmented, virtual and mixed reality, and 3D content", "prev_context": "Magic123\\cite{Qian:Magic123:2024}", "next_context": "is the latest image-to-3D tool that uses  2D and 3D priors simultaneously  to produce high-quality high-resolution 3D geometry and textures."}], "importance_score": 1.25}, "tang:dreamgaussian:2024": {"bib_key": "tang:dreamgaussian:2024", "bib_title": "DreamGaussian: Generative Gaussian Splatting for Efficient {3D} Content Creation", "bib_author ": "Jiaxiang Tang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Augmented, virtual and mixed reality, and 3D content", "prev_context": "DreamGaussian\\cite{tang:dreamgaussian:2024}", "next_context": "offers text-to-3D and image-to-3D by adapting 3D Gaussian splatting (more in Section\\ref{sssec:3DGS}) into generative settings using a diffusion prior."}], "importance_score": 1.5833333333333333}, "ren:dreamgaussian4d:2023": {"bib_key": "ren:dreamgaussian4d:2023", "bib_title": "{DreamGaussian4D: Generative} 4D Gaussian Splatting", "bib_author ": "Ren, Jiawei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Augmented, virtual and mixed reality, and 3D content", "prev_context": "DreamGaussian4D\\cite{ren:dreamgaussian4d:2023}", "next_context": "employs image-to-video diffusion and a 4D Gaussian Splatting representation to generate an image-to-4D model."}], "importance_score": 1.3333333333333333}, "zhao2024clear": {"bib_key": "zhao2024clear", "bib_title": "{CleAR}: Robust Context-Guided Generative Lighting Estimation for Mobile Augmented Reality", "bib_author ": "Zhao, Yiqin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Augmented, virtual and mixed reality, and 3D content", "prev_context": "There has also been an attempt to use autoregressive and generative models to estimate lighting, achieving a visually coherent environment between virtual and physical spaces in AR\\cite{zhao2024clear}", "next_context": "."}], "importance_score": 1.3333333333333333}, "sun:text:2023": {"bib_key": "sun:text:2023", "bib_title": "Text Classification via Large Language Models", "bib_author ": "Xiaofei Sun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{sun:text:2023, shi:chatgraph:2023, Hou:promptboosting:2023, AI2025125952}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Text categorization", "prev_context": "\\cite{sun:text:2023}", "next_context": "applies kNN to integrate diagnostic reasoning process for final decision."}], "importance_score": 1.25}, "Hou:promptboosting:2023": {"bib_key": "Hou:promptboosting:2023", "bib_title": "{P}rompt{B}oosting: Black-Box Text Classification with Ten Forward Passes", "bib_author ": "Hou, Bairu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{sun:text:2023, shi:chatgraph:2023, Hou:promptboosting:2023, AI2025125952}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Text categorization", "prev_context": "Multiple learners are also used to enhance the performances\\cite{Hou:promptboosting:2023,AI2025125952}", "next_context": "."}], "importance_score": 0.75}, "AI2025125952": {"bib_key": "AI2025125952", "bib_title": "Contrastive multi-graph learning with neighbor hierarchical sifting for semi-supervised text classification", "bib_author ": "Wei Ai", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{sun:text:2023, shi:chatgraph:2023, Hou:promptboosting:2023, AI2025125952}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Text categorization", "prev_context": "Multiple learners are also used to enhance the performances\\cite{Hou:promptboosting:2023,AI2025125952}", "next_context": "."}], "importance_score": 0.75}, "Mao:Biases:2023": {"bib_key": "Mao:Biases:2023", "bib_title": "The Biases of Pre-Trained Language Models: An Empirical Study on Prompt-Based Sentiment Analysis and Emotion Detection", "bib_author ": "Mao, Rui", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Advertisements and film analysis", "prev_context": "Technically, LLMs learn complex patterns and relationships in text data for sentiment classification\\cite{Mao:Biases:2023, krugmann:sentiment:2024}", "next_context": "."}], "importance_score": 0.8333333333333333}, "krugmann:sentiment:2024": {"bib_key": "krugmann:sentiment:2024", "bib_title": "Sentiment Analysis in the Age of Generative {AI}", "bib_author ": "Krugmann, J. O.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Advertisements and film analysis", "prev_context": "Technically, LLMs learn complex patterns and relationships in text data for sentiment classification\\cite{Mao:Biases:2023, krugmann:sentiment:2024}", "next_context": "."}], "importance_score": 0.8333333333333333}, "Hartmann:More:2023": {"bib_key": "Hartmann:More:2023", "bib_title": "More than a Feeling: Accuracy and Application of Sentiment Analysis", "bib_author ": "Jochen Hartmann", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Advertisements and film analysis", "prev_context": "SiEBERT\\cite{Hartmann:More:2023}", "next_context": "provides pre-trained model with open-source scripts to be fine-tuned to further improve accuracy for novel applications."}], "importance_score": 1.3333333333333333}, "Metzler:Rethinking:2021": {"bib_key": "Metzler:Rethinking:2021", "bib_title": "Rethinking search: making domain experts out of dilettantes", "bib_author ": "Metzler, Donald", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Content retrieval and recommendation services", "prev_context": "\\cite{Metzler:Rethinking:2021}", "next_context": "."}], "importance_score": 1.1666666666666667}, "Yan:Universal:2023": {"bib_key": "Yan:Universal:2023", "bib_title": "Universal Instance Perception As Object Discovery and Retrieval", "bib_author ": "Yan, Bin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": null, "prev_context": "\\cite{Yan:Universal:2023}", "next_context": "have categorized information extraction tasks based on the Format-Time-Reference space, as illustrated in Fig."}], "importance_score": 1.1666666666666667}, "Lu:content:2023": {"bib_key": "Lu:content:2023", "bib_title": "Content-based Search for Deep Generative Models", "bib_author ": "Lu, Daohan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Content retrieval and recommendation services", "prev_context": "When retrieving visual content, recent work exploits generative models to enhance content-based model search\\cite{Lu:content:2023}", "next_context": "."}], "importance_score": 1.1666666666666667}, "Rajput:recommender:2023": {"bib_key": "Rajput:recommender:2023", "bib_title": "Recommender Systems with Generative Retrieval", "bib_author ": "Rajput, Shashank", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Content retrieval and recommendation services", "prev_context": "\\cite{Rajput:recommender:2023}", "next_context": ", which utilizes GR."}], "importance_score": 1.1666666666666667}, "li:unigen:2024": {"bib_key": "li:unigen:2024", "bib_title": "{UniGen: A} Unified Generative Framework for Retrieval and Question Answering with Large Language Models", "bib_author ": "Li, X", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Content retrieval and recommendation services", "prev_context": "Conversational question answering techniques have been integrated to enhance the document retrieval\\cite{li:unigen:2024}", "next_context": "."}], "importance_score": 1.1666666666666667}, "li2024learning": {"bib_key": "li2024learning", "bib_title": "Learning to Rank in Generative Retrieval", "bib_author ": "Li, Yongqi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Content retrieval and recommendation services", "prev_context": "\\cite{li2024learning}", "next_context": "introduces learning-to-rank training to enhance the performance system up to 30\\%."}], "importance_score": 1.1666666666666667}, "Jin:DiffusionRet:2023": {"bib_key": "Jin:DiffusionRet:2023", "bib_title": "{DiffusionRet: Generative} Text-Video Retrieval with Diffusion Model", "bib_author ": "Jin, Peng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Jin:DiffusionRet:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Content retrieval and recommendation services", "prev_context": "DMs are also employed for visual retrieval tasks, where they learn joint data distributions between text queries and video candidates\\cite{Jin:DiffusionRet:2023}", "next_context": "."}], "importance_score": 2.0}, "King:Sasha:2024": {"bib_key": "King:Sasha:2024", "bib_title": "{Sasha: Creative} Goal-Oriented Reasoning in Smart Homes with Large Language Models", "bib_author ": "King, Evan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{King:Sasha:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Intelligent assistants", "prev_context": "Current LLMs obviously enhance the performance of intelligent assistants, designed to understand complex inquiries and generate more natural conversational responses, such as Sasha\\cite{King:Sasha:2024}", "next_context": "."}], "importance_score": 2.0}, "Xu:SNR:2022": {"bib_key": "Xu:SNR:2022", "bib_title": "SNR-Aware Low-light Image Enhancement", "bib_author ": "Xu, Xiaogang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "Meanwhile, SNR-Aware\\cite{Xu:SNR:2022}", "next_context": "estimates spatial-varying Signal-to-Noise Ratio (SNR) maps and proposes local and global learning branches using ResNet and transformer architectures, respectively."}], "importance_score": 1.1666666666666667}, "liang:RVRT:2022": {"bib_key": "liang:RVRT:2022", "bib_title": "Recurrent video restoration transformer with guided deformable attention", "bib_author ": "Liang, J.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "\\cite{Liang:VRT:2024}and its improved version with recurrent process (RVRT)\\cite{liang:RVRT:2022}", "next_context": ", have emerged as the state of the arts for video super-resolution, deblurring, denoising, and frame interpolation."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "The deblurring performance is comparable to RVRT\\cite{liang:RVRT:2022}", "next_context": ", but it is 5 times faster."}], "importance_score": 2.166666666666667}, "Wang:Ultra:2023": {"bib_key": "Wang:Ultra:2023", "bib_title": "Ultra-high-definition low-light image enhancement: A benchmark and transformer-based method", "bib_author ": "Wang, Tao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "LLFormer\\cite{Wang:Ultra:2023}", "next_context": ", and significantly better than  INR-based method, NeRCo\\cite{Yang:Implicit:2023}, on a real low-light image benchmarking dataset."}], "importance_score": 1.1666666666666667}, "Lin:SPATIO:2024": {"bib_key": "Lin:SPATIO:2024", "bib_title": "A SPATIO-TEMPORAL ALIGNED SUNET MODEL FOR LOW-LIGHT VIDEO ENHANCEMENT", "bib_author ": "Lin, Ruirui", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "STA-SUNet\\cite{Lin:SPATIO:2024}", "next_context": "has demonstrated that using transformers for low-light video enhancement outperforms CNN-based methods\\cite{anantrasirichai:BVI:2024}."}], "importance_score": 1.1666666666666667}, "Youk:FMA:2024": {"bib_key": "Youk:FMA:2024", "bib_title": "{FMA-Net: Flow}-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring", "bib_author ": "Geunhyuk Youk", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "FMA-Net\\cite{Youk:FMA:2024}", "next_context": "proposed multi-attention for joint Video super-resolution and deblurring, achieving fast runtime with nearly 40\\%improvement over RVRT, and the restored quality was reported better by up to 3\\%."}], "importance_score": 1.1666666666666667}, "Liang:VRT:2024": {"bib_key": "Liang:VRT:2024", "bib_title": "VRT: A Video Restoration Transformer", "bib_author ": "Liang, Jingyun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024,wang2025seedvr}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "\\cite{Liang:VRT:2024}", "next_context": "and its improved version with recurrent process (RVRT)\\cite{liang:RVRT:2022}, have emerged as the state of the arts for video super-resolution, deblurring, denoising, and frame interpolation."}], "importance_score": 1.3277777777777777}, "HOU:Global:2023": {"bib_key": "HOU:Global:2023", "bib_title": "Global Structure-Aware Diffusion Process for Low-light Image Enhancement", "bib_author ": "HOU, Jinhui", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "Diffusion models (DMs) have also become popular choices for low-light image enhancement\\cite{HOU:Global:2023,Yi:Diff:2023,Jiang:Low:2023}", "next_context": "."}], "importance_score": 0.5833333333333333}, "Yi:Diff:2023": {"bib_key": "Yi:Diff:2023", "bib_title": "Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative Diffusion Model", "bib_author ": "Yi, Xunpeng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "Diffusion models (DMs) have also become popular choices for low-light image enhancement\\cite{HOU:Global:2023,Yi:Diff:2023,Jiang:Low:2023}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "Diff-Retinex\\cite{Yi:Diff:2023}", "next_context": "formulates the low-light image enhancement problem into Retinex decomposition, and employs multi-path generative diffusion networks to reconstruct the normal-light Retinex probability distribution."}], "importance_score": 1.5833333333333333}, "Jiang:Low:2023": {"bib_key": "Jiang:Low:2023", "bib_title": "Low-light image enhancement with wavelet-based diffusion models", "bib_author ": "Jiang, Hai", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023, cao2025zero, feng2025residual}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "Diffusion models (DMs) have also become popular choices for low-light image enhancement\\cite{HOU:Global:2023,Yi:Diff:2023,Jiang:Low:2023}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "A recent state-of-the-art approach presented in\\cite{Jiang:Low:2023}", "next_context": "decomposes images into high and low frequencies using wavelet transform."}], "importance_score": 1.7261904761904763}, "lin2024lowlight": {"bib_key": "lin2024lowlight", "bib_title": "Low-light Video Enhancement with Conditional Diffusion Models and Wavelet Interscale Attentions", "bib_author ": "Lin, R.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "The technique has been extended for video enhancement in\\cite{lin2024lowlight}", "next_context": "."}], "importance_score": 1.25}, "Yang:Implicit:2023": {"bib_key": "Yang:Implicit:2023", "bib_title": "Implicit Neural Representation for Cooperative Low-light Image Enhancement", "bib_author ": "Yang, Shuzhou", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Yang:Implicit:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "NeRCo\\cite{Yang:Implicit:2023}", "next_context": "address low-light problem with INR, which unifies the diverse degradation factors of real-world scenes with a controllable fitting function."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "LLFormer\\cite{Wang:Ultra:2023}, and significantly better than  INR-based method, NeRCo\\cite{Yang:Implicit:2023}", "next_context": ", on a real low-light image benchmarking dataset."}], "importance_score": 3.0}, "Deng:StyTr2:2022": {"bib_key": "Deng:StyTr2:2022", "bib_title": "{StyTr2: Image} Style Transfer With Transformers", "bib_author ": "Deng, Yingying", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Style transfer", "prev_context": "\\cite{Deng:StyTr2:2022}", "next_context": "is the first transformer-based method for style transfer, applying content as a query and style as a key of attention."}], "importance_score": 1.3333333333333333}, "Moon:generalizable:2023": {"bib_key": "Moon:generalizable:2023", "bib_title": "Generalizable Style Transfer for Implicit Neural Representation", "bib_author ": "Jaeho Moon", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Moon:generalizable:2023,Kim:Controllable:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Style transfer", "prev_context": "\\cite{Moon:generalizable:2023}", "next_context": "combined INRs with vision transformers for generalizable style transfer; however, the results remain limited in quality."}], "importance_score": 1.8333333333333333}, "Chung_2024_CVPR": {"bib_key": "Chung_2024_CVPR", "bib_title": "Style Injection in Diffusion: A Training-free Approach for Adapting Large-scale Diffusion Models for Style Transfer", "bib_author ": "Chung, Jiwoo", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Style transfer", "prev_context": "\\cite{Chung_2024_CVPR}", "next_context": ", where the style is injected to manipulate the self-attention of the decoder."}], "importance_score": 1.3333333333333333}, "Zhang:Inversion:2023": {"bib_key": "Zhang:Inversion:2023", "bib_title": "Inversion-Based Style Transfer With Diffusion Models", "bib_author ": "Zhang, Yuxin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Zhang:Inversion:2023,Chai:StableVideo:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Style transfer", "prev_context": "InST\\cite{Zhang:Inversion:2023}", "next_context": "utilizes Stable Diffusion Models as the generative backbone and introduces an attention-based textual inversion module to learn the description of the content."}], "importance_score": 1.5}, "Chai:StableVideo:2023": {"bib_key": "Chai:StableVideo:2023", "bib_title": "{StableVideo: Text}-driven Consistency-aware Diffusion Video Editing", "bib_author ": "Chai, Wenhao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Zhang:Inversion:2023,Chai:StableVideo:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Style transfer", "prev_context": "StableVideo\\cite{Chai:StableVideo:2023}", "next_context": "uses a text prompt to describe the desired appearance of the output, transforming the input video to have a new look based on a diffusion model."}], "importance_score": 1.5}, "Kim:Controllable:2024": {"bib_key": "Kim:Controllable:2024", "bib_title": "Controllable Style Transfer via Test-time Training of Implicit Neural Representation", "bib_author ": "Sunwoo Kim", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Moon:generalizable:2023,Kim:Controllable:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Style transfer", "prev_context": "\\cite{Kim:Controllable:2024}", "next_context": "uses multilayer perceptrons (MLPs) to map image coordinates to the colors of the stylized output, guided by features extracted from both the content and style inputs to allow controllability."}], "importance_score": 1.5}, "Liang:SwinIR:2021": {"bib_key": "Liang:SwinIR:2021", "bib_title": "SwinIR: Image Restoration Using Swin Transformer", "bib_author ": "Liang, Jingyun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024,wang2025seedvr}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "The winning solution of the NTIRE 2025 Challenge on UGC Video Enhancement\\cite{safonov2025ntire}implemented a pipeline of four sequential modules: color enhancement, denoising, BasicVSR++ restoration\\cite{Chan:BasicVSR:2022}, and SwinIR\\cite{Liang:SwinIR:2021}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "\\cite{Liang:SwinIR:2021}", "next_context": ", employs several concatenated Swin Transformer blocks\\cite{Liu:Swin:2021}."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "Denoising with diffusion models (DMs)\\cite{yang:realworld:2023}has been proposed by diffusing with estimated noise that is closer to real-world noise rather than Gaussian noise, achieving better performance than SwinIR\\cite{Liang:SwinIR:2021}", "next_context": "and Uformer\\cite{Wang:Uformer:2022}."}], "importance_score": 3.161111111111111}, "Lu:Transformer:2022": {"bib_key": "Lu:Transformer:2022", "bib_title": "Transformer for Single Image Super-Resolution", "bib_author ": "Lu, Zhisheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024,wang2025seedvr}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "This was done in the feature domain extracted by a lightweight CNN module\\cite{Lu:Transformer:2022}", "next_context": ", outperforming those that use only CNNs."}], "importance_score": 1.1111111111111112}, "Liu:Learning:2022": {"bib_key": "Liu:Learning:2022", "bib_title": "Learning Trajectory-Aware Transformer for Video Super-Resolution", "bib_author ": "Liu, Chengxu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024,wang2025seedvr}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "\\cite{Liu:Learning:2022}", "next_context": "."}], "importance_score": 1.1111111111111112}, "Chen:Activating:2023": {"bib_key": "Chen:Activating:2023", "bib_title": "Activating More Pixels in Image Super-Resolution Transformer", "bib_author ": "Chen, Xiangyu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024,wang2025seedvr}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "The Hybrid Attention Transformer (HAT)\\cite{Chen:Activating:2023}", "next_context": "was introduced, which improves the SR quality over ESRT by more than 2dB when upscaling 2\\times-4\\times."}], "importance_score": 1.1111111111111112}, "li:GRL:2023": {"bib_key": "li:GRL:2023", "bib_title": "Efficient and Explicit Modelling of Image Hierarchies for Image Restoration", "bib_author ": "Yawei Li", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024,wang2025seedvr}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "\\cite{li:GRL:2023}", "next_context": "exploits a hierarchy of features in a global, regional, and local range using different ways to compute self-attentions as an image often show similarity within itself in different scales and areas."}], "importance_score": 1.161111111111111}, "kang:gigagan:2023": {"bib_key": "kang:gigagan:2023", "bib_title": "Scaling up GANs for Text-to-Image Synthesis", "bib_author ": "Kang, Minguk", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024,wang2025seedvr}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "This is achieved by adding flow estimation and temporal self-attention to the GigaGAN upsampler\\cite{kang:gigagan:2023}", "next_context": ", which is primarily used for image SR, and text-to-image synthesis."}], "importance_score": 1.1111111111111112}, "xu:videogigagan:2024": {"bib_key": "xu:videogigagan:2024", "bib_title": "{VideoGigaGAN: Towards} Detail-rich Video Super-Resolution", "bib_author ": "Xu, Yiran", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024,wang2025seedvr}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "Recently, Adobe announced their VideoGigaGAN\\cite{xu:videogigagan:2024}", "next_context": ", which can perform 8\\timesupsampling."}], "importance_score": 1.1111111111111112}, "wang2025seedvr": {"bib_key": "wang2025seedvr", "bib_title": "{SeedVR: Seeding} Infinity in Diffusion Transformer Towards Generic Video Restoration", "bib_author ": "Wang, Jianyi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024,wang2025seedvr}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023, cao2025zero, wang2025seedvr}", "next_context": ""}], "importance_score": 0.3111111111111111}, "Saharia:image:2023": {"bib_key": "Saharia:image:2023", "bib_title": "Image Super-Resolution via Iterative Refinement", "bib_author ": "Saharia, Chitwan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023, cao2025zero, wang2025seedvr}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "For DMs, SR3 by Google\\cite{Saharia:image:2023}", "next_context": "has produced truly impressive results."}], "importance_score": 1.4}, "Moliner:solving:2023": {"bib_key": "Moliner:solving:2023", "bib_title": "Solving Audio Inverse Problems with a Diffusion Model", "bib_author ": "Moliner, Eloi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023, cao2025zero, wang2025seedvr}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Moliner:solving:2023, Fei:Generative:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "\\cite{Moliner:solving:2023}", "next_context": "tackle problems such as audio bandwidth extension, inpainting, and declipping by treating them as inverse problems using a diffusion model."}], "importance_score": 1.7}, "Gao:Implicit:2023": {"bib_key": "Gao:Implicit:2023", "bib_title": "Implicit Diffusion Models for Continuous Super-Resolution", "bib_author ": "Gao, Sicheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023, cao2025zero, wang2025seedvr}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "Later, IDM\\cite{Gao:Implicit:2023}", "next_context": "combines INR with a U-Net denoising model in the reverse process of the DM."}], "importance_score": 1.4}, "cao2025zero": {"bib_key": "cao2025zero", "bib_title": "Zero-shot Video Restoration and Enhancement Using Pre-Trained Image Diffusion Model", "bib_author ": "Chao Cao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023, cao2025zero, wang2025seedvr}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023, cao2025zero, feng2025residual}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "\\cite{cao2025zero}", "next_context": "introduce a zero-shot video super-resolution framework that leverages a pre-trained image diffusion model, and replaces the spatial self-attention layer with a novel short-long-range (SLR) temporal attention layer."}], "importance_score": 1.342857142857143}, "Chen:Learning:2021": {"bib_key": "Chen:Learning:2021", "bib_title": "Learning Continuous Image Representation with Local Implicit Image Function", "bib_author ": "Chen, Yinbo", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "The latter exploits INR to upsample a continuous coordinate-based flow map, similar to SISR technique proposed in\\cite{Chen:Learning:2021}", "next_context": "."}], "importance_score": 1.2}, "Fei:Generative:2023": {"bib_key": "Fei:Generative:2023", "bib_title": "Generative Diffusion Prior for Unified Image Restoration and Enhancement", "bib_author ": "Fei, Ben", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023, cao2025zero, feng2025residual}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Moliner:solving:2023, Fei:Generative:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "introduced the Generative Diffusion Prior\\cite{Fei:Generative:2023}", "next_context": "for unsupervised learning, aiming to model posterior distributions for image restoration and enhancement."}], "importance_score": 1.842857142857143}, "Yin:CLE:2023": {"bib_key": "Yin:CLE:2023", "bib_title": "{CLE Diffusion: Controllable} Light Enhancement Diffusion Model", "bib_author ": "Yin, Yuyang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "For instance, CLE Diffusion\\cite{Yin:CLE:2023}", "next_context": "enables user-friendly editing of lighting with fine-grained regional controllability."}], "importance_score": 1.2}, "Wang:Uformer:2022": {"bib_key": "Wang:Uformer:2022", "bib_title": "Uformer: A General U-Shaped Transformer for Image Restoration", "bib_author ": "Wang, Zhendong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "Other two popular approaches that emerged in the same timeframe are Uformer\\cite{Wang:Uformer:2022}", "next_context": "and Restormer\\cite{Zamir:Restormer:2022}."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "Denoising with diffusion models (DMs)\\cite{yang:realworld:2023}has been proposed by diffusing with estimated noise that is closer to real-world noise rather than Gaussian noise, achieving better performance than SwinIR\\cite{Liang:SwinIR:2021}and Uformer\\cite{Wang:Uformer:2022}", "next_context": "."}], "importance_score": 2.05}, "Zamir:Restormer:2022": {"bib_key": "Zamir:Restormer:2022", "bib_title": "Restormer: Efficient Transformer for High-Resolution Image Restoration", "bib_author ": "Zamir, Syed Waqas", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "Other two popular approaches that emerged in the same timeframe are Uformer\\cite{Wang:Uformer:2022}and Restormer\\cite{Zamir:Restormer:2022}", "next_context": "."}], "importance_score": 1.05}, "yang:ldp:2023": {"bib_key": "yang:ldp:2023", "bib_title": "{LDP}: Language-driven Dual-Pixel Image Defocus Deblurring Network", "bib_author ": "Hao Yang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "\\cite{yang:ldp:2023}", "next_context": "."}], "importance_score": 1.05}, "Morris:DaBiT:2024": {"bib_key": "Morris:DaBiT:2024", "bib_title": "{DaBiT: Depth} and Blur informed Transformer for Video Deblurring", "bib_author ": "C Morris", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "DaBiT\\cite{Morris:DaBiT:2024}", "next_context": "mitigates focal blur content with depth information and applies SR for further enhancing fine details."}], "importance_score": 1.05}, "Yu:DBT:2022": {"bib_key": "Yu:DBT:2022", "bib_title": "DBT-Net: Dual-Branch Federative Magnitude and Phase Estimation With Attention-in-Attention Transformer for Monaural Speech Enhancement", "bib_author ": "Yu, Guochen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "There have been efforts to utilize AI for learning global contextual information to aid in the removal of unwanted sounds, leading to better final quality\\cite{Yu:DBT:2022}", "next_context": "."}], "importance_score": 1.05}, "Song:vision:2023": {"bib_key": "Song:vision:2023", "bib_title": "Vision Transformers for Single Image Dehazing", "bib_author ": "Song, Yuda", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "iii)\\textbf{Dehazing}: Vision transformers for single image dehazing was proposed in DehazeFormer\\cite{Song:vision:2023}", "next_context": "."}], "importance_score": 1.05}, "Xu:Video:2023": {"bib_key": "Xu:Video:2023", "bib_title": "Video Dehazing via a Multi-Range Temporal Alignment Network With Physical Prior", "bib_author ": "Xu, Jiaqi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "\\cite{Xu:Video:2023}", "next_context": "introduced a recurrent multi-range scene radiance recovery module with the space-time deformable attention."}], "importance_score": 1.05}, "mao:single:2022": {"bib_key": "mao:single:2022", "bib_title": "Single Frame Atmospheric Turbulence Mitigation: A Benchmark Study and a New Physics-Inspired Transformer Model", "bib_author ": "Mao, Zhiyuan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025}", "next_context": ""}], "importance_score": 0.05}, "Zhang:Image:2024": {"bib_key": "Zhang:Image:2024", "bib_title": "Imaging Through the Atmosphere Using Turbulence Mitigation Transformer", "bib_author ": "Zhang, Xingguang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "However, diffusion models have shown superior performance on single-image restoration tasks\\cite{Nair:AT-DDPM:2023}, while transformer-based methods remained the state-of-the-art for video restoration\\cite{Zhang:Image:2024, zou2024deturb}", "next_context": "."}], "importance_score": 0.55}, "zou2024deturb": {"bib_key": "zou2024deturb", "bib_title": "{DeTurb: Atmospheric} Turbulence Mitigation with Deformable 3D Convolutions and 3D Swin Transformers", "bib_author ": "Zou, Z.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "However, diffusion models have shown superior performance on single-image restoration tasks\\cite{Nair:AT-DDPM:2023}, while transformer-based methods remained the state-of-the-art for video restoration\\cite{Zhang:Image:2024, zou2024deturb}", "next_context": "."}], "importance_score": 0.55}, "fang2025guided": {"bib_key": "fang2025guided", "bib_title": "Guided Real Image Dehazing Using YCbCr Color Space", "bib_author ": "Weizhi Fang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "The Fast Fourier Transform (FFT) has been employed in\\cite{fang2025guided}", "next_context": "due to the phase spectrum conveying more structural detail than the amplitude spectrum and demonstrating greater robustness to contrast distortion and noise."}], "importance_score": 1.05}, "Yue:RViDeformer:2025": {"bib_key": "Yue:RViDeformer:2025", "bib_title": "{RViDeformer: Efficient} Raw Video Denoising Transformer with a Larger Benchmark Dataset", "bib_author ": "Yue, Huanjing", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "Similarly, recent advances in video denoising also adopt a hybrid approach that integrates both architectures\\cite{Jin:Masked:2025,  Yue:RViDeformer:2025}", "next_context": "."}], "importance_score": 0.6}, "Jin:Masked:2025": {"bib_key": "Jin:Masked:2025", "bib_title": "Masked Video Pretraining Advances Real-World Video Denoising", "bib_author ": "Jin, Yi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "Similarly, recent advances in video denoising also adopt a hybrid approach that integrates both architectures\\cite{Jin:Masked:2025,  Yue:RViDeformer:2025}", "next_context": "."}], "importance_score": 0.55}, "Shi:VmambaIR:2025": {"bib_key": "Shi:VmambaIR:2025", "bib_title": "{VmambaIR: Visual} State Space Model for Image Restoration", "bib_author ": "Shi, Yuan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "VmambaIR\\cite{Shi:VmambaIR:2025}", "next_context": "incorporates Mamba blocks into the U-Net architecture, achieving superior performance compared to SwinIR and Restormer in both visual quality and model size."}], "importance_score": 1.05}, "yang:realworld:2023": {"bib_key": "yang:realworld:2023", "bib_title": "Real-World Denoising via Diffusion Model", "bib_author ": "Yang, Cheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023, cao2025zero, feng2025residual}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "Denoising with diffusion models (DMs)\\cite{yang:realworld:2023}", "next_context": "has been proposed by diffusing with estimated noise that is closer to real-world noise rather than Gaussian noise, achieving better performance than SwinIR\\cite{Liang:SwinIR:2021}and Uformer\\cite{Wang:Uformer:2022}."}], "importance_score": 1.1428571428571428}, "Nair:AT-DDPM:2023": {"bib_key": "Nair:AT-DDPM:2023", "bib_title": "{AT-DDPM: Restoring} Faces Degraded by Atmospheric Turbulence Using Denoising Diffusion Probabilistic Models", "bib_author ": "Nair, Nithin Gopalakrishnan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023, cao2025zero, feng2025residual}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "However, diffusion models have shown superior performance on single-image restoration tasks\\cite{Nair:AT-DDPM:2023}", "next_context": ", while transformer-based methods remained the state-of-the-art for video restoration\\cite{Zhang:Image:2024, zou2024deturb}."}], "importance_score": 1.1428571428571428}, "Jaiswal:Physics:2023": {"bib_key": "Jaiswal:Physics:2023", "bib_title": "Physics-Driven Turbulence Image Restoration with Stochastic Refinement", "bib_author ": "Jaiswal, Ajay", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023, cao2025zero, feng2025residual}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "iv)\\textbf{Mitigating atmospheric turbulence}: Similar to dehazing, physics-inspired models have been widely developed to remove turbulence distortion\\cite{Jaiswal:Physics:2023,Jiang:NeRT:2023}", "next_context": ", while complex-valued CNNs have been proposed to exploit phase information\\cite{Atmospheric:2023}."}], "importance_score": 0.6428571428571428}, "feng2025residual": {"bib_key": "feng2025residual", "bib_title": "Residual Diffusion Deblurring Model for Single Image Defocus Deblurring", "bib_author ": "Haotian Feng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023, cao2025zero, feng2025residual}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "\\cite{feng2025residual}", "next_context": "proposed a novel residual diffusion deblurring framework that integrates a conditional diffusion model guided by a defocus map and incorporates residual learning into the single-image defocus deblurring process."}], "importance_score": 1.1428571428571428}, "Jiang:NeRT:2023": {"bib_key": "Jiang:NeRT:2023", "bib_title": "{NeRT: Implicit} Neural Representations for Unsupervised Atmospheric Turbulence Mitigation", "bib_author ": "Jiang, Weiyun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Jiang:NeRT:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "iv)\\textbf{Mitigating atmospheric turbulence}: Similar to dehazing, physics-inspired models have been widely developed to remove turbulence distortion\\cite{Jaiswal:Physics:2023,Jiang:NeRT:2023}", "next_context": ", while complex-valued CNNs have been proposed to exploit phase information\\cite{Atmospheric:2023}."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "There was also an attempt to use instance normalization (INR) to address this issue, providing solutions for tile and blur correction\\cite{Jiang:NeRT:2023}", "next_context": "."}], "importance_score": 2.5}, "Li:MAT:2022": {"bib_key": "Li:MAT:2022", "bib_title": "{MAT: Mask}-Aware Transformer for Large Hole Image Inpainting", "bib_author ": "Li, Wenbo", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Inpainting", "prev_context": "\\cite{Li:MAT:2022}", "next_context": "offers several outputs to fill a large missing area, consisting of a convolutional head, a transformer body, and a convolutional tail for reconstruction, along with a Conv-U-Net for refinement."}], "importance_score": 1.2}, "Liu:Reduce:2022": {"bib_key": "Liu:Reduce:2022", "bib_title": "Reduce Information Loss in Transformers for Pluralistic Image Inpainting", "bib_author ": "Liu, Qiankun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Inpainting", "prev_context": "PUT\\cite{Liu:Reduce:2022}", "next_context": "proposes a patch-based vector VQ-VAE and unquantized Transformer to minimize information loss."}], "importance_score": 1.2}, "Ren:DLFormer:2022": {"bib_key": "Ren:DLFormer:2022", "bib_title": "{DLFormer: Discrete} Latent Transformer for Video Inpainting", "bib_author ": "Ren, Jingjing", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Inpainting", "prev_context": "\\cite{Ren:DLFormer:2022}", "next_context": "and  ProPainter\\cite{Zhou:ProPainter:2023}."}], "importance_score": 1.2}, "Zhou:ProPainter:2023": {"bib_key": "Zhou:ProPainter:2023", "bib_title": "{ProPainter: Improving} Propagation and Transformer for Video Inpainting", "bib_author ": "Zhou, Shangchen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Inpainting", "prev_context": "\\cite{Ren:DLFormer:2022}and  ProPainter\\cite{Zhou:ProPainter:2023}", "next_context": "."}], "importance_score": 1.2}, "HUANG:Sparse:2024": {"bib_key": "HUANG:Sparse:2024", "bib_title": "Sparse self-attention transformer for image inpainting", "bib_author ": "Wenli Huang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Inpainting", "prev_context": "Spa-former\\cite{HUANG:Sparse:2024}", "next_context": "employs a UNet-like architecture, where each level performs transformer with sparse self-attention to remove coefficients with low or no correlation, leading to memory reduction, while improving result quality by up to 5\\%compared to PUT."}], "importance_score": 1.2}, "Ma:SwinFusion:2022": {"bib_key": "Ma:SwinFusion:2022", "bib_title": "{SwinFusion: Cross-domain} Long-range Learning for General Image Fusion via Swin Transformer", "bib_author ": "Ma, Jiayi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023, LI2024102147}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Image Fusion", "prev_context": "Most methods use CNNs for feature extraction, with transformers operating in the latent space\\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Image Fusion", "prev_context": "\\cite{Ma:SwinFusion:2022}", "next_context": ", which utilizes a self-attention-based intra-domain fusion unit and a cross-attention-based inter-domain fusion unit to achieve multi-modal and digital photography image fusion."}], "importance_score": 1.75}, "Rao:TGFuse:2023": {"bib_key": "Rao:TGFuse:2023", "bib_title": "{TGFuse: An} Infrared and Visible Image Fusion Approach Based on Transformer and Generative Adversarial Network", "bib_author ": "Rao, Dongyu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023, LI2024102147}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Image Fusion", "prev_context": "Most methods use CNNs for feature extraction, with transformers operating in the latent space\\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023}", "next_context": "."}], "importance_score": 0.75}, "Liu:Multi:2023": {"bib_key": "Liu:Multi:2023", "bib_title": "Multi-interactive Feature Learning and a Full-time Multi-modality Benchmark for Image Fusion and Segmentation", "bib_author ": "Liu, Jinyuan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023, LI2024102147}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Image Fusion", "prev_context": "Transformer-based image fusion has also been applied to downstream tasks like segmentation\\cite{Liu:Multi:2023}", "next_context": ", achieving superior results by leveraging the additional information."}], "importance_score": 1.25}, "LI2024102147": {"bib_key": "LI2024102147", "bib_title": "{CrossFuse: A} novel cross attention mechanism based infrared and visible image fusion approach", "bib_author ": "Hui Li", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023, LI2024102147}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Image Fusion", "prev_context": "Self-attention blocks are employed to enhance intra-feature representations, while the cross-attention mechanism integrates inter-feature information to improve the quality of the fused output\\cite{LI2024102147}", "next_context": "."}], "importance_score": 1.25}, "Zhao:DDFM:2023": {"bib_key": "Zhao:DDFM:2023", "bib_title": "{DDFM: Denoising} Diffusion Model for Multi-Modality Image Fusion", "bib_author ": "Zhao, Zixiang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Zhao:DDFM:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Image Fusion", "prev_context": "\\cite{Zhao:DDFM:2023}", "next_context": "."}], "importance_score": 2.0}, "Shi:Motion-I2V:2024": {"bib_key": "Shi:Motion-I2V:2024", "bib_title": "{Motion-I2V: Consistent} and Controllable Image-to-Video Generation with Explicit Motion Modeling", "bib_author ": "Shi, Xiaoyu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Shi:Motion-I2V:2024}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Shi:Motion-I2V:2024,guo2024liveportrait}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Editing and Visual Special Effects (VFX)", "prev_context": "Motion-I2V\\cite{Shi:Motion-I2V:2024}", "next_context": "provides motion blur and motion drag tools to control specific areas of an image to add motion."}], "importance_score": 2.5}, "guo2024liveportrait": {"bib_key": "guo2024liveportrait", "bib_title": "{LivePortrait: Efficient} Portrait Animation with Stitching and Retargeting Control", "bib_author ": "Guo, Jianzhu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Shi:Motion-I2V:2024,guo2024liveportrait}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Editing and Visual Special Effects (VFX)", "prev_context": "Editing or modifying specific areas of an image is much easier with DM technologies, particularly for headshot photos, such as targeting the eyes and mouth on the face\\cite{guo2024liveportrait}", "next_context": "."}], "importance_score": 1.5}, "Bowen:Marked:2022": {"bib_key": "Bowen:Marked:2022", "bib_title": "Masked-attention Mask Transformer for Universal Image Segmentation", "bib_author ": "Bowen Cheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2,Zhang:DVISp:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "\\cite{Bowen:Marked:2022}", "next_context": "."}], "importance_score": 1.1111111111111112}, "Kirillov:SAM:2023": {"bib_key": "Kirillov:SAM:2023", "bib_title": "Segment Anything", "bib_author ": "Kirillov, Alex", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2,Zhang:DVISp:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "Among them, Segment Anything (SAM) by Meta AI\\cite{Kirillov:SAM:2023}", "next_context": "stands out as a pioneer in promptable segmentation approaches."}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "TAM combines SAM\\cite{Kirillov:SAM:2023}", "next_context": "and XMem\\cite{cheng:xmem:2022}, offering tracking and segmentation performance on the human-selected target."}], "importance_score": 2.111111111111111}, "Ke:SAM-HQ:2023": {"bib_key": "Ke:SAM-HQ:2023", "bib_title": "Segment Anything in High Quality", "bib_author ": "Ke, Lei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2,Zhang:DVISp:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "HQ-SAM\\cite{Ke:SAM-HQ:2023}", "next_context": "enhances SAM by incorporating global-local feature fusion, leading to high-quality mask predictions."}], "importance_score": 1.1111111111111112}, "Wang:SegGPT:2023": {"bib_key": "Wang:SegGPT:2023", "bib_title": "{SegGPT}: Towards Segmenting Everything In Context", "bib_author ": "Wang, Xinlong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2,Zhang:DVISp:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "SegGPT\\cite{Wang:SegGPT:2023}", "next_context": "proposed context ensemble strategies and allows users to tune a prompt for a specific dataset, scene, or even a person, while SEEM\\cite{Zou:Segment:2023}provides a completely promptable and interactive segmentation interface."}], "importance_score": 1.1111111111111112}, "Zou:Segment:2023": {"bib_key": "Zou:Segment:2023", "bib_title": "Segment Everything Everywhere All at Once", "bib_author ": "Zou, Xueyan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2,Zhang:DVISp:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "SegGPT\\cite{Wang:SegGPT:2023}proposed context ensemble strategies and allows users to tune a prompt for a specific dataset, scene, or even a person, while SEEM\\cite{Zou:Segment:2023}", "next_context": "provides a completely promptable and interactive segmentation interface."}], "importance_score": 1.1111111111111112}, "Oquab:DINOv2:2024": {"bib_key": "Oquab:DINOv2:2024", "bib_title": "{DINOv2: Learning} Robust Visual Features without Supervision", "bib_author ": "Maxime Oquab", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2,Zhang:DVISp:2025}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024, LIU:DSEM:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": null, "prev_context": "\\cite{Oquab:DINOv2:2024}", "next_context": "has introduced DINOv2, aimed at enriching information about visual content through self-supervised learning."}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Depth Estimation", "prev_context": "\\cite{Oquab:DINOv2:2024}", "next_context": ", which is trained with vision transformers."}], "importance_score": 2.344877344877345}, "ravi2024sam2": {"bib_key": "ravi2024sam2", "bib_title": "SAM 2: Segment Anything in Images and Videos", "bib_author ": "Ravi, Nikhila", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2,Zhang:DVISp:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "More recently, SAM2\\cite{ravi2024sam2}", "next_context": "introduced support for real-time video segmentation."}], "importance_score": 1.1111111111111112}, "Zhang:DVISp:2025": {"bib_key": "Zhang:DVISp:2025", "bib_title": "{DVIS++: Improved} Decoupled Framework for Universal Video Segmentation", "bib_author ": "Zhang, Tao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2,Zhang:DVISp:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "DVIS++ by\\cite{Zhang:DVISp:2025}", "next_context": "introduces a universal video segmentation framework capable of producing instance, semantic, and panoptic segmentation outputs."}], "importance_score": 1.1111111111111112}, "Wu:DiffuMask:2023": {"bib_key": "Wu:DiffuMask:2023", "bib_title": "{DiffuMask: Synthesizing} Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models", "bib_author ": "Wu, Weijia", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "DiffuMask\\cite{Wu:DiffuMask:2023}", "next_context": "automatically generate image and pixel-level semantic annotation using pre-trained Stable Diffusion with input as a text prompt."}], "importance_score": 1.3333333333333333}, "Xu:Open:2023": {"bib_key": "Xu:Open:2023", "bib_title": "Open-Vocabulary Panoptic Segmentation With Text-to-Image Diffusion Models", "bib_author ": "Xu, Jiarui", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "\\cite{Xu:Open:2023}", "next_context": ", outperforming the previous methods by up to 7.6\\%."}], "importance_score": 1.3333333333333333}, "Gu:Diffusioninst:2024": {"bib_key": "Gu:Diffusioninst:2024", "bib_title": "{Diffusioninst: Diffusion} Model for Instance Segmentation", "bib_author ": "Gu, Zhangxuan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024}", "next_context": ""}], "importance_score": 0.3333333333333333}, "Gong:Continuous:2023": {"bib_key": "Gong:Continuous:2023", "bib_title": "Continuous Pseudo-Label Rectified Domain Adaptive Semantic Segmentation With Implicit Neural Representations", "bib_author ": "Gong, Rui", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Gong:Continuous:2023, Cen:Segment:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "In creative technologies, unsupervised domain adaptation (UDA) and INRs are used for continuous rectification function modeling in\\cite{Gong:Continuous:2023}", "next_context": ", achieving superior segmentation results in night vision."}], "importance_score": 1.5}, "Cen:Segment:2023": {"bib_key": "Cen:Segment:2023", "bib_title": "Segment Anything in {3D with NeRFs}", "bib_author ": "Cen, Jiazhong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Gong:Continuous:2023, Cen:Segment:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "However, the recent SA3D approach\\cite{Cen:Segment:2023}", "next_context": "segments 3D objects using NeRFs as the structural prior."}], "importance_score": 1.5}, "Carion:DERT:2020": {"bib_key": "Carion:DERT:2020", "bib_title": "End-to-end object detection with transformers", "bib_author ": "Carion, Nicolas", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "Introduced in 2020, DETR by Facebook AI\\cite{Carion:DERT:2020}", "next_context": "was one of the first to adopt a transformer architecture for object detection."}], "importance_score": 1.0909090909090908}, "Zhu:Deformable:2021": {"bib_key": "Zhu:Deformable:2021", "bib_title": "{Deformable DETR: Deformable} Transformers for End-to-End Object Detection", "bib_author ": "Zhu, Xizhou", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "Deformable convolution has alson been used, (Deformable DETR\\cite {Zhu:Deformable:2021}", "next_context": "), resulting in training faster with approximately 5\\%accuracy improvement."}], "importance_score": 1.0909090909090908}, "Neimark:video:2021": {"bib_key": "Neimark:video:2021", "bib_title": "Video Transformer Network", "bib_author ": "Neimark, Daniel", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12}", "next_context": ""}], "importance_score": 0.09090909090909091}, "Huang:MonoDTR:2022": {"bib_key": "Huang:MonoDTR:2022", "bib_title": "{MonoDTR: Monocular} {3D} Object Detection with Depth-Aware Transformer", "bib_author ": "Kuan-Chih Huang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "To detect 3D objects, the transformer-based method MonoDTR\\cite{Huang:MonoDTR:2022}", "next_context": "incorporates depth estimation from a single 2D image\\cite{Yang:depthanything:2024}to predict 3D bounding boxes."}], "importance_score": 1.0909090909090908}, "zhao:videoprism:2024": {"bib_key": "zhao:videoprism:2024", "bib_title": "{VideoPrism: A} Foundational Visual Encoder for Video Understanding", "bib_author ": "Zhao, Long", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": null, "prev_context": "Google have introduced VideoPrism\\cite{zhao:videoprism:2024}", "next_context": ", a tool for scene understanding including classification, localization, retrieval, captioning, and question answering (QA)."}], "importance_score": 1.0909090909090908}, "im2025gate3d": {"bib_key": "im2025gate3d", "bib_title": "{GATE3D: Generalized Attention-based Task-synergized Estimation in 3D}", "bib_author ": "Eunsoo Im", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "More 3D object detection methods have been developed for autonomous driving\\cite{10637966}; however, these approaches can also be adapted for AR and VR applications\\cite{im2025gate3d}", "next_context": "."}], "importance_score": 1.0909090909090908}, "tian2025yolov12": {"bib_key": "tian2025yolov12", "bib_title": "YOLOv12: Attention-Centric Real-Time Object Detectors", "bib_author ": "Tian, Yunjie", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "Recently, YOLO12\\cite{tian2025yolov12}", "next_context": "introduced an attention-centric architecture, achieving a 2.1\\%and 1.2\\%mAP improvement over YOLOv10-N and YOLOv11-N respectively, with only a slight decrease in speed."}], "importance_score": 1.0909090909090908}, "Li:Your:2023": {"bib_key": "Li:Your:2023", "bib_title": "Your Diffusion Model is Secretly a Zero-Shot Classifier", "bib_author ": "Li, Alex", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:Your:2023,Chen:DiffusionDet:2023, Zhang:DiffAD:2025, WU2025102965}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "\\cite{Li:Your:2023}", "next_context": "."}], "importance_score": 1.25}, "Chen:DiffusionDet:2023": {"bib_key": "Chen:DiffusionDet:2023", "bib_title": "DiffusionDet: Diffusion Model for Object Detection", "bib_author ": "Chen, Shoufa", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:Your:2023,Chen:DiffusionDet:2023, Zhang:DiffAD:2025, WU2025102965}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "DiffusionDet\\cite{Chen:DiffusionDet:2023}", "next_context": "formulates object detection as a denoising diffusion process from noisy boxes to object boxes, reporting performance that surpasses DETR."}], "importance_score": 1.25}, "Zhang:DiffAD:2025": {"bib_key": "Zhang:DiffAD:2025", "bib_title": "{DiffusionAD: Norm}-Guided One-Step Denoising Diffusion for Anomaly Detection", "bib_author ": "Zhang, Hui", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:Your:2023,Chen:DiffusionDet:2023, Zhang:DiffAD:2025, WU2025102965}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "DMs have also been employed for anomaly detection\\cite{Zhang:DiffAD:2025, WU2025102965}", "next_context": ", functioning similarly to zero-shot classifiers."}], "importance_score": 0.75}, "WU2025102965": {"bib_key": "WU2025102965", "bib_title": "{MAFCD: Multi}-level and adaptive conditional diffusion model for anomaly detection", "bib_author ": "Zhichao Wu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:Your:2023,Chen:DiffusionDet:2023, Zhang:DiffAD:2025, WU2025102965}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "DMs have also been employed for anomaly detection\\cite{Zhang:DiffAD:2025, WU2025102965}", "next_context": ", functioning similarly to zero-shot classifiers."}], "importance_score": 0.75}, "Meinhardt:TrackFormer:2022": {"bib_key": "Meinhardt:TrackFormer:2022", "bib_title": "TrackFormer: Multi-Object Tracking With Transformers", "bib_author ": "Meinhardt, Tim", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "The first three tracking-by-attention approaches are TrackFormer\\cite{Meinhardt:TrackFormer:2022}", "next_context": ", MixFormer\\cite{cui:mixformer:2022}, and ToMP\\cite{Mayer:Transforming:2022}."}], "importance_score": 1.1111111111111112}, "zeng:motr:2022": {"bib_key": "zeng:motr:2022", "bib_title": "MOTR: End-to-End Multiple-Object Tracking with TRansformer", "bib_author ": "Zeng, Fangao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "MOTRv2\\cite{Zhang:MOTRv2:2023}combines YOLOX\\cite{ge:yolox:2021}for object recognition and MOTR\\cite{zeng:motr:2022}", "next_context": "for tracking, outperforming TrackFormer by 20\\%."}], "importance_score": 1.1111111111111112}, "cui:mixformer:2022": {"bib_key": "cui:mixformer:2022", "bib_title": "Mixformer: End-to-end tracking with iterative mixed attention", "bib_author ": "Cui, Yutao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "The first three tracking-by-attention approaches are TrackFormer\\cite{Meinhardt:TrackFormer:2022}, MixFormer\\cite{cui:mixformer:2022}", "next_context": ", and ToMP\\cite{Mayer:Transforming:2022}."}], "importance_score": 1.1111111111111112}, "Mayer:Transforming:2022": {"bib_key": "Mayer:Transforming:2022", "bib_title": "Transforming Model Prediction for Tracking", "bib_author ": "Mayer, Christoph", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "The first three tracking-by-attention approaches are TrackFormer\\cite{Meinhardt:TrackFormer:2022}, MixFormer\\cite{cui:mixformer:2022}, and ToMP\\cite{Mayer:Transforming:2022}", "next_context": "."}], "importance_score": 1.1111111111111112}, "yang:track:2023": {"bib_key": "yang:track:2023", "bib_title": "Track Anything: Segment Anything Meets Videos", "bib_author ": "Jinyu Yang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "Many more methods have been proposed, including SeqTrack\\cite{Chen:SeqTrack:2023}and Track Anything Model (TAM)\\cite{yang:track:2023}", "next_context": "."}], "importance_score": 1.1111111111111112}, "Chen:SeqTrack:2023": {"bib_key": "Chen:SeqTrack:2023", "bib_title": "SeqTrack: Sequence to Sequence Learning for Visual Object Tracking", "bib_author ": "Chen, Xin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "Many more methods have been proposed, including SeqTrack\\cite{Chen:SeqTrack:2023}", "next_context": "and Track Anything Model (TAM)\\cite{yang:track:2023}."}], "importance_score": 1.1111111111111112}, "Zhang:MOTRv2:2023": {"bib_key": "Zhang:MOTRv2:2023", "bib_title": "{MOTRv2: Bootstrapping} End-to-End Multi-Object Tracking by Pretrained Object Detectors", "bib_author ": "Y. Zhang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "MOTRv2\\cite{Zhang:MOTRv2:2023}", "next_context": "combines YOLOX\\cite{ge:yolox:2021}for object recognition and MOTR\\cite{zeng:motr:2022}for tracking, outperforming TrackFormer by 20\\%."}], "importance_score": 1.1111111111111112}, "Yi:Comprehensive:2024": {"bib_key": "Yi:Comprehensive:2024", "bib_title": "A Comprehensive Study of Object Tracking in Low-Light Environments", "bib_author ": "Yi, A", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "Additionally, some methods have been specifically proposed for challenging environments, such as low light\\cite{Yi:Comprehensive:2024}", "next_context": "and small objects, as seen in AnyFlow\\cite{Jung:AnyFlow:2023}."}], "importance_score": 1.1111111111111112}, "kang2025exploring": {"bib_key": "kang2025exploring", "bib_title": "Exploring Enhanced Contextual Information for Video-Level Object Tracking", "bib_author ": "Ben Kang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "The current state-of-the-art for single-object tracking\\footnote{https://paperswithcode.com/sota/visual-object-tracking-on-lasot}, however, is based on cross-attention and Mamba\\cite{kang2025exploring}", "next_context": "."}], "importance_score": 1.1111111111111112}, "Luo:DiffusionTrack:2024": {"bib_key": "Luo:DiffusionTrack:2024", "bib_title": "{DiffusionTrack: Diffusion} Model for Multi-Object Tracking", "bib_author ": "Luo, Run", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "However, a spatial-temporal fusion module has been added to the diffusion head to exploit temporal video features\\cite{Luo:DiffusionTrack:2024}", "next_context": "."}], "importance_score": 1.3333333333333333}, "Xie:DiffusionTrack:2024": {"bib_key": "Xie:DiffusionTrack:2024", "bib_title": "{DiffusionTrack: Point} Set Diffusion Model for Visual Object Tracking", "bib_author ": "Xie, Fei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "DiffusionTrack\\cite{Xie:DiffusionTrack:2024}", "next_context": "localizes the target in a progressive diffusion manner, which is claimed to better handle challenging scenarios."}], "importance_score": 1.3333333333333333}, "Zhang:DiffusionTracker:2024": {"bib_key": "Zhang:DiffusionTracker:2024", "bib_title": "{DiffusionTracker: Targets} Denoising Based on Diffusion Model for Visual Tracking", "bib_author ": "Zhang, Runqing", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "The method in\\cite{Zhang:DiffusionTracker:2024}", "next_context": "exploits spatial-temporal weighting to suppress the probability of the tracker changing the target to the distractors."}], "importance_score": 1.3333333333333333}, "Jung:AnyFlow:2023": {"bib_key": "Jung:AnyFlow:2023", "bib_title": "{AnyFlow: Arbitrary} Scale Optical Flow With Implicit Neural Representation", "bib_author ": "Jung, Hyunyoung", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Jung:AnyFlow:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "Additionally, some methods have been specifically proposed for challenging environments, such as low light\\cite{Yi:Comprehensive:2024}and small objects, as seen in AnyFlow\\cite{Jung:AnyFlow:2023}", "next_context": "."}], "importance_score": 2.0}, "Wang:multi:2021": {"bib_key": "Wang:multi:2021", "bib_title": "Multi-View {3D} Reconstruction With Transformers", "bib_author ": "Wang, Dan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024, LIU:DSEM:2025}", "next_context": ""}], "importance_score": 0.14285714285714285}, "Zhang:Lite:2023": {"bib_key": "Zhang:Lite:2023", "bib_title": "{Lite-Mono: A} Lightweight CNN and Transformer Architecture for Self-Supervised Monocular Depth Estimation", "bib_author ": "Zhang, Ning", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024, LIU:DSEM:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Depth Estimation", "prev_context": "There have been attempts to use transformers, such as\\cite{Zhang:Lite:2023}", "next_context": "and\\cite{Chen:Vision:2023}, and diffusion models, such as\\cite{Ji:DDP:2023}and\\cite{Ke:Repurposing:2024}."}], "importance_score": 1.1428571428571428}, "Chen:Vision:2023": {"bib_key": "Chen:Vision:2023", "bib_title": "Vision Transformer Adapter for Dense Predictions", "bib_author ": "Zhe Chen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024, LIU:DSEM:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Depth Estimation", "prev_context": "There have been attempts to use transformers, such as\\cite{Zhang:Lite:2023}and\\cite{Chen:Vision:2023}", "next_context": ", and diffusion models, such as\\cite{Ji:DDP:2023}and\\cite{Ke:Repurposing:2024}."}], "importance_score": 1.1428571428571428}, "Yang:depthanything:2024": {"bib_key": "Yang:depthanything:2024", "bib_title": "Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data", "bib_author ": "Yang, Lihe", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024, LIU:DSEM:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "To detect 3D objects, the transformer-based method MonoDTR\\cite{Huang:MonoDTR:2022}incorporates depth estimation from a single 2D image\\cite{Yang:depthanything:2024}", "next_context": "to predict 3D bounding boxes."}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Depth Estimation", "prev_context": "It is built on the previous version\\cite{Yang:depthanything:2024}", "next_context": ", jointly trained on large-scale labeled and unlabeled images and uses semantic priors from pretrained encoders."}], "importance_score": 2.142857142857143}, "Yang:depthanythingv2:2024": {"bib_key": "Yang:depthanythingv2:2024", "bib_title": "Depth Anything V2", "bib_author ": "Yang, Lihe", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024, LIU:DSEM:2025}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Depth Estimation", "prev_context": "Amongst these, Depth Anything v2\\cite{Yang:depthanythingv2:2024}", "next_context": "has become a state-of-the-art monocular depth estimation method."}], "importance_score": 1.1428571428571428}, "LIU:DSEM:2025": {"bib_key": "LIU:DSEM:2025", "bib_title": "{DSEM-NeRF: Multimodal} feature fusion and global\u2013local attention for enhanced 3D scene reconstruction", "bib_author ": "Dong Liu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024, LIU:DSEM:2025}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "DSEM-NeR\\cite{LIU:DSEM:2025}", "next_context": "integrates the pretrained CLIP model to extract multimodal features\u2014including color, depth, and semantics\u2014from multi-view 2D images, thereby enhancing the reconstruction quality of complex scenes."}], "importance_score": 1.2197802197802199}, "Barron:Mip-NeRF360:2022": {"bib_key": "Barron:Mip-NeRF360:2022", "bib_title": "{Mip-NeRF 360: Unbounded} Anti-Aliased Neural Radiance Fields", "bib_author ": "Barron, Jonathan T.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "Mip-NeRF360\\cite{Barron:Mip-NeRF360:2022}", "next_context": "proposed unbounded anti-aliased technique achieving full 360 degree content."}], "importance_score": 1.3269230769230769}, "Ji:DDP:2023": {"bib_key": "Ji:DDP:2023", "bib_title": "{DDP: Diffusion} Model for Dense Visual Prediction", "bib_author ": "Ji, Yuanfeng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Depth Estimation", "prev_context": "There have been attempts to use transformers, such as\\cite{Zhang:Lite:2023}and\\cite{Chen:Vision:2023}, and diffusion models, such as\\cite{Ji:DDP:2023}", "next_context": "and\\cite{Ke:Repurposing:2024}."}], "importance_score": 1.25}, "wynn:diffusionerf:2023": {"bib_key": "wynn:diffusionerf:2023", "bib_title": "{DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models}", "bib_author ": "Jamie Wynn", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "Diffusion models are integrated to regularize NeRF reconstructions\\cite{wynn:diffusionerf:2023}", "next_context": ", resulting in smoother depth continuity and clearer edges where depth discontinuities occur."}], "importance_score": 1.25}, "Ke:Repurposing:2024": {"bib_key": "Ke:Repurposing:2024", "bib_title": "Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation", "bib_author ": "Ke, Bingxin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Depth Estimation", "prev_context": "There have been attempts to use transformers, such as\\cite{Zhang:Lite:2023}and\\cite{Chen:Vision:2023}, and diffusion models, such as\\cite{Ji:DDP:2023}and\\cite{Ke:Repurposing:2024}", "next_context": "."}], "importance_score": 1.25}, "pumarola:DNeRF:2020": {"bib_key": "pumarola:DNeRF:2020", "bib_title": "{D-NeRF: Neural} Radiance Fields for Dynamic Scenes", "bib_author ": "Pumarola, Albert", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "\\cite{pumarola:DNeRF:2020}", "next_context": ", known as D-NeRF."}], "importance_score": 1.0769230769230769}, "mueller:instant:2022": {"bib_key": "mueller:instant:2022", "bib_title": "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding", "bib_author ": "Thomas M\\\"uller", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Augmented, virtual and mixed reality, and 3D content", "prev_context": "RealFusion\\cite{Melas:RealFusion:2023}, a single image to 3D object generator, merges 2D diffusion models with NeRF, improving Instant-NGP\\cite{mueller:instant:2022}", "next_context": ", which provides an API for VR controls."}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "The Instant-NGP tool developed by Nvidia\\cite{mueller:instant:2022}", "next_context": "enables real-time training of NeRFs by bypassing sampling in empty spaces and dense areas, and by incorporating multi-resolution hash encoding techniques."}], "importance_score": 2.0769230769230766}, "Mildenhall:NeRFDark:2022": {"bib_key": "Mildenhall:NeRFDark:2022", "bib_title": "{NeRF in the Dark: High} Dynamic Range View Synthesis From Noisy Raw Images", "bib_author ": "Mildenhall, Ben", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "Google Research\\cite{Mildenhall:NeRFDark:2022}", "next_context": "trains NeRF from noisy RAW images captured in the dark scene, allowing changing viewpoint, focus, exposure, and tone mapping simultaneously."}], "importance_score": 1.0769230769230769}, "Fang:Fast:2022": {"bib_key": "Fang:Fast:2022", "bib_title": "Fast Dynamic Radiance Fields with Time-Aware Neural Voxels", "bib_author ": "Fang, Jiemin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "However, the current leading method for generating high-quality novel views of real dynamic scenes is TiNeuVox\\cite{Fang:Fast:2022}", "next_context": "."}], "importance_score": 1.0769230769230769}, "Guo:neural:2022": {"bib_key": "Guo:neural:2022", "bib_title": "Neural {3D} Scene Reconstruction With the Manhattan-World Assumption", "bib_author ": "Guo, Haoyu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "With segmentation techniques significantly advanced (see Section\\ref{sssec:seg}), there have been integrations utilizing semantic segmentation to enhance 3D representation\\cite{Guo:neural:2022}", "next_context": "."}], "importance_score": 1.0769230769230769}, "9879447": {"bib_key": "9879447", "bib_title": "{EfficientNeRF - Efficient} Neural Radiance Fields", "bib_author ": "Hu, Tao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "3D Gaussian Splatting", "prev_context": "The main issue with NeRFs as a method to generate high-quality novel views is training time, which can exceed a day for high-resolution content on a single RTX 3090 GPU~\\cite{9879447}", "next_context": "."}], "importance_score": 1.0769230769230769}, "Liu_2024_CVPR": {"bib_key": "Liu_2024_CVPR", "bib_title": "DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing", "bib_author ": "Liu, Jia-Wei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "DynVideo-E~\\cite{Liu_2024_CVPR}", "next_context": "adds an MLP to predict motion fields but focuses on human-centric content."}], "importance_score": 1.0769230769230769}, "azzarelli:waveplanes:2023": {"bib_key": "azzarelli:waveplanes:2023", "bib_title": "{WavePlanes: A} Compact Wavelet Representation for Dynamic Neural Radiance Fields", "bib_author ": "Azzarelli, Adrian", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "Wavelet transform are employed in\\cite{azzarelli:waveplanes:2023}", "next_context": "to further reduce model size."}], "importance_score": 1.0769230769230769}, "zhan2024kfd": {"bib_key": "zhan2024kfd", "bib_title": "{KFD-NeRF}: Rethinking Dynamic NeRF with Kalman Filter", "bib_author ": "Yifan Zhan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "KFD-NeRF~\\cite{zhan2024kfd}", "next_context": "incorporates a Kalman filter-guided deformation field for more accurate motion estimation."}], "importance_score": 1.0769230769230769}, "Tang_2024_CVPR": {"bib_key": "Tang_2024_CVPR", "bib_title": "PaReNeRF: Toward Fast Large-scale Dynamic NeRF with Patch-based Reference", "bib_author ": "Tang, Xiao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "PaReNeRF~\\cite{Tang_2024_CVPR}", "next_context": "addresses large-scale dynamic scenes using patch-based sampling."}], "importance_score": 1.0769230769230769}, "Fridovich:kplanes:2023": {"bib_key": "Fridovich:kplanes:2023", "bib_title": "K-Planes: Explicit Radiance Fields in Space, Time, and Appearance", "bib_author ": "{Sara Fridovich-Keil", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}", "next_context": "^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "Therefore,K-planes\\cite{Fridovich:kplanes:2023}", "next_context": "propose a simple planar factorization for volumetric rendering, achieving low memory usage (1000\\timescompression over a full 4D grid)."}], "importance_score": 1.125}, "kerbl:3Dgaussians:2023": {"bib_key": "kerbl:3Dgaussians:2023", "bib_title": "{3D} Gaussian Splatting for Real-Time Radiance Field Rendering", "bib_author ": "Kerbl, Bernhard", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}", "next_context": "^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "3D Gaussian Splatting", "prev_context": "3D Gaussian Splatting (3D-GS)\\cite{kerbl:3Dgaussians:2023}", "next_context": "has been introduced to address this, using anisotropic 3D Gaussians to form a high-quality, unstructured representation of radiance fields."}], "importance_score": 1.125}, "wu:4dgaussians:2024": {"bib_key": "wu:4dgaussians:2024", "bib_title": "4D Gaussian Splatting for Real-Time Dynamic Scene Rendering", "bib_author ": "Wu, Guanjun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}", "next_context": "^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "3D Gaussian Splatting", "prev_context": "For dynamic scenes, 4D Gaussian Splatting (4D-GS)\\cite{wu:4dgaussians:2024}", "next_context": "introduces a Gaussian deformation field for motion and shape."}], "importance_score": 1.125}, "Yu:CoGS:2024": {"bib_key": "Yu:CoGS:2024", "bib_title": "{CoGS: Controllable} Gaussian Splatting", "bib_author ": "Yu, Heng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}", "next_context": "^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "3D Gaussian Splatting", "prev_context": "Instead of developing in 4D, CoGS\\cite{Yu:CoGS:2024}", "next_context": "exploits 3D-GS by integrating control mechanisms in separate regions to learn individual temporal dimensions."}], "importance_score": 1.125}, "Huang:SCGS:2024": {"bib_key": "Huang:SCGS:2024", "bib_title": "{SC-GS: Sparse}-Controlled Gaussian Splatting for Editable Dynamic Scenes", "bib_author ": "Huang, Yi-Hua", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}", "next_context": "^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "3D Gaussian Splatting", "prev_context": "SC-GS\\cite{Huang:SCGS:2024}", "next_context": "extracts sparse control points and uses an MLP to predict time-varying 6 DoF transformations."}], "importance_score": 1.125}, "Wang2025": {"bib_key": "Wang2025", "bib_title": "{UW-GS: Distractor}-Aware 3D Gaussian Splatting for Enhanced Underwater Scene Reconstruction", "bib_author ": "H. Wang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}", "next_context": "^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "3D Gaussian Splatting", "prev_context": "Physics-inspired approaches are also integrated to improve 3D modeling in different media, such as 3D underwater scenes\\cite{Wang2025}", "next_context": "."}], "importance_score": 1.125}, "junkawitsch2025eva": {"bib_key": "junkawitsch2025eva", "bib_title": "{EVA: Expressive Virtual Avatars from Multi-view Videos}", "bib_author ": "Hendrik Junkawitsch", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}", "next_context": "^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "3D Gaussian Splatting", "prev_context": "For example, EVA\\cite{junkawitsch2025eva}", "next_context": "disentangles the 3D Gaussian appearance into skeletal motion, facial expressions, body movements, and skin."}], "importance_score": 1.125}, "kong2025efficient": {"bib_key": "kong2025efficient", "bib_title": "Efficient Gaussian Splatting for Monocular Dynamic Scene Rendering via Sparse Time-Variant Attribute Modeling", "bib_author ": "Hongwei Kong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}", "next_context": "^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "3D Gaussian Splatting", "prev_context": "\\cite{kong2025efficient}", "next_context": "represent dynamic scenes using sparse, time-variant attribute modeling with a deformable MLP, while efficiently filtering out anchors corresponding to static regions."}], "importance_score": 1.125}, "zhu2022transformer": {"bib_key": "zhu2022transformer", "bib_title": "Transformer-based transform coding", "bib_author ": "Zhu, Yinhao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{zhu2022transformer,zou2022devil,liu2023learned}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "\\cite{zhu2022transformer}", "next_context": ", STF\\cite{zou2022devil}and LIC-TCM\\cite{liu2023learned}."}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "SwinT-ChARM\\cite{zhu2022transformer}", "next_context": "employs Swin transformers for non-linear transforms and outperforms the latest standard image codec, the Versatile Video Coding (VVC) Test Model (VTM, All Intra)."}], "importance_score": 2.333333333333333}, "zou2022devil": {"bib_key": "zou2022devil", "bib_title": "The devil is in the details: Window-based attention for image compression", "bib_author ": "Zou, Renjie", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{zhu2022transformer,zou2022devil,liu2023learned}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "\\cite{zhu2022transformer}, STF\\cite{zou2022devil}", "next_context": "and LIC-TCM\\cite{liu2023learned}."}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "STF\\cite{zou2022devil}", "next_context": "is based on a symmetrical transformer framework containing absolute transformer blocks in both the down-sampling encoder and the up-sampling decoder, which also shows improved rate-quality performance over VTM."}], "importance_score": 2.333333333333333}, "liu2023learned": {"bib_key": "liu2023learned", "bib_title": "Learned image compression with mixed transformer-cnn architectures", "bib_author ": "Liu, Jinming", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{zhu2022transformer,zou2022devil,liu2023learned}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "\\cite{zhu2022transformer}, STF\\cite{zou2022devil}and LIC-TCM\\cite{liu2023learned}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "LIC-TCM\\cite{liu2023learned}", "next_context": "exploits the local modeling ability of CNN and the non-local modeling performance of transformers, and proposes a parallel transformer-CNN mixture block."}], "importance_score": 2.333333333333333}, "careil2023towards": {"bib_key": "careil2023towards", "bib_title": "Towards image compression with perfect realism at ultra-low bitrates", "bib_author ": "Careil, Marlene", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "More recently, diffusion models have been applied in image compression to allow realistic reconstruction at ultra-low bitrates\\cite{careil2023towards}", "next_context": "achieving competitive performance compared to GAN-based models\\cite{yang2024lossy}."}], "importance_score": 1.25}, "yang2024lossy": {"bib_key": "yang2024lossy", "bib_title": "Lossy image compression with conditional diffusion models", "bib_author ": "Yang, Ruihan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "More recently, diffusion models have been applied in image compression to allow realistic reconstruction at ultra-low bitrates\\cite{careil2023towards}achieving competitive performance compared to GAN-based models\\cite{yang2024lossy}", "next_context": "."}], "importance_score": 1.25}, "hoogeboom2023high": {"bib_key": "hoogeboom2023high", "bib_title": "High-fidelity image compression with score-based generative models", "bib_author ": "Hoogeboom, Emiel", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "\\cite{hoogeboom2023high}", "next_context": "and the diffusion-based residual augmentation codec (DIRAC)\\cite{ghouse2023residual}."}], "importance_score": 1.25}, "ghouse2023residual": {"bib_key": "ghouse2023residual", "bib_title": "A residual diffusion model for high perceptual quality codec augmentation", "bib_author ": "Ghouse, Noor Fathima", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "\\cite{hoogeboom2023high}and the diffusion-based residual augmentation codec (DIRAC)\\cite{ghouse2023residual}", "next_context": "."}], "importance_score": 1.25}, "sitzmann2020implicit": {"bib_key": "sitzmann2020implicit", "bib_title": "Implicit neural representations with periodic activation functions", "bib_author ": "Sitzmann, Vincent", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "\\cite{dupont2021coin,dupontcoin++}and\\cite{strumpler2022implicit}that combine SIREN networks\\cite{sitzmann2020implicit}", "next_context": "with positional encoding."}], "importance_score": 1.25}, "dupont2021coin": {"bib_key": "dupont2021coin", "bib_title": "{COIN}: COmpression with Implicit Neural representations", "bib_author ": "Dupont, Emilien", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "\\cite{dupont2021coin,dupontcoin++}", "next_context": "and\\cite{strumpler2022implicit}that combine SIREN networks\\cite{sitzmann2020implicit}with positional encoding."}], "importance_score": 0.75}, "dupontcoin++": {"bib_key": "dupontcoin++", "bib_title": "COIN++: Neural Compression Across Modalities", "bib_author ": "Dupont, Emilien", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "\\cite{dupont2021coin,dupontcoin++}", "next_context": "and\\cite{strumpler2022implicit}that combine SIREN networks\\cite{sitzmann2020implicit}with positional encoding."}], "importance_score": 0.75}, "strumpler2022implicit": {"bib_key": "strumpler2022implicit", "bib_title": "Implicit neural representations for image compression", "bib_author ": "Str{\\\"u}mpler, Yannick", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "\\cite{dupont2021coin,dupontcoin++}and\\cite{strumpler2022implicit}", "next_context": "that combine SIREN networks\\cite{sitzmann2020implicit}with positional encoding."}], "importance_score": 1.25}, "xiang2022mimt": {"bib_key": "xiang2022mimt", "bib_title": "Mimt: Masked image modeling transformer for video compression", "bib_author ": "Xiang, Jinxi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{xiang2022mimt,mentzer2022vct}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "New architectures have also been proposed such as CANF-VC~\\cite{ho2022canf}based on a video generative model, MTMT\\cite{xiang2022mimt}", "next_context": "using a masked image modeling transformer-based entropy model and VCT\\cite{mentzer2022vct}based on a video compression transformer."}], "importance_score": 1.5}, "mentzer2022vct": {"bib_key": "mentzer2022vct", "bib_title": "VCT: A Video Compression Transformer", "bib_author ": "Mentzer, Fabian", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{xiang2022mimt,mentzer2022vct}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "New architectures have also been proposed such as CANF-VC~\\cite{ho2022canf}based on a video generative model, MTMT\\cite{xiang2022mimt}using a masked image modeling transformer-based entropy model and VCT\\cite{mentzer2022vct}", "next_context": "based on a video compression transformer."}], "importance_score": 1.5}, "li2024extreme": {"bib_key": "li2024extreme", "bib_title": "Extreme video compression with pre-trained diffusion models", "bib_author ": "Li, Bohan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{li2024extreme}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "More recently, research has been conducted to further improve the performance of these learning-based coding tools utilizing more advanced network architectures, including ViTs\\cite{kathariya2023joint}, and diffusion models\\cite{li2024extreme}", "next_context": "."}], "importance_score": 2.0}, "chen2021nerv": {"bib_key": "chen2021nerv", "bib_title": "{NeRV}: Neural representations for videos", "bib_author ": "Chen, Hao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "~\\cite{chen2021nerv}", "next_context": ", patch~\\cite{bai2023ps}or disentangled spatial/grid coordinates~\\cite{li2022nerv}as model input, while content-based approaches~\\cite{kwan2024hinerv,kim2024c3,leguay2024cool}have content-specific embedding as inputs."}], "importance_score": 1.1111111111111112}, "bai2023ps": {"bib_key": "bai2023ps", "bib_title": "{PS-NeRV}: Patch-wise stylized neural representations for videos", "bib_author ": "Bai, Yunpeng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "~\\cite{chen2021nerv}, patch~\\cite{bai2023ps}", "next_context": "or disentangled spatial/grid coordinates~\\cite{li2022nerv}as model input, while content-based approaches~\\cite{kwan2024hinerv,kim2024c3,leguay2024cool}have content-specific embedding as inputs."}], "importance_score": 1.1111111111111112}, "kim2024c3": {"bib_key": "kim2024c3", "bib_title": "C3: High-performance and low-complexity neural compression from a single image or video", "bib_author ": "Kim, Hyunjik", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "~\\cite{kwan2024hinerv,kim2024c3,leguay2024cool}", "next_context": "have content-specific embedding as inputs."}], "importance_score": 0.4444444444444444}, "leguay2024cool": {"bib_key": "leguay2024cool", "bib_title": "{Cool-chic video: Learned} video coding with 800 parameters", "bib_author ": "Leguay, Thomas", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "~\\cite{kwan2024hinerv,kim2024c3,leguay2024cool}", "next_context": "have content-specific embedding as inputs."}], "importance_score": 0.4444444444444444}, "kwan2024nvrc": {"bib_key": "kwan2024nvrc", "bib_title": "{NVRC}: Neural Video Representation Compression", "bib_author ": "Kwan, Ho Man", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "\\cite{kwan2024nvrc}", "next_context": "has already achieved a performance similar to that of VVC VTM (RA), but with a much lower decoding complexity compared to autoencoder-based neural codecs."}], "importance_score": 1.1111111111111112}, "gao2024pnvc": {"bib_key": "gao2024pnvc", "bib_title": "{PNVC}: Towards Practical {INR}-based Video Compression", "bib_author ": "Gao, Ge", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "To address this limitation, significant advances  have been made~\\cite{gao2024pnvc}", "next_context": "towards more practical INR-based video compression (such as the Low Delay and Random Access modes in VVC VTM\\cite{bossen2023vtmctc}) by combining pre-training and online model overfitting."}], "importance_score": 1.1111111111111112}, "ruan2024point": {"bib_key": "ruan2024point", "bib_title": "Point Cloud Compression with Implicit Neural Representations: A Unified Framework", "bib_author ": "Ruan, Hongning", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "Some of these models have also been applied to volumetric video content~\\cite{ruan2024point,kwan2024immersive}", "next_context": ", demonstrating their potential to compete with standard and other learning-based methods."}], "importance_score": 0.6111111111111112}, "kwan2024immersive": {"bib_key": "kwan2024immersive", "bib_title": "Immersive Video Compression using Implicit Neural Representations", "bib_author ": "Kwan, Ho Man", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "Some of these models have also been applied to volumetric video content~\\cite{ruan2024point,kwan2024immersive}", "next_context": ", demonstrating their potential to compete with standard and other learning-based methods."}], "importance_score": 0.6111111111111112}, "cheon2021perceptual": {"bib_key": "cheon2021perceptual", "bib_title": "Perceptual image quality assessment with transformers", "bib_author ": "Cheon, Manri", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{cheon2021perceptual}", "next_context": ", TRes\\cite{golestaneh2022no}, SaTQA\\cite{shi2024transformer}, FastVQA\\cite{wu2022fast}and RankDVQA\\cite{feng2024rankdvqa}."}], "importance_score": 1.3333333333333333}, "golestaneh2022no": {"bib_key": "golestaneh2022no", "bib_title": "No-reference image quality assessment via transformers, relative ranking, and self-consistency", "bib_author ": "Golestaneh, S Alireza", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{cheon2021perceptual}, TRes\\cite{golestaneh2022no}", "next_context": ", SaTQA\\cite{shi2024transformer}, FastVQA\\cite{wu2022fast}and RankDVQA\\cite{feng2024rankdvqa}."}], "importance_score": 1.3333333333333333}, "shi2024transformer": {"bib_key": "shi2024transformer", "bib_title": "Transformer-based no-reference image quality assessment via supervised contrastive learning", "bib_author ": "Shi, Jinsong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{cheon2021perceptual}, TRes\\cite{golestaneh2022no}, SaTQA\\cite{shi2024transformer}", "next_context": ", FastVQA\\cite{wu2022fast}and RankDVQA\\cite{feng2024rankdvqa}."}], "importance_score": 1.3333333333333333}, "wu2022fast": {"bib_key": "wu2022fast", "bib_title": "{Fast-VQA}: Efficient end-to-end video quality assessment with fragment sampling", "bib_author ": "Wu, Haoning", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{cheon2021perceptual}, TRes\\cite{golestaneh2022no}, SaTQA\\cite{shi2024transformer}, FastVQA\\cite{wu2022fast}", "next_context": "and RankDVQA\\cite{feng2024rankdvqa}."}], "importance_score": 1.2}, "feng2024rankdvqa": {"bib_key": "feng2024rankdvqa", "bib_title": "Rankdvqa: Deep vqa based on ranking-inspired hybrid training", "bib_author ": "Feng, Chen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{cheon2021perceptual}, TRes\\cite{golestaneh2022no}, SaTQA\\cite{shi2024transformer}, FastVQA\\cite{wu2022fast}and RankDVQA\\cite{feng2024rankdvqa}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{liu2017rankiqa}and UNIQUE\\cite{zhang2021uncertainty}for the image quality assessment task, and VFIPS\\cite{hou2022perceptual}and RankDVQA\\cite{feng2024rankdvqa}", "next_context": "for video quality assessment."}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Performance and main challenges", "prev_context": "\\cite{feng2024rankdvqa}", "next_context": ", which is based on a ranking-inspired training methodology."}], "importance_score": 3.2}, "wu2023exploringvideo": {"bib_key": "wu2023exploringvideo", "bib_title": "Exploring video quality assessment on user generated contents from aesthetic and technical perspectives", "bib_author ": "Wu, Haoning", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "The former has been further extended as DOVER\\cite{wu2023exploringvideo}", "next_context": "and COVER\\cite{he2024cover}when aesthetic and/or semantic aspects in the content are taken into account."}], "importance_score": 1.2}, "he2024cover": {"bib_key": "he2024cover", "bib_title": "{COVER}: A comprehensive video quality evaluator", "bib_author ": "He, Chenlong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "The former has been further extended as DOVER\\cite{wu2023exploringvideo}and COVER\\cite{he2024cover}", "next_context": "when aesthetic and/or semantic aspects in the content are taken into account."}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Similar works have also been proposed for video quality assessment, such as BVQI\\cite{wu2023exploring,wu2023towards}and COVER\\cite{he2024cover}", "next_context": "."}], "importance_score": 2.2}, "peng2024rmt": {"bib_key": "peng2024rmt", "bib_title": "{RMT-BVQA}: Recurrent memory transformer-based blind video quality assessment for enhanced video content", "bib_author ": "Peng, Tianhao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "More recently, quality-aware contrastive loss has been designed in\\cite{zhao2023quality,peng2024rmt}", "next_context": "to stabilize the learning process."}], "importance_score": 0.7}, "encyclopedia_ai_v1": {"bib_key": "encyclopedia_ai_v1", "bib_title": "Encyclopedia of Artificial Intelligence", "bib_author ": "", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": null, "prev_context": "The roots of AI art can be traced back to the 20th century, exemplified by AARON, a computer program initiated in 1972 to autonomously produce paintings and drawings\\cite{encyclopedia_ai_v1}", "next_context": "."}], "importance_score": 1.0}, "ippolito:creative:2022": {"bib_key": "ippolito:creative:2022", "bib_title": "Creative Writing with an {AI}-Powered Writing Assistant: Perspectives from Professional Writers", "bib_author ": "Ippolito, Daphne", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "These tools go beyond mere grammar and spelling checks; they boast advancements enabling them to analyze the style and tone of written material, adding images, videos and tables, offering suggestions to enhance clarity, coherence, and overall readability\\cite{ippolito:creative:2022}", "next_context": "."}], "importance_score": 1.0}, "Azzarelli:Reviewing:2024": {"bib_key": "Azzarelli:Reviewing:2024", "bib_title": "Intelligent Cinematography: a review of AI research for cinematographic production", "bib_author ": "A Azzarelli", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "AI script generators serve as beneficial aids for writers, filmmakers, and game developers, offering inspiration, idea generation, and assistance in crafting entire scripts\\cite{Jeary2024, Azzarelli:Reviewing:2024}", "next_context": "."}], "importance_score": 0.5}, "guo:exploring:2024": {"bib_key": "guo:exploring:2024", "bib_title": "Exploring the Interaction of Creative Writers with {AI}-Powered Writing Tools", "bib_author ": "Guo, Alicia", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "Human-AI brainstorming is helpful and saves time\\cite{guo:exploring:2024}", "next_context": "."}], "importance_score": 1.0}, "Mirowski:cowriting:2023": {"bib_key": "Mirowski:cowriting:2023", "bib_title": "Co-Writing Screenplays and Theatre Scripts with Language Models: Evaluation by Industry Professionals", "bib_author ": "Mirowski, Piotr", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "Dramatron, developed by Google\\cite{Mirowski:cowriting:2023}", "next_context": ", introduces hierarchical language generation, enabling the creation of cohesive scripts and screenplays spanning long ranges."}], "importance_score": 1.0}, "Beckett:Generating:2023": {"bib_key": "Beckett:Generating:2023", "bib_title": "Generating Change: {A} global survey of what news organisations are doing with {AI}", "bib_author ": "Charlie Beckett", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "\\cite{Beckett:Generating:2023}", "next_context": "."}], "importance_score": 1.0}, "Stefanini:From:2023": {"bib_key": "Stefanini:From:2023", "bib_title": "From Show to Tell: A Survey on Deep Learning-Based Image Captioning", "bib_author ": "Stefanini, Matteo", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "\\cite{Stefanini:From:2023}", "next_context": ") and with text prompts."}], "importance_score": 1.0}, "radford2021learning": {"bib_key": "radford2021learning", "bib_title": "Learning Transferable Visual Models From Natural Language Supervision", "bib_author ": "Alec Radford", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "The most well-known technique is Contrastive Language-Image Pre-training (CLIP)\\cite{radford2021learning}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Moreover, recent research works also focus on using pre-trained vision-language models, such as CLIP\\cite{radford2021learning}", "next_context": ", which align better image and text modalities."}], "importance_score": 2.0}, "Zhang:vision:2024": {"bib_key": "Zhang:vision:2024", "bib_title": "Vision-Language Models for Vision Tasks: A Survey", "bib_author ": "Zhang, Jingyi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "A comprehensive survey of VLMs for vision tasks can be found in\\cite{Zhang:vision:2024}", "next_context": "."}], "importance_score": 1.0}, "Oord:Neural:2017": {"bib_key": "Oord:Neural:2017", "bib_title": "Neural discrete representation learning", "bib_author ": "van den Oord, Aaron", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Audio and music generation", "prev_context": "\\cite{Oord:Neural:2017}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Audio Compression", "prev_context": "Similarly to images and videos, learning-based solutions have also been researched to compress audio signals, and most neural audio codecs are based on VQ-VAE\\cite{Oord:Neural:2017}", "next_context": "."}], "importance_score": 2.0}, "Wang:One:2022": {"bib_key": "Wang:One:2022", "bib_title": "One-Shot Voice Conversion For Style Transfer Based On Speaker Adaptation", "bib_author ": "Wang, Zhichao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Audio and music generation", "prev_context": "Voice style transfer often uses zero-shot learning (a model is trained to recognize classes or categories that it has never encountered during training)\\cite{Huang:GenerSpeech:2022}or few-shot learning (a model trained with only one or a few examples per class)\\cite{Wang:One:2022}", "next_context": "."}], "importance_score": 1.0}, "Huang:Diff:2025": {"bib_key": "Huang:Diff:2025", "bib_title": "Diffusion Model-Based Image Editing: A Survey", "bib_author ": "Huang, Yi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Image generation", "prev_context": "Since then, text-driven image editing has seen significant improvements in quality, with most recent approaches adopting Diffusion Transformer architectures\\cite{Feng_Ma_2025, Huang:Diff:2025}", "next_context": "."}], "importance_score": 0.5}, "XU2025103402": {"bib_key": "XU2025103402", "bib_title": "Integrating augmented reality and LLM for enhanced cognitive support in critical audio communications", "bib_author ": "Fang Xu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Augmented, virtual and mixed reality, and 3D content", "prev_context": "While the benefits of LLMs in Augmented Reality (AR) directly target educational purposes, enhance cognitive support, and facilitate communication\\cite{XU2025103402}", "next_context": ", mixed reality (MR) has once again become exciting since the release of the Apple Vision Pro in February 2024."}], "importance_score": 1.0}, "deitke:Objaverse:2023": {"bib_key": "deitke:Objaverse:2023", "bib_title": "{Objaverse-XL: A} Universe of 10M+ {3D} Objects", "bib_author ": "Deitke, Matt", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Augmented, virtual and mixed reality, and 3D content", "prev_context": "3D objects are gathered from Objaverse\\cite{deitke:Objaverse:2023}", "next_context": ", a dataset with 800K+ annotated 3D objects."}], "importance_score": 1.0}, "feizi:Online:2023": {"bib_key": "feizi:Online:2023", "bib_title": "Online Advertisements with LLMs: Opportunities and Challenges", "bib_author ": "Feizi, Soheil", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Advertisements and film analysis", "prev_context": "Not only does AI assist in generating ideas and content, but it can also aid creators in effectively matching content to their audiences, particularly on an individual level\\cite{feizi:Online:2023}", "next_context": "."}], "importance_score": 1.0}, "Ryu:Cinema:2025": {"bib_key": "Ryu:Cinema:2025", "bib_title": "Cinema Multiverse Lounge: Enhancing Film Appreciation via Multi-Agent Conversations", "bib_author ": "Ryu, Jeongwoo", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Advertisements and film analysis", "prev_context": "Cinema Multiverse Lounge\\cite{Ryu:Cinema:2025}", "next_context": ", a multi-agent conversational system, allows users to interact with LLM-driven agents, each embodying a distinct film-related target users."}], "importance_score": 1.0}, "Li:From:2025": {"bib_key": "Li:From:2025", "bib_title": "From Matching to Generation: A Survey on Generative Information Retrieval", "bib_author ": "Li, Xiaoxi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Content retrieval and recommendation services", "prev_context": "A comprehensive survey on Generative Information Retrieval is available in\\cite{Li:From:2025}", "next_context": "."}], "importance_score": 1.0}, "CHUA:AI:2023": {"bib_key": "CHUA:AI:2023", "bib_title": "AI-enabled investment advice: Will users buy it?", "bib_author ": "Alton Y.K. Chua", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Content retrieval and recommendation services", "prev_context": "\\cite{CHUA:AI:2023}", "next_context": "has reported a positive association between buyers' attitudes toward AI and their behavioral intention to accept AI-based recommendations, with potential for further growth."}], "importance_score": 1.0}, "aggarwal2025evolution": {"bib_key": "aggarwal2025evolution", "bib_title": "Evolution of Recommendation Systems in the Age of Generative AI", "bib_author ": "Ankur Aggarwal", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Content retrieval and recommendation services", "prev_context": "~\\cite{aggarwal2025evolution}", "next_context": "states that the recommendation accuracy of recommendation services has increased from 45.0\\%to 91.5\\%with the integration of generative AI."}], "importance_score": 1.0}, "brynjolfsson:generative:2023": {"bib_key": "brynjolfsson:generative:2023", "bib_title": "Generative {AI} at Work", "bib_author ": "Brynjolfsson, Erik", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Intelligent assistants", "prev_context": "\\cite{brynjolfsson:generative:2023}", "next_context": "examined the implementation of a generative AI tool designed to offer conversational guidance to customer support agents."}], "importance_score": 1.0}, "Lee:design:2024": {"bib_key": "Lee:design:2024", "bib_title": "A Design Space for Intelligent and Interactive Writing Assistants", "bib_author ": "Lee, Mina", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Intelligent assistants", "prev_context": "AI-based intelligent assistants may currently be more focused on educational purposes, but they can clearly help artists write more efficiently\\cite{Lee:design:2024}", "next_context": "or assist in customizing personal requirements\\cite{sajja2024ai}."}], "importance_score": 1.0}, "sajja2024ai": {"bib_key": "sajja2024ai", "bib_title": "Artificial Intelligence-Enabled Intelligent Assistant for Personalized and Adaptive Learning in Higher Education", "bib_author ": "Sajja, R.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Intelligent assistants", "prev_context": "AI-based intelligent assistants may currently be more focused on educational purposes, but they can clearly help artists write more efficiently\\cite{Lee:design:2024}or assist in customizing personal requirements\\cite{sajja2024ai}", "next_context": "."}], "importance_score": 1.0}, "jiang2025domain": {"bib_key": "jiang2025domain", "bib_title": "Domain-Tailored Generative AI for Personalized Assistant", "bib_author ": "Ning Jiang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Intelligent assistants", "prev_context": "The performance of personalized assistants can be enhanced with domain-specific knowledge to provide more in-depth responses to users\\cite{jiang2025domain}", "next_context": "."}], "importance_score": 1.0}, "Zhou:LEDNet:2022": {"bib_key": "Zhou:LEDNet:2022", "bib_title": "LEDNet: Joint Low-Light Enhancement and Deblurring in the Dark", "bib_author ": "Zhou, Shangchen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "To address this, LEDNet\\cite{Zhou:LEDNet:2022}", "next_context": "has introduced a synthetic dataset for such scenarios and incorporated a learnable non-linear activation function within the network to enhance feature intensities."}], "importance_score": 1.0}, "huang2025bayesian": {"bib_key": "huang2025bayesian", "bib_title": "Bayesian Neural Networks for One-to-Many Mapping in Image Enhancement", "bib_author ": "Guoxi Huang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "For example, a Bayesian Enhancement Model (BEM)\\cite{huang2025bayesian}", "next_context": "incorporates Bayesian Neural Networks (BNNs) to capture data uncertainty and produce diverse outputs."}], "importance_score": 1.0}, "anantrasirichai:BVI:2024": {"bib_key": "anantrasirichai:BVI:2024", "bib_title": "{BVI-Lowlight: Fully} Registered Benchmark Dataset for Low-Light Video Enhancement", "bib_author ": "Anantrasirichai, Nantheera", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "STA-SUNet\\cite{Lin:SPATIO:2024}has demonstrated that using transformers for low-light video enhancement outperforms CNN-based methods\\cite{anantrasirichai:BVI:2024}", "next_context": "."}], "importance_score": 1.0}, "huang2025bvi": {"bib_key": "huang2025bvi", "bib_title": "BVI-Mamba: Video Enhancement Using a Visual State-Space Model for Low-Light and Underwater Environments", "bib_author ": "Guoxi Huang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "The recent Mamba-based network\\cite{huang2025bvi}", "next_context": "also demonstrates promising results, outperforming STA-SUNet by more than 2 dB in PSNR."}], "importance_score": 1.0}, "Lin:BVI-RLV:2024": {"bib_key": "Lin:BVI-RLV:2024", "bib_title": "{BVI-RLV: A} Fully Registered Dataset and Benchmarks for Low-Light Video Enhancement", "bib_author ": "R Lin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "While most training datasets use normal lighting conditions as ground truth\\cite{Lin:BVI-RLV:2024}", "next_context": ", the enhanced images and videos may alter the mood and tone of the content."}], "importance_score": 1.0}, "safonov2025ntire": {"bib_key": "safonov2025ntire", "bib_title": "{NTIRE} 2025 Challenge on {UGC} Video Enhancement: Methods and Results", "bib_author ": "Nikolay Safonov", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "The winning solution of the NTIRE 2025 Challenge on UGC Video Enhancement\\cite{safonov2025ntire}", "next_context": "implemented a pipeline of four sequential modules: color enhancement, denoising, BasicVSR++ restoration\\cite{Chan:BasicVSR:2022}, and SwinIR\\cite{Liang:SwinIR:2021}."}], "importance_score": 1.0}, "Chan:BasicVSR:2022": {"bib_key": "Chan:BasicVSR:2022", "bib_title": "BasicVSR++: Improving Video Super-Resolution With Enhanced Propagation and Alignment", "bib_author ": "Chan, Kelvin C.K.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "The winning solution of the NTIRE 2025 Challenge on UGC Video Enhancement\\cite{safonov2025ntire}implemented a pipeline of four sequential modules: color enhancement, denoising, BasicVSR++ restoration\\cite{Chan:BasicVSR:2022}", "next_context": ", and SwinIR\\cite{Liang:SwinIR:2021}."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "Although the results are slightly inferior to those of BasicVSR++\\cite{Chan:BasicVSR:2022}", "next_context": ", which employs CNN and was introduced around the same time, both methods significantly enhance detail and sharpness compared to previous approaches, albeit not in real time."}], "importance_score": 2.0}, "ZHOU:Bridging:2025": {"bib_key": "ZHOU:Bridging:2025", "bib_title": "Bridging the metrics gap in image style transfer: A comprehensive survey of models and criteria", "bib_author ": "Xiaotong Zhou", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Style transfer", "prev_context": "A survey of style transfer using transformers and diffusion models can be found in\\cite{ZHOU:Bridging:2025}", "next_context": "."}], "importance_score": 1.0}, "Conde:Efficient:2023": {"bib_key": "Conde:Efficient:2023", "bib_title": "Efficient Deep Models for Real-Time 4K Image Super-Resolution. NTIRE 2023 Benchmark and Report", "bib_author ": "Conde, Marcos V.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "However, the NTIRE 2023 Real-Time Super-Resolution Challenge\\cite{Conde:Efficient:2023}", "next_context": "showed that the winner, Bicubic++\\cite{Bilecen:Bicubic:2023}, uses only convolutional layers and achieves the fastest speed at 1.17ms in upscaling 720p to 4K images."}], "importance_score": 1.0}, "Bilecen:Bicubic:2023": {"bib_key": "Bilecen:Bicubic:2023", "bib_title": "{Bicubic++: Slim,} Slimmer, Slimmest Designing an Industry-Grade Super-Resolution Network", "bib_author ": "B. Bilecen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "However, the NTIRE 2023 Real-Time Super-Resolution Challenge\\cite{Conde:Efficient:2023}showed that the winner, Bicubic++\\cite{Bilecen:Bicubic:2023}", "next_context": ", uses only convolutional layers and achieves the fastest speed at 1.17ms in upscaling 720p to 4K images."}], "importance_score": 1.0}, "chen2025ntire": {"bib_key": "chen2025ntire", "bib_title": "NTIRE 2025 Challenge on Image Super-Resolution (\u00d74): Methods and Results", "bib_author ": "Zheng Chen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "This method is significantly faster than any of the participants in the NTIRE 2025 Challenge\\cite{chen2025ntire}", "next_context": ", where Transformer-based architectures continue to dominate as the mainstream approach."}], "importance_score": 1.0}, "moser:diffusion:2024": {"bib_key": "moser:diffusion:2024", "bib_title": "Diffusion Models, Image Super-Resolution, and Everything: A Survey", "bib_author ": "Moser, Brian B.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "A survey in SISR using DMs can be found in\\cite{moser:diffusion:2024}", "next_context": "."}], "importance_score": 1.0}, "Fuoli:Fast:2023": {"bib_key": "Fuoli:Fast:2023", "bib_title": "Fast Online Video Super-Resolution With Deformable Attention Pyramid", "bib_author ": "Fuoli, Dario", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "To address this limitation, the Deformable Attention Pyramid\\cite{Fuoli:Fast:2023}", "next_context": "has been introduced, offering slightly lower quality but a speed-up of over 3\\times."}], "importance_score": 1.0}, "10902142": {"bib_key": "10902142", "bib_title": "Diffusion Models in Low-Level Vision: A Survey", "bib_author ": "He, Chunming", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "For a comprehensive survey on the use of diffusion models in restoration tasks, refer to\\cite{10902142}", "next_context": "."}], "importance_score": 1.0}, "Pan:Deep:2023": {"bib_key": "Pan:Deep:2023", "bib_title": "Deep Discriminative Spatial and Temporal Network for Efficient Video Deblurring", "bib_author ": "Pan, Jinshan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "i)\\textbf{Deblurring}: A lightweight deep CNN model was recently proposed in\\cite{Pan:Deep:2023}", "next_context": ", where a new discriminative temporal feature fusion has been introduced to select the most useful spatial and temporal features from adjacent frames."}], "importance_score": 1.0}, "Choi:Exploring:2023": {"bib_key": "Choi:Exploring:2023", "bib_title": "Exploring Positional Characteristics of Dual-Pixel Data for Camera Autofocus", "bib_author ": "Choi, Myungsub", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "\\cite{Choi:Exploring:2023}", "next_context": "."}], "importance_score": 1.0}, "Yang:K3DN:2023": {"bib_key": "Yang:K3DN:2023", "bib_title": "{K3DN: Disparity}-Aware Kernel Estimation for Dual-Pixel Defocus Deblurring", "bib_author ": "Yang, Yan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "Remarkably, their model achieves comparable results to previous state-of-the-art methods while being more lightweight\\cite{Yang:K3DN:2023}", "next_context": "."}], "importance_score": 1.0}, "sun2025tenth": {"bib_key": "sun2025tenth", "bib_title": "The Tenth NTIRE 2025 Image Denoising Challenge Report", "bib_author ": "Lei Sun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "The NTIRE 2025 Image Denoising Challenge\\cite{sun2025tenth}", "next_context": "revealed that the top-performing methods combined transformer-based and convolutional network architectures."}], "importance_score": 1.0}, "Atmospheric:2023": {"bib_key": "Atmospheric:2023", "bib_title": "Atmospheric turbulence removal with complex-valued convolutional neural network", "bib_author ": "Nantheera Anantrasirichai", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "iv)\\textbf{Mitigating atmospheric turbulence}: Similar to dehazing, physics-inspired models have been widely developed to remove turbulence distortion\\cite{Jaiswal:Physics:2023,Jiang:NeRT:2023}, while complex-valued CNNs have been proposed to exploit phase information\\cite{Atmospheric:2023}", "next_context": "."}], "importance_score": 1.0}, "hill2025mamat": {"bib_key": "hill2025mamat", "bib_title": "{MAMAT: 3D} Mamba-Based Atmospheric Turbulence Removal and its Object Detection Capability", "bib_author ": "Paul Hill", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "More recently, Mamba-based architectures have demonstrated their effectiveness in both visual quality and model efficiency\\cite{hill2025mamat}", "next_context": "."}], "importance_score": 1.0}, "Hill2025": {"bib_key": "Hill2025", "bib_title": "Deep Learning Techniques for Atmospheric Turbulence Removal: A Review", "bib_author ": "Paul Hill", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "A recent review can be found in\\cite{Hill2025}", "next_context": "."}], "importance_score": 1.0}, "zheng:pluralistic:2019": {"bib_key": "zheng:pluralistic:2019", "bib_title": "Pluralistic Image Completion", "bib_author ": "Zheng, Chuanxia", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Inpainting", "prev_context": "This means users can now mask large areas of an image, and AI tools generate multiple results for users to choose from, a technique known as pluralistic inpainting\\cite{zheng:pluralistic:2019}", "next_context": "."}], "importance_score": 1.0}, "Zhang:DINet:2023": {"bib_key": "Zhang:DINet:2023", "bib_title": "{DINet: Deformation} inpainting network for realistic face visually dubbing on high resolution video", "bib_author ": "Zhang, Zhimeng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Inpainting", "prev_context": "DINet\\cite{Zhang:DINet:2023}", "next_context": "replaces the mouth area to synchronize with a new language being spoken."}], "importance_score": 1.0}, "quan:deep:2024": {"bib_key": "quan:deep:2024", "bib_title": "Deep Learning-Based Image and Video Inpainting: A Survey", "bib_author ": "Quan, W.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Inpainting", "prev_context": "A comprehensive survey of learning-based image and video inpainting, covering approaches such as CNNs, VAEs, GANs, transformers, and diffusion models, can be found in\\cite{quan:deep:2024}", "next_context": "."}], "importance_score": 1.0}, "elharrouss2025transformer": {"bib_key": "elharrouss2025transformer", "bib_title": "Transformer-based Image and Video Inpainting: Current Challenges and Future Directions", "bib_author ": "Omar Elharrouss", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Inpainting", "prev_context": "\\cite{elharrouss2025transformer}", "next_context": "provide an in-depth review of the current challenges and future directions specific to transformer-based inpainting techniques."}], "importance_score": 1.0}, "Karim:Current:2023": {"bib_key": "Karim:Current:2023", "bib_title": "Current advances and future perspectives of image fusion: A comprehensive review", "bib_author ": "Shahid Karim", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Image Fusion", "prev_context": "For an in-depth review, the reader is referred to recent work in\\cite{Karim:Current:2023, Zhang:Visible:2023}", "next_context": "."}], "importance_score": 0.5}, "Tous:Lester:2024": {"bib_key": "Tous:Lester:2024", "bib_title": "{Lester: Rotoscope} animation through video object segmentation and tracking", "bib_author ": "Ruben Tous", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Editing and Visual Special Effects (VFX)", "prev_context": "Generative AI has also simplified and accelerated automated processes, such as rotoscoping\\cite{Tous:Lester:2024}", "next_context": ", an animation technique where animators trace over motion picture footage frame by frame to create realistic action."}], "importance_score": 1.0}, "Baranchuk:label:2022": {"bib_key": "Baranchuk:label:2022", "bib_title": "Label-Efficient Semantic Segmentation with Diffusion Models", "bib_author ": "Dmitry Baranchuk", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "", "next_context": "have investigated semantic representation, and found DMs outperform other few-shot learning approaches."}], "importance_score": 1.0}, "Lin:Feature:2024": {"bib_key": "Lin:Feature:2024", "bib_title": "Feature Denoising for Low-Light Instance Segmentation Using Weighted Non-Local Blocks", "bib_author ": "Lin, Joanne", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "Recently, this work has been integrated with a non-local means block in\\cite{Lin:Feature:2024}", "next_context": "showning a significant improvement for instant segmentation in low-light scenes."}], "importance_score": 1.0}, "Goel:Interactive:2023": {"bib_key": "Goel:Interactive:2023", "bib_title": "Interactive Segmentation of Radiance Fields", "bib_author ": "Goel, Rahul", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "In radiance fields, earlier segmentation methods required additional modules such as using k-means clustering to separate objects from the background\\cite{Goel:Interactive:2023}", "next_context": "."}], "importance_score": 1.0}, "HE2025102722": {"bib_key": "HE2025102722", "bib_title": "Deep learning based 3D segmentation in computer vision: A survey", "bib_author ": "Yong He", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "A comprehensive survey of 3D segmentation in computer vision can be found in\\cite{HE2025102722}", "next_context": "."}], "importance_score": 1.0}, "Ren:Faster:2027": {"bib_key": "Ren:Faster:2027", "bib_title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "bib_author ": "Ren, Shaoqing", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "The approach achieves comparable results to an optimized Faster R-CNN\\cite{Ren:Faster:2027}", "next_context": ", introduced in 2015."}], "importance_score": 1.0}, "Zou:object:2023": {"bib_key": "Zou:object:2023", "bib_title": "Object Detection in 20 Years: A Survey", "bib_author ": "Zou, Zhengxia", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "A survey until 2022\\cite{Zou:object:2023}", "next_context": "reported that Deformable DETR and Swin Transformers\\cite{Liu:Swin:2021}outperform pure CNN-based YOLOv4\\cite{bochkovskiy2020yolov4}."}], "importance_score": 1.0}, "bochkovskiy2020yolov4": {"bib_key": "bochkovskiy2020yolov4", "bib_title": "{YOLOv4: Optimal} speed and accuracy of object detection", "bib_author ": "Bochkovskiy, A.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "A survey until 2022\\cite{Zou:object:2023}reported that Deformable DETR and Swin Transformers\\cite{Liu:Swin:2021}outperform pure CNN-based YOLOv4\\cite{bochkovskiy2020yolov4}", "next_context": "."}], "importance_score": 1.0}, "lv2:detrs:2024": {"bib_key": "lv2:detrs:2024", "bib_title": "DETRs Beat YOLOs on Real-time Object Detection", "bib_author ": "Yian Zhao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "Later, RT-DETR\\cite{lv2:detrs:2024}", "next_context": "improved inference speed by decoupling the intra-scale interaction and cross-scale fusion of features with different scales."}], "importance_score": 1.0}, "wang:yolov10:2024": {"bib_key": "wang:yolov10:2024", "bib_title": "{YOLOv10: Real}-Time End-to-End Object Detection", "bib_author ": "Wang, Ao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "Recently, YOLOv10\\cite{wang:yolov10:2024}", "next_context": "has been released."}], "importance_score": 1.0}, "Li:Transformer:2023": {"bib_key": "Li:Transformer:2023", "bib_title": "Transformer for object detection: Review and benchmark", "bib_author ": "Yong Li", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "A review of transformer-based methods for object detection can be found in\\cite{Li:Transformer:2023, KHEDDAR2025103347}", "next_context": "."}], "importance_score": 0.5}, "KHEDDAR2025103347": {"bib_key": "KHEDDAR2025103347", "bib_title": "Transformers and large language models for efficient intrusion detection systems: A comprehensive survey", "bib_author ": "Hamza Kheddar", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "A review of transformer-based methods for object detection can be found in\\cite{Li:Transformer:2023, KHEDDAR2025103347}", "next_context": "."}], "importance_score": 0.5}, "10637966": {"bib_key": "10637966", "bib_title": "Robustness-Aware 3D Object Detection in Autonomous Driving: A Review and Outlook", "bib_author ": "Song, Ziying", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "More 3D object detection methods have been developed for autonomous driving\\cite{10637966}", "next_context": "; however, these approaches can also be adapted for AR and VR applications\\cite{im2025gate3d}."}], "importance_score": 1.0}, "Wu:datasetDM:2023": {"bib_key": "Wu:datasetDM:2023", "bib_title": "{DatasetDM: Synthesizing} Data with Perception Annotations Using Diffusion Models", "bib_author ": "Wu, Weijia", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "While DMs are primarily used to generate synthetic datasets\\cite{Wu:datasetDM:2023, Fang:Data:2024}", "next_context": ", they have also been demonstrated to function as zero-shot classifiers by Li et al."}], "importance_score": 0.5}, "Fang:Data:2024": {"bib_key": "Fang:Data:2024", "bib_title": "Data Augmentation for Object Detection via Controllable Diffusion Models", "bib_author ": "Fang, Haoyang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "While DMs are primarily used to generate synthetic datasets\\cite{Wu:datasetDM:2023, Fang:Data:2024}", "next_context": ", they have also been demonstrated to function as zero-shot classifiers by Li et al."}], "importance_score": 0.5}, "Kugarajeevan:Transformers:2023": {"bib_key": "Kugarajeevan:Transformers:2023", "bib_title": "Transformers in Single Object Tracking: An Experimental Survey", "bib_author ": "Kugarajeevan, Janani", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "An experimental survey cited in\\cite{Kugarajeevan:Transformers:2023}", "next_context": "reveals that transformer-based methods consistently rank at the top of the leaderboard across various datasets."}], "importance_score": 1.0}, "cheng:xmem:2022": {"bib_key": "cheng:xmem:2022", "bib_title": "Xmem: Long-term video object segmentation with an Atkinson-Shiffrin memory model", "bib_author ": "Cheng, Ho Kei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "TAM combines SAM\\cite{Kirillov:SAM:2023}and XMem\\cite{cheng:xmem:2022}", "next_context": ", offering tracking and segmentation performance on the human-selected target."}], "importance_score": 1.0}, "ge:yolox:2021": {"bib_key": "ge:yolox:2021", "bib_title": "YOLOX: Exceeding YOLO Series in 2021", "bib_author ": "Ge, Zheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "MOTRv2\\cite{Zhang:MOTRv2:2023}combines YOLOX\\cite{ge:yolox:2021}", "next_context": "for object recognition and MOTR\\cite{zeng:motr:2022}for tracking, outperforming TrackFormer by 20\\%."}], "importance_score": 1.0}, "Azzarelli2024": {"bib_key": "Azzarelli2024", "bib_title": "Exploring Dynamic Novel View Synthesis Technologies for Cinematography", "bib_author ": "Adrian Azzarelli", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "They have hence gained significant attention in cinematography\\cite{Azzarelli2024}", "next_context": ", as they offer reduced time and cost, particularly for outdoor shooting."}], "importance_score": 1.0}, "schoenberger:sfm:2016": {"bib_key": "schoenberger:sfm:2016", "bib_title": "Structure-from-Motion Revisited", "bib_author ": "Sch\\\"{o}nberger, Johannes Lutz", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "\\ref{fig:3Drepresentation}(a)), the camera positions and orientations are typically estimated from a series of 2D images using techniques like feature-mapping and Structure-from-Motion (SfM), as demonstrated in\\cite{schoenberger:sfm:2016}", "next_context": "."}], "importance_score": 1.0}, "10521791": {"bib_key": "10521791", "bib_title": "3D Gaussian Splatting as New Era: A Survey", "bib_author ": "Fei, Ben", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "3D Gaussian Splatting", "prev_context": "A survey of 3D-GS can be found in\\cite{10521791}", "next_context": "."}], "importance_score": 1.0}, "jiang:vrgs:2024": {"bib_key": "jiang:vrgs:2024", "bib_title": "{VR-GS: A} Physical Dynamics-Aware Interactive Gaussian Splatting System in Virtual Reality", "bib_author ": "Jiang, Ying", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "3D Gaussian Splatting", "prev_context": "VR-GS\\cite{jiang:vrgs:2024}", "next_context": "offers intuitive and interactive physics-based game-play with deformable virtual objects and realistic environments represented with 3D-GS."}], "importance_score": 1.0}, "Bull:intelligent:2021": {"bib_key": "Bull:intelligent:2021", "bib_title": "Intelligent image and video compression: communicating pictures", "bib_author ": "Bull, David", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": null, "prev_context": "Data compression plays an important role in the delivery of creative content to audiences, effectively reducing memory and bandwidth requirements during signal storage and transmission\\cite{Bull:intelligent:2021}", "next_context": "."}], "importance_score": 1.0}, "balle2016density": {"bib_key": "balle2016density", "bib_title": "Density modeling of images using a generalized normalization transformation", "bib_author ": "Ball{\\'e}, Johannes", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "Since the first neural image codec\\cite{balle2016density}", "next_context": "was proposed in 2016, numerous learning-based image compression methods have been developed, with significant performance improvements reported\\cite{balle2018variational,cheng2020learned}."}], "importance_score": 1.0}, "balle2018variational": {"bib_key": "balle2018variational", "bib_title": "Variational image compression with a scale hyperprior", "bib_author ": "Ball{\\'e}, Johannes", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "Since the first neural image codec\\cite{balle2016density}was proposed in 2016, numerous learning-based image compression methods have been developed, with significant performance improvements reported\\cite{balle2018variational,cheng2020learned}", "next_context": "."}], "importance_score": 0.5}, "cheng2020learned": {"bib_key": "cheng2020learned", "bib_title": "Learned image compression with discretized gaussian mixture likelihoods and attention modules", "bib_author ": "Cheng, Zhengxue", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "Since the first neural image codec\\cite{balle2016density}was proposed in 2016, numerous learning-based image compression methods have been developed, with significant performance improvements reported\\cite{balle2018variational,cheng2020learned}", "next_context": "."}], "importance_score": 0.5}, "agustsson2019generative": {"bib_key": "agustsson2019generative", "bib_title": "Generative adversarial networks for extreme learned image compression", "bib_author ": "Agustsson, Eirikur", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "Early works\\cite{agustsson2019generative,mentzer2020high}", "next_context": "employed GANs to generate more photo realistic results with improved visual quality."}], "importance_score": 0.5}, "mentzer2020high": {"bib_key": "mentzer2020high", "bib_title": "High-fidelity generative image compression", "bib_author ": "Mentzer, Fabian", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "Early works\\cite{agustsson2019generative,mentzer2020high}", "next_context": "employed GANs to generate more photo realistic results with improved visual quality."}], "importance_score": 0.5}, "Bovik_MSSSIM": {"bib_key": "Bovik_MSSSIM", "bib_title": "Multi-scale structural similarity for image quality assessment", "bib_author ": "Wang, Z.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "Although these models fail to outperform conventional, CNN-based or transformer-based approaches, when distortion-based quality metrics, e.g., PSNR, are used for performance evaluation, they have been reported to perform well when perceptual quality models, such as MS-SSIM\\cite{Bovik_MSSSIM}", "next_context": "and VMAF\\cite{VMAFblog}, or subjective tests are employed to measure perceived video quality."}], "importance_score": 1.0}, "VMAFblog": {"bib_key": "VMAFblog", "bib_title": "{The NETFLIX tech blog: Toward a practical perceptual video quality metric}", "bib_author ": "Z. Li", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "Although these models fail to outperform conventional, CNN-based or transformer-based approaches, when distortion-based quality metrics, e.g., PSNR, are used for performance evaluation, they have been reported to perform well when perceptual quality models, such as MS-SSIM\\cite{Bovik_MSSSIM}and VMAF\\cite{VMAFblog}", "next_context": ", or subjective tests are employed to measure perceived video quality."}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "These hand-crafted quality models have also been combined with features within a regression-based framework in order to achieve more accurate prediction performance - VMAF is one such example\\cite{VMAFblog}", "next_context": "."}], "importance_score": 2.0}, "clic": {"bib_key": "clic", "bib_title": "{ Challenge on Learned Image Compression}", "bib_author ": "", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "One of the most well-known of these is the Challenge on Learned Image Compression (CLIC)\\cite{clic}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Performance and main challenges", "prev_context": "The Sixth Challenge on Learned Image Compression (CLIC)\\cite{clic}", "next_context": "associated with the Data Compression Conference 2024 is one of the latest examples which includes two quality assessment tracks for image and video compression."}], "importance_score": 2.0}, "li2024semantic": {"bib_key": "li2024semantic", "bib_title": "Semantic Ensemble Loss and Latent Refinement for High-Fidelity Neural Image Compression", "bib_author ": "Li, Daxin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "In its latest competition, the best performing learned image codec\\cite{li2024semantic}", "next_context": ", which is based on a GAN-enhanced Vector Quantized Variational AutoEncoder (VQ-VAE) framework, offered up to 0.6dB PSNR gain over VTM (version 22.2, All Intra) at similar bitrates; this codec is based on an autoencoder architecture with latent refinement and perceptual losses."}], "importance_score": 1.0}, "ascenso2023jpeg": {"bib_key": "ascenso2023jpeg", "bib_title": "The {JPEG AI standard}: Providing efficient human and machine visual data consumption", "bib_author ": "Ascenso, Joao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "To support the deployment of neural image codecs, the International Organization for Standardization (ISO)/International Electrotechnical Commission(IEC) has developed a royalty-free learned image coding standard, denoted as JPEG AI\\cite{ascenso2023jpeg}", "next_context": ", which aims to offer significant performance improvement over existing standards for both human and machine vision tasks."}], "importance_score": 1.0}, "JPEGAIN100634": {"bib_key": "JPEGAIN100634", "bib_title": "{JPEG AI: Future Plans and Timeline v2}", "bib_author ": "Elena Alshina", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "The Call for Proposals of JPEG AI was published in 2022, while the Working Draft and the Committee Draft outlining its core coding system were released in 2023\\cite{JPEGAIN100634}", "next_context": ", with its first version published in October 2024\\cite{JPEGAIN100634}."}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "The Call for Proposals of JPEG AI was published in 2022, while the Working Draft and the Committee Draft outlining its core coding system were released in 2023\\cite{JPEGAIN100634}, with its first version published in October 2024\\cite{JPEGAIN100634}", "next_context": "."}], "importance_score": 2.0}, "JPEGAIM101081": {"bib_key": "JPEGAIM101081", "bib_title": "{JPEG AI sw v4.x status}", "bib_author ": "Alex", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "JPEG AI follows the same framework (the auto-encoder structure) as most existing neural image codecs, and its test model JPEG AI VM (version 4.3) has been reported to achieve up to 28.5\\%coding gains over VVC VTM (All Intra mode)\\cite{JPEGAIM101081}", "next_context": "."}], "importance_score": 1.0}, "li2021deepqtmt": {"bib_key": "li2021deepqtmt", "bib_title": "{DeepQTMT}: A deep learning approach for fast {QTMT-based CU} partition of intra-mode {VVC}", "bib_author ": "Li, Tianyi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "\\cite{li2021deepqtmt}", "next_context": ", inter prediction\\cite{jin2021deep}, in-loop filtering\\cite{feng2024low}, post filtering\\cite{zhang2023wcdann}and resolution re-sampling\\cite{wang2023compression}."}], "importance_score": 1.0}, "jin2021deep": {"bib_key": "jin2021deep", "bib_title": "Deep affine motion compensation network for inter prediction in {VVC}", "bib_author ": "Jin, Dengchao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "\\cite{li2021deepqtmt}, inter prediction\\cite{jin2021deep}", "next_context": ", in-loop filtering\\cite{feng2024low}, post filtering\\cite{zhang2023wcdann}and resolution re-sampling\\cite{wang2023compression}."}], "importance_score": 1.0}, "feng2024low": {"bib_key": "feng2024low", "bib_title": "Low Complexity In-Loop Filter for {VVC} Based on Convolution and Transformer", "bib_author ": "Feng, Zhen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "\\cite{li2021deepqtmt}, inter prediction\\cite{jin2021deep}, in-loop filtering\\cite{feng2024low}", "next_context": ", post filtering\\cite{zhang2023wcdann}and resolution re-sampling\\cite{wang2023compression}."}], "importance_score": 1.0}, "zhang2023wcdann": {"bib_key": "zhang2023wcdann", "bib_title": "{WCDANN}: A Lightweight {CNN} Post-Processing Filter for {VVC-based} Video Compression", "bib_author ": "Zhang, Hao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "\\cite{li2021deepqtmt}, inter prediction\\cite{jin2021deep}, in-loop filtering\\cite{feng2024low}, post filtering\\cite{zhang2023wcdann}", "next_context": "and resolution re-sampling\\cite{wang2023compression}."}], "importance_score": 1.0}, "wang2023compression": {"bib_key": "wang2023compression", "bib_title": "Compression-aware video super-resolution", "bib_author ": "Wang, Yingwei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "\\cite{li2021deepqtmt}, inter prediction\\cite{jin2021deep}, in-loop filtering\\cite{feng2024low}, post filtering\\cite{zhang2023wcdann}and resolution re-sampling\\cite{wang2023compression}", "next_context": "."}], "importance_score": 1.0}, "li2023designs": {"bib_key": "li2023designs", "bib_title": "Designs and Implementations in Neural Network-based Video Coding", "bib_author ": "Li, Yue", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "To facilitate efficient integration, the MPEG Joint Video Experts Team (JVET)  built a test model in 2022 based on VTM 11, named Neural Network-based Video Coding (NNVC)\\cite{li2023designs}", "next_context": ", with its latest version NNVC-7.1 containing two major learning-based coding tools, neural-network based intra prediction and in-loop filtering, which has achieved an up to 13\\%coding gain over VTM 11 (Random Access mode)\\cite{JVET-AG0014}."}], "importance_score": 1.0}, "JVET-AG0014": {"bib_key": "JVET-AG0014", "bib_title": "{NNVC} software development {AhG14} ", "bib_author ": "F. Galpin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "To facilitate efficient integration, the MPEG Joint Video Experts Team (JVET)  built a test model in 2022 based on VTM 11, named Neural Network-based Video Coding (NNVC)\\cite{li2023designs}, with its latest version NNVC-7.1 containing two major learning-based coding tools, neural-network based intra prediction and in-loop filtering, which has achieved an up to 13\\%coding gain over VTM 11 (Random Access mode)\\cite{JVET-AG0014}", "next_context": "."}], "importance_score": 1.0}, "joshi2023switchable": {"bib_key": "joshi2023switchable", "bib_title": "Switchable CNNs for in-loop restoration and super-resolution for AV2", "bib_author ": "Joshi, Urvang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "The latest proposals focus on the trade-off between performance and complexity, with one of them based on inloop filtering and super-resolution, which achieves an average BD-rate saving of 3.9\\%(in PSNR) over AVM, the test model of AV2, but only requires a much lower computational complexity (below 1.5kMACs/pixel)\\cite{joshi2023switchable}", "next_context": "."}], "importance_score": 1.0}, "kathariya2023joint": {"bib_key": "kathariya2023joint", "bib_title": "Joint Pixel and Frequency Feature Learning and Fusion via Channel-wise Transformer for High-Efficiency Learned In-Loop Filter in VVC", "bib_author ": "Kathariya, Birendra", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "More recently, research has been conducted to further improve the performance of these learning-based coding tools utilizing more advanced network architectures, including ViTs\\cite{kathariya2023joint}", "next_context": ", and diffusion models\\cite{li2024extreme}."}], "importance_score": 1.0}, "chadha2021deep": {"bib_key": "chadha2021deep", "bib_title": "Deep perceptual preprocessing for video coding", "bib_author ": "Chadha, Aaron", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "There are also investigations on applying preprocessing before compression\\cite{chadha2021deep,tan2024joint}", "next_context": ", where the training of the deep preprocessors is based on proxy video codecs and/or rate-distortion loss functions to simulate the behavior of conventional video coding algorithms."}], "importance_score": 0.5}, "tan2024joint": {"bib_key": "tan2024joint", "bib_title": "Joint Frame-Level and Block-Level Rate-Perception Optimized Preprocessing for Video Coding", "bib_author ": "Tan, Huajie", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "There are also investigations on applying preprocessing before compression\\cite{chadha2021deep,tan2024joint}", "next_context": ", where the training of the deep preprocessors is based on proxy video codecs and/or rate-distortion loss functions to simulate the behavior of conventional video coding algorithms."}], "importance_score": 0.5}, "lu2019dvc": {"bib_key": "lu2019dvc", "bib_title": "{DVC}: An end-to-end deep video compression framework", "bib_author ": "Lu, Guo", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "The performance of these neural video codecs has advanced significantly in the last five years, since the first attempt, DVC\\cite{lu2019dvc}", "next_context": ", was published."}], "importance_score": 1.0}, "li2024neural": {"bib_key": "li2024neural", "bib_title": "Neural video compression with feature modulation", "bib_author ": "Li, Jiahao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "However currently, learned video coding algorithms (e.g., DCVC-FM\\cite{li2024neural}", "next_context": "and DCVC-LCG\\cite{Qi2024longterm}) are able to compete or even outperform the state-of-the-art standard codecs, such as VVC VTM under certain coding configurations."}], "importance_score": 1.0}, "Qi2024longterm": {"bib_key": "Qi2024longterm", "bib_title": "Long-term Temporal Context Gathering for Neural Video Compression", "bib_author ": "Linfeng Qi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "However currently, learned video coding algorithms (e.g., DCVC-FM\\cite{li2024neural}and DCVC-LCG\\cite{Qi2024longterm}", "next_context": ") are able to compete or even outperform the state-of-the-art standard codecs, such as VVC VTM under certain coding configurations."}], "importance_score": 1.0}, "hu2021fvc": {"bib_key": "hu2021fvc", "bib_title": "{FVC}: A new framework towards deep video compression in feature space", "bib_author ": "Hu, Zhihao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "These learning-based methods often focus on enhancement from different perspectives, including feature space conditional coding (e.g., FVC~\\cite{hu2021fvc}", "next_context": "and DCVC\\cite{li2021deep}), instance adaptation~\\cite{khani2021efficient,yang2024parameter}, and motion estimation (e.g., DCVC-DC~\\cite{li2023neural})."}], "importance_score": 1.0}, "li2021deep": {"bib_key": "li2021deep", "bib_title": "{Deep contextual video compression}", "bib_author ": "Li, Jiahao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "These learning-based methods often focus on enhancement from different perspectives, including feature space conditional coding (e.g., FVC~\\cite{hu2021fvc}and DCVC\\cite{li2021deep}", "next_context": "), instance adaptation~\\cite{khani2021efficient,yang2024parameter}, and motion estimation (e.g., DCVC-DC~\\cite{li2023neural})."}], "importance_score": 1.0}, "khani2021efficient": {"bib_key": "khani2021efficient", "bib_title": "Efficient video compression via content-adaptive super-resolution", "bib_author ": "Khani, Mehrdad", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "These learning-based methods often focus on enhancement from different perspectives, including feature space conditional coding (e.g., FVC~\\cite{hu2021fvc}and DCVC\\cite{li2021deep}), instance adaptation~\\cite{khani2021efficient,yang2024parameter}", "next_context": ", and motion estimation (e.g., DCVC-DC~\\cite{li2023neural})."}], "importance_score": 0.5}, "yang2024parameter": {"bib_key": "yang2024parameter", "bib_title": "Parameter-Efficient Instance-Adaptive Neural Video Compression", "bib_author ": "Oh, Seungjun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "These learning-based methods often focus on enhancement from different perspectives, including feature space conditional coding (e.g., FVC~\\cite{hu2021fvc}and DCVC\\cite{li2021deep}), instance adaptation~\\cite{khani2021efficient,yang2024parameter}", "next_context": ", and motion estimation (e.g., DCVC-DC~\\cite{li2023neural})."}], "importance_score": 0.5}, "li2023neural": {"bib_key": "li2023neural", "bib_title": "Neural Video Compression with Diverse Contexts", "bib_author ": "Jiahao Li", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "These learning-based methods often focus on enhancement from different perspectives, including feature space conditional coding (e.g., FVC~\\cite{hu2021fvc}and DCVC\\cite{li2021deep}), instance adaptation~\\cite{khani2021efficient,yang2024parameter}, and motion estimation (e.g., DCVC-DC~\\cite{li2023neural}", "next_context": ")."}], "importance_score": 1.0}, "ho2022canf": {"bib_key": "ho2022canf", "bib_title": "{CANF-VC}: Conditional augmented normalizing flows for video compression", "bib_author ": "Ho, Yung-Han", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "New architectures have also been proposed such as CANF-VC~\\cite{ho2022canf}", "next_context": "based on a video generative model, MTMT\\cite{xiang2022mimt}using a masked image modeling transformer-based entropy model and VCT\\cite{mentzer2022vct}based on a video compression transformer."}], "importance_score": 1.0}, "guo2023evc": {"bib_key": "guo2023evc", "bib_title": "EVC: Towards Real-Time Neural Image Compression with Mask Decay", "bib_author ": "Guo-Hua, Wang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "To address this issue, researchers attempted to achieve complexity reduction while maintaining the coding performance through model pruning and knowledge distillation~\\cite{guo2023evc,peng2024accelerating}", "next_context": "."}], "importance_score": 0.5}, "peng2024accelerating": {"bib_key": "peng2024accelerating", "bib_title": "Accelerating learnt video codecs with gradient decay and layer-wise distillation", "bib_author ": "Peng, Tianhao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "To address this issue, researchers attempted to achieve complexity reduction while maintaining the coding performance through model pruning and knowledge distillation~\\cite{guo2023evc,peng2024accelerating}", "next_context": "."}], "importance_score": 0.5}, "nawala2024bvi": {"bib_key": "nawala2024bvi", "bib_title": "{BVI-AOM}: A New Training Dataset for Deep Video Compression Optimization", "bib_author ": "Nawa{\\l}a, Jakub", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "It should be noted that the neural codecs mentioned above are typically trained offline with diverse video content\\cite{nawala2024bvi}", "next_context": ", and deployed online for inference."}], "importance_score": 1.0}, "li2022nerv": {"bib_key": "li2022nerv", "bib_title": "{E-NeRV}: Expedite neural video representation with disentangled spatial-temporal context", "bib_author ": "Li, Zizhang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "~\\cite{chen2021nerv}, patch~\\cite{bai2023ps}or disentangled spatial/grid coordinates~\\cite{li2022nerv}", "next_context": "as model input, while content-based approaches~\\cite{kwan2024hinerv,kim2024c3,leguay2024cool}have content-specific embedding as inputs."}], "importance_score": 1.0}, "bossen2023vtmctc": {"bib_key": "bossen2023vtmctc", "bib_title": "{VTM} Common Test Conditions and Software Reference Configurations for {SDR} Video", "bib_author ": "F. Bossen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "To address this limitation, significant advances  have been made~\\cite{gao2024pnvc}towards more practical INR-based video compression (such as the Low Delay and Random Access modes in VVC VTM\\cite{bossen2023vtmctc}", "next_context": ") by combining pre-training and online model overfitting."}], "importance_score": 1.0}, "iscas2024": {"bib_key": "iscas2024", "bib_title": "{ISCAS 2024} Grand Challenge on Neural Network-based Video Coding", "bib_author ": "", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "The best performer in ISCAS 2024 NN-based Video Coding Grand Challenge offers an overall 55\\%BD-rate saving over HEVC Test Model HM\\cite{iscas2024}", "next_context": ", while the winner of the CLIC (video coding track) in 2024, a neural-network enhanced ECM codec\\cite{zhao2024neural}with a CNN-based in-loop filter, shows a more than 2dB (in PSNR) gain compared to VTM (RA) at the same bitrates."}], "importance_score": 1.0}, "zhao2024neural": {"bib_key": "zhao2024neural", "bib_title": "A Neural-network Enhanced Video Coding Framework beyond {ECM}", "bib_author ": "Zhao, Yanchen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "The best performer in ISCAS 2024 NN-based Video Coding Grand Challenge offers an overall 55\\%BD-rate saving over HEVC Test Model HM\\cite{iscas2024}, while the winner of the CLIC (video coding track) in 2024, a neural-network enhanced ECM codec\\cite{zhao2024neural}", "next_context": "with a CNN-based in-loop filter, shows a more than 2dB (in PSNR) gain compared to VTM (RA) at the same bitrates."}], "importance_score": 1.0}, "zeghidour2021soundstream": {"bib_key": "zeghidour2021soundstream", "bib_title": "Soundstream: An end-to-end neural audio codec", "bib_author ": "Zeghidour, Neil", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Audio Compression", "prev_context": "SoundStream\\cite{zeghidour2021soundstream}", "next_context": "is one of such models, which can encode audio content at various bitrates."}], "importance_score": 1.0}, "kumar2024high": {"bib_key": "kumar2024high", "bib_title": "High-fidelity audio compression with improved rvqgan", "bib_author ": "Kumar, Rithesh", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Audio Compression", "prev_context": "A more advanced universal model has been further developed\\cite{kumar2024high}", "next_context": "based on improved adversarial and reconstruction losses, which can compress different types of audio."}], "importance_score": 1.0}, "siuzdak2024snac": {"bib_key": "siuzdak2024snac", "bib_title": "{SNAC}: Multi-Scale Neural Audio Codec", "bib_author ": "Hubert Siuzdak", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Audio Compression", "prev_context": "RVQ has also been extended from a single scale to multiple scales\\cite{siuzdak2024snac}", "next_context": ", which performs hierarchical quantization at variable frame rates."}], "importance_score": 1.0}, "yang2024uniaudio": {"bib_key": "yang2024uniaudio", "bib_title": "{UniAudio 1.5}: Large Language Model-Driven Audio Codec is A Few-Shot Audio Task Learner", "bib_author ": "Dongchao Yang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Audio Compression", "prev_context": "UniAudio 1.5\\cite{yang2024uniaudio}", "next_context": "is one of such attempts, which converts an audio into the textural space, which can be represented by a pre-trained LLM that shares a similar backbone of UniAudio\\cite{yang2023uniaudio}, a universal audio foundation model."}], "importance_score": 1.0}, "yang2023uniaudio": {"bib_key": "yang2023uniaudio", "bib_title": "{UniAudio}: An audio foundation model toward universal audio generation", "bib_author ": "Yang, Dongchao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Audio Compression", "prev_context": "UniAudio 1.5\\cite{yang2024uniaudio}is one of such attempts, which converts an audio into the textural space, which can be represented by a pre-trained LLM that shares a similar backbone of UniAudio\\cite{yang2023uniaudio}", "next_context": ", a universal audio foundation model."}], "importance_score": 1.0}, "zhai2020perceptual": {"bib_key": "zhai2020perceptual", "bib_title": "Perceptual image quality assessment: a survey", "bib_author ": "Zhai, Guangtao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": null, "prev_context": "\\cite{zhai2020perceptual,zheng2024video,zhang2024quality}", "next_context": "."}], "importance_score": 0.3333333333333333}, "zheng2024video": {"bib_key": "zheng2024video", "bib_title": "Video Quality Assessment: A Comprehensive Survey", "bib_author ": "Zheng, Qi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": null, "prev_context": "\\cite{zhai2020perceptual,zheng2024video,zhang2024quality}", "next_context": "."}], "importance_score": 0.3333333333333333}, "zhang2024quality": {"bib_key": "zhang2024quality", "bib_title": "Quality assessment in the era of large models: A survey", "bib_author ": "Zhang, Zicheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": null, "prev_context": "\\cite{zhai2020perceptual,zheng2024video,zhang2024quality}", "next_context": "."}], "importance_score": 0.3333333333333333}, "Bovik_SSIM": {"bib_key": "Bovik_SSIM", "bib_title": "Image quality assessment: from error visibility to structural similarity", "bib_author ": "Z. {Wang}", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants\\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}", "next_context": "), distortion\\cite{chandler2007vsnr,larson2010most,STMAD}, and artifacts\\cite{ou2010perceptual,zhu2014no,zhang2015perception}."}], "importance_score": 0.3333333333333333}, "wang2003multiscale": {"bib_key": "wang2003multiscale", "bib_title": "Multiscale structural similarity for image quality assessment", "bib_author ": "Wang, Zhou", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants\\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}", "next_context": "), distortion\\cite{chandler2007vsnr,larson2010most,STMAD}, and artifacts\\cite{ou2010perceptual,zhu2014no,zhang2015perception}."}], "importance_score": 0.3333333333333333}, "rehman2015display": {"bib_key": "rehman2015display", "bib_title": "Display device-adapted video quality-of-experience assessment", "bib_author ": "Rehman, Abdul", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants\\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}", "next_context": "), distortion\\cite{chandler2007vsnr,larson2010most,STMAD}, and artifacts\\cite{ou2010perceptual,zhu2014no,zhang2015perception}."}], "importance_score": 0.3333333333333333}, "chandler2007vsnr": {"bib_key": "chandler2007vsnr", "bib_title": "{VSNR}: A wavelet-based visual signal-to-noise ratio for natural images", "bib_author ": "Ch", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants\\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}), distortion\\cite{chandler2007vsnr,larson2010most,STMAD}", "next_context": ", and artifacts\\cite{ou2010perceptual,zhu2014no,zhang2015perception}."}], "importance_score": 0.3333333333333333}, "larson2010most": {"bib_key": "larson2010most", "bib_title": "Most apparent distortion: full-reference image quality assessment and the role of strategy", "bib_author ": "Larson, Eric C", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants\\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}), distortion\\cite{chandler2007vsnr,larson2010most,STMAD}", "next_context": ", and artifacts\\cite{ou2010perceptual,zhu2014no,zhang2015perception}."}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{sheikh2006astatistical}, CSIQ\\cite{larson2010most}", "next_context": ", TID2013\\cite{ponomarenko2013color}, PieAPP and PIPAL, while video quality databases such as LIVE-VQA~\\cite{seshadrinathan2010study}, KoNViD-1K~\\cite{hosu2017konstanz}, YouTube UGC~\\cite{wang2019youtube}and LIVE-VQC~\\cite{sinno2018large}are typically employed for benchmarking in the literature."}], "importance_score": 1.3333333333333333}, "STMAD": {"bib_key": "STMAD", "bib_title": "A spatiotemporal most-apparent-distortion model for video quality assessment", "bib_author ": "P. V. {Vu}", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants\\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}), distortion\\cite{chandler2007vsnr,larson2010most,STMAD}", "next_context": ", and artifacts\\cite{ou2010perceptual,zhu2014no,zhang2015perception}."}], "importance_score": 0.3333333333333333}, "ou2010perceptual": {"bib_key": "ou2010perceptual", "bib_title": "Perceptual quality assessment of video considering both frame rate and quantization artifacts", "bib_author ": "Ou, Yen-Fu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants\\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}), distortion\\cite{chandler2007vsnr,larson2010most,STMAD}, and artifacts\\cite{ou2010perceptual,zhu2014no,zhang2015perception}", "next_context": "."}], "importance_score": 0.3333333333333333}, "zhu2014no": {"bib_key": "zhu2014no", "bib_title": "No-reference video quality assessment based on artifact measurement and statistical analysis", "bib_author ": "Zhu, Kongfeng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants\\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}), distortion\\cite{chandler2007vsnr,larson2010most,STMAD}, and artifacts\\cite{ou2010perceptual,zhu2014no,zhang2015perception}", "next_context": "."}], "importance_score": 0.3333333333333333}, "zhang2015perception": {"bib_key": "zhang2015perception", "bib_title": "A perception-based hybrid model for video quality assessment", "bib_author ": "Zhang, Fan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants\\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}), distortion\\cite{chandler2007vsnr,larson2010most,STMAD}, and artifacts\\cite{ou2010perceptual,zhu2014no,zhang2015perception}", "next_context": "."}], "importance_score": 0.3333333333333333}, "helmholtz1896handbook": {"bib_key": "helmholtz1896handbook", "bib_title": "Handbook of Physiological Optics", "bib_author ": "H. L. F. von Helmholtz", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "In many cases, the extracted features are further processed by models that simulate texture masking\\cite{helmholtz1896handbook}", "next_context": ", contrast sensitivity\\cite{kelly1977visual}, and saliency\\cite{itti2001computational}."}], "importance_score": 1.0}, "kelly1977visual": {"bib_key": "kelly1977visual", "bib_title": "Visual contrast sensitivity", "bib_author ": "Kelly, DH", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "In many cases, the extracted features are further processed by models that simulate texture masking\\cite{helmholtz1896handbook}, contrast sensitivity\\cite{kelly1977visual}", "next_context": ", and saliency\\cite{itti2001computational}."}], "importance_score": 1.0}, "itti2001computational": {"bib_key": "itti2001computational", "bib_title": "Computational modelling of visual attention", "bib_author ": "Itti, Laurent", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "In many cases, the extracted features are further processed by models that simulate texture masking\\cite{helmholtz1896handbook}, contrast sensitivity\\cite{kelly1977visual}, and saliency\\cite{itti2001computational}", "next_context": "."}], "importance_score": 1.0}, "kim2017deep": {"bib_key": "kim2017deep", "bib_title": "Deep learning of human visual sensitivity in image quality assessment framework", "bib_author ": "Kim, Jongyoo", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Initially, convolutional neural networks were typically used, with notable examples such as DeepQA\\cite{kim2017deep}", "next_context": ", LPIPS\\cite{zhang2018unreasonable}and CONTRIQUE\\cite{madhusudana2022image}for image quality assessment, and TLVQA\\cite{korhonen2019two}, C3DVQA\\cite{xu2020c3dvqa}and DeepVQA\\cite{kim2018deep}for video quality assessment."}], "importance_score": 1.0}, "zhang2018unreasonable": {"bib_key": "zhang2018unreasonable", "bib_title": "The unreasonable effectiveness of deep features as a perceptual metric", "bib_author ": "Zhang, Richard", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Initially, convolutional neural networks were typically used, with notable examples such as DeepQA\\cite{kim2017deep}, LPIPS\\cite{zhang2018unreasonable}", "next_context": "and CONTRIQUE\\cite{madhusudana2022image}for image quality assessment, and TLVQA\\cite{korhonen2019two}, C3DVQA\\cite{xu2020c3dvqa}and DeepVQA\\cite{kim2018deep}for video quality assessment."}], "importance_score": 1.0}, "madhusudana2022image": {"bib_key": "madhusudana2022image", "bib_title": "Image quality assessment using contrastive learning", "bib_author ": "Madhusudana, Pavan C", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Initially, convolutional neural networks were typically used, with notable examples such as DeepQA\\cite{kim2017deep}, LPIPS\\cite{zhang2018unreasonable}and CONTRIQUE\\cite{madhusudana2022image}", "next_context": "for image quality assessment, and TLVQA\\cite{korhonen2019two}, C3DVQA\\cite{xu2020c3dvqa}and DeepVQA\\cite{kim2018deep}for video quality assessment."}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{madhusudana2022image}", "next_context": "learns relevant features from an unannotated image database based on the prediction of distortion types and degrees through contrastive learning."}], "importance_score": 2.0}, "korhonen2019two": {"bib_key": "korhonen2019two", "bib_title": "Two-level approach for no-reference consumer video quality assessment", "bib_author ": "Korhonen, Jari", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Initially, convolutional neural networks were typically used, with notable examples such as DeepQA\\cite{kim2017deep}, LPIPS\\cite{zhang2018unreasonable}and CONTRIQUE\\cite{madhusudana2022image}for image quality assessment, and TLVQA\\cite{korhonen2019two}", "next_context": ", C3DVQA\\cite{xu2020c3dvqa}and DeepVQA\\cite{kim2018deep}for video quality assessment."}], "importance_score": 1.0}, "xu2020c3dvqa": {"bib_key": "xu2020c3dvqa", "bib_title": "C3DVQA: Full-reference video quality assessment with 3D convolutional neural network", "bib_author ": "Xu, Munan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Initially, convolutional neural networks were typically used, with notable examples such as DeepQA\\cite{kim2017deep}, LPIPS\\cite{zhang2018unreasonable}and CONTRIQUE\\cite{madhusudana2022image}for image quality assessment, and TLVQA\\cite{korhonen2019two}, C3DVQA\\cite{xu2020c3dvqa}", "next_context": "and DeepVQA\\cite{kim2018deep}for video quality assessment."}], "importance_score": 1.0}, "kim2018deep": {"bib_key": "kim2018deep", "bib_title": "Deep video quality assessor: From spatio-temporal visual sensitivity to a convolutional neural aggregation network", "bib_author ": "Kim, Woojae", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Initially, convolutional neural networks were typically used, with notable examples such as DeepQA\\cite{kim2017deep}, LPIPS\\cite{zhang2018unreasonable}and CONTRIQUE\\cite{madhusudana2022image}for image quality assessment, and TLVQA\\cite{korhonen2019two}, C3DVQA\\cite{xu2020c3dvqa}and DeepVQA\\cite{kim2018deep}", "next_context": "for video quality assessment."}], "importance_score": 1.0}, "touvron2023llama": {"bib_key": "touvron2023llama", "bib_title": "Llama: Open and efficient foundation language models", "bib_author ": "Touvron, Hugo", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "More recently, inspired by the success of large language models (LLMs)\\cite{openai:gpt4:2023,touvron2023llama}", "next_context": "in other machine learning tasks, these have been utilized in image and video quality assessment,  demonstrating significant potential to achieve better model generalization."}], "importance_score": 0.5}, "wu2024qbench": {"bib_key": "wu2024qbench", "bib_title": "Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision", "bib_author ": "Wu, Haoning", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Q-Bench\\cite{wu2024qbench}", "next_context": "is one of the first attempts that employs multimodal large language models to predict the perceptual quality of images based on prompt-driven evaluation."}], "importance_score": 1.0}, "wu2024qalign": {"bib_key": "wu2024qalign", "bib_title": "{Q-Align}: Teaching {LMMs} for Visual Scoring via Discrete Text-Defined Levels", "bib_author ": "Wu, Haoning", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{wu2024qalign}", "next_context": "."}], "importance_score": 1.0}, "chen2023x": {"bib_key": "chen2023x", "bib_title": "{X-iqe}: explainable image quality evaluation for text-to-image generation with visual large language models", "bib_author ": "Chen, Yixiong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{chen2023x}", "next_context": "that performs the quality prompt in a multi-iteration manner focusing on both image fidelity and aesthetics."}], "importance_score": 1.0}, "zhu20242afc": {"bib_key": "zhu20242afc", "bib_title": "{2AFC} Prompting of Large Multimodal Models for Image Quality Assessment", "bib_author ": "Zhu, Hanwei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Prompt-based approaches have also been proposed for differentiating the quality difference between multiple images, such as 2AFC-LMMs\\cite{zhu20242afc}", "next_context": "based on a two-alternative forced choice prompt and MAP (maximum a posteriori) estimation."}], "importance_score": 1.0}, "miyata2024zen": {"bib_key": "miyata2024zen", "bib_title": "ZEN-IQA: Zero-Shot Explainable and No-Reference Image Quality Assessment With Vision Language Model", "bib_author ": "Miyata, Takamichi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{miyata2024zen}", "next_context": ", QA-CLIP\\cite{pan2023quality}and PromptIQA\\cite{chen2025promptiqa}."}], "importance_score": 1.0}, "pan2023quality": {"bib_key": "pan2023quality", "bib_title": "Quality-aware clip for blind image quality assessment", "bib_author ": "Pan, Wensheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{miyata2024zen}, QA-CLIP\\cite{pan2023quality}", "next_context": "and PromptIQA\\cite{chen2025promptiqa}."}], "importance_score": 1.0}, "chen2025promptiqa": {"bib_key": "chen2025promptiqa", "bib_title": "Promptiqa: Boosting the performance and generalization for no-reference image quality assessment via prompts", "bib_author ": "Chen, Zewen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{miyata2024zen}, QA-CLIP\\cite{pan2023quality}and PromptIQA\\cite{chen2025promptiqa}", "next_context": "."}], "importance_score": 1.0}, "wu2023exploring": {"bib_key": "wu2023exploring", "bib_title": "Exploring Opinion-Unaware Video Quality Assessment with Semantic Affinity Criterion", "bib_author ": "Wu, Haoning", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Similar works have also been proposed for video quality assessment, such as BVQI\\cite{wu2023exploring,wu2023towards}", "next_context": "and COVER\\cite{he2024cover}."}], "importance_score": 0.5}, "wu2023towards": {"bib_key": "wu2023towards", "bib_title": "Towards robust text-prompted semantic criterion for in-the-wild video quality assessment", "bib_author ": "Wu, Haoning", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Similar works have also been proposed for video quality assessment, such as BVQI\\cite{wu2023exploring,wu2023towards}", "next_context": "and COVER\\cite{he2024cover}."}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "There are also databases developed that investigate the impact of specific video formats and/or artifacts, such as LIVE-YT-HFR\\cite{madhusudana2021subjective}focusing on frame rates, VSR-QAD\\cite{zhou2024database}on spatial resolution (or super-resolution artifacts), BAND-2k\\cite{chen2024band2k}on banding artifacts and Maxwell\\cite{wu2023towards}", "next_context": "/BVI-Artifact\\cite{feng2024bvi}containing multiple artifacts commonly produced in video streaming."}], "importance_score": 1.5}, "sheikh2006astatistical": {"bib_key": "sheikh2006astatistical", "bib_title": "A statistical evaluation of recent full reference image quality assessment algorithms", "bib_author ": "Sheikh, Hamid R.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{sheikh2006astatistical}", "next_context": ", CSIQ\\cite{larson2010most}, TID2013\\cite{ponomarenko2013color}, PieAPP and PIPAL, while video quality databases such as LIVE-VQA~\\cite{seshadrinathan2010study}, KoNViD-1K~\\cite{hosu2017konstanz}, YouTube UGC~\\cite{wang2019youtube}and LIVE-VQC~\\cite{sinno2018large}are typically employed for benchmarking in the literature."}], "importance_score": 1.0}, "ponomarenko2013color": {"bib_key": "ponomarenko2013color", "bib_title": "Color image database TID2013: Peculiarities and preliminary results", "bib_author ": "Ponomarenko, Nikolay", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{sheikh2006astatistical}, CSIQ\\cite{larson2010most}, TID2013\\cite{ponomarenko2013color}", "next_context": ", PieAPP and PIPAL, while video quality databases such as LIVE-VQA~\\cite{seshadrinathan2010study}, KoNViD-1K~\\cite{hosu2017konstanz}, YouTube UGC~\\cite{wang2019youtube}and LIVE-VQC~\\cite{sinno2018large}are typically employed for benchmarking in the literature."}], "importance_score": 1.0}, "seshadrinathan2010study": {"bib_key": "seshadrinathan2010study", "bib_title": "Study of subjective and objective quality assessment of video", "bib_author ": "Seshadrinathan, Kalpana", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{sheikh2006astatistical}, CSIQ\\cite{larson2010most}, TID2013\\cite{ponomarenko2013color}, PieAPP and PIPAL, while video quality databases such as LIVE-VQA~\\cite{seshadrinathan2010study}", "next_context": ", KoNViD-1K~\\cite{hosu2017konstanz}, YouTube UGC~\\cite{wang2019youtube}and LIVE-VQC~\\cite{sinno2018large}are typically employed for benchmarking in the literature."}], "importance_score": 1.0}, "hosu2017konstanz": {"bib_key": "hosu2017konstanz", "bib_title": "The Konstanz natural video database \n(KoNViD-1k)", "bib_author ": "Hosu, Vlad", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{sheikh2006astatistical}, CSIQ\\cite{larson2010most}, TID2013\\cite{ponomarenko2013color}, PieAPP and PIPAL, while video quality databases such as LIVE-VQA~\\cite{seshadrinathan2010study}, KoNViD-1K~\\cite{hosu2017konstanz}", "next_context": ", YouTube UGC~\\cite{wang2019youtube}and LIVE-VQC~\\cite{sinno2018large}are typically employed for benchmarking in the literature."}], "importance_score": 1.0}, "wang2019youtube": {"bib_key": "wang2019youtube", "bib_title": "YouTube {UGC} dataset for video compression research", "bib_author ": "Wang, Yilin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{sheikh2006astatistical}, CSIQ\\cite{larson2010most}, TID2013\\cite{ponomarenko2013color}, PieAPP and PIPAL, while video quality databases such as LIVE-VQA~\\cite{seshadrinathan2010study}, KoNViD-1K~\\cite{hosu2017konstanz}, YouTube UGC~\\cite{wang2019youtube}", "next_context": "and LIVE-VQC~\\cite{sinno2018large}are typically employed for benchmarking in the literature."}], "importance_score": 1.0}, "sinno2018large": {"bib_key": "sinno2018large", "bib_title": "Large-scale study of perceptual video quality", "bib_author ": "Sinno, Zeina", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{sheikh2006astatistical}, CSIQ\\cite{larson2010most}, TID2013\\cite{ponomarenko2013color}, PieAPP and PIPAL, while video quality databases such as LIVE-VQA~\\cite{seshadrinathan2010study}, KoNViD-1K~\\cite{hosu2017konstanz}, YouTube UGC~\\cite{wang2019youtube}and LIVE-VQC~\\cite{sinno2018large}", "next_context": "are typically employed for benchmarking in the literature."}], "importance_score": 1.0}, "madhusudana2021subjective": {"bib_key": "madhusudana2021subjective", "bib_title": "Subjective and objective quality assessment of high frame rate videos", "bib_author ": "Madhusudana, Pavan C", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "There are also databases developed that investigate the impact of specific video formats and/or artifacts, such as LIVE-YT-HFR\\cite{madhusudana2021subjective}", "next_context": "focusing on frame rates, VSR-QAD\\cite{zhou2024database}on spatial resolution (or super-resolution artifacts), BAND-2k\\cite{chen2024band2k}on banding artifacts and Maxwell\\cite{wu2023towards}/BVI-Artifact\\cite{feng2024bvi}containing multiple artifacts commonly produced in video streaming."}], "importance_score": 1.0}, "zhou2024database": {"bib_key": "zhou2024database", "bib_title": "A Database and Model for the Visual Quality Assessment of Super-Resolution Videos", "bib_author ": "Zhou, Fei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "There are also databases developed that investigate the impact of specific video formats and/or artifacts, such as LIVE-YT-HFR\\cite{madhusudana2021subjective}focusing on frame rates, VSR-QAD\\cite{zhou2024database}", "next_context": "on spatial resolution (or super-resolution artifacts), BAND-2k\\cite{chen2024band2k}on banding artifacts and Maxwell\\cite{wu2023towards}/BVI-Artifact\\cite{feng2024bvi}containing multiple artifacts commonly produced in video streaming."}], "importance_score": 1.0}, "chen2024band2k": {"bib_key": "chen2024band2k", "bib_title": "BAND-2k: Banding Artifact Noticeable Database for Banding Detection and Quality Assessment", "bib_author ": "Chen, Zijian", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "There are also databases developed that investigate the impact of specific video formats and/or artifacts, such as LIVE-YT-HFR\\cite{madhusudana2021subjective}focusing on frame rates, VSR-QAD\\cite{zhou2024database}on spatial resolution (or super-resolution artifacts), BAND-2k\\cite{chen2024band2k}", "next_context": "on banding artifacts and Maxwell\\cite{wu2023towards}/BVI-Artifact\\cite{feng2024bvi}containing multiple artifacts commonly produced in video streaming."}], "importance_score": 1.0}, "feng2024bvi": {"bib_key": "feng2024bvi", "bib_title": "BVI-Artefact: An artefact detection benchmark dataset for streamed videos", "bib_author ": "Feng, Chen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "There are also databases developed that investigate the impact of specific video formats and/or artifacts, such as LIVE-YT-HFR\\cite{madhusudana2021subjective}focusing on frame rates, VSR-QAD\\cite{zhou2024database}on spatial resolution (or super-resolution artifacts), BAND-2k\\cite{chen2024band2k}on banding artifacts and Maxwell\\cite{wu2023towards}/BVI-Artifact\\cite{feng2024bvi}", "next_context": "containing multiple artifacts commonly produced in video streaming."}], "importance_score": 1.0}, "liu2017rankiqa": {"bib_key": "liu2017rankiqa", "bib_title": "RankIQA: Learning from Rankings for No-Reference Image Quality Assessment", "bib_author ": "Liu, Xialei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{liu2017rankiqa}", "next_context": "and UNIQUE\\cite{zhang2021uncertainty}for the image quality assessment task, and VFIPS\\cite{hou2022perceptual}and RankDVQA\\cite{feng2024rankdvqa}for video quality assessment."}], "importance_score": 1.0}, "zhang2021uncertainty": {"bib_key": "zhang2021uncertainty", "bib_title": "Uncertainty-aware blind image quality assessment in the laboratory and wild", "bib_author ": "Zhang, Weixia", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{liu2017rankiqa}and UNIQUE\\cite{zhang2021uncertainty}", "next_context": "for the image quality assessment task, and VFIPS\\cite{hou2022perceptual}and RankDVQA\\cite{feng2024rankdvqa}for video quality assessment."}], "importance_score": 1.0}, "hou2022perceptual": {"bib_key": "hou2022perceptual", "bib_title": "A perceptual quality metric for video frame interpolation", "bib_author ": "Hou, Qiqi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{liu2017rankiqa}and UNIQUE\\cite{zhang2021uncertainty}for the image quality assessment task, and VFIPS\\cite{hou2022perceptual}", "next_context": "and RankDVQA\\cite{feng2024rankdvqa}for video quality assessment."}], "importance_score": 1.0}, "madhusudana2023conviqt": {"bib_key": "madhusudana2023conviqt", "bib_title": "Conviqt: Contrastive video quality estimator", "bib_author ": "Madhusudana, Pavan C", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "This method has been further applied to video quality assessment, resulting in a contrastive video quality estimator, CONVIQT\\cite{madhusudana2023conviqt}", "next_context": "."}], "importance_score": 1.0}, "zhao2023quality": {"bib_key": "zhao2023quality", "bib_title": "Quality-aware pre-trained models for blind image quality assessment", "bib_author ": "Zhao, Kai", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "More recently, quality-aware contrastive loss has been designed in\\cite{zhao2023quality,peng2024rmt}", "next_context": "to stabilize the learning process."}], "importance_score": 0.5}, "zhong:LDB:2024": {"bib_key": "zhong:LDB:2024", "bib_title": "{LDB: A} Large Language Model Debugger via Verifying Runtime Execution Step-by-step", "bib_author ": "Li Zhong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Closing Thoughts and Future of AI in Creativity", "subsection": "Challenges for AI in the Creative Sector", "subsubsection": null, "prev_context": "Many generative tools were originally designed for tasks like software development, content automation, or optimization\\cite{zhong:LDB:2024}", "next_context": ", and are ill-suited for open-ended, exploratory creation."}], "importance_score": 1.0}, "deepseekv3": {"bib_key": "deepseekv3", "bib_title": "DeepSeek-V3 Technical Report", "bib_author ": "DeepSeek-AI", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Closing Thoughts and Future of AI in Creativity", "subsection": "The future of AI technologies", "subsubsection": null, "prev_context": "Recent highly successful platforms, such as  DeepSeek-V3\\cite{deepseekv3}", "next_context": "and Qwen2.5-Max\\cite{qwen25}, are based on Mixture-of-Experts (MoE) models, which tackle complex problems by dividing them into simpler sub-tasks, each handled by a specialized``expert.\""}], "importance_score": 1.0}, "qwen25": {"bib_key": "qwen25", "bib_title": "Qwen2.5 technical report", "bib_author ": "Qwen Team", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Closing Thoughts and Future of AI in Creativity", "subsection": "The future of AI technologies", "subsubsection": null, "prev_context": "Recent highly successful platforms, such as  DeepSeek-V3\\cite{deepseekv3}and Qwen2.5-Max\\cite{qwen25}", "next_context": ", are based on Mixture-of-Experts (MoE) models, which tackle complex problems by dividing them into simpler sub-tasks, each handled by a specialized``expert.\""}], "importance_score": 1.0}, "CreativeIndustriesCouncil2021": {"bib_key": "CreativeIndustriesCouncil2021", "bib_title": "How CreaTech added 1+1 to make \u00a3981m", "bib_author ": "Creative Industries Council", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Closing Thoughts and Future of AI in Creativity", "subsection": "The future of AI technologies", "subsubsection": null, "prev_context": "Finally, as stated in\\cite{Jeary2024}, the rapid advancement of AI technologies has revolutionized cultural experiences, often referred to as `CreaTech'\u2014the convergence of the creative and digital sectors\\cite{CreativeIndustriesCouncil2021}", "next_context": "."}], "importance_score": 1.0}}, "refs": [], "table": [{"original": "\\begin{table}\n\\caption{Creative applications and corresponding AI-based methods mentioned in this paper}\n%\\tiny\n %\\hskip-5.0cm\n %\\begin{tabular}{p{1cm}p{1.4cm}|p{4cm}p{4cm}p{4cm}p{4cm}}\n \\resizebox{\\linewidth}{!}{\\begin{tabular}{lr|lll}\n \\\\\n \\toprule\n\\multicolumn{2}{c}{\\multirow{2}{*}{Application}} & \\multicolumn{3}{|c}{Technology} \\\\ \\cmidrule{3-5}\n& & Trans./Attn.$^1$ & Diffusion model$^2$  & INR\\\\\n\\midrule\n{\\bf Creation} & Text & \\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}  & \\\\\n& audio/music & \\cite{Alayrac:Flamingo:2022, Huang:GenerSpeech:2022} & \\cite{Li:Diffusion:2022, Yang:Diffsound:2023, Evans:stable:2025} & \\\\\n& Image &  \\cite{Alayrac:Flamingo:2022,esser:scaling:2024} & \\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024, Feng_Ma_2025, Liu_Ma_2025} & \\\\\n% ---------------------------------------------------------------\n& Animation/video & \\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024} & \\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024, wang2025lavie, wu2025customcrafter} &\\\\\n% ---------------------------------------------------------------\n & 3D/AR/VR & \\cite{yang:Holodeck:2024} & \\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024} & \\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear} \\\\\n% ---------------------------------------------------------------\n\\midrule\n{\\bf Information} & Text categorization & \\cite{sun:text:2023, shi:chatgraph:2023, Hou:promptboosting:2023, AI2025125952} \\\\\n% ---------------------------------------------------------------\n{\\bf Analysis} & Film analysis & \\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}  \\\\\n % ---------------------------------------------------------------\n & Content retrieval & \\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning} & \\cite{Jin:DiffusionRet:2023} \\\\\n % ---------------------------------------------------------------\n & Intelligent assistants & \\cite{King:Sasha:2024} \\\\\n % ---------------------------------------------------------------\n \\midrule\n{\\bf Content} & Enhancement  & \\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024} & \\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight} & \\cite{Yang:Implicit:2023} \\\\\n{\\bf  Enhancement  } & Style transfer & \\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR} & \\cite{Zhang:Inversion:2023,Chai:StableVideo:2023} & \\cite{Moon:generalizable:2023,Kim:Controllable:2024} \\\\\n% ---------------------------------------------------------------\n {\\bf  and Post} & Super-resolution & \\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024,wang2025seedvr} & \\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023, cao2025zero, wang2025seedvr} & \\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}\\\\\n% ---------------------------------------------------------------\n{\\bf Production} & {Restoration} & \\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025} & \\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023, cao2025zero, feng2025residual} & \\cite{Jiang:NeRT:2023} \\\\\n  % ---------------------------------------------------------------\n & Inpainting & \\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024} & \\cite{Moliner:solving:2023, Fei:Generative:2023} & \\\\\n % ---------------------------------------------------------------\n& Fusion & \\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023, LI2024102147} & \\cite{Zhao:DDFM:2023} & \\\\\n & Editing/VFX & \\cite{Shi:Motion-I2V:2024} & \\cite{Shi:Motion-I2V:2024,guo2024liveportrait} \\\\\n \\midrule\n % ---------------------------------------------------------------\n{\\bf Information} & Segmentation & \\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2,Zhang:DVISp:2025} & \\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024} & \\cite{Gong:Continuous:2023, Cen:Segment:2023} \\\\\n % ---------------------------------------------------------------\n{\\bf  Extraction} & Recognition &  \\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12} & \\cite{Li:Your:2023,Chen:DiffusionDet:2023, Zhang:DiffAD:2025, WU2025102965}\\\\\n % ---------------------------------------------------------------\n{\\bf  and}  & Tracking &  \\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring} & \\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024} & \\cite{Jung:AnyFlow:2023}\\\\\n % ---------------------------------------------------------------\n{\\bf  Understanding}  & 3D Reconstruction  & \\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024, LIU:DSEM:2025} & \\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024} & \\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}$^\\dag$  \\\\\n\\midrule\n\n% ---------------------------------------------------------------\n{\\bf Compression} & Image$^\\ast$ & \\cite{zhu2022transformer,zou2022devil,liu2023learned}& \\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual} & \\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}\\\\\n& Video& \\cite{xiang2022mimt,mentzer2022vct}& \\cite{li2024extreme}&\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}\\\\\n&Audio$^\\ast$ & & &\\\\\n\\midrule\n% ---------------------------------------------------------------\n{\\bf Quailty}  &  Image$^\\ast$ & \\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}& &\\\\\n\\textbf{Assessment}& Video$^\\ast$ & \\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}& & \\\\\n\\bottomrule\n\\multicolumn{5}{l}{$^1$ Trans./Attn. include transformers, mamba and CNN-based architectures that use attention module.} \\\\\n\\multicolumn{5}{l}{$^2$ Some diffusion models employ the transformer in their denoising autoencoders.}\n \\end{tabular}}\n \\footnotesize\n $^\\dag$ \nThese methods are based on explicit neural representations. \\\\\n$^*$ It is noted that for some compression and quality assessment tasks, there are other dominant network architectures in existing works. For example, LLMs have been used for image and audio compression, and visual quality assessment. Many neural audio codecs are also based on VQ-VAE models.\n \n\\label{tab:gather}\n\\end{table}", "caption": "\\caption{Creative applications and corresponding AI-based methods mentioned in this paper}", "label": "\\label{tab:gather}", "tabular": "\\begin{tabular}{lr|lll}\n \\\\\n \\toprule\n\\multicolumn{2}{c}{\\multirow{2}{*}{Application}} & \\multicolumn{3}{|c}{Technology} \\\\ \\cmidrule{3-5}\n& & Trans./Attn.$^1$ & Diffusion model$^2$  & INR\\\\\n\\midrule\n{\\bf Creation} & Text & \\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}  & \\\\\n& audio/music & \\cite{Alayrac:Flamingo:2022, Huang:GenerSpeech:2022} & \\cite{Li:Diffusion:2022, Yang:Diffsound:2023, Evans:stable:2025} & \\\\\n& Image &  \\cite{Alayrac:Flamingo:2022,esser:scaling:2024} & \\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024, Feng_Ma_2025, Liu_Ma_2025} & \\\\\n% ---------------------------------------------------------------\n& Animation/video & \\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024} & \\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024, wang2025lavie, wu2025customcrafter} &\\\\\n% ---------------------------------------------------------------\n & 3D/AR/VR & \\cite{yang:Holodeck:2024} & \\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024} & \\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear} \\\\\n% ---------------------------------------------------------------\n\\midrule\n{\\bf Information} & Text categorization & \\cite{sun:text:2023, shi:chatgraph:2023, Hou:promptboosting:2023, AI2025125952} \\\\\n% ---------------------------------------------------------------\n{\\bf Analysis} & Film analysis & \\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}  \\\\\n % ---------------------------------------------------------------\n & Content retrieval & \\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning} & \\cite{Jin:DiffusionRet:2023} \\\\\n % ---------------------------------------------------------------\n & Intelligent assistants & \\cite{King:Sasha:2024} \\\\\n % ---------------------------------------------------------------\n \\midrule\n{\\bf Content} & Enhancement  & \\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024} & \\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight} & \\cite{Yang:Implicit:2023} \\\\\n{\\bf  Enhancement  } & Style transfer & \\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR} & \\cite{Zhang:Inversion:2023,Chai:StableVideo:2023} & \\cite{Moon:generalizable:2023,Kim:Controllable:2024} \\\\\n% ---------------------------------------------------------------\n {\\bf  and Post} & Super-resolution & \\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024,wang2025seedvr} & \\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023, cao2025zero, wang2025seedvr} & \\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}\\\\\n% ---------------------------------------------------------------\n{\\bf Production} & {Restoration} & \\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025} & \\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023, cao2025zero, feng2025residual} & \\cite{Jiang:NeRT:2023} \\\\\n  % ---------------------------------------------------------------\n & Inpainting & \\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024} & \\cite{Moliner:solving:2023, Fei:Generative:2023} & \\\\\n % ---------------------------------------------------------------\n& Fusion & \\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023, LI2024102147} & \\cite{Zhao:DDFM:2023} & \\\\\n & Editing/VFX & \\cite{Shi:Motion-I2V:2024} & \\cite{Shi:Motion-I2V:2024,guo2024liveportrait} \\\\\n \\midrule\n % ---------------------------------------------------------------\n{\\bf Information} & Segmentation & \\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2,Zhang:DVISp:2025} & \\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024} & \\cite{Gong:Continuous:2023, Cen:Segment:2023} \\\\\n % ---------------------------------------------------------------\n{\\bf  Extraction} & Recognition &  \\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12} & \\cite{Li:Your:2023,Chen:DiffusionDet:2023, Zhang:DiffAD:2025, WU2025102965}\\\\\n % ---------------------------------------------------------------\n{\\bf  and}  & Tracking &  \\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring} & \\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024} & \\cite{Jung:AnyFlow:2023}\\\\\n % ---------------------------------------------------------------\n{\\bf  Understanding}  & 3D Reconstruction  & \\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024, LIU:DSEM:2025} & \\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024} & \\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}$^\\dag$  \\\\\n\\midrule\n\n% ---------------------------------------------------------------\n{\\bf Compression} & Image$^\\ast$ & \\cite{zhu2022transformer,zou2022devil,liu2023learned}& \\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual} & \\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}\\\\\n& Video& \\cite{xiang2022mimt,mentzer2022vct}& \\cite{li2024extreme}&\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}\\\\\n&Audio$^\\ast$ & & &\\\\\n\\midrule\n% ---------------------------------------------------------------\n{\\bf Quailty}  &  Image$^\\ast$ & \\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}& &\\\\\n\\textbf{Assessment}& Video$^\\ast$ & \\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}& & \\\\\n\\bottomrule\n\\multicolumn{5}{l}{$^1$ Trans./Attn. include transformers, mamba and CNN-based architectures that use attention module.} \\\\\n\\multicolumn{5}{l}{$^2$ Some diffusion models employ the transformer in their denoising autoencoders.}\n \\end{tabular}", "subtables": []}], "figure": [{"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/generativemodel.png}\n    \\caption{Generative AI. (a) Transformer architecture \\cite{Vaswani:attention:2017}. (b) The top row represents the diffusion process and the bottom row represents the generation process of the new image \\cite{Yang:diffusion:2023}. (c) Latent Diffusion Models (LDM) \\cite{Rombach:LDM:2022}. }\n    \\label{fig:generativemodel}\n\\end{figure}", "caption": "\\caption{Generative AI. (a) Transformer architecture \\cite{Vaswani:attention:2017}. (b) The top row represents the diffusion process and the bottom row represents the generation process of the new image \\cite{Yang:diffusion:2023}. (c) Latent Diffusion Models (LDM) \\cite{Rombach:LDM:2022}. }", "label": "\\label{fig:generativemodel}", "subfigures": [], "figure_paths": ["figures/generativemodel.png"]}, {"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/FLASK_LLM_and_history.png}\n    \\caption{(a) Timeline of large language models. (b) Performance comparison evaluated by FLASK \\cite{Ye:FLASK:2024}.}\n    \\label{fig:FLASK_LLM_and_history}\n\\end{figure}", "caption": "\\caption{(a) Timeline of large language models. (b) Performance comparison evaluated by FLASK \\cite{Ye:FLASK:2024}.}", "label": "\\label{fig:FLASK_LLM_and_history}", "subfigures": [], "figure_paths": ["figures/FLASK_LLM_and_history.png"]}, {"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/LLMGround.jpg}\n    \\caption{Text-to-image generation (generated on 27 November 2024). (a) text-to-image generation by Ideogram v1, DALL\u00b7E 3, Photoshop 2025, and sdxy-turbo by Nvidia. (b) The top-row images were generated by DALL\u00b7E in ChatGPT 4. The bottom-row images are generated by LLM-grounded Diffusion \\cite{Lian:LLMG:2024}. }\n    \\label{fig:LLMGround}\n\\end{figure}", "caption": "\\caption{Text-to-image generation (generated on 27 November 2024). (a) text-to-image generation by Ideogram v1, DALL\u00b7E 3, Photoshop 2025, and sdxy-turbo by Nvidia. (b) The top-row images were generated by DALL\u00b7E in ChatGPT 4. The bottom-row images are generated by LLM-grounded Diffusion \\cite{Lian:LLMG:2024}. }", "label": "\\label{fig:LLMGround}", "subfigures": [], "figure_paths": ["figures/LLMGround.jpg"]}, {"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/Deepmotion_Vasa.png}\n    \\caption{(Left) Video-to-3D post animation by DeepMotion. (Right) Image and audio to video by VASA-1 \\cite{xu:VASA-1:2024}}\n    \\label{fig:Deepmotion_Vasa}\n\\end{figure}", "caption": "\\caption{(Left) Video-to-3D post animation by DeepMotion. (Right) Image and audio to video by VASA-1 \\cite{xu:VASA-1:2024}}", "label": "\\label{fig:Deepmotion_Vasa}", "subfigures": [], "figure_paths": ["figures/Deepmotion_Vasa.png"]}, {"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/SR_results.png}\n    \\caption{(Left) Examples of SR ($\\times$4) using generative model. (Right) Real-time portrait editing with FacePoke.} \n    \\label{fig:SR}\n\\end{figure}", "caption": "\\caption{(Left) Examples of SR ($\\times$4) using generative model. (Right) Real-time portrait editing with FacePoke.}", "label": "\\label{fig:SR}", "subfigures": [], "figure_paths": ["figures/SR_results.png"]}, {"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation.png}\n    \\caption{(a) Tasks in Object-centric understanding defined by Yan et al. \\cite{Yan:Universal:2023} (REC:,  Referring Expression Comprehension, RES: Referring Expression Segmentation, VOS:  Video Object Segmentation, RVOS: Referring Video Object Segmentation, MOT: Multiple Object Tracking, MOTS: Multi-Object Tracking and Segmentation, VIS: Video Instance Segmentation, SOT: Single Object Tracking. (b) Current high-quality segmentation \\cite{Ke:SAM-HQ:2023}.}\n    \\label{fig:segmentation}\n\\end{figure}", "caption": "\\caption{(a) Tasks in Object-centric understanding defined by Yan et al. \\cite{Yan:Universal:2023} (REC:,  Referring Expression Comprehension, RES: Referring Expression Segmentation, VOS:  Video Object Segmentation, RVOS: Referring Video Object Segmentation, MOT: Multiple Object Tracking, MOTS: Multi-Object Tracking and Segmentation, VIS: Video Instance Segmentation, SOT: Single Object Tracking. (b) Current high-quality segmentation \\cite{Ke:SAM-HQ:2023}.}", "label": "\\label{fig:segmentation}", "subfigures": [], "figure_paths": ["figures/segmentation.png"]}, {"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/3Drepresentation.png}\n    \\caption{3D representation. (a) Neural Radiance Fields (NeRFs) \\cite{Mildenhall:NeRF:2020}. (b) Gaussian Splatting \\cite{kerbl:3Dgaussians:2023}. (c) Example scenes of VR-GS system for 3D content interaction in VR \\cite{jiang:vrgs:2024}. }\n    \\label{fig:3Drepresentation}\n\\end{figure}", "caption": "\\caption{3D representation. (a) Neural Radiance Fields (NeRFs) \\cite{Mildenhall:NeRF:2020}. (b) Gaussian Splatting \\cite{kerbl:3Dgaussians:2023}. (c) Example scenes of VR-GS system for 3D content interaction in VR \\cite{jiang:vrgs:2024}. }", "label": "\\label{fig:3Drepresentation}", "subfigures": [], "figure_paths": ["figures/3Drepresentation.png"]}], "equations": ["\\begin{equation}\n    \\text{Attention}(Q, K, V ) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) V,\n    \\label{eqn:attention}\n\\end{equation}", "\\begin{equation}\n\\begin{split}\n        \\text{MultiHead}(Q, K, V ) &= \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O,\\\\\n        \\text{head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V ). \n\\end{split}\n\\label{eqn:MultiHead}\n\\end{equation}", "\\begin{equation}\nF(x, \\Phi, \\nabla_x \\Phi, \\nabla_x^2 \\Phi, \\dots) = 0, \\quad \\Phi : x \\mapsto \\Phi(x).\n\\label{eqn:inr}\n\\end{equation}"], "algorithm": [], "sections": {"Acknowledgements": {"content": "\n\nThis work has been funded by the UKRI MyWorld Strength in Places Programme (SIPF00006/1).\n\n\n\\newpage\n\n\\tableofcontents\n\n\\newpage\n\n", "appendix": false}, "Introduction": {"content": "\n\nThe influence of artificial intelligence (AI) has grown dramatically over the past few years, particularly due to the rise of generative AI and large language models (LLMs). These advancements are widely regarded as beneficial by many countries, creating significant opportunities for growth (e.g. as outlined in the UK, by the Authority of the House of Lords \\cite{UK:Large:2024}). These advances have also had significant direct and indirect impacts on the creative industries, influencing the direction of their growth. Generative AI, for instance, primarily focuses on generating new data that is not identical to the training data yet shares similarities with it. However, the cardinality of the training data can be huge,  larger than what any individual human has ever encountered. The resulting output may therefore act as a new source of insipration. \n\nAI tools also provide opportunities for a wider range of users to work more efficiently and  effectively,  with even greater creativity. Moreover, these new technologies not only influence creators,  but they also enable new ways for audiences to experience art and culture \\cite{Jeary2024}.\n\nA major breakthrough in generative AI has been led by OpenAI\\footnote{\\url{https://openai.com/}}, an AI research and deployment company, with their introduction of Generative Pre-trained Transformer (GPT) models for LLMs. LLMs are specifically designed to understand and generate human language. They are characterized by their vast size in terms of parameters and the amount of training data used to create them. This breakthrough was particularly impactful when the company released ChatGPT in 2022, which was fine-tuned from a model in the GPT-3.5 series. ChatGPT is a conversational model that includes advanced safety features that mitigate the generation of inappropriate content. Several other LLM platforms were also developed contemporaneously, such as LaMDA and PaLM by Google AI, Ernie Bot by Baidu, and BLOOM by BigScience. Additionally, Anthropic launched Claude, the LLM trained specifically to be harmless and honest, leveraging reinforcement learning from human feedback (RLHF) - a technique used to train AI systems to appear more human \\cite{Bai:Train:2021}. Nonetheless, ChatGPT stands out as the most renowned, thanks to its quick and efficient responses, and notably its public accessibility, being available for free. \n\nAnother breakthrough in 2022 was in the area of text-to-image models. OpenAI achieved a significant milestone with DALL\u00b7E 2, producing impressive artworks and photorealistic images despite its limited language understanding. Midjourney by Midjourney, Inc., another well-known text-to-image generator, supports higher resolution images, up to 4096$\\times$\\times4096 pixels. Stable Diffusion by Stability AI, for which the code and model weights are publicly available\\footnote{\\url{https://github.com/Stability-AI/stablediffusion}}, allows developers and artists to further adapt AI to suit their own specific applications.\n\nThe next breakthrough happened in 2023 when OpenAI unveiled GPT-4, a significantly larger model with estimated 1.8 trillion  parameters and improved performance compared to its predecessors \\cite{openai:gpt4:2023}. However, this still represents less than 1\\% of the human brain\u2019s approximately 600 trillion synaptic connections\\footnote{\\url{https://www.rsb.org.uk/biologist-features/ai-versus-the-brain}}. GPT-4 is a multimodal large language model that can generate responses to both text and images. It incorporates DALL\u00b7E 3, enabling it to comprehend a much broader range of nuances and details than earlier versions. In March 2024, Claude 3 Opus by Anthropic was released, boasting multimodal capabilities in generating images, tables, graphs and diagrams. Moreover, Anthropic claims that Claude 3 Opus outperforms GPT-4 in generating human-like dialog and contextually aware responses. These rapid advances have, in turn,  led the creative industries to face significant challenges. For example, DMG Media, the Financial Times, and Guardian Media Group have highlighted concerns about the potential impact on print journalism, particularly if AI tools reduce the need for users to click through to news websites, affecting advertising and subscription revenues \\cite{UK:Large:2024}. There is also concern about `AI-generated slop'\u2014low-quality, mass-produced content created by AI that often lacks coherence or originality\\footnote{\\url{https://reutersinstitute.politics.ox.ac.uk/news/ai-generated-slop-quietly-conquering-internet-it-threat-journalism-or-problem-will-fix-itself}}. It is typically used for spam, search engines, or clickbait, and is criticized for cluttering the internet and undermining genuine human-created content.\n\nThe generation of videos is significantly more challenging for AI than generating images. In February 2024, Google announced Gemini 1.5 which had the capability to process approximately 8 times more data than GPT-4, opening opportunities for  video  and audio processing \\footnote{\\url{https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/}}. In the same month, OpenAI provided its first preview of Sora,  a model capable of generating impressive realistic videos up to 1 minute long. Based on the videos released by OpenAI, Sora appears to outperform other text-to-video models. Sora is currently available to ChatGPT subscribers.  A month later, Gemini 1.5 announced its support for native audio understanding in 180$+$+ countries.  With the emergence of these tools, together with the prospect of further advances, it is clear that video content creation will be a major beneficiary. This will further open up the media landscape for creativity and provide more opportunities for diverse storytellers, while also reducing production time. A recent example is the AI-generated Christmas commercial by Coca-Cola\\footnote{\\url{https://www.youtube.com/watch?v=4RSTupbfGog}}. Such advertisements overcome the limitations of current technologies by using very short videos with rapid scene transitions, ensuring that any artifacts, such as unnatural fingers, are less apparent.\n\nFor the case of post-production workflows, generative AI may not have a direct impact, but the neural networks originally proposed for generative AI have been widely adapted to serve this purpose. This has led to significant improvements in both output quality and computational speed. Moreover, there is  a noticeable trend  towards adopting a unified framework rather than addressing individual tasks, as it better reflects real-world scenarios. For instance, natural history filmmaking involves challenging acquisition environments and high production standards. Filming often takes place in low light conditions, in the presence of heat haze, underwater or in adverse weather conditions. This often results in increased noise levels, focus issues, low contrast, color balance problems, and blurriness in the footage. In such cases, Unified models can offer advantages in generalizing to diverse tasks and providing flexibility. Take Painter by BAAI Vision \\cite{Wang:Painter:2023} as an example, which employs an image pair as a task prompt (similar to a text prompt in LLMs), their model transfers the input image to produce a similar output as the task prompt, enabling it to undertake various tasks such as segmentation, low-light enhancement or rain removal.\n\nWhile generative AI can facilitate and accelerate the creation and post-processing of digital media, there is an equivalent need to transmit or stream it efficiently to users. Although AI-based solutions have been proposed both for the enhancement of conventional video coding tools and for new compression frameworks, they are yet to be  adopted in practical applications due to hardware constraints, complexity issues and a lack of standardization. Despite this,  the latest learning-based video codecs have already demonstrated their potential to compete with conventional standardized video codecs and are being actively investigated in various standards bodies such as MPEG and AOM.\n\nFurthermore, in recent years,  AI has also impacted our ability to assess and monitor the perceptual quality of visual media. Advances have included new model architectures based on different attention mechanisms and the application of LLMs, which evidently improve model generalization. New training methodologies have also been proposed based on weakly/unsupervised learning, which address issues associated with the limited availability of labeled training content.\n\n\nOne of the exciting aspects of using LLMs in the creative sector is that `The human in the loop' \\cite{chung:human-loop:2021} is simplified through text prompts, with sophisticated, multilingual language capabilities enabling artists to convey complex emotions and narratives. This is important because generative AI does produce mistakes, known as hallucinations. Human oversight is thus essential to correct this through reinforcement learning with feedback \\cite{Wu:brief:2023}.\n\n\n\nIn this paper, the objective is to reveal to the reader, the latest technology advancements that have emerged since our previous review paper on AI in the creative industries (published in 2022)  \\cite{Anantrasirichai:AI:2022}. Compared to this earlier paper, which was written when most AI technologies were used as support tools, this updated review describes thesignificant disruptive shifts that have emerged over the past 3-4 years driven by generative AI and other  recent AI-based technologies.  Similar to \\cite{Anantrasirichai:AI:2022}, we first provide a high-level overview of current advanced AI technologies (Section \\ref{sec:overview}), followed by a selection of key creative domain applications (Section \\ref{sec:existing}) where current AI technologies are changing creative practice. Finally, we discuss the  challenges and the future potential of AI associated with the creative industries (Section \\ref{sec:discussion}).\n\n\n", "appendix": false}, "Current Advanced AI Technologies": {"content": "\n\\label{sec:overview}\n\nThis paper provides a review of AI in the creative industries,  building on our previous publication in 2022 \\cite{Anantrasirichai:AI:2022}. The reader is referred to that work for an introduction to AI, basic neurons, convolutional neural networks (CNNs), generative adversarial networks (GANs), recurrent neural networks (RNNs) and deep reinforcement learning (DRL). In this paper,  we emphasize four key technologies that have grown in importance since 2022 that have had a significant impact on the creative industries. These are Transformers, Large language models (LLMs), Diffusion Models (DMs), and Implicit Neural Representations (INRs). It is important to note that, while these newer technologies are gaining prominence, those from previous generations remain in widespread use, often in conjunction with the newer ones. For instance, CNNs complement transformers since CNNs effectively capture local features and semantic meaning, while the attention mechanism in transformers capture global dependencies.\n\nOne important class within AI that has become dominant since our previous review comprises Foundation models (FMs). These were described by The Stanford Institute for Human-Centered Artificial Intelligence in 2021 \\cite{Bommasani2021FoundationModels} as ``any model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks\". Foundation models have been enabled by rapid advances in AI-oriented computing power and have been underpinned the emergence and success of Large Language Models, particularly following the launch of ChatGPT by OpenAI in  2022. ChatGPT has become the fastest-growing consumer software application in history\\footnote{\\url{https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/}}.\n\nThese technologies are expanded on below.\n\n\n\n\\subsection{Transformers}\n\\label{ssec:transformers}\n\nIn 2017, Google AI introduced the concept of  `Transformer' architectures in their publication `Attention Is All You Need' \\cite{Vaswani:attention:2017}. This work has since, been instrumental in the development and success  of large language models alongside many other applications, including vision understanding \\cite{Dosovitskiy:image:2021}, and multiple modality learning (e.g., Gato \\cite{Reed:Generalist:2022}). \n\nBefore the advent of transformers, natural language processing (NLP) was performed using recurrent neural networks (RNNs), processing data sequences sequentially. In contrast, the ability of transformers to capture long-range dependencies through self-attention mechanisms that extend across all words in the sequence, meant that the importance of different words could be established globally,  understanding relationships regardless of their positions. This context-aware representation enables parallel processing of the entire sequence, making the transformers computationally efficient. A set of several attention layers running in parallel is called Multi-Head Attention. \n\nThe Transformer architecture, shown in Fig. \\ref{fig:generativemodel} (a), comprises Encoder and Decoder sections, similar to many CNN-based generators. However the encoder is now a stack of identical layers, concatenating a multi-head self-attention mechanism and a fully connected feed-forward network. The decoder is also a stack of identical layers, in which each layer has additional sub-layer to perform multi-head attention over the output of the encoder stack.\n\nMathematically, the attention function is computed from inputs: query $Q$Q, keys $K$K, and values $V$V. The matrix of outputs of  attention function is\n\\begin{equation}\n    \\text{Attention}(Q, K, V ) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) V,\n    \\label{eqn:attention}\n\\end{equation}\\begin{equation}\n    \\text{Attention}(Q, K, V ) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) V,\n    \\label{eqn:attention}\n\\end{equation}\n    \\text{Attention}(Q, K, V ) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) V,\n    \\label{eqn:attention}\n\n\\noindent where $d_k$d_k is a  dimension of $K$K. The term ${QK^T}${QK^T}QK^T is Dot-Product Attention, which yields a high similarity value when the two words are closely related.  If $Q$Q and $K$K are from the same sentence, Eq.~\\ref{eqn:attention} refers to self-attention, but if $Q$Q and $K$K are from different sentences, it is referred to as cross-attention. Within the network, multi-head attention is actually employed to concurrently process attention and enable the model to collectively focus on information from distinct representation subspaces at various positions through the learnable parameters $W$Ws.\n\\begin{equation}\n\\begin{split}\n        \\text{MultiHead}(Q, K, V ) &= \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O,\\\\\n        \\text{head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V ). \n\\end{split}\n\\label{eqn:MultiHead}\n\\end{equation}\\begin{equation}\n\\begin{split}\n        \\text{MultiHead}(Q, K, V ) &= \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O,\\\\\n        \\text{head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V ). \n\\end{split}\n\\label{eqn:MultiHead}\n\\end{equation}\n\n        \\text{MultiHead}(Q, K, V ) &= \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O,\\\\\n        \\text{head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V ). \n\n\\label{eqn:MultiHead}\n\nIt should be noted that attention modules are not solely used in transformers, but have also been successfully integrated into other deep learning architectures such as CNNs, used for image classification \\cite{Li:HAM:2022}, object detection \\cite{Woo:CBAM:2018}, and other computer vision tasks \\cite{Guo:Attention:2022}.\n\nIn 2020, the first successful training of a transformer encoder for image recognition was published \\cite{Dosovitskiy:image:2021}, reeferred to as a Vision Transformer (ViT). The ViT decomposes an input image into patches, similar to words in a sentence, and processes them through multi-head attention. Additionally, a Multilayer Perceptron (MLP) is employed as the feedforward network.  In later work Microsoft introduced  a hierarchical division of image inputs and a shifted window approach in their Swin Transformer \\cite{Liu:Swin:2021}. This was reported to outperform ViT by 2.4\\% in ImageNet-22K classification (21,841 different categories). Its version 2 \\cite{Liu:Swinv2:2022} applied a cosine function in the attention module. enabling the scaling of capacity and resolution. More detail on transformer-based object detection is discussed in Section \\ref{sssec:recog}. To date, Swin Transformers have been widely adopted in a range of applications including image restoration \\cite{Fan:SUNet:2022}. \n\n\n\nComprehensive surveys on the use of transformers for image and video processing can be found in \\cite{Khan:Transformers:2022} and \\cite{Selva:video:2023}, respectively.\n \nTransformers have been widely used and offer better performance across many tasks. One reason for this widespread adoption has been the availability of open-source Transformer libraries such as Hugging   Face\\footnote{\\url{https://huggingface.co/}}, a platform that assists developers to build applications for tasks including computer vision, NLP, audio, tabular data, multimodal tasks, and reinforcement learning. The platform also provides access to model zoo \\footnote{Such as \\url{https://modelzoo.co/} and \\url{https://pytorch.org/serve/model_zoo.html}} pretrained networks and datasets. \n\nIn recent years state space models \\cite{gu2023mamba, zhu2024vision}, commonly known as `Mamba' have emerged. These are  a linear variant of Transformersdistinguished by their linear complexity in attention modeling. They are acknowledged to offer an equivalent or better performance than traditional Transformers, while demanding fewer computational resources and less memory.\n\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/generativemodel.png}\n    \\caption{Generative AI. (a) Transformer architecture \\cite{Vaswani:attention:2017}. (b) The top row represents the diffusion process and the bottom row represents the generation process of the new image \\cite{Yang:diffusion:2023}. (c) Latent Diffusion Models (LDM) \\cite{Rombach:LDM:2022}. }\n    \\label{fig:generativemodel}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/generativemodel.png}\n    \\caption{Generative AI. (a) Transformer architecture \\cite{Vaswani:attention:2017}. (b) The top row represents the diffusion process and the bottom row represents the generation process of the new image \\cite{Yang:diffusion:2023}. (c) Latent Diffusion Models (LDM) \\cite{Rombach:LDM:2022}. }\n    \\label{fig:generativemodel}\n\n\n\\subsection{Large language models}\n\\label{ssec:llms}\n\nLLMs are based on transformer models using self-attention mechanisms as their core modules. Training LLMs comprises two steps: i) pre-training in an unsupervised learning manner, and ii) fine-tuning to a specific task or prompt-tuning for better user inputs. The models are first `pre-trained' with a large amount of unlabelled text data to learn the meaning of words, and the relationships between those words, before using it to adapt to a downstream task. This is why OpenAI refers to their model as a Generative Pre-trained Transformer (GPT). \n\nFine-tuning involves training the model on new datasets. The drawback is however that these data need to be large enough to ensure generalization to  new tasks. Prompt-tuning and prompt engineering are relatively new disciplines for developing and optimizing prompts to efficiently use language models. Prompts guide the way AI models interpret and respond to user queries. Prompt engineering is the process of structuring text or phrasing that guide the model towards generating the desired output. This relies heavily on trial and error, and an understanding of how the model responds. \nPrompt-tuning, on the other hand, involves training a small set of parameters before utilizing the LLM, thus requiring a relatively small amount of new data. This approach essentially converts text inputs into task-specific virtual inputs, referred to as tokens, while the pre-trained LLM remains unchanged \\cite{Lester:power:2021}.  The main drawback of prompt-tuning is lack of interpretability. This paradigm has however extended to other domains, such as visual prompt tuning \\cite{Jia:VPT:2022}. For a comprehensive survey of LLMs, please refer to \\cite{zhao:survey:2023}.\n\nTo date, there are many LLM platforms as shown in Fig. \\ref{fig:FLASK_LLM_and_history}. Fig. \\ref{fig:FLASK_LLM_and_history}(a) shows their timeline. Many surveys and evaluations of LLMs are also available \\cite{zhao:survey:2023,Chang:Survey:2024,YAO:Survey:2024}. These include FLASK (Fine-grained Language Model Evaluation based on Alignment SKill Sets) \\cite{Ye:FLASK:2024} which evaluates LLMs based on 12 fine-grained skills for comprehensive language model evaluation: logical correctness, logical robustness, logical efficiency, factuality, commonsense understanding, comprehension, insightfulness, completeness, metacognition, conciseness, readability, and harmlessness. Evaluation results from FLASK are shown in Fig. \\ref{fig:FLASK_LLM_and_history} (b).\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/FLASK_LLM_and_history.png}\n    \\caption{(a) Timeline of large language models. (b) Performance comparison evaluated by FLASK \\cite{Ye:FLASK:2024}.}\n    \\label{fig:FLASK_LLM_and_history}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/FLASK_LLM_and_history.png}\n    \\caption{(a) Timeline of large language models. (b) Performance comparison evaluated by FLASK \\cite{Ye:FLASK:2024}.}\n    \\label{fig:FLASK_LLM_and_history}\n\n\n\n\n\n\\subsection{Diffusion Models}\n\\label{ssec:DMs}\n\nA generative model, in the context of AI, exploits machine learning to learn a probability distribution of the training data to generate new data samples. The very first models were based on Autoencoders (AEs) that learn to encode input data into a lower-dimensional representation (latent space) and then decode it back to its original form. A specific type of AE, a variational autoencoders (VAE) \\cite{Kingma:auto:2014}, learns the latent space as statistical parameters of probabilistic distributions, leading to significant improvement of the generated results. Concurrently,  Goodfellow et al. \\cite{Goodfellow:GAN:2014} introduced an alternative architecture known as a Generative Adversarial Network (GAN). GANs comprise two competing AI modules: a generator, which creates a sample, and a discriminator, which determines whether the received sample is real or generated. When comparing VAEs to GANs, VAEs exhibit greater stability during training, whereas GANs excel at producing realistic images. More details about AEs and GANs for creative technologies can be found in our previous review \\cite{Anantrasirichai:AI:2022}.\n\nAn important factor in driving the rapid growth of generative AI has been  the development of diffusion probabilistic models (referred to as diffusion models (DMs)). The first DM was introduced in 2015 by Sohl-Dickstein et al. \\cite{Dickstein:Deep:2015}, using Nonequilibrium Thermodynamics. However, it took a further 5 years for DMs to generate desirable results: the era of DMs began with Denoising Diffusion Probabilistic Models (DDPMs) proposed by Ho et al. \\cite{Ho:DDPM:2020} in 2020 and Score-based diffusion models proposed by Song et al. \\cite{Song:Score:2021} in 2021. These involve a simplified process using a denoising autoencoder to approximate Bayesian inference. In brief, the models leverage a diffusion process to learn a probability distribution of the input data. As the name suggests, the data is diffused by gradually adding noise at each iteration step as shown in Fig. \\ref{fig:generativemodel} (b). A deep neural network (DNN) is then trained to remove this noise, called the denoising process or reverse process. Consequently, the trained model uses random noise to generate data with characteristics similar to those of the training samples. Comparing to GANs, DMs provide higher diversity samples \\cite{Dhariwal:Diffusion:2021} and a training process that is much more stable and does not suffer from mode collapse. DMs are however computationally intensive and require longer training times compared to GANs. The complexity can significantly reduced by training the DMs in latent space. Latent Diffusion Models (LDM) \\cite{Rombach:LDM:2022} use pretrained networks to convert images to feature maps, and perform training on a low-dimensional space. The diagram of LDM is shown in Fig. \\ref{fig:generativemodel} (c). \n\nGenerating a synthesized sample at random might not be particularly useful, especially for creative industry applications. Therefore, conditional diffusion models have been proposed, supporting a wide range of applications such as text-to-sound, text-to-images, and image-to-videos. For DMs, the conditional distributions are modelled using a conditional denoising autoencoder. Classifier guidance was introduced in \\cite{Dhariwal:Diffusion:2021} to improve the generation of images of a desired class. For example, when we provide the model with information, such as `a flower', the DM will synthesize a variety of flower images, as the word `flower' guides the model toward the latent distribution that is formed by various images of flowers. The work in \\cite{Choi:ILVR:2021} simply refines the latent space of well-trained unconditional DDPM so that the higher-level semantics of the synthetic samples are similar to the reference (conditioning).\nThe LDM \\cite{Rombach:LDM:2022} offers more flexible conditional image generators by adding cross-attention layer (referred to Transformers in Section \\ref{ssec:transformers}) to the denoising autoencoder. A survey on the methods and applications of DMs prior to 2024 can be found in \\cite{Cao:survey:2024}.\n\n\n\n\\subsection{Implicit Neural Representations}\n\nImplicit Neural Representations (INR), also called neural fields, neural implicits or coordinate-based neural networks, represent input content implicitly through learned functions $F$F, as shown in Eq.~\\ref{eqn:inr}. They can be considered as fields $x$x (represented by a scalar, vector, or a tensor with a value, such as magnetic field in physics) that are fully or partially parameterized by a neural network $\\Phi$\\Phi, typically an MLP \\cite{xie2022neural}.\n\\begin{equation}\nF(x, \\Phi, \\nabla_x \\Phi, \\nabla_x^2 \\Phi, \\dots) = 0, \\quad \\Phi : x \\mapsto \\Phi(x).\n\\label{eqn:inr}\n\\end{equation}\\begin{equation}\nF(x, \\Phi, \\nabla_x \\Phi, \\nabla_x^2 \\Phi, \\dots) = 0, \\quad \\Phi : x \\mapsto \\Phi(x).\n\\label{eqn:inr}\n\\end{equation}\nF(x, \\Phi, \\nabla_x \\Phi, \\nabla_x^2 \\Phi, \\dots) = 0, \\quad \\Phi : x \\mapsto \\Phi(x).\n\\label{eqn:inr}\n\n\n Although this concept appears complex,  the process is actually very straightforward. For example, in the case of an image, the coordinates of each pixel $(x, y)$(x, y) contain color information $(r, g, b)$(r, g, b). The INR inputs $(x, y)$(x, y) to the MLP and learns to provide the output $(r, g, b)$(r, g, b). The weights and biases of the MLP now represent such an image. Usually, the number of parameters of the MLP is smaller than the total number of pixels multiplied by 3, accounting for the 3 color channels. Hence, one of its emerging applications is in data compression \\cite{kwan2024hinerv}. Moreover, the INR can handle complex and high-dimensional data efficiently, attracting attention for visual computing applications such as  3D scene reconstruction.\n\nTraditional MLPs employ ReLU (rectified linear unit) for non-linear activation due to its simplicity. However, Sitzmann et al. \\cite{sitzmann:siren:2020} demonstrated that using periodic functions, such as sinusoids, are more suitable for representing complex natural signals, offering a better fit to the first- and second-order derivatives of the signals. However, this activation can cause ringing artifacts. Saragadam et al. instead proposed using complex Gabor wavelets \\cite{Saragadam:wire:2023}, which learn to represent high frequencies better and simultaneously are robust to noise.\n\nOne of the fastest-growing areas that exploits INRs is \\textbf{Neural Radiance Fields (NeRF)}, evidenced by 57 papers presented at CVPR, the largest annual conference in computer vision, in 2022 growing to 175 papers in 2023\\footnote{\\url{https://markboss.me/post/nerf_at_cvpr23/}}, before dropping to 71 in 2024, largely due to competition from 3D Gaussian Splatting\\footnote{\\url{https://github.com/Yubel426/NeRF-3DGS-at-CVPR-2024}}. First introduced in 2020 by Mildenhall et al. \\cite{Mildenhall:NeRF:2020}, NeRF is a form of neural rendering, a subset of generative AI, that generates novel views of a scene based on a partial set of 2D images. It achieves this by learning a mapping from 3D spatial coordinates and view directions $(x,y,z,\\theta,\\phi)$(x,y,z,\\theta,\\phi) to colors and density $(r,g,b,\\sigma)$(r,g,b,\\sigma). This implicit representation allows NeRF to handle complex scenes with varying geometry and appearance,  resulting in highly realistic renderings that include accurate lighting, shadows, and reflections. More detail can be found in Section \\ref{sssec:nerf}.\n\n\n\n\n\\begin{table}\n\\caption{Creative applications and corresponding AI-based methods mentioned in this paper}\n%\\tiny\n %\\hskip-5.0cm\n %\\begin{tabular}{p{1cm}p{1.4cm}|p{4cm}p{4cm}p{4cm}p{4cm}}\n \\resizebox{\\linewidth}{!}{\\begin{tabular}{lr|lll}\n \\\\\n \\toprule\n\\multicolumn{2}{c}{\\multirow{2}{*}{Application}} & \\multicolumn{3}{|c}{Technology} \\\\ \\cmidrule{3-5}\n& & Trans./Attn.$^1$ & Diffusion model$^2$  & INR\\\\\n\\midrule\n{\\bf Creation} & Text & \\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}  & \\\\\n& audio/music & \\cite{Alayrac:Flamingo:2022, Huang:GenerSpeech:2022} & \\cite{Li:Diffusion:2022, Yang:Diffsound:2023, Evans:stable:2025} & \\\\\n& Image &  \\cite{Alayrac:Flamingo:2022,esser:scaling:2024} & \\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024, Feng_Ma_2025, Liu_Ma_2025} & \\\\\n% ---------------------------------------------------------------\n& Animation/video & \\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024} & \\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024, wang2025lavie, wu2025customcrafter} &\\\\\n% ---------------------------------------------------------------\n & 3D/AR/VR & \\cite{yang:Holodeck:2024} & \\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024} & \\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear} \\\\\n% ---------------------------------------------------------------\n\\midrule\n{\\bf Information} & Text categorization & \\cite{sun:text:2023, shi:chatgraph:2023, Hou:promptboosting:2023, AI2025125952} \\\\\n% ---------------------------------------------------------------\n{\\bf Analysis} & Film analysis & \\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}  \\\\\n % ---------------------------------------------------------------\n & Content retrieval & \\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning} & \\cite{Jin:DiffusionRet:2023} \\\\\n % ---------------------------------------------------------------\n & Intelligent assistants & \\cite{King:Sasha:2024} \\\\\n % ---------------------------------------------------------------\n \\midrule\n{\\bf Content} & Enhancement  & \\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024} & \\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight} & \\cite{Yang:Implicit:2023} \\\\\n{\\bf  Enhancement  } & Style transfer & \\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR} & \\cite{Zhang:Inversion:2023,Chai:StableVideo:2023} & \\cite{Moon:generalizable:2023,Kim:Controllable:2024} \\\\\n% ---------------------------------------------------------------\n {\\bf  and Post} & Super-resolution & \\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024,wang2025seedvr} & \\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023, cao2025zero, wang2025seedvr} & \\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}\\\\\n% ---------------------------------------------------------------\n{\\bf Production} & {Restoration} & \\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025} & \\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023, cao2025zero, feng2025residual} & \\cite{Jiang:NeRT:2023} \\\\\n  % ---------------------------------------------------------------\n & Inpainting & \\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024} & \\cite{Moliner:solving:2023, Fei:Generative:2023} & \\\\\n % ---------------------------------------------------------------\n& Fusion & \\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023, LI2024102147} & \\cite{Zhao:DDFM:2023} & \\\\\n & Editing/VFX & \\cite{Shi:Motion-I2V:2024} & \\cite{Shi:Motion-I2V:2024,guo2024liveportrait} \\\\\n \\midrule\n % ---------------------------------------------------------------\n{\\bf Information} & Segmentation & \\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2,Zhang:DVISp:2025} & \\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024} & \\cite{Gong:Continuous:2023, Cen:Segment:2023} \\\\\n % ---------------------------------------------------------------\n{\\bf  Extraction} & Recognition &  \\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12} & \\cite{Li:Your:2023,Chen:DiffusionDet:2023, Zhang:DiffAD:2025, WU2025102965}\\\\\n % ---------------------------------------------------------------\n{\\bf  and}  & Tracking &  \\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring} & \\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024} & \\cite{Jung:AnyFlow:2023}\\\\\n % ---------------------------------------------------------------\n{\\bf  Understanding}  & 3D Reconstruction  & \\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024, LIU:DSEM:2025} & \\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024} & \\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}$^\\dag$  \\\\\n\\midrule\n\n% ---------------------------------------------------------------\n{\\bf Compression} & Image$^\\ast$ & \\cite{zhu2022transformer,zou2022devil,liu2023learned}& \\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual} & \\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}\\\\\n& Video& \\cite{xiang2022mimt,mentzer2022vct}& \\cite{li2024extreme}&\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}\\\\\n&Audio$^\\ast$ & & &\\\\\n\\midrule\n% ---------------------------------------------------------------\n{\\bf Quailty}  &  Image$^\\ast$ & \\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}& &\\\\\n\\textbf{Assessment}& Video$^\\ast$ & \\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}& & \\\\\n\\bottomrule\n\\multicolumn{5}{l}{$^1$ Trans./Attn. include transformers, mamba and CNN-based architectures that use attention module.} \\\\\n\\multicolumn{5}{l}{$^2$ Some diffusion models employ the transformer in their denoising autoencoders.}\n \\end{tabular}}\n \\footnotesize\n $^\\dag$ \nThese methods are based on explicit neural representations. \\\\\n$^*$ It is noted that for some compression and quality assessment tasks, there are other dominant network architectures in existing works. For example, LLMs have been used for image and audio compression, and visual quality assessment. Many neural audio codecs are also based on VQ-VAE models.\n \n\\label{tab:gather}\n\\end{table}\n\\caption{Creative applications and corresponding AI-based methods mentioned in this paper}\n\\resizebox{\\linewidth}\\linewidth{!}!{\\begin{tabular}{lr|lll}\n \\\\\n \\toprule\n\\multicolumn{2}{c}{\\multirow{2}{*}{Application}} & \\multicolumn{3}{|c}{Technology} \\\\ \\cmidrule{3-5}\n& & Trans./Attn.$^1$ & Diffusion model$^2$  & INR\\\\\n\\midrule\n{\\bf Creation} & Text & \\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}  & \\\\\n& audio/music & \\cite{Alayrac:Flamingo:2022, Huang:GenerSpeech:2022} & \\cite{Li:Diffusion:2022, Yang:Diffsound:2023, Evans:stable:2025} & \\\\\n& Image &  \\cite{Alayrac:Flamingo:2022,esser:scaling:2024} & \\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024, Feng_Ma_2025, Liu_Ma_2025} & \\\\\n% ---------------------------------------------------------------\n& Animation/video & \\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024} & \\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024, wang2025lavie, wu2025customcrafter} &\\\\\n% ---------------------------------------------------------------\n & 3D/AR/VR & \\cite{yang:Holodeck:2024} & \\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024} & \\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear} \\\\\n% ---------------------------------------------------------------\n\\midrule\n{\\bf Information} & Text categorization & \\cite{sun:text:2023, shi:chatgraph:2023, Hou:promptboosting:2023, AI2025125952} \\\\\n% ---------------------------------------------------------------\n{\\bf Analysis} & Film analysis & \\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}  \\\\\n % ---------------------------------------------------------------\n & Content retrieval & \\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning} & \\cite{Jin:DiffusionRet:2023} \\\\\n % ---------------------------------------------------------------\n & Intelligent assistants & \\cite{King:Sasha:2024} \\\\\n % ---------------------------------------------------------------\n \\midrule\n{\\bf Content} & Enhancement  & \\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024} & \\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight} & \\cite{Yang:Implicit:2023} \\\\\n{\\bf  Enhancement  } & Style transfer & \\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR} & \\cite{Zhang:Inversion:2023,Chai:StableVideo:2023} & \\cite{Moon:generalizable:2023,Kim:Controllable:2024} \\\\\n% ---------------------------------------------------------------\n {\\bf  and Post} & Super-resolution & \\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024,wang2025seedvr} & \\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023, cao2025zero, wang2025seedvr} & \\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}\\\\\n% ---------------------------------------------------------------\n{\\bf Production} & {Restoration} & \\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025} & \\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023, cao2025zero, feng2025residual} & \\cite{Jiang:NeRT:2023} \\\\\n  % ---------------------------------------------------------------\n & Inpainting & \\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024} & \\cite{Moliner:solving:2023, Fei:Generative:2023} & \\\\\n % ---------------------------------------------------------------\n& Fusion & \\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023, LI2024102147} & \\cite{Zhao:DDFM:2023} & \\\\\n & Editing/VFX & \\cite{Shi:Motion-I2V:2024} & \\cite{Shi:Motion-I2V:2024,guo2024liveportrait} \\\\\n \\midrule\n % ---------------------------------------------------------------\n{\\bf Information} & Segmentation & \\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2,Zhang:DVISp:2025} & \\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024} & \\cite{Gong:Continuous:2023, Cen:Segment:2023} \\\\\n % ---------------------------------------------------------------\n{\\bf  Extraction} & Recognition &  \\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12} & \\cite{Li:Your:2023,Chen:DiffusionDet:2023, Zhang:DiffAD:2025, WU2025102965}\\\\\n % ---------------------------------------------------------------\n{\\bf  and}  & Tracking &  \\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring} & \\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024} & \\cite{Jung:AnyFlow:2023}\\\\\n % ---------------------------------------------------------------\n{\\bf  Understanding}  & 3D Reconstruction  & \\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024, LIU:DSEM:2025} & \\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024} & \\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}$^\\dag$  \\\\\n\\midrule\n\n% ---------------------------------------------------------------\n{\\bf Compression} & Image$^\\ast$ & \\cite{zhu2022transformer,zou2022devil,liu2023learned}& \\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual} & \\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}\\\\\n& Video& \\cite{xiang2022mimt,mentzer2022vct}& \\cite{li2024extreme}&\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}\\\\\n&Audio$^\\ast$ & & &\\\\\n\\midrule\n% ---------------------------------------------------------------\n{\\bf Quailty}  &  Image$^\\ast$ & \\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}& &\\\\\n\\textbf{Assessment}& Video$^\\ast$ & \\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}& & \\\\\n\\bottomrule\n\\multicolumn{5}{l}{$^1$ Trans./Attn. include transformers, mamba and CNN-based architectures that use attention module.} \\\\\n\\multicolumn{5}{l}{$^2$ Some diffusion models employ the transformer in their denoising autoencoders.}\n \\end{tabular}}\n \\\\\n \\toprule\n\\multicolumn{2}2{c}c{\\multirow{2}{*}{Application}}\\multirow{2}2{*}*{Application}Application & \\multicolumn{3}3{|c}|c{Technology}Technology \\\\ \\cmidrule{3-5}3-5\n& & Trans./Attn.$^1$^1 & Diffusion model$^2$^2  & INR\\\\\n\\midrule\n{\\bf Creation}\\bf Creation & Text & \\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}  & \\\\\n& audio/music & \\cite{Alayrac:Flamingo:2022, Huang:GenerSpeech:2022} & \\cite{Li:Diffusion:2022, Yang:Diffsound:2023, Evans:stable:2025} & \\\\\n& Image &  \\cite{Alayrac:Flamingo:2022,esser:scaling:2024} & \\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024, Feng_Ma_2025, Liu_Ma_2025} & \\\\\n& Animation/video & \\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024} & \\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024, wang2025lavie, wu2025customcrafter} &\\\\\n& 3D/AR/VR & \\cite{yang:Holodeck:2024} & \\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024} & \\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear} \\\\\n\\midrule\n{\\bf Information}\\bf Information & Text categorization & \\cite{sun:text:2023, shi:chatgraph:2023, Hou:promptboosting:2023, AI2025125952} \\\\\n{\\bf Analysis}\\bf Analysis & Film analysis & \\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}  \\\\\n & Content retrieval & \\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning} & \\cite{Jin:DiffusionRet:2023} \\\\\n & Intelligent assistants & \\cite{King:Sasha:2024} \\\\\n \\midrule\n{\\bf Content}\\bf Content & Enhancement  & \\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024} & \\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight} & \\cite{Yang:Implicit:2023} \\\\\n{\\bf  Enhancement  }\\bf  Enhancement   & Style transfer & \\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR} & \\cite{Zhang:Inversion:2023,Chai:StableVideo:2023} & \\cite{Moon:generalizable:2023,Kim:Controllable:2024} \\\\\n{\\bf  and Post}\\bf  and Post & Super-resolution & \\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024,wang2025seedvr} & \\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023, cao2025zero, wang2025seedvr} & \\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}\\\\\n{\\bf Production}\\bf Production & {Restoration}Restoration & \\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb, fang2025guided, Yue:RViDeformer:2025, Jin:Masked:2025,  Yue:RViDeformer:2025, Shi:VmambaIR:2025} & \\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023, cao2025zero, feng2025residual} & \\cite{Jiang:NeRT:2023} \\\\\n  & Inpainting & \\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024} & \\cite{Moliner:solving:2023, Fei:Generative:2023} & \\\\\n & Fusion & \\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023, LI2024102147} & \\cite{Zhao:DDFM:2023} & \\\\\n & Editing/VFX & \\cite{Shi:Motion-I2V:2024} & \\cite{Shi:Motion-I2V:2024,guo2024liveportrait} \\\\\n \\midrule\n {\\bf Information}\\bf Information & Segmentation & \\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2,Zhang:DVISp:2025} & \\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024} & \\cite{Gong:Continuous:2023, Cen:Segment:2023} \\\\\n {\\bf  Extraction}\\bf  Extraction & Recognition &  \\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024, im2025gate3d, tian2025yolov12} & \\cite{Li:Your:2023,Chen:DiffusionDet:2023, Zhang:DiffAD:2025, WU2025102965}\\\\\n {\\bf  and}\\bf  and  & Tracking &  \\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring} & \\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024} & \\cite{Jung:AnyFlow:2023}\\\\\n {\\bf  Understanding}\\bf  Understanding  & 3D Reconstruction  & \\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024, LIU:DSEM:2025} & \\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024} & \\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, 9879447, Liu_2024_CVPR, azzarelli:waveplanes:2023, zhan2024kfd, Tang_2024_CVPR, LIU:DSEM:2025},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025, junkawitsch2025eva, kong2025efficient}$^\\dag$^\\dag  \\\\\n\\midrule\n\n{\\bf Compression}\\bf Compression & Image$^\\ast$^\\ast & \\cite{zhu2022transformer,zou2022devil,liu2023learned}& \\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual} & \\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}\\\\\n& Video& \\cite{xiang2022mimt,mentzer2022vct}& \\cite{li2024extreme}&\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}\\\\\n&Audio$^\\ast$^\\ast & & &\\\\\n\\midrule\n{\\bf Quailty}\\bf Quailty  &  Image$^\\ast$^\\ast & \\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}& &\\\\\n\\textbf{Assessment}& Video$^\\ast$^\\ast & \\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}& & \\\\\n\\bottomrule\n\\multicolumn{5}5{l}l{$^1$ Trans./Attn. include transformers, mamba and CNN-based architectures that use attention module.}$^1$^1 Trans./Attn. include transformers, mamba and CNN-based architectures that use attention module. \\\\\n\\multicolumn{5}5{l}l{$^2$ Some diffusion models employ the transformer in their denoising autoencoders.}$^2$^2 Some diffusion models employ the transformer in their denoising autoencoders.\n \n \\footnotesize\n $^\\dag$^\\dag \nThese methods are based on explicit neural representations. \\\\\n$^*$^* It is noted that for some compression and quality assessment tasks, there are other dominant network architectures in existing works. For example, LLMs have been used for image and audio compression, and visual quality assessment. Many neural audio codecs are also based on VQ-VAE models.\n \n\\label{tab:gather}\n\n ", "appendix": false}, "Advanced AI for the creative industries": {"content": "\n\\label{sec:existing}\n\nSimilarly to our previous (2021) review of AI for the creative industries~\\cite{Anantrasirichai:AI:2022}, Table \\ref{tab:gather} categorizes applications and corresponding AI-based solutions. These areas are explored in more detail below.\n\n\\subsection{Content creation}\n\nContent creation is a fundamental activity of artists and designers and the term `\\textit{AI art}' refers to artforms created with the assistance of an AI algorithms or entirely by an AI system. This can refer to various digital forms including images, texts, audio, and videos. The roots of AI art can be traced back to the 20th century, exemplified by AARON, a computer program initiated in 1972 to autonomously produce paintings and drawings \\cite{encyclopedia_ai_v1}. The practicality of AI art has been enhanced  with advancements in deep learning, particularly GANs from 2014 and, more recently, transformers, DMs and INRs. \n\n\\subsubsection{Text generation, script and journalism}\n\nIn the era of LLMs, AI writing tools have been widely used to assist various writing tasks, including generation written articles, blog posts, essays, and reports. These tools go beyond mere grammar and spelling checks; they boast advancements enabling them to analyze the style and tone of written material, adding images, videos and tables, offering suggestions to enhance clarity, coherence, and overall readability \\cite{ippolito:creative:2022}. Moreover, AI tools extend their utility beyond content generation by automating tasks like keyword generation, meta tags, and descriptions, thereby increasing search rankings using search engine optimization (SEO). Additionally, they support the process of publishing across multiple online platforms. Transformers have been used to generate image captions by combining information from the images with a word prefix or questions \\cite{Wang:SIMVLM:2022}.\n\nAI script generators serve as beneficial aids for writers, filmmakers, and game developers, offering inspiration, idea generation, and assistance in crafting entire scripts \\cite{Jeary2024, Azzarelli:Reviewing:2024}. Human-AI brainstorming is helpful and saves time \\cite{guo:exploring:2024}. Presently, there are numerous software and websites providing both free and paid script generation services. However, many of these tools are still constrained when it comes to longform creative writing. Dramatron, developed by Google \\cite{Mirowski:cowriting:2023}, introduces hierarchical language generation, enabling the creation of cohesive scripts and screenplays spanning long ranges. This includes elements such as titles, characters, story beats, location descriptions, and dialogue.\n\nAs discussed earlier, chatbots are now powered by LLMs, effectively simulating human conversation. These fundamental LLMs are specialized for specific tasks. For instance journalist AI and blog AI writers\\footnote{For example, see \\url{https://tryjournalist.com/}} generate content with layouts suitable for print or online publication. Additionally,  AI tools exist that are designed to detect AI-generated content (e.g., for checking for copyright), AI-writing styles, content originality and to ensure the naturalness and flow of articles. Undoubtedly, generative AI is reshaping the way artists and journalists operate. For an in-depth exploration of the impact and implications of these technological advancements on news organizations, refer to the survey conducted by Beckett et al. \\cite{Beckett:Generating:2023}.\n\nGenerating text and scripts automatically can also be done through image and video inputs without text prompts (e.g., image captioning \\cite{Stefanini:From:2023}) and with text prompts. These approaches are referred to as Vision Language Models (VLMs):  multimodal models that learn from images and text. The most common and prominent models often consist of an image encoder, an embedding projector to align image and text representation, often via a dense neural network, and a text decoder stacked in this order. The most well-known technique is Contrastive Language-Image Pre-training (CLIP) \\cite{radford2021learning}. More recent work in \\cite{wei2024vary} scales up the vision vocabulary by incorporating new image features into the existing CLIP model, resulting in improved content understanding. A comprehensive survey of VLMs for vision tasks can be found in \\cite{Zhang:vision:2024}.\n\n\\subsubsection{Audio and music generation}\n\\label{sssec:musicgen}\n\nSimilar to language models, AI-based music generation has rapidly advanced due to unsupervised learning on large datasets and the use of transformers (see Section \\ref{ssec:llms}). Examples of such systems include MuseNet\\footnote{\\url{https://openai.com/research/musenet}}, Magenta Studio\\footnote{\\url{https://magenta.tensorflow.org/studio}}, and Musicfy\\footnote{\\url{https://musicfy.lol/}}. These tools assist in music composition by learning complex musical patterns, predicting the next word or music note in a sequence, and mixing specified instruments. Moreover, AI tools can convert one type of sound into another, such as from whistling to violin or from flute to saxophone\\footnote{See an example by Ummet Ozcan at \\url{https://www.youtube.com/watch?v=lI1LCfTx2lI}}. This capability is invaluable for artists who may not be proficient in playing all the instruments they wish to incorporate, saving both time and costs. In  2024, Suno has released a model capable of producing radio-quality music that can be created in 2 minutes\\footnote{\\url{https://www.suno.ai/blog/v3}}. Later, Udio \\footnote{\\url{https://www.udio.com/}}, was launched. This offers a prompt to create lyrics and music with a maximum duration of 90 seconds, and also appears to have, at least some, awareness of copyright.\n\nAI voice software changes vocalizations from one person to another, for example enabling users to train the model to convert other people's voices into their own, e.g. lalals\\footnote{\\url{https://lalals.com/}}, Kits\\footnote{\\url{https://www.kits.ai/}}, Media.io\\footnote{\\url{https://www.media.io/online-voice-changer.html}}, etc. Certain software, such as Voice.ai\\footnote{\\url{https://voice.ai/}}, even offers real-time voice changing capabilities. The technologies behind this uses a transformer to learn voice features and patterns in mel-spectrogram form. For example, the framework proposed in \\cite{Yang:Diffsound:2023} uses a DM-based method with a transformer backbone to turn text input into a mel-spectrogram using the vector quantized variational autoencoder (VQ-VAE) \\cite{Oord:Neural:2017}. Next, this mel-spectrogram is transformed into a sound wave.  Unlike a regular spectrogram, the mel-spectrogram is based on the mel-frequency scale, which offers higher resolution for lower frequencies. Voice style transfer often uses zero-shot learning (a model is trained to recognize classes or categories that it has never encountered during training) \\cite{Huang:GenerSpeech:2022} or few-shot learning (a model trained with only one or a few examples per class) \\cite{Wang:One:2022}. Stable Audio Open \\cite{Evans:stable:2025} introduces a text-conditioned generative model for non-speech audio, trained on Creative Commons licensed data, capable of producing state-of-the-art 44.1kHz stereo audio.\n\nAnother emerging AI technology application is in the field of spatial audio. In 2022, Apple Music revealed that, in just over a year, more than 80\\% of its worldwide subscribers were enjoying the spatial audio experience, with monthly plays in spatial audio increasing by over 1,000\\%\\footnote{\\url{https://www.apple.com/uk/newsroom/2023/01/apple-celebrates-a-groundbreaking-year-in-entertainment/}}. With head tracking, this technology significantly enhances the immersive experience. Masterchannel has launched SpatialAI\\footnote{\\url{https://platform.masterchannel.ai/spatial}}, claiming it to be the world's first spatial mastering AI. This processes audio files and returns an optimized track for streaming platforms, along with an individually optimized stereo version for traditional distribution. All these advancements leverage transformer-based technologies.\n\n\\subsubsection{Image generation}\n\nAs described in Section \\ref{ssec:DMs}, recent advances in AI technologies for image generation are based on Diffusion Models (DMs). Well-known and highly competitive text-to-image models include Stable Diffusion\\footnote{\\url{https://stability.ai/stable-image}}, Midjourney\\footnote{\\url{https://www.midjourney.com/home}}, DALL\u00b7E\\footnote{\\url{https://openai.com/dall-e-3}}, and Ideogram\\footnote{\\url{https://ideogram.ai/}}. Released in March 2024, the latest version of Stable Diffusion (SD3), has been reported to outperform state-of-the-art text-to-image generation systems such as DALL\u00b7E 3 (released August 2023) \\cite{esser:scaling:2024}, Midjourney v6 (released December 2023), and Ideogram v1 (released February 2024) in terms of typography and prompt adherence, based on human preference evaluations. These open-source tools are built on a Multimodal Diffusion Transformer (MM-DiT) architecture, which integrates attention from both text and images. LLM4GEN \\cite{Liu_Ma_2025} fuses features from LLM and CLIP models to enhance the semantic understanding in text-to-image diffusion models, enabling them to better handle complex and dense prompts involving multiple objects. Examples of text-to-image generation are shown in Fig. \\ref{fig:LLMGround} (a) comparing the performance of four models, i.e. Ideogram v1, DALL\u00b7E 3, Photoshop 2025, and sdxy-turbo by Nvidia. It is clear that hands are one of the most difficult features to generate, e.g., one hand has six fingers.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/LLMGround.jpg}\n    \\caption{Text-to-image generation (generated on 27 November 2024). (a) text-to-image generation by Ideogram v1, DALL\u00b7E 3, Photoshop 2025, and sdxy-turbo by Nvidia. (b) The top-row images were generated by DALL\u00b7E in ChatGPT 4. The bottom-row images are generated by LLM-grounded Diffusion \\cite{Lian:LLMG:2024}. }\n    \\label{fig:LLMGround}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/LLMGround.jpg}\n    \\caption{Text-to-image generation (generated on 27 November 2024). (a) text-to-image generation by Ideogram v1, DALL\u00b7E 3, Photoshop 2025, and sdxy-turbo by Nvidia. (b) The top-row images were generated by DALL\u00b7E in ChatGPT 4. The bottom-row images are generated by LLM-grounded Diffusion \\cite{Lian:LLMG:2024}. }\n    \\label{fig:LLMGround}\n\n\n\nDALL\u00b7E 3, available on ChatGPT 4, also provides an inpainting tool, allowing the user to manually select the area to edit. However, as of April 2024, its performance is still limited. As illustrated in Fig. \\ref{fig:LLMGround} (b), the selected area is the white car, and with the follow-up request to change the white car to the red car, DALL\u00b7E 3 generates correctly. However, if asked to replace it with a bicycle, it does not work. LLM-grounded Diffusion \\cite{Lian:LLMG:2024} was the first to introduce a framework that allows multiple rounds of user requests without the need for manual selection on the image. This is achieved by generating layout-grounded images, first using stable diffusion and then masking the latent variables as priors for the next round of generation\\footnote{Images in Fig. \\ref{fig:LLMGround} (b) were generated using their demo: https://huggingface.co/spaces/longlian/llm-grounded-diffusion.}. Since then, text-driven image editing has seen significant improvements in quality, with most recent approaches adopting Diffusion Transformer architectures \\cite{Feng_Ma_2025, Huang:Diff:2025}. \n\nSimilar to DALL\u00b7E 3, Photoshop features a Generative Fill tool\\footnote{\\url{https://www.adobe.com/th_en/products/photoshop/generative-fill.html}} designed to generate new images or assist with photo editing. It accepts a text prompt and provides several generation choices. After defining the editing area, users can remove and add new objects (more inpainting tasks are discussed in Section \\ref{ssec:inpaiting}), transfer to new styles, and expand content within images. Recently, Brooks et al. introduced InstructPix2Pix \\cite{Brooks:InstructPix2Pix:2023},  a conditional diffusion model that generates image editing examples without predefined editing areas. By combining GPT-3 and Stable Diffusion, the model effectively captures and matches the semantic meaning of the content in both text and image. Sometimes, style and context are not easy to describe in words. Textual Inversion \\cite{gal:Image:2023} personalizes large pre-trained text-to-image diffusion models based on specific objects and styles, using 3-5 images of a user-provided concept. ByteDance announced Hyper-SD \\cite{ren:hypersd:2024} which proposed trajectory segmented consistency distillation and provides real-time high-resolution image generation from drawing with a control text prompt. \n\n\n\n\\subsubsection{Video generation and animation} \n\\label{sssec:videogen}\n\nDespite the success of text-to-image generation, text-to-video generation has not advanced at the same pace, only starting to grow more rapidly in 2024,  largely due to its computational expense and content complexity. Several major companies and private platforms have however now released offerings, including Gemini 1.5 by Google, Make-A-Video by Meta, and Sora by OpenAI. Make-A-Video \\cite{singer:Make:2023}, through a spatiotemporally factorized diffusion model, leverages joint text-image priors and super-resolution in space and time. Some results however contain  flickering artifacts\\footnote{\\url{https://makeavideo.studio/}}. Gen-2 by Runway\\footnote{\\url{https://research.runwayml.com/gen2}} offers both text- and image-to-video and can generate a smooth 4-sec video. In April 2024, Adobe Premier Pro announced their integration of generative AI tools for video extension with third-party models by OpenAI, Runway and Pika Labs\\footnote{\\url{https://www.adobe.com/products/premiere/ai-video-editing.html}}. This new update also includes a contextual-selection tool, inpainting for object removal, and object addition to the defined areas in the videos with a text prompt. \n\nText-to-video technologies, combined with AI voice, have been tested not only by artists or producers but also by a wider audience. Results from these tests, such as automatically turning scripts into movie trailers and music videos, have been widely shared on public online platforms\\footnote{\\url{https://twitter.com/minchoi/status/1775907105813217398}}. However, scene composition and transitions still require further editing to align with producers' needs\\footnote{See an example by Curious Refuge at \\url{https://www.youtube.com/watch?v=fJQbP34GoHQ}}.\nIn April 2024, Microsoft introduced VASA-1 \\cite{xu:VASA-1:2024}, which turns a single static image and a speech audio clip into a video clip of realistic talking faces mimicking human facial expressions and head movements, as shown in Fig. \\ref{fig:Deepmotion_Vasa} (right). The overall quality of the generated videos is better than VLOGGER by Google \\cite{corona:vlogger:2024}, which is based on similar technology -- diffusion models. However, VLOGGER also offers movement of the upper body and hand gestures. Recently, ByteDance introduced an audio-driven interactive head generation \\cite{Zhu:INFP:2024} that offers listening and speaking states during multi-turn conversations. This framework is based on a conditional diffusion transformer\n.\nThe main technologies underpinning text-to-video and image-to-video tasks are based on DMs with a combination of 3D convolutions (or separately spatial and temporal convolutions), and spatial and temporal attention modules \\cite{wang:modelscope:2023}. Tune-A-Video \\cite{wu:tune:2023} modifies the style of an input video using a text prompt. The method leverages pretrained text-to-image models and introduces attention tuning to ensure temporal consistency. Early video generation methods often exhibit flickering, as observed in the CVPR2023 competition on text-guided video editing, where all results suffered from temporal inconsistency. Dreamix \\cite{molad:dreamix:2023} videos do not have this issue, but they are very blurry. As an example of a transformer-based approach, CogVideo \\cite{hong:cogvideo:2023} employs VQ-VAE to convert input frames to tokens, which are then fused with text tokens to produce a new video. Phenaki \\cite{villegas:phenaki:2023} exploits transformers to generate variable length videos, but the quality is lower than those based on DMs. Evaluations of these methods can be found in \\cite{Liu:FETV:2023}. More recent work has applied spatiotemporal layers to model temporal dynamics \\cite{Gupta:Photorealistic:2024}. The transformer blocks have been redesigned for latent video diffusion modeling with window-restricted spatial and spatiotemporal attention. LaVie \\cite{wang2025lavie} demonstrates that simple temporal self-attention mechanisms, when combined with rotary positional encoding, are sufficient to capture the temporal correlations inherent in video data. The image-to-video generation process is analogous to text-to-video methods, but it conditions diffusion models on images rather than text. Some approaches further enhance generation by incorporating both textual descriptions (to guide motion) and images (to define objects and scenes) as inputs \\cite{wu2025customcrafter}.\nMany more free and commercial tools for video generation are now emerging. These include Veo 3 by Google DeepMind\\footnote{https://deepmind.google/models/veo/}, Kling AI\\footnote{https://www.klingai.com/}, Pika 2.2\\footnote{https://pikartai.com/pika-2-2/}, Hailuo AI\\footnote{https://hailuoai.video/}, etc. Though not perfect, the generated videos are close to reality (visit their websites for showcase examples).\n\nGenerating characters with human posture and motion from text prompts has also become popular. Make-An-Animation \\cite{Azadi:Make:2023} trains on image-text datasets and fine-tunes on motion capture data, adding additional layers to model the temporal dimension. Animate Anyone by Alibaba Group \\cite{Hu_2024_CVPR} inputs a real photo or anime of a person with a sequence of guided poses. The results are significantly better than existing techniques, including Disco \\cite{wang:disco:2024} and Bidirectionally Deformable Motion Modulation (BDMM) \\cite{Yu:Bidirectionally:2023}. They also suggest using Animate Anyone  with Outfit Anyone\\footnote{\\url{https://humanaigc.github.io/outfit-anyone/}} to produce a character with a reference outfit.\n\nViggle\\footnote{\\url{https://viggle.ai/}} claims to be the first video-3D foundation model embodying an actual understanding of physics. It combines a character and a text prompt about motion to generate character animation. Available AI tools for 3D on the market include DeepMotion\\footnote{\\url{https://www.deepmotion.com/}} that offers text-to-3D post animation and video-to-3D post animation, shown in Fig. \\ref{fig:Deepmotion_Vasa} (left). The later function can track multiple people from real video and generates replicated characters with the same motions.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/Deepmotion_Vasa.png}\n    \\caption{(Left) Video-to-3D post animation by DeepMotion. (Right) Image and audio to video by VASA-1 \\cite{xu:VASA-1:2024}}\n    \\label{fig:Deepmotion_Vasa}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/Deepmotion_Vasa.png}\n    \\caption{(Left) Video-to-3D post animation by DeepMotion. (Right) Image and audio to video by VASA-1 \\cite{xu:VASA-1:2024}}\n    \\label{fig:Deepmotion_Vasa}\n\n\n\\subsubsection{Augmented, virtual and mixed reality, and 3D content}\n\nWhile the benefits of LLMs in Augmented Reality (AR) directly target educational purposes, enhance cognitive support, and facilitate communication \\cite{XU2025103402}, mixed reality (MR) has once again become exciting since the release of the Apple Vision Pro in February 2024. This demonstrated the potential of MR experiences by merging real-world environments with computer-generated ones. Thanks to the rapid growth of AI-based 3D representation (see Section \\ref{ssec:3Dreconstruct}), the generation of AR/VR/MR content has advanced significantly. Real-time rendering with immersive interaction has improved, and real scenes can now be generated avoiding uncanny valley effects. There has also been an attempt to use autoregressive and generative models to estimate lighting, achieving a visually coherent environment between virtual and physical spaces in AR \\cite{zhao2024clear}.\n\nSimilar to other content generation tools, LLMs have been influenced on immersive technologies, including text-to-3D and image-to-3D. Exciting examples include\nHolodeck \\cite{yang:Holodeck:2024}, which automatically generates 3D embodied environments via text-prompt interactions with a large language model (GPT-4). 3D objects are gathered from Objaverse \\cite{deitke:Objaverse:2023}, a dataset with 800K+ annotated 3D objects. RealFusion \\cite{Melas:RealFusion:2023}, a single image to 3D object generator, merges 2D diffusion models with NeRF, improving Instant-NGP \\cite{mueller:instant:2022}, which provides an API for VR controls. NeuralLift-360 \\cite{Xu:NeuralLift:2023} also uses diffusion models to generate priors for novel view synthesis. Magic123 \\cite{Qian:Magic123:2024} is the latest image-to-3D tool that uses  2D and 3D priors simultaneously  to produce high-quality high-resolution 3D geometry and textures. DreamGaussian \\cite{tang:dreamgaussian:2024} offers text-to-3D and image-to-3D by adapting 3D Gaussian splatting (more in Section \\ref{sssec:3DGS}) into generative settings using a diffusion prior. This generates photo-realistic 3D assets with explicit mesh and texture maps within only 2 minutes. DreamGaussian4D \\cite{ren:dreamgaussian4d:2023} employs image-to-video diffusion and a 4D Gaussian Splatting representation to generate an image-to-4D model. The results are not very sharp, but they can be further edited with Blender. \n\nIn July 2024, Shutterstock launched its Generative 3D service in commercial beta, powered by NVIDIA Edify, a multimodal generative AI architecture. This service enables creators to rapidly prototype 3D assets and generate 360-degree HDRi backgrounds to light scenes using text or image prompts. In conjunction with OpenUSD, the created scenes can be rendered into 2D images and used as input for AI-powered image generators, allowing for the production of precise, brand-accurate visuals.\n\n\n\n\n\n\n\n\\subsection{Information analysis}\n\n\\subsubsection{Text categorization}\n\nApplications of text categorization include detecting spam emails, automating customer support, monitoring social media for harmful content, etc. At its core, text categorization involves assigning predefined labels to text documents, which can be anything from a tweet to a lengthy article. LLMs are particularly well-suited for this task due to their ability to comprehend complex and nuanced language. One of the main advantages of using LLMs in text categorization is their transfer learning capability. Models can be pre-trained on a large amount of text and then fine-tuned on a smaller, task-specific dataset, with or without further post-processing technique. For example, CARP \\cite{sun:text:2023} applies kNN to integrate diagnostic reasoning process for final decision. ChatGraph, proposed by Shi et al. \\cite{shi:chatgraph:2023}, utilizes ChatGPT to refine text documents. It uses a knowledge graph, extracted using another specific defined prompt, and finally, a linear model is trained on the text graph for classification. Multiple learners are also used to enhance the performances \\cite{Hou:promptboosting:2023,AI2025125952}.\n\n\\subsubsection{Advertisements and film analysis}\n\nNot only does AI assist in generating ideas and content, but it can also aid creators in effectively matching content to their audiences, particularly on an individual level \\cite{feizi:Online:2023}. This effectively helps in advertising personalization\u2014eMarketer\\footnote{\\url{https://www.emarketer.com/content/spotlight-marketing-personalization}} reported that nearly nine out of ten consumers are comfortable with their browsing history being utilized to create personalized ads. In contrast to outdated syntax-style searches, advanced LLM tools can comprehensively grasp user intent behind each search through conversation prompts, providing advertisers with a high level of granularity.\n\nCurrent advances in generative AI would greatly benefit sentiment analysis, also known as opinion mining, where opinions are gather from social media, articles, customer feedback, and corporate communication and are analysed to understand emotion of the owners. This is a potential tool for filmmakers and studios, enabling the creation of effective and targeted marketing campaigns. By analyzing viewer emotions and opinions, AI can provide valuable insights into audience preferences, aiding in the optimization of film marketing strategies. Sentiment analysis with modern generative AI produce more accurate results. Technically, LLMs learn complex patterns and relationships in text data for sentiment classification \\cite{Mao:Biases:2023, krugmann:sentiment:2024}. SiEBERT \\cite{Hartmann:More:2023} provides pre-trained model with open-source scripts to be fine-tuned to further improve accuracy for novel applications. Cinema Multiverse Lounge \\cite{Ryu:Cinema:2025}, a multi-agent conversational system, allows users to interact with LLM-driven agents, each embodying a distinct film-related target users.\n\n\\subsubsection{Content retrieval and recommendation services}\n\nGenerative retrieval (GR) was pioneered by Metzler et al. \\cite{Metzler:Rethinking:2021}. Unlike traditional retrieval, which adheres to the ``index-retrieve-then-rank\" paradigm, the GR paradigm employs a single model to obtain results from query input. The model generally involve deep-learning based transformers, generating output token-by-token. More recent work in \\cite{li2024learning} introduces learning-to-rank training to enhance the performance system up to 30\\%.\nGR has several advantages including substituting the bulky external index with an internal index (i.e., model parameters), significantly reducing memory usage, and enabling optimization during end-to-end model training towards a universal objective for information retrieval tasks. Conversational question answering techniques have been integrated to enhance the document retrieval \\cite{li:unigen:2024}. \n\nWhen retrieving visual content, recent work exploits generative models to enhance content-based model search \\cite{Lu:content:2023}. These models decode the text, image, or video query into samples of possible outputs, which are then used to learn statistics for better matching between the query and output candidates. DMs are also employed for visual retrieval tasks, where they learn joint data distributions between text queries and video candidates \\cite{Jin:DiffusionRet:2023}. A comprehensive survey on Generative Information Retrieval is available in \\cite{Li:From:2025}.\n\n\n\nWhile the retrieval task involves users directly defining a specific query input, recommendation services operate by retrieving content based on previous usage patterns. Essentially, a recommendation engine is a system that suggests products, services, or information to users through data analysis. Research in \\cite{CHUA:AI:2023} has reported a positive association between buyers' attitudes toward AI and their behavioral intention to accept AI-based recommendations, with potential for further growth. Notable examples include the recommendation framework developed by Google \\cite{Rajput:recommender:2023}, which utilizes GR. This framework assigns Semantic IDs to each item and trains a retrieval model to predict the Semantic ID of an item that a given user may engage with. A report by Aggarwal et al.~\\cite{aggarwal2025evolution} states that the recommendation accuracy of recommendation services has increased from 45.0\\% to 91.5\\% with the integration of generative AI.\n\n\\subsubsection{Intelligent assistants}\n\nIntelligent assistants refer to software programs or applications that use AI and NLP to interact with users and provide helpful responses or perform tasks. These assistants can range from simple chatbots to sophisticated virtual agents capable of understanding and responding to complex queries. They're designed to assist users in various tasks, from answering questions and providing information to scheduling appointments and controlling smart home devices.\n\nCurrent LLMs obviously enhance the performance of intelligent assistants, designed to understand complex inquiries and generate more natural conversational responses, such as Sasha \\cite{King:Sasha:2024}. Generative AI can also be used to enhance the performance of human customer support agents, aiding in search and summarization, as discussed in the previous section. Brynjolfsson et al. \\cite{brynjolfsson:generative:2023} examined the implementation of a generative AI tool designed to offer conversational guidance to customer support agents. Their research revealed that AI assistance significantly enhances problem resolution and customer satisfaction. Furthermore, they observed that AI recommendations prompt low-skill workers to adopt communication styles akin to those of high-skill workers. AI-based intelligent assistants may currently be more focused on educational purposes, but they can clearly help artists write more efficiently \\cite{Lee:design:2024} or assist in customizing personal requirements \\cite{sajja2024ai}. The performance of personalized assistants can be enhanced with domain-specific knowledge to provide more in-depth responses to users \\cite{jiang2025domain}.\n \n\\subsection{Content enhancement and post production workflows}\n\n\\subsubsection{Enhancement}\n\nIn our previous review paper \\cite{Anantrasirichai:AI:2022}, we discussed AI technologies for contrast enhancement and colorization as separate topics, as methods were developed specifically for each task. However, in recent years, there has been a shift towards addressing more complex issues, such as those encountered in low-light environments and underwater scenarios. These real-world situations often involve a combination of challenges, including low contrast, color imbalance, and noise.\n\nIn low-light conditions, scenes often exhibit low contrast, leading to focusing difficulties or the need for long exposures, which can result in blurred images and videos. To address this, LEDNet \\cite{Zhou:LEDNet:2022} has introduced a synthetic dataset for such scenarios and incorporated a learnable non-linear activation function within the network to enhance feature intensities. Meanwhile, SNR-Aware \\cite{Xu:SNR:2022} estimates spatial-varying Signal-to-Noise Ratio (SNR) maps and proposes local and global learning branches using ResNet and transformer architectures, respectively. NeRCo \\cite{Yang:Implicit:2023} address low-light problem with INR, which unifies the diverse degradation factors of real-world scenes with a controllable fitting function.  Diffusion models (DMs) have also become popular choices for low-light image enhancement \\cite{HOU:Global:2023,Yi:Diff:2023,Jiang:Low:2023}. Diff-Retinex \\cite{Yi:Diff:2023} formulates the low-light image enhancement problem into Retinex decomposition, and employs multi-path generative diffusion networks to reconstruct the normal-light Retinex probability distribution. A recent state-of-the-art approach presented in \\cite{Jiang:Low:2023} decomposes images into high and low frequencies using wavelet transform. High frequencies are enhanced using a transformer-based pipeline, while the low frequencies undergo a diffusion process. This method achieves nearly 2.8dB improvement over the state-of-the-art transformer-based approach, e.g. LLFormer \\cite{Wang:Ultra:2023}, and significantly better than  INR-based method, NeRCo \\cite{Yang:Implicit:2023}, on a real low-light image benchmarking dataset. The technique has been extended for video enhancement in \\cite{lin2024lowlight}. The output of the enhancement typically depends on user preferences. This has been viewed as a one-to-many inverse problem, with attempts to solve it using Bayesian approaches. For example, a Bayesian Enhancement Model (BEM) \\cite{huang2025bayesian} incorporates Bayesian Neural Networks (BNNs) to capture data uncertainty and produce diverse outputs. The method can be used with Transformers or Mamba as the architecture backbone. \n\nRegarding video enhancement, transformer and DMs are still in their early stages. STA-SUNet \\cite{Lin:SPATIO:2024} has demonstrated that using transformers for low-light video enhancement outperforms CNN-based methods \\cite{anantrasirichai:BVI:2024}. The recent Mamba-based network \\cite{huang2025bvi} also demonstrates promising results, outperforming STA-SUNet by more than 2 dB in PSNR. It is important to note that low-light enhancement is subjective. While most training datasets use normal lighting conditions as ground truth \\cite{Lin:BVI-RLV:2024}, the enhanced images and videos may alter the mood and tone of the content. Therefore, the tools for creative industries should be adjustable, not only for entire images and videos but also adaptive to specific areas and content. For instance, CLE Diffusion \\cite{Yin:CLE:2023} enables user-friendly editing of lighting with fine-grained regional controllability. \n\nRecent efforts have focused on enhancing User-Generated Content (UGC) videos\u2014authentic recordings created by individuals rather than brands, often showcasing real experiences with products or services. The winning solution of the NTIRE 2025 Challenge on UGC Video Enhancement \\cite{safonov2025ntire} implemented a pipeline of four sequential modules: color enhancement, denoising, BasicVSR++ restoration \\cite{Chan:BasicVSR:2022}, and SwinIR \\cite{Liang:SwinIR:2021}. This method achieved a 17\\% higher subjective score than the second-place entry, which used a two-stage framework, highlighting a notable improvement in perceived visual quality.\n\n\\subsubsection{Style transfer}\n\nStyle transfer in AI art refers to a technique where the artistic style of one image (or video) is applied to another image (or video) while preserving the content of the latter. Style transfer has numerous applications in art, design, and image editing, allowing artists and designers to create unique and visually appealing compositions by blending different artistic styles with existing images (or videos). The applications also include image-to-image and sequence-to-sequence translations.\n\nStyTr2 \\cite{Deng:StyTr2:2022} is the first transformer-based method for style transfer, applying content as a query and style as a key of attention. InST \\cite{Zhang:Inversion:2023} utilizes Stable Diffusion Models as the generative backbone and introduces an attention-based textual inversion module to learn the description of the content. StableVideo \\cite{Chai:StableVideo:2023} uses a text prompt to describe the desired appearance of the output, transforming the input video to have a new look based on a diffusion model. For instance, a video of a white car driving in summer can be altered to show a red car driving in winter. A large pre-trained DM is employed in \\cite{Chung_2024_CVPR}, where the style is injected to manipulate the self-attention of the decoder. To deal with the disharmonious color, they propose an adaptive instance normalization. A survey of style transfer using transformers and diffusion models can be found in \\cite{ZHOU:Bridging:2025}. Implicit Neural Representations (INRs) are less commonly used in style transfer tasks due to the difficulty of modeling the cross-representation between style and content. Moon et al. \\cite{Moon:generalizable:2023} combined INRs with vision transformers for generalizable style transfer; however, the results remain limited in quality. In contrast, the method proposed by Kim et al. \\cite{Kim:Controllable:2024} uses multilayer perceptrons (MLPs) to map image coordinates to the colors of the stylized output, guided by features extracted from both the content and style inputs to allow controllability.\n\n\n\\subsubsection{Upscaling imagery: super-resolution (SR)}\n\nImpressive super-resolution (SR) results from transformer and diffusion models have been published extensively in the past few years. Originally, SR methods were developed using multiple low-resolution (LS) images, as different features in each image are combined to construct an enhanced one. However, these methods are not practical, as in most cases only one LS image is available. Hence, more methods have been developed for single image super-resolution (SISR).\n\nThe first use of a transformer, called ESRT, was for capturing long-term dependencies, such as repeating patterns in buildings. This was done in the feature domain extracted by a lightweight CNN module \\cite{Lu:Transformer:2022}, outperforming those that use only CNNs. Since then, most SISR methods have been based on transformers. The Hybrid Attention Transformer (HAT) \\cite{Chen:Activating:2023} was introduced, which improves the SR quality over ESRT by more than 2dB when upscaling 2$\\times$\\times-4$\\times$\\times. However, the NTIRE 2023 Real-Time Super-Resolution Challenge \\cite{Conde:Efficient:2023} showed that the winner, Bicubic++ \\cite{Bilecen:Bicubic:2023}, uses only convolutional layers and achieves the fastest speed at 1.17ms in upscaling 720p to 4K images. This method is significantly faster than any of the participants in the NTIRE 2025 Challenge \\cite{chen2025ntire}, where Transformer-based architectures continue to dominate as the mainstream approach.\n\nFor DMs, SR3 by Google \\cite{Saharia:image:2023} has produced truly impressive results. It operates by learning to transform a standard normal distribution into an empirical data distribution through a sequence of refinement steps, interpolating in a cascaded manner\u2014upscaling 4$\\times$\\times at a time. Later, IDM \\cite{Gao:Implicit:2023} combines INR with a U-Net denoising model in the reverse process of the DM. It is crucial to emphasize again that DMs are generative models. The SR results are generated based on the statistics we provide to the model during training (LR training samples). This is not for a restoration task, but rather for synthetic generation. A survey in SISR using DMs can be found in \\cite{moser:diffusion:2024}.\n\nFor video SR, numerous methods have emerged as part of a unified enhancement framework, as discussed in the previous section. One of the pioneering works to incorporate transformers specifically for video SR tasks is the Trajectory-aware Transformer for Video Super-Resolution (TTVSR) \\cite{Liu:Learning:2022}. Although the results are slightly inferior to those of BasicVSR++ \\cite{Chan:BasicVSR:2022}, which employs CNN and was introduced around the same time, both methods significantly enhance detail and sharpness compared to previous approaches, albeit not in real time. To address this limitation, the Deformable Attention Pyramid \\cite{Fuoli:Fast:2023} has been introduced, offering slightly lower quality but a speed-up of over 3$\\times$\\times. Recently, Adobe announced their VideoGigaGAN \\cite{xu:videogigagan:2024}, which can perform 8$\\times$\\times upsampling. This is achieved by adding flow estimation and temporal self-attention to the GigaGAN upsampler \\cite{kang:gigagan:2023}, which is primarily used for image SR, and text-to-image synthesis. Cao et al. \\cite{cao2025zero} introduce a zero-shot video super-resolution framework that leverages a pre-trained image diffusion model, and replaces the spatial self-attention layer with a novel short-long-range (SLR) temporal attention layer. Recently, SeedVR integrated text information (captions) into a Diffusion Transformer (DiT) model, achieving state-of-the-art performance in video super-resolution.\n\nCompared to traditional upscaling methods, generative AI can add details that did not exist in the original input image. These methods excel at generating high-quality natural images and structures, such as buildings, which are commonly included in training datasets. However, the process can be slow and may produce unpredictable results if the input image has very low resolution or contains content rarely seen in natural images. As shown in Fig. \\ref{fig:SR} (left), generative AI fails to upscale the knitting texture areas, instead generating lines more commonly found in typical images. While AI methods produce sharper edges, they perform less effectively on text.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/SR_results.png}\n    \\caption{(Left) Examples of SR ($\\times$4) using generative model. (Right) Real-time portrait editing with FacePoke.} \n    \\label{fig:SR}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/SR_results.png}\n    \\caption{(Left) Examples of SR ($\\times$4) using generative model. (Right) Real-time portrait editing with FacePoke.} \n    \\label{fig:SR}\n\n\n\\subsubsection{Restoration}\n\nIn our previous review paper \\cite{Anantrasirichai:AI:2022}, we categorized the work on restoration into several different types of distortions, including deblurring, denoising, dehazing, and mitigating atmospheric turbulence. Recent work however uses a unified network architecture to address these as inverse problems $y = hx + n$y = hx + n, where $x$x and $y$y are the ideal and observed data, respectively. $h$h is a degradation function, such as blur, and $n$n is additive noise. Often the super-resolution task is also considered as an inverse problem, meaning $h$h includes downsampling process. Note that although designed as a single network, the model is trained with each distorted dataset separately. \n\nThe pioneering transformer-based method for image restoration, SwinIR \\cite{Liang:SwinIR:2021}, employs several concatenated Swin Transformer blocks \\cite{Liu:Swin:2021}. SwinIR surpasses state-of-the-art CNN-based methods proposed up to the year 2021 in super-resolution and denoising tasks. The model is smaller and reconstructs fine details more effectively.\nOther two popular approaches that emerged in the same timeframe are Uformer \\cite{Wang:Uformer:2022} and Restormer \\cite{Zamir:Restormer:2022}. Both incorporate Transformer blocks into hierarchical encoder-decoder networks, employing skip connections similar to those in U-Net. Their objective was to restore noisy images, sharpen blurry images, and remove rain. The networks focused on predicting the residual $R$R and obtaining the restored image $\\hat{x}$\\hat{x} through $\\hat{x} = y + R$\\hat{x} = y + R. While their performance is very similar, Restormer has half the parameters of Uformer. More recent, GRL by Li et al. \\cite{li:GRL:2023} exploits a hierarchy of features in a global, regional, and local range using different ways to compute self-attentions as an image often show similarity within itself in different scales and areas. GRL outperforms SwinIR and Restormer. Additionally, Fei et al. introduced the Generative Diffusion Prior \\cite{Fei:Generative:2023} for unsupervised learning, aiming to model posterior distributions for image restoration and enhancement. VmambaIR \\cite{Shi:VmambaIR:2025} incorporates Mamba blocks into the U-Net architecture, achieving superior performance compared to SwinIR and Restormer in both visual quality and model size.\n\nFor video restoration, the general framework comprises frame alignment, feature fusion and reconstruction. The process could be similar to image restoration but input multiple frames and run through the sequences in sliding window manner to exploit temporal information of a number of consecutive frames. \nRecently, Video Restoration Transformer (VRT) \\cite{Liang:VRT:2024} and its improved version with recurrent process (RVRT) \\cite{liang:RVRT:2022}, have emerged as the state of the arts for video super-resolution, deblurring, denoising, and frame interpolation. This method introduces temporal reciprocal self-attention in the transformer architecture and parallel warping using MLP. These innovations enable parallel computation and outperform the previous state-of-the-art methods by up to 2.16dB on benchmark datasets. FMA-Net \\cite{Youk:FMA:2024} proposed multi-attention for joint Video super-resolution and deblurring, achieving fast runtime with nearly 40\\% improvement over RVRT, and the restored quality was reported better by up to 3\\%. \n\nFor audio restoration, most software discussed in Section \\ref{sssec:musicgen} offers tools for enhancing audio quality, such as eliminating background noise, echo, microphone rumble, and occasionally room reverberation, which have been well-established even before the advent of deep learning. There have been efforts to utilize AI for learning global contextual information to aid in the removal of unwanted sounds, leading to better final quality \\cite{Yu:DBT:2022}. The latest advancements in this domain are primarily focused on addressing issues where significant portions of the audio data are missing. For instance, Moliner et al. \\cite{Moliner:solving:2023} tackle problems such as audio bandwidth extension, inpainting, and declipping by treating them as inverse problems using a diffusion model. For a comprehensive survey on the use of diffusion models in restoration tasks, refer to \\cite{10902142}. \n\nThe following methods have been proposed for specific problems, but ideally, they should be adaptable for other tasks, even though they may not perform as well as they do for the original task.\n \ni) \\textbf{Deblurring}: A lightweight deep CNN model was recently proposed in \\cite{Pan:Deep:2023}, where a new discriminative temporal feature fusion has been introduced to select the most useful spatial and temporal features from adjacent frames. Feature propagation along the video is done in the wavelet domain. The deblurring performance is comparable to RVRT \\cite{liang:RVRT:2022}, but it is 5 times faster. DaBiT \\cite{Morris:DaBiT:2024} mitigates focal blur content with depth information and applies SR for further enhancing fine details.\nNote that not only in software, but AI technologies have also been integrated into hardware. This includes autofocus, which is crucial for capturing sharp images of subjects, especially in dynamic environments where manual adjustments are impractical due to rapid movement. AI-driven autofocus methods have emerged, often tailored for specific camera hardware. For instance, Choi et al. proposed an autofocus model optimized for dual-pixel Canon cameras \\cite{Choi:Exploring:2023}. Additionally, Yang et al. investigated the correlation between language input and blur map estimation, utilizing semantic cues to enhance autofocus performance \\cite{yang:ldp:2023}. Remarkably, their model achieves comparable results to previous state-of-the-art methods while being more lightweight \\cite{Yang:K3DN:2023}. Autofocus could be used in conjunction with real-time object tracking (see Section \\ref{sssec:tracking}) to produce desirable sharpness for moving objects in the video. Recently, Feng et al. \\cite{feng2025residual} proposed a novel residual diffusion deblurring framework that integrates a conditional diffusion model guided by a defocus map and incorporates residual learning into the single-image defocus deblurring process.\n\nii) \\textbf{Denoising}: SUNet \\cite{Fan:SUNet:2022} applies Swin transformer blocks combined in a UNet-like architecture. Denoising with diffusion models (DMs) \\cite{yang:realworld:2023} has been proposed by diffusing with estimated noise that is closer to real-world noise rather than Gaussian noise, achieving better performance than SwinIR \\cite{Liang:SwinIR:2021} and Uformer \\cite{Wang:Uformer:2022}. INR with complex Gabor wavelets as activation functions show promising denoising results \\cite{Saragadam:wire:2023}. The NTIRE 2025 Image Denoising Challenge \\cite{sun2025tenth} revealed that the top-performing methods combined transformer-based and convolutional network architectures. Similarly, recent advances in video denoising also adopt a hybrid approach that integrates both architectures \\cite{Jin:Masked:2025,  Yue:RViDeformer:2025}.\n\niii) \\textbf{Dehazing}: Vision transformers for single image dehazing was proposed in DehazeFormer \\cite{Song:vision:2023}. Similar to SUNet, it is UNet-like architecture, but introduced Rescale Layer Normalization for better suit on improving contrast. The Fast Fourier Transform (FFT) has been employed in \\cite{fang2025guided} due to the phase spectrum conveying more structural detail than the amplitude spectrum and demonstrating greater robustness to contrast distortion and noise. Then cross-attention between the RGB and YCbCr color spaces is applied. This approach achieves nearly 5 dB higher PSNR than DehazeFormer on a real-world smoke dataset.  For video dehazing, Xu et al. \\cite{Xu:Video:2023} introduced a recurrent multi-range scene radiance recovery module with the space-time deformable attention. They also employs physics prior to inform haze attenuation. This method outperforms DehazeFormer by approximately 1dB.\n\niv) \\textbf{Mitigating atmospheric turbulence}: Similar to dehazing, physics-inspired models have been widely developed to remove turbulence distortion \\cite{Jaiswal:Physics:2023,Jiang:NeRT:2023}, while complex-valued CNNs have been proposed to exploit phase information \\cite{Atmospheric:2023}. There was also an attempt to use instance normalization (INR) to address this issue, providing solutions for tile and blur correction \\cite{Jiang:NeRT:2023}. However, diffusion models have shown superior performance on single-image restoration tasks \\cite{Nair:AT-DDPM:2023}, while transformer-based methods remained the state-of-the-art for video restoration \\cite{Zhang:Image:2024, zou2024deturb}. More recently, Mamba-based architectures have demonstrated their effectiveness in both visual quality and model efficiency \\cite{hill2025mamat}. A recent review can be found in \\cite{Hill2025}.\n\n\\subsubsection{Inpainting}\n\\label{ssec:inpaiting}\n\nVisual inpainting is the process of filling in lost or damaged parts of an image or video. CNNs and GANs have already achieved impressive results (see our previous review paper \\cite{Anantrasirichai:AI:2022}). Recent work has focused more on editing rather than simply filling in the missing areas. This means users can now mask large areas of an image, and AI tools generate multiple results for users to choose from, a technique known as pluralistic inpainting \\cite{zheng:pluralistic:2019}. Some notable methods include the following: Mask-Aware Transformer (MAT) \\cite{Li:MAT:2022} offers several outputs to fill a large missing area, consisting of a convolutional head, a transformer body, and a convolutional tail for reconstruction, along with a Conv-U-Net for refinement. PUT \\cite{Liu:Reduce:2022} proposes a patch-based vector VQ-VAE and unquantized Transformer to minimize information loss. Spa-former \\cite{HUANG:Sparse:2024} employs a UNet-like architecture, where each level performs transformer with sparse self-attention to remove coefficients with low or no correlation, leading to memory reduction, while improving result quality by up to 5\\% compared to PUT.\n\nVideo inpainting presents greater complexity compared to image inpainting, despite the abundance of information available in an image sequence. The process typically involves tracking masks across frames, estimating optical flow, and ensuring temporal consistency.  The current state-of-the-art methods include DLFormer \\cite{Ren:DLFormer:2022} and  ProPainter \\cite{Zhou:ProPainter:2023}. DLFormer conducts inpainting in latent space and utilizes discrete codes for video representation. On the other hand, ProPainter employs flow-based deformable alignment to enhance robustness to occlusion and inaccurate flow completion. The method excels in filling complete and rich textures, achieving a speed of 12 fps for full HD video. Video inpainting is also used for dubbing. DINet \\cite{Zhang:DINet:2023} replaces the mouth area to synchronize with a new language being spoken.\n\nA comprehensive survey of learning-based image and video inpainting, covering approaches such as CNNs, VAEs, GANs, transformers, and diffusion models, can be found in \\cite{quan:deep:2024}. Additionally, Elharrouss et al. \\cite{elharrouss2025transformer} provide an in-depth review of the current challenges and future directions specific to transformer-based inpainting techniques.\n\n\\subsubsection{Image Fusion}\n\\label{sssec:fusion}\n\nImage fusion is the process of  merging multiple images from either the same source (such as varying focal points or exposures) or different modalities (e.g. visible and infrared cameras) into a single image. This process integrates complementary information from the various images to enhance overall quality, improve interpretation, and increase the usability of the final image.\n\nTransformers and CNNs have been combined to extract global and local information, respectively. Most methods use CNNs for feature extraction, with transformers operating in the latent space \\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023}. Notable methods include SwinFusion \\cite{Ma:SwinFusion:2022}, which utilizes a self-attention-based intra-domain fusion unit and a cross-attention-based inter-domain fusion unit to achieve multi-modal and digital photography image fusion. Transformer-based image fusion has also been applied to downstream tasks like segmentation \\cite{Liu:Multi:2023}, achieving superior results by leveraging the additional information. Self-attention blocks are employed to enhance intra-feature representations, while the cross-attention mechanism integrates inter-feature information to improve the quality of the fused output \\cite{LI2024102147}.\n\nDDFM, the first diffusion model-based image fusion method, estimates noise in the reverse process by combining multiple inputs \\cite{Zhao:DDFM:2023}. The expectation-maximization (EM) algorithm is integrated to estimate the noise distribution parameters, resulting in sharper images compared to traditional DDPM. For an in-depth review, the reader is referred to recent work in \\cite{Karim:Current:2023, Zhang:Visible:2023}.\n\n\\subsubsection{Editing and Visual Special Effects (VFX)}\n\nEditing or modifying specific areas of an image is much easier with DM technologies, particularly for headshot photos, such as targeting the eyes and mouth on the face \\cite{guo2024liveportrait}. This capability has been extended to video generation (see Section \\ref{sssec:videogen}). Fig. \\ref{fig:SR} shows an example of the online tool, FacePoke\\footnote{\\url{https://huggingface.co/spaces/jbilcke-hf/FacePoke}}, which allows users to move the head and modify the shapes of the eyes and mouth in real time. Motion-I2V \\cite{Shi:Motion-I2V:2024} provides motion blur and motion drag tools to control specific areas of an image to add motion. The method is based on a diffusion-based motion field predictor and motion-augmented temporal attention.\n\nVFX aims to create and/or manipulate imagery outside the context of a live-action shot in filmmaking and video production. When adding objects, scenes, and effects into traditional photographic videos, generative AI has obviously become an important tool, but some manual operations are still required. For example, in After Effects (EA)\\footnote{\\url{https://www.adobe.com/uk/products/aftereffects.html}}, the user selects the area where the object will be added and uses text prompts to describe such object. Subsequently, with the current EA version, the user will need to apply motion tracking so the generated object is moved accordingly.\n\nAI technologies can upscale, enhance, and restore low-quality or old footage. For example, standard definition videos can be converted to high definition or even 4K quality without traditional manual remastering processes. This is particularly useful for remastering old movies or enhancing visual details in scenes. Generative AI has also simplified and accelerated automated processes, such as rotoscoping \\cite{Tous:Lester:2024}, an animation technique where animators trace over motion picture footage frame by frame to create realistic action. AI models can accurately detect and segment objects and characters in video frames, significantly speeding up the post-production process. Additionally, AI can assist the rapid creation of 3D models from 2D images generating realistic animations with minimal input data, facilitating complex human motions and synchronized facial expressions to voiceovers. One restriction is that current technologies still cannot yet generate full 4K accurate visual effects.\n\n\n\n\n\n\\subsection{Information Extraction and Understanding}\n\\label{ssec:infoextract}\n\nAI plays a crucial role in automating and optimizing the process of information extraction and understanding, enabling organizations to derive actionable insights from large and diverse data. Yan et al. \\cite{Yan:Universal:2023} have categorized information extraction tasks based on the Format-Time-Reference space, as illustrated in Fig. \\ref{fig:segmentation} (a), where object detection and video object segmentation (VOS) are considered to be the simplest and the most complex tasks, respectively. Recent advancements in this field draw significant inspiration from LLMs. These advancements include the utilization of prompts as conditional inputs for acquiring information. Moreover, following the pipeline approach used in LLMs, there is a growing trend towards leveraging very large datasets to pre-train models before fine-tuning them for specific downstream tasks. For instance, Meta AI \\cite{Oquab:DINOv2:2024} has introduced DINOv2, aimed at enriching information about visual content through self-supervised learning. This model was trained with 142 million carefully selected images, employing the ViT architecture. Google have introduced VideoPrism \\cite{zhao:videoprism:2024}, a tool for scene understanding including classification, localization, retrieval, captioning, and question answering (QA). The model was trained on an extensive and diverse dataset consisting of 36 million high-quality video-text pairs and 582 million video clips accompanied by noisy or machine-generated parallel text.\n\n\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation.png}\n    \\caption{(a) Tasks in Object-centric understanding defined by Yan et al. \\cite{Yan:Universal:2023} (REC:,  Referring Expression Comprehension, RES: Referring Expression Segmentation, VOS:  Video Object Segmentation, RVOS: Referring Video Object Segmentation, MOT: Multiple Object Tracking, MOTS: Multi-Object Tracking and Segmentation, VIS: Video Instance Segmentation, SOT: Single Object Tracking. (b) Current high-quality segmentation \\cite{Ke:SAM-HQ:2023}.}\n    \\label{fig:segmentation}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation.png}\n    \\caption{(a) Tasks in Object-centric understanding defined by Yan et al. \\cite{Yan:Universal:2023} (REC:,  Referring Expression Comprehension, RES: Referring Expression Segmentation, VOS:  Video Object Segmentation, RVOS: Referring Video Object Segmentation, MOT: Multiple Object Tracking, MOTS: Multi-Object Tracking and Segmentation, VIS: Video Instance Segmentation, SOT: Single Object Tracking. (b) Current high-quality segmentation \\cite{Ke:SAM-HQ:2023}.}\n    \\label{fig:segmentation}\n\n\n\\subsubsection{Segmentation}\n\\label{sssec:seg}\n\nThe need for segmentation has grown dramatically in the past few years, given its central role in visual perception. Many segmentation methods now integrate an input prompt for users to define their preferred output appearances, such as pixel-wise segmentation, bounding boxes around objects, or segmented areas of interest. Most of these methods utilize transformer architectures \\cite{Bowen:Marked:2022}. Among them, Segment Anything (SAM) by Meta AI \\cite{Kirillov:SAM:2023} stands out as a pioneer in promptable segmentation approaches. This method computes masks in real-time and has been trained with over 1 billion masks across 11 million images, facilitating transferability from zero-shot to new image distributions and tasks. HQ-SAM \\cite{Ke:SAM-HQ:2023} enhances SAM by incorporating global-local feature fusion, leading to high-quality mask predictions. SegGPT \\cite{Wang:SegGPT:2023} proposed context ensemble strategies and allows users to tune a prompt for a specific dataset, scene, or even a person, while SEEM \\cite{Zou:Segment:2023} provides a completely promptable and interactive segmentation interface. More recently, SAM2 \\cite{ravi2024sam2} introduced support for real-time video segmentation. It is a unified model trained on a larger dataset than SAM. Interactive tools enable users to mark areas of interest and specify regions to exclude from the segmentation map. Zhou et al. propose an audio-visual segmentation (AVS) to generate pixel-level segmentation masks for sounding objects in audible videos. DVIS++ by \\cite{Zhang:DVISp:2025} introduces a universal video segmentation framework capable of producing instance, semantic, and panoptic segmentation outputs. This transformer-based architecture comprises a segmentor, tracker, and refinement module, achieving state-of-the-art performance across several video segmentation benchmarks.\n\nWith DMs tehchnologies, Baranchuk et al. \\cite{Baranchuk:label:2022} have investigated semantic representation, and found DMs outperform other few-shot learning approaches. DiffuMask \\cite{Wu:DiffuMask:2023} automatically generate image and pixel-level semantic annotation using pre-trained Stable Diffusion with input as a text prompt. It has been proven that using these synthetic data improve segmentation accuracy. Currently, the state-of-the-art panoptic segmentation is the method developed by Nvdia, which is based on text-to-image DMs \\cite{Xu:Open:2023}, outperforming the previous methods by up to 7.6\\%.\n\nApplying INRs to segmentation is more popular in the medical domain, as the specific signals used, such as computed tomography (CT) and magnetic resonance imaging (MRI), can be formulated as continuous functions. In creative technologies, unsupervised domain adaptation (UDA) and INRs are used for continuous rectification function modeling in \\cite{Gong:Continuous:2023}, achieving superior segmentation results in night vision. Recently, this work has been integrated with a non-local means block in \\cite{Lin:Feature:2024}  showning a significant improvement for instant segmentation in low-light scenes.\n\n3D segmentation is also crucial for scene manipulation. In radiance fields, earlier segmentation methods required additional modules such as using k-means clustering to separate objects from the background \\cite{Goel:Interactive:2023}. However, the recent SA3D approach \\cite{Cen:Segment:2023} segments 3D objects using NeRFs as the structural prior. SA3D operates by taking a trained NeRF and a set of prompts from a single view, then performing an iterative procedure. This involves rendering novel 2D views, self-prompting SAM for 2D segmentation, and projecting the segmentation back onto 3D mask grids. A comprehensive survey of 3D segmentation in computer vision can be found in \\cite{HE2025102722}.\n\n\\subsubsection{Detection and recognition}\n\\label{sssec:recog}\n\nIntroduced in 2020, DETR by Facebook AI \\cite{Carion:DERT:2020} was one of the first to adopt a transformer architecture for object detection. The approach achieves comparable results to an optimized Faster R-CNN \\cite{Ren:Faster:2027}, introduced in 2015. Deformable convolution has alson been used, (Deformable DETR \\cite {Zhu:Deformable:2021}), resulting in training faster with approximately 5\\% accuracy improvement.\nA survey until 2022  \\cite{Zou:object:2023} reported that Deformable DETR and Swin Transformers \\cite{Liu:Swin:2021} outperform pure CNN-based YOLOv4 \\cite{bochkovskiy2020yolov4}. \nSwinV2 improves the first version by replacing original dot product attention with scaled cosine attention, improving accuracy by approximately 5\\%. Later, RT-DETR \\cite{lv2:detrs:2024} improved inference speed by decoupling the intra-scale interaction and cross-scale fusion of features with different scales. RT-DETR is 25\\% faster than YOLOv8\\footnote{\\url{https://github.com/ultralytics/ultralytics}} with 6\\% improvement on MS COCO Object Detection dataset. Recently, YOLOv10 \\cite{wang:yolov10:2024} has been released. YOLOv10 further improves the speed of detection approximately by 30\\% over RT-DETR with the same accuracy. A review of transformer-based methods for object detection can be found in \\cite{Li:Transformer:2023, KHEDDAR2025103347}. Recently, YOLO12 \\cite{tian2025yolov12} introduced an attention-centric architecture, achieving a 2.1\\% and 1.2\\% mAP improvement over YOLOv10-N and YOLOv11-N respectively, with only a slight decrease in speed.\n\nTo detect 3D objects, the transformer-based method MonoDTR \\cite{Huang:MonoDTR:2022} incorporates depth estimation from a single 2D image \\cite{Yang:depthanything:2024} to predict 3D bounding boxes. More 3D object detection methods have been developed for autonomous driving \\cite{10637966}; however, these approaches can also be adapted for AR and VR applications \\cite{im2025gate3d}.\n\nWhile DMs are primarily used to generate synthetic datasets \\cite{Wu:datasetDM:2023, Fang:Data:2024}, they have also been demonstrated to function as zero-shot classifiers by Li et al. \\cite{Li:Your:2023}. DMs are also of interest for detection tasks, Although feature extractors are still predominantly based on CNNs, such as ResNet, or Transformers (like Swin). DiffusionDet \\cite{Chen:DiffusionDet:2023} formulates object detection as a denoising diffusion process from noisy boxes to object boxes, reporting performance that surpasses DETR. DMs have also been employed for anomaly detection \\cite{Zhang:DiffAD:2025, WU2025102965}, functioning similarly to zero-shot classifiers.\n\n\n\\subsubsection{Tracking}\n\\label{sssec:tracking}\n\nObject tracking stands out as one of the tasks that greatly benefits from transformers since \\textit{attention} is needed in both space and time. An experimental survey cited in \\cite{Kugarajeevan:Transformers:2023} reveals that transformer-based methods consistently rank at the top of the leaderboard across various datasets. In the Visual Object Tracking (VOT) challenges of 2023\\footnote{https://eu.aihub.ml/competitions/201\\#results}, all of the top-10 employed transformer-based methodologies. The highest-performing approach achieved a 10\\% improvement in tracking quality compared to the winner in 2020. The current state-of-the-art for single-object tracking\\footnote{https://paperswithcode.com/sota/visual-object-tracking-on-lasot}, however, is based on cross-attention and Mamba \\cite{kang2025exploring}.\n\nThe first three tracking-by-attention approaches are TrackFormer \\cite{Meinhardt:TrackFormer:2022}, MixFormer \\cite{cui:mixformer:2022}, and ToMP \\cite{Mayer:Transforming:2022}. TrackFormer extracts visual features using a CNN-based encoder, which are then tracked using a vanilla transformer \\cite{Vaswani:attention:2017} in a frame sequence, while MixFormer introduces cross-attention between the target and search regions. ToMP tracks the objects using prediction aspects. Many more methods have been proposed, including SeqTrack \\cite{Chen:SeqTrack:2023} and Track Anything Model (TAM) \\cite{yang:track:2023}. SeqTrack extracts visual features with a bidirectional transformer, while the decoder generates a sequence of bounding box values autoregressively with a causal transformer. TAM combines SAM \\cite{Kirillov:SAM:2023} and XMem \\cite{cheng:xmem:2022}, offering tracking and segmentation performance on the human-selected target. However, the masked area is still not very sharp, and there is a subtle degree of temporal inconsistency. MOTRv2 \\cite{Zhang:MOTRv2:2023} combines YOLOX \\cite{ge:yolox:2021} for object recognition and MOTR \\cite{zeng:motr:2022} for tracking, outperforming TrackFormer by 20\\%. Additionally, some methods have been specifically proposed for challenging environments, such as low light \\cite{Yi:Comprehensive:2024} and small objects, as seen in AnyFlow \\cite{Jung:AnyFlow:2023}. The latter exploits INR to upsample a continuous coordinate-based flow map, similar to SISR technique proposed in \\cite{Chen:Learning:2021}.\n\nSimilarly to detection tasks, DMs for tracking tasks are used as downstream processes by concatenating the diffusion head to the feature extraction backbone. However, a spatial-temporal fusion module has been added to the diffusion head to exploit temporal video features\\cite{Luo:DiffusionTrack:2024}. DiffusionTrack \\cite{Xie:DiffusionTrack:2024} localizes the target in a progressive diffusion manner, which is claimed to better handle challenging scenarios. The method in \\cite{Zhang:DiffusionTracker:2024} exploits spatial-temporal weighting to suppress the probability of the tracker changing the target to the distractors. It, however, reports under-performance compared to  MixFormer.\n\n\n\n\\subsection{3D Reconstruction and Rendering}\n\\label{ssec:3Dreconstruct}\n\nBridging the gap between digital and physical realms, 3D reconstruction and rendering are integral to various creative technologies.  In film and animation, they enable the creation of detailed digital models that blend seamlessly with live-action footage. Video games and digital twins leverage these technologies for dynamic environmental rendering. VR and AR use 3D reconstruction to create immersive and interactive experiences, with AR integrating digital content into real-world contexts. With recent AI technologies, 3D reconstruction and rendering have become faster and closer to reality. In particular, neural radiance fields and Gaussian Splatting enable artists and film producers to create shots that cannot be one in the real shooting environments.\n\n\\subsubsection{Depth Estimation}\n\\label{sssec:depth}\n\nAccurate depth information (alongside texture data) is typically required to construct 3D models. Depth sensors, such as lidar (Light Detection and Ranging) and structured-light 3D scanners, can be used for this purpose, but their applications are often limited by distance and cost. Consequently, vision-based sensors have become widely used. These sensors utilize two or more cameras to simulate human binocular vision or employ a single camera to capture images from different locations.\n\nAs deep learning can capture monocular cues such as object size, texture gradients, and perspective, depth estimation from a single image can produce accurate results. There have been attempts to use transformers, such as \\cite{Zhang:Lite:2023} and \\cite{Chen:Vision:2023}, and diffusion models, such as \\cite{Ji:DDP:2023} and \\cite{Ke:Repurposing:2024}. Amongst these, Depth Anything v2 \\cite{Yang:depthanythingv2:2024} has become a state-of-the-art monocular depth estimation method. It is built on the previous version \\cite{Yang:depthanything:2024}, jointly trained on large-scale labeled and unlabeled images and uses semantic priors from pretrained encoders. Depth Anything v2 significantly outperforms V1 in fine-grained details and robustness by using synthetic images and pseudo-labeled real images, as well as by extracting intermediate features from DINOv2 \\cite{Oquab:DINOv2:2024}, which is trained with vision transformers. One of the notable capabilities of Depth Anything v2 is its ability to predict depth of transparent and reflective surfaces.\n\n\\subsubsection{Neural Radiance Fields}\n\\label{sssec:nerf}\n\nNeural Radiance Fields (NeRFs), introduced in \\cite{Mildenhall:NeRF:2020}, have demonstrated the ability to learn a 3D scene from a smaller number of images captured from various viewpoints, as opposed to photogrammetry. They excel in neural rendering, particularly in view-dependent novel view synthesis, and have effectively tackled several challenges associated with automated 3D capture \\cite{xie2022neural}, such as accurately representing the reflectance properties of the scene. NeRFs offer high-resolution photo-realistic novel views and flexibility in postprocessing. They have hence gained significant attention in cinematography \\cite{Azzarelli2024}, as they offer reduced time and cost, particularly for outdoor shooting.\n\nIn the NeRF process (see Fig. \\ref{fig:3Drepresentation} (a)), the camera positions and orientations are typically estimated from a series of 2D images using techniques like feature-mapping and Structure-from-Motion (SfM), as demonstrated in \\cite{schoenberger:sfm:2016}. Leveraging INR, each image (or camera pose) is mapped into camera rays that traverse the scene, generating 3D points with directional radiance (towards the camera). These points are then processed by an MLP to predict volume density and emitted radiance. Subsequently, volume rendering techniques are employed to generate an image, which is compared with the original via loss calculation. The MLP iteratively refines the model by minimizing this loss.\n\nSince their introduction, there have been many variants of NeRFs aimed at improving their performance. Mip-NeRF360 \\cite{Barron:Mip-NeRF360:2022} proposed unbounded anti-aliased technique achieving full 360 degree content. Google Research\n\\cite{Mildenhall:NeRFDark:2022} trains NeRF from noisy RAW images captured in the dark scene, allowing changing viewpoint, focus, exposure, and tone mapping simultaneously. With segmentation techniques significantly advanced (see Section \\ref{sssec:seg}), there have been integrations utilizing semantic segmentation to enhance 3D representation \\cite{Guo:neural:2022}. DSEM-NeR \\cite{LIU:DSEM:2025} integrates the pretrained CLIP model to extract multimodal features\u2014including color, depth, and semantics\u2014from multi-view 2D images, thereby enhancing the reconstruction quality of complex scenes.\n\nWhile the rendering quality of NeRF is very good, training and rendering times remain extremely high. The Instant-NGP tool developed by Nvidia \\cite{mueller:instant:2022} enables real-time training of NeRFs by bypassing sampling in empty spaces and dense areas, and by incorporating multi-resolution hash encoding techniques. These advancements substantially reduce the computational burden associated with representing high-resolution image features -- training times have been reduced from hours to just a few seconds. Moreover, it offers VR controls for immersive 3D rendering experiences using OpenXR\\footnote{An open-source, royalty-free standard for access to virtual reality and augmented reality platforms and devices. \\url{https://www.khronos.org/openxr/}}. This allows users to navigate scenes, manipulate objects, and interact with the environment directly through VR headsets. Diffusion models are integrated to regularize NeRF reconstructions \\cite{wynn:diffusionerf:2023}, resulting in smoother depth continuity and clearer edges where depth discontinuities occur.\n\nThe initial application of NeRFs to dynamic scenes was undertaken by Pumarola et al. \\cite{pumarola:DNeRF:2020}, known as D-NeRF.\nHowever, the current leading method for generating high-quality novel views of real dynamic scenes is TiNeuVox \\cite{Fang:Fast:2022}.  It enhances temporal information by interpolating voxel features before feeding them into the radiance network to estimate density and color, similar to ordinary NeRF. DynVideo-E~\\cite{Liu_2024_CVPR} adds an MLP to predict motion fields but focuses on human-centric content. PaReNeRF~\\cite{Tang_2024_CVPR} addresses large-scale dynamic scenes using patch-based sampling. The main drawback of these methods is the large model size and/or long training time. Therefore, $K$K-planes \\cite{Fridovich:kplanes:2023} propose a simple planar factorization for volumetric rendering, achieving low memory usage (1000$\\times$\\times compression over a full 4D grid). Wavelet transform are employed in \\cite{azzarelli:waveplanes:2023} to further reduce model size. KFD-NeRF~\\cite{zhan2024kfd} incorporates a Kalman filter-guided deformation field for more accurate motion estimation.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/3Drepresentation.png}\n    \\caption{3D representation. (a) Neural Radiance Fields (NeRFs) \\cite{Mildenhall:NeRF:2020}. (b) Gaussian Splatting \\cite{kerbl:3Dgaussians:2023}. (c) Example scenes of VR-GS system for 3D content interaction in VR \\cite{jiang:vrgs:2024}. }\n    \\label{fig:3Drepresentation}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/3Drepresentation.png}\n    \\caption{3D representation. (a) Neural Radiance Fields (NeRFs) \\cite{Mildenhall:NeRF:2020}. (b) Gaussian Splatting \\cite{kerbl:3Dgaussians:2023}. (c) Example scenes of VR-GS system for 3D content interaction in VR \\cite{jiang:vrgs:2024}. }\n    \\label{fig:3Drepresentation}\n\n\n\\subsubsection{3D Gaussian Splatting}\n\\label{sssec:3DGS}\n\nThe main issue with NeRFs as a method to generate high-quality novel views is training time, which can exceed a day for high-resolution content on a single RTX 3090 GPU~\\cite{9879447}. 3D Gaussian Splatting (3D-GS) \\cite{kerbl:3Dgaussians:2023} has been introduced to address this, using anisotropic 3D Gaussians to form a high-quality, unstructured representation of radiance fields. The process estimates a sparse point cloud through SfM. Each point possesses 3D Gaussian properties, such as position, covariance matrix, opacity, and spherical harmonics coefficients representing colors. The optimization of these parameters is interleaved with steps that control the density of the Gaussians to better represent the scene, as shown in Fig. \\ref{fig:3Drepresentation} (b). A survey of 3D-GS can be found in \\cite{10521791}.\n\nIn contrast to traditional NeRFs based on implicit scene representations, 3D-GS provides an explicit representation that can be seamlessly integrated with post-processing manipulations, such as animating and editing. VR-GS \\cite{jiang:vrgs:2024} offers intuitive and interactive physics-based game-play with deformable virtual objects and realistic environments represented with 3D-GS. The example scenes are shown in Fig.~\\ref{fig:3Drepresentation} (c). Physics-inspired approaches are also integrated to improve 3D modeling in different media, such as 3D underwater scenes \\cite{Wang2025}.\n\nFor dynamic scenes, 4D Gaussian Splatting (4D-GS) \\cite{wu:4dgaussians:2024} introduces a Gaussian deformation field for motion and shape. It exploits a multi-resolution encoding method, achieving real-time rendering of up to 82 fps at a resolution of 800$\\times$\\times800 pixels on an RTX 3090 GPU. Instead of developing in 4D, CoGS \\cite{Yu:CoGS:2024} exploits 3D-GS by integrating control mechanisms in separate regions to learn individual temporal dimensions. SC-GS \\cite{Huang:SCGS:2024} extracts sparse control points and uses an MLP to predict time-varying 6 DoF transformations. While the results show better visual quality than 4D-GS and CoGS, the performance heavily relies on camera pose estimation. Kong et al. \\cite{kong2025efficient} represent dynamic scenes using sparse, time-variant attribute modeling with a deformable MLP, while efficiently filtering out anchors corresponding to static regions. Their model achieves fast rendering speeds of over 110 FPS at a resolution of 960$\\times$\\times540\u2014nearly 10 times faster than SC-GS\u2014and delivers a 1 dB improvement in PSNR.\n\nLUMA AI\\footnote{\\url{https://lumalabs.ai/interactive-scenes}} and \nPolycam\\footnote{\\url{https://poly.cam/captures}} offer free tools for Gaussian splatting and photogrammetry creation for non-commercial use. The 3D objects created can be experienced with VR headsets for more immersive 3D and further used or developed in other applications. However, these tools have limitations in handling dynamic scenes due to occlusions, sparse observations per timestamp, and object reappearances over time. Rendering dynamic avatars can produce higher quality results by incorporating additional information. For example, EVA \\cite{junkawitsch2025eva} disentangles the 3D Gaussian appearance into skeletal motion, facial expressions, body movements, and skin. These components are then splatted to render the final photorealistic image.\n\n\\subsubsection{Digital Twins}\n\\label{sssec:digital_twins}\n\nA digital twin is a virtual replica of a physical object, system, or process, continuously updated with real-time data for purposes such as simulation, testing, monitoring, and maintenance. This technology is increasingly adopted across various applications within the creative industries. For example, in product design and branding, it enables immediate observation of how a design performs in various contexts, facilitating the development of user-friendly products. Unilever reported that integrating digital product twins with 3D technologies, such as NVIDIA Omniverse\\footnote{\\url{https://www.nvidia.com/en-gb/omniverse/}}, enabled the creation of product imagery twice as fast and 50\\% more cost-effective\\footnote{\\url{https://www.unilever.com/news/press-and-media/press-releases/2025/unilever-reinvents-product-shoots-with-digital-twins-and-ai/}}. Digital twins also allow consumers to explore products or spaces virtually, simulating real-world interactions. \n\nAccenture plc, a global professional services company, collaborated with Walt Disney Studios to develop digital twin technologies aimed at transforming the filmmaking process\\footnote{\\url{https://www.accenture.com/mx-es/case-studies/communications-media/empowering-film-creatives-digital-twins}}. Their goal is to generate remotely accessible 3D models, enabling virtual exploration of potential shooting locations without requiring physical visits. The Virtual StudioLAB provides a digital replica created using 360-degree imagery and 3D modeling. These innovations have streamlined pre-production workflows for major productions from Marvel Studios and 20th Century Studios.\n\nDigital representations such as avatars, proxies, and digital twins are increasingly being explored in artistic contexts, particularly in relation to identity, presence, and embodiment in virtual environments. The Tate Modern\u2019s film programme Avatars, Proxies and Digital Twins (Feb\u2013May 2025) investigated these themes through curated audiovisual works, offering critical reflections on digital personhood. By engaging with diverse narrative forms, the programme highlighted the sociocultural implications of digital self-representation, prompting discourse on authenticity, agency, and the role of immersive media in shaping future human\u2013machine interaction.\n\n\\subsection{Data Compression}\n\\label{ssec:compression}\n\nData compression plays an important role in the delivery of creative content to audiences, effectively reducing memory and bandwidth requirements during signal storage and transmission \\cite{Bull:intelligent:2021}. Although coding methods based on conventional signal processing theories are still widely employed in most standards and application scenarios, learning-based solutions have emerged in research, showing great potential to achieve competitive performance in recent years. This subsection provides a brief overview of the recent advances in image, video, and audio compression, in particular focusing on the approaches proposed after 2021.\n\n\\subsubsection{Image Compression}\n\nSince the first neural image codec \\cite{balle2016density} was proposed in 2016, numerous learning-based image compression methods have been developed, with significant performance improvements reported \\cite{balle2018variational,cheng2020learned}. Driven by the latest advances in neural network architectures, neural image codecs now outperform standard image codecs. Instead of using CNNs as the basic network structure, transformer-based architectures have become popular, offering the potential for  better compression efficiency.  Notable examples include SwinT-ChARM \\cite{zhu2022transformer}, STF \\cite{zou2022devil} and LIC-TCM \\cite{liu2023learned}. SwinT-ChARM \\cite{zhu2022transformer} employs Swin transformers for non-linear transforms and outperforms the latest standard image codec, the Versatile Video Coding (VVC) Test Model (VTM, All Intra). STF \\cite{zou2022devil} is based on a symmetrical transformer framework containing absolute transformer blocks in both the down-sampling encoder and the up-sampling decoder, which also shows improved rate-quality performance over VTM. LIC-TCM \\cite{liu2023learned} exploits the local modeling ability of CNN and the non-local modeling performance of transformers, and proposes a parallel transformer-CNN mixture block. This new network structure, together with a channel-wise entropy model based on attention modules using Swin transformers, contributes to the superior performance of STF, with a more than 10\\% bitrate saving over VTM. \n\nAn alternative approach to learned image coding is based on advanced generative models. Early works \\cite{agustsson2019generative,mentzer2020high} employed GANs to generate more photo realistic results with improved visual quality. Although these models fail to outperform conventional, CNN-based or transformer-based approaches, when distortion-based quality metrics, e.g., PSNR, are used for performance evaluation, they have been reported to perform well when perceptual quality models, such as MS-SSIM \\cite{Bovik_MSSSIM} and VMAF \\cite{VMAFblog}, or subjective tests are employed to measure perceived video quality. More recently, diffusion models have been applied in image compression to allow realistic reconstruction at ultra-low bitrates \\cite{careil2023towards} achieving competitive performance compared to GAN-based models \\cite{yang2024lossy}. However, it should be noted that some of these generative models aim to generate (or synthesize) images with ``perfect realism'' rather than reconstruct results which are most similar to the original content. Notable work in this category includes image codecs using score-based generative models \\cite{hoogeboom2023high} and the diffusion-based residual augmentation codec (DIRAC) \\cite{ghouse2023residual}. Moreover, another type of generative model based on INR has been employed for image compression; this learns a mapping between the spatial coordinates and the respective pixel values for the input image. The learned INR model is then compressed through parameter quantization and model compression to minimize the required bitrate. Notable INR-based image codecs include COIN/COIN++ \\cite{dupont2021coin,dupontcoin++} and \\cite{strumpler2022implicit} that combine SIREN networks \\cite{sitzmann2020implicit} with positional encoding.   \n\nIn order to evaluate and compare neural image codecs under fair test conditions, public grand challenges have been increasingly run,  typically associated with international conferences. One of the most well-known of these is the Challenge on Learned Image Compression (CLIC) \\cite{clic}. In its latest competition, the best performing learned image codec \\cite{li2024semantic}, which is based on a GAN-enhanced Vector Quantized Variational AutoEncoder (VQ-VAE) framework, offered up to 0.6dB PSNR gain over VTM (version 22.2, All Intra) at similar bitrates; this codec is based on an autoencoder architecture with latent refinement and perceptual losses. \n\nTo support the deployment of neural image codecs, the International Organization for Standardization (ISO)/International Electrotechnical Commission(IEC) has developed a royalty-free learned image coding standard, denoted as JPEG AI \\cite{ascenso2023jpeg}, which aims to offer significant performance improvement over existing standards for both human and machine vision tasks. The Call for Proposals of JPEG AI was published in 2022, while the Working Draft and the Committee Draft outlining its core coding system were released in 2023 \\cite{JPEGAIN100634}, with its first version published in October 2024 \\cite{JPEGAIN100634}. JPEG AI follows the same framework (the auto-encoder structure) as most existing neural image codecs, and its test model JPEG AI VM (version 4.3) has been reported to achieve up to 28.5\\% coding gains over VVC VTM (All Intra mode) \\cite{JPEGAIM101081}.\n\n\\subsubsection{Video Compression}\n\nCompared to image coding, the compression of video content is a much more challenging task, particularly for immersive video formats and diverse content types. Although video coding standards including H.264/AVC (Advanced Video Coding), H.265/HEVC (High Efficiency Video Coding) and H.266/VVC (Versatile Video Coding) are still predominant in real-world applications, learning-based video coding has advanced dramatically in the past five years, with new deep learning enhanced conventional coding tools and end-to-end optimized neural video coding frameworks proposed.    \n\ni) \\textbf{The enhancement of conventional coding tools} focuses on employing deep learning techniques to improve the performance of one (or multiple) coding modules in a standard-applicant codec. These modules include intra prediction \\cite{li2021deepqtmt}, inter prediction \\cite{jin2021deep}, in-loop filtering \\cite{feng2024low}, post filtering \\cite{zhang2023wcdann} and resolution re-sampling \\cite{wang2023compression}. To facilitate efficient integration, the MPEG Joint Video Experts Team (JVET)  built a test model in 2022 based on VTM 11, named Neural Network-based Video Coding (NNVC) \\cite{li2023designs}, with its latest version NNVC-7.1 containing two major learning-based coding tools, neural-network based intra prediction and in-loop filtering, which has achieved an up to 13\\% coding gain over VTM 11 (Random Access mode) \\cite{JVET-AG0014}. However, this learning-based codec requires much higher computational complexity (up to 477 kMACs/pixel) and high-spec GPU support compared to conventional codecs. Meanwhile, members of the Alliance of Open Media (AOM) have also developed multiple CNN-based coding tools for the next generation of video coding standard beyond AV1. The latest proposals focus on the trade-off between performance and complexity, with one of them based on inloop filtering and super-resolution, which achieves an average BD-rate saving of 3.9\\% (in PSNR) over AVM, the test model of AV2, but only requires a much lower computational complexity (below 1.5kMACs/pixel) \\cite{joshi2023switchable}. More recently, research has been conducted to further improve the performance of these learning-based coding tools utilizing more advanced network architectures, including ViTs \\cite{kathariya2023joint}, and diffusion models \\cite{li2024extreme}. There are also investigations on applying preprocessing before compression \\cite{chadha2021deep,tan2024joint}, where the training of the deep preprocessors is based on proxy video codecs and/or rate-distortion loss functions to simulate the behavior of conventional video coding algorithms.\n\nii) \\textbf{End-to-end optimized neural video codecs.} Alongside the enhancement of coding tools in conventional video codecs, more recent research activities have focused on using neural networks to implement the whole coding workflow, enabling data-driven end-to-end optimization. The performance of these neural video codecs has advanced significantly in the last five years, since the first attempt, DVC \\cite{lu2019dvc}, was published. DVC  matched the performance of a fast implementation of H.264 (x264). However currently, learned video coding algorithms (e.g., DCVC-FM \\cite{li2024neural} and DCVC-LCG \\cite{Qi2024longterm}) are able to compete or even outperform the state-of-the-art standard codecs, such as VVC VTM under certain coding configurations. These learning-based methods often focus on enhancement from different perspectives, including feature space conditional coding (e.g., FVC~\\cite{hu2021fvc} and DCVC \\cite{li2021deep}), instance adaptation ~\\cite{khani2021efficient,yang2024parameter}, and motion estimation (e.g., DCVC-DC~\\cite{li2023neural}). New architectures have also been proposed such as CANF-VC~\\cite{ho2022canf} based on a video generative model, MTMT \\cite{xiang2022mimt} using a masked image modeling transformer-based entropy model and VCT \\cite{mentzer2022vct} based on a video compression transformer. It is noted that although promising coding performance has been achieved in the aforementioned works, these neural video codecs (in particular those based on autoencoder backbones) are typically associated with high computational complexity (especially in the decoder), which constrains their deployment for practical applications. To address this issue, researchers attempted to achieve complexity reduction while maintaining the coding performance through model pruning and knowledge distillation~\\cite{guo2023evc,peng2024accelerating}. \n\nIt should be noted that the neural codecs mentioned above are typically trained offline with diverse video content \\cite{nawala2024bvi}, and deployed online for inference. In this case, model generalization becomes important, and this is why these codecs often have a large model capacity, resulting in large model sizes and slow inference runtime. Inspired by recent advances in implicit neural representations (INR), a new type of video codec has emerged that employs INR models to ``represent'' the video  by learning a coordinate-based mapping and compressing the network parameters for transmission. This approach converts a video coding problem into a model compression task, which allows the use of a much smaller network to ``overfit'' the input video, with the real potential for fast decoding. Existing implicit neural video representation (NeRV) models can be classified into index-based and content-based methods. The former takes frame~\\cite{chen2021nerv}, patch~\\cite{bai2023ps} or disentangled spatial/grid coordinates~\\cite{li2022nerv} as model input, while content-based approaches~\\cite{kwan2024hinerv,kim2024c3,leguay2024cool} have content-specific embedding as inputs. Currently, one of the best INR-based video codecs \\cite{kwan2024nvrc} has already achieved a performance similar to that of VVC VTM (RA), but with a much lower decoding complexity compared to autoencoder-based neural codecs. Some of these models have also been applied to volumetric video content~\\cite{ruan2024point,kwan2024immersive}, demonstrating their potential to compete with standard and other learning-based methods. However, it should be noted that the training of most NeRV models is based on an entire video sequence or even datasets; this results in a high system delay and does not meet the requirement of many low latency video streaming or real-time applications. To address this limitation, significant advances  have been made~\\cite{gao2024pnvc} towards more practical INR-based video compression (such as the Low Delay and Random Access modes in VVC VTM \\cite{bossen2023vtmctc}) by combining pre-training and online model overfitting.\n\nSimilarly to image compression, international grand challenges are used to compare neural video compression methods, with notable venues including the NN-based Video Coding Grand Challenge associated with The IEEE International Symposium on Circuits and Systems (ISCAS) and the Challenge on Learned Image Compression (CLIC, video coding track) with IEEE/CVF CVPR and Data Compression Conference (in 2024). The best performer in ISCAS 2024 NN-based Video Coding Grand Challenge offers an overall 55\\% BD-rate saving over HEVC Test Model HM \n \\cite{iscas2024}, while the winner of the CLIC (video coding track) in 2024, a neural-network enhanced ECM codec \\cite{zhao2024neural} with a CNN-based in-loop filter, shows a more than 2dB (in PSNR) gain compared to VTM (RA) at the same bitrates. \n\n\\subsubsection{Audio Compression}\n\nSimilarly to images and videos, learning-based solutions have also been researched to compress audio signals, and most neural audio codecs are based on VQ-VAE \\cite{Oord:Neural:2017}. SoundStream \\cite{zeghidour2021soundstream} is one of such models, which can encode audio content at various bitrates. It is based on a residual vector quantizer (RVQ) which trades off between rate, distortion, and complexity. This work has been further enhanced with a multi-scale spectrogram adversary and a loss balancer mechanism, resulting in improved rate-distortion performance. A more advanced universal model has been further developed \\cite{kumar2024high} based on improved adversarial and reconstruction losses, which can compress different types of audio. RVQ has also been extended from a single scale to multiple scales \\cite{siuzdak2024snac}, which performs hierarchical quantization at variable frame rates. \n\nMore recently, researchers have started to exploit the use of LLMs for audio compression, leveraging the audio generation/synthesis abilities of generative models. UniAudio 1.5 \\cite{yang2024uniaudio} is one of such attempts, which converts an audio into the textural space, which can be represented by a pre-trained LLM that shares a similar backbone of UniAudio \\cite{yang2023uniaudio}, a universal audio foundation model. LFSC is another neural audio codec based on LLMs, which achieved fast LLM training and inference through finite scalar quantization and adversarial training. \n\n\\subsection{Visual Quality Assessment}\n\\label{ssec:asssessment}\n\nAssessing the quality of visual signals remains an important and challenging task for many image and video processing applications. While subjective tests involving human participants remain the gold standard, objective quality models are frequently used  because of their time and cost efficiency. These quality assessment methods are typically used to evaluate the performance of different visual processing approaches, and they can also be converted to loss functions, which are employed for optimizing learning-based processing models.  \n\nIn recent years, quality assessment methods have  been enhanced using deep learning techniques. The resulting learning-based quality models can quickly adapt to a specific type of content, leading to better performance compared to conventional, hand-crafted quality metrics. This section provides a brief summary of existing works in this research area, and highlights the main challenges which should be addressed in the near future. A more comprehensive overview of the image and video quality assessment literature can be found in \\cite{zhai2020perceptual,zheng2024video,zhang2024quality}.\n\n\\subsubsection{Quality assessment models}\n\nImage and video quality assessment methods can be classified into two primary categories according to the availability of the corresponding reference content to the distorted test version: full-reference and no-reference models\\footnote{Reduced-reference quality metrics do existing in the literature, but the research in this field is less active in recent years.}. Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants \\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}), distortion \\cite{chandler2007vsnr,larson2010most,STMAD}, and artifacts \\cite{ou2010perceptual,zhu2014no,zhang2015perception}. In many cases, the extracted features are further processed by models that simulate texture masking \\cite{helmholtz1896handbook}, contrast sensitivity \\cite{kelly1977visual}, and saliency \\cite{itti2001computational}. These hand-crafted quality models have also been combined with features within a regression-based framework in order to achieve more accurate prediction performance - VMAF is one such example \\cite{VMAFblog}. When neural networks are involved for feature extraction, they are trained to capture information which can directly contribute to quality prediction through an end-to-end optimization strategy. Initially, convolutional neural networks were typically used, with notable examples such as DeepQA \\cite{kim2017deep}, LPIPS \\cite{zhang2018unreasonable} and CONTRIQUE \\cite{madhusudana2022image} for image quality assessment, and TLVQA \\cite{korhonen2019two}, C3DVQA \\cite{xu2020c3dvqa} and DeepVQA \\cite{kim2018deep} for video quality assessment. Recent works have been reported to achieve better performance when Vision Transformers (ViTs) (or similar variants) are employed due to the effectiveness of their self-attention mechanism. Important works in this class include IQT \\cite{cheon2021perceptual}, TRes \\cite{golestaneh2022no}, SaTQA \\cite{shi2024transformer}, FastVQA \\cite{wu2022fast} and RankDVQA \\cite{feng2024rankdvqa}. The former has been further extended as DOVER \\cite{wu2023exploringvideo} and COVER \\cite{he2024cover} when aesthetic and/or semantic aspects in the content are taken into account. \n\nMore recently, inspired by the success of large language models (LLMs) \\cite{openai:gpt4:2023,touvron2023llama} in other machine learning tasks, these have been utilized in image and video quality assessment,  demonstrating significant potential to achieve better model generalization. Q-Bench \\cite{wu2024qbench} is one of the first attempts that employs multimodal large language models to predict the perceptual quality of images based on prompt-driven evaluation. It queries the LLMs to provide information related to the final quality rating of the input image and the quality description. This has been further extended for video quality assessment tasks in Q-Align \\cite{wu2024qalign}. Other notable works include X-iqe \\cite{chen2023x} that performs the quality prompt in a multi-iteration manner focusing on both image fidelity and aesthetics. Prompt-based approaches have also been proposed for differentiating the quality difference between multiple images, such as 2AFC-LMMs \\cite{zhu20242afc} based on a two-alternative forced choice prompt and MAP (maximum a posteriori) estimation. Moreover, recent research works also focus on using pre-trained vision-language models, such as CLIP \\cite{radford2021learning}, which align better image and text modalities. Important examples in this class for image quality assessment include ZEN-IQA \\cite{miyata2024zen}, QA-CLIP \\cite{pan2023quality} and PromptIQA \\cite{chen2025promptiqa}. Similar works have also been proposed for video quality assessment, such as BVQI \\cite{wu2023exploring,wu2023towards} and COVER \\cite{he2024cover}.\n\n\n\nTo support the training and validation of learning-based quality assessment models, image or video databases containing ground-truth subjective quality scores are typically employed. Commonly used image quality databases include LIVE \\cite{sheikh2006astatistical}, CSIQ \\cite{larson2010most}, TID2013 \\cite{ponomarenko2013color}, PieAPP and PIPAL, while video quality databases such as LIVE-VQA~\\cite{seshadrinathan2010study}, KoNViD-1K~\\cite{hosu2017konstanz}, YouTube UGC~\\cite{wang2019youtube} and LIVE-VQC~\\cite{sinno2018large} are typically employed for benchmarking in the literature. There are also databases developed that investigate the impact of specific video formats and/or artifacts, such as LIVE-YT-HFR  \\cite{madhusudana2021subjective} focusing on frame rates, VSR-QAD \\cite{zhou2024database} on spatial resolution (or super-resolution artifacts), BAND-2k \\cite{chen2024band2k} on banding artifacts and Maxwell \\cite{wu2023towards}/BVI-Artifact \\cite{feng2024bvi} containing multiple artifacts commonly produced in video streaming. Based on these databases, many learning-based quality assessment models are trained to minimize the difference (L1 or L2 norm) between predicted quality indices and subjective scores. However, due to the limited number of ground-truth quality labels associated with these databases and the resourcing-costing nature for collecting subjective data through human participants involved in psychophysical experiments, this type of training methodology cannot offer satisfactory performance, in particular when the model capacity is large. Moreover, since the experimental settings and conditions used for quality labeling are different in these databases, intra-database cross-validation is always required due to the limited model generalization and potential overfitting problems. \n\nTo address these issues, various proxy quality metrics have been used to label images and videos, which avoid expensive subjective tests and enable the generation of a large amount of training material with pseudo-ground-truth quality annotations. To further improve the reliability of quality labels, instead of learning the absolute values of the quality labels, ranking-inspired training strategies have been developed, which focus on improving the monotonicity characteristics of quality. Important examples based on these weakly supervised training methodologies include RankIQA \\cite{liu2017rankiqa} and UNIQUE \\cite{zhang2021uncertainty} for the image quality assessment task, and VFIPS \\cite{hou2022perceptual} and RankDVQA \\cite{feng2024rankdvqa} for video quality assessment. Moreover, different self-supervised learning approaches have also been employed, which transform quality labeling to an auxiliary task. For example, CONTRIQUE \\cite{madhusudana2022image} learns relevant features from an unannotated image database based on the prediction of distortion types and degrees through contrastive learning. This method has been further applied to video quality assessment, resulting in a contrastive video quality estimator, CONVIQT \\cite{madhusudana2023conviqt}. More recently, quality-aware contrastive loss has been designed in \\cite{zhao2023quality,peng2024rmt} to stabilize the learning process.\n\n\\subsubsection{Performance and main challenges}\n\nDue to the lack of standard test conditions and limited model generalization within many existing image and video quality assessment models, deep compression methods are typically trained and benchmarked using different databases in conjunction with intra-database cross-validation. This can result in inconsistent evaluation results and conclusions. To enable a fair and meaningful comparison, various challenges and contests have been held for visual quality assessment. The Sixth Challenge on Learned Image Compression (CLIC) \\cite{clic} associated with the Data Compression Conference 2024 is one of the latest examples which includes two quality assessment tracks for image and video compression. The best performer in the video quality assessment track achieves a Spearman Ranking Correlation Coefficient value of 0.825 \\cite{feng2024rankdvqa}, which is based on a ranking-inspired training methodology. Other notable challenges include the IEEE/CVF WACV 2023 HDR VQA Grand Challenge and the Video Super-Resolution Quality Assessment Challenge in ECCV 2024, which focus on high dynamic range and super-resolved content, respectively. \n\nAlthough significant progress has been made in the past few years in visual quality assessment, including new models and training methodology, challenges remain, including limited model generalization and high computational complexity. \n\nAnother important use of quality metrics is as embedded loss functions for image and video processing optimization. This requires further capability and robustness, alongside complexity reduction, all topics to be addressed in future work. \n\n", "appendix": false}, "Closing Thoughts and Future of AI in Creativity": {"content": "\n\\label{sec:discussion}\n\nThis paper has presented a comprehensive review of current AI technologies and their applications that have emerged in recent years. Generative methods have driven a rapid growth in AI usage, particularly in the creative sector, significantly advancing the state of the art across various creative applications such as content creation, information extraction and analysis, content enhancement and data compression.\n\nThrough these applications, generative AI has not only broadened creative possibilities but has also reduced the manual effort and time traditionally associated with the production pipeline, allowing for greater creative experimentation and quicker production cycles. As this technology advances, it promises to unlock even more sophisticated capabilities in the creative industry. However, creative technologists, artists and other users must adapt,  learn to use, and build these tools effectively and safely.\n\n\n\\subsection{Challenges for AI in the Creative Sector}\n\nOne of the primary challenges for artists engaging with modern generative AI and LLMs is the lack of consistent, controllable output. These models operate via stochastic sampling from high-dimensional latent spaces, meaning that identical prompts can yield different results across runs. This unpredictability makes it difficult for artists to achieve and iterate toward a precise creative vision. Although prompt engineering has emerged as a technique to guide model behavior, it requires technical knowledge and iterative refinement, which may not align with the intuitive or exploratory approaches common in artistic practice. \n\nMoreover, there is a fundamental tension between the structured nature of current AI pipelines and the nonlinear, often improvisational workflows of creative disciplines. Many generative tools were originally designed for tasks like software development, content automation, or optimization \\cite{zhong:LDB:2024}, and are ill-suited for open-ended, exploratory creation. Artists typically work in cycles of ideation, experimentation, and revision\u2014processes that demand fluid, real-time interaction and control, which existing AI systems struggle to support. These limitations point to a gap in current AI design: a need for systems that not only generate high-quality content but also adapt to the iterative, interpretive nature of artistic production. One possible approach addressed to these challenges is a shift toward top-down creative workflows, where artists define high-level concepts, themes, or goals via text prompts before refining specific outputs. This approach helps align AI-generated results with artistic intent, offering a degree of control over inherently stochastic systems.\n\nSpeaking at the World Government Summit in Dubai in 2024,\\footnote{\\url{https://blogs.nvidia.com/blog/world-governments-summit/}} NVIDIA CEO Jensen Huang argued that with rapid advancements in AI, learning to code may become less essential for newcomers to the tech sector. He envisioned a future where traditional programming could be replaced by more intuitive AI-driven tools, thereby automating complex tasks and enhancing productivity\u2014particularly for artists without coding expertise. While this perspective remains debated, it highlights the potential for AI to become more accessible within creative fields. However, AI-assisted coding tools are insufficient for creative practitioners, as artistic workflows tend to be unstructured and rely on domain-specific data. Artists must hence turn to techniques such as fine-tuning pre-trained models, few-shot learning, or domain adaptation\u2014methods that are powerful yet typically inaccessible without machine learning expertise.\n\nThere is also broader concerns persist regarding the long-term impact of AI on the creative industries, particularly with the potential emergence of artificial general intelligence (AGI). Envisioned by organizations like OpenAI, DeepMind, and Anthropic, AGI could surpass human cognitive abilities, raising ethical and existential questions about the role of human agency in artistic expression.\n\n\n\\subsection{Ethical Issues, Fakes and Bias}\n\n\n\nAdvancements in generative AI, exemplified by models like Sora and Gemini 1.5 Pro, provoke ethical concerns and societal implications. These models, capable of generating highly realistic content, escalate the risk of misuse, through malicious deepfakes and misinformation. We are now in a situation where AI results transcend the uncanny valley, further complicating matters and challenging perceptions of authenticity. For example, the artist Miles Astray demonstrated that even authentic photographs could be mistaken for AI-generated images. His real photograph `F L A M I N G O N E' won both the jury\u2019s award and the people\u2019s choice award in the AI category of the 1839 Awards. His aim wasto highlight the ethical dilemmas inherent in AI, suggesting that the benefits of discussing AI's ethical implications could surpass the ethical concerns related to viewer deception\\footnote{\\url{https://www.milesastray.com/news/newsflash-reclaiming-the-brain}}.\n\nWhile democratizing AI tools no-doubt presents opportunities to transform creative processes and workflows, it also necessitates robust regulatory frameworks to safeguard privacy and ownership. For example, deepfake technologies stimulate  significant concerns about the spread of misinformation and other malicious uses. Efforts to detect and identify increasingly realistic deepfakes are thus as important as the generative methods used to produce them. These must however be accompanied by increased media literacy, and policies that address the ethical and legal implications.\n\nDiversity and representation is a key issue when using AI tools. Unified Concept Editing \\cite{Gandikota:Unified:2024} has been proposed as a basis for image generation in digital mediums. This aims to ensure the production of safe content with diverse representation, reducing gender and racial biases. Hallucination in generative AI (the production of outputs that are not faithful representations of reality but instead contain imagined or unrealistic elements) are a further cause of concern. These undermine trust in AI processes and can be due to limitations in the training data, biases in the model architecture or imperfections in the optimization process. Hallucinations associated with LLMs are one of the issues highlighted by the UK Government  \\cite{UK:Large:2024}, alongside bias, regurgitation of private data, difficulties with multi-step tasks and challenges in interpreting black-box processes.\n\nGovernments across the world  are increasingly expressing concerns about the challenges and uncertainties that generative AI technologies pose to rights holders and human creativity \\cite{Jeary2024}. Generative AI presents substantial legal challenges, including the copyright status of AI-generated work and the intellectual property and copyright implications of the datasets used in training AI models. Viewpoints on this issue do however differ. For example, the track ``Heart on My Sleeve,\" penned by an (as yet unidentified) human author, featured AI-generated vocals that replicated the voices of Drake and The Weeknd. Released independently on April 4, 2023, it was accessible via streaming platforms including Apple Music, Spotify, and YouTube. The song quickly became viral, accumulating over 20 million views across all platforms\\footnote{\\url{https://www.nbcnews.com/pop-culture/viral-ai-powered-drake-weeknd-song-removed-streaming-services-rcna80098}}, prior to its removal by Universal Music Group, Drake's recording label. In contrast, Canadian artist Grimes has extended an invitation to musicians to emulate her voice via AI for the creation of new musical pieces, stipulating that the lyrics should not be harmful. She has advocated for the democratization of art and the abolition of copyright\\footnote{\\url{https://www.bbc.com/news/entertainment-arts-65385382}}. Additionally, Grimes has employed AI to design visual content for her LED backdrop at Coachella in 2024.\n\nFinally, the rapid development of AI technologies has also raised concerns about job displacement and the balance between automation and human participation in creative processes. Ensuring that AI augments, rather than undermines, human effort poses a significant challenge for developers and policymakers. \n\n\n\n\\subsection{The future of AI technologies}\n\nSeveral key technological issues remain which need to be addressed if AI is to deliver its full potential. These in particular relate to training data, computational complexity and their depth of reasoning or planning, and are discussed below.\n\nA substantial amount of data is essential for training AI models in order to achieve high performance and good generalisation. Major companies such as Google, Meta, and NVIDIA, with their respective models: BERT, Segment Anything, and Canvas, dominate this space, benefiting from leveraged resources to gather data and process it to train sophisticated models. However,  in November 2024, Bloomberg reported that OpenAI, Anthropic, and Google are all experiencing relatively slow growth in the performance of their AI models, with one of the key challenges being training data\\footnote{\\url{https://www.bloomberg.com/news/articles/2024-11-13/openai-google-and-anthropic-are-struggling-to-build-more-advanced-ai}}.\n\nLLMs excel in applications involving complex tasks, advanced reasoning, data analysis, and understanding context. \nHowever, these models typically require high computational resources or cloud computing for development, operation and fine-tuning. A new trend emerging in alongside LLMs is the development of Small language models (SLMs), such as Phi-3 by Microsoft\\footnote{\\url{https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/}}. SLMs offer promising solutions for regulated industries and sectors encountering scenarios where high-quality results are essential while keeping data 'on-site'. Their potential is particularly relevant when deploying more capable SLMs on smartphones and other mobile devices, allowing them to operate `at the edge' without relying on cloud connectivity. Recent highly successful platforms, such as  DeepSeek-V3 \\cite{deepseekv3} and Qwen2.5-Max \\cite{qwen25}, are based on Mixture-of-Experts (MoE) models, which tackle complex problems by dividing them into simpler sub-tasks, each handled by a specialized ``expert.\"\n\nDespite the evident advancements in AI, current models still struggle with tasks requiring planning or deep reasoning and are prone to errors when encountering unexpected data. This, in turn, reduces the confidence of users and trust in the results.  AI algorithms can learn through reinforcement learning, but this process often identifies the best outcome as an anomaly rather than the norm. Yann LeCun, Professor at NYU and Chief AI Scientist at Meta, noted that while LLMs show a degree of comprehension in processing and generating text, their understanding lacks depth, often leading to results that defy common sense\\footnote{\\url{https://twitter.com/ylecun/status/1728496457601183865}}. He advocates for self-supervised learning as a pivotal future direction for AI, emphasizing its potential to derive insights from unlabeled data. Concurrently, Andrew Ng, Adjunct Professor at Stanford University and Founder of DeepLearning.AI, sees iterative AI agentic workflows\\footnote{\\url{https://www.youtube.com/watch?v=sal78ACtGTc}} as a key advancement for enhancing AI tool capabilities through an interactive approach by AI agents. These workflows involve autonomous agents that interactively learn from experience, understand natural language, and execute tasks on behalf of users.\n\nThe increasing openness of code and datasets is seen by many as a catalyst for accelerating AI advancements, with major firms like Microsoft, Google, and Meta supporting open access technologies. However, this openness also introduces security risks, necessitating new regulatory measures to monitor models post-release, to standardize documentation, and to assess the safety of  software code and training data disclosure.\n\nFinally, as stated in \\cite{Jeary2024}, the rapid advancement of AI technologies has revolutionized cultural experiences, often referred to as `CreaTech'\u2014the convergence of the creative and digital sectors \\cite{CreativeIndustriesCouncil2021}. Such innovations not only reshape how people engage with art and creative work (e.g., through AR/VR/MR) but also drive the evolution of the technologies themselves.\n\n\n\n\n\\bibliographystyle{IEEEbib}IEEEbib\n\\bibliography{literature_review}\n\n", "appendix": false}}, "categories": ["cs.AI"], "published": "2025-01-06 02:46:33+00:00", "primary_category": "cs.AI", "summary": "The rapid advancements in artificial intelligence (AI), particularly in\ngenerative AI and large language models (LLMs), have profoundly impacted the\ncreative industries, enabling more innovative content creation, enhancing\nworkflows, and democratizing access to creative tools. This paper explores\nthese technological shifts, with particular focus on how those that have\nemerged since our previous review in 2022 have expanded creative opportunities\nand improved efficiency. These technological advancements have enhanced the\ncapabilities of text-to-image, text-to-video, and multimodal generation\ntechnologies. In particular, key breakthroughs in LLMs have established new\nbenchmarks in conversational AI, while advancements in image generators have\nrevolutionized content creation. We also discuss the integration of AI into\npost-production workflows, which has significantly accelerated and improved\ntraditional processes. Once content has been created, it must be delivered to\nits audiences the media industry is facing the demands of increased\ncommunication traffic due to creative content. We therefore include a\ndiscussion of how AI is beginning to transform the way we represent and\ncompress media content. We highlight the trend toward unified AI frameworks\ncapable of addressing and integrating multiple creative tasks, and we\nunderscore the importance of human insight to drive the creative process and\noversight to mitigate AI-generated inaccuracies. Finally, we explore AI's\nfuture potential in the creative sector, stressing the need to navigate\nemerging challenges and to maximize its benefits while addressing the\nassociated risks."}