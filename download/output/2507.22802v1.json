{"title": "Advancing Fetal Ultrasound Image Quality Assessment in Low-Resource Settings", "author": "Dongli He", "abstract": "\\begin{abstract}\nAccurate fetal biometric measurements, such as abdominal circumference, play a vital role in prenatal care. However, obtaining high-quality ultrasound images for these measurements heavily depends on the expertise of sonographers, posing a significant challenge in low-income countries due to the scarcity of trained personnel. To address this issue, we leverage FetalCLIP, a vision-language model pretrained on a curated dataset of over 210,000 fetal ultrasound image-caption pairs, to perform automated fetal ultrasound image quality assessment (IQA) on blind-sweep ultrasound data. We introduce FetalCLIP\\textsubscript{CLS}, an IQA model adapted from FetalCLIP using Low-Rank Adaptation (LoRA), and evaluate it on the ACOUSLIC-AI dataset against six CNN and Transformer baselines. FetalCLIP\\textsubscript{CLS} achieves the highest F1 score of 0.757. Moreover, we show that an adapted segmentation model, when repurposed for classification, further improves performance, achieving an F1 score of 0.771. Our work demonstrates how parameter-efficient fine-tuning of fetal ultrasound foundation models can enable task-specific adaptations, advancing prenatal care in resource-limited settings. The experimental code is available at: \\url{https://github.com/donglihe-hub/FetalCLIP-IQA}.\n\n\\keywords{Fetal ultrasound \\and Image quality assessment \\and Parameter-efficient fine-tuning.}\n% Authors must provide keywords and are not allowed to remove this Keyword section.\n\n\\end{abstract}", "citations": {"Whitworth2010": {"bib_key": "Whitworth2010", "bib_title": "10.1002/14651858.CD007058.pub3", "bib_author ": "Whitworth, M., Bricker, L., Neilson, J., Dowswell, T.: Ultrasound for fetal assessment in early pregnancy. Cochrane Database of Systematic Reviews  (2010).", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Ultrasound has long played a vital role in routine prenatal care~\\cite{Whitworth2010}", "next_context": "."}], "importance_score": 1.0}, "Self2022CALOPUS": {"bib_key": "Self2022CALOPUS", "bib_title": "11", "bib_author ": "Self, A., Chen, Q., Desiraju, B.K., Dhariwal, S., Gleed, A.D., Mishra, D., Thiruvengadam, R., Chandramohan, V., Craik, R., Wilden, E., Khurana, A., Group, T.C.S., Bhatnagar, S., Papageorghiou, A.T., Noble, J.A.: Developing clinical artificial intelligence for obstetric ultrasound to improve access in underserved regions: Protocol for a computer-assisted low-cost point-of-care ultrasound (calopus) study. JMIR Res Protoc", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "In regions where experienced personnel are scarce, blind-sweep ultrasound, typically performed by novice operators using low-cost portable probes, is often employed to collect obstetric scans~\\cite{Self2022CALOPUS}", "next_context": "."}], "importance_score": 1.0}, "7875138": {"bib_key": "7875138", "bib_title": "47", "bib_author ": "Wu, L., Cheng, J.Z., Li, S., Lei, B., Wang, T., Ni, D.: Fuiqa: Fetal ultrasound image quality assessment with deep convolutional networks. IEEE Transactions on Cybernetics", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "One important application is automated assessment of fetal ultrasound image quality~\\cite{7875138}", "next_context": ", which is particularly useful for free-hand ultrasound sequences acquired through blind-sweep protocols as it enables accurate biometric measurements even in the absence of skilled sonographers."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "~\\cite{7875138}", "next_context": "employ two CNNs to assess ultrasound scans of the fetal abdominal region, demonstrating performance comparable to subjective ratings from medical experts."}], "importance_score": 2.0}, "maani2025fetalclipvisuallanguagefoundationmodel": {"bib_key": "maani2025fetalclipvisuallanguagefoundationmodel", "bib_title": "https://arxiv.org/abs/2502.14807", "bib_author ": "Maani, F., Saeed, N., Saleem, T., Farooq, Z., Alasmawi, H., Diehl, W., Mohammad, A., Waring, G., Valappi, S., Bricker, L., Yaqub, M.: Fetalclip: A visual-language foundation model for fetal ultrasound image analysis (2025),", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "In this work, we build our models upon FetalCLIP~\\cite{maani2025fetalclipvisuallanguagefoundationmodel}", "next_context": ", a vision-language foundation model specifically trained for fetal ultrasound, to address the task of fetal ultrasound image quality assessment (IQA)."}, {"section": "Methodology", "subsection": "Model Architecture", "subsubsection": null, "prev_context": "We utilize the image encoder from FetalCLIP~\\cite{maani2025fetalclipvisuallanguagefoundationmodel}", "next_context": ", which was trained on over 210,000 fetal ultrasound image-caption pairs using CLIP~\\cite{radford2021learningtransferablevisualmodels}."}], "importance_score": 2.0}, "hu2021loralowrankadaptationlarge": {"bib_key": "hu2021loralowrankadaptationlarge", "bib_title": "https://arxiv.org/abs/2106.09685", "bib_author ": "Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.: Lora: Low-rank adaptation of large language models (2021),", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "To enable efficient model adaptation, we employ parameter-efficient fine-tuning using Low-Rank Adaptation (LoRA)~\\cite{hu2021loralowrankadaptationlarge}", "next_context": ", which allows efficient task-specific tuning with minimal trainable parameters."}], "importance_score": 1.0}, "ma2025surveyimagequalityassessment": {"bib_key": "ma2025surveyimagequalityassessment", "bib_title": "https://arxiv.org/abs/2502.08540", "bib_author ": "Ma, C., Shi, Z., Lu, Z., Xie, S., Chao, F., Sui, Y.: A survey on image quality assessment: Insights, analysis, and future outlook (2025),", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Methods for image quality assessment (IQA)~\\cite{ma2025surveyimagequalityassessment}", "next_context": "can be broadly classified into two categories: statistical approaches and machine learning-based techniques."}], "importance_score": 1.0}, "1284395": {"bib_key": "1284395", "bib_title": "13", "bib_author ": "Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "~\\cite{1284395}", "next_context": ", transform domain-based techniques like BLIINDS-II~\\cite{6172573}, and natural scene statistics-based approaches such as the Information Fidelity Criterion (IFC)~\\cite{1576816}."}], "importance_score": 1.0}, "6172573": {"bib_key": "6172573", "bib_title": "21", "bib_author ": "Saad, M.A., Bovik, A.C., Charrier, C.: Blind image quality assessment: A natural scene statistics approach in the dct domain. IEEE Transactions on Image Processing", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "~\\cite{1284395}, transform domain-based techniques like BLIINDS-II~\\cite{6172573}", "next_context": ", and natural scene statistics-based approaches such as the Information Fidelity Criterion (IFC)~\\cite{1576816}."}], "importance_score": 1.0}, "1576816": {"bib_key": "1576816", "bib_title": "15", "bib_author ": "Sheikh, H., Bovik, A.: Image information and visual quality. IEEE Transactions on Image Processing", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "~\\cite{1284395}, transform domain-based techniques like BLIINDS-II~\\cite{6172573}, and natural scene statistics-based approaches such as the Information Fidelity Criterion (IFC)~\\cite{1576816}", "next_context": "."}], "importance_score": 1.0}, "6272356": {"bib_key": "6272356", "bib_title": "21", "bib_author ": "Mittal, A., Moorthy, A.K., Bovik, A.C.: No-reference image quality assessment in the spatial domain. IEEE Transactions on Image Processing", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Traditional machine learning methods such as BRISQUE~\\cite{6272356}", "next_context": "rely on hand-crafted features derived from natural scene statistics to estimate image quality."}], "importance_score": 1.0}, "6909620": {"bib_key": "6909620", "bib_title": "10.1109/CVPR.2014.224", "bib_author ": "Kang, L., Ye, P., Li, Y., Doermann, D.: Convolutional neural networks for no-reference image quality assessment. In: 2014 IEEE Conference on Computer Vision and Pattern Recognition. pp. 1733--1740 (2014).", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "CNN-based methods such as IQA-CNN~\\cite{6909620}", "next_context": "extract deep features from image data to improve prediction accuracy."}], "importance_score": 1.0}, "yang2022maniqamultidimensionattentionnetwork": {"bib_key": "yang2022maniqamultidimensionattentionnetwork", "bib_title": "https://arxiv.org/abs/2204.08958", "bib_author ": "Yang, S., Wu, T., Shi, S., Lao, S., Gong, Y., Cao, M., Wang, J., Yang, Y.: Maniqa: Multi-dimension attention network for no-reference image quality assessment (2022),", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "More recently, Transformer-based methods like TRIQ~\\cite{yang2022maniqamultidimensionattentionnetwork}", "next_context": "have been proposed to address the locality bias inherent in CNNs by capturing long-range dependencies, leading to improved performance on IQA tasks."}], "importance_score": 1.0}, "cengiz2023fusqafetalultrasoundsegmentation": {"bib_key": "cengiz2023fusqafetalultrasoundsegmentation", "bib_title": "https://arxiv.org/abs/2303.04418", "bib_author ": "Cengiz, S., Almakky, I., Yaqub, M.: Fusqa: Fetal ultrasound segmentation quality assessment (2023),", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "~\\cite{cengiz2023fusqafetalultrasoundsegmentation}", "next_context": "propose an automatic method to evaluate the quality of predicted segmentation masks."}], "importance_score": 1.0}, "boumeridja2025enhancing": {"bib_key": "boumeridja2025enhancing", "bib_title": "15", "bib_author ": "Boumeridja, H., Ammar, M., Alzubaidi, M., Mahmoudi, S., Benamer, L.N., Agus, M., Househ, M., Lekadir, K., Daho, M.E.H.: Enhancing fetal ultrasound image quality and anatomical plane recognition in low-resource settings using super-resolution models. Scientific Reports", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "~\\cite{boumeridja2025enhancing}", "next_context": "introduce a super-resolution technique that enhances ultrasound image resolution to improve downstream classification performance in low-resource settings."}], "importance_score": 1.0}, "sendrabalcells2023generalisabilityfetalultrasounddeep": {"bib_key": "sendrabalcells2023generalisabilityfetalultrasounddeep", "bib_title": "https://arxiv.org/abs/2209.09610", "bib_author ": "Sendra-Balcells, C., Campello, V.M., Torrents-Barrena, J., Ahmed, Y.A., Elattar, M., Botwe, B.O., Nyangulu, P., Stones, W., Ammar, M., Benamer, L.N., Kisembo, H.N., Sereke, S.G., Wanyonyi, S.Z., Temmerman, M., Gratac\u00f3s, E., Bonet, E., Eixarch, E., Mikolaj, K., Tolsgaard, M.G., Lekadir, K.: Generalisability of fetal ultrasound deep learning models to low-resource imaging settings in five african countries (2023),", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "The majority of models for fetal ultrasound IQA have adopted pretrained models due to their superior performance~\\cite{sendrabalcells2023generalisabilityfetalultrasounddeep}", "next_context": "."}], "importance_score": 1.0}, "radford2021learningtransferablevisualmodels": {"bib_key": "radford2021learningtransferablevisualmodels", "bib_title": "https://arxiv.org/abs/2103.00020", "bib_author ": "Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from natural language supervision (2021),", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "~\\cite{radford2021learningtransferablevisualmodels}", "next_context": "."}, {"section": "Methodology", "subsection": "Model Architecture", "subsubsection": null, "prev_context": "~\\cite{radford2021learningtransferablevisualmodels}", "next_context": "."}], "importance_score": 2.0}, "hatamizadeh2021unetrtransformers3dmedical": {"bib_key": "hatamizadeh2021unetrtransformers3dmedical", "bib_title": "https://arxiv.org/abs/2103.10504", "bib_author ": "Hatamizadeh, A., Tang, Y., Nath, V., Yang, D., Myronenko, A., Landman, B., Roth, H., Xu, D.: Unetr: Transformers for 3d medical image segmentation (2021),", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Leveraging Segmentation Model for Classification Task", "subsubsection": null, "prev_context": "~\\cite{hatamizadeh2021unetrtransformers3dmedical}", "next_context": "."}], "importance_score": 1.0}, "SAPPIA2025103640": {"bib_key": "SAPPIA2025103640", "bib_title": "de Korte", "bib_author ": "Sappia, M.S.,", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Dataset", "subsubsection": null, "prev_context": "We use a 2D B-mode fetal ultrasound dataset from the MICCAI 2024 ACOUSLIC-AI Challenge~\\cite{SAPPIA2025103640}", "next_context": "."}], "importance_score": 1.0}, "10.5555/180895.180940": {"bib_key": "10.5555/180895.180940", "bib_title": "thebibliography", "bib_author ": "Zuiderveld, K.: Contrast limited adaptive histogram equalization, p. 474\u2013485. Academic Press Professional, Inc., USA (1994)", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Implementation Details", "subsubsection": null, "prev_context": "Data augmentation is then applied to the training set, including color jittering, contrast-limited adaptive histogram equalization (CLAHE)~\\cite{10.5555/180895.180940}", "next_context": ", and affine transformations."}], "importance_score": 1.0}, "loshchilov2019decoupledweightdecayregularization": {"bib_key": "loshchilov2019decoupledweightdecayregularization", "bib_title": "https://arxiv.org/abs/1711.05101", "bib_author ": "Loshchilov, I., Hutter, F.: Decoupled weight decay regularization (2019),", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Implementation Details", "subsubsection": null, "prev_context": "We train all models using the AdamW optimizer~\\cite{loshchilov2019decoupledweightdecayregularization}", "next_context": "with a fixed learning rate of3\\times10^-4for 5 epochs."}], "importance_score": 1.0}, "milletari2016vnetfullyconvolutionalneural": {"bib_key": "milletari2016vnetfullyconvolutionalneural", "bib_title": "https://arxiv.org/abs/1606.04797", "bib_author ": "Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks for volumetric medical image segmentation (2016),", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Implementation Details", "subsubsection": null, "prev_context": "All classification models are optimized using binary cross-entropy loss, while FetalCLIP\\textsubscriptSEGis trained with the Dice loss~\\cite{milletari2016vnetfullyconvolutionalneural}", "next_context": "."}], "importance_score": 1.0}, "huang2018denselyconnectedconvolutionalnetworks": {"bib_key": "huang2018denselyconnectedconvolutionalnetworks", "bib_title": "https://arxiv.org/abs/1608.06993", "bib_author ": "Huang, G., Liu, Z., van~der Maaten, L., Weinberger, K.Q.: Densely connected convolutional networks (2018),", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Results", "subsubsection": null, "prev_context": "We compare FetalCLIP\\textsubscriptCLSagainst six baseline models: three CNN-based models\u2014DenseNet~\\cite{huang2018denselyconnectedconvolutionalnetworks}", "next_context": ", EfficientNet~\\cite{tan2020efficientnetrethinkingmodelscaling}, and VGG~\\cite{simonyan2015deepconvolutionalnetworkslargescale}\u2014and three Transformer-based models\u2014Swin Transformer~\\cite{liu2021swintransformerhierarchicalvision}, DeiT~\\cite{touvron2022deitiiirevengevit}, and Vision Transformer (ViT)~\\cite{dosovitskiy2021imageworth16x16words}."}], "importance_score": 1.0}, "tan2020efficientnetrethinkingmodelscaling": {"bib_key": "tan2020efficientnetrethinkingmodelscaling", "bib_title": "https://arxiv.org/abs/1905.11946", "bib_author ": "Tan, M., Le, Q.V.: Efficientnet: Rethinking model scaling for convolutional neural networks (2020),", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Results", "subsubsection": null, "prev_context": "We compare FetalCLIP\\textsubscriptCLSagainst six baseline models: three CNN-based models\u2014DenseNet~\\cite{huang2018denselyconnectedconvolutionalnetworks}, EfficientNet~\\cite{tan2020efficientnetrethinkingmodelscaling}", "next_context": ", and VGG~\\cite{simonyan2015deepconvolutionalnetworkslargescale}\u2014and three Transformer-based models\u2014Swin Transformer~\\cite{liu2021swintransformerhierarchicalvision}, DeiT~\\cite{touvron2022deitiiirevengevit}, and Vision Transformer (ViT)~\\cite{dosovitskiy2021imageworth16x16words}."}], "importance_score": 1.0}, "simonyan2015deepconvolutionalnetworkslargescale": {"bib_key": "simonyan2015deepconvolutionalnetworkslargescale", "bib_title": "https://arxiv.org/abs/1409.1556", "bib_author ": "Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition (2015),", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Results", "subsubsection": null, "prev_context": "We compare FetalCLIP\\textsubscriptCLSagainst six baseline models: three CNN-based models\u2014DenseNet~\\cite{huang2018denselyconnectedconvolutionalnetworks}, EfficientNet~\\cite{tan2020efficientnetrethinkingmodelscaling}, and VGG~\\cite{simonyan2015deepconvolutionalnetworkslargescale}", "next_context": "\u2014and three Transformer-based models\u2014Swin Transformer~\\cite{liu2021swintransformerhierarchicalvision}, DeiT~\\cite{touvron2022deitiiirevengevit}, and Vision Transformer (ViT)~\\cite{dosovitskiy2021imageworth16x16words}."}], "importance_score": 1.0}, "liu2021swintransformerhierarchicalvision": {"bib_key": "liu2021swintransformerhierarchicalvision", "bib_title": "https://arxiv.org/abs/2103.14030", "bib_author ": "Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows (2021),", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Results", "subsubsection": null, "prev_context": "We compare FetalCLIP\\textsubscriptCLSagainst six baseline models: three CNN-based models\u2014DenseNet~\\cite{huang2018denselyconnectedconvolutionalnetworks}, EfficientNet~\\cite{tan2020efficientnetrethinkingmodelscaling}, and VGG~\\cite{simonyan2015deepconvolutionalnetworkslargescale}\u2014and three Transformer-based models\u2014Swin Transformer~\\cite{liu2021swintransformerhierarchicalvision}", "next_context": ", DeiT~\\cite{touvron2022deitiiirevengevit}, and Vision Transformer (ViT)~\\cite{dosovitskiy2021imageworth16x16words}."}], "importance_score": 1.0}, "touvron2022deitiiirevengevit": {"bib_key": "touvron2022deitiiirevengevit", "bib_title": "https://arxiv.org/abs/2204.07118", "bib_author ": "Touvron, H., Cord, M., J\u00e9gou, H.: Deit iii: Revenge of the vit (2022),", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Results", "subsubsection": null, "prev_context": "We compare FetalCLIP\\textsubscriptCLSagainst six baseline models: three CNN-based models\u2014DenseNet~\\cite{huang2018denselyconnectedconvolutionalnetworks}, EfficientNet~\\cite{tan2020efficientnetrethinkingmodelscaling}, and VGG~\\cite{simonyan2015deepconvolutionalnetworkslargescale}\u2014and three Transformer-based models\u2014Swin Transformer~\\cite{liu2021swintransformerhierarchicalvision}, DeiT~\\cite{touvron2022deitiiirevengevit}", "next_context": ", and Vision Transformer (ViT)~\\cite{dosovitskiy2021imageworth16x16words}."}], "importance_score": 1.0}, "dosovitskiy2021imageworth16x16words": {"bib_key": "dosovitskiy2021imageworth16x16words", "bib_title": "https://arxiv.org/abs/2010.11929", "bib_author ": "Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale (2021),", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Results", "subsubsection": null, "prev_context": "We compare FetalCLIP\\textsubscriptCLSagainst six baseline models: three CNN-based models\u2014DenseNet~\\cite{huang2018denselyconnectedconvolutionalnetworks}, EfficientNet~\\cite{tan2020efficientnetrethinkingmodelscaling}, and VGG~\\cite{simonyan2015deepconvolutionalnetworkslargescale}\u2014and three Transformer-based models\u2014Swin Transformer~\\cite{liu2021swintransformerhierarchicalvision}, DeiT~\\cite{touvron2022deitiiirevengevit}, and Vision Transformer (ViT)~\\cite{dosovitskiy2021imageworth16x16words}", "next_context": "."}], "importance_score": 1.0}, "5206848": {"bib_key": "5206848", "bib_title": "10.1109/CVPR.2009.5206848", "bib_author ": "Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition. pp. 248--255 (2009).", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Results", "subsubsection": null, "prev_context": "\\hlineThe CNN baselines are pretrained on ImageNet-1K~\\cite{5206848}", "next_context": ", and we perform full-parameter fine-tuning for these models."}], "importance_score": 1.0}, "Cherti_2023": {"bib_key": "Cherti_2023", "bib_title": "10.1109/cvpr52729.2023.00276", "bib_author ": "Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C., Schuhmann, C., Schmidt, L., Jitsev, J.: Reproducible scaling laws for contrastive language-image learning. In: 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). p. 2818\u20132829. IEEE (Jun 2023).", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Results", "subsubsection": null, "prev_context": "Swin Transformer and DeiT are pretrained on ImageNet-22K, while the ViT baseline is pretrained on WIT-400M image-text pairs using CLIP~\\cite{Cherti_2023}", "next_context": "."}], "importance_score": 1.0}}, "refs": [], "table": [{"original": "\\begin{table}[h]\n\\caption{\nModel performance on fetal ultrasound IQA. We compare FetalCLIP\\textsubscript{CLS} against six baseline models. Metrics are reported as mean \u00b1 standard deviation across five independent runs. The last column indicates the number of trainable parameters for each model. The best average scores for each metric are \\textbf{bolded}, while second-best scores are \\underline{underlined}.\n} \\label{tab:model_performance}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{|l|l|c|c|c|c|c|}\n\\hline\n\\textbf{Architecture} & \\textbf{Models} & \\textbf{Accuracy}$\\uparrow$ & \\textbf{F1 Score}$\\uparrow$ & \\textbf{Precision}$\\uparrow$ & \\textbf{Recall}$\\uparrow$ & \\textbf{\\# Trainable} \\\\\n\\hline\n\\multirow{3}{*}{CNN} & DenseNet & $0.9516 \\pm 0.002$ & $0.7024 \\pm 0.028$ & $\\underline{0.7805} \\pm 0.026$ & $0.6420 \\pm 0.059$ & 7.0 M \\\\\n& EfficientNet & $0.9537 \\pm 0.004$ & $0.7253 \\pm 0.030$ & $0.7725 \\pm 0.025$ & $0.6855 \\pm 0.053$ & 4.0 M \\\\\n& VGG & $0.9510 \\pm 0.002$ & $0.7084 \\pm 0.021$ & $0.7580 \\pm 0.023$ & $0.6671 \\pm 0.048$ & 134 M \\\\\n\\hline\n% MobileNet & $0.9474 \\pm 0.0012$ & $0.6727 \\pm 0.0301$ & $0.7656 \\pm 0.0485$ & $0.6111 \\pm 0.0874$ & 1.5 M \\\\\n\\multirow{4}{*}{Transformer} & Swin & $0.9565 \\pm 0.003$ & $0.7429 \\pm 0.039$ & $\\textbf{0.7864} \\pm 0.032$ & $0.7113 \\pm 0.087$ & 1.7 M \\\\\n& DEIT & $0.9554 \\pm 0.001$ & $0.7466 \\pm 0.014$ & $0.7619 \\pm 0.035$ & $0.7363 \\pm 0.059$ & 2.4 M \\\\\n% ViT & $\\underline{0.9563} \\pm 0.0021$ & $\\underline{0.7388} \\pm 0.0237$ & $\\textbf{0.7935} \\pm 0.0166$ & $\\underline{0.6936} \\pm 0.0537$ & 2.8 M\\\\\n& ViT\\textsubscript{400M} & $\\underline{0.9560} \\pm 0.003$ & $\\underline{0.7506} \\pm 0.019$ & $0.7657 \\pm 0.042$ & $\\textbf{0.7417} \\pm 0.067$ & 2.4 M \\\\\n& FetalCLIP\\textsubscript{CLS} & $\\textbf{0.9575} \\pm 0.001$ & $\\textbf{0.7570} \\pm 0.007$ & $0.7782 \\pm 0.034$ & $\\underline{0.7397} \\pm 0.041$ & 2.4 M \\\\\n\\hline\n\\end{tabular}\n}\n\\end{table}", "caption": "\\caption{\nModel performance on fetal ultrasound IQA. We compare FetalCLIP\\textsubscript{CLS} against six baseline models. Metrics are reported as mean \u00b1 standard deviation across five independent runs. The last column indicates the number of trainable parameters for each model. The best average scores for each metric are \\textbf{bolded}, while second-best scores are \\underline{underlined}.\n}", "label": "\\label{tab:model_performance}", "tabular": "\\begin{tabular}{|l|l|c|c|c|c|c|}\n\\hline\n\\textbf{Architecture} & \\textbf{Models} & \\textbf{Accuracy}$\\uparrow$ & \\textbf{F1 Score}$\\uparrow$ & \\textbf{Precision}$\\uparrow$ & \\textbf{Recall}$\\uparrow$ & \\textbf{\\# Trainable} \\\\\n\\hline\n\\multirow{3}{*}{CNN} & DenseNet & $0.9516 \\pm 0.002$ & $0.7024 \\pm 0.028$ & $\\underline{0.7805} \\pm 0.026$ & $0.6420 \\pm 0.059$ & 7.0 M \\\\\n& EfficientNet & $0.9537 \\pm 0.004$ & $0.7253 \\pm 0.030$ & $0.7725 \\pm 0.025$ & $0.6855 \\pm 0.053$ & 4.0 M \\\\\n& VGG & $0.9510 \\pm 0.002$ & $0.7084 \\pm 0.021$ & $0.7580 \\pm 0.023$ & $0.6671 \\pm 0.048$ & 134 M \\\\\n\\hline\n% MobileNet & $0.9474 \\pm 0.0012$ & $0.6727 \\pm 0.0301$ & $0.7656 \\pm 0.0485$ & $0.6111 \\pm 0.0874$ & 1.5 M \\\\\n\\multirow{4}{*}{Transformer} & Swin & $0.9565 \\pm 0.003$ & $0.7429 \\pm 0.039$ & $\\textbf{0.7864} \\pm 0.032$ & $0.7113 \\pm 0.087$ & 1.7 M \\\\\n& DEIT & $0.9554 \\pm 0.001$ & $0.7466 \\pm 0.014$ & $0.7619 \\pm 0.035$ & $0.7363 \\pm 0.059$ & 2.4 M \\\\\n% ViT & $\\underline{0.9563} \\pm 0.0021$ & $\\underline{0.7388} \\pm 0.0237$ & $\\textbf{0.7935} \\pm 0.0166$ & $\\underline{0.6936} \\pm 0.0537$ & 2.8 M\\\\\n& ViT\\textsubscript{400M} & $\\underline{0.9560} \\pm 0.003$ & $\\underline{0.7506} \\pm 0.019$ & $0.7657 \\pm 0.042$ & $\\textbf{0.7417} \\pm 0.067$ & 2.4 M \\\\\n& FetalCLIP\\textsubscript{CLS} & $\\textbf{0.9575} \\pm 0.001$ & $\\textbf{0.7570} \\pm 0.007$ & $0.7782 \\pm 0.034$ & $\\underline{0.7397} \\pm 0.041$ & 2.4 M \\\\\n\\hline\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[h]\n\\caption{\nModel performance of FetalCLIP\\textsubscript{CLS} and FetalCLIP\\textsubscript{SEG}. The best average scores for each metric are \\textbf{bolded}.\n} \\label{tab:seg_model}\n\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{|l|c|c|c|c|c|c|}\n\\hline\n\\textbf{Models} & \\textbf{DICE}$\\uparrow$ & \\textbf{Accuracy}$\\uparrow$ & \\textbf{F1 Score}$\\uparrow$ & \\textbf{Precision}$\\uparrow$ & \\textbf{Recall}$\\uparrow$ & \\# \\textbf{Trainable}\\\\\n\\hline\nFetalCLIP\\textsubscript{CLS} & / & $\\textbf{0.9575} \\pm 0.001$ & $0.7570 \\pm 0.007$ & $\\textbf{0.7782} \\pm 0.034$ & $0.7397 \\pm 0.041$ & 2.4 M \\\\\n\\hline\nFetalCLIP\\textsubscript{SEG} & $0.7244 \\pm 0.007$ & $0.9543 \\pm 0.001$ & $\\textbf{0.7708} \\pm 0.005$ & $0.6988 \\pm 0.010$ & $\\textbf{0.8599} \\pm 0.017$ & 4.0 M \\\\\n\\hline\n\\end{tabular}\n}\n\\end{table}", "caption": "\\caption{\nModel performance of FetalCLIP\\textsubscript{CLS} and FetalCLIP\\textsubscript{SEG}. The best average scores for each metric are \\textbf{bolded}.\n}", "label": "\\label{tab:seg_model}", "tabular": "\\begin{tabular}{|l|c|c|c|c|c|c|}\n\\hline\n\\textbf{Models} & \\textbf{DICE}$\\uparrow$ & \\textbf{Accuracy}$\\uparrow$ & \\textbf{F1 Score}$\\uparrow$ & \\textbf{Precision}$\\uparrow$ & \\textbf{Recall}$\\uparrow$ & \\# \\textbf{Trainable}\\\\\n\\hline\nFetalCLIP\\textsubscript{CLS} & / & $\\textbf{0.9575} \\pm 0.001$ & $0.7570 \\pm 0.007$ & $\\textbf{0.7782} \\pm 0.034$ & $0.7397 \\pm 0.041$ & 2.4 M \\\\\n\\hline\nFetalCLIP\\textsubscript{SEG} & $0.7244 \\pm 0.007$ & $0.9543 \\pm 0.001$ & $\\textbf{0.7708} \\pm 0.005$ & $0.6988 \\pm 0.010$ & $\\textbf{0.8599} \\pm 0.017$ & 4.0 M \\\\\n\\hline\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[ht]\n\\centering\n\\caption{Comparison of fine-tuning strategies for DenseNet and ViT. LP and FP denote linear probing and full-parameter fine-tuning respectively. The best average scores for each metric per model are \\textbf{bolded}.}\n\n\\label{tab:ablation}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{|l|c|c|c|c|c|c|}\n\\hline\n\\textbf{Model} & \\textbf{Strategy} & \\textbf{Accuracy}$\\uparrow$ & \\textbf{F1 Score}$\\uparrow$ & \\textbf{Precision}$\\uparrow$ & \\textbf{Recall}$\\uparrow$ & \\textbf{\\# Trainable} \\\\\n\\hline\n\\multirow{2}{*}{DenseNet} & LP & $0.9187 \\pm 0.000$ & $0.3569 \\pm 0.018$ & $0.6113 \\pm 0.010$ & $0.2525 \\pm 0.019$ & 1.0 K \\\\\n& FP & $\\textbf{0.9516} \\pm 0.002$ & $\\textbf{0.7024} \\pm 0.028$ & $\\textbf{0.7805} \\pm 0.026$ & $\\textbf{0.6420} \\pm 0.059$ & 7.0 M \\\\\n\\hline\n\\multirow{3}{*}{ViT\\textsubscript{400M}} & LP & $0.9339 \\pm 0.001$ & $0.5273 \\pm 0.009$ & $0.7319 \\pm 0.007$ & $0.4123 \\pm 0.013$ & 1.0 K \\\\\n& FP & $0.9105 \\pm 0.000$ & $0.0000 \\pm 0.000$ & $0.0000 \\pm 0.000$ & $0.0000 \\pm 0.000$ & 303 M \\\\\n& LoRA & $\\textbf{0.9560} \\pm 0.003$ & $\\textbf{0.7506} \\pm 0.019$ & $\\textbf{0.7657} \\pm 0.042$ & $\\textbf{0.7417} \\pm 0.067$ & 2.4 M \\\\\n\\hline\nViT\\textsubscript{small} & FP & $0.9360 \\pm 0.005$ & $0.5814 \\pm 0.059$ & $0.7002 \\pm 0.039$ & $0.5052 \\pm 0.097$ & 21.7 M \\\\\n\\hline\n\\end{tabular}\n}\n\\end{table}", "caption": "\\caption{Comparison of fine-tuning strategies for DenseNet and ViT. LP and FP denote linear probing and full-parameter fine-tuning respectively. The best average scores for each metric per model are \\textbf{bolded}.}", "label": "\\label{tab:ablation}", "tabular": "\\begin{tabular}{|l|c|c|c|c|c|c|}\n\\hline\n\\textbf{Model} & \\textbf{Strategy} & \\textbf{Accuracy}$\\uparrow$ & \\textbf{F1 Score}$\\uparrow$ & \\textbf{Precision}$\\uparrow$ & \\textbf{Recall}$\\uparrow$ & \\textbf{\\# Trainable} \\\\\n\\hline\n\\multirow{2}{*}{DenseNet} & LP & $0.9187 \\pm 0.000$ & $0.3569 \\pm 0.018$ & $0.6113 \\pm 0.010$ & $0.2525 \\pm 0.019$ & 1.0 K \\\\\n& FP & $\\textbf{0.9516} \\pm 0.002$ & $\\textbf{0.7024} \\pm 0.028$ & $\\textbf{0.7805} \\pm 0.026$ & $\\textbf{0.6420} \\pm 0.059$ & 7.0 M \\\\\n\\hline\n\\multirow{3}{*}{ViT\\textsubscript{400M}} & LP & $0.9339 \\pm 0.001$ & $0.5273 \\pm 0.009$ & $0.7319 \\pm 0.007$ & $0.4123 \\pm 0.013$ & 1.0 K \\\\\n& FP & $0.9105 \\pm 0.000$ & $0.0000 \\pm 0.000$ & $0.0000 \\pm 0.000$ & $0.0000 \\pm 0.000$ & 303 M \\\\\n& LoRA & $\\textbf{0.9560} \\pm 0.003$ & $\\textbf{0.7506} \\pm 0.019$ & $\\textbf{0.7657} \\pm 0.042$ & $\\textbf{0.7417} \\pm 0.067$ & 2.4 M \\\\\n\\hline\nViT\\textsubscript{small} & FP & $0.9360 \\pm 0.005$ & $0.5814 \\pm 0.059$ & $0.7002 \\pm 0.039$ & $0.5052 \\pm 0.097$ & 21.7 M \\\\\n\\hline\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[ht]\n\\centering\n\\caption{Model performance of ViT models pretrained on generic image-text datasets at varying scales and FetalCLIP\\textsubscript{CLS}. The best average scores per metric are \\textbf{bolded}.}\n\\label{tab:pretrain_scale}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{|l|c|c|c|c|c|c|}\n\\hline\n\\textbf{Model} & \\textbf{Accuracy}$\\uparrow$ & \\textbf{F1 Score}$\\uparrow$ & \\textbf{Precision}$\\uparrow$ & \\textbf{Recall}$\\uparrow$ & \\textbf{\\# Trainable} \\\\\n\\hline\nViT\\textsubscript{400M} & $0.9560 \\pm 0.003$ & $0.7506 \\pm 0.019$ & $0.7657 \\pm 0.042$ & $\\textbf{0.7417} \\pm 0.067$ & 2.4 M \\\\\nViT\\textsubscript{2B} & $0.9555 \\pm 0.002$ & $0.7429 \\pm 0.011$ & $0.7687 \\pm 0.017$ & $0.7196 \\pm 0.026$ & 2.4 M \\\\\n\nFetalCLIP\\textsubscript{CLS} & $\\textbf{0.9575} \\pm 0.001$ & $\\textbf{0.7570} \\pm 0.007$ & $\\textbf{0.7782} \\pm 0.034$ & $0.7397 \\pm 0.041$ & 2.4 M \\\\\n\\hline\n\\end{tabular}\n}\n\\end{table}", "caption": "\\caption{Model performance of ViT models pretrained on generic image-text datasets at varying scales and FetalCLIP\\textsubscript{CLS}. The best average scores per metric are \\textbf{bolded}.}", "label": "\\label{tab:pretrain_scale}", "tabular": "\\begin{tabular}{|l|c|c|c|c|c|c|}\n\\hline\n\\textbf{Model} & \\textbf{Accuracy}$\\uparrow$ & \\textbf{F1 Score}$\\uparrow$ & \\textbf{Precision}$\\uparrow$ & \\textbf{Recall}$\\uparrow$ & \\textbf{\\# Trainable} \\\\\n\\hline\nViT\\textsubscript{400M} & $0.9560 \\pm 0.003$ & $0.7506 \\pm 0.019$ & $0.7657 \\pm 0.042$ & $\\textbf{0.7417} \\pm 0.067$ & 2.4 M \\\\\nViT\\textsubscript{2B} & $0.9555 \\pm 0.002$ & $0.7429 \\pm 0.011$ & $0.7687 \\pm 0.017$ & $0.7196 \\pm 0.026$ & 2.4 M \\\\\n\nFetalCLIP\\textsubscript{CLS} & $\\textbf{0.9575} \\pm 0.001$ & $\\textbf{0.7570} \\pm 0.007$ & $\\textbf{0.7782} \\pm 0.034$ & $0.7397 \\pm 0.041$ & 2.4 M \\\\\n\\hline\n\\end{tabular}", "subtables": []}], "figure": [{"original": "\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.96\\textwidth]{images/model_architecture.drawio.png}\n    \\caption{\n    (a) Contrastive pretraining of FetalCLIP, where the model learns to align ultrasound scans with their corresponding text descriptions. (b) FetalCLIP\\textsubscript{CLS} utilizes the pretrained image encoder from FetalCLIP followed by a linear head to predict whether a frame is optimal for fetal biometric measurement. The image encoder remains frozen during FetalCLIP\\textsubscript{CLS} training, with only the LoRA modules and linear head being trainable.\n    } \\label{fig:model_architecture}\n\\end{figure}", "caption": "\\caption{\n    (a) Contrastive pretraining of FetalCLIP, where the model learns to align ultrasound scans with their corresponding text descriptions. (b) FetalCLIP\\textsubscript{CLS} utilizes the pretrained image encoder from FetalCLIP followed by a linear head to predict whether a frame is optimal for fetal biometric measurement. The image encoder remains frozen during FetalCLIP\\textsubscript{CLS} training, with only the LoRA modules and linear head being trainable.\n    }", "label": "\\label{fig:model_architecture}", "subfigures": [], "figure_paths": ["images/model_architecture.drawio.png"]}, {"original": "\\begin{figure}[htbp]\n    \\begin{subfigure}[b]{0.45\\textwidth}\n        \\includegraphics[width=\\textwidth]{images/pca_fetalclip.png}\n        \\caption{PCA}\n        \\label{fig:right}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[b]{0.45\\textwidth}\n    \\includegraphics[width=\\textwidth]{images/tsne_fetalclip.png}\n    \\caption{t-SNE}\n    \\label{fig:left}\n    \\end{subfigure}\n    \\caption{Visualizations of the feature embeddings extracted from the penultimate layer of FetalCLIP\\textsubscript{CLS}, using PCA and t-SNE.} \\label{fig:tsne}\n\\end{figure}", "caption": "\\caption{Visualizations of the feature embeddings extracted from the penultimate layer of FetalCLIP\\textsubscript{CLS}, using PCA and t-SNE.}", "label": "\\label{fig:tsne}", "subfigures": [{"caption": "\\caption{PCA}", "label": "\\label{fig:right}", "figure_paths": ["images/pca_fetalclip.png"], "subfigures": []}, {"caption": "\\caption{t-SNE}", "label": "\\label{fig:left}", "figure_paths": ["images/tsne_fetalclip.png"], "subfigures": []}], "figure_paths": ["images/pca_fetalclip.png", "images/tsne_fetalclip.png"]}, {"original": "\\begin{subfigure}[b]{0.45\\textwidth}\n        \\includegraphics[width=\\textwidth]{images/pca_fetalclip.png}\n        \\caption{PCA}\n        \\label{fig:right}\n    \\end{subfigure}", "caption": "\\caption{PCA}", "label": "\\label{fig:right}", "subfigures": [], "figure_paths": ["images/pca_fetalclip.png"]}, {"original": "\\begin{subfigure}[b]{0.45\\textwidth}\n    \\includegraphics[width=\\textwidth]{images/tsne_fetalclip.png}\n    \\caption{t-SNE}\n    \\label{fig:left}\n    \\end{subfigure}", "caption": "\\caption{t-SNE}", "label": "\\label{fig:left}", "subfigures": [], "figure_paths": ["images/tsne_fetalclip.png"]}, {"original": "\\begin{figure}[htbp]\n    \\centering\n    \\textbf{Ground Truth} \\\\\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/166_gt.png}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/148_gt.png}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/1187_gt.png}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/3017_gt.png}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/784_gt.png}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/0_target_7_gt.png}\n    \\end{subfigure}\n\n    \\textbf{Prediction} \\\\\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/166_pred.png}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/148_pred.png}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/1187_pred.png}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/3017_pred.png}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/784_pred.png}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/0_target_7_pred.png}\n    \\end{subfigure}\n    \\caption{Each vertical pair displays the ground truth annotation (top) and its corresponding predicted segmentation mask (bottom). In the rightmost example, both the ground truth and prediction contain no mask, indicating a correctly segmented frame.}\n    \\label{fig:seg_samples}\n\\end{figure}", "caption": "\\caption{Each vertical pair displays the ground truth annotation (top) and its corresponding predicted segmentation mask (bottom). In the rightmost example, both the ground truth and prediction contain no mask, indicating a correctly segmented frame.}", "label": "\\label{fig:seg_samples}", "subfigures": [{"caption": "", "label": null, "figure_paths": ["images/166_gt.png"], "subfigures": []}, {"caption": "", "label": null, "figure_paths": ["images/148_gt.png"], "subfigures": []}, {"caption": "", "label": null, "figure_paths": ["images/1187_gt.png"], "subfigures": []}, {"caption": "", "label": null, "figure_paths": ["images/3017_gt.png"], "subfigures": []}, {"caption": "", "label": null, "figure_paths": ["images/784_gt.png"], "subfigures": []}, {"caption": "", "label": null, "figure_paths": ["images/0_target_7_gt.png"], "subfigures": []}, {"caption": "", "label": null, "figure_paths": ["images/166_pred.png"], "subfigures": []}, {"caption": "", "label": null, "figure_paths": ["images/148_pred.png"], "subfigures": []}, {"caption": "", "label": null, "figure_paths": ["images/1187_pred.png"], "subfigures": []}, {"caption": "", "label": null, "figure_paths": ["images/3017_pred.png"], "subfigures": []}, {"caption": "", "label": null, "figure_paths": ["images/784_pred.png"], "subfigures": []}, {"caption": "", "label": null, "figure_paths": ["images/0_target_7_pred.png"], "subfigures": []}], "figure_paths": ["images/166_gt.png", "images/148_gt.png", "images/1187_gt.png", "images/3017_gt.png", "images/784_gt.png", "images/0_target_7_gt.png", "images/166_pred.png", "images/148_pred.png", "images/1187_pred.png", "images/3017_pred.png", "images/784_pred.png", "images/0_target_7_pred.png"]}, {"original": "\\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/166_gt.png}\n    \\end{subfigure}", "caption": "", "label": null, "subfigures": [], "figure_paths": ["images/166_gt.png"]}, {"original": "\\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/148_gt.png}\n    \\end{subfigure}", "caption": "", "label": null, "subfigures": [], "figure_paths": ["images/148_gt.png"]}, {"original": "\\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/1187_gt.png}\n    \\end{subfigure}", "caption": "", "label": null, "subfigures": [], "figure_paths": ["images/1187_gt.png"]}, {"original": "\\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/3017_gt.png}\n    \\end{subfigure}", "caption": "", "label": null, "subfigures": [], "figure_paths": ["images/3017_gt.png"]}, {"original": "\\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/784_gt.png}\n    \\end{subfigure}", "caption": "", "label": null, "subfigures": [], "figure_paths": ["images/784_gt.png"]}, {"original": "\\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/0_target_7_gt.png}\n    \\end{subfigure}", "caption": "", "label": null, "subfigures": [], "figure_paths": ["images/0_target_7_gt.png"]}, {"original": "\\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/166_pred.png}\n    \\end{subfigure}", "caption": "", "label": null, "subfigures": [], "figure_paths": ["images/166_pred.png"]}, {"original": "\\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/148_pred.png}\n    \\end{subfigure}", "caption": "", "label": null, "subfigures": [], "figure_paths": ["images/148_pred.png"]}, {"original": "\\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/1187_pred.png}\n    \\end{subfigure}", "caption": "", "label": null, "subfigures": [], "figure_paths": ["images/1187_pred.png"]}, {"original": "\\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/3017_pred.png}\n    \\end{subfigure}", "caption": "", "label": null, "subfigures": [], "figure_paths": ["images/3017_pred.png"]}, {"original": "\\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/784_pred.png}\n    \\end{subfigure}", "caption": "", "label": null, "subfigures": [], "figure_paths": ["images/784_pred.png"]}, {"original": "\\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/0_target_7_pred.png}\n    \\end{subfigure}", "caption": "", "label": null, "subfigures": [], "figure_paths": ["images/0_target_7_pred.png"]}], "equations": [], "algorithm": [], "sections": {"Introduction": {"content": "\n\nUltrasound has long played a vital role in routine prenatal care~\\cite{Whitworth2010}. Fetal biometric measurements, such as abdominal circumference (AC), biparietal diameter (BPD), and femur length (FL), are essential for assessing fetal growth and monitoring high-risk pregnancies. However, acquiring accurate measurements requires considerable expertise from sonographers to identify appropriate standard planes. In regions where experienced personnel are scarce, blind-sweep ultrasound, typically performed by novice operators using low-cost portable probes, is often employed to collect obstetric scans~\\cite{Self2022CALOPUS}. These blind-sweep scans usually yield low-quality ultrasound data that may lack the precise anatomical planes conventionally required for accurate biometric assessment.\n\n\nRecent advances in machine learning have demonstrated remarkable capabilities in medical domains. One important application is automated assessment of fetal ultrasound image quality~\\cite{7875138}, which is particularly useful for free-hand ultrasound sequences acquired through blind-sweep protocols as it enables accurate biometric measurements even in the absence of skilled sonographers.\n\n\nTo develop robust machine learning models, a common strategy is to train them on data collected from the same regions where they will be deployed. However, real-world deployment of medical models in resource-constrained settings is often limited by scarce data and computational resources. An effective alternative is to leverage large pretrained models, commonly referred to as foundation models, which are trained on extensive datasets and subsequently adapted to specific downstream tasks through transfer learning.\n\n\nIn this work, we build our models upon FetalCLIP~\\cite{maani2025fetalclipvisuallanguagefoundationmodel}, a vision-language foundation model specifically trained for fetal ultrasound, to address the task of fetal ultrasound image quality assessment (IQA). Our objective is to assist less-experienced sonographers in identifying high-quality frames for abdominal circumference measurement. To enable efficient model adaptation, we employ parameter-efficient fine-tuning using Low-Rank Adaptation (LoRA)~\\cite{hu2021loralowrankadaptationlarge}, which allows efficient task-specific tuning with minimal trainable parameters. This approach is well-suited for deployment in settings with limited computational resources. Our main contributions are as follows:\n\n\\begin{itemize}\n    \\item We propose \\mbox{FetalCLIP\\textsubscript{CLS}}, a model adapted from the fetal ultrasound foundation model FetalCLIP using LoRA. FetalCLIP\\textsubscript{CLS} consistently outperforms strong CNN and Transformer baselines on the fetal ultrasound IQA task, while requiring only a small number of trainable parameters.\n    \\item We demonstrate the feasibility of repurposing a segmentation model for IQA tasks. This adapted segmentation model, named FetalCLIP\\textsubscript{SEG}, further improves classification performance, achieving a higher F1 score and recall compared to classification approaches.\n\\end{itemize}\\begin{itemize}\n    \\item We propose \\mbox{FetalCLIP\\textsubscript{CLS}}, a model adapted from the fetal ultrasound foundation model FetalCLIP using LoRA. FetalCLIP\\textsubscript{CLS} consistently outperforms strong CNN and Transformer baselines on the fetal ultrasound IQA task, while requiring only a small number of trainable parameters.\n    \\item We demonstrate the feasibility of repurposing a segmentation model for IQA tasks. This adapted segmentation model, named FetalCLIP\\textsubscript{SEG}, further improves classification performance, achieving a higher F1 score and recall compared to classification approaches.\n\\end{itemize}\n    \\item We propose \\mbox{FetalCLIP\\textsubscript{CLS}}, a model adapted from the fetal ultrasound foundation model FetalCLIP using LoRA. FetalCLIP\\textsubscript{CLS}CLS consistently outperforms strong CNN and Transformer baselines on the fetal ultrasound IQA task, while requiring only a small number of trainable parameters.\n    \\item We demonstrate the feasibility of repurposing a segmentation model for IQA tasks. This adapted segmentation model, named FetalCLIP\\textsubscript{SEG}SEG, further improves classification performance, achieving a higher F1 score and recall compared to classification approaches.\n\n\n\n", "appendix": false}, "Related Work": {"content": "\n\nMethods for image quality assessment (IQA)~\\cite{ma2025surveyimagequalityassessment} can be broadly classified into two categories: statistical approaches and machine learning-based techniques. Statistical methods include human visual system-based metrics such as the Structural Similarity Index (SSIM)~\\cite{1284395}, transform domain-based techniques like BLIINDS-II~\\cite{6172573}, and natural scene statistics-based approaches such as the Information Fidelity Criterion (IFC)~\\cite{1576816}.\n\nMachine learning-based IQA methods can be categorized into traditional machine learning, convolutional neural network (CNN)-based, and Transformer-based approaches. Traditional machine learning methods such as BRISQUE~\\cite{6272356} rely on hand-crafted features derived from natural scene statistics to estimate image quality. CNN-based methods such as IQA-CNN~\\cite{6909620} extract deep features from image data to improve prediction accuracy. More recently, Transformer-based methods like TRIQ~\\cite{yang2022maniqamultidimensionattentionnetwork} have been proposed to address the locality bias inherent in CNNs by capturing long-range dependencies, leading to improved performance on IQA tasks.\n\nState-of-the-art fetal ultrasound IQA methods predominantly rely on machine learning. Wu et al.~\\cite{7875138} employ two CNNs to assess ultrasound scans of the fetal abdominal region, demonstrating performance comparable to subjective ratings from medical experts. Cengiz et al.~\\cite{cengiz2023fusqafetalultrasoundsegmentation} propose an automatic method to evaluate the quality of predicted segmentation masks. Boumeridja et al.~\\cite{boumeridja2025enhancing} introduce a super-resolution technique that enhances ultrasound image resolution to improve downstream classification performance in low-resource settings.\n\nThe majority of models for fetal ultrasound IQA have adopted pretrained models due to their superior performance~\\cite{sendrabalcells2023generalisabilityfetalultrasounddeep}. However, most publicly available foundation models are pretrained on natural images, which may not be optimal for domain-specific tasks. FetalCLIP is a foundation model pretrained on fetal ultrasound image-caption pairs using Contrastive Language-Image Pretraining (CLIP)~\\cite{radford2021learningtransferablevisualmodels}. We are interested in evaluating its transferability in low-resource settings.\n\n\n\n\n\n\n\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.96\\textwidth]{images/model_architecture.drawio.png}\n    \\caption{\n    (a) Contrastive pretraining of FetalCLIP, where the model learns to align ultrasound scans with their corresponding text descriptions. (b) FetalCLIP\\textsubscript{CLS} utilizes the pretrained image encoder from FetalCLIP followed by a linear head to predict whether a frame is optimal for fetal biometric measurement. The image encoder remains frozen during FetalCLIP\\textsubscript{CLS} training, with only the LoRA modules and linear head being trainable.\n    } \\label{fig:model_architecture}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=0.96\\textwidth]{images/model_architecture.drawio.png}\n    \\caption{\n    (a) Contrastive pretraining of FetalCLIP, where the model learns to align ultrasound scans with their corresponding text descriptions. (b) FetalCLIP\\textsubscript{CLS} utilizes the pretrained image encoder from FetalCLIP followed by a linear head to predict whether a frame is optimal for fetal biometric measurement. The image encoder remains frozen during FetalCLIP\\textsubscript{CLS} training, with only the LoRA modules and linear head being trainable.\n    } \\label{fig:model_architecture}\n\n\n", "appendix": false}, "Methodology": {"content": " \\label{methodology}\n\\subsection{Task Formulation}\n\nWe formulate fetal ultrasound image quality assessment (IQA) as a binary classification problem to identify frames containing clear anatomical structures suitable for fetal biometric measurement. Formally, given a 2D ultrasound frame \\( X \\) X , the model predicts a binary label \\( y \\in \\{0, 1\\} \\) y \\in \\{0, 1\\} , where \\( y = 1 \\) y = 1  indicates the presence of relevant anatomical structures and \\( y = 0 \\) y = 0  indicates their absence.\n\n\\subsection{Model Architecture}\n\nWe utilize the image encoder from FetalCLIP~\\cite{maani2025fetalclipvisuallanguagefoundationmodel}, which was trained on over 210,000 fetal ultrasound image-caption pairs using CLIP~\\cite{radford2021learningtransferablevisualmodels}. As illustrated in Figure~\\ref{fig:model_architecture}(a), this encoder captures rich semantic and spatial features by aligning ultrasound images with their corresponding textual descriptions during pretraining.\n\n\nIn our approach, we keep the pretrained image encoder frozen and insert LoRA modules within the attention and feed-forward blocks to enable parameter-efficient fine-tuning. Our ablation study in Appendix~\\ref{sec:ablation} demonstrates that fine-tuning transformer-based models with LoRA outperforms both linear probing and full-parameter fine-tuning.\n\nUltrasound scans are passed through the encoder to produce a one-dimensional embedding that encapsulates semantic and structural information about the images. A classification head, consisting of a single fully connected layer, is appended to the image encoder to map the embedding to a binary prediction indicating whether the frame is suitable for fetal biometric measurement. We refer to this classification pipeline as FetalCLIP\\textsubscript{CLS}CLS, as illustrated in Figure~\\ref{fig:model_architecture}(b).\n\n\\subsection{Leveraging Segmentation Model for Classification Task}\n\nBased on the hypothesis that a segmentation model capable of perfectly annotating an image can also classify it without error, we propose FetalCLIP\\textsubscript{SEG}SEG, a segmentation model that employs a thresholding strategy to convert predicted segmentation masks into binary classification labels.\n\n\nThis model leverages the same frozen image encoder from FetalCLIP with embedded LoRA modules for parameter-efficient fine-tuning, followed by a lightweight U-shaped network inspired by UNETR~\\cite{hatamizadeh2021unetrtransformers3dmedical}. Ground truth masks, as detailed in Section~\\ref{sec:dataset}, are used during training to guide the segmentation process.\n\n\n", "appendix": false}, "Experiments": {"content": "\n\\subsection{Dataset} \\label{sec:dataset}\n\nWe use a 2D B-mode fetal ultrasound dataset from the MICCAI 2024 ACOUSLIC-AI Challenge~\\cite{SAPPIA2025103640}. The dataset was originally developed for operator-agnostic abdominal circumference measurement in low-income countries. It comprises fetal abdominal ultrasound scans collected from pregnant women between 20 and 32 weeks of gestation in Sierra Leone and Tanzania. The scans were acquired by novice users with only one hour of training using a low-cost portable ultrasound probe, and each scan is accompanied by an expert-annotated mask. Since not all frames contain abdominal structures, some masks may be empty. Each patient undergoes six blind sweeps: three transverse sweeps in the caudocranial direction and three sagittal sweeps from the patient's left to right, with each sweep containing 140 frames.\n\n\nIn the challenge, a total of 300 cases are provided for training. On average, only 2.6 out of every 100 frames contain abdominal structures, resulting in a highly imbalanced label distribution. To mitigate this imbalance, we exclude sweeps without annotated masks. This filtering process yields a more balanced dataset, increasing the average number of frames with clear abdominal planes to 8.6 per 100 scans.\n\n\nSince the official validation and test sets are not publicly available, we randomly split the 300 cases into 210 for training, 30 for validation, and 60 for testing. To prevent data leakage, all sweeps from the same patient are assigned to the same split. This results in 52,500 training, 8,540 validation, and 16,380 test ultrasound images.\n\n\n\\subsection{Evaluation Metrics}\n\nWe evaluate model performance using widely adopted classification metrics that focus on different aspects: overall correctness (accuracy), the ability to identify optimal frames (precision and recall), and the harmonic mean of precision and recall (F1 score).\n\n\n\\subsection{Implementation Details}\n\nDuring preprocessing, each image is first padded to a square shape. Data augmentation is then applied to the training set, including color jittering, contrast-limited adaptive histogram equalization (CLAHE)~\\cite{10.5555/180895.180940}, and affine transformations. We generate two additional augmented versions for each training frame. All images are subsequently resized to $224 \\times 224$224 \\times 224 pixels.\n\n\nWe train all models using the AdamW optimizer~\\cite{loshchilov2019decoupledweightdecayregularization} with a fixed learning rate of $3 \\times 10^{-4}$3 \\times 10^{-4}-4 for 5 epochs. All experiments are conducted on a single NVIDIA RTX 4090 GPU. Each experiment is repeated five times, and the checkpoint with the lowest validation loss is selected for evaluation on the test set.\n\n\nAll classification models are optimized using binary cross-entropy loss, while FetalCLIP\\textsubscript{SEG}SEG is trained with the Dice loss~\\cite{milletari2016vnetfullyconvolutionalneural}. To convert segmentation outputs into binary labels, we apply a thresholding strategy: if the number of foreground pixels in the predicted mask exceeds 1\\% of the image area (approximately 500 pixels), the frame is labeled as $y = 1$y = 1; otherwise, it is labeled as $y = 0$y = 0.\n\n\n\\subsection{Results}\n\nWe compare FetalCLIP\\textsubscript{CLS}CLS against six baseline models: three CNN-based models\u2014DenseNet~\\cite{huang2018denselyconnectedconvolutionalnetworks}, EfficientNet~\\cite{tan2020efficientnetrethinkingmodelscaling}, and VGG~\\cite{simonyan2015deepconvolutionalnetworkslargescale}\u2014and three Transformer-based models\u2014Swin Transformer~\\cite{liu2021swintransformerhierarchicalvision}, DeiT~\\cite{touvron2022deitiiirevengevit}, and Vision Transformer (ViT)~\\cite{dosovitskiy2021imageworth16x16words}.\n\n\\begin{table}[h]\n\\caption{\nModel performance on fetal ultrasound IQA. We compare FetalCLIP\\textsubscript{CLS} against six baseline models. Metrics are reported as mean \u00b1 standard deviation across five independent runs. The last column indicates the number of trainable parameters for each model. The best average scores for each metric are \\textbf{bolded}, while second-best scores are \\underline{underlined}.\n} \\label{tab:model_performance}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{|l|l|c|c|c|c|c|}\n\\hline\n\\textbf{Architecture} & \\textbf{Models} & \\textbf{Accuracy}$\\uparrow$ & \\textbf{F1 Score}$\\uparrow$ & \\textbf{Precision}$\\uparrow$ & \\textbf{Recall}$\\uparrow$ & \\textbf{\\# Trainable} \\\\\n\\hline\n\\multirow{3}{*}{CNN} & DenseNet & $0.9516 \\pm 0.002$ & $0.7024 \\pm 0.028$ & $\\underline{0.7805} \\pm 0.026$ & $0.6420 \\pm 0.059$ & 7.0 M \\\\\n& EfficientNet & $0.9537 \\pm 0.004$ & $0.7253 \\pm 0.030$ & $0.7725 \\pm 0.025$ & $0.6855 \\pm 0.053$ & 4.0 M \\\\\n& VGG & $0.9510 \\pm 0.002$ & $0.7084 \\pm 0.021$ & $0.7580 \\pm 0.023$ & $0.6671 \\pm 0.048$ & 134 M \\\\\n\\hline\n% MobileNet & $0.9474 \\pm 0.0012$ & $0.6727 \\pm 0.0301$ & $0.7656 \\pm 0.0485$ & $0.6111 \\pm 0.0874$ & 1.5 M \\\\\n\\multirow{4}{*}{Transformer} & Swin & $0.9565 \\pm 0.003$ & $0.7429 \\pm 0.039$ & $\\textbf{0.7864} \\pm 0.032$ & $0.7113 \\pm 0.087$ & 1.7 M \\\\\n& DEIT & $0.9554 \\pm 0.001$ & $0.7466 \\pm 0.014$ & $0.7619 \\pm 0.035$ & $0.7363 \\pm 0.059$ & 2.4 M \\\\\n% ViT & $\\underline{0.9563} \\pm 0.0021$ & $\\underline{0.7388} \\pm 0.0237$ & $\\textbf{0.7935} \\pm 0.0166$ & $\\underline{0.6936} \\pm 0.0537$ & 2.8 M\\\\\n& ViT\\textsubscript{400M} & $\\underline{0.9560} \\pm 0.003$ & $\\underline{0.7506} \\pm 0.019$ & $0.7657 \\pm 0.042$ & $\\textbf{0.7417} \\pm 0.067$ & 2.4 M \\\\\n& FetalCLIP\\textsubscript{CLS} & $\\textbf{0.9575} \\pm 0.001$ & $\\textbf{0.7570} \\pm 0.007$ & $0.7782 \\pm 0.034$ & $\\underline{0.7397} \\pm 0.041$ & 2.4 M \\\\\n\\hline\n\\end{tabular}\n}\n\\end{table}\n\\caption{\nModel performance on fetal ultrasound IQA. We compare FetalCLIP\\textsubscript{CLS} against six baseline models. Metrics are reported as mean \u00b1 standard deviation across five independent runs. The last column indicates the number of trainable parameters for each model. The best average scores for each metric are \\textbf{bolded}, while second-best scores are \\underline{underlined}.\n} \\label{tab:model_performance}\n\\resizebox{\\textwidth}\\textwidth{!}!{\n\\begin{tabular}{|l|l|c|c|c|c|c|}\n\\hline\n\\textbf{Architecture} & \\textbf{Models} & \\textbf{Accuracy}$\\uparrow$ & \\textbf{F1 Score}$\\uparrow$ & \\textbf{Precision}$\\uparrow$ & \\textbf{Recall}$\\uparrow$ & \\textbf{\\# Trainable} \\\\\n\\hline\n\\multirow{3}{*}{CNN} & DenseNet & $0.9516 \\pm 0.002$ & $0.7024 \\pm 0.028$ & $\\underline{0.7805} \\pm 0.026$ & $0.6420 \\pm 0.059$ & 7.0 M \\\\\n& EfficientNet & $0.9537 \\pm 0.004$ & $0.7253 \\pm 0.030$ & $0.7725 \\pm 0.025$ & $0.6855 \\pm 0.053$ & 4.0 M \\\\\n& VGG & $0.9510 \\pm 0.002$ & $0.7084 \\pm 0.021$ & $0.7580 \\pm 0.023$ & $0.6671 \\pm 0.048$ & 134 M \\\\\n\\hline\n% MobileNet & $0.9474 \\pm 0.0012$ & $0.6727 \\pm 0.0301$ & $0.7656 \\pm 0.0485$ & $0.6111 \\pm 0.0874$ & 1.5 M \\\\\n\\multirow{4}{*}{Transformer} & Swin & $0.9565 \\pm 0.003$ & $0.7429 \\pm 0.039$ & $\\textbf{0.7864} \\pm 0.032$ & $0.7113 \\pm 0.087$ & 1.7 M \\\\\n& DEIT & $0.9554 \\pm 0.001$ & $0.7466 \\pm 0.014$ & $0.7619 \\pm 0.035$ & $0.7363 \\pm 0.059$ & 2.4 M \\\\\n% ViT & $\\underline{0.9563} \\pm 0.0021$ & $\\underline{0.7388} \\pm 0.0237$ & $\\textbf{0.7935} \\pm 0.0166$ & $\\underline{0.6936} \\pm 0.0537$ & 2.8 M\\\\\n& ViT\\textsubscript{400M} & $\\underline{0.9560} \\pm 0.003$ & $\\underline{0.7506} \\pm 0.019$ & $0.7657 \\pm 0.042$ & $\\textbf{0.7417} \\pm 0.067$ & 2.4 M \\\\\n& FetalCLIP\\textsubscript{CLS} & $\\textbf{0.9575} \\pm 0.001$ & $\\textbf{0.7570} \\pm 0.007$ & $0.7782 \\pm 0.034$ & $\\underline{0.7397} \\pm 0.041$ & 2.4 M \\\\\n\\hline\n\\end{tabular}\n}\n\n\\hline\n\\textbf{Architecture} & \\textbf{Models} & \\textbf{Accuracy}$\\uparrow$\\uparrow & \\textbf{F1 Score}$\\uparrow$\\uparrow & \\textbf{Precision}$\\uparrow$\\uparrow & \\textbf{Recall}$\\uparrow$\\uparrow & \\textbf{\\# Trainable} \\\\\n\\hline\n\\multirow{3}3{*}*{CNN}CNN & DenseNet & $0.9516 \\pm 0.002$0.9516 \\pm 0.002 & $0.7024 \\pm 0.028$0.7024 \\pm 0.028 & $\\underline{0.7805} \\pm 0.026$\\underline{0.7805} \\pm 0.026 & $0.6420 \\pm 0.059$0.6420 \\pm 0.059 & 7.0 M \\\\\n& EfficientNet & $0.9537 \\pm 0.004$0.9537 \\pm 0.004 & $0.7253 \\pm 0.030$0.7253 \\pm 0.030 & $0.7725 \\pm 0.025$0.7725 \\pm 0.025 & $0.6855 \\pm 0.053$0.6855 \\pm 0.053 & 4.0 M \\\\\n& VGG & $0.9510 \\pm 0.002$0.9510 \\pm 0.002 & $0.7084 \\pm 0.021$0.7084 \\pm 0.021 & $0.7580 \\pm 0.023$0.7580 \\pm 0.023 & $0.6671 \\pm 0.048$0.6671 \\pm 0.048 & 134 M \\\\\n\\hline\n\\multirow{4}4{*}*{Transformer}Transformer & Swin & $0.9565 \\pm 0.003$0.9565 \\pm 0.003 & $0.7429 \\pm 0.039$0.7429 \\pm 0.039 & $\\textbf{0.7864} \\pm 0.032$\\textbf{0.7864} \\pm 0.032 & $0.7113 \\pm 0.087$0.7113 \\pm 0.087 & 1.7 M \\\\\n& DEIT & $0.9554 \\pm 0.001$0.9554 \\pm 0.001 & $0.7466 \\pm 0.014$0.7466 \\pm 0.014 & $0.7619 \\pm 0.035$0.7619 \\pm 0.035 & $0.7363 \\pm 0.059$0.7363 \\pm 0.059 & 2.4 M \\\\\n& ViT\\textsubscript{400M}400M & $\\underline{0.9560} \\pm 0.003$\\underline{0.9560} \\pm 0.003 & $\\underline{0.7506} \\pm 0.019$\\underline{0.7506} \\pm 0.019 & $0.7657 \\pm 0.042$0.7657 \\pm 0.042 & $\\textbf{0.7417} \\pm 0.067$\\textbf{0.7417} \\pm 0.067 & 2.4 M \\\\\n& FetalCLIP\\textsubscript{CLS}CLS & $\\textbf{0.9575} \\pm 0.001$\\textbf{0.9575} \\pm 0.001 & $\\textbf{0.7570} \\pm 0.007$\\textbf{0.7570} \\pm 0.007 & $0.7782 \\pm 0.034$0.7782 \\pm 0.034 & $\\underline{0.7397} \\pm 0.041$\\underline{0.7397} \\pm 0.041 & 2.4 M \\\\\n\\hline\n\n\n\n\nThe CNN baselines are pretrained on ImageNet-1K~\\cite{5206848}, and we perform full-parameter fine-tuning for these models. For the Transformer baselines, we freeze the encoder and apply LoRA, following an architecture similar to FetalCLIP\\textsubscript{CLS}CLS. In fact, the only difference lies in the choice of encoder for Transformer-based models. We choose Transformer baselines with a comparable number of parameters to FetalCLIP\\textsubscript{CLS}CLS to ensure a fair comparison. Swin Transformer and DeiT are pretrained on ImageNet-22K, while the ViT baseline is pretrained on WIT-400M image-text pairs using CLIP~\\cite{Cherti_2023}.\n\nAs shown in Table~\\ref{tab:model_performance}, FetalCLIP\\textsubscript{CLS}CLS achieves superior performance in accuracy and F1 score compared to all baselines while maintaining competitive precision and recall with a small number of trainable parameters. Note that FetalCLIP is pretrained on over 210,000 fetal ultrasound image-caption pairs. Although the ViT baseline is pretrained on 400M image-text pairs\u2014a dataset more than 1,900 times larger than that used for FetalCLIP\u2014FetalCLIP\\textsubscript{CLS}CLS still outperforms ViT\\textsubscript{400M}400M in accuracy, F1 score, and precision. This demonstrates the effectiveness of foundation models' domain-specific pretraining and highlights the advantages of FetalCLIP\\textsubscript{CLS}CLS for tasks requiring high-quality fetal ultrasound representations.\n\nWe observe that Transformer-based models consistently outperform CNN-based models in accuracy, F1 score, and recall. Although CNNs were once the preferred choice for many machine learning practitioners, they have been surpassed by Transformers for fetal ultrasound IQA tasks, according to our experimental results. As a general recommendation, practitioners seeking superior model performance should favor Transformer-based models over CNNs when developing fetal ultrasound IQA systems.\n\n\\begin{figure}[htbp]\n    \\begin{subfigure}[b]{0.45\\textwidth}\n        \\includegraphics[width=\\textwidth]{images/pca_fetalclip.png}\n        \\caption{PCA}\n        \\label{fig:right}\n    \\end{subfigure}\n    \\hfill\n    \\begin{subfigure}[b]{0.45\\textwidth}\n    \\includegraphics[width=\\textwidth]{images/tsne_fetalclip.png}\n    \\caption{t-SNE}\n    \\label{fig:left}\n    \\end{subfigure}\n    \\caption{Visualizations of the feature embeddings extracted from the penultimate layer of FetalCLIP\\textsubscript{CLS}, using PCA and t-SNE.} \\label{fig:tsne}\n\\end{figure}\n    [b]{0.45\\textwidth}0.45\\textwidth\n        \\includegraphics[width=\\textwidth]{images/pca_fetalclip.png}\n        \\caption{PCA}\n        \\label{fig:right}\n    \n    \\hfill\n    [b]{0.45\\textwidth}0.45\\textwidth\n    \\includegraphics[width=\\textwidth]{images/tsne_fetalclip.png}\n    \\caption{t-SNE}\n    \\label{fig:left}\n    \n    \\caption{Visualizations of the feature embeddings extracted from the penultimate layer of FetalCLIP\\textsubscript{CLS}, using PCA and t-SNE.} \\label{fig:tsne}\n\n\nWe present PCA and t-SNE visualizations of the feature embeddings extracted from FetalCLIP\\textsubscript{CLS}CLS in Figure~\\ref{fig:tsne}. Both plots exhibit relatively low intra-class variance, indicating discriminative embeddings. However, noticeable inter-class overlap is observed, which may be attributed to the similarity of suboptimal frames adjacent to the optimal ones, reflecting the inherent difficulty of the dataset.\n\n\\subsection{Repurposing Segmentation Model for IQA}\n\nAs shown in Table~\\ref{tab:seg_model}, FetalCLIP\\textsubscript{SEG}SEG achieves a higher F1 score and recall compared to FetalCLIP\\textsubscript{CLS}CLS. Moreover, the strong Dice score (0.7244) confirms that the segmentation decoder produces accurate masks. The results demonstrate the feasibility of repurposing a segmentation model for classification tasks through mask thresholding and show that an adapted segmentation model, supervised with pixel-wise annotations, can achieve remarkable classification performance, even surpassing a dedicated classification model on critical metrics.\n\n\\begin{table}[h]\n\\caption{\nModel performance of FetalCLIP\\textsubscript{CLS} and FetalCLIP\\textsubscript{SEG}. The best average scores for each metric are \\textbf{bolded}.\n} \\label{tab:seg_model}\n\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{|l|c|c|c|c|c|c|}\n\\hline\n\\textbf{Models} & \\textbf{DICE}$\\uparrow$ & \\textbf{Accuracy}$\\uparrow$ & \\textbf{F1 Score}$\\uparrow$ & \\textbf{Precision}$\\uparrow$ & \\textbf{Recall}$\\uparrow$ & \\# \\textbf{Trainable}\\\\\n\\hline\nFetalCLIP\\textsubscript{CLS} & / & $\\textbf{0.9575} \\pm 0.001$ & $0.7570 \\pm 0.007$ & $\\textbf{0.7782} \\pm 0.034$ & $0.7397 \\pm 0.041$ & 2.4 M \\\\\n\\hline\nFetalCLIP\\textsubscript{SEG} & $0.7244 \\pm 0.007$ & $0.9543 \\pm 0.001$ & $\\textbf{0.7708} \\pm 0.005$ & $0.6988 \\pm 0.010$ & $\\textbf{0.8599} \\pm 0.017$ & 4.0 M \\\\\n\\hline\n\\end{tabular}\n}\n\\end{table}\n\\caption{\nModel performance of FetalCLIP\\textsubscript{CLS} and FetalCLIP\\textsubscript{SEG}. The best average scores for each metric are \\textbf{bolded}.\n} \\label{tab:seg_model}\n\n\\resizebox{\\textwidth}\\textwidth{!}!{\n\\begin{tabular}{|l|c|c|c|c|c|c|}\n\\hline\n\\textbf{Models} & \\textbf{DICE}$\\uparrow$ & \\textbf{Accuracy}$\\uparrow$ & \\textbf{F1 Score}$\\uparrow$ & \\textbf{Precision}$\\uparrow$ & \\textbf{Recall}$\\uparrow$ & \\# \\textbf{Trainable}\\\\\n\\hline\nFetalCLIP\\textsubscript{CLS} & / & $\\textbf{0.9575} \\pm 0.001$ & $0.7570 \\pm 0.007$ & $\\textbf{0.7782} \\pm 0.034$ & $0.7397 \\pm 0.041$ & 2.4 M \\\\\n\\hline\nFetalCLIP\\textsubscript{SEG} & $0.7244 \\pm 0.007$ & $0.9543 \\pm 0.001$ & $\\textbf{0.7708} \\pm 0.005$ & $0.6988 \\pm 0.010$ & $\\textbf{0.8599} \\pm 0.017$ & 4.0 M \\\\\n\\hline\n\\end{tabular}\n}\n\n\\hline\n\\textbf{Models} & \\textbf{DICE}$\\uparrow$\\uparrow & \\textbf{Accuracy}$\\uparrow$\\uparrow & \\textbf{F1 Score}$\\uparrow$\\uparrow & \\textbf{Precision}$\\uparrow$\\uparrow & \\textbf{Recall}$\\uparrow$\\uparrow & \\# \\textbf{Trainable}\\\\\n\\hline\nFetalCLIP\\textsubscript{CLS}CLS & / & $\\textbf{0.9575} \\pm 0.001$\\textbf{0.9575} \\pm 0.001 & $0.7570 \\pm 0.007$0.7570 \\pm 0.007 & $\\textbf{0.7782} \\pm 0.034$\\textbf{0.7782} \\pm 0.034 & $0.7397 \\pm 0.041$0.7397 \\pm 0.041 & 2.4 M \\\\\n\\hline\nFetalCLIP\\textsubscript{SEG}SEG & $0.7244 \\pm 0.007$0.7244 \\pm 0.007 & $0.9543 \\pm 0.001$0.9543 \\pm 0.001 & $\\textbf{0.7708} \\pm 0.005$\\textbf{0.7708} \\pm 0.005 & $0.6988 \\pm 0.010$0.6988 \\pm 0.010 & $\\textbf{0.8599} \\pm 0.017$\\textbf{0.8599} \\pm 0.017 & 4.0 M \\\\\n\\hline\n\n\n\n\nThe primary drawback of FetalCLIP\\textsubscript{SEG}SEG is its reduced precision (0.6988), approximately 8\\% lower than that of FetalCLIP\\textsubscript{CLS}CLS. These results suggest that while the segmentation-based approach is effective in identifying relevant anatomical structures, it may also produce more false positives by potentially classifying suboptimal or ambiguous frames as positive.\n\nOn the other hand, since the encoder remains frozen and the introduced decoder is lightweight, FetalCLIP\\textsubscript{SEG}SEG maintains computational efficiency and remains well-suited for deployment in environments with limited computational resources.\n\n\\begin{figure}[htbp]\n    \\centering\n    \\textbf{Ground Truth} \\\\\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/166_gt.png}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/148_gt.png}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/1187_gt.png}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/3017_gt.png}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/784_gt.png}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/0_target_7_gt.png}\n    \\end{subfigure}\n\n    \\textbf{Prediction} \\\\\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/166_pred.png}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/148_pred.png}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/1187_pred.png}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/3017_pred.png}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/784_pred.png}\n    \\end{subfigure}\n    \\begin{subfigure}[b]{0.15\\textwidth}\n        \\includegraphics[width=\\linewidth]{images/0_target_7_pred.png}\n    \\end{subfigure}\n    \\caption{Each vertical pair displays the ground truth annotation (top) and its corresponding predicted segmentation mask (bottom). In the rightmost example, both the ground truth and prediction contain no mask, indicating a correctly segmented frame.}\n    \\label{fig:seg_samples}\n\\end{figure}\n    \\centering\n    \\textbf{Ground Truth} \\\\\n    [b]{0.15\\textwidth}0.15\\textwidth\n        \\includegraphics[width=\\linewidth]{images/166_gt.png}\n    \n    [b]{0.15\\textwidth}0.15\\textwidth\n        \\includegraphics[width=\\linewidth]{images/148_gt.png}\n    \n    [b]{0.15\\textwidth}0.15\\textwidth\n        \\includegraphics[width=\\linewidth]{images/1187_gt.png}\n    \n    [b]{0.15\\textwidth}0.15\\textwidth\n        \\includegraphics[width=\\linewidth]{images/3017_gt.png}\n    \n    [b]{0.15\\textwidth}0.15\\textwidth\n        \\includegraphics[width=\\linewidth]{images/784_gt.png}\n    \n    [b]{0.15\\textwidth}0.15\\textwidth\n        \\includegraphics[width=\\linewidth]{images/0_target_7_gt.png}\n    \n\n    \\textbf{Prediction} \\\\\n    [b]{0.15\\textwidth}0.15\\textwidth\n        \\includegraphics[width=\\linewidth]{images/166_pred.png}\n    \n    [b]{0.15\\textwidth}0.15\\textwidth\n        \\includegraphics[width=\\linewidth]{images/148_pred.png}\n    \n    [b]{0.15\\textwidth}0.15\\textwidth\n        \\includegraphics[width=\\linewidth]{images/1187_pred.png}\n    \n    [b]{0.15\\textwidth}0.15\\textwidth\n        \\includegraphics[width=\\linewidth]{images/3017_pred.png}\n    \n    [b]{0.15\\textwidth}0.15\\textwidth\n        \\includegraphics[width=\\linewidth]{images/784_pred.png}\n    \n    [b]{0.15\\textwidth}0.15\\textwidth\n        \\includegraphics[width=\\linewidth]{images/0_target_7_pred.png}\n    \n    \\caption{Each vertical pair displays the ground truth annotation (top) and its corresponding predicted segmentation mask (bottom). In the rightmost example, both the ground truth and prediction contain no mask, indicating a correctly segmented frame.}\n    \\label{fig:seg_samples}\n\n\nFigure~\\ref{fig:seg_samples} presents six pairs of predicted segmentation masks along with their corresponding ground truth annotations, illustrating FetalCLIP\\textsubscript{SEG}SEG's ability to accurately localize the abdominal region in fetal ultrasound frames.\n\n\n\n\n\n\n\n\n\n", "appendix": false}, "Conclusion": {"content": "\n\nIn this work, we investigated the application of FetalCLIP for fetal ultrasound IQA, aiming to assist less-experienced sonographers in identifying suitable frames for abdominal circumference measurement from blind sweeps.\n\nOur results demonstrate that FetalCLIP produces more effective ultrasound-specific embeddings than several of the most commonly used foundation models. FetalCLIP\\textsubscript{CLS}CLS consistently outperforms CNN and Transformer baselines in both accuracy and F1 score while maintaining competitive precision and recall with low computational overhead. Additionally, incorporating mask supervision in FetalCLIP\\textsubscript{SEG}SEG yields an improved F1 score and recall for optimal frame selection, albeit at the cost of precision.\n\nOur study highlights that ultrasound-specific foundation models can improve diagnostic accuracy while being deployable in a resource-efficient way. Future work may extend this framework to additional fetal biometry tasks and incorporate datasets spanning a broader range of difficulty levels, facilitating more diverse and robust analyses aimed at improving prenatal care in low-resource settings.\n\n\n\n\n\n\n\n\n\\begin{comment}  %% removed for anonymized MICCAI 2025 submission.\n    \n    % The following acknowledgement and disclaimer sections should be removed for the double-blind review process.  \n    % If and when your paper is accepted, reinsert the acknowledgement and the disclaimer clause in your final camera-ready version.\n\n\\begin{credits}\n\\subsubsection{\\ackname} A bold run-in heading in small font size at the end of the paper is\nused for general acknowledgments, for example: This study was funded\nby X (grant number Y).\n\n\\subsubsection{\\discintname}\nIt is now necessary to declare any competing interests or to specifically\nstate that the authors have no competing interests. Please place the\nstatement with a bold run-in heading in small font size beneath the\n(optional) acknowledgments\\footnote{If EquinOCS, our proceedings submission\nsystem, is used, then the disclaimer can be provided directly in the system.},\nfor example: The authors have no competing interests to declare that are\nrelevant to the content of this article. Or: Author A has received research\ngrants from Company W. Author B has received a speaker honorarium from\nCompany X and owns stock in Company Y. Author C is a member of committee Z.\n\\end{credits}\n\n\\end{comment}\\begin{comment}  %% removed for anonymized MICCAI 2025 submission.\n    \n    % The following acknowledgement and disclaimer sections should be removed for the double-blind review process.  \n    % If and when your paper is accepted, reinsert the acknowledgement and the disclaimer clause in your final camera-ready version.\n\n\\begin{credits}\n\\subsubsection{\\ackname} A bold run-in heading in small font size at the end of the paper is\nused for general acknowledgments, for example: This study was funded\nby X (grant number Y).\n\n\\subsubsection{\\discintname}\nIt is now necessary to declare any competing interests or to specifically\nstate that the authors have no competing interests. Please place the\nstatement with a bold run-in heading in small font size beneath the\n(optional) acknowledgments\\footnote{If EquinOCS, our proceedings submission\nsystem, is used, then the disclaimer can be provided directly in the system.},\nfor example: The authors have no competing interests to declare that are\nrelevant to the content of this article. Or: Author A has received research\ngrants from Company W. Author B has received a speaker honorarium from\nCompany X and owns stock in Company Y. Author C is a member of committee Z.\n\\end{credits}\n\n\\end{comment}  \n\n\n\\subsubsection{\\ackname} A bold run-in heading in small font size at the end of the paper is\nused for general acknowledgments, for example: This study was funded\nby X (grant number Y).\n\n\\subsubsection{\\discintname}\nIt is now necessary to declare any competing interests or to specifically\nstate that the authors have no competing interests. Please place the\nstatement with a bold run-in heading in small font size beneath the\n(optional) acknowledgments\\footnote{If EquinOCS, our proceedings submission\nsystem, is used, then the disclaimer can be provided directly in the system.},\nfor example: The authors have no competing interests to declare that are\nrelevant to the content of this article. Or: Author A has received research\ngrants from Company W. Author B has received a speaker honorarium from\nCompany X and owns stock in Company Y. Author C is a member of committee Z.\n\n\n\n\\bibliographystyle{splncs04}splncs04\n\\bibliography{mybibliography}\n\n\n\n", "appendix": true}, "Ablation Study": {"content": "\n\\subsection{Selection of Fine-Tuning Strategies}\n\\label{sec:ablation}\nWe evaluate three fine-tuning strategies, including linear probing, full-parameter fine-tuning, and LoRA, using DenseNet and ViT as representative CNN- and Transformer-based models respectively.\n\n\\begin{table}[ht]\n\\centering\n\\caption{Comparison of fine-tuning strategies for DenseNet and ViT. LP and FP denote linear probing and full-parameter fine-tuning respectively. The best average scores for each metric per model are \\textbf{bolded}.}\n\n\\label{tab:ablation}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{|l|c|c|c|c|c|c|}\n\\hline\n\\textbf{Model} & \\textbf{Strategy} & \\textbf{Accuracy}$\\uparrow$ & \\textbf{F1 Score}$\\uparrow$ & \\textbf{Precision}$\\uparrow$ & \\textbf{Recall}$\\uparrow$ & \\textbf{\\# Trainable} \\\\\n\\hline\n\\multirow{2}{*}{DenseNet} & LP & $0.9187 \\pm 0.000$ & $0.3569 \\pm 0.018$ & $0.6113 \\pm 0.010$ & $0.2525 \\pm 0.019$ & 1.0 K \\\\\n& FP & $\\textbf{0.9516} \\pm 0.002$ & $\\textbf{0.7024} \\pm 0.028$ & $\\textbf{0.7805} \\pm 0.026$ & $\\textbf{0.6420} \\pm 0.059$ & 7.0 M \\\\\n\\hline\n\\multirow{3}{*}{ViT\\textsubscript{400M}} & LP & $0.9339 \\pm 0.001$ & $0.5273 \\pm 0.009$ & $0.7319 \\pm 0.007$ & $0.4123 \\pm 0.013$ & 1.0 K \\\\\n& FP & $0.9105 \\pm 0.000$ & $0.0000 \\pm 0.000$ & $0.0000 \\pm 0.000$ & $0.0000 \\pm 0.000$ & 303 M \\\\\n& LoRA & $\\textbf{0.9560} \\pm 0.003$ & $\\textbf{0.7506} \\pm 0.019$ & $\\textbf{0.7657} \\pm 0.042$ & $\\textbf{0.7417} \\pm 0.067$ & 2.4 M \\\\\n\\hline\nViT\\textsubscript{small} & FP & $0.9360 \\pm 0.005$ & $0.5814 \\pm 0.059$ & $0.7002 \\pm 0.039$ & $0.5052 \\pm 0.097$ & 21.7 M \\\\\n\\hline\n\\end{tabular}\n}\n\\end{table}\n\\centering\n\\caption{Comparison of fine-tuning strategies for DenseNet and ViT. LP and FP denote linear probing and full-parameter fine-tuning respectively. The best average scores for each metric per model are \\textbf{bolded}.}\n\n\\label{tab:ablation}\n\\resizebox{\\textwidth}\\textwidth{!}!{\n\\begin{tabular}{|l|c|c|c|c|c|c|}\n\\hline\n\\textbf{Model} & \\textbf{Strategy} & \\textbf{Accuracy}$\\uparrow$ & \\textbf{F1 Score}$\\uparrow$ & \\textbf{Precision}$\\uparrow$ & \\textbf{Recall}$\\uparrow$ & \\textbf{\\# Trainable} \\\\\n\\hline\n\\multirow{2}{*}{DenseNet} & LP & $0.9187 \\pm 0.000$ & $0.3569 \\pm 0.018$ & $0.6113 \\pm 0.010$ & $0.2525 \\pm 0.019$ & 1.0 K \\\\\n& FP & $\\textbf{0.9516} \\pm 0.002$ & $\\textbf{0.7024} \\pm 0.028$ & $\\textbf{0.7805} \\pm 0.026$ & $\\textbf{0.6420} \\pm 0.059$ & 7.0 M \\\\\n\\hline\n\\multirow{3}{*}{ViT\\textsubscript{400M}} & LP & $0.9339 \\pm 0.001$ & $0.5273 \\pm 0.009$ & $0.7319 \\pm 0.007$ & $0.4123 \\pm 0.013$ & 1.0 K \\\\\n& FP & $0.9105 \\pm 0.000$ & $0.0000 \\pm 0.000$ & $0.0000 \\pm 0.000$ & $0.0000 \\pm 0.000$ & 303 M \\\\\n& LoRA & $\\textbf{0.9560} \\pm 0.003$ & $\\textbf{0.7506} \\pm 0.019$ & $\\textbf{0.7657} \\pm 0.042$ & $\\textbf{0.7417} \\pm 0.067$ & 2.4 M \\\\\n\\hline\nViT\\textsubscript{small} & FP & $0.9360 \\pm 0.005$ & $0.5814 \\pm 0.059$ & $0.7002 \\pm 0.039$ & $0.5052 \\pm 0.097$ & 21.7 M \\\\\n\\hline\n\\end{tabular}\n}\n\n\\hline\n\\textbf{Model} & \\textbf{Strategy} & \\textbf{Accuracy}$\\uparrow$\\uparrow & \\textbf{F1 Score}$\\uparrow$\\uparrow & \\textbf{Precision}$\\uparrow$\\uparrow & \\textbf{Recall}$\\uparrow$\\uparrow & \\textbf{\\# Trainable} \\\\\n\\hline\n\\multirow{2}2{*}*{DenseNet}DenseNet & LP & $0.9187 \\pm 0.000$0.9187 \\pm 0.000 & $0.3569 \\pm 0.018$0.3569 \\pm 0.018 & $0.6113 \\pm 0.010$0.6113 \\pm 0.010 & $0.2525 \\pm 0.019$0.2525 \\pm 0.019 & 1.0 K \\\\\n& FP & $\\textbf{0.9516} \\pm 0.002$\\textbf{0.9516} \\pm 0.002 & $\\textbf{0.7024} \\pm 0.028$\\textbf{0.7024} \\pm 0.028 & $\\textbf{0.7805} \\pm 0.026$\\textbf{0.7805} \\pm 0.026 & $\\textbf{0.6420} \\pm 0.059$\\textbf{0.6420} \\pm 0.059 & 7.0 M \\\\\n\\hline\n\\multirow{3}3{*}*{ViT\\textsubscript{400M}}ViT\\textsubscript{400M}400M & LP & $0.9339 \\pm 0.001$0.9339 \\pm 0.001 & $0.5273 \\pm 0.009$0.5273 \\pm 0.009 & $0.7319 \\pm 0.007$0.7319 \\pm 0.007 & $0.4123 \\pm 0.013$0.4123 \\pm 0.013 & 1.0 K \\\\\n& FP & $0.9105 \\pm 0.000$0.9105 \\pm 0.000 & $0.0000 \\pm 0.000$0.0000 \\pm 0.000 & $0.0000 \\pm 0.000$0.0000 \\pm 0.000 & $0.0000 \\pm 0.000$0.0000 \\pm 0.000 & 303 M \\\\\n& LoRA & $\\textbf{0.9560} \\pm 0.003$\\textbf{0.9560} \\pm 0.003 & $\\textbf{0.7506} \\pm 0.019$\\textbf{0.7506} \\pm 0.019 & $\\textbf{0.7657} \\pm 0.042$\\textbf{0.7657} \\pm 0.042 & $\\textbf{0.7417} \\pm 0.067$\\textbf{0.7417} \\pm 0.067 & 2.4 M \\\\\n\\hline\nViT\\textsubscript{small}small & FP & $0.9360 \\pm 0.005$0.9360 \\pm 0.005 & $0.5814 \\pm 0.059$0.5814 \\pm 0.059 & $0.7002 \\pm 0.039$0.7002 \\pm 0.039 & $0.5052 \\pm 0.097$0.5052 \\pm 0.097 & 21.7 M \\\\\n\\hline\n\n\n\n\nAs shown in Table~\\ref{tab:ablation}, full-parameter fine-tuning outperforms linear probing for DenseNet. However, full-parameter fine-tuning of ViT\\textsubscript{400M}400M yields zero positive predictions, likely due to its overwhelmingly large number of trainable parameters. In contrast, LoRA uses a small number of parameters and outperforms the other two strategies.\n\nWe conduct a simple experiment to verify whether zero positive predictions result from a mismatch between model and data sizes. Table~\\ref{tab:ablation} shows that full-parameter fine-tuning of a smaller ViT yields marginally acceptable performance. Based on these results, we adopt full-parameter fine-tuning for CNN-based models and LoRA for Transformer-based models.\n\n\n\\subsection{Domain Relevance Matters in Foundation Models}\n\n\\begin{table}[ht]\n\\centering\n\\caption{Model performance of ViT models pretrained on generic image-text datasets at varying scales and FetalCLIP\\textsubscript{CLS}. The best average scores per metric are \\textbf{bolded}.}\n\\label{tab:pretrain_scale}\n\\resizebox{\\textwidth}{!}{\n\\begin{tabular}{|l|c|c|c|c|c|c|}\n\\hline\n\\textbf{Model} & \\textbf{Accuracy}$\\uparrow$ & \\textbf{F1 Score}$\\uparrow$ & \\textbf{Precision}$\\uparrow$ & \\textbf{Recall}$\\uparrow$ & \\textbf{\\# Trainable} \\\\\n\\hline\nViT\\textsubscript{400M} & $0.9560 \\pm 0.003$ & $0.7506 \\pm 0.019$ & $0.7657 \\pm 0.042$ & $\\textbf{0.7417} \\pm 0.067$ & 2.4 M \\\\\nViT\\textsubscript{2B} & $0.9555 \\pm 0.002$ & $0.7429 \\pm 0.011$ & $0.7687 \\pm 0.017$ & $0.7196 \\pm 0.026$ & 2.4 M \\\\\n\nFetalCLIP\\textsubscript{CLS} & $\\textbf{0.9575} \\pm 0.001$ & $\\textbf{0.7570} \\pm 0.007$ & $\\textbf{0.7782} \\pm 0.034$ & $0.7397 \\pm 0.041$ & 2.4 M \\\\\n\\hline\n\\end{tabular}\n}\n\\end{table}\n\\centering\n\\caption{Model performance of ViT models pretrained on generic image-text datasets at varying scales and FetalCLIP\\textsubscript{CLS}. The best average scores per metric are \\textbf{bolded}.}\n\\label{tab:pretrain_scale}\n\\resizebox{\\textwidth}\\textwidth{!}!{\n\\begin{tabular}{|l|c|c|c|c|c|c|}\n\\hline\n\\textbf{Model} & \\textbf{Accuracy}$\\uparrow$ & \\textbf{F1 Score}$\\uparrow$ & \\textbf{Precision}$\\uparrow$ & \\textbf{Recall}$\\uparrow$ & \\textbf{\\# Trainable} \\\\\n\\hline\nViT\\textsubscript{400M} & $0.9560 \\pm 0.003$ & $0.7506 \\pm 0.019$ & $0.7657 \\pm 0.042$ & $\\textbf{0.7417} \\pm 0.067$ & 2.4 M \\\\\nViT\\textsubscript{2B} & $0.9555 \\pm 0.002$ & $0.7429 \\pm 0.011$ & $0.7687 \\pm 0.017$ & $0.7196 \\pm 0.026$ & 2.4 M \\\\\n\nFetalCLIP\\textsubscript{CLS} & $\\textbf{0.9575} \\pm 0.001$ & $\\textbf{0.7570} \\pm 0.007$ & $\\textbf{0.7782} \\pm 0.034$ & $0.7397 \\pm 0.041$ & 2.4 M \\\\\n\\hline\n\\end{tabular}\n}\n\n\\hline\n\\textbf{Model} & \\textbf{Accuracy}$\\uparrow$\\uparrow & \\textbf{F1 Score}$\\uparrow$\\uparrow & \\textbf{Precision}$\\uparrow$\\uparrow & \\textbf{Recall}$\\uparrow$\\uparrow & \\textbf{\\# Trainable} \\\\\n\\hline\nViT\\textsubscript{400M}400M & $0.9560 \\pm 0.003$0.9560 \\pm 0.003 & $0.7506 \\pm 0.019$0.7506 \\pm 0.019 & $0.7657 \\pm 0.042$0.7657 \\pm 0.042 & $\\textbf{0.7417} \\pm 0.067$\\textbf{0.7417} \\pm 0.067 & 2.4 M \\\\\nViT\\textsubscript{2B}2B & $0.9555 \\pm 0.002$0.9555 \\pm 0.002 & $0.7429 \\pm 0.011$0.7429 \\pm 0.011 & $0.7687 \\pm 0.017$0.7687 \\pm 0.017 & $0.7196 \\pm 0.026$0.7196 \\pm 0.026 & 2.4 M \\\\\n\nFetalCLIP\\textsubscript{CLS}CLS & $\\textbf{0.9575} \\pm 0.001$\\textbf{0.9575} \\pm 0.001 & $\\textbf{0.7570} \\pm 0.007$\\textbf{0.7570} \\pm 0.007 & $\\textbf{0.7782} \\pm 0.034$\\textbf{0.7782} \\pm 0.034 & $0.7397 \\pm 0.041$0.7397 \\pm 0.041 & 2.4 M \\\\\n\\hline\n\n\n\n\nTable~\\ref{tab:model_performance} shows that ViT\\textsubscript{400M}400M underperforms FetalCLIP\\textsubscript{CLS}CLS in accuracy, F1 score, and precision, despite FetalCLIP\\textsubscript{CLS}CLS being trained on only 210,000 domain-specific samples. To examine whether scaling up the generic pretraining data could bridge this performance gap, we evaluate a ViT model sharing the same architecture as ViT\\textsubscript{400M}400M, pretrained on the LAION-2B dataset containing 2 billion image-text pairs, and present the results in Table~\\ref{tab:pretrain_scale}. Notably, FetalCLIP\\textsubscript{CLS}CLS still outperforms ViT\\textsubscript{2B}2B even though the latter is pretrained on a dataset 10,000 times larger.\n\nWhile large quantities of training data are key to the generalizability of large pretrained models, our results show that comparable performance can be achieved using models pretrained on fewer but domain-specific data, highlighting the importance of domain relevance in foundation models.\n\n", "appendix": false}}, "categories": ["cs.CV", "cs.AI"], "published": "2025-07-30 16:09:29+00:00", "primary_category": "cs.CV", "summary": "Accurate fetal biometric measurements, such as abdominal circumference, play\na vital role in prenatal care. However, obtaining high-quality ultrasound\nimages for these measurements heavily depends on the expertise of sonographers,\nposing a significant challenge in low-income countries due to the scarcity of\ntrained personnel. To address this issue, we leverage FetalCLIP, a\nvision-language model pretrained on a curated dataset of over 210,000 fetal\nultrasound image-caption pairs, to perform automated fetal ultrasound image\nquality assessment (IQA) on blind-sweep ultrasound data. We introduce\nFetalCLIP$_{CLS}$, an IQA model adapted from FetalCLIP using Low-Rank\nAdaptation (LoRA), and evaluate it on the ACOUSLIC-AI dataset against six CNN\nand Transformer baselines. FetalCLIP$_{CLS}$ achieves the highest F1 score of\n0.757. Moreover, we show that an adapted segmentation model, when repurposed\nfor classification, further improves performance, achieving an F1 score of\n0.771. Our work demonstrates how parameter-efficient fine-tuning of fetal\nultrasound foundation models can enable task-specific adaptations, advancing\nprenatal care in resource-limited settings. The experimental code is available\nat: https://github.com/donglihe-hub/FetalCLIP-IQA."}