{"title": "Graph World Model", "author": "Tao Feng", "abstract": "\\begin{abstract}\n\nWorld models (WMs) demonstrate strong capabilities in prediction, generation, and planning tasks.\nExisting WMs primarily focus on unstructured data while cannot leverage the ubiquitous structured data, often represented as graphs, in the digital world. While multiple graph foundation models have been proposed, they focus on graph learning tasks and cannot extend to diverse multi-modal data and interdisciplinary tasks. To address these challenges, we propose the Graph World Model (GWM), a world model that supports both unstructured and graph-structured states with multi-modal information and represents diverse tasks as actions. The core of a GWM is a generic message-passing algorithm to aggregate structured information, either over a unified multi-modal token space by converting multi-modal data into text (GWM-T) or a unified multi-modal embedding space by modality-specific encoders (GWM-E). Notably, GWM introduces action nodes to support diverse tasks, where action nodes are linked to other nodes via direct reference or similarity computation. Extensive experiments on 6 tasks from diverse domains, including multi-modal generation and matching, recommendation, graph prediction, multi-agent, retrieval-augmented generation, and planning and optimization, show that the same GWM outperforms or matches domain-specific baselines' performance, benefits from multi-hop structures, and demonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our codes for GWM is released at \\url{https://github.com/ulab-uiuc/GWM}.\n\n\\end{abstract}", "citations": {"ha2018recurrent": {"bib_key": "ha2018recurrent", "bib_title": "Recurrent world models facilitate policy evolution", "bib_author ": "Ha, David", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "A world model (WM)\\citep{ha2018recurrent}", "next_context": "constructs the world observations as states and predicts future states based on given actions."}, {"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "\\xhdrWorld ModelThe WM\\citep{ha2018recurrent}", "next_context": "is to construct the world observations as states and predict future states based on given actions."}], "importance_score": 2.0}, "liu2024world": {"bib_key": "liu2024world", "bib_title": "World model on million-length video and language with blockwise ringattention", "bib_author ": "Liu, Hao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Modern world models are trained with massive data\\citep{liu2024world,cui2023universal}", "next_context": ", demonstrating successful prediction, generation, and planning capabilities."}], "importance_score": 0.5}, "cui2023universal": {"bib_key": "cui2023universal", "bib_title": "A Universal World Model Learned from Large Scale and Diverse Videos", "bib_author ": "Cui, Hanchen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Modern world models are trained with massive data\\citep{liu2024world,cui2023universal}", "next_context": ", demonstrating successful prediction, generation, and planning capabilities."}], "importance_score": 0.5}, "jin2018junction": {"bib_key": "jin2018junction", "bib_title": "Junction tree variational autoencoder for molecular graph generation", "bib_author ": "Jin, Wengong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "However, existing world models do not directly generalize to structured data, primarily graphs, that are ubiquitous in science\\cite{jin2018junction,you2018graph}", "next_context": "and industry\\cite{ying2018graph,you2022roland}and can be further enriched with multi-modal information\\cite{ektefaie2023multimodal}."}], "importance_score": 0.5}, "you2018graph": {"bib_key": "you2018graph", "bib_title": "Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation", "bib_author ": "You, Jiaxuan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "However, existing world models do not directly generalize to structured data, primarily graphs, that are ubiquitous in science\\cite{jin2018junction,you2018graph}", "next_context": "and industry\\cite{ying2018graph,you2022roland}and can be further enriched with multi-modal information\\cite{ektefaie2023multimodal}."}], "importance_score": 0.5}, "ying2018graph": {"bib_key": "ying2018graph", "bib_title": "Graph Convolutional Neural Networks for Web-Scale Recommender Systems", "bib_author ": "Ying, Rex", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "However, existing world models do not directly generalize to structured data, primarily graphs, that are ubiquitous in science\\cite{jin2018junction,you2018graph}and industry\\cite{ying2018graph,you2022roland}", "next_context": "and can be further enriched with multi-modal information\\cite{ektefaie2023multimodal}."}], "importance_score": 0.5}, "you2022roland": {"bib_key": "you2022roland", "bib_title": "ROLAND: graph learning framework for dynamic graphs", "bib_author ": "You, Jiaxuan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "However, existing world models do not directly generalize to structured data, primarily graphs, that are ubiquitous in science\\cite{jin2018junction,you2018graph}and industry\\cite{ying2018graph,you2022roland}", "next_context": "and can be further enriched with multi-modal information\\cite{ektefaie2023multimodal}."}], "importance_score": 0.5}, "ektefaie2023multimodal": {"bib_key": "ektefaie2023multimodal", "bib_title": "Multimodal learning with graphs", "bib_author ": "Ektefaie, Yasha", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "However, existing world models do not directly generalize to structured data, primarily graphs, that are ubiquitous in science\\cite{jin2018junction,you2018graph}and industry\\cite{ying2018graph,you2022roland}and can be further enriched with multi-modal information\\cite{ektefaie2023multimodal}", "next_context": "."}], "importance_score": 1.0}, "wu2024ivideogpt": {"bib_key": "wu2024ivideogpt", "bib_title": "iVideoGPT: Interactive VideoGPTs are Scalable World Models", "bib_author ": "Wu, Jialong", "arxiv_id": null, "short_id": "2405.15223", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "For example, iVideoGPT\\citep{wu2024ivideogpt}", "next_context": "and Genie\\citep{bruce2024genie}are successful world models over video data."}, {"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "Existing WMs\\citep{wu2024ivideogpt,bruce2024genie}", "next_context": "primarily focus on how to utilize unstructured data to predict state transitions, thereby enhancing the effectiveness of sequence generation tasks."}], "importance_score": 1.5}, "bruce2024genie": {"bib_key": "bruce2024genie", "bib_title": "Genie: Generative interactive environments", "bib_author ": "Bruce, Jake", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "For example, iVideoGPT\\citep{wu2024ivideogpt}and Genie\\citep{bruce2024genie}", "next_context": "are successful world models over video data."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "\\midruleGenie\\citep{bruce2024genie}", "next_context": ""}, {"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "Existing WMs\\citep{wu2024ivideogpt,bruce2024genie}", "next_context": "primarily focus on how to utilize unstructured data to predict state transitions, thereby enhancing the effectiveness of sequence generation tasks."}, {"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "Genie\\citep{bruce2024genie}", "next_context": "trained a foundation world model using a massive amount of unlabelled, serialized internet videos, which has provided benefits for the planning outcomes of downstream tasks."}], "importance_score": 3.5}, "zhang2021world": {"bib_key": "zhang2021world", "bib_title": "World model as a graph: Learning latent landmarks for planning", "bib_author ": "Zhang, Lunjun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Although some works\\citep{zhang2021world,zhu2022value}", "next_context": "attempt to model structured data in WM using graphs, they have focused solely on planning problems in a specific domain."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "L^3P\\citep{zhang2021world}", "next_context": ""}, {"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "\\citep{zhang2021world,zhu2022value}", "next_context": "have attempted to integrate structured data with GWM."}, {"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "L^3P\\citep{zhang2021world}", "next_context": "uses graphs to model each step of the agent's decision-making process and their connections, thus enhancing scalable planning in reinforcement learning."}], "importance_score": 3.0}, "zhu2022value": {"bib_key": "zhu2022value", "bib_title": "Value memory graph: A graph-structured world model for offline reinforcement learning", "bib_author ": "Zhu, Deyao", "arxiv_id": null, "short_id": "2206.04384", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Although some works\\citep{zhang2021world,zhu2022value}", "next_context": "attempt to model structured data in WM using graphs, they have focused solely on planning problems in a specific domain."}, {"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "\\citep{zhang2021world,zhu2022value}", "next_context": "have attempted to integrate structured data with GWM."}], "importance_score": 1.0}, "liu2023one": {"bib_key": "liu2023one", "bib_title": "One for all: Towards training one graph model for all classification tasks", "bib_author ": "Liu, Hao", "arxiv_id": null, "short_id": "2310.00149", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "In recent years, researchers have also explored the concept of the Graph Foundation Model (GFM)~\\citep{liu2023one,chen2024llaga}", "next_context": "."}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We compare GWM  against two traditional graph baselines, GCN\\citep{kipf2016semi}and GAT\\citep{velivckovic2017graph}, as well as two GFM baselines, LLAGA\\citep{chen2024llaga}and OFA\\citep{liu2023one}", "next_context": "."}, {"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "To further address the vast array of tasks and data, scholars have proposed the GFM\\citep{chen2024llaga,liu2023one}", "next_context": "to explore GNNs' zero-shot or few-shot capabilities\\citep{fey2023relational,cao2023relational,gao2020ci,chen2022gndan}to tackle challenges such as the cold start problem in recommendations."}, {"section": "More on GWM Task", "subsection": "Traditional graph prediction", "subsubsection": null, "prev_context": "(3) HIV~\\cite{liu2023one}", "next_context": ": HIV is a molecular dataset constructed from MOLHIV dataset~\\cite{wu2018moleculenet}that contains over 40,000 compounds annotated for their ability to inhibit HIV replication."}, {"section": "More on GWM Task", "subsection": "Traditional graph prediction", "subsubsection": null, "prev_context": "\\xhdrBaselines detailsThe settings for the baselines primarily follow LLAGA\\citep{chen2024llaga}and OFA\\citep{liu2023one}", "next_context": "."}, {"section": "More on GWM Task", "subsection": "Traditional graph prediction", "subsubsection": null, "prev_context": "\\item\\textbf{OFA}~\\cite{liu2023one}", "next_context": ": Unifies vision, language, and multi-modal learning tasks within a single framework, leveraging pre-trained knowledge to facilitate cross-modal understanding and adaptation."}], "importance_score": 5.0}, "chen2024llaga": {"bib_key": "chen2024llaga", "bib_title": "Llaga: Large language and graph assistant", "bib_author ": "Chen, Runjin", "arxiv_id": null, "short_id": "2402.08170", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "In recent years, researchers have also explored the concept of the Graph Foundation Model (GFM)~\\citep{liu2023one,chen2024llaga}", "next_context": "."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "LLAGA\\citep{chen2024llaga}", "next_context": ""}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We compare GWM  against two traditional graph baselines, GCN\\citep{kipf2016semi}and GAT\\citep{velivckovic2017graph}, as well as two GFM baselines, LLAGA\\citep{chen2024llaga}", "next_context": "and OFA\\citep{liu2023one}."}, {"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "To further address the vast array of tasks and data, scholars have proposed the GFM\\citep{chen2024llaga,liu2023one}", "next_context": "to explore GNNs' zero-shot or few-shot capabilities\\citep{fey2023relational,cao2023relational,gao2020ci,chen2022gndan}to tackle challenges such as the cold start problem in recommendations."}, {"section": "More on GWM Task", "subsection": "Traditional graph prediction", "subsubsection": null, "prev_context": "\\xhdrBaselines detailsThe settings for the baselines primarily follow LLAGA\\citep{chen2024llaga}", "next_context": "and OFA\\citep{liu2023one}."}, {"section": "More on GWM Task", "subsection": "Traditional graph prediction", "subsubsection": null, "prev_context": "\\item\\textbf{LLAGA}~\\cite{chen2024llaga}", "next_context": ": Integrates LLM with graph structures to enhance reasoning and information retrieval in multi-modal and structured data scenarios."}], "importance_score": 5.0}, "rombach2022high": {"bib_key": "rombach2022high", "bib_title": "High-resolution image synthesis with latent diffusion models", "bib_author ": "Rombach, Robin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Finally, the target nodes on the state graph and prompted action nodes will be fed into multi-modal decoders such as LLMs and Stable Diffusion\\cite{rombach2022high}", "next_context": "."}, {"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "\\citep{rombach2022high,zhang2023adding}", "next_context": "involves predicting missing modalities given the available modal information and their interconnections."}, {"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "The multi-modal matching task\\citep{rombach2022high}", "next_context": ", similar to CLIP's pre-training task\\citep{radford2021learning}, predicts the correspondence between modalities."}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We compare our approach against several baselines, including Stable Diffusion 1.5 (SD-1.5)\\citep{rombach2022high}", "next_context": ", its fine-tuned variant (SD-1.5 FT), the image-to-image model ControlNet\\citep{zhang2023adding}, and the SOTA INSTRUCTG2I model\\citep{jin2024instructg2i}."}, {"section": "More on GWM Task", "subsection": "Multi-modal generation and matching", "subsubsection": null, "prev_context": "\\item\\textbf{SD-1.5}: A pre-trained Stable Diffusion v1.5 model~\\cite{rombach2022high}", "next_context": "used for text-to-image generation without task-specific fine-tuning."}], "importance_score": 4.5}, "wangbiobridge": {"bib_key": "wangbiobridge", "bib_title": "BioBridge: Bridging Biomedical Foundation Models via Knowledge Graphs", "bib_author ": "Wang, Zifeng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "BioBridge\\citep{wangbiobridge}", "next_context": ""}], "importance_score": 1.0}, "hu2020open": {"bib_key": "hu2020open", "bib_title": "Open graph benchmark: Datasets for machine learning on graphs", "bib_author ": "Hu, Weihua", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Multi-modal World Represented by Graphs", "subsubsection": null, "prev_context": "For example, in the ogbn-arxiv dataset\\citep{hu2020open}", "next_context": ", edges are determined based on references between papers and historical collaborations between authors."}], "importance_score": 1.0}, "heumos2023best": {"bib_key": "heumos2023best", "bib_title": "Best practices for single-cell analysis across modalities", "bib_author ": "Heumos, Lukas", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Multi-modal World Represented by Graphs", "subsubsection": null, "prev_context": "A typical example is in many protein datasets\\citep{heumos2023best,stuart2019comprehensive,stuart2019integrative}", "next_context": ", where edges are obtained based on the similarity of certain node feature embeddings."}], "importance_score": 0.3333333333333333}, "stuart2019comprehensive": {"bib_key": "stuart2019comprehensive", "bib_title": "Comprehensive integration of single-cell data", "bib_author ": "Stuart, Tim", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Multi-modal World Represented by Graphs", "subsubsection": null, "prev_context": "A typical example is in many protein datasets\\citep{heumos2023best,stuart2019comprehensive,stuart2019integrative}", "next_context": ", where edges are obtained based on the similarity of certain node feature embeddings."}], "importance_score": 0.3333333333333333}, "stuart2019integrative": {"bib_key": "stuart2019integrative", "bib_title": "Integrative single-cell analysis", "bib_author ": "Stuart, Tim", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Multi-modal World Represented by Graphs", "subsubsection": null, "prev_context": "A typical example is in many protein datasets\\citep{heumos2023best,stuart2019comprehensive,stuart2019integrative}", "next_context": ", where edges are obtained based on the similarity of certain node feature embeddings."}], "importance_score": 0.3333333333333333}, "zhang2023adding": {"bib_key": "zhang2023adding", "bib_title": "Adding conditional control to text-to-image diffusion models", "bib_author ": "Zhang, Lvmin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "\\citep{rombach2022high,zhang2023adding}", "next_context": "involves predicting missing modalities given the available modal information and their interconnections."}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We compare our approach against several baselines, including Stable Diffusion 1.5 (SD-1.5)\\citep{rombach2022high}, its fine-tuned variant (SD-1.5 FT), the image-to-image model ControlNet\\citep{zhang2023adding}", "next_context": ", and the SOTA INSTRUCTG2I model\\citep{jin2024instructg2i}."}, {"section": "More on GWM Task", "subsection": "Multi-modal generation and matching", "subsubsection": null, "prev_context": "\\item\\textbf{ControlNet}: An extension of Stable Diffusion that incorporates structural guidance, such as edge maps or depth maps, to enhance control over generated images~\\cite{zhang2023adding}", "next_context": "."}], "importance_score": 2.5}, "radford2021learning": {"bib_key": "radford2021learning", "bib_title": "Learning transferable visual models from natural language supervision", "bib_author ": "Radford, Alec", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "The multi-modal matching task\\citep{rombach2022high}, similar to CLIP's pre-training task\\citep{radford2021learning}", "next_context": ", predicts the correspondence between modalities."}, {"section": "Token-based GFM", "subsection": "Instruction tuning", "subsubsection": null, "prev_context": "The text featuresh(c_T)\\in\\mathbf{R}^d\\timesl_c_Tare extracted using CLIP's text encoder\\citep{radford2021learning}", "next_context": "h(c_T)=\\text{CLIP}(c_T), wherel_c_Trepresents the prompt length andddenotes the feature dimensionality."}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "The generated images are evaluated using CLIP Score\\citep{radford2021learning}", "next_context": "and DINOv2\\citep{oquab2023dinov2}."}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "Meanwhile, our edge-level multi-modal matching task evaluates the correspondence between different modalities, using Contrastive MLP\\citep{liu2022mlp}, CLIP\\citep{radford2021learning}", "next_context": ", and fine-tuned CLIP on metrics such as Accuracy, Recall, and F1."}, {"section": "More on GWM Task", "subsection": "Multi-modal generation and matching", "subsubsection": null, "prev_context": "\\item\\textbf{CLIP}: A pre-trained vision-language model designed for image-text alignment, using contrastive learning to map corresponding image and text embeddings into a shared space~\\cite{radford2021learning}", "next_context": "."}], "importance_score": 5.0}, "jin2024instructg2i": {"bib_key": "jin2024instructg2i", "bib_title": "InstructG2I: Synthesizing Images from Multimodal Attributed Graphs", "bib_author ": "Jin, Bowen", "arxiv_id": null, "short_id": "2410.07157", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "It treats each modality as a state node and the correspondences between modalities (including cross-modality similarity relationships)\\citep{jin2024instructg2i}", "next_context": "as edges."}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "\\citep{jin2024instructg2i}", "next_context": "and the Multi-Modal-Paper dataset (detailed in Appendix~\\ref{ap:multi-modal generation and matching g})."}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We compare our approach against several baselines, including Stable Diffusion 1.5 (SD-1.5)\\citep{rombach2022high}, its fine-tuned variant (SD-1.5 FT), the image-to-image model ControlNet\\citep{zhang2023adding}, and the SOTA INSTRUCTG2I model\\citep{jin2024instructg2i}", "next_context": "."}, {"section": "More on GWM Task", "subsection": "Multi-modal generation and matching", "subsubsection": null, "prev_context": "\\textbf{(1) Goodreads}: The Goodreads dataset is a large-scale collection of book-related metadata, textual descriptions, and cover images, widely used in prior multi-modal research~\\cite{jin2024instructg2i}", "next_context": "."}], "importance_score": 4.0}, "ni2023content": {"bib_key": "ni2023content", "bib_title": "A content-driven micro-video recommendation dataset at scale", "bib_author ": "Ni, Yongxin", "arxiv_id": null, "short_id": "2309.15379", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "\\textbf{(2) Recommendation.}Recommendations\\citep{ni2023content,isinkaye2015recommendation,ko2022survey}", "next_context": "are based on the historical interactions and features of users and items to predict future interactions, as shown in Figure\\ref{fig: Instantiations of GWM}(b)."}], "importance_score": 0.3333333333333333}, "isinkaye2015recommendation": {"bib_key": "isinkaye2015recommendation", "bib_title": "Recommendation systems: Principles, methods and evaluation", "bib_author ": "Isinkaye, Folasade Olubusola", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "\\textbf{(2) Recommendation.}Recommendations\\citep{ni2023content,isinkaye2015recommendation,ko2022survey}", "next_context": "are based on the historical interactions and features of users and items to predict future interactions, as shown in Figure\\ref{fig: Instantiations of GWM}(b)."}], "importance_score": 0.3333333333333333}, "ko2022survey": {"bib_key": "ko2022survey", "bib_title": "A survey of recommendation systems: recommendation models, techniques, and application fields", "bib_author ": "Ko, Hyeyoung", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "\\textbf{(2) Recommendation.}Recommendations\\citep{ni2023content,isinkaye2015recommendation,ko2022survey}", "next_context": "are based on the historical interactions and features of users and items to predict future interactions, as shown in Figure\\ref{fig: Instantiations of GWM}(b)."}], "importance_score": 0.3333333333333333}, "kipf2016semi": {"bib_key": "kipf2016semi", "bib_title": "Semi-supervised classification with graph convolutional networks", "bib_author ": "Kipf, Thomas N", "arxiv_id": null, "short_id": "1609.02907", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "\\textbf{(3) Traditional graph prediction.}Traditional graph prediction\\citep{kipf2016semi,velivckovic2017graph,hamilton2017inductive}", "next_context": "primarily focuses on three types of tasks: node-level, edge-level, and graph-level."}, {"section": "Token-based GFM", "subsection": "Token-level message passing", "subsubsection": null, "prev_context": "In contrast to traditional graph message passing\\citep{kipf2016semi,hamilton2017inductive,velivckovic2017graph}", "next_context": ", we employ token-level message passing here, which aggregates the text information of neighboring nodes."}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We compare GWM  against two traditional graph baselines, GCN\\citep{kipf2016semi}", "next_context": "and GAT\\citep{velivckovic2017graph}, as well as two GFM baselines, LLAGA\\citep{chen2024llaga}and OFA\\citep{liu2023one}."}], "importance_score": 1.6666666666666665}, "velivckovic2017graph": {"bib_key": "velivckovic2017graph", "bib_title": "Graph attention networks", "bib_author ": "Veli{\\v{c}}kovi{\\'c}, Petar", "arxiv_id": null, "short_id": "1710.10903", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "\\textbf{(3) Traditional graph prediction.}Traditional graph prediction\\citep{kipf2016semi,velivckovic2017graph,hamilton2017inductive}", "next_context": "primarily focuses on three types of tasks: node-level, edge-level, and graph-level."}, {"section": "Token-based GFM", "subsection": "Token-level message passing", "subsubsection": null, "prev_context": "In contrast to traditional graph message passing\\citep{kipf2016semi,hamilton2017inductive,velivckovic2017graph}", "next_context": ", we employ token-level message passing here, which aggregates the text information of neighboring nodes."}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We compare GWM  against two traditional graph baselines, GCN\\citep{kipf2016semi}and GAT\\citep{velivckovic2017graph}", "next_context": ", as well as two GFM baselines, LLAGA\\citep{chen2024llaga}and OFA\\citep{liu2023one}."}], "importance_score": 1.6666666666666665}, "hamilton2017inductive": {"bib_key": "hamilton2017inductive", "bib_title": "Inductive representation learning on large graphs", "bib_author ": "Hamilton, Will", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "\\textbf{(3) Traditional graph prediction.}Traditional graph prediction\\citep{kipf2016semi,velivckovic2017graph,hamilton2017inductive}", "next_context": "primarily focuses on three types of tasks: node-level, edge-level, and graph-level."}, {"section": "Token-based GFM", "subsection": "Token-level message passing", "subsubsection": null, "prev_context": "In contrast to traditional graph message passing\\citep{kipf2016semi,hamilton2017inductive,velivckovic2017graph}", "next_context": ", we employ token-level message passing here, which aggregates the text information of neighboring nodes."}], "importance_score": 0.6666666666666666}, "zhuge2024language": {"bib_key": "zhuge2024language", "bib_title": "Language agents as optimizable graphs", "bib_author ": "Zhuge, Mingchen", "arxiv_id": null, "short_id": "2402.16823", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "\\xhdrWorld generation\\textbf{(4) Multi-agent collaboration.}As shown in Figure\\ref{fig: Instantiations of GWM}(d), the purpose\\citep{zhuge2024language,liu2023dynamic,wu2024agentkit}", "next_context": "of this task is to generate task-oriented outputs based on the interaction between agents and external knowledge, as well as communication among agents."}, {"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "\\citep{zhuge2024language,liu2023dynamic}", "next_context": "."}], "importance_score": 0.8333333333333333}, "liu2023dynamic": {"bib_key": "liu2023dynamic", "bib_title": "Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization", "bib_author ": "Liu, Zijun", "arxiv_id": null, "short_id": "2310.02170", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "\\xhdrWorld generation\\textbf{(4) Multi-agent collaboration.}As shown in Figure\\ref{fig: Instantiations of GWM}(d), the purpose\\citep{zhuge2024language,liu2023dynamic,wu2024agentkit}", "next_context": "of this task is to generate task-oriented outputs based on the interaction between agents and external knowledge, as well as communication among agents."}, {"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "\\citep{zhuge2024language,liu2023dynamic}", "next_context": "."}], "importance_score": 0.8333333333333333}, "wu2024agentkit": {"bib_key": "wu2024agentkit", "bib_title": "AgentKit: Flow Engineering with Graphs, not Coding", "bib_author ": "Wu, Yue", "arxiv_id": null, "short_id": "2404.11483", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "\\xhdrWorld generation\\textbf{(4) Multi-agent collaboration.}As shown in Figure\\ref{fig: Instantiations of GWM}(d), the purpose\\citep{zhuge2024language,liu2023dynamic,wu2024agentkit}", "next_context": "of this task is to generate task-oriented outputs based on the interaction between agents and external knowledge, as well as communication among agents."}], "importance_score": 0.3333333333333333}, "lewis2020retrieval": {"bib_key": "lewis2020retrieval", "bib_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks", "bib_author ": "Lewis, Patrick", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "\\textbf{(5) Retrieval-augmented generation.}The purpose of Retrieval-Augmented Generation (RAG) is to enhance the generation capabilities of Large Language Models (LLMs) by retrieving information from external knowledge\\citep{lewis2020retrieval,gao2023retrieval,zhao2024retrieval}", "next_context": "."}, {"section": "More on GWM Task", "subsection": "Retrieval-augmented generation", "subsubsection": null, "prev_context": "The purpose of Retrieval-Augmented Generation (RAG) is to enhance the generation capabilities of Large Language Models (LLMs) by retrieving information from external knowledge\\citep{lewis2020retrieval,gao2023retrieval,zhao2024retrieval}", "next_context": "."}], "importance_score": 0.6666666666666666}, "gao2023retrieval": {"bib_key": "gao2023retrieval", "bib_title": "Retrieval-augmented generation for large language models: A survey", "bib_author ": "Gao, Yunfan", "arxiv_id": null, "short_id": "2312.10997", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "\\textbf{(5) Retrieval-augmented generation.}The purpose of Retrieval-Augmented Generation (RAG) is to enhance the generation capabilities of Large Language Models (LLMs) by retrieving information from external knowledge\\citep{lewis2020retrieval,gao2023retrieval,zhao2024retrieval}", "next_context": "."}, {"section": "More on GWM Task", "subsection": "Retrieval-augmented generation", "subsubsection": null, "prev_context": "The purpose of Retrieval-Augmented Generation (RAG) is to enhance the generation capabilities of Large Language Models (LLMs) by retrieving information from external knowledge\\citep{lewis2020retrieval,gao2023retrieval,zhao2024retrieval}", "next_context": "."}], "importance_score": 0.6666666666666666}, "zhao2024retrieval": {"bib_key": "zhao2024retrieval", "bib_title": "Retrieval-augmented generation for ai-generated content: A survey", "bib_author ": "Zhao, Penghao", "arxiv_id": null, "short_id": "2402.19473", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "\\textbf{(5) Retrieval-augmented generation.}The purpose of Retrieval-Augmented Generation (RAG) is to enhance the generation capabilities of Large Language Models (LLMs) by retrieving information from external knowledge\\citep{lewis2020retrieval,gao2023retrieval,zhao2024retrieval}", "next_context": "."}, {"section": "More on GWM Task", "subsection": "Retrieval-augmented generation", "subsubsection": null, "prev_context": "The purpose of Retrieval-Augmented Generation (RAG) is to enhance the generation capabilities of Large Language Models (LLMs) by retrieving information from external knowledge\\citep{lewis2020retrieval,gao2023retrieval,zhao2024retrieval}", "next_context": "."}], "importance_score": 0.6666666666666666}, "edge2024local": {"bib_key": "edge2024local", "bib_title": "From local to global: A graph rag approach to query-focused summarization", "bib_author ": "Edge, Darren", "arxiv_id": null, "short_id": "2404.16130", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "Recent studies such as GraphRAG\\citep{edge2024local,peng2024graph}", "next_context": "have shown that modeling the relationships between data chunks in external knowledge can enhance the generative capabilities of RAG."}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "Following previous work like GraphRAG\\citep{edge2024local}", "next_context": ", we divide long context into chunks as nodes of the graph, and the edges between nodes are the similarity of their BERT embeddings."}, {"section": "More on GWM Task", "subsection": "Retrieval-augmented generation", "subsubsection": null, "prev_context": "In alignment with methodologies from previous studies such as GraphRAG\\citep{edge2024local}", "next_context": ", we segment long contexts into chunks that serve as graph nodes, with edges defined by the similarity of their BERT embeddings."}], "importance_score": 2.5}, "peng2024graph": {"bib_key": "peng2024graph", "bib_title": "Graph retrieval-augmented generation: A survey", "bib_author ": "Peng, Boci", "arxiv_id": null, "short_id": "2408.08921", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "Recent studies such as GraphRAG\\citep{edge2024local,peng2024graph}", "next_context": "have shown that modeling the relationships between data chunks in external knowledge can enhance the generative capabilities of RAG."}], "importance_score": 0.5}, "chen2021decision": {"bib_key": "chen2021decision", "bib_title": "Decision transformer: Reinforcement learning via sequence modeling", "bib_author ": "Chen, Lili", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "\\xhdrWorld optimization\\textbf{(6) Planning and optimization.}World optimization involves generating the next best decision based on a sequence of historical decisions\\citep{chen2021decision,zheng2022online,siebenborn2022crucial}", "next_context": "."}], "importance_score": 0.3333333333333333}, "zheng2022online": {"bib_key": "zheng2022online", "bib_title": "Online decision transformer", "bib_author ": "Zheng, Qinqing", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "\\xhdrWorld optimization\\textbf{(6) Planning and optimization.}World optimization involves generating the next best decision based on a sequence of historical decisions\\citep{chen2021decision,zheng2022online,siebenborn2022crucial}", "next_context": "."}], "importance_score": 0.3333333333333333}, "siebenborn2022crucial": {"bib_key": "siebenborn2022crucial", "bib_title": "How crucial is transformer in decision transformer?", "bib_author ": "Siebenborn, Max", "arxiv_id": null, "short_id": "2211.14655", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "\\xhdrWorld optimization\\textbf{(6) Planning and optimization.}World optimization involves generating the next best decision based on a sequence of historical decisions\\citep{chen2021decision,zheng2022online,siebenborn2022crucial}", "next_context": "."}], "importance_score": 0.3333333333333333}, "jiang2018graph": {"bib_key": "jiang2018graph", "bib_title": "Graph convolutional reinforcement learning", "bib_author ": "Jiang, Jiechuan", "arxiv_id": null, "short_id": "1810.09202", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "Many studies have shown that modeling the relationships between historical decisions using graphs can enhance the decision-making effectiveness of world optimization\\citep{jiang2018graph,munikoti2023challenges}", "next_context": "."}, {"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "The edges between these state nodes are often modeled based on their relationships, such as distance relationships\\citep{prates2019learning}and the similarity of embeddings\\citep{munikoti2023challenges, jiang2018graph}", "next_context": "."}], "importance_score": 1.0}, "munikoti2023challenges": {"bib_key": "munikoti2023challenges", "bib_title": "Challenges and opportunities in deep reinforcement learning with graph neural networks: A comprehensive review of algorithms and applications", "bib_author ": "Munikoti, Sai", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "Many studies have shown that modeling the relationships between historical decisions using graphs can enhance the decision-making effectiveness of world optimization\\citep{jiang2018graph,munikoti2023challenges}", "next_context": "."}, {"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "The edges between these state nodes are often modeled based on their relationships, such as distance relationships\\citep{prates2019learning}and the similarity of embeddings\\citep{munikoti2023challenges, jiang2018graph}", "next_context": "."}], "importance_score": 1.0}, "prates2019learning": {"bib_key": "prates2019learning", "bib_title": "Learning to solve np-complete problems: A graph neural network for decision tsp", "bib_author ": "Prates, Marcelo", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Graph World Model", "subsection": "Instantiations of GWM", "subsubsection": null, "prev_context": "The edges between these state nodes are often modeled based on their relationships, such as distance relationships\\citep{prates2019learning}", "next_context": "and the similarity of embeddings\\citep{munikoti2023challenges, jiang2018graph}."}], "importance_score": 1.0}, "liu2024visual": {"bib_key": "liu2024visual", "bib_title": "Visual instruction tuning", "bib_author ": "Liu, Haotian", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Token-based GFM", "subsection": "Multi-modality as token", "subsubsection": null, "prev_context": "Specifically, as shown in Figure\\ref{fig:framework}, for image nodesv^a, we utilize a pretrained image-to-text LLaVA model\\citep{liu2024visual}", "next_context": "Lto transform them into text nodesv^ta=L(v^a)."}], "importance_score": 1.0}, "zhang2023instruction": {"bib_key": "zhang2023instruction", "bib_title": "Instruction tuning for large language models: A survey", "bib_author ": "Zhang, Shengyu", "arxiv_id": null, "short_id": "2308.10792", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Token-based GFM", "subsection": "Instruction tuning", "subsubsection": null, "prev_context": "We followed the standard instruction tuning practice\\citep{zhang2023instruction,peng2023instruction}", "next_context": ", which encourages LLMs to adhere to user requests when returning the outputs."}], "importance_score": 0.5}, "peng2023instruction": {"bib_key": "peng2023instruction", "bib_title": "Instruction tuning with gpt-4", "bib_author ": "Peng, Baolin", "arxiv_id": null, "short_id": "2304.03277", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Token-based GFM", "subsection": "Instruction tuning", "subsubsection": null, "prev_context": "We followed the standard instruction tuning practice\\citep{zhang2023instruction,peng2023instruction}", "next_context": ", which encourages LLMs to adhere to user requests when returning the outputs."}], "importance_score": 0.5}, "wu2019simplifying": {"bib_key": "wu2019simplifying", "bib_title": "Simplifying graph convolutional networks", "bib_author ": "Wu, Felix", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Embedding-based GFM", "subsection": "Embedding-level message passing", "subsubsection": null, "prev_context": "\\xhdrMulti-hop aggregationWe designed a simplified GCN\\citep{wu2019simplifying,he2020lightgcn}", "next_context": "to implement multi-hop aggregation, which directly accomplishes parameter-free feature aggregation at the node feature level."}], "importance_score": 0.5}, "he2020lightgcn": {"bib_key": "he2020lightgcn", "bib_title": "Lightgcn: Simplifying and powering graph convolution network for recommendation", "bib_author ": "He, Xiangnan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Embedding-based GFM", "subsection": "Embedding-level message passing", "subsubsection": null, "prev_context": "\\xhdrMulti-hop aggregationWe designed a simplified GCN\\citep{wu2019simplifying,he2020lightgcn}", "next_context": "to implement multi-hop aggregation, which directly accomplishes parameter-free feature aggregation at the node feature level."}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "For these edge-level tasks, we benchmark our GWM model against recent state-of-the-art recommendation approaches, including FREEDOM\\citep{zhou2023tale}, as well as representative graph-based models such as LightGCN\\citep{he2020lightgcn}", "next_context": ", MMGCN\\citep{wei2019mmgcn}, and GRCN\\citep{wei2020graph}."}], "importance_score": 1.5}, "li2021prefix": {"bib_key": "li2021prefix", "bib_title": "Prefix-tuning: Optimizing continuous prompts for generation", "bib_author ": "Li, Xiang Lisa", "arxiv_id": null, "short_id": "2101.00190", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Embedding-based GFM", "subsection": "Projector tuning", "subsubsection": null, "prev_context": "We use a training approach similar to prefix tuning\\citep{li2021prefix}", "next_context": ", where we fix the LLM's parameters and only fine-tune the projectorf_c's parameters."}], "importance_score": 1.0}, "oquab2023dinov2": {"bib_key": "oquab2023dinov2", "bib_title": "Dinov2: Learning robust visual features without supervision", "bib_author ": "Oquab, Maxime", "arxiv_id": null, "short_id": "2304.07193", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "The generated images are evaluated using CLIP Score\\citep{radford2021learning}and DINOv2\\citep{oquab2023dinov2}", "next_context": "."}], "importance_score": 1.0}, "liu2022mlp": {"bib_key": "liu2022mlp", "bib_title": "An mlp-based algorithm for efficient contrastive graph recommendations", "bib_author ": "Liu, Siwei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "Meanwhile, our edge-level multi-modal matching task evaluates the correspondence between different modalities, using Contrastive MLP\\citep{liu2022mlp}", "next_context": ", CLIP\\citep{radford2021learning}, and fine-tuned CLIP on metrics such as Accuracy, Recall, and F1."}, {"section": "More on GWM Task", "subsection": "Multi-modal generation and matching", "subsubsection": null, "prev_context": "\\item\\textbf{Contrastive MLP}\\citep{liu2022mlp}", "next_context": ": A multi-layer perceptron trained to predict multi-modal matching by processing embeddings from different modalities."}], "importance_score": 2.0}, "mcauley2015image": {"bib_key": "mcauley2015image", "bib_title": "Image-based recommendations on styles and substitutes", "bib_author ": "McAuley, Julian", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "\\textbf{(2) Recommendation (Rec):}As Table~\\ref{tab:rec_data}of Appendix\\ref{apd:rec}illustrates, we utilize three benchmark datasets of varying scales\u2014Baby, Sports, and Clothing\u2014from Amazon's real-world product collections\\citep{mcauley2015image}", "next_context": "."}, {"section": "More on GWM Task", "subsection": "Recommendation", "subsubsection": null, "prev_context": "\\xhdrDataset descriptionsIn the recommendation task, we conduct extensive evaluations using three Amazon datasets extensively recognized in prior research~\\citep{mcauley2015image}", "next_context": ", specifically: Baby, Sports, and Outdoors, as well as Clothing Shoes, and Jewelry."}], "importance_score": 2.0}, "mmgcn": {"bib_key": "mmgcn", "bib_title": "MMGCN: Multi-modal graph convolution network for personalized recommendation of micro-video", "bib_author ": "Wei, Yinwei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "These datasets are commonly used in existing multi-modal graph recommendation systems\\citep{mmgcn, grcn}", "next_context": "."}, {"section": "More on GWM Task", "subsection": "Recommendation", "subsubsection": null, "prev_context": "\\item\\textbf{MMGCN}: Learns user preferences across multiple modalities via message-passing on modality-specific user-item graphs, improving recommendations in multimedia contexts~\\citep{mmgcn}", "next_context": "."}], "importance_score": 1.5}, "grcn": {"bib_key": "grcn", "bib_title": "Graph-refined convolutional network for multimedia recommendation with implicit feedback", "bib_author ": "Wei, Yinwei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "These datasets are commonly used in existing multi-modal graph recommendation systems\\citep{mmgcn, grcn}", "next_context": "."}, {"section": "More on GWM Task", "subsection": "Recommendation", "subsubsection": null, "prev_context": "\\item\\textbf{GRCN}: Refines interaction graphs using multimedia content to identify and remove noisy edges, thereby sharpening the recommendation process~\\citep{grcn}", "next_context": "."}], "importance_score": 1.5}, "zhou2023tale": {"bib_key": "zhou2023tale", "bib_title": "A tale of two graphs: Freezing and denoising graph structures for multimodal recommendation", "bib_author ": "Zhou, Xin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "For these edge-level tasks, we benchmark our GWM model against recent state-of-the-art recommendation approaches, including FREEDOM\\citep{zhou2023tale}", "next_context": ", as well as representative graph-based models such as LightGCN\\citep{he2020lightgcn}, MMGCN\\citep{wei2019mmgcn}, and GRCN\\citep{wei2020graph}."}], "importance_score": 1.0}, "wei2019mmgcn": {"bib_key": "wei2019mmgcn", "bib_title": "MMGCN: Multi-modal graph convolution network for personalized recommendation of micro-video", "bib_author ": "Wei, Yinwei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "For these edge-level tasks, we benchmark our GWM model against recent state-of-the-art recommendation approaches, including FREEDOM\\citep{zhou2023tale}, as well as representative graph-based models such as LightGCN\\citep{he2020lightgcn}, MMGCN\\citep{wei2019mmgcn}", "next_context": ", and GRCN\\citep{wei2020graph}."}], "importance_score": 1.0}, "wei2020graph": {"bib_key": "wei2020graph", "bib_title": "Graph-refined convolutional network for multimedia recommendation with implicit feedback", "bib_author ": "Wei, Yinwei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "For these edge-level tasks, we benchmark our GWM model against recent state-of-the-art recommendation approaches, including FREEDOM\\citep{zhou2023tale}, as well as representative graph-based models such as LightGCN\\citep{he2020lightgcn}, MMGCN\\citep{wei2019mmgcn}, and GRCN\\citep{wei2020graph}", "next_context": "."}], "importance_score": 1.0}, "chen2024exploring": {"bib_key": "chen2024exploring", "bib_title": "Exploring the potential of large language models (llms) in learning on graphs", "bib_author ": "Chen, Zhikai", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "\\textbf{(3) Traditional graph prediction (Graph):}We utilize Cora\\citep{chen2024exploring}", "next_context": ", PubMed\\citep{chen2024exploring}, and HIV\\citep{wu2018moleculenet}datasets."}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "\\textbf{(3) Traditional graph prediction (Graph):}We utilize Cora\\citep{chen2024exploring}, PubMed\\citep{chen2024exploring}", "next_context": ", and HIV\\citep{wu2018moleculenet}datasets."}, {"section": "More on GWM Task", "subsection": "Traditional graph prediction", "subsubsection": null, "prev_context": "(1) Cora~\\cite{chen2024exploring}", "next_context": ": Cora is a citation network in the computer science domain, where nodes represent research papers and edges denote citation relationships."}, {"section": "More on GWM Task", "subsection": "Traditional graph prediction", "subsubsection": null, "prev_context": "~\\cite{chen2024exploring}", "next_context": ": PubMed is a biomedical citation network, similar to Cora, with nodes representing papers and edges indicating citation relationships."}], "importance_score": 4.0}, "wu2018moleculenet": {"bib_key": "wu2018moleculenet", "bib_title": "MoleculeNet: a benchmark for molecular machine learning", "bib_author ": "Wu, Zhenqin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "\\textbf{(3) Traditional graph prediction (Graph):}We utilize Cora\\citep{chen2024exploring}, PubMed\\citep{chen2024exploring}, and HIV\\citep{wu2018moleculenet}", "next_context": "datasets."}, {"section": "More on GWM Task", "subsection": "Traditional graph prediction", "subsubsection": null, "prev_context": "(3) HIV~\\cite{liu2023one}: HIV is a molecular dataset constructed from MOLHIV dataset~\\cite{wu2018moleculenet}", "next_context": "that contains over 40,000 compounds annotated for their ability to inhibit HIV replication."}], "importance_score": 2.0}, "schmidgall2024agentclinic": {"bib_key": "schmidgall2024agentclinic", "bib_title": "AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments", "bib_author ": "Schmidgall, Samuel", "arxiv_id": null, "short_id": "2405.07960", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "\\textbf{(1) Multi-agent collaboration (Multi-agent):}We utilize a multi-modal agent benchmark called AgentClinic\\citep{schmidgall2024agentclinic}", "next_context": "(in Appendix\\ref{apd:agent}) to evaluate LLMs within simulated clinical environments."}, {"section": "More on GWM Task", "subsection": "Multi-agent collaboration", "subsubsection": null, "prev_context": "\\xhdrDataset descriptionsIn the multi-agent collaboration task, we evaluate GWM on AgentClinic~\\cite{schmidgall2024agentclinic}", "next_context": "benchmark, specifically AgentClinic-NEJM collected from the New England Journal of Medicine (NEJM) case challenges."}], "importance_score": 2.0}, "wei2022chain": {"bib_key": "wei2022chain", "bib_title": "Chain-of-thought prompting elicits reasoning in large language models", "bib_author ": "Wei, Jason", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We compare our approach with three LLM-based baselines: CoT\\citep{wei2022chain}", "next_context": ", ToT\\citep{yao2024tree}, and Few-Shot\\citep{madotto2021few}, as well as two additional baselines fine-tuned on the AgentClinic dataset."}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We compare GWM with three LLM baselines\u2014COT\\citep{wei2022chain}", "next_context": ", TOT\\citep{yao2024tree}, and T5\\citep{raffel2020exploring}fine-tuned on our dataset (T5 FT) \u2014using BERT-Score\\citep{zhang2019bertscore}(Precision, Recall, and F1 Score) as metrics."}, {"section": "More on GWM Task", "subsection": "Multi-agent collaboration", "subsubsection": null, "prev_context": "\\item\\textbf{CoT}: Adopts Chain-of-Thought~\\cite{wei2022chain}", "next_context": "prompting, which enhances reasoning by decomposing complex problems into intermediate steps, improving performance on multi-step reasoning tasks."}, {"section": "More on GWM Task", "subsection": "Planning and optimization", "subsubsection": null, "prev_context": "\\item\\textbf{COT}: It adopts Chain-of-Thought~\\cite{wei2022chain}", "next_context": "prompting into baseline Normal to enhance reasoning ability when predicting."}], "importance_score": 4.0}, "yao2024tree": {"bib_key": "yao2024tree", "bib_title": "Tree of thoughts: Deliberate problem solving with large language models", "bib_author ": "Yao, Shunyu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We compare our approach with three LLM-based baselines: CoT\\citep{wei2022chain}, ToT\\citep{yao2024tree}", "next_context": ", and Few-Shot\\citep{madotto2021few}, as well as two additional baselines fine-tuned on the AgentClinic dataset."}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We compare GWM with three LLM baselines\u2014COT\\citep{wei2022chain}, TOT\\citep{yao2024tree}", "next_context": ", and T5\\citep{raffel2020exploring}fine-tuned on our dataset (T5 FT) \u2014using BERT-Score\\citep{zhang2019bertscore}(Precision, Recall, and F1 Score) as metrics."}, {"section": "More on GWM Task", "subsection": "Multi-agent collaboration", "subsubsection": null, "prev_context": "\\item\\textbf{ToT}: Adopts Tree-of-Thought~\\cite{yao2024tree}", "next_context": "prompting, which explores multiple reasoning paths in a tree-like structure, enabling iterative evaluation and refinement for more robust decision-making."}], "importance_score": 3.0}, "madotto2021few": {"bib_key": "madotto2021few", "bib_title": "Few-shot bot: Prompt-based learning for dialogue systems", "bib_author ": "Madotto, Andrea", "arxiv_id": null, "short_id": "2110.08118", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We compare our approach with three LLM-based baselines: CoT\\citep{wei2022chain}, ToT\\citep{yao2024tree}, and Few-Shot\\citep{madotto2021few}", "next_context": ", as well as two additional baselines fine-tuned on the AgentClinic dataset."}], "importance_score": 1.0}, "beltagy2020longformer": {"bib_key": "beltagy2020longformer", "bib_title": "Longformer: The long-document transformer", "bib_author ": "Beltagy, Iz", "arxiv_id": null, "short_id": "2004.05150", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "Longformer\\citep{beltagy2020longformer}", "next_context": "is a strong baseline for long-document understanding."}], "importance_score": 1.0}, "bai2024longbench": {"bib_key": "bai2024longbench", "bib_title": "LongBench v2: Towards deeper understanding and reasoning on realistic long-context multitasks", "bib_author ": "Bai, Yushi", "arxiv_id": null, "short_id": "2412.15204", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "\\textbf{(2) Retrieval-augmented generation (RAG):}We utilize LongBench v2\\citep{bai2024longbench}", "next_context": ", a benchmark designed for challenging long-context question-answering (in Appendix\\ref{apd:RAG})."}, {"section": "More on GWM Task", "subsection": "Retrieval-augmented generation", "subsubsection": null, "prev_context": "We introduce its dataset and baselines as follows:\\xhdrDataset descriptionsWe employ LongBench v2\\citep{bai2024longbench}", "next_context": ", a benchmark specifically designed to test long-context understanding and reasoning."}], "importance_score": 2.0}, "robertson2009probabilistic": {"bib_key": "robertson2009probabilistic", "bib_title": "The probabilistic relevance framework: BM25 and beyond", "bib_author ": "Robertson, Stephen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We conduct comparisons with two RAG-based baselines\u2014BM25\\citep{robertson2009probabilistic}", "next_context": "and Dragon\\citep{lin2023train}\u2014and three long-context LLMs (128k), including Mistral Large 2, Command R+, and GPT-4o mini."}, {"section": "More on GWM Task", "subsection": "Retrieval-augmented generation", "subsubsection": null, "prev_context": "\\xhdrBaselines detailsWe conduct comparisons with two RAG-based baselines\u2014BM25\\citep{robertson2009probabilistic}", "next_context": "and Dragon\\citep{lin2023train}\u2014and three long-context LLMs (128k), including Mistral Large 2\\footnote{\\href{https://huggingface.co/mistralai/Mistral-Large-Instruct-2407}{https://huggingface.co/mistralai/Mistral-Large-Instruct-2407}}, Command R+\\footnote{\\href{https://huggingface.co/CohereForAI/c4ai-command-r-plus}{https://huggingface.co/CohereForAI/c4ai-command-r-plus}}, and GPT-4o mini\\footnote{\\href{https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}{https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}}."}], "importance_score": 2.0}, "lin2023train": {"bib_key": "lin2023train", "bib_title": "How to train your dragon: Diverse augmentation towards generalizable dense retrieval", "bib_author ": "Lin, Sheng-Chieh", "arxiv_id": null, "short_id": "2302.07452", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We conduct comparisons with two RAG-based baselines\u2014BM25\\citep{robertson2009probabilistic}and Dragon\\citep{lin2023train}", "next_context": "\u2014and three long-context LLMs (128k), including Mistral Large 2, Command R+, and GPT-4o mini."}, {"section": "More on GWM Task", "subsection": "Retrieval-augmented generation", "subsubsection": null, "prev_context": "\\xhdrBaselines detailsWe conduct comparisons with two RAG-based baselines\u2014BM25\\citep{robertson2009probabilistic}and Dragon\\citep{lin2023train}", "next_context": "\u2014and three long-context LLMs (128k), including Mistral Large 2\\footnote{\\href{https://huggingface.co/mistralai/Mistral-Large-Instruct-2407}{https://huggingface.co/mistralai/Mistral-Large-Instruct-2407}}, Command R+\\footnote{\\href{https://huggingface.co/CohereForAI/c4ai-command-r-plus}{https://huggingface.co/CohereForAI/c4ai-command-r-plus}}, and GPT-4o mini\\footnote{\\href{https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}{https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}}."}], "importance_score": 2.0}, "ho2016generative": {"bib_key": "ho2016generative", "bib_title": "Generative adversarial imitation learning", "bib_author ": "Ho, Jonathan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "\\item\\textbf{World optimization (Optimization)}:  Many existing works\\citep{ho2016generative,hussein2017imitation,yang2024embodied}", "next_context": "attempt optimization tasks by imitating the trajectory of expert strategies."}], "importance_score": 0.3333333333333333}, "hussein2017imitation": {"bib_key": "hussein2017imitation", "bib_title": "Imitation learning: A survey of learning methods", "bib_author ": "Hussein, Ahmed", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "\\item\\textbf{World optimization (Optimization)}:  Many existing works\\citep{ho2016generative,hussein2017imitation,yang2024embodied}", "next_context": "attempt optimization tasks by imitating the trajectory of expert strategies."}], "importance_score": 0.3333333333333333}, "yang2024embodied": {"bib_key": "yang2024embodied", "bib_title": "Embodied multi-modal agent trained by an llm from a parallel textworld", "bib_author ": "Yang, Yijun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "\\item\\textbf{World optimization (Optimization)}:  Many existing works\\citep{ho2016generative,hussein2017imitation,yang2024embodied}", "next_context": "attempt optimization tasks by imitating the trajectory of expert strategies."}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "Here, we utilize the expert strategy dataset from the text-based embodied task ALFWorld\\citep{shridhar2020alfworld,yang2024embodied}", "next_context": "."}, {"section": "More on GWM Task", "subsection": "Planning and optimization", "subsubsection": null, "prev_context": "\\xhdrDataset descriptionsWe employ the expert strategy dataset from the text-based embodied task framework, ALFWorld\\citep{shridhar2020alfworld,yang2024embodied}", "next_context": "."}], "importance_score": 1.3333333333333333}, "shridhar2020alfworld": {"bib_key": "shridhar2020alfworld", "bib_title": "Alfworld: Aligning text and embodied environments for interactive learning", "bib_author ": "Shridhar, Mohit", "arxiv_id": null, "short_id": "2010.03768", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "Here, we utilize the expert strategy dataset from the text-based embodied task ALFWorld\\citep{shridhar2020alfworld,yang2024embodied}", "next_context": "."}, {"section": "More on GWM Task", "subsection": "Planning and optimization", "subsubsection": null, "prev_context": "\\xhdrDataset descriptionsWe employ the expert strategy dataset from the text-based embodied task framework, ALFWorld\\citep{shridhar2020alfworld,yang2024embodied}", "next_context": "."}], "importance_score": 1.0}, "raffel2020exploring": {"bib_key": "raffel2020exploring", "bib_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "bib_author ": "Raffel, Colin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We compare GWM with three LLM baselines\u2014COT\\citep{wei2022chain}, TOT\\citep{yao2024tree}, and T5\\citep{raffel2020exploring}", "next_context": "fine-tuned on our dataset (T5 FT) \u2014using BERT-Score\\citep{zhang2019bertscore}(Precision, Recall, and F1 Score) as metrics."}], "importance_score": 1.0}, "zhang2019bertscore": {"bib_key": "zhang2019bertscore", "bib_title": "Bertscore: Evaluating text generation with bert", "bib_author ": "Zhang, Tianyi", "arxiv_id": null, "short_id": "1904.09675", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We compare GWM with three LLM baselines\u2014COT\\citep{wei2022chain}, TOT\\citep{yao2024tree}, and T5\\citep{raffel2020exploring}fine-tuned on our dataset (T5 FT) \u2014using BERT-Score\\citep{zhang2019bertscore}", "next_context": "(Precision, Recall, and F1 Score) as metrics."}], "importance_score": 1.0}, "diederik2014adam": {"bib_key": "diederik2014adam", "bib_title": "Adam: A method for stochastic optimization", "bib_author ": "Diederik, P Kingma", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We use Adam optimizer\\citep{diederik2014adam}", "next_context": "for model training and gradually decay the learning rate with LambdaLR scheduler."}], "importance_score": 1.0}, "fey2023relational": {"bib_key": "fey2023relational", "bib_title": "Relational deep learning: Graph representation learning on relational databases", "bib_author ": "Fey, Matthias", "arxiv_id": null, "short_id": "2312.04615", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "\\xhdrGraph for Modelling RelationsGraphs are highly effective in modeling complex relationships\\citep{fey2023relational,cao2023relational,gao2020ci,chen2022gndan,wu2022graph,yang2021consisrec}", "next_context": ", extracting nodes and edges to model relational data with embeddings."}, {"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "To further address the vast array of tasks and data, scholars have proposed the GFM\\citep{chen2024llaga,liu2023one}to explore GNNs' zero-shot or few-shot capabilities\\citep{fey2023relational,cao2023relational,gao2020ci,chen2022gndan}", "next_context": "to tackle challenges such as the cold start problem in recommendations."}], "importance_score": 0.41666666666666663}, "cao2023relational": {"bib_key": "cao2023relational", "bib_title": "Relational multi-task learning: Modeling relations between data and tasks", "bib_author ": "Cao, Kaidi", "arxiv_id": null, "short_id": "2303.07666", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "\\xhdrGraph for Modelling RelationsGraphs are highly effective in modeling complex relationships\\citep{fey2023relational,cao2023relational,gao2020ci,chen2022gndan,wu2022graph,yang2021consisrec}", "next_context": ", extracting nodes and edges to model relational data with embeddings."}, {"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "To further address the vast array of tasks and data, scholars have proposed the GFM\\citep{chen2024llaga,liu2023one}to explore GNNs' zero-shot or few-shot capabilities\\citep{fey2023relational,cao2023relational,gao2020ci,chen2022gndan}", "next_context": "to tackle challenges such as the cold start problem in recommendations."}], "importance_score": 0.41666666666666663}, "gao2020ci": {"bib_key": "gao2020ci", "bib_title": "CI-GNN: Building a category-instance graph for zero-shot video classification", "bib_author ": "Gao, Junyu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "\\xhdrGraph for Modelling RelationsGraphs are highly effective in modeling complex relationships\\citep{fey2023relational,cao2023relational,gao2020ci,chen2022gndan,wu2022graph,yang2021consisrec}", "next_context": ", extracting nodes and edges to model relational data with embeddings."}, {"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "To further address the vast array of tasks and data, scholars have proposed the GFM\\citep{chen2024llaga,liu2023one}to explore GNNs' zero-shot or few-shot capabilities\\citep{fey2023relational,cao2023relational,gao2020ci,chen2022gndan}", "next_context": "to tackle challenges such as the cold start problem in recommendations."}], "importance_score": 0.41666666666666663}, "chen2022gndan": {"bib_key": "chen2022gndan", "bib_title": "GNDAN: Graph navigated dual attention network for zero-shot learning", "bib_author ": "Chen, Shiming", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "\\xhdrGraph for Modelling RelationsGraphs are highly effective in modeling complex relationships\\citep{fey2023relational,cao2023relational,gao2020ci,chen2022gndan,wu2022graph,yang2021consisrec}", "next_context": ", extracting nodes and edges to model relational data with embeddings."}, {"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "To further address the vast array of tasks and data, scholars have proposed the GFM\\citep{chen2024llaga,liu2023one}to explore GNNs' zero-shot or few-shot capabilities\\citep{fey2023relational,cao2023relational,gao2020ci,chen2022gndan}", "next_context": "to tackle challenges such as the cold start problem in recommendations."}], "importance_score": 0.41666666666666663}, "wu2022graph": {"bib_key": "wu2022graph", "bib_title": "Graph neural networks in recommender systems: a survey", "bib_author ": "Wu, Shiwen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "\\xhdrGraph for Modelling RelationsGraphs are highly effective in modeling complex relationships\\citep{fey2023relational,cao2023relational,gao2020ci,chen2022gndan,wu2022graph,yang2021consisrec}", "next_context": ", extracting nodes and edges to model relational data with embeddings."}], "importance_score": 0.16666666666666666}, "yang2021consisrec": {"bib_key": "yang2021consisrec", "bib_title": "Consisrec: Enhancing gnn for social recommendation via consistent neighbor aggregation", "bib_author ": "Yang, Liangwei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "\\xhdrGraph for Modelling RelationsGraphs are highly effective in modeling complex relationships\\citep{fey2023relational,cao2023relational,gao2020ci,chen2022gndan,wu2022graph,yang2021consisrec}", "next_context": ", extracting nodes and edges to model relational data with embeddings."}], "importance_score": 0.16666666666666666}, "Thomas2017GCN": {"bib_key": "Thomas2017GCN", "bib_title": "Semi-Supervised Classification with Graph Convolutional Networks", "bib_author ": "Thomas N. Kipf", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "Graph Neural Networks (GNNs)\\citep{Thomas2017GCN,hamilton2017graphsage,velivckovic2017gat,schlichtkrull2017modeling}", "next_context": "have emerged as a dominant approach, particularly in recommendation systems\\citep{Erxue2022recommender}and social networks\\citep{wu2020social}."}, {"section": "More on GWM Task", "subsection": "Traditional graph prediction", "subsubsection": null, "prev_context": "\\item\\textbf{GCN}~\\cite{Thomas2017GCN}", "next_context": ": Applies spectral-based convolution operations to capture local graph structures and propagate information across nodes, serving as a fundamental baseline for graph-based learning."}], "importance_score": 1.25}, "hamilton2017graphsage": {"bib_key": "hamilton2017graphsage", "bib_title": "Inductive representation learning on large graphs", "bib_author ": "Hamilton, Will", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "Graph Neural Networks (GNNs)\\citep{Thomas2017GCN,hamilton2017graphsage,velivckovic2017gat,schlichtkrull2017modeling}", "next_context": "have emerged as a dominant approach, particularly in recommendation systems\\citep{Erxue2022recommender}and social networks\\citep{wu2020social}."}], "importance_score": 0.25}, "velivckovic2017gat": {"bib_key": "velivckovic2017gat", "bib_title": "Graph attention networks", "bib_author ": "Veli{\\v{c}}kovi{\\'c}, Petar", "arxiv_id": null, "short_id": "1710.10903", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "Graph Neural Networks (GNNs)\\citep{Thomas2017GCN,hamilton2017graphsage,velivckovic2017gat,schlichtkrull2017modeling}", "next_context": "have emerged as a dominant approach, particularly in recommendation systems\\citep{Erxue2022recommender}and social networks\\citep{wu2020social}."}, {"section": "More on GWM Task", "subsection": "Traditional graph prediction", "subsubsection": null, "prev_context": "\\item\\textbf{GAT}~\\cite{velivckovic2017gat}", "next_context": ": Enhances node representation learning by incorporating attention mechanisms, allowing adaptive weighting of neighboring nodes to improve feature aggregation."}], "importance_score": 1.25}, "schlichtkrull2017modeling": {"bib_key": "schlichtkrull2017modeling", "bib_title": "Modeling relational data with graph convolutional networks. arXiv", "bib_author ": "Schlichtkrull, M", "arxiv_id": null, "short_id": "1703.06103", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "Graph Neural Networks (GNNs)\\citep{Thomas2017GCN,hamilton2017graphsage,velivckovic2017gat,schlichtkrull2017modeling}", "next_context": "have emerged as a dominant approach, particularly in recommendation systems\\citep{Erxue2022recommender}and social networks\\citep{wu2020social}."}], "importance_score": 0.25}, "Erxue2022recommender": {"bib_key": "Erxue2022recommender", "bib_title": "Masked Transformer for Neighhourhood-aware Click-Through Rate Prediction", "bib_author ": "Erxue Min", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "Graph Neural Networks (GNNs)\\citep{Thomas2017GCN,hamilton2017graphsage,velivckovic2017gat,schlichtkrull2017modeling}have emerged as a dominant approach, particularly in recommendation systems\\citep{Erxue2022recommender}", "next_context": "and social networks\\citep{wu2020social}."}], "importance_score": 1.0}, "wu2020social": {"bib_key": "wu2020social", "bib_title": "Graph convolutional networks with markov random field reasoning for social spammer detection", "bib_author ": "Wu, Yongji", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Additional Related Work", "subsection": null, "subsubsection": null, "prev_context": "Graph Neural Networks (GNNs)\\citep{Thomas2017GCN,hamilton2017graphsage,velivckovic2017gat,schlichtkrull2017modeling}have emerged as a dominant approach, particularly in recommendation systems\\citep{Erxue2022recommender}and social networks\\citep{wu2020social}", "next_context": "."}], "importance_score": 1.0}, "wan-etal-2019-fine": {"bib_key": "wan-etal-2019-fine", "bib_title": "Fine-Grained Spoiler Detection from Large-Scale Review Corpora", "bib_author ": "Wan, Mengting", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "More on GWM Task", "subsection": "Multi-modal generation and matching", "subsubsection": null, "prev_context": "\\xhdrDataset descriptionsIn this study, we utilize two datasets for multi-modal generation and matching: the Goodreads dataset\\cite{wan-etal-2019-fine}", "next_context": "and our curated Multi-Modal-Paper dataset."}], "importance_score": 1.0}, "zhou2023mmrec": {"bib_key": "zhou2023mmrec", "bib_title": "Mmrec: Simplifying multimodal recommendation", "bib_author ": "Zhou, Xin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "More on GWM Task", "subsection": "Recommendation", "subsubsection": null, "prev_context": "Each dataset encompasses both visual and textual modalities and we use the extracted visual and textual features from existing work~\\citep{zhou2023mmrec}", "next_context": "."}], "importance_score": 1.0}, "lightgcn": {"bib_key": "lightgcn", "bib_title": "Lightgcn: Simplifying and powering graph convolution network for recommendation", "bib_author ": "He, Xiangnan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "More on GWM Task", "subsection": "Recommendation", "subsubsection": null, "prev_context": "\\item\\textbf{LightGCN}: Employs a simplified graph convolutional network to learn user and item interaction graph~\\citep{lightgcn}", "next_context": "."}], "importance_score": 1.0}, "brown2020languagemodelsfewshotlearners": {"bib_key": "brown2020languagemodelsfewshotlearners", "bib_title": "Language Models are Few-Shot Learners", "bib_author ": "Tom B. Brown", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "More on GWM Task", "subsection": "Multi-agent collaboration", "subsubsection": null, "prev_context": "\\item\\textbf{Few-shots}: Adopts Few-shot~\\cite{brown2020languagemodelsfewshotlearners}", "next_context": "prompting, where the model is provided with a limited number of in-context examples to guide task-specific reasoning without requiring fine-tuning."}], "importance_score": 1.0}, "hu2021lora": {"bib_key": "hu2021lora", "bib_title": "Lora: Low-rank adaptation of large language models", "bib_author ": "Hu, Edward J", "arxiv_id": null, "short_id": "2106.09685", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Hyper-parameters", "subsection": null, "subsubsection": null, "prev_context": "We apply Lora (Lora rank = 8)\\citep{hu2021lora}", "next_context": "for efficient training."}], "importance_score": 1.0}}, "refs": [], "table": [{"original": "\\begin{table}[t]\n    \\setlength\\tabcolsep{2pt}\n    \\caption{\\textbf{Comparison with existing representative works from three perspectives: task type, data structure, and model type.} Compared with existing WM and GFM, GWM can tackle multi-domain tasks and be applied to both structured and unstructured data. } \n    \\label{Tab:GWM_intro}\n    \\centering\n    \\resizebox{0.48\\textwidth}{!}{\n    \\begin{tabular}{lccc}\n        \\toprule\n        \\textbf{Method} & \\textbf{Task Type} & \\textbf{Data Structure} & \\textbf{Model Type} \\\\\n        \\midrule\n         Genie \\citep{bruce2024genie} & Video generation  & Unstructured & WM \\\\\n         $L^3P$ \\citep{zhang2021world} & Planning  & Structured & WM \\\\\n         BioBridge \\citep{wangbiobridge}  & Biomedical domain  & Structured & GFM \\\\ \n         LLAGA \\citep{chen2024llaga}  & Graph domain  & Structured & GFM \\\\ \\midrule\n        \\rowcolor{cyan!10} GWM-T  & Multiple domains & Both & GWM \\\\\n        \\rowcolor{cyan!10} GWM-E & Multiple domains & Both & GWM \\\\\n        \\bottomrule\n        \\end{tabular}}\n    % \\vspace{-5mm}\n\\end{table}", "caption": "\\caption{\\textbf{Comparison with existing representative works from three perspectives: task type, data structure, and model type.} Compared with existing WM and GFM, GWM can tackle multi-domain tasks and be applied to both structured and unstructured data. }", "label": "\\label{Tab:GWM_intro}", "tabular": "\\begin{tabular}{lccc}\n        \\toprule\n        \\textbf{Method} & \\textbf{Task Type} & \\textbf{Data Structure} & \\textbf{Model Type} \\\\\n        \\midrule\n         Genie \\citep{bruce2024genie} & Video generation  & Unstructured & WM \\\\\n         $L^3P$ \\citep{zhang2021world} & Planning  & Structured & WM \\\\\n         BioBridge \\citep{wangbiobridge}  & Biomedical domain  & Structured & GFM \\\\ \n         LLAGA \\citep{chen2024llaga}  & Graph domain  & Structured & GFM \\\\ \\midrule\n        \\rowcolor{cyan!10} GWM-T  & Multiple domains & Both & GWM \\\\\n        \\rowcolor{cyan!10} GWM-E & Multiple domains & Both & GWM \\\\\n        \\bottomrule\n        \\end{tabular}", "subtables": []}, {"original": "\\begin{table}[!tbp]\n\\setlength\\tabcolsep{3pt}\n\\centering\n\\begin{footnotesize}\n\\vspace{-4mm}\n\\caption{\\textbf{Multi-modal generation results on Goodreads and Multi-Modal-Paper}. This task is to predict the missing modality based on the given modality. Compared to specific baselines in image generation, GWM achieved the best results.} %\n\\label{tab:multi-modal generation}\n\\resizebox{0.43\\textwidth}{!}{\n\\begin{tabular}{ccccc}\n\\toprule\n& \\multicolumn{2}{c}{\\texttt{Goodreads}} & \\multicolumn{2}{c}{\\texttt{Multi-Modal-Paper}}  \\\\ \\cmidrule(lr){2-3} \\cmidrule(lr){4-5}\n\\textbf{Model} & CLIP   & DINOv2   & CLIP   & DINOv2    \\\\ \\midrule\nSD-1.5 & 42.16 &14.84  & 52.62 & 23.64\\\\\nSD-1.5 FT  &\\underline{45.81} &18.97  & 58.49  & 24.13 \\\\ \nControlNet  &42.20  &19.77  & 52.89 & \\underline{24.77}\\\\ \\midrule\nINSTRUCTG2I & \\textbf{50.37} & \\textbf{25.54} & 56.37 & 18.80 \\\\ \\midrule\nGWM-T &  47.46 & 20.91 & \\textbf{59.92} & 23.10 \\\\\nGWM-E & 45.23 & \\underline{20.87} & \\underline{59.84}  & \\textbf{26.03}  \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{footnotesize}\n\\end{table}", "caption": "\\caption{\\textbf{Multi-modal generation results on Goodreads and Multi-Modal-Paper}. This task is to predict the missing modality based on the given modality. Compared to specific baselines in image generation, GWM achieved the best results.}", "label": "\\label{tab:multi-modal generation}", "tabular": "\\begin{tabular}{ccccc}\n\\toprule\n& \\multicolumn{2}{c}{\\texttt{Goodreads}} & \\multicolumn{2}{c}{\\texttt{Multi-Modal-Paper}}  \\\\ \\cmidrule(lr){2-3} \\cmidrule(lr){4-5}\n\\textbf{Model} & CLIP   & DINOv2   & CLIP   & DINOv2    \\\\ \\midrule\nSD-1.5 & 42.16 &14.84  & 52.62 & 23.64\\\\\nSD-1.5 FT  &\\underline{45.81} &18.97  & 58.49  & 24.13 \\\\ \nControlNet  &42.20  &19.77  & 52.89 & \\underline{24.77}\\\\ \\midrule\nINSTRUCTG2I & \\textbf{50.37} & \\textbf{25.54} & 56.37 & 18.80 \\\\ \\midrule\nGWM-T &  47.46 & 20.91 & \\textbf{59.92} & 23.10 \\\\\nGWM-E & 45.23 & \\underline{20.87} & \\underline{59.84}  & \\textbf{26.03}  \\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[!tbp]\n\\setlength\\tabcolsep{3pt}\n\\centering\n\\begin{footnotesize}\n% \\vspace{-1mm}\n\\caption{\\textbf{Multi-modal matching results on Goodreads and Multi-Modal-Paper}. It aims to predict the correspondence between different modal-ities. For Goodreads, this task is to predict text-image correspondences. As for Multi-Modal-Paper, it aims to predict text-image, text-table, and table-image correspondences. Note that CLIP and CLIP FT cannot be applied to Multi-Modal-Paper since it includes matching tasks beyond text-image.} %\n\\label{tab:Multi-modal matching}\n\\resizebox{0.49\\textwidth}{!}{\n\\begin{tabular}{ccccccc}\n\\toprule\n & \\multicolumn{3}{c}{\\texttt{Goodreads}} & \\multicolumn{3}{c}{\\texttt{Multi-Modal-Paper}}  \\\\ \\cmidrule(lr){2-4} \\cmidrule(lr){5-7}\n\\textbf{Model} & Accuracy  & Recall  & F1 Score & Accuracy  & Recall  & F1 Score \\\\ \\midrule\nContrastive MLP &  54.70 & 54.67 & 54.79& 51.77 & 51.55 & 50.31\\\\ \\midrule\nCLIP & 83.80 & 83.80 & 83.84 & - & -& - \\\\\nCLIP FT   & \\textbf{92.60} & \\textbf{92.58} & \\textbf{92.61} & -& - & -   \\\\ \\midrule\nGWM-T   &84.22  &85.66  &85.29  & 88.26   &90.35   &90.11  \\\\\nGWM-E &\\underline{88.82}  &\\underline{89.73}  &\\underline{89.06}  & \\textbf{96.23}   & \\textbf{97.21}  & \\textbf{97.13}   \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{footnotesize}\n% \\vspace{-4mm}\n\\end{table}", "caption": "\\caption{\\textbf{Multi-modal matching results on Goodreads and Multi-Modal-Paper}. It aims to predict the correspondence between different modal-ities. For Goodreads, this task is to predict text-image correspondences. As for Multi-Modal-Paper, it aims to predict text-image, text-table, and table-image correspondences. Note that CLIP and CLIP FT cannot be applied to Multi-Modal-Paper since it includes matching tasks beyond text-image.}", "label": "\\label{tab:Multi-modal matching}", "tabular": "\\begin{tabular}{ccccccc}\n\\toprule\n & \\multicolumn{3}{c}{\\texttt{Goodreads}} & \\multicolumn{3}{c}{\\texttt{Multi-Modal-Paper}}  \\\\ \\cmidrule(lr){2-4} \\cmidrule(lr){5-7}\n\\textbf{Model} & Accuracy  & Recall  & F1 Score & Accuracy  & Recall  & F1 Score \\\\ \\midrule\nContrastive MLP &  54.70 & 54.67 & 54.79& 51.77 & 51.55 & 50.31\\\\ \\midrule\nCLIP & 83.80 & 83.80 & 83.84 & - & -& - \\\\\nCLIP FT   & \\textbf{92.60} & \\textbf{92.58} & \\textbf{92.61} & -& - & -   \\\\ \\midrule\nGWM-T   &84.22  &85.66  &85.29  & 88.26   &90.35   &90.11  \\\\\nGWM-E &\\underline{88.82}  &\\underline{89.73}  &\\underline{89.06}  & \\textbf{96.23}   & \\textbf{97.21}  & \\textbf{97.13}   \\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[!tbp]\n\\setlength\\tabcolsep{3pt}\n\\centering\n\\begin{footnotesize}\n\\vspace{-1mm}\n\\caption{\\textbf{Recommendation on Baby, Sports, and Clothing}. Compared with three classical graph baselines, GWM achieved state-of-the-art results on most metrics.} %\n\\label{tab:Recommender systems}\n\\resizebox{0.48\\textwidth}{!}{\n\\begin{tabular}{cccccccccc}\n\\toprule\n& \\multicolumn{2}{c}{\\texttt{Baby}} & \\multicolumn{2}{c}{\\texttt{Sports}} & \\multicolumn{2}{c}{\\texttt{Clothing}} \\\\ \\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-7}\n\\textbf{Model}   & Recall  & F1 Score   & Recall  & F1 Score & Recall  & F1 Score \\\\ \\midrule\nFREEDOM & 60.35 & 66.16 & 63.47 & 70.53 & 70.20 & \\underline{78.40} \\\\\n\\midrule\nLightGCN & 51.11 & 38.22 & \\underline{85.36} & \\textbf{91.32} & 69.08 & 77.21 \\\\\nMMGCN & 57.34 & 61.31 & 61.69 & 68.08 & 64.09 & 71.26 \\\\\nGRCN & \\underline{74.35} & \\underline{82.47} & 57.31 & 61.23 & 57.60 & 61.74 \\\\ \\midrule\nGWM-T &  70.84 & 75.08  & 84.29 & 88.60   & \\underline{71.73} & 74.26    \\\\\nGWM-E  & \\textbf{76.72} & \\textbf{84.74}  & \\textbf{88.78} & \\underline{90.32}   & \\textbf{75.27} & \\textbf{84.06}  \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{footnotesize}\n\\end{table}", "caption": "\\caption{\\textbf{Recommendation on Baby, Sports, and Clothing}. Compared with three classical graph baselines, GWM achieved state-of-the-art results on most metrics.}", "label": "\\label{tab:Recommender systems}", "tabular": "\\begin{tabular}{cccccccccc}\n\\toprule\n& \\multicolumn{2}{c}{\\texttt{Baby}} & \\multicolumn{2}{c}{\\texttt{Sports}} & \\multicolumn{2}{c}{\\texttt{Clothing}} \\\\ \\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-7}\n\\textbf{Model}   & Recall  & F1 Score   & Recall  & F1 Score & Recall  & F1 Score \\\\ \\midrule\nFREEDOM & 60.35 & 66.16 & 63.47 & 70.53 & 70.20 & \\underline{78.40} \\\\\n\\midrule\nLightGCN & 51.11 & 38.22 & \\underline{85.36} & \\textbf{91.32} & 69.08 & 77.21 \\\\\nMMGCN & 57.34 & 61.31 & 61.69 & 68.08 & 64.09 & 71.26 \\\\\nGRCN & \\underline{74.35} & \\underline{82.47} & 57.31 & 61.23 & 57.60 & 61.74 \\\\ \\midrule\nGWM-T &  70.84 & 75.08  & 84.29 & 88.60   & \\underline{71.73} & 74.26    \\\\\nGWM-E  & \\textbf{76.72} & \\textbf{84.74}  & \\textbf{88.78} & \\underline{90.32}   & \\textbf{75.27} & \\textbf{84.06}  \\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[!tbp]\n\\setlength\\tabcolsep{5pt} % \u8bbe\u7f6e\u5217\u95f4\u8ddd\n\\centering\n\\begin{footnotesize}\n% \\vspace{-1mm}\n\\caption{\\textbf{Traditional graph prediction results on Cora, PubMed, and HIV}. It covers representative tasks at the node-level, edge-level, and graph-level. Compared to classic graph baselines and GFM methods, our GWM can match their performance with one unified model.}%It covers representative tasks at the node-level, edge-level, and graph-level. Compared to classic graph baselines and GFM methods, our GWM can match their performance with one unified model. \n\\label{tab:Traditional graph prediction}\n\\resizebox{0.4\\textwidth}{!}{\n\\begin{tabular}{lccccccc}\n\\toprule\n\\textbf{Model} & \\multicolumn{2}{c}{\\texttt{Cora}} & \\multicolumn{2}{c}{\\texttt{PubMed}} & \\multicolumn{1}{c}{\\texttt{HIV}} \\\\ \\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-6}\nTask Type & Node   & Link  & Node   & Link  & Graph  \\\\\\midrule\n\nGCN & 78.86  & 90.40  &74.49  &91.10  &86.72  \\\\\nGAT & 82.76  & \\underline{93.70}  &75.24  &91.20  &87.84   \\\\ \\midrule\nLLAGA &\\textbf{89.22}   & 89.18   &\\textbf{95.03}   &89.18   & 85.42  \\\\ \nOFA &73.21  & 93.12  &77.80  &\\textbf{96.39}  &92.04  \\\\ \\midrule\nGWM-T &81.92  &88.24  &\\underline{92.91}  &91.88  &\\underline{92.20}  \\\\\nGWM-E &\\underline{83.03}  & \\textbf{94.31}  & 84.22  &\\underline{94.01}  &\\textbf{93.86}  \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{footnotesize}\n% \\vspace{-6mm}\n\\end{table}", "caption": "\\caption{\\textbf{Traditional graph prediction results on Cora, PubMed, and HIV}. It covers representative tasks at the node-level, edge-level, and graph-level. Compared to classic graph baselines and GFM methods, our GWM can match their performance with one unified model.}", "label": "\\label{tab:Traditional graph prediction}", "tabular": "\\begin{tabular}{lccccccc}\n\\toprule\n\\textbf{Model} & \\multicolumn{2}{c}{\\texttt{Cora}} & \\multicolumn{2}{c}{\\texttt{PubMed}} & \\multicolumn{1}{c}{\\texttt{HIV}} \\\\ \\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-6}\nTask Type & Node   & Link  & Node   & Link  & Graph  \\\\\\midrule\n\nGCN & 78.86  & 90.40  &74.49  &91.10  &86.72  \\\\\nGAT & 82.76  & \\underline{93.70}  &75.24  &91.20  &87.84   \\\\ \\midrule\nLLAGA &\\textbf{89.22}   & 89.18   &\\textbf{95.03}   &89.18   & 85.42  \\\\ \nOFA &73.21  & 93.12  &77.80  &\\textbf{96.39}  &92.04  \\\\ \\midrule\nGWM-T &81.92  &88.24  &\\underline{92.91}  &91.88  &\\underline{92.20}  \\\\\nGWM-E &\\underline{83.03}  & \\textbf{94.31}  & 84.22  &\\underline{94.01}  &\\textbf{93.86}  \\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[ht]\n\\setlength\\tabcolsep{3pt}\n\\centering\n\\begin{footnotesize}\n% \\vspace{-6mm}\n\\caption{\\textbf{Multi-agent collaboration results on AgentClinic}. This task is to answer medical questions by leveraging interactions between agents and external knowledge sources. Compared to classic LLM baselines, GWM-T achieves state-of-the-art results.} %This task is to answer medical questions by leveraging interactions between agents and external knowledge sources. Compared to classic LLM baselines, GWM-T achieves state-of-the-art results.\n\\label{tab:Multi-agent collaboration}\n\\resizebox{0.35\\textwidth}{!}{\n\\begin{tabular}{cccc}\n\\toprule\n\\textbf{Model} & Accuracy  & Recall  & F1 Score \\\\ \\midrule\nCOT & \\underline{45.00} & 33.42 & 32.25 \\\\\nTOT &  35.00  & 33.71 & 29.71 \\\\ \nFew-shots & 40.00 & 40.63 & 29.37 \\\\\\midrule\nLongformer & 25.00 & 20.20 & 14.00 \\\\ \nFT & \\underline{45.00} & \\underline{45.40} & \\underline{44.00} \\\\ \n\\midrule\nGWM-T & \\textbf{50.00}   &\\textbf{46.42}  &\\textbf{48.20}  \\\\\nGWM-E &\\underline{45.00}  &39.57  & 35.56  \\\\\n\\bottomrule\n\\end{tabular}\n}\n%\\vspace{-3.5mm}\n\\end{footnotesize}\n\\end{table}", "caption": "\\caption{\\textbf{Multi-agent collaboration results on AgentClinic}. This task is to answer medical questions by leveraging interactions between agents and external knowledge sources. Compared to classic LLM baselines, GWM-T achieves state-of-the-art results.}", "label": "\\label{tab:Multi-agent collaboration}", "tabular": "\\begin{tabular}{cccc}\n\\toprule\n\\textbf{Model} & Accuracy  & Recall  & F1 Score \\\\ \\midrule\nCOT & \\underline{45.00} & 33.42 & 32.25 \\\\\nTOT &  35.00  & 33.71 & 29.71 \\\\ \nFew-shots & 40.00 & 40.63 & 29.37 \\\\\\midrule\nLongformer & 25.00 & 20.20 & 14.00 \\\\ \nFT & \\underline{45.00} & \\underline{45.40} & \\underline{44.00} \\\\ \n\\midrule\nGWM-T & \\textbf{50.00}   &\\textbf{46.42}  &\\textbf{48.20}  \\\\\nGWM-E &\\underline{45.00}  &39.57  & 35.56  \\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[!tbp]\n\\setlength\\tabcolsep{3pt}\n\\centering\n\\begin{footnotesize}\n% \\vspace{-4mm}\n\\caption{\\textbf{Retrieval-augmented generation on LongBench v2}. It is a challenging long-context question-answering task that is categorized into easy and hard levels. Compared to classic RAG baselines and LLM models with extended contexts, GWM with limited context length achieved the best results. The result also demonstrates the superiority of GWM-E over GWM-T in tasks involving long contexts.} %It is a challenging long-context question-answering task that is categorized into easy and hard levels. Compared to classic RAG baselines and LLM models with extended contexts, GWM with limited context length achieved the best results. The result also demonstrates the superiority of GWM-E over GWM-T in tasks involving long contexts.\n\\label{tab:Retrieval-augmented generation}\n\\resizebox{0.39\\textwidth}{!}{\n\\begin{tabular}{cccc}\n\\toprule\n\\textbf{Model} & Overall  & Easy  & Hard    \\\\ \\midrule\n\nBM25 (2k) & 27.45 & \\textbf{41.18} & 20.59 \\\\ \nDragon (2k) & 23.53 & 35.29 & 17.65 \\\\ \\midrule\n% Full Context (left) & 27.45 & 35.29 & 23.53 \\\\\n% Full Context (right)  & 31.37 & 41.18 & 26.47 \\\\ \\midrule\n\nMistral Large 2 (128k) & 26.31 & 29.42  & 24.45    \\\\\nCommand R+ (128k) & 27.43 & 30.19  & 26.32    \\\\\nGPT-4o mini (128k) &29.01  &30.23  & \\underline{28.03}  \\\\ \\midrule\nGWM-T (2k) & \\underline{29.40}  &35.71  &21.74  \\\\\nGWM-E (2k) & \\textbf{33.32}  & \\underline{39.16}  & \\textbf{29.52}  \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{footnotesize}\n\\end{table}", "caption": "\\caption{\\textbf{Retrieval-augmented generation on LongBench v2}. It is a challenging long-context question-answering task that is categorized into easy and hard levels. Compared to classic RAG baselines and LLM models with extended contexts, GWM with limited context length achieved the best results. The result also demonstrates the superiority of GWM-E over GWM-T in tasks involving long contexts.}", "label": "\\label{tab:Retrieval-augmented generation}", "tabular": "\\begin{tabular}{cccc}\n\\toprule\n\\textbf{Model} & Overall  & Easy  & Hard    \\\\ \\midrule\n\nBM25 (2k) & 27.45 & \\textbf{41.18} & 20.59 \\\\ \nDragon (2k) & 23.53 & 35.29 & 17.65 \\\\ \\midrule\n% Full Context (left) & 27.45 & 35.29 & 23.53 \\\\\n% Full Context (right)  & 31.37 & 41.18 & 26.47 \\\\ \\midrule\n\nMistral Large 2 (128k) & 26.31 & 29.42  & 24.45    \\\\\nCommand R+ (128k) & 27.43 & 30.19  & 26.32    \\\\\nGPT-4o mini (128k) &29.01  &30.23  & \\underline{28.03}  \\\\ \\midrule\nGWM-T (2k) & \\underline{29.40}  &35.71  &21.74  \\\\\nGWM-E (2k) & \\textbf{33.32}  & \\underline{39.16}  & \\textbf{29.52}  \\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[!tbp]\n\\setlength\\tabcolsep{3pt}\n\\centering\n\\begin{footnotesize}\n% \\vspace{-4mm}\n\\caption{\\textbf{Planning and optimization results on ALFWorld}. It is to evaluate how well the methods can imitate the trajectory of expert strategies to effectively assist in solving optimization problems. Compared to classic LLM baselines and text generation baselines, GWM-E has achieved the best results.} %It is to evaluate how well the methods can imitate the trajectory of expert strategies to effectively assist in solving optimization problems. Compared to classic LLM baselines and text generation baselines, GWM-E has achieved the best results.\n\\label{tab:Planning and optimization}\n\\resizebox{0.32\\textwidth}{!}{\n\\begin{tabular}{cccc}\n\\toprule\n\\textbf{Model} & Precision  & Recall  & F1 Score \\\\ \\midrule\n\nNormal & 89.62 & 88.86 & 89.21 \\\\ \n% Few-shots & 99.30 & 99.21 & 99.25 \\\\\nCOT & 86.87 & 87.74 & 87.27 \\\\  \\midrule\nT5 FT & \\underline{92.06} & \\underline{91.52} & \\underline{91.82} \\\\ \\midrule\nGWM-T &88.10   & 87.05  & 87.42  \\\\\nGWM-E &\\textbf{93.27}  &\\textbf{92.36}  & \\textbf{92.13} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{footnotesize}\n% \\vspace{-4mm}\n\\end{table}", "caption": "\\caption{\\textbf{Planning and optimization results on ALFWorld}. It is to evaluate how well the methods can imitate the trajectory of expert strategies to effectively assist in solving optimization problems. Compared to classic LLM baselines and text generation baselines, GWM-E has achieved the best results.}", "label": "\\label{tab:Planning and optimization}", "tabular": "\\begin{tabular}{cccc}\n\\toprule\n\\textbf{Model} & Precision  & Recall  & F1 Score \\\\ \\midrule\n\nNormal & 89.62 & 88.86 & 89.21 \\\\ \n% Few-shots & 99.30 & 99.21 & 99.25 \\\\\nCOT & 86.87 & 87.74 & 87.27 \\\\  \\midrule\nT5 FT & \\underline{92.06} & \\underline{91.52} & \\underline{91.82} \\\\ \\midrule\nGWM-T &88.10   & 87.05  & 87.42  \\\\\nGWM-E &\\textbf{93.27}  &\\textbf{92.36}  & \\textbf{92.13} \\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[t]\n\\small\n \\setlength{\\tabcolsep}{7pt}\n \\centering\n \\renewcommand{\\arraystretch}{1}\n \\setlength\\tabcolsep{1pt}\n    \\caption{\\textbf{Detailed summarization of all collected datasets in GWM.} We summarize the dataset names, tasks, action level, multi-modality, nodes, and edges in the table.}\n    \\label{tab:dataset_all}\n    \\centering\n    \\begin{tabular}{l|ccccc}\n        \\toprule\n        \\textbf{Dataset}  & \\textbf{Task} & \\textbf{Action Level} &  \\textbf{Multi-modality} & \\textbf{Nodes} & \\textbf{Edges}    \\\\ \\midrule\n        Goodreads & Multi-modal generation/matching & Node level & Text/image & Text/image nodes &  Similar-book semantic   \\\\\n        Multi-Modal-Paper & Multi-modal generation/matching & Node level & Text/image/table & Text/image/table nodes & References \\\\\n        Baby & Recommendation & Link level & Text/table/image & User/item nodes & User-item interactions  \\\\\n        Sports & Recommendation & Link level & Text/table/image & User/item nodes & User-item interactions  \\\\\n        Clothing & Recommendation & Link level & Text/table/image & User/item nodes & User-item interactions  \\\\\n        Cora & Traditional graph prediction  & Node/edge level & Text & Research paper nodes & Citation  \\\\\n        PubMed & Traditional graph prediction  & Node/edge level & Text & Research paper nodes & Citation  \\\\\n        HIV & Traditional graph prediction & Graph level & Text & Atoms nodes & Atoms bonds \\\\\n        AgentClinic&  Multi-agent collaboration  & Graph level & Text/image & Agent/image/text nodes& Agent-agent/image/text \\\\\n        LongBench v2& Retrieval-augmented generation & Unintended action  & Text & Chunk nodes& Chunk-chunk similarity \\\\\n        ALFWorld& Planning and optimization & Graph level & Text/image & State  nodes &  State images similarity \\\\ \n        \\bottomrule\n    \\end{tabular}\n% }\n\n\\end{table*}", "caption": "\\caption{\\textbf{Detailed summarization of all collected datasets in GWM.} We summarize the dataset names, tasks, action level, multi-modality, nodes, and edges in the table.}", "label": "\\label{tab:dataset_all}", "tabular": "\\begin{tabular}{l|ccccc}\n        \\toprule\n        \\textbf{Dataset}  & \\textbf{Task} & \\textbf{Action Level} &  \\textbf{Multi-modality} & \\textbf{Nodes} & \\textbf{Edges}    \\\\ \\midrule\n        Goodreads & Multi-modal generation/matching & Node level & Text/image & Text/image nodes &  Similar-book semantic   \\\\\n        Multi-Modal-Paper & Multi-modal generation/matching & Node level & Text/image/table & Text/image/table nodes & References \\\\\n        Baby & Recommendation & Link level & Text/table/image & User/item nodes & User-item interactions  \\\\\n        Sports & Recommendation & Link level & Text/table/image & User/item nodes & User-item interactions  \\\\\n        Clothing & Recommendation & Link level & Text/table/image & User/item nodes & User-item interactions  \\\\\n        Cora & Traditional graph prediction  & Node/edge level & Text & Research paper nodes & Citation  \\\\\n        PubMed & Traditional graph prediction  & Node/edge level & Text & Research paper nodes & Citation  \\\\\n        HIV & Traditional graph prediction & Graph level & Text & Atoms nodes & Atoms bonds \\\\\n        AgentClinic&  Multi-agent collaboration  & Graph level & Text/image & Agent/image/text nodes& Agent-agent/image/text \\\\\n        LongBench v2& Retrieval-augmented generation & Unintended action  & Text & Chunk nodes& Chunk-chunk similarity \\\\\n        ALFWorld& Planning and optimization & Graph level & Text/image & State  nodes &  State images similarity \\\\ \n        \\bottomrule\n    \\end{tabular}", "subtables": []}, {"original": "\\begin{table}[ht]\n\t\\centering\n    \\caption{\\textbf{Data statistics for multi-modal generation and matching.} In Multi-Modal-Paper, there are 58565 text-nodes, 7380 figure-nodes, and 6792 table-nodes.}\n\t\\begin{tabular}{lrrr}\n\t\t\\toprule\n\t\t\\textbf{Dataset} & \\textbf{\\#Node} & \\textbf{\\#Edges} \\\\\n\t\t\\midrule\n\t\t\\textbf{Goodreads} &  93,475 & 637,210\\\\\n\t\t\\textbf{Multi-Modal-Paper} & 72,737 &51,840\\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t\\label{tab:multimodal_data}%\n\\end{table}", "caption": "\\caption{\\textbf{Data statistics for multi-modal generation and matching.} In Multi-Modal-Paper, there are 58565 text-nodes, 7380 figure-nodes, and 6792 table-nodes.}", "label": "\\label{tab:multimodal_data}", "tabular": "\\begin{tabular}{lrrr}\n\t\t\\toprule\n\t\t\\textbf{Dataset} & \\textbf{\\#Node} & \\textbf{\\#Edges} \\\\\n\t\t\\midrule\n\t\t\\textbf{Goodreads} &  93,475 & 637,210\\\\\n\t\t\\textbf{Multi-Modal-Paper} & 72,737 &51,840\\\\\n\t\t\\bottomrule\n\t\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[ht]\n\t\\centering\n    \\caption{\\textbf{Data statistics for recommendation.} It includes three datasets of different scales, with the sizes ranging from small to large as follows: Baby, Sports, and Clothing.}\n\t\\begin{tabular}{lrrrr}\n\t\t\\toprule\n\t\t\\textbf{Dataset} & \\textbf{\\#User} & \\textbf{\\#Item} & \\textbf{\\#Edges} & \\textbf{Sparsity}\\\\\n\t\t\\midrule\n\t\t\\textbf{Baby} & 19,445 & 7,050 & 160,792  & 99.883\\% \\\\\n\t\t\\textbf{Sports} & 35,598 & 18,357 & 296,337  & 99.955\\% \\\\\n\t\t\\textbf{Clothing} & 39,387 & 23,033 & 278,677  & 99.969\\% \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t\\label{tab:rec_data}%\n\\end{table}", "caption": "\\caption{\\textbf{Data statistics for recommendation.} It includes three datasets of different scales, with the sizes ranging from small to large as follows: Baby, Sports, and Clothing.}", "label": "\\label{tab:rec_data}", "tabular": "\\begin{tabular}{lrrrr}\n\t\t\\toprule\n\t\t\\textbf{Dataset} & \\textbf{\\#User} & \\textbf{\\#Item} & \\textbf{\\#Edges} & \\textbf{Sparsity}\\\\\n\t\t\\midrule\n\t\t\\textbf{Baby} & 19,445 & 7,050 & 160,792  & 99.883\\% \\\\\n\t\t\\textbf{Sports} & 35,598 & 18,357 & 296,337  & 99.955\\% \\\\\n\t\t\\textbf{Clothing} & 39,387 & 23,033 & 278,677  & 99.969\\% \\\\\n\t\t\\bottomrule\n\t\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[ht]\n\\caption{Hyper-parameter configuration for model training.}\\label{apx:tab:hyper}\n\\centering\n\\scalebox{0.63}{\n\\begin{tabular}{ccccc}\n\\toprule\nParameter & GWM-T LLM & GWM-T SD & GWM-E LLM & GWM-E SD \\\\\n\\midrule\nOptimizer & AdamW & AdamW & AdamW & AdamW \\\\\n% \\midrule\n% \\midrule\nAdam $\\epsilon$ & 1e-8  & 1e-8 & 1e-8  & 1e-8 \\\\\nAdam $(\\beta_1, \\beta_2)$ & (0.9, 0.999) & (0.9, 0.999) & (0.9, 0.999)  & (0.9, 0.999)  \\\\\nWeight decay & 1e-2 & 1e-2 & 1e-2  & 1e-2 \\\\\n\nBatch size per GPU & 4 & 1 & 10 & 16\\\\\nGradient Accumulation & 8 & 4 & 1 & 4\\\\\n\nEpochs & 4 & 5 & 1  & 30 \\\\\nResolution & - & 512 & - & 256 \\\\\n% \\midrule\nLearning rate & 3e-4 & 1e-5 &  3e-4  & 1e-5 \\\\\nBackbone SD &  Llama-3-8B & SD-v1-5 &  Llama-3-8B & SD-v1-5   \\\\\n\\bottomrule            \n\\end{tabular}\n}\n\\end{table}", "caption": "\\caption{Hyper-parameter configuration for model training.}", "label": "\\label{apx:tab:hyper}", "tabular": "\\begin{tabular}{ccccc}\n\\toprule\nParameter & GWM-T LLM & GWM-T SD & GWM-E LLM & GWM-E SD \\\\\n\\midrule\nOptimizer & AdamW & AdamW & AdamW & AdamW \\\\\n% \\midrule\n% \\midrule\nAdam $\\epsilon$ & 1e-8  & 1e-8 & 1e-8  & 1e-8 \\\\\nAdam $(\\beta_1, \\beta_2)$ & (0.9, 0.999) & (0.9, 0.999) & (0.9, 0.999)  & (0.9, 0.999)  \\\\\nWeight decay & 1e-2 & 1e-2 & 1e-2  & 1e-2 \\\\\n\nBatch size per GPU & 4 & 1 & 10 & 16\\\\\nGradient Accumulation & 8 & 4 & 1 & 4\\\\\n\nEpochs & 4 & 5 & 1  & 30 \\\\\nResolution & - & 512 & - & 256 \\\\\n% \\midrule\nLearning rate & 3e-4 & 1e-5 &  3e-4  & 1e-5 \\\\\nBackbone SD &  Llama-3-8B & SD-v1-5 &  Llama-3-8B & SD-v1-5   \\\\\n\\bottomrule            \n\\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Action prompt of multi-modal generation}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    This is a multi-modal generation task. Please predict the missing modality based on the given modality: \\{modality\\}.\\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:multi-modal generation}\n\\end{tabular}\n\\end{table*}", "caption": "\\caption{\\textbf{Action prompt of multi-modal generation}.}", "label": null, "tabular": "\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    This is a multi-modal generation task. Please predict the missing modality based on the given modality: \\{modality\\}.\\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:multi-modal generation}\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Action prompt of multi-modal matching}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    This task involves matching multi-modal information. Given two modalities: \\{modality 1\\} and \\{modality 2\\}, please determine whether they correspond with each other. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:multi-modal matching}\n\\end{tabular}\n\\end{table*}", "caption": "\\caption{\\textbf{Action prompt of multi-modal matching}.}", "label": null, "tabular": "\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    This task involves matching multi-modal information. Given two modalities: \\{modality 1\\} and \\{modality 2\\}, please determine whether they correspond with each other. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:multi-modal matching}\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Action prompt of recommendation}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    This is a recommendation task. Given the user node and item node: \\{user node\\} and \\{item node\\}, please tell me whether these two nodes should connect to each other.\\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:recommendation}\n\\end{tabular}\n\\end{table*}", "caption": "\\caption{\\textbf{Action prompt of recommendation}.}", "label": null, "tabular": "\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    This is a recommendation task. Given the user node and item node: \\{user node\\} and \\{item node\\}, please tell me whether these two nodes should connect to each other.\\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:recommendation}\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Action prompt of node classification of Cora}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    Given a node-centered graph: \\{node\\}, each node represents a paper, we need to classify the center node into 7 classes: Case Based, Genetic Algorithms, Neural Networks, Probabilistic Methods, Reinforcement Learning, Rule Learning, Theory, please tell me which class the center node belongs to?\\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:Cora_node}\n\\end{tabular}\n\\end{table*}", "caption": "\\caption{\\textbf{Action prompt of node classification of Cora}.}", "label": null, "tabular": "\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    Given a node-centered graph: \\{node\\}, each node represents a paper, we need to classify the center node into 7 classes: Case Based, Genetic Algorithms, Neural Networks, Probabilistic Methods, Reinforcement Learning, Rule Learning, Theory, please tell me which class the center node belongs to?\\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:Cora_node}\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Action prompt of node classification of PubMed}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    Given a node-centered graph: \\{node\\}, each node represents a paper about Diabetes, we need to classify the center node into 3 classes: Diabetes Mellitus Experimental, Diabetes Mellitus Type 1, and Diabetes Mellitus Type 2, please tell me which class the center node belongs to?\\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:PubMed_node}\n\\end{tabular}\n\\end{table*}", "caption": "\\caption{\\textbf{Action prompt of node classification of PubMed}.}", "label": null, "tabular": "\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    Given a node-centered graph: \\{node\\}, each node represents a paper about Diabetes, we need to classify the center node into 3 classes: Diabetes Mellitus Experimental, Diabetes Mellitus Type 1, and Diabetes Mellitus Type 2, please tell me which class the center node belongs to?\\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:PubMed_node}\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Action prompt of link prediction of  Cora and PubMed}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    Given two nodes information: \\{node 1\\} and \\{node 2\\}, please tell me whether two center nodes in the subgraphs should connect to each other. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:link prediction}\n\\end{tabular}\n\\end{table*}", "caption": "\\caption{\\textbf{Action prompt of link prediction of  Cora and PubMed}.}", "label": null, "tabular": "\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    Given two nodes information: \\{node 1\\} and \\{node 2\\}, please tell me whether two center nodes in the subgraphs should connect to each other. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:link prediction}\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Action prompt of graph classification of  HIV}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    Human immunodeficiency viruses (HIV) are a type of retrovirus, which induces acquired immune deficiency syndrome (AIDs). Please determine whether this molecule \\{molecule\\} is effective for this assay. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:HIV}\n\\end{tabular}\n\\end{table*}", "caption": "\\caption{\\textbf{Action prompt of graph classification of  HIV}.}", "label": null, "tabular": "\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    Human immunodeficiency viruses (HIV) are a type of retrovirus, which induces acquired immune deficiency syndrome (AIDs). Please determine whether this molecule \\{molecule\\} is effective for this assay. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:HIV}\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Action prompt of multi-agent collaboration}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    This is a Multi-Agent Collaborative Generation task for creating dynamic conversational interactions. Given a user query: \\{user query\\} and context of three distinct agents: \\{Patient Agent Context\\}, \\{Measurement Agent Context\\}, and \\{Moderato Agent Context\\}, Please generate a well-rounded response to the user's question.\\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:multi-agent collaboration}\n\\end{tabular}\n\\end{table*}", "caption": "\\caption{\\textbf{Action prompt of multi-agent collaboration}.}", "label": null, "tabular": "\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    This is a Multi-Agent Collaborative Generation task for creating dynamic conversational interactions. Given a user query: \\{user query\\} and context of three distinct agents: \\{Patient Agent Context\\}, \\{Measurement Agent Context\\}, and \\{Moderato Agent Context\\}, Please generate a well-rounded response to the user's question.\\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:multi-agent collaboration}\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Action prompt of retrieval-augmented generation}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    This is a Retrieval-Augmented Generation task for improving response quality in dialogue systems. Given a user query: \\{user query\\} and a set of retrieved documents: \\{retrieved documents\\}, the goal is to generate a coherent and contextually relevant response. Please generate a response that integrates information from the retrieved documents to accurately address the user's query. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:retrieval-augmented generation}\n\\end{tabular}\n\\end{table*}", "caption": "\\caption{\\textbf{Action prompt of retrieval-augmented generation}.}", "label": null, "tabular": "\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    This is a Retrieval-Augmented Generation task for improving response quality in dialogue systems. Given a user query: \\{user query\\} and a set of retrieved documents: \\{retrieved documents\\}, the goal is to generate a coherent and contextually relevant response. Please generate a response that integrates information from the retrieved documents to accurately address the user's query. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:retrieval-augmented generation}\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Action prompt of planning and optimization}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    This is an embodied household task, please predict the next decision-making behavior based on multimodal historical information: \\{historical information\\}. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:planning and optimization}\n\\end{tabular}\n\\end{table*}", "caption": "\\caption{\\textbf{Action prompt of planning and optimization}.}", "label": null, "tabular": "\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    This is an embodied household task, please predict the next decision-making behavior based on multimodal historical information: \\{historical information\\}. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:planning and optimization}\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Prompt $P_{u}$ of multi-modality as token in GWM-T}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    The image's text description is: \\{image's text description\\}, original text is: \\{original text\\}, table description is: \\{table description\\}. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:multi-modality as token}\n\\end{tabular}\n\\end{table*}", "caption": "\\caption{\\textbf{Prompt $P_{u}$ of multi-modality as token in GWM-T}.}", "label": null, "tabular": "\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    The image's text description is: \\{image's text description\\}, original text is: \\{original text\\}, table description is: \\{table description\\}. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:multi-modality as token}\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Prompt $f_{v}(\\cdot)$ of aggregating central node and neighbor nodes in GWM-T}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    The text description of the central node is: \\{center node\\}, and the text descriptions of the neighboring nodes are: \\{neighbor nodes\\}. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:aggregating}\n\\end{tabular}\n\\end{table*}", "caption": "\\caption{\\textbf{Prompt $f_{v}(\\cdot)$ of aggregating central node and neighbor nodes in GWM-T}.}", "label": null, "tabular": "\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    The text description of the central node is: \\{center node\\}, and the text descriptions of the neighboring nodes are: \\{neighbor nodes\\}. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:aggregating}\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n    \\centering\n    \\caption{\\textbf{Task description and output comparison of multi-modal generation.} This task is to predict the missing modality based on the given\nmodality. Here we utilize one case of Goodreads dataset as examples. We show the output results of GWM-T, the best performing GWM, and ControlNet , the strongest baseline.}\n    \\label{tab:multimodal_generation}\n    \n    \\renewcommand{\\arraystretch}{1.2} % Slightly increase row height\n    \n    \\begin{tabular}{|m{3cm}|P{3cm}|P{3cm}|P{3cm}|}\n        \\toprule\n        \\multicolumn{4}{|c|}{\\textbf{Task Description}} \\\\\n        \\midrule\n        \\centering \\textbf{Task Name} & \\multicolumn{3}{c|}{Multi-modal generation} \\\\\n        \\midrule\n         \\centering \\textbf{Given modality} & \\textbf{Action prompt} & \\multicolumn{2}{c|}{\\textbf{Ground truth}} \\\\\n        \\midrule\n         \\parbox{3cm}{\\centering Title: The Shark-Infested Custard}& \n         \\parbox{3cm}{\\centering\\arraybackslash This is a multi-modal generation task. Please predict the missing modality based on the given modality: \\{modality\\}.} & \n        \\multicolumn{2}{m{4cm}|}{\\includegraphics[width=2.8cm]{figs/Goodreads_gt_shark.jpg}} \\\\\n        \\midrule\n        \\centering \\textbf{Method} & \\multicolumn{3}{c|}{\\textbf{Output Results}} \\\\\n        \\midrule\n        \\centering\\arraybackslash\n        ControlNet  & \\multicolumn{3}{m{8.5cm}|}{\\centering\\arraybackslash\\includegraphics[width=6cm]{figs/Goodreads_controlnet_shark.png}} \\\\\n        \\midrule\n        \\centering\\arraybackslash\n        GWM-T & \\multicolumn{3}{m{8.5cm}|}{\\centering\\arraybackslash\\includegraphics[width=6cm]{figs/Goodreads_GWMT_shark.jpg}} \\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table*}", "caption": "\\caption{\\textbf{Task description and output comparison of multi-modal generation.} This task is to predict the missing modality based on the given\nmodality. Here we utilize one case of Goodreads dataset as examples. We show the output results of GWM-T, the best performing GWM, and ControlNet , the strongest baseline.}", "label": "\\label{tab:multimodal_generation}", "tabular": "\\begin{tabular}{|m{3cm}|P{3cm}|P{3cm}|P{3cm}|}\n        \\toprule\n        \\multicolumn{4}{|c|}{\\textbf{Task Description}} \\\\\n        \\midrule\n        \\centering \\textbf{Task Name} & \\multicolumn{3}{c|}{Multi-modal generation} \\\\\n        \\midrule\n         \\centering \\textbf{Given modality} & \\textbf{Action prompt} & \\multicolumn{2}{c|}{\\textbf{Ground truth}} \\\\\n        \\midrule\n         \\parbox{3cm}{\\centering Title: The Shark-Infested Custard}& \n         \\parbox{3cm}{\\centering\\arraybackslash This is a multi-modal generation task. Please predict the missing modality based on the given modality: \\{modality\\}.} & \n        \\multicolumn{2}{m{4cm}|}{\\includegraphics[width=2.8cm]{figs/Goodreads_gt_shark.jpg}} \\\\\n        \\midrule\n        \\centering \\textbf{Method} & \\multicolumn{3}{c|}{\\textbf{Output Results}} \\\\\n        \\midrule\n        \\centering\\arraybackslash\n        ControlNet  & \\multicolumn{3}{m{8.5cm}|}{\\centering\\arraybackslash\\includegraphics[width=6cm]{figs/Goodreads_controlnet_shark.png}} \\\\\n        \\midrule\n        \\centering\\arraybackslash\n        GWM-T & \\multicolumn{3}{m{8.5cm}|}{\\centering\\arraybackslash\\includegraphics[width=6cm]{figs/Goodreads_GWMT_shark.jpg}} \\\\\n        \\bottomrule\n    \\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n    \\centering\n    \\caption{\\textbf{Task description and output comparison of multi-modal matching.} This task is to predict whether two modalities correspond with each other. Here we utilize one case of Multi-Modal-Paper dataset as examples. We show the output results of GWM-E, the best performing GWM, and Contrastive MLP, the strongest baseline. Note that modality can be an image, a table, or a text.}\n    \\label{tab:multimodal_matching}\n    \n    \\renewcommand{\\arraystretch}{1.2} % Slightly increase row height\n    \n    % Define a new column type for centered text in m columns\n    \\newcolumntype{C}[1]{>{\\centering\\arraybackslash}m{#1}}\n    \n    \\begin{tabular}{|C{5cm}|C{3cm}|C{5cm}|C{3cm}|C{3cm}|}\n        \\toprule\n        \\multicolumn{5}{|c|}{\\textbf{Task Description}} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{\\textbf{Task Name}} & \\multicolumn{3}{c|}{Multi-modal matching} \\\\\n        \\midrule\n        \\textbf{Modality 1: figure} & \\textbf{Modality 2: text} & \\textbf{Action prompt} & \\multicolumn{2}{c|}{\\textbf{Ground truth}} \\\\\n        \\midrule\n        \\includegraphics[width=5cm]{figs/figures_749.png} & \n        Illustration of different formats of STL expressions. (a) Different expression formats of the same STL. (b) The binary tree representation of STL. & \n        This task involves matching multi-modal information. Given two modalities: \\{modality 1\\} and \\{modality 2\\}, please determine whether they correspond with each other. & \n        \\multicolumn{2}{c|}{yes} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{\\textbf{Method}} & \\multicolumn{3}{c|}{\\textbf{Output Results}} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{Contrastive MLP} & \\multicolumn{3}{c|}{no} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{GWM-E} & \\multicolumn{3}{c|}{yes} \\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table*}", "caption": "\\caption{\\textbf{Task description and output comparison of multi-modal matching.} This task is to predict whether two modalities correspond with each other. Here we utilize one case of Multi-Modal-Paper dataset as examples. We show the output results of GWM-E, the best performing GWM, and Contrastive MLP, the strongest baseline. Note that modality can be an image, a table, or a text.}", "label": "\\label{tab:multimodal_matching}", "tabular": "\\begin{tabular}{|C{5cm}|C{3cm}|C{5cm}|C{3cm}|C{3cm}|}\n        \\toprule\n        \\multicolumn{5}{|c|}{\\textbf{Task Description}} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{\\textbf{Task Name}} & \\multicolumn{3}{c|}{Multi-modal matching} \\\\\n        \\midrule\n        \\textbf{Modality 1: figure} & \\textbf{Modality 2: text} & \\textbf{Action prompt} & \\multicolumn{2}{c|}{\\textbf{Ground truth}} \\\\\n        \\midrule\n        \\includegraphics[width=5cm]{figs/figures_749.png} & \n        Illustration of different formats of STL expressions. (a) Different expression formats of the same STL. (b) The binary tree representation of STL. & \n        This task involves matching multi-modal information. Given two modalities: \\{modality 1\\} and \\{modality 2\\}, please determine whether they correspond with each other. & \n        \\multicolumn{2}{c|}{yes} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{\\textbf{Method}} & \\multicolumn{3}{c|}{\\textbf{Output Results}} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{Contrastive MLP} & \\multicolumn{3}{c|}{no} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{GWM-E} & \\multicolumn{3}{c|}{yes} \\\\\n        \\bottomrule\n    \\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n    \\centering\n    \\caption{\\textbf{Task description and output comparison of recommendation.} This task is to predict whether the user node and the item node are connected. Here we utilize one case of Baby dataset as examples. We show the output results of GWM-E, the best performing GWM, and LightGCN, the strongest baseline.}\n    \\label{tab:recommendation}\n    \n    \\renewcommand{\\arraystretch}{1.2} % Slightly increase row height\n    \n    % Define a new column type for centered text in m columns\n    \\newcolumntype{C}[1]{>{\\centering\\arraybackslash}m{#1}}\n    \n    \\begin{tabular}{|C{4cm}|C{4cm}|C{4cm}|C{3cm}|C{3cm}|}\n        \\toprule\n        \\multicolumn{5}{|c|}{\\textbf{Task Description}} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{\\textbf{Task Name}} & \\multicolumn{3}{c|}{Recommendation} \\\\\n        \\midrule\n        \\textbf{User node} & \\textbf{Item node} & \\textbf{Action prompt} & \\multicolumn{2}{c|}{\\textbf{Ground truth}} \\\\\n        \\midrule\n        User online product reviews series: I struggled to find full slips, especially larger ones. The first one was too small; the second fit well and was affordable. The beads looked stunning, perfect for beadwork. The earrings broke immediately due to poor quality. I love the 3 flower sister Hawaii glass beads on my Pandora bracelet. They're pretty and large, and my eight-year-old finds them comfy enough to sleep in ...  \n          & \\includegraphics[width=4cm]{figs/B004R1REP2.jpg} & \n        This is a recommendation task. Given the user node and item node: \\{user node\\} and \\{item node\\}, please tell me whether these two nodes should connect to each other. & \n        \\multicolumn{2}{c|}{no} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{\\textbf{Method}} & \\multicolumn{3}{c|}{\\textbf{Output Results}} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{LightGCN} & \\multicolumn{3}{c|}{yes} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{GWM-E} & \\multicolumn{3}{c|}{no} \\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table*}", "caption": "\\caption{\\textbf{Task description and output comparison of recommendation.} This task is to predict whether the user node and the item node are connected. Here we utilize one case of Baby dataset as examples. We show the output results of GWM-E, the best performing GWM, and LightGCN, the strongest baseline.}", "label": "\\label{tab:recommendation}", "tabular": "\\begin{tabular}{|C{4cm}|C{4cm}|C{4cm}|C{3cm}|C{3cm}|}\n        \\toprule\n        \\multicolumn{5}{|c|}{\\textbf{Task Description}} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{\\textbf{Task Name}} & \\multicolumn{3}{c|}{Recommendation} \\\\\n        \\midrule\n        \\textbf{User node} & \\textbf{Item node} & \\textbf{Action prompt} & \\multicolumn{2}{c|}{\\textbf{Ground truth}} \\\\\n        \\midrule\n        User online product reviews series: I struggled to find full slips, especially larger ones. The first one was too small; the second fit well and was affordable. The beads looked stunning, perfect for beadwork. The earrings broke immediately due to poor quality. I love the 3 flower sister Hawaii glass beads on my Pandora bracelet. They're pretty and large, and my eight-year-old finds them comfy enough to sleep in ...  \n          & \\includegraphics[width=4cm]{figs/B004R1REP2.jpg} & \n        This is a recommendation task. Given the user node and item node: \\{user node\\} and \\{item node\\}, please tell me whether these two nodes should connect to each other. & \n        \\multicolumn{2}{c|}{no} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{\\textbf{Method}} & \\multicolumn{3}{c|}{\\textbf{Output Results}} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{LightGCN} & \\multicolumn{3}{c|}{yes} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{GWM-E} & \\multicolumn{3}{c|}{no} \\\\\n        \\bottomrule\n    \\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n    \\centering\n    \\caption{\\textbf{Task description and output comparison of traditional graph prediction.} This task aims to perform predictions at three different levels: node, link, and graph. Here we utilize one case of Cora's node prediction. We show the output results of GWM-E, the best performing GWM, and LLAGA, the strongest baseline.}\n    \\label{tab:graph_prediction}\n    \n    \\renewcommand{\\arraystretch}{1.2} % Slightly increase row height\n    \n    % Define a new column type for centered text in m columns\n    \\newcolumntype{C}[1]{>{\\centering\\arraybackslash}m{#1}}\n    \n    \\begin{tabular}{|C{7cm}|C{5cm}|C{3cm}|}\n        \\hline\n        \\multicolumn{3}{|c|}{\\textbf{Task Description}} \\\\\n        \\hline\n        \\textbf{Task Name} & \\multicolumn{2}{c|}{Traditional graph prediction} \\\\\n        \\hline\n        \\textbf{Node} & \\textbf{Action prompt} & \\textbf{Ground truth} \\\\\n        \\hline\n        Learning under persistent drift: In this paper we study learning algorithms for environments which are changing over time. Unlike most previous work, we are interested in the case where the changes might be rapid but their \"direction\" is relatively constant. We model this type of change by assuming that the target distribution is changing continuously at a constant rate from one extreme distribution to another. We show in this case how to use a simple weighting scheme to estimate the error of an hypothesis, and using this estimate, to minimize the error of the prediction. & \n        Given a node-centered graph: \\{node\\}, each node represents a paper, we need to classify the center node into 7 classes: Case Based, Genetic Algorithms, Neural Networks, Probabilistic Methods, Reinforcement Learning, Rule Learning, Theory, please tell me which class the center node belongs to? & \n        Theory \\\\\n        \\hline\n        \\textbf{Method} & \\multicolumn{2}{c|}{\\textbf{Output Results}} \\\\\n        \\hline\n        LLAGA & \\multicolumn{2}{c|}{Neural Networks} \\\\\n        \\hline\n        GWM-E & \\multicolumn{2}{c|}{Theory} \\\\\n        \\hline\n    \\end{tabular}\n\\end{table*}", "caption": "\\caption{\\textbf{Task description and output comparison of traditional graph prediction.} This task aims to perform predictions at three different levels: node, link, and graph. Here we utilize one case of Cora's node prediction. We show the output results of GWM-E, the best performing GWM, and LLAGA, the strongest baseline.}", "label": "\\label{tab:graph_prediction}", "tabular": "\\begin{tabular}{|C{7cm}|C{5cm}|C{3cm}|}\n        \\hline\n        \\multicolumn{3}{|c|}{\\textbf{Task Description}} \\\\\n        \\hline\n        \\textbf{Task Name} & \\multicolumn{2}{c|}{Traditional graph prediction} \\\\\n        \\hline\n        \\textbf{Node} & \\textbf{Action prompt} & \\textbf{Ground truth} \\\\\n        \\hline\n        Learning under persistent drift: In this paper we study learning algorithms for environments which are changing over time. Unlike most previous work, we are interested in the case where the changes might be rapid but their \"direction\" is relatively constant. We model this type of change by assuming that the target distribution is changing continuously at a constant rate from one extreme distribution to another. We show in this case how to use a simple weighting scheme to estimate the error of an hypothesis, and using this estimate, to minimize the error of the prediction. & \n        Given a node-centered graph: \\{node\\}, each node represents a paper, we need to classify the center node into 7 classes: Case Based, Genetic Algorithms, Neural Networks, Probabilistic Methods, Reinforcement Learning, Rule Learning, Theory, please tell me which class the center node belongs to? & \n        Theory \\\\\n        \\hline\n        \\textbf{Method} & \\multicolumn{2}{c|}{\\textbf{Output Results}} \\\\\n        \\hline\n        LLAGA & \\multicolumn{2}{c|}{Neural Networks} \\\\\n        \\hline\n        GWM-E & \\multicolumn{2}{c|}{Theory} \\\\\n        \\hline\n    \\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n    \\centering\n    \\caption{\\textbf{Multi-agent collaboration task and output comparison.} Disease-related query answering through agent interaction and external knowledge. Results show GWM-T vs COT baseline. \\textbf{Note: Medical images may cause discomfort but are from real datasets.}}\n    \\label{tab:multiagent_collaboration}\n    \n    \\renewcommand{\\arraystretch}{1.0}\n    \\small % \u4f7f\u7528\u5c0f\u5b57\u4f53\n    \n    \\begin{tabular}{|>{\\centering\\arraybackslash}m{4cm}|>{\\centering\\arraybackslash}m{5cm}|>{\\centering\\arraybackslash}m{5.5cm}|}\n        \\hline\n        \\multicolumn{3}{|c|}{\\textbf{Task Description}} \\\\\n        \\hline\n        \\textbf{Task} & \\multicolumn{2}{c|}{Multi-agent collaboration} \\\\\n        \\hline\n        \\textbf{User Query} & \\textbf{Patient Agent} & \\textbf{Moderator Agent} \\\\\n        \\hline\n        \n        53-year-old man, 3-year history: itchy rash, Raynaud's, dysphagia, burning hands. Exam: firm papules on forehead with glabellar grooves, waxy papules on hands with thickening and contractures. Similar changes on nose, lips, ears, trunk, feet. No telangiectasia/calcinosis. Sensory neuropathy in hands/arms/face. Normal thyroid. IgG-monoclonal gammopathy, normal bone marrow.\n        \n        Choices: (A) AL amyloidosis (B) Multiple myeloma (C) Scleredema (D) Scleromyxedema (E) Systemic sclerosis\n        & \n        Role: 53-year-old patient with 3-year symptoms including itchy rash, firm forehead papules causing brow grooves, waxy hand papules with thickening and finger contractures. Experience Raynaud's phenomenon, dysphagia, burning hands, and numbness in hands/arms/face. Aware of normal thyroid tests and abnormal blood protein but unaware of diagnosis implications.\n        & \n        Moderator organizing case information:\n        \n        Test results: Normal thyroid function, IgG-monoclonal gammopathy detected, normal bone marrow biopsy.\n        \n        Key findings: Extracellular yellow-brown deposits in dermis on skin biopsy.\n        \\\\\n        \\hline\n        \\textbf{Measurement Agent} & \\textbf{Action Prompt} & \\textbf{Ground Truth} \\\\\n        \\hline\n        \\centering\n        \\includegraphics[width=3.5cm]{figs/6.png} \n        & \n        Multi-Agent Collaborative Generation task for dynamic conversational interactions. Generate response using Patient, Measurement, and Moderator agent contexts for the given user query.\n        & \n        D\n        \\\\\n        \\hline\n        \\textbf{Method} & \\multicolumn{2}{c|}{\\textbf{Output Results}} \\\\\n        \\hline\n        COT & \\multicolumn{2}{m{10.5cm}|}{\\centering Analysis of patient symptoms and test results suggests monoclonal gammopathy with characteristic skin findings. DIAGNOSIS: Multiple myeloma (B)} \\\\\n        \\hline\n        GWM-T & \\multicolumn{2}{m{10.5cm}|}{\\centering D} \\\\\n        \\hline\n    \\end{tabular}\n\\end{table*}", "caption": "\\caption{\\textbf{Multi-agent collaboration task and output comparison.} Disease-related query answering through agent interaction and external knowledge. Results show GWM-T vs COT baseline. \\textbf{Note: Medical images may cause discomfort but are from real datasets.}}", "label": "\\label{tab:multiagent_collaboration}", "tabular": "\\begin{tabular}{|>{\\centering\\arraybackslash}m{4cm}|>{\\centering\\arraybackslash}m{5cm}|>{\\centering\\arraybackslash}m{5.5cm}|}\n        \\hline\n        \\multicolumn{3}{|c|}{\\textbf{Task Description}} \\\\\n        \\hline\n        \\textbf{Task} & \\multicolumn{2}{c|}{Multi-agent collaboration} \\\\\n        \\hline\n        \\textbf{User Query} & \\textbf{Patient Agent} & \\textbf{Moderator Agent} \\\\\n        \\hline\n        \n        53-year-old man, 3-year history: itchy rash, Raynaud's, dysphagia, burning hands. Exam: firm papules on forehead with glabellar grooves, waxy papules on hands with thickening and contractures. Similar changes on nose, lips, ears, trunk, feet. No telangiectasia/calcinosis. Sensory neuropathy in hands/arms/face. Normal thyroid. IgG-monoclonal gammopathy, normal bone marrow.\n        \n        Choices: (A) AL amyloidosis (B) Multiple myeloma (C) Scleredema (D) Scleromyxedema (E) Systemic sclerosis\n        & \n        Role: 53-year-old patient with 3-year symptoms including itchy rash, firm forehead papules causing brow grooves, waxy hand papules with thickening and finger contractures. Experience Raynaud's phenomenon, dysphagia, burning hands, and numbness in hands/arms/face. Aware of normal thyroid tests and abnormal blood protein but unaware of diagnosis implications.\n        & \n        Moderator organizing case information:\n        \n        Test results: Normal thyroid function, IgG-monoclonal gammopathy detected, normal bone marrow biopsy.\n        \n        Key findings: Extracellular yellow-brown deposits in dermis on skin biopsy.\n        \\\\\n        \\hline\n        \\textbf{Measurement Agent} & \\textbf{Action Prompt} & \\textbf{Ground Truth} \\\\\n        \\hline\n        \\centering\n        \\includegraphics[width=3.5cm]{figs/6.png} \n        & \n        Multi-Agent Collaborative Generation task for dynamic conversational interactions. Generate response using Patient, Measurement, and Moderator agent contexts for the given user query.\n        & \n        D\n        \\\\\n        \\hline\n        \\textbf{Method} & \\multicolumn{2}{c|}{\\textbf{Output Results}} \\\\\n        \\hline\n        COT & \\multicolumn{2}{m{10.5cm}|}{\\centering Analysis of patient symptoms and test results suggests monoclonal gammopathy with characteristic skin findings. DIAGNOSIS: Multiple myeloma (B)} \\\\\n        \\hline\n        GWM-T & \\multicolumn{2}{m{10.5cm}|}{\\centering D} \\\\\n        \\hline\n    \\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n    \\centering\n    \\caption{\\textbf{Task description and output comparison of retrieval-augmented generation.} This task is to generate a response that integrates information from the retrieved documents to accurately address the user's query. Here we utilize one case of LongBench v2 dataset as examples. We show the output results of GWM-E, the best performing GWM, and GPT-4o mini, the strongest baseline.}\n    \\label{tab:rag}\n    \n    \\renewcommand{\\arraystretch}{1.2} % Slightly increase row height\n    \n    % Define a new column type for centered text in m columns\n    \\newcolumntype{C}[1]{>{\\centering\\arraybackslash}m{#1}}\n    \n    \\begin{tabular}{|C{4.5cm}|C{4.5cm}|C{4cm}|C{2cm}|C{1cm}|}\n        \\toprule\n        \\multicolumn{5}{|c|}{\\textbf{Task Description}} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{\\textbf{Task Name}} & \\multicolumn{3}{c|}{Retrieval-augmented generation} \\\\\n        \\midrule\n        \\textbf{User query} & \\textbf{Document} & \\textbf{Action prompt} & \\multicolumn{2}{c|}{\\textbf{Ground truth}} \\\\\n        \\midrule\n        What is the correct answer to this question: You are given a grammar book of Kalamang language, now translate the following Kalamang sentence into English: Faisal emun me mindi don bolonet me ma he kademor.\nChoices:\n(A) Faisal's mother is still angry at him for a little thing like that.\n(B) Faisal's mother turns furious at him for a big thing like that.\n(C) Faisal's mother gets frustrated at him for a big thing like this.\n(D) Faisal's mother gets angry at him for a little thing like that.\n\nFormat your response as follows: \"The correct answer is (insert answer here)\". & \n        There are very few households with two fluent Kalamang-speaking parents and children born after 1990, but even in those households the children are not raised in Kalamang. As indicated above, non-fluent speakers have a good passive command of Kalamang ... Fluent Kalamang speakers do not necessarily shift to Papuan Malay when they join the conversation, but they are not expected to actively contribute, although they can express themselves in a simple way in Kalamang ... & \n        This is a Retrieval-Augmented Generation task for improving response quality in dialogue systems. Given a user query: \\{user query\\} and a set of retrieved documents: \\{retrieved documents\\}, the goal is to generate a coherent and contextually relevant response. Please generate a response that integrates information from the retrieved documents to accurately address the user's query. & \n        \\multicolumn{2}{c|}{D} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{\\textbf{Method}} & \\multicolumn{3}{c|}{\\textbf{Output Results}} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{GPT-4o mini} & \\multicolumn{3}{c|}{A} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{GWM-E} & \\multicolumn{3}{c|}{D} \\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table*}", "caption": "\\caption{\\textbf{Task description and output comparison of retrieval-augmented generation.} This task is to generate a response that integrates information from the retrieved documents to accurately address the user's query. Here we utilize one case of LongBench v2 dataset as examples. We show the output results of GWM-E, the best performing GWM, and GPT-4o mini, the strongest baseline.}", "label": "\\label{tab:rag}", "tabular": "\\begin{tabular}{|C{4.5cm}|C{4.5cm}|C{4cm}|C{2cm}|C{1cm}|}\n        \\toprule\n        \\multicolumn{5}{|c|}{\\textbf{Task Description}} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{\\textbf{Task Name}} & \\multicolumn{3}{c|}{Retrieval-augmented generation} \\\\\n        \\midrule\n        \\textbf{User query} & \\textbf{Document} & \\textbf{Action prompt} & \\multicolumn{2}{c|}{\\textbf{Ground truth}} \\\\\n        \\midrule\n        What is the correct answer to this question: You are given a grammar book of Kalamang language, now translate the following Kalamang sentence into English: Faisal emun me mindi don bolonet me ma he kademor.\nChoices:\n(A) Faisal's mother is still angry at him for a little thing like that.\n(B) Faisal's mother turns furious at him for a big thing like that.\n(C) Faisal's mother gets frustrated at him for a big thing like this.\n(D) Faisal's mother gets angry at him for a little thing like that.\n\nFormat your response as follows: \"The correct answer is (insert answer here)\". & \n        There are very few households with two fluent Kalamang-speaking parents and children born after 1990, but even in those households the children are not raised in Kalamang. As indicated above, non-fluent speakers have a good passive command of Kalamang ... Fluent Kalamang speakers do not necessarily shift to Papuan Malay when they join the conversation, but they are not expected to actively contribute, although they can express themselves in a simple way in Kalamang ... & \n        This is a Retrieval-Augmented Generation task for improving response quality in dialogue systems. Given a user query: \\{user query\\} and a set of retrieved documents: \\{retrieved documents\\}, the goal is to generate a coherent and contextually relevant response. Please generate a response that integrates information from the retrieved documents to accurately address the user's query. & \n        \\multicolumn{2}{c|}{D} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{\\textbf{Method}} & \\multicolumn{3}{c|}{\\textbf{Output Results}} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{GPT-4o mini} & \\multicolumn{3}{c|}{A} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{GWM-E} & \\multicolumn{3}{c|}{D} \\\\\n        \\bottomrule\n    \\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n    \\centering\n    \\caption{\\textbf{Planning and optimization task and output comparison.} Agent decision-making prediction using ALFWorld dataset. Results show GWM-E vs T5 FT baseline.}\n    \\label{tab:planning_optimization}\n    \\renewcommand{\\arraystretch}{1.0}\n    \\small % \u4f7f\u7528\u5c0f\u5b57\u4f53\n    \\begin{tabular}{|>{\\centering\\arraybackslash}m{3cm}|>{\\centering\\arraybackslash}m{2.5cm}|>{\\centering\\arraybackslash}m{4cm}|>{\\centering\\arraybackslash}m{2cm}|}\n        \\hline\n        \\multicolumn{4}{|c|}{\\textbf{Task Description}} \\\\\n        \\hline\n        \\multicolumn{2}{|c|}{\\textbf{Task}} & \\multicolumn{2}{c|}{Planning and optimization} \\\\\n        \\hline\n        \\textbf{Image} & \\textbf{Text} & \\textbf{Action Prompt} & \\textbf{Ground Truth} \\\\\n        \\hline\n        \\includegraphics[width=2.8cm]{figs/000000000.jpg} & \n        Task: put a potato in countertop & \n        Embodied household task: predict next decision-making behavior based on multimodal information. & \n        go to garbagecan 1 \\\\\n        \\hline\n        \\multicolumn{2}{|c|}{\\textbf{Method}} & \\multicolumn{2}{c|}{\\textbf{Output Results}} \\\\\n        \\hline\n        \\multicolumn{2}{|c|}{T5 FT} & \\multicolumn{2}{m{6cm}|}{\\centering go to microwave 1} \\\\\n        \\hline\n        \\multicolumn{2}{|c|}{GWM-E} & \\multicolumn{2}{m{6cm}|}{\\centering go to garbagecan 1} \\\\\n        \\hline\n    \\end{tabular}\n\\end{table*}", "caption": "\\caption{\\textbf{Planning and optimization task and output comparison.} Agent decision-making prediction using ALFWorld dataset. Results show GWM-E vs T5 FT baseline.}", "label": "\\label{tab:planning_optimization}", "tabular": "\\begin{tabular}{|>{\\centering\\arraybackslash}m{3cm}|>{\\centering\\arraybackslash}m{2.5cm}|>{\\centering\\arraybackslash}m{4cm}|>{\\centering\\arraybackslash}m{2cm}|}\n        \\hline\n        \\multicolumn{4}{|c|}{\\textbf{Task Description}} \\\\\n        \\hline\n        \\multicolumn{2}{|c|}{\\textbf{Task}} & \\multicolumn{2}{c|}{Planning and optimization} \\\\\n        \\hline\n        \\textbf{Image} & \\textbf{Text} & \\textbf{Action Prompt} & \\textbf{Ground Truth} \\\\\n        \\hline\n        \\includegraphics[width=2.8cm]{figs/000000000.jpg} & \n        Task: put a potato in countertop & \n        Embodied household task: predict next decision-making behavior based on multimodal information. & \n        go to garbagecan 1 \\\\\n        \\hline\n        \\multicolumn{2}{|c|}{\\textbf{Method}} & \\multicolumn{2}{c|}{\\textbf{Output Results}} \\\\\n        \\hline\n        \\multicolumn{2}{|c|}{T5 FT} & \\multicolumn{2}{m{6cm}|}{\\centering go to microwave 1} \\\\\n        \\hline\n        \\multicolumn{2}{|c|}{GWM-E} & \\multicolumn{2}{m{6cm}|}{\\centering go to garbagecan 1} \\\\\n        \\hline\n    \\end{tabular}", "subtables": []}], "figure": [{"original": "\\begin{figure*}[ht]\n\\centering\n\\includegraphics[width=\\linewidth]{figs/r1.pdf} \n\\vspace{-5mm}\n\\caption{\\textbf{Multi-modal world state transition can be modeled via graphs}. We model the current state as a graph and each node contains one or more modalities from image, table, and text. Further, the world action is modeled as an action node that queries the current state nodes. We categorize actions into two types: intended actions, which include three levels\u2014node, edge, and graph\u2014and unintended actions, whose implementation involves similarity computation similar to RAG. Finally, the transition function updates states at three different levels based on state and action: update nodes, update edges, and update graphs.}\n\\label{fig:graph_world}\n\\vspace{-2mm}\n\\end{figure*}", "caption": "\\caption{\\textbf{Multi-modal world state transition can be modeled via graphs}. We model the current state as a graph and each node contains one or more modalities from image, table, and text. Further, the world action is modeled as an action node that queries the current state nodes. We categorize actions into two types: intended actions, which include three levels\u2014node, edge, and graph\u2014and unintended actions, whose implementation involves similarity computation similar to RAG. Finally, the transition function updates states at three different levels based on state and action: update nodes, update edges, and update graphs.}", "label": "\\label{fig:graph_world}", "subfigures": [], "figure_paths": ["figs/r1.pdf"]}, {"original": "\\begin{figure*}[ht]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{figs/Instantiations.pdf} \n\\vspace{-2mm}\n\\caption{\\textbf{Instantiations of GWM}.  \n(a) Multi-modal generation\nand matching contains two sub-tasks. For multi-modal generation, it models modal clusters as nodes, whereas for multi-modal matching, it models nodes for each modality. It includes edges that represent inter-modal correspondences and cross-modal similarities. For these two types of subtasks, there are node-level and edge-level action nodes, respectively. (b) The state nodes of recommendation include user nodes and item nodes. Moreover, its edges are primarily derived from user-item interactions. We model edge-level action nodes to perform interaction prediction. (c) In traditional graph prediction, we follow existing work to construct task nodes and edges, and model three levels of action nodes according to different types of tasks. (d) The state nodes of multi-agent collaboration include agent nodes and multi-modal nodes from external knowledge. Its edges primarily consist of communications between agents and interactions between agents and external knowledge. We set up a graph-level action node to generate content based on the interactions of agents. (e)  Retrieval-augmented generation treats each data chunk as a state node and builds edges through the embedding similarity between chunks. For this task, we have established unintended action nodes.  (f) For planning and optimization, we model each decision as a state node and construct edges based on their relationships. We have established graph-level action nodes to generate the next decision.}\n\\label{fig: Instantiations of GWM}\n\\vspace{-3mm}\n\\end{figure*}", "caption": "\\caption{\\textbf{Instantiations of GWM}.  \n(a) Multi-modal generation\nand matching contains two sub-tasks. For multi-modal generation, it models modal clusters as nodes, whereas for multi-modal matching, it models nodes for each modality. It includes edges that represent inter-modal correspondences and cross-modal similarities. For these two types of subtasks, there are node-level and edge-level action nodes, respectively. (b) The state nodes of recommendation include user nodes and item nodes. Moreover, its edges are primarily derived from user-item interactions. We model edge-level action nodes to perform interaction prediction. (c) In traditional graph prediction, we follow existing work to construct task nodes and edges, and model three levels of action nodes according to different types of tasks. (d) The state nodes of multi-agent collaboration include agent nodes and multi-modal nodes from external knowledge. Its edges primarily consist of communications between agents and interactions between agents and external knowledge. We set up a graph-level action node to generate content based on the interactions of agents. (e)  Retrieval-augmented generation treats each data chunk as a state node and builds edges through the embedding similarity between chunks. For this task, we have established unintended action nodes.  (f) For planning and optimization, we model each decision as a state node and construct edges based on their relationships. We have established graph-level action nodes to generate the next decision.}", "label": "\\label{fig: Instantiations of GWM}", "subfigures": [], "figure_paths": ["figs/Instantiations.pdf"]}, {"original": "\\begin{figure*}[ht]\n\\centering\n\\vspace{-1mm}\n\\includegraphics[width=\\linewidth]{figs/r2.pdf}\n\\vspace{-3mm}\n\\caption{\\textbf{Framework of GWM.} For both token-based and embedding-based GFM, we initially unify the multi-modal current state into graph nodes, conduct message passing, and then combine actions to predict the next state across different modalities through respective decoders. The key distinctions are: 1) Token-based GWM integrates multi-modalities into text, whereas embedding-based GWM uses modality-specific encoders to process them into embeddings; 2) Token-based GWM utilizes text-based methods for message passing, while embedding-based GWM operates at the embedding level; 3) Token-based GWM converts information into prompt form for the decoder, while embedding-based GWM employs a multi-hop projector to manage embedding-level information.}\n\\label{fig:framework}\n\\vspace{-4mm}\n\\end{figure*}", "caption": "\\caption{\\textbf{Framework of GWM.} For both token-based and embedding-based GFM, we initially unify the multi-modal current state into graph nodes, conduct message passing, and then combine actions to predict the next state across different modalities through respective decoders. The key distinctions are: 1) Token-based GWM integrates multi-modalities into text, whereas embedding-based GWM uses modality-specific encoders to process them into embeddings; 2) Token-based GWM utilizes text-based methods for message passing, while embedding-based GWM operates at the embedding level; 3) Token-based GWM converts information into prompt form for the decoder, while embedding-based GWM employs a multi-hop projector to manage embedding-level information.}", "label": "\\label{fig:framework}", "subfigures": [], "figure_paths": ["figs/r2.pdf"]}, {"original": "\\begin{figure}[ht]\n\\centering\n\\includegraphics[width=\\linewidth]{figs/hops_radar.pdf} \n\\vspace{-4mm}\n\\caption{\\textbf{Multi-hop graphs enhance GWM's performance across representative tasks in six domains}. We can observe that the introduction of graphs has benefited GWM-E across all tasks compared to no graph. Moreover, excessive hops can lead to over-smoothing, thereby decreasing performance.} %We can observe that the introduction of graphs has benefited GWM-E across all tasks compared to no graph. Moreover, excessive hops can lead to over-smoothing, thereby decreasing performance.\n\\label{fig:performance_hops}\n\\vspace{-3mm}\n\\end{figure}", "caption": "\\caption{\\textbf{Multi-hop graphs enhance GWM's performance across representative tasks in six domains}. We can observe that the introduction of graphs has benefited GWM-E across all tasks compared to no graph. Moreover, excessive hops can lead to over-smoothing, thereby decreasing performance.}", "label": "\\label{fig:performance_hops}", "subfigures": [], "figure_paths": ["figs/hops_radar.pdf"]}, {"original": "\\begin{figure}[t]\n\\centering\n\\includegraphics[width=\\linewidth]{figs/zero_few_shot.pdf} \n\\caption{\\textbf{GWM boosts zero-shot/few-shot performance on multi-agent collaboration (Agent) and retrieval-augmented generation (RAG) tasks}. Note that ``-T'' and ``-E'' respectively represent the experimental results of GWM-T and GWM-E. It can be observed that  GWM can quickly adapt to new tasks with a small amount of domain-specific training data. Moreover, GWM's strong generalization ability can boost the performance of Agent and RAG.} %It can be observed that  GWM can quickly adapt to new tasks with a small amount of domain-specific training data. Moreover, GWM's strong generalization ability can boost the performance of Agent and RAG.\n\\label{fig:zero_few_shot}\n\\end{figure}", "caption": "\\caption{\\textbf{GWM boosts zero-shot/few-shot performance on multi-agent collaboration (Agent) and retrieval-augmented generation (RAG) tasks}. Note that ``-T'' and ``-E'' respectively represent the experimental results of GWM-T and GWM-E. It can be observed that  GWM can quickly adapt to new tasks with a small amount of domain-specific training data. Moreover, GWM's strong generalization ability can boost the performance of Agent and RAG.}", "label": "\\label{fig:zero_few_shot}", "subfigures": [], "figure_paths": ["figs/zero_few_shot.pdf"]}], "equations": ["\\begin{equation}\n    \\label{eq:sage}\n    \\mb{h}_v^{(l)} =f_{v}\\Big(\\textsc{Concat}(\\mb{h}_v^{(l-1)},\\{\\mb{h}_u^{(l-1)}, u \\in {N}(v) \\})\\Big),\n\\end{equation}", "\\begin{equation}\\label{equ:sft}\n\\hspace{-1mm}\\mathcal{L}_{SFT}=-\\sum_{t}logP_{f_{\\text{SFT}}}(y_t|P_{sa}(\\mathbf{h}_{vr},a),y_1, \\ldots y_{t-1}).\n\\end{equation}", "\\begin{equation}\\label{eq:max}\n\\mathcal{L}_{SFT}=-\\sum_{t}logP_{f_{\\text{SFT}}}(y_t|X_G,a,y_1, \\ldots y_{t-1}).\n\\end{equation}"], "algorithm": [], "sections": {"Introduction": {"content": "\n\\label{sec:intro}\n\nA world model (WM) \\citep{ha2018recurrent} constructs the world observations as states and predicts future states based on given actions. Modern world models are trained with massive data \\citep{liu2024world,cui2023universal}, demonstrating successful prediction, generation, and planning capabilities. However, existing world models do not directly generalize to structured data, primarily graphs, that are ubiquitous in science \\cite{jin2018junction,you2018graph} and industry \\cite{ying2018graph,you2022roland} and can be further enriched with multi-modal information \\cite{ektefaie2023multimodal}.\nTherefore, our paper aims to raise attention to this pressing research question: \n\\textit{Can we extend a WM to handle graph-structured data across a broad range of tasks?}\n\nExisting WMs mainly focus on unstructured data. For example, iVideoGPT \\citep{wu2024ivideogpt} and Genie \\citep{bruce2024genie} are successful world models over video data. However, the relations and structures in the data are rarely explored in these works. Although some works \\citep{zhang2021world,zhu2022value} attempt to model structured data in WM using graphs, they have focused solely on planning problems in a specific domain.\nIn recent years, researchers have also explored the concept of the Graph Foundation Model (GFM)~\\citep{liu2023one,chen2024llaga}. However, these methods are confined to predefined graph learning tasks, which cannot easily extend to: (1) multi-modal input data including images and text, (2) diverse tasks beyond standard graph prediction tasks, and (3) data without explicit structure, \\ie, standard unstructured data.\n\n\nTo address these challenges, we propose the Graph World Model (GWM) that embeds the capabilities of the graph into the WM, which models the current state as a graph and the action as a node (see Table~\\ref{Tab:GWM_intro} for comparison with existing methods).\nVarious tasks can be expressed as action nodes; for example, in a graph prediction task, predicting the label of a given node/edge/subgraph leads to \\textit{intended} action nodes that link relevant nodes in the state graph, \\ie, target nodes, to the action node; in a retrieval-augmented generation (RAG) task, we can also represent a user query as an \\textit{unintended} action node that links to target nodes in the state graph via embedding similarities.\n\n\nTo build GWMs, we first introduce a simplified token-based GWM (GWM-T), which integrates multi-modal data like image, table, and text into text modality and represents them as nodes in a graph state. We further develop a token-level message-passing algorithm that aggregates the neighbor information to update the text representation of the state node. Finally, the target nodes on the state graph and prompted action nodes will be fed into multi-modal decoders such as LLMs and Stable Diffusion \\cite{rombach2022high}. Despite its simplicity, GWM-T sometimes suffers from high token costs and limited context length. Inspired by latent diffusion models, which introduced modeling in latent space rather than directly on pixels like diffusion to enhance model performance and efficiency, we further develop an embedding-based GWM (GWM-E). GWM-E first employs modality-specific encoders to process different modalities into node embeddings. Then it utilizes embedding-level message\npassing to update the node embedding. Finally, the multi-modal information in target state nodes is consolidated through a multi-hop projector before passing them to the decoders. \n\nWe conduct extensive experiments on 6 tasks from diverse domains, including world prediction (multi-modal generation and matching, recommendation, graph prediction), world generation (multi-agent collaboration, retrieval-augmented generation), and world optimization (planning and optimization), with both proposed GWM variants and domain-specific baselines. Results show that (1) GWMs generalize across domains, as the same GWM outperforms or matches domain-specific baselines' performance, (2) graph information matters in GWM, as GWMs benefit from multi-hop graph information, and (3) GWMs demonstrate strong zero-shot/few-shot capabilities on unseen new tasks.\n\n\n\n\n\n\n\n\n\n\n\n\\begin{table}[t]\n    \\setlength\\tabcolsep{2pt}\n    \\caption{\\textbf{Comparison with existing representative works from three perspectives: task type, data structure, and model type.} Compared with existing WM and GFM, GWM can tackle multi-domain tasks and be applied to both structured and unstructured data. } \n    \\label{Tab:GWM_intro}\n    \\centering\n    \\resizebox{0.48\\textwidth}{!}{\n    \\begin{tabular}{lccc}\n        \\toprule\n        \\textbf{Method} & \\textbf{Task Type} & \\textbf{Data Structure} & \\textbf{Model Type} \\\\\n        \\midrule\n         Genie \\citep{bruce2024genie} & Video generation  & Unstructured & WM \\\\\n         $L^3P$ \\citep{zhang2021world} & Planning  & Structured & WM \\\\\n         BioBridge \\citep{wangbiobridge}  & Biomedical domain  & Structured & GFM \\\\ \n         LLAGA \\citep{chen2024llaga}  & Graph domain  & Structured & GFM \\\\ \\midrule\n        \\rowcolor{cyan!10} GWM-T  & Multiple domains & Both & GWM \\\\\n        \\rowcolor{cyan!10} GWM-E & Multiple domains & Both & GWM \\\\\n        \\bottomrule\n        \\end{tabular}}\n    % \\vspace{-5mm}\n\\end{table}\n    \\setlength\\tabcolsep{2pt}\n    \\caption{\\textbf{Comparison with existing representative works from three perspectives: task type, data structure, and model type.} Compared with existing WM and GFM, GWM can tackle multi-domain tasks and be applied to both structured and unstructured data. } \n    \\label{Tab:GWM_intro}\n    \\centering\n    \\resizebox{0.48\\textwidth}0.48\\textwidth{!}!{\n    \\begin{tabular}{lccc}\n        \\toprule\n        \\textbf{Method} & \\textbf{Task Type} & \\textbf{Data Structure} & \\textbf{Model Type} \\\\\n        \\midrule\n         Genie \\citep{bruce2024genie} & Video generation  & Unstructured & WM \\\\\n         $L^3P$ \\citep{zhang2021world} & Planning  & Structured & WM \\\\\n         BioBridge \\citep{wangbiobridge}  & Biomedical domain  & Structured & GFM \\\\ \n         LLAGA \\citep{chen2024llaga}  & Graph domain  & Structured & GFM \\\\ \\midrule\n        \\rowcolor{cyan!10} GWM-T  & Multiple domains & Both & GWM \\\\\n        \\rowcolor{cyan!10} GWM-E & Multiple domains & Both & GWM \\\\\n        \\bottomrule\n        \\end{tabular}}\n    \n        \\toprule\n        \\textbf{Method} & \\textbf{Task Type} & \\textbf{Data Structure} & \\textbf{Model Type} \\\\\n        \\midrule\n         Genie \\citep{bruce2024genie} & Video generation  & Unstructured & WM \\\\\n         $L^3P$L^3P \\citep{zhang2021world} & Planning  & Structured & WM \\\\\n         BioBridge \\citep{wangbiobridge}  & Biomedical domain  & Structured & GFM \\\\ \n         LLAGA \\citep{chen2024llaga}  & Graph domain  & Structured & GFM \\\\ \\midrule\n        \\rowcolor{cyan!10}cyan!10 GWM-T  & Multiple domains & Both & GWM \\\\\n        \\rowcolor{cyan!10}cyan!10 GWM-E & Multiple domains & Both & GWM \\\\\n        \\bottomrule\n        \n    \n\n\n\n\n\n", "appendix": false}, "Graph World Model": {"content": "\n\\label{sec:pre}\n\n\\begin{figure*}[ht]\n\\centering\n\\includegraphics[width=\\linewidth]{figs/r1.pdf} \n\\vspace{-5mm}\n\\caption{\\textbf{Multi-modal world state transition can be modeled via graphs}. We model the current state as a graph and each node contains one or more modalities from image, table, and text. Further, the world action is modeled as an action node that queries the current state nodes. We categorize actions into two types: intended actions, which include three levels\u2014node, edge, and graph\u2014and unintended actions, whose implementation involves similarity computation similar to RAG. Finally, the transition function updates states at three different levels based on state and action: update nodes, update edges, and update graphs.}\n\\label{fig:graph_world}\n\\vspace{-2mm}\n\\end{figure*}\n\\centering\n\\includegraphics[width=\\linewidth]{figs/r1.pdf} \n\\vspace{-5mm}\n\\caption{\\textbf{Multi-modal world state transition can be modeled via graphs}. We model the current state as a graph and each node contains one or more modalities from image, table, and text. Further, the world action is modeled as an action node that queries the current state nodes. We categorize actions into two types: intended actions, which include three levels\u2014node, edge, and graph\u2014and unintended actions, whose implementation involves similarity computation similar to RAG. Finally, the transition function updates states at three different levels based on state and action: update nodes, update edges, and update graphs.}\n\\label{fig:graph_world}\n\\vspace{-2mm}\n\n\n\\subsection{World Model Preliminaries} \\label{sec:tra_wm} A world model aims to predict future states based on the current state and action, which contains the following main components: \\textbf{(1) State}. The state $s$s of the world model estimates the observation of the world. It usually consists of multi-modal information and the state of the $t$t step/time slot can be depicted as $s_t$s_t. \\textbf{(2) Action}. The action $a_t$a_t  at step/time $t$t is task-related. It can be a real-world operation, a code function in the digital world, and even some queries and instructions. \\textbf{(3) Transition}. The transition $P(s_{t+1}|s_t, a_t)$P(s_{t+1}t+1|s_t, a_t) depicts the transit probability from the current state  $s_t$s_t to its next state $s_{t+1}$s_{t+1}t+1 after the execution of action $a_t$a_t.\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{figure*}[ht]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{figs/Instantiations.pdf} \n\\vspace{-2mm}\n\\caption{\\textbf{Instantiations of GWM}.  \n(a) Multi-modal generation\nand matching contains two sub-tasks. For multi-modal generation, it models modal clusters as nodes, whereas for multi-modal matching, it models nodes for each modality. It includes edges that represent inter-modal correspondences and cross-modal similarities. For these two types of subtasks, there are node-level and edge-level action nodes, respectively. (b) The state nodes of recommendation include user nodes and item nodes. Moreover, its edges are primarily derived from user-item interactions. We model edge-level action nodes to perform interaction prediction. (c) In traditional graph prediction, we follow existing work to construct task nodes and edges, and model three levels of action nodes according to different types of tasks. (d) The state nodes of multi-agent collaboration include agent nodes and multi-modal nodes from external knowledge. Its edges primarily consist of communications between agents and interactions between agents and external knowledge. We set up a graph-level action node to generate content based on the interactions of agents. (e)  Retrieval-augmented generation treats each data chunk as a state node and builds edges through the embedding similarity between chunks. For this task, we have established unintended action nodes.  (f) For planning and optimization, we model each decision as a state node and construct edges based on their relationships. We have established graph-level action nodes to generate the next decision.}\n\\label{fig: Instantiations of GWM}\n\\vspace{-3mm}\n\\end{figure*}\n\\centering\n\\includegraphics[width=0.95\\linewidth]{figs/Instantiations.pdf} \n\\vspace{-2mm}\n\\caption{\\textbf{Instantiations of GWM}.  \n(a) Multi-modal generation\nand matching contains two sub-tasks. For multi-modal generation, it models modal clusters as nodes, whereas for multi-modal matching, it models nodes for each modality. It includes edges that represent inter-modal correspondences and cross-modal similarities. For these two types of subtasks, there are node-level and edge-level action nodes, respectively. (b) The state nodes of recommendation include user nodes and item nodes. Moreover, its edges are primarily derived from user-item interactions. We model edge-level action nodes to perform interaction prediction. (c) In traditional graph prediction, we follow existing work to construct task nodes and edges, and model three levels of action nodes according to different types of tasks. (d) The state nodes of multi-agent collaboration include agent nodes and multi-modal nodes from external knowledge. Its edges primarily consist of communications between agents and interactions between agents and external knowledge. We set up a graph-level action node to generate content based on the interactions of agents. (e)  Retrieval-augmented generation treats each data chunk as a state node and builds edges through the embedding similarity between chunks. For this task, we have established unintended action nodes.  (f) For planning and optimization, we model each decision as a state node and construct edges based on their relationships. We have established graph-level action nodes to generate the next decision.}\n\\label{fig: Instantiations of GWM}\n\\vspace{-3mm}\n\n\n\n\n\\subsection{Multi-modal World Represented by Graphs}\\label{sec:Multi-modal World} \n\n\\xhdr{Graph for state modeling}Graph for state modeling \nWe define the world state as a graph $ \\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ \\mathcal{G} = (\\mathcal{V}, \\mathcal{E}) to represent multi-modal data with complex relationships, shown in Figure \\ref{fig:graph_world}. Specifically, $\\mathcal{V}=\\{v\\}$\\mathcal{V}=\\{v\\} represents the set of nodes where each node $v=[v^a, v^b, v^e]$v=[v^a, v^b, v^e] consists of multi-modal information, including image $ v^a $ v^a , table $ v^b $ v^b , and text $ v^e $ v^e  modality; when a modality is absent, the corresponding tensor will be empty. $ \\mathcal{E} = \\mathcal{E}_p \\cup \\mathcal{E}_m \\}$ \\mathcal{E} = \\mathcal{E}_p \\cup \\mathcal{E}_m \\} is the edge set consists of explicit edges $ \\mathcal{E}_p$ \\mathcal{E}_p and implicit edges $\\mathcal{E}_m$\\mathcal{E}_m. Explicit edges $\\mathcal{E}_p$\\mathcal{E}_p are often those established through expert knowledge or ground truth observations. For example, in the ogbn-arxiv dataset \\citep{hu2020open}, edges are determined based on references between papers and historical collaborations between authors. Implicit edges $\\mathcal{E}_m$\\mathcal{E}_m are those constructed through connections represented by embedding similarities in the dataset. A typical example is in many protein datasets \\citep{heumos2023best,stuart2019comprehensive,stuart2019integrative}, where edges are obtained based on the similarity of certain node feature embeddings.\n\n\n\n\n\\xhdr{Different levels of world action and state transition}Different levels of world action and state transition  We model the action $a$a as an action node that queries the current state nodes $v$v to obtain the target nodes $v_r$v_r using function $R$R, whose process can be formulated as $v_r=R(v,a)$v_r=R(v,a). We further categorize the world's actions into two types, as shown in Figure \\ref{fig:graph_world}: one is directly related to the specific structures on the graph, called intended action $a_d$a_d. It includes three levels: node-level, edge-level, and graph-level.  The other is indirectly related to the specific structures on the graph through semantic relations such as Retrieval-augmented Generation (RAG), called unintended action $a_u$a_u. As shown in Figure \\ref{fig:graph_world}, to implement this action, we can first calculate the similarity between the action node and state nodes, and then retrieve the top-k state nodes for querying.  According to the introduction in Section \\ref{sec:tra_wm}, we can conclude that an action causes a transition of state $s_{t+1}=f_{tr}(s_t,a_t)$s_{t+1}t+1=f_{tr}tr(s_t,a_t), which includes three types: update nodes, update edges, and update graphs. Here, $f_{tr}$f_{tr}tr means a transition function, which can be a neural network.\n\n\n\n\n\n\n\n\\begin{figure*}[ht]\n\\centering\n\\vspace{-1mm}\n\\includegraphics[width=\\linewidth]{figs/r2.pdf}\n\\vspace{-3mm}\n\\caption{\\textbf{Framework of GWM.} For both token-based and embedding-based GFM, we initially unify the multi-modal current state into graph nodes, conduct message passing, and then combine actions to predict the next state across different modalities through respective decoders. The key distinctions are: 1) Token-based GWM integrates multi-modalities into text, whereas embedding-based GWM uses modality-specific encoders to process them into embeddings; 2) Token-based GWM utilizes text-based methods for message passing, while embedding-based GWM operates at the embedding level; 3) Token-based GWM converts information into prompt form for the decoder, while embedding-based GWM employs a multi-hop projector to manage embedding-level information.}\n\\label{fig:framework}\n\\vspace{-4mm}\n\\end{figure*}\n\\centering\n\\vspace{-1mm}\n\\includegraphics[width=\\linewidth]{figs/r2.pdf}\n\\vspace{-3mm}\n\\caption{\\textbf{Framework of GWM.} For both token-based and embedding-based GFM, we initially unify the multi-modal current state into graph nodes, conduct message passing, and then combine actions to predict the next state across different modalities through respective decoders. The key distinctions are: 1) Token-based GWM integrates multi-modalities into text, whereas embedding-based GWM uses modality-specific encoders to process them into embeddings; 2) Token-based GWM utilizes text-based methods for message passing, while embedding-based GWM operates at the embedding level; 3) Token-based GWM converts information into prompt form for the decoder, while embedding-based GWM employs a multi-hop projector to manage embedding-level information.}\n\\label{fig:framework}\n\\vspace{-4mm}\n\n\n\n\n\\subsection{Instantiations of GWM} \\label{sec:Instantiations} \n\nAs shown in  Figure \\ref{fig: Instantiations of GWM}, we have listed some representative instantiations that can be unified into a graph world model from three aspects: (a) world prediction, (b) world generation, and (c) world optimization.\n\n\\xhdr{World prediction}World prediction\n\\textbf{(1) Multi-modal generation and matching.} As shown in Figure \\ref{fig: Instantiations of GWM}(a), the task includes two subtasks. The multi-modal generation task \\citep{rombach2022high,zhang2023adding} involves predicting missing modalities given the available modal information and their interconnections. Specifically, it considers clusters of corresponding modalities (such as an image, table, and text describing the same entity) as state nodes, and the relationships between clusters, such as similarity, are treated as edges. Thus, the action node here is at the node level. The multi-modal matching task \\citep{rombach2022high}, similar to CLIP's pre-training task \\citep{radford2021learning}, predicts the correspondence between modalities. It treats each modality as a state node and the correspondences between modalities (including cross-modality similarity relationships) \\citep{jin2024instructg2i} as edges. Here, the action node is at the edge level. \\textbf{(2) Recommendation.} Recommendations \\citep{ni2023content,isinkaye2015recommendation,ko2022survey} are based on the historical interactions and features of users and items to predict future interactions, as shown in Figure \\ref{fig: Instantiations of GWM}(b). Specifically, it models the user nodes and item nodes as state nodes. Moreover, the action node is edge-level.\n\\textbf{(3) Traditional graph prediction.} Traditional graph prediction \\citep{kipf2016semi,velivckovic2017graph,hamilton2017inductive} primarily focuses on three types of tasks: node-level, edge-level, and graph-level. We follow previous work's settings of nodes and edges and define the action nodes of three levels.\n\n\n\n\n\\xhdr{World generation}World generation\n\\textbf{(4) Multi-agent collaboration.} As shown in Figure \\ref{fig: Instantiations of GWM}(d), the purpose \\citep{zhuge2024language,liu2023dynamic,wu2024agentkit} of this task is to generate task-oriented outputs based on the interaction between agents and external knowledge, as well as communication among agents. Specifically, its state nodes consist of agent nodes with different profiles, along with multi-modal nodes in external knowledge. Its edges include agent-agent and agent-knowledge relationships. The action node is graph level and the target nodes primarily include various agent nodes \\citep{zhuge2024language,liu2023dynamic}.\n\\textbf{(5) Retrieval-augmented generation.} The purpose of Retrieval-Augmented Generation (RAG) is to enhance the generation capabilities of Large Language Models (LLMs) by retrieving information from external knowledge \\citep{lewis2020retrieval,gao2023retrieval,zhao2024retrieval}. Recent studies such as GraphRAG \\citep{edge2024local,peng2024graph} have shown that modeling the relationships between data chunks in external knowledge can enhance the generative capabilities of RAG. As illustrated in Figure \\ref{fig: Instantiations of GWM}(e), we model data chunks as nodes and the similarity of embeddings between chunks as edges. As introduced in Section \\ref{sec:Multi-modal World}, we design an unintended action node for RAG tasks.\n\n\n\\xhdr{World optimization}World optimization\n\\textbf{(6) Planning and optimization.} World optimization involves generating the next best decision based on a sequence of historical decisions \\citep{chen2021decision,zheng2022online,siebenborn2022crucial}. Many studies have shown that modeling the relationships between historical decisions using graphs can enhance the decision-making effectiveness of world optimization \\citep{jiang2018graph,munikoti2023challenges}. Following them, we model decision states as  nodes. The edges between these state nodes are often modeled based on their relationships, such as distance relationships \\citep{prates2019learning} and the similarity of embeddings \\citep{munikoti2023challenges, jiang2018graph}. We model the action node as the graph level.\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "appendix": false}, "Token-based GFM": {"content": "\\label{sec:Token-based GFM}\n\n\\subsection{Multi-modality as token} \\label{sec:Multi-modal as token}\n\nOne of the easiest ways to unify multi-modalities is to transfer them into text. Specifically, as shown in Figure \\ref{fig:framework}, for image nodes ${v}^a${v}v^a, we utilize a pretrained image-to-text LLaVA model \\citep{liu2024visual} $L$L to transform them into text nodes ${v}^{ta}=L({v}^a)${v}v^{ta}ta=L({v}v^a). For table nodes ${v}^b${v}v^b, we employ a table-prompt model $T$T to transform them into text nodes ${v}^{tb}=T({v}^b)${v}v^{tb}tb=T({v}v^b) given the column names and feature values.  Specifically, each\nvalue is paired with the corresponding column in the format\nof ``\\{column 1\\} is \\{value 1\\}, \\{column 2\\} is \\{value 2\\}, ...''. Finally, we used a prompt template $P_{u}$P_{u}u (specified Table \\ref{prompt-template:multi-modality as token} in Appendix \\ref{sec:prompt}) to unify the three modal nodes into a single text node $v_c=P_{u}({v}^{ta},{v}^{tb},v^e)$v_c=P_{u}u({v}v^{ta}ta,{v}v^{tb}tb,v^e).\n\n\\subsection{Token-level message passing} \n\nIn contrast to traditional graph message passing \\citep{kipf2016semi,hamilton2017inductive,velivckovic2017graph}, we employ token-level message passing here, which aggregates the text information of neighboring nodes. Specifically, as shown in the middle part of Figure \\ref{fig:framework}, for the each unified text node $v_c$v_c, the node embeddings update of the $l$l-th layer is represented as: \n\\vspace{-3.5mm}\n\\begin{equation}\n    \\label{eq:sage}\n    \\mb{h}_v^{(l)} =f_{v}\\Big(\\textsc{Concat}(\\mb{h}_v^{(l-1)},\\{\\mb{h}_u^{(l-1)}, u \\in {N}(v) \\})\\Big),\n\\end{equation}\\begin{equation}\n    \\label{eq:sage}\n    \\mb{h}_v^{(l)} =f_{v}\\Big(\\textsc{Concat}(\\mb{h}_v^{(l-1)},\\{\\mb{h}_u^{(l-1)}, u \\in {N}(v) \\})\\Big),\n\\end{equation}\n    \\label{eq:sage}\n    \\mb{h}h_v^{(l)}(l) =f_{v}v\\Big(\\textsc{Concat}(\\mb{h}h_v^{(l-1)}(l-1),\\{\\mb{h}h_u^{(l-1)}(l-1), u \\in {N}N(v) \\})\\Big),\n\nwhere $\\mb{h}_v^{(l)}$\\mb{h}h_v^{(l)}(l) is the node text presentation after $l$l iterations, $\\mb{h_v}^{(0)}$\\mb{h_v}h_v^{(0)}(0) has been initialized as $\\mb{h_v}^{(0)}=v_c$\\mb{h_v}h_v^{(0)}(0)=v_c. In addition, ${N}(v)${N}N(v) denotes the direct neighbors of node $v$v and $f_{v}(\\cdot)$f_{v}v(\\cdot) denotes prompting strategy functions (specified in Table \\ref{prompt-template:aggregating} of Appendix \\ref{sec:prompt}) to unify nodes information of different hops. \n\n\n\n\n\n\\subsection{Instruction tuning} \\label{sec:Instruction tuning}  Based on token-level message passing, we can obtain node text representations $\\mathbf{h}_v$\\mathbf{h}_v for each node. Combining the discussion in Section \\ref{sec:Multi-modal World}, we identify the target nodes $v_r$v_r and their node text representations $\\mathbf{h}_{vr}$\\mathbf{h}_{vr}vr, along with the action node $a$a and the state node. Further, we describe the action node using text and utilize a task-oriented prompt template $P_{sa}$P_{sa}sa (specified in Appendix \\ref{sec:prompt}) to combine the information from the target nodes and the action node, as shown in the right part of Figure \\ref{fig:framework}.  In response to the different modalities in next states, we designed two types of decoders. We first designed stable diffusion (SD) to generate images.\n\n\\xhdr{Instruction tuning of SD}Instruction tuning of SD SD operates by performing diffusion in a compressed latent space rather than directly on pixels. Initially, the system maps an input image $x$x to a lower-dimensional latent code $\\mathbf{z}=\\text{Enc}(x)$\\mathbf{z}=\\text{Enc}(x) through an encoder network.\nThe generated latent representation $\\mathbf{z}'$\\mathbf{z}' is subsequently transformed back into image space via a decoder network, producing the final output $x'=\\text{Dec}(\\mathbf{z}')$x'=\\text{Dec}(\\mathbf{z}'). This latent representation $\\mathbf{z}'$\\mathbf{z}' is generated by the diffusion model using textual guidance from a prompt $c_T=P_{sa}(\\mathbf{h}_{vr},a)$c_T=P_{sa}sa(\\mathbf{h}_{vr}vr,a).\nThe fundamental optimization objective for training SD can be expressed mathematically as: \n\\vspace{-1mm}\n\\begin{gather}\n    \\hspace{-1mm}\\mathcal{L}=\\mathbb{E}_{\\mathbf{z} \\sim \\text{Enc}(x), c_T, \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ \\|\\epsilon - \\epsilon_\\theta(\\mathbf{z}_t, t, h(c_T))\\|^2 \\right]\n\\end{gather}\\begin{gather}\n    \\hspace{-1mm}\\mathcal{L}=\\mathbb{E}_{\\mathbf{z} \\sim \\text{Enc}(x), c_T, \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ \\|\\epsilon - \\epsilon_\\theta(\\mathbf{z}_t, t, h(c_T))\\|^2 \\right]\n\\end{gather}\n    \\hspace{-1mm}\\mathcal{L}=\\mathbb{E}_{\\mathbf{z} \\sim \\text{Enc}(x), c_T, \\epsilon \\sim \\mathcal{N}(0,1), t}\\mathbf{z} \\sim \\text{Enc}(x), c_T, \\epsilon \\sim \\mathcal{N}(0,1), t \\left[ \\|\\epsilon - \\epsilon_\\theta(\\mathbf{z}_t, t, h(c_T))\\|^2 \\right]\n\nDuring each iterative step $t$t, a specialized denoising network $\\epsilon_\\theta(\\cdot)$\\epsilon_\\theta(\\cdot) estimates noise patterns by jointly processing three inputs: the current latent state $\\mathbf{z}_t$\\mathbf{z}_t, a temporal position indicator $t$t, and encoded text features $h(c_T)$h(c_T). The text features $h(c_T)\\in\\mathbf{R}^{d\\times l_{c_T}}$h(c_T)\\in\\mathbf{R}^{d\\times l_{c_T}}d\\times l_{c_T}c_T are extracted using CLIP's text encoder \\citep{radford2021learning} $h(c_T)=\\text{CLIP}(c_T)$h(c_T)=\\text{CLIP}(c_T), where $l_{c_T}$l_{c_T}c_T represents the prompt length and $d$d denotes the feature dimensionality.\n\n\n\n\n\n\\xhdr{Instruction tuning of LLM}Instruction tuning of LLM We then design LLM to generate texts for both text and table modalities. We followed the standard instruction tuning practice \\citep{zhang2023instruction,peng2023instruction}, which encourages LLMs to adhere to user requests when returning the outputs. For the input instruction $P_{sa}(\\mathbf{h}_{vr},a)$P_{sa}sa(\\mathbf{h}_{vr}vr,a), we supply a response representing the next predicted states, consisting of $t$t tokens and denoted as $y=\\{y_1, \\ldots y_t\\}$y=\\{y_1, \\ldots y_t\\}. We train the LLM to yield $f_{\\text{SFT}}$f_{\\text{SFT}}\\text{SFT} via\n\\vspace{-1mm}\n\\begin{equation}\\label{equ:sft}\n\\hspace{-1mm}\\mathcal{L}_{SFT}=-\\sum_{t}logP_{f_{\\text{SFT}}}(y_t|P_{sa}(\\mathbf{h}_{vr},a),y_1, \\ldots y_{t-1}).\n\\end{equation}\\begin{equation}\\label{equ:sft}\n\\hspace{-1mm}\\mathcal{L}_{SFT}=-\\sum_{t}logP_{f_{\\text{SFT}}}(y_t|P_{sa}(\\mathbf{h}_{vr},a),y_1, \\ldots y_{t-1}).\n\\end{equation}\\label{equ:sft}\n\\hspace{-1mm}\\mathcal{L}_{SFT}SFT=-\\sum_{t}tlogP_{f_{\\text{SFT}}}f_{\\text{SFT}}\\text{SFT}(y_t|P_{sa}sa(\\mathbf{h}_{vr}vr,a),y_1, \\ldots y_{t-1}t-1).\n\n\\vspace{-7mm}\n\n\n\n\n\n\n\n", "appendix": false}, "Embedding-based GFM": {"content": "\\label{sec:Embedding-based GFM} \n\nAlthough token-based GFM can relatively simply construct a multi-modal world, it is still limited by the high token cost and a restricted multi-hop field of view. Inspired by stable diffusion, which introduces modeling in latent space rather than directly on pixels to enhance model performance and efficiency, we have introduced embedding-based GFM as shown in the bottom half of Figure \\ref{fig:framework}.\n\n\\vspace{-1mm}\n\\subsection{Multi-modality as embedding} The embedding-based GFM unifies the multi-modal nodes in the embedding space. Firstly, as shown in Figure \\ref{fig:framework}, for each modality of the node, we would assign a specific encoder. For the text modality  $ v^e$ v^e, we utilize a BERT model as the encoder $E_{b}$E_{b}b to obtain its embedding $e_{t}$e_{t}t. As for the table node $ v^b$ v^b, we first transform it into text description as discussed in Section \\ref{sec:Multi-modal as token} and then utilize a BERT model as the encoder $E_{b}$E_{b}b to obtain its embedding $e_{b}$e_{b}b. Finally, we deploy a CLIP $E_{c}$E_{c}c model to encode the image node $ v^a$ v^a into image embedding $e_{a}$e_{a}a. We finally obtain the node embedding $e_v=\\textsc{Concat}(e_{a},e_{t},e_{b})$e_v=\\textsc{Concat}(e_{a}a,e_{t}t,e_{b}b) by concatenating the embeddings of all modalities involved with this node. Specifically, if a node misses some modalities, we use zero vectors for them.\n\n\n\n\n\n\n\n\n\\subsection{Embedding-level message passing} \n\nIn this section, we first model the relationships between nodes on the graph through multi-hop aggregation, then aggregate the information from different modalities within the nodes through cross-modal fusion into unified embeddings to pass to the subsequent decoders.\n\n\\xhdr{Multi-hop aggregation}Multi-hop aggregation  We designed a simplified GCN \\citep{wu2019simplifying,he2020lightgcn} to implement multi-hop aggregation, which directly accomplishes parameter-free feature aggregation at the node feature level. Specifically, for the adjacency matrix $\\mathcal{A}$\\mathcal{A} between nodes, we first normalize it to obtain the matrix $\\mathcal{\\tilde{A}}=\\mathcal{D}^{-\\frac{1}{2}}\\mathcal{A}\\mathcal{D}^{-\\frac{1}{2}}$\\mathcal{\\tilde{A}}=\\mathcal{D}^{-\\frac{1}{2}}-\\frac{1}{2}\\mathcal{A}\\mathcal{D}^{-\\frac{1}{2}}-\\frac{1}{2}, where $\\mathcal{D}$\\mathcal{D} represents the degree matrix of $\\mathcal{A}$\\mathcal{A}. Then, for the node vector $X_e$X_e composed of all node embeddings $e_v$e_v, we use the obtained normalized adjacency matrix $\\mathcal{\\tilde{A}}$\\mathcal{\\tilde{A}} to perform $l$l-hop graph aggregation: $X_{e}^{(l)}=\\mathcal{\\tilde{A}}^{l}*X_e$X_{e}e^{(l)}(l)=\\mathcal{\\tilde{A}}^{l}l*X_e, where $X_{e}^{(l)}$X_{e}e^{(l)}(l) is the $l$l-hop graph embedding. We retain the embeddings of the first $L$L hops $[X_e,X_{e}^{(1)},\\ldots,X_{e}^{(L)}]$[X_e,X_{e}e^{(1)}(1),\\ldots,X_{e}e^{(L)}(L)] to the subsequent modules.  \n\n\n\n\n\n\n\n\\xhdr{Cross-modal fusion}Cross-modal fusion We further apply a parameterized projector $f_c$f_c to transform the multi-modalities in the node to unified embeddings: $X_{c}^{(l)}=f_c(X_{e}^{(l)})$X_{c}c^{(l)}(l)=f_c(X_{e}e^{(l)}(l)), where $X_{c}^{(l)}$X_{c}c^{(l)}(l) is the unified node embedding. Specifically, we utilize a simple MLP as projector $f_c$f_c and output the $L$L hops embeddings $X_G=[X_c, X_{c}^{(1)}, \\ldots, X_{c}^{(L)}]$X_G=[X_c, X_{c}c^{(1)}(1), \\ldots, X_{c}c^{(L)}(L)] to the decoders. Note that GWM-E can be extended to heterogeneous graphs by performing separate multi-hop aggregations for each edge type, followed by flattening the resulting node embeddings into a sequence format suitable for input into the LLM decoder.\n\n\n\n\n\\subsection{Projector tuning} \\label{sec:Projector tuning}\n\nAs discussed in Section \\ref{sec:Instruction tuning}, in this section we discuss the tuning of projectors for two different modalities separately.\n\n\\xhdr{Projector tuning of SD}Projector tuning of SD We first incorporate graph conditioning tokens $h_G(c_G)=X_G$h_G(c_G)=X_G into the SD models, functioning concurrently with the pre-existing text conditions $h_T(c_T)$h_T(c_T): $h(c_T, c_G) = [h_T(c_T), h_G(c_G)] \\in \\mathbf{R}^{d \\times (l_{c_T}+l_{c_G})}$h(c_T, c_G) = [h_T(c_T), h_G(c_G)] \\in \\mathbf{R}^{d \\times (l_{c_T}+l_{c_G})}d \\times (l_{c_T}c_T+l_{c_G}c_G), where $l_{c_G}$l_{c_G}c_G is the length of the graph condition. The training objective then becomes:\n\\vspace{-1mm}\n\\begin{gather}\n    \\mathcal{L}=\\mathbb{E}_{\\mathbf{z} \\sim \\text{Enc}(x), c_T, c_G, \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ \\|\\epsilon - \\epsilon_\\theta(\\mathbf{z}_t, t, h(c_T, c_G))\\|^2 \\right].\n\\end{gather}\\begin{gather}\n    \\mathcal{L}=\\mathbb{E}_{\\mathbf{z} \\sim \\text{Enc}(x), c_T, c_G, \\epsilon \\sim \\mathcal{N}(0,1), t} \\left[ \\|\\epsilon - \\epsilon_\\theta(\\mathbf{z}_t, t, h(c_T, c_G))\\|^2 \\right].\n\\end{gather}\n    \\mathcal{L}=\\mathbb{E}_{\\mathbf{z} \\sim \\text{Enc}(x), c_T, c_G, \\epsilon \\sim \\mathcal{N}(0,1), t}\\mathbf{z} \\sim \\text{Enc}(x), c_T, c_G, \\epsilon \\sim \\mathcal{N}(0,1), t \\left[ \\|\\epsilon - \\epsilon_\\theta(\\mathbf{z}_t, t, h(c_T, c_G))\\|^2 \\right].\n\n\\vspace{-6mm}\n\n\n\n\n\n\n\n\n\n\\xhdr{Projector tuning of LLM}Projector tuning of LLM We describe the action node $a$a using text as in Section \\ref{sec:Instruction tuning}. We further introduce graph tokens $X_G$X_G into LLM.  The training objective of LLM is to maximize the probability of generating the correct next states. Combining the discussion in Section \\ref{sec:Instruction tuning}, we train the LLM to yield $f_{\\text{SFT}}$f_{\\text{SFT}}\\text{SFT} via \n\\vspace{-1mm}\n\\begin{equation}\\label{eq:max}\n\\mathcal{L}_{SFT}=-\\sum_{t}logP_{f_{\\text{SFT}}}(y_t|X_G,a,y_1, \\ldots y_{t-1}).\n\\end{equation}\\begin{equation}\\label{eq:max}\n\\mathcal{L}_{SFT}=-\\sum_{t}logP_{f_{\\text{SFT}}}(y_t|X_G,a,y_1, \\ldots y_{t-1}).\n\\end{equation}\\label{eq:max}\n\\mathcal{L}_{SFT}SFT=-\\sum_{t}tlogP_{f_{\\text{SFT}}}f_{\\text{SFT}}\\text{SFT}(y_t|X_G,a,y_1, \\ldots y_{t-1}t-1).\n\nWe use a training approach similar to prefix tuning \\citep{li2021prefix}, where we fix the LLM's parameters and only fine-tune the projector $f_c$f_c's parameters.\n\n\n\n\n\n\n", "appendix": false}, "Experiments": {"content": " \\label{sec:exps}\n\nWe employ \\textbf{one unified GWM model across multiple tasks}, comparing its performance against domain-specific methods. Initially, we introduce the tasks within the GWM framework.\n\n\\xhdr{Task description}Task description \nThe details of the tasks are summarized across three aspects in Table \\ref{tab:dataset_all} of the Appendix, with further information on tasks and datasets available in Appendix \\ref{app:dataset}, and specific action node prompts in Appendix \\ref{sec:prompt}.\n\n\\begin{itemize}[leftmargin=1em, itemsep=0pt]\n\n\\item \\textbf{World prediction}: It contains three subtasks. \\textbf{(1) Multi-modal generation and matching (Multi-modal):} We investigate the node-level multi-modal generation task, where the goal is to predict missing images based on textual captions. We use data from Goodreads \\citep{jin2024instructg2i} and the Multi-Modal-Paper dataset (detailed in Appendix~\\ref{ap:multi-modal generation and matching g}). The generated images are evaluated using CLIP Score \\citep{radford2021learning} and DINOv2 \\citep{oquab2023dinov2}. We compare our approach against several baselines, including Stable Diffusion 1.5 (SD-1.5) \\citep{rombach2022high}, its fine-tuned variant (SD-1.5 FT), the image-to-image model ControlNet \\citep{zhang2023adding}, and the SOTA INSTRUCTG2I model \\citep{jin2024instructg2i}. Meanwhile, our edge-level multi-modal matching task evaluates the correspondence between different modalities, using Contrastive MLP \\citep{liu2022mlp}, CLIP \\citep{radford2021learning}, and fine-tuned CLIP on metrics such as Accuracy, Recall, and F1. %Goodreads serves as a literature graph with nodes representing books' titles and images linked by similar-book semantics, while Multi-Modal-Paper, sourced from arXiv, comprises nodes with images, tables, and text, including captions, structured around paper references. \nPlease note that since the multi-modal matching task of Multi-Modal-Paper also includes matching between text and tables, CLIP cannot be applied to this subtask.   \n\n\\textbf{(2) Recommendation (Rec):} As Table~\\ref{tab:rec_data} of Appendix \\ref{apd:rec} illustrates, we utilize three benchmark datasets of varying scales\u2014Baby, Sports, and Clothing\u2014from Amazon's real-world product collections \\citep{mcauley2015image}. These datasets are commonly used in existing multi-modal graph recommendation systems \\citep{mmgcn, grcn}. For these edge-level tasks, we benchmark our GWM model against recent state-of-the-art recommendation approaches, including FREEDOM \\citep{zhou2023tale}, as well as representative graph-based models such as LightGCN \\citep{he2020lightgcn}, MMGCN \\citep{wei2019mmgcn}, and GRCN \\citep{wei2020graph}. We use Recall and F1 Score as the primary evaluation metrics.  \\textbf{(3) Traditional graph prediction (Graph):}  We utilize Cora \\citep{chen2024exploring}, PubMed \\citep{chen2024exploring}, and HIV \\citep{wu2018moleculenet} datasets. For the Cora and PubMed datasets, we perform node-level and edge-level tasks, while for the HIV dataset, we undertake graph-level tasks. We compare GWM  against two traditional graph baselines, GCN \\citep{kipf2016semi} and GAT \\citep{velivckovic2017graph}, as well as two GFM baselines, LLAGA \\citep{chen2024llaga} and OFA \\citep{liu2023one}. We adopt accuracy as the metric. Details can be seen in Appendix \\ref{apd:graph}.\n\n\\begin{table}[!tbp]\n\\setlength\\tabcolsep{3pt}\n\\centering\n\\begin{footnotesize}\n\\vspace{-4mm}\n\\caption{\\textbf{Multi-modal generation results on Goodreads and Multi-Modal-Paper}. This task is to predict the missing modality based on the given modality. Compared to specific baselines in image generation, GWM achieved the best results.} %\n\\label{tab:multi-modal generation}\n\\resizebox{0.43\\textwidth}{!}{\n\\begin{tabular}{ccccc}\n\\toprule\n& \\multicolumn{2}{c}{\\texttt{Goodreads}} & \\multicolumn{2}{c}{\\texttt{Multi-Modal-Paper}}  \\\\ \\cmidrule(lr){2-3} \\cmidrule(lr){4-5}\n\\textbf{Model} & CLIP   & DINOv2   & CLIP   & DINOv2    \\\\ \\midrule\nSD-1.5 & 42.16 &14.84  & 52.62 & 23.64\\\\\nSD-1.5 FT  &\\underline{45.81} &18.97  & 58.49  & 24.13 \\\\ \nControlNet  &42.20  &19.77  & 52.89 & \\underline{24.77}\\\\ \\midrule\nINSTRUCTG2I & \\textbf{50.37} & \\textbf{25.54} & 56.37 & 18.80 \\\\ \\midrule\nGWM-T &  47.46 & 20.91 & \\textbf{59.92} & 23.10 \\\\\nGWM-E & 45.23 & \\underline{20.87} & \\underline{59.84}  & \\textbf{26.03}  \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{footnotesize}\n\\end{table}\n\n\\begin{table}[!tbp]\n\\setlength\\tabcolsep{3pt}\n\\centering\n\\begin{footnotesize}\n% \\vspace{-1mm}\n\\caption{\\textbf{Multi-modal matching results on Goodreads and Multi-Modal-Paper}. It aims to predict the correspondence between different modal-ities. For Goodreads, this task is to predict text-image correspondences. As for Multi-Modal-Paper, it aims to predict text-image, text-table, and table-image correspondences. Note that CLIP and CLIP FT cannot be applied to Multi-Modal-Paper since it includes matching tasks beyond text-image.} %\n\\label{tab:Multi-modal matching}\n\\resizebox{0.49\\textwidth}{!}{\n\\begin{tabular}{ccccccc}\n\\toprule\n & \\multicolumn{3}{c}{\\texttt{Goodreads}} & \\multicolumn{3}{c}{\\texttt{Multi-Modal-Paper}}  \\\\ \\cmidrule(lr){2-4} \\cmidrule(lr){5-7}\n\\textbf{Model} & Accuracy  & Recall  & F1 Score & Accuracy  & Recall  & F1 Score \\\\ \\midrule\nContrastive MLP &  54.70 & 54.67 & 54.79& 51.77 & 51.55 & 50.31\\\\ \\midrule\nCLIP & 83.80 & 83.80 & 83.84 & - & -& - \\\\\nCLIP FT   & \\textbf{92.60} & \\textbf{92.58} & \\textbf{92.61} & -& - & -   \\\\ \\midrule\nGWM-T   &84.22  &85.66  &85.29  & 88.26   &90.35   &90.11  \\\\\nGWM-E &\\underline{88.82}  &\\underline{89.73}  &\\underline{89.06}  & \\textbf{96.23}   & \\textbf{97.21}  & \\textbf{97.13}   \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{footnotesize}\n% \\vspace{-4mm}\n\\end{table}\n\n\\item \\textbf{World generation}: It contains two sub-tasks. \\textbf{(1) Multi-agent collaboration (Multi-agent):} We utilize a multi-modal agent benchmark called AgentClinic \\citep{schmidgall2024agentclinic} (in Appendix \\ref{apd:agent}) to evaluate LLMs within simulated clinical environments. This environment is structured as a graph, with nodes representing different profile-based agents such as patients, measurements, and moderators, and containing various modalities of external knowledge including medical images and patient records. The edges represent interactions between agents and their engagement with knowledge resources. Given the objective of integrating information from all agents to answer medical questions, we define this as a graph-level task. We compare our approach with three LLM-based baselines: CoT \\citep{wei2022chain}, ToT \\citep{yao2024tree}, and Few-Shot \\citep{madotto2021few}, as well as two additional baselines fine-tuned on the AgentClinic dataset. FT refers to a LLaMA-3-8B model fine-tuned directly on the task. Longformer \\citep{beltagy2020longformer} is a strong baseline for long-document understanding. We use Accuracy, Recall, and F1 Score as evaluation metrics to assess the correctness of the generated responses.  \\textbf{(2) Retrieval-augmented generation (RAG):} We utilize LongBench v2 \\citep{bai2024longbench}, a benchmark designed for challenging long-context question-answering (in Appendix \\ref{apd:RAG}). %This benchmark consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories. The questions are categorized into easy and hard levels, determined by the difficulty experienced by human experts and models in answering them. \nFollowing previous work like GraphRAG \\citep{edge2024local}, we divide long context into chunks as nodes of the graph, and the edges between nodes are the similarity of their BERT embeddings. We conduct comparisons with two RAG-based baselines\u2014BM25 \\citep{robertson2009probabilistic} and Dragon \\citep{lin2023train}\u2014and three long-context LLMs ($128k$), including Mistral Large 2, Command R+, and GPT-4o mini. Accuracy serves as our evaluation metric.\n\n\\item \\textbf{World optimization (Optimization)}:  Many existing works \\citep{ho2016generative,hussein2017imitation,yang2024embodied} attempt optimization tasks by imitating the trajectory of expert strategies. Here, we utilize the expert strategy dataset from the text-based embodied task ALFWorld \\citep{shridhar2020alfworld,yang2024embodied}. %It includes descriptions of the expert's strategy state at each decision step (comprising both images and text) as well as the decisions made (text). The action node of GWM is to prompt GWM to predict the next expert decision. to assess the ability to imitate expert strategies \nWe model each decision state as graph nodes and derive the edges between nodes based on the similarity of the state images associated with the decisions.  We compare GWM with three LLM baselines\u2014COT \\citep{wei2022chain}, TOT \\citep{yao2024tree}, and T5 \\citep{raffel2020exploring} fine-tuned on our dataset (T5 FT) \u2014using BERT-Score \\citep{zhang2019bertscore} (Precision, Recall, and F1 Score) as metrics. Details can be seen in Appendix \\ref{apd:opt}. \n\n\n\n\\end{itemize}\\begin{itemize}[leftmargin=1em, itemsep=0pt]\n\n\\item \\textbf{World prediction}: It contains three subtasks. \\textbf{(1) Multi-modal generation and matching (Multi-modal):} We investigate the node-level multi-modal generation task, where the goal is to predict missing images based on textual captions. We use data from Goodreads \\citep{jin2024instructg2i} and the Multi-Modal-Paper dataset (detailed in Appendix~\\ref{ap:multi-modal generation and matching g}). The generated images are evaluated using CLIP Score \\citep{radford2021learning} and DINOv2 \\citep{oquab2023dinov2}. We compare our approach against several baselines, including Stable Diffusion 1.5 (SD-1.5) \\citep{rombach2022high}, its fine-tuned variant (SD-1.5 FT), the image-to-image model ControlNet \\citep{zhang2023adding}, and the SOTA INSTRUCTG2I model \\citep{jin2024instructg2i}. Meanwhile, our edge-level multi-modal matching task evaluates the correspondence between different modalities, using Contrastive MLP \\citep{liu2022mlp}, CLIP \\citep{radford2021learning}, and fine-tuned CLIP on metrics such as Accuracy, Recall, and F1. %Goodreads serves as a literature graph with nodes representing books' titles and images linked by similar-book semantics, while Multi-Modal-Paper, sourced from arXiv, comprises nodes with images, tables, and text, including captions, structured around paper references. \nPlease note that since the multi-modal matching task of Multi-Modal-Paper also includes matching between text and tables, CLIP cannot be applied to this subtask.   \n\n\\textbf{(2) Recommendation (Rec):} As Table~\\ref{tab:rec_data} of Appendix \\ref{apd:rec} illustrates, we utilize three benchmark datasets of varying scales\u2014Baby, Sports, and Clothing\u2014from Amazon's real-world product collections \\citep{mcauley2015image}. These datasets are commonly used in existing multi-modal graph recommendation systems \\citep{mmgcn, grcn}. For these edge-level tasks, we benchmark our GWM model against recent state-of-the-art recommendation approaches, including FREEDOM \\citep{zhou2023tale}, as well as representative graph-based models such as LightGCN \\citep{he2020lightgcn}, MMGCN \\citep{wei2019mmgcn}, and GRCN \\citep{wei2020graph}. We use Recall and F1 Score as the primary evaluation metrics.  \\textbf{(3) Traditional graph prediction (Graph):}  We utilize Cora \\citep{chen2024exploring}, PubMed \\citep{chen2024exploring}, and HIV \\citep{wu2018moleculenet} datasets. For the Cora and PubMed datasets, we perform node-level and edge-level tasks, while for the HIV dataset, we undertake graph-level tasks. We compare GWM  against two traditional graph baselines, GCN \\citep{kipf2016semi} and GAT \\citep{velivckovic2017graph}, as well as two GFM baselines, LLAGA \\citep{chen2024llaga} and OFA \\citep{liu2023one}. We adopt accuracy as the metric. Details can be seen in Appendix \\ref{apd:graph}.\n\n\\begin{table}[!tbp]\n\\setlength\\tabcolsep{3pt}\n\\centering\n\\begin{footnotesize}\n\\vspace{-4mm}\n\\caption{\\textbf{Multi-modal generation results on Goodreads and Multi-Modal-Paper}. This task is to predict the missing modality based on the given modality. Compared to specific baselines in image generation, GWM achieved the best results.} %\n\\label{tab:multi-modal generation}\n\\resizebox{0.43\\textwidth}{!}{\n\\begin{tabular}{ccccc}\n\\toprule\n& \\multicolumn{2}{c}{\\texttt{Goodreads}} & \\multicolumn{2}{c}{\\texttt{Multi-Modal-Paper}}  \\\\ \\cmidrule(lr){2-3} \\cmidrule(lr){4-5}\n\\textbf{Model} & CLIP   & DINOv2   & CLIP   & DINOv2    \\\\ \\midrule\nSD-1.5 & 42.16 &14.84  & 52.62 & 23.64\\\\\nSD-1.5 FT  &\\underline{45.81} &18.97  & 58.49  & 24.13 \\\\ \nControlNet  &42.20  &19.77  & 52.89 & \\underline{24.77}\\\\ \\midrule\nINSTRUCTG2I & \\textbf{50.37} & \\textbf{25.54} & 56.37 & 18.80 \\\\ \\midrule\nGWM-T &  47.46 & 20.91 & \\textbf{59.92} & 23.10 \\\\\nGWM-E & 45.23 & \\underline{20.87} & \\underline{59.84}  & \\textbf{26.03}  \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{footnotesize}\n\\end{table}\n\n\\begin{table}[!tbp]\n\\setlength\\tabcolsep{3pt}\n\\centering\n\\begin{footnotesize}\n% \\vspace{-1mm}\n\\caption{\\textbf{Multi-modal matching results on Goodreads and Multi-Modal-Paper}. It aims to predict the correspondence between different modal-ities. For Goodreads, this task is to predict text-image correspondences. As for Multi-Modal-Paper, it aims to predict text-image, text-table, and table-image correspondences. Note that CLIP and CLIP FT cannot be applied to Multi-Modal-Paper since it includes matching tasks beyond text-image.} %\n\\label{tab:Multi-modal matching}\n\\resizebox{0.49\\textwidth}{!}{\n\\begin{tabular}{ccccccc}\n\\toprule\n & \\multicolumn{3}{c}{\\texttt{Goodreads}} & \\multicolumn{3}{c}{\\texttt{Multi-Modal-Paper}}  \\\\ \\cmidrule(lr){2-4} \\cmidrule(lr){5-7}\n\\textbf{Model} & Accuracy  & Recall  & F1 Score & Accuracy  & Recall  & F1 Score \\\\ \\midrule\nContrastive MLP &  54.70 & 54.67 & 54.79& 51.77 & 51.55 & 50.31\\\\ \\midrule\nCLIP & 83.80 & 83.80 & 83.84 & - & -& - \\\\\nCLIP FT   & \\textbf{92.60} & \\textbf{92.58} & \\textbf{92.61} & -& - & -   \\\\ \\midrule\nGWM-T   &84.22  &85.66  &85.29  & 88.26   &90.35   &90.11  \\\\\nGWM-E &\\underline{88.82}  &\\underline{89.73}  &\\underline{89.06}  & \\textbf{96.23}   & \\textbf{97.21}  & \\textbf{97.13}   \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{footnotesize}\n% \\vspace{-4mm}\n\\end{table}\n\n\\item \\textbf{World generation}: It contains two sub-tasks. \\textbf{(1) Multi-agent collaboration (Multi-agent):} We utilize a multi-modal agent benchmark called AgentClinic \\citep{schmidgall2024agentclinic} (in Appendix \\ref{apd:agent}) to evaluate LLMs within simulated clinical environments. This environment is structured as a graph, with nodes representing different profile-based agents such as patients, measurements, and moderators, and containing various modalities of external knowledge including medical images and patient records. The edges represent interactions between agents and their engagement with knowledge resources. Given the objective of integrating information from all agents to answer medical questions, we define this as a graph-level task. We compare our approach with three LLM-based baselines: CoT \\citep{wei2022chain}, ToT \\citep{yao2024tree}, and Few-Shot \\citep{madotto2021few}, as well as two additional baselines fine-tuned on the AgentClinic dataset. FT refers to a LLaMA-3-8B model fine-tuned directly on the task. Longformer \\citep{beltagy2020longformer} is a strong baseline for long-document understanding. We use Accuracy, Recall, and F1 Score as evaluation metrics to assess the correctness of the generated responses.  \\textbf{(2) Retrieval-augmented generation (RAG):} We utilize LongBench v2 \\citep{bai2024longbench}, a benchmark designed for challenging long-context question-answering (in Appendix \\ref{apd:RAG}). %This benchmark consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories. The questions are categorized into easy and hard levels, determined by the difficulty experienced by human experts and models in answering them. \nFollowing previous work like GraphRAG \\citep{edge2024local}, we divide long context into chunks as nodes of the graph, and the edges between nodes are the similarity of their BERT embeddings. We conduct comparisons with two RAG-based baselines\u2014BM25 \\citep{robertson2009probabilistic} and Dragon \\citep{lin2023train}\u2014and three long-context LLMs ($128k$), including Mistral Large 2, Command R+, and GPT-4o mini. Accuracy serves as our evaluation metric.\n\n\\item \\textbf{World optimization (Optimization)}:  Many existing works \\citep{ho2016generative,hussein2017imitation,yang2024embodied} attempt optimization tasks by imitating the trajectory of expert strategies. Here, we utilize the expert strategy dataset from the text-based embodied task ALFWorld \\citep{shridhar2020alfworld,yang2024embodied}. %It includes descriptions of the expert's strategy state at each decision step (comprising both images and text) as well as the decisions made (text). The action node of GWM is to prompt GWM to predict the next expert decision. to assess the ability to imitate expert strategies \nWe model each decision state as graph nodes and derive the edges between nodes based on the similarity of the state images associated with the decisions.  We compare GWM with three LLM baselines\u2014COT \\citep{wei2022chain}, TOT \\citep{yao2024tree}, and T5 \\citep{raffel2020exploring} fine-tuned on our dataset (T5 FT) \u2014using BERT-Score \\citep{zhang2019bertscore} (Precision, Recall, and F1 Score) as metrics. Details can be seen in Appendix \\ref{apd:opt}. \n\n\n\n\\end{itemize}\n\n\\item \\textbf{World prediction}: It contains three subtasks. \\textbf{(1) Multi-modal generation and matching (Multi-modal):} We investigate the node-level multi-modal generation task, where the goal is to predict missing images based on textual captions. We use data from Goodreads \\citep{jin2024instructg2i} and the Multi-Modal-Paper dataset (detailed in Appendix~\\ref{ap:multi-modal generation and matching g}). The generated images are evaluated using CLIP Score \\citep{radford2021learning} and DINOv2 \\citep{oquab2023dinov2}. We compare our approach against several baselines, including Stable Diffusion 1.5 (SD-1.5) \\citep{rombach2022high}, its fine-tuned variant (SD-1.5 FT), the image-to-image model ControlNet \\citep{zhang2023adding}, and the SOTA INSTRUCTG2I model \\citep{jin2024instructg2i}. Meanwhile, our edge-level multi-modal matching task evaluates the correspondence between different modalities, using Contrastive MLP \\citep{liu2022mlp}, CLIP \\citep{radford2021learning}, and fine-tuned CLIP on metrics such as Accuracy, Recall, and F1. Please note that since the multi-modal matching task of Multi-Modal-Paper also includes matching between text and tables, CLIP cannot be applied to this subtask.   \n\n\\textbf{(2) Recommendation (Rec):} As Table~\\ref{tab:rec_data} of Appendix \\ref{apd:rec} illustrates, we utilize three benchmark datasets of varying scales\u2014Baby, Sports, and Clothing\u2014from Amazon's real-world product collections \\citep{mcauley2015image}. These datasets are commonly used in existing multi-modal graph recommendation systems \\citep{mmgcn, grcn}. For these edge-level tasks, we benchmark our GWM model against recent state-of-the-art recommendation approaches, including FREEDOM \\citep{zhou2023tale}, as well as representative graph-based models such as LightGCN \\citep{he2020lightgcn}, MMGCN \\citep{wei2019mmgcn}, and GRCN \\citep{wei2020graph}. We use Recall and F1 Score as the primary evaluation metrics.  \\textbf{(3) Traditional graph prediction (Graph):}  We utilize Cora \\citep{chen2024exploring}, PubMed \\citep{chen2024exploring}, and HIV \\citep{wu2018moleculenet} datasets. For the Cora and PubMed datasets, we perform node-level and edge-level tasks, while for the HIV dataset, we undertake graph-level tasks. We compare GWM  against two traditional graph baselines, GCN \\citep{kipf2016semi} and GAT \\citep{velivckovic2017graph}, as well as two GFM baselines, LLAGA \\citep{chen2024llaga} and OFA \\citep{liu2023one}. We adopt accuracy as the metric. Details can be seen in Appendix \\ref{apd:graph}.\n\n\n\\setlength\\tabcolsep{3pt}\n\\centering\n\n\\vspace{-4mm}\n\\caption{\\textbf{Multi-modal generation results on Goodreads and Multi-Modal-Paper}. This task is to predict the missing modality based on the given modality. Compared to specific baselines in image generation, GWM achieved the best results.} \\label{tab:multi-modal generation}\n\\resizebox{0.43\\textwidth}0.43\\textwidth{!}!{\n\\begin{tabular}{ccccc}\n\\toprule\n& \\multicolumn{2}{c}{\\texttt{Goodreads}} & \\multicolumn{2}{c}{\\texttt{Multi-Modal-Paper}}  \\\\ \\cmidrule(lr){2-3} \\cmidrule(lr){4-5}\n\\textbf{Model} & CLIP   & DINOv2   & CLIP   & DINOv2    \\\\ \\midrule\nSD-1.5 & 42.16 &14.84  & 52.62 & 23.64\\\\\nSD-1.5 FT  &\\underline{45.81} &18.97  & 58.49  & 24.13 \\\\ \nControlNet  &42.20  &19.77  & 52.89 & \\underline{24.77}\\\\ \\midrule\nINSTRUCTG2I & \\textbf{50.37} & \\textbf{25.54} & 56.37 & 18.80 \\\\ \\midrule\nGWM-T &  47.46 & 20.91 & \\textbf{59.92} & 23.10 \\\\\nGWM-E & 45.23 & \\underline{20.87} & \\underline{59.84}  & \\textbf{26.03}  \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\toprule\n& \\multicolumn{2}2{c}c{\\texttt{Goodreads}}\\texttt{Goodreads} & \\multicolumn{2}2{c}c{\\texttt{Multi-Modal-Paper}}\\texttt{Multi-Modal-Paper}  \\\\ \\cmidrule(lr){2-3}2-3 \\cmidrule(lr){4-5}4-5\n\\textbf{Model} & CLIP   & DINOv2   & CLIP   & DINOv2    \\\\ \\midrule\nSD-1.5 & 42.16 &14.84  & 52.62 & 23.64\\\\\nSD-1.5 FT  &\\underline{45.81} &18.97  & 58.49  & 24.13 \\\\ \nControlNet  &42.20  &19.77  & 52.89 & \\underline{24.77}\\\\ \\midrule\nINSTRUCTG2I & \\textbf{50.37} & \\textbf{25.54} & 56.37 & 18.80 \\\\ \\midrule\nGWM-T &  47.46 & 20.91 & \\textbf{59.92} & 23.10 \\\\\nGWM-E & 45.23 & \\underline{20.87} & \\underline{59.84}  & \\textbf{26.03}  \\\\\n\\bottomrule\n\n\n\n\n\n\n\\setlength\\tabcolsep{3pt}\n\\centering\n\n\\caption{\\textbf{Multi-modal matching results on Goodreads and Multi-Modal-Paper}. It aims to predict the correspondence between different modal-ities. For Goodreads, this task is to predict text-image correspondences. As for Multi-Modal-Paper, it aims to predict text-image, text-table, and table-image correspondences. Note that CLIP and CLIP FT cannot be applied to Multi-Modal-Paper since it includes matching tasks beyond text-image.} \\label{tab:Multi-modal matching}\n\\resizebox{0.49\\textwidth}0.49\\textwidth{!}!{\n\\begin{tabular}{ccccccc}\n\\toprule\n & \\multicolumn{3}{c}{\\texttt{Goodreads}} & \\multicolumn{3}{c}{\\texttt{Multi-Modal-Paper}}  \\\\ \\cmidrule(lr){2-4} \\cmidrule(lr){5-7}\n\\textbf{Model} & Accuracy  & Recall  & F1 Score & Accuracy  & Recall  & F1 Score \\\\ \\midrule\nContrastive MLP &  54.70 & 54.67 & 54.79& 51.77 & 51.55 & 50.31\\\\ \\midrule\nCLIP & 83.80 & 83.80 & 83.84 & - & -& - \\\\\nCLIP FT   & \\textbf{92.60} & \\textbf{92.58} & \\textbf{92.61} & -& - & -   \\\\ \\midrule\nGWM-T   &84.22  &85.66  &85.29  & 88.26   &90.35   &90.11  \\\\\nGWM-E &\\underline{88.82}  &\\underline{89.73}  &\\underline{89.06}  & \\textbf{96.23}   & \\textbf{97.21}  & \\textbf{97.13}   \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\toprule\n & \\multicolumn{3}3{c}c{\\texttt{Goodreads}}\\texttt{Goodreads} & \\multicolumn{3}3{c}c{\\texttt{Multi-Modal-Paper}}\\texttt{Multi-Modal-Paper}  \\\\ \\cmidrule(lr){2-4}2-4 \\cmidrule(lr){5-7}5-7\n\\textbf{Model} & Accuracy  & Recall  & F1 Score & Accuracy  & Recall  & F1 Score \\\\ \\midrule\nContrastive MLP &  54.70 & 54.67 & 54.79& 51.77 & 51.55 & 50.31\\\\ \\midrule\nCLIP & 83.80 & 83.80 & 83.84 & - & -& - \\\\\nCLIP FT   & \\textbf{92.60} & \\textbf{92.58} & \\textbf{92.61} & -& - & -   \\\\ \\midrule\nGWM-T   &84.22  &85.66  &85.29  & 88.26   &90.35   &90.11  \\\\\nGWM-E &\\underline{88.82}  &\\underline{89.73}  &\\underline{89.06}  & \\textbf{96.23}   & \\textbf{97.21}  & \\textbf{97.13}   \\\\\n\\bottomrule\n\n\n\n\n\n\\item \\textbf{World generation}: It contains two sub-tasks. \\textbf{(1) Multi-agent collaboration (Multi-agent):} We utilize a multi-modal agent benchmark called AgentClinic \\citep{schmidgall2024agentclinic} (in Appendix \\ref{apd:agent}) to evaluate LLMs within simulated clinical environments. This environment is structured as a graph, with nodes representing different profile-based agents such as patients, measurements, and moderators, and containing various modalities of external knowledge including medical images and patient records. The edges represent interactions between agents and their engagement with knowledge resources. Given the objective of integrating information from all agents to answer medical questions, we define this as a graph-level task. We compare our approach with three LLM-based baselines: CoT \\citep{wei2022chain}, ToT \\citep{yao2024tree}, and Few-Shot \\citep{madotto2021few}, as well as two additional baselines fine-tuned on the AgentClinic dataset. FT refers to a LLaMA-3-8B model fine-tuned directly on the task. Longformer \\citep{beltagy2020longformer} is a strong baseline for long-document understanding. We use Accuracy, Recall, and F1 Score as evaluation metrics to assess the correctness of the generated responses.  \\textbf{(2) Retrieval-augmented generation (RAG):} We utilize LongBench v2 \\citep{bai2024longbench}, a benchmark designed for challenging long-context question-answering (in Appendix \\ref{apd:RAG}). Following previous work like GraphRAG \\citep{edge2024local}, we divide long context into chunks as nodes of the graph, and the edges between nodes are the similarity of their BERT embeddings. We conduct comparisons with two RAG-based baselines\u2014BM25 \\citep{robertson2009probabilistic} and Dragon \\citep{lin2023train}\u2014and three long-context LLMs ($128k$128k), including Mistral Large 2, Command R+, and GPT-4o mini. Accuracy serves as our evaluation metric.\n\n\\item \\textbf{World optimization (Optimization)}:  Many existing works \\citep{ho2016generative,hussein2017imitation,yang2024embodied} attempt optimization tasks by imitating the trajectory of expert strategies. Here, we utilize the expert strategy dataset from the text-based embodied task ALFWorld \\citep{shridhar2020alfworld,yang2024embodied}. We model each decision state as graph nodes and derive the edges between nodes based on the similarity of the state images associated with the decisions.  We compare GWM with three LLM baselines\u2014COT \\citep{wei2022chain}, TOT \\citep{yao2024tree}, and T5 \\citep{raffel2020exploring} fine-tuned on our dataset (T5 FT) \u2014using BERT-Score \\citep{zhang2019bertscore} (Precision, Recall, and F1 Score) as metrics. Details can be seen in Appendix \\ref{apd:opt}. \n\n\n\n\n\n\n\\begin{table}[!tbp]\n\\setlength\\tabcolsep{3pt}\n\\centering\n\\begin{footnotesize}\n\\vspace{-1mm}\n\\caption{\\textbf{Recommendation on Baby, Sports, and Clothing}. Compared with three classical graph baselines, GWM achieved state-of-the-art results on most metrics.} %\n\\label{tab:Recommender systems}\n\\resizebox{0.48\\textwidth}{!}{\n\\begin{tabular}{cccccccccc}\n\\toprule\n& \\multicolumn{2}{c}{\\texttt{Baby}} & \\multicolumn{2}{c}{\\texttt{Sports}} & \\multicolumn{2}{c}{\\texttt{Clothing}} \\\\ \\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-7}\n\\textbf{Model}   & Recall  & F1 Score   & Recall  & F1 Score & Recall  & F1 Score \\\\ \\midrule\nFREEDOM & 60.35 & 66.16 & 63.47 & 70.53 & 70.20 & \\underline{78.40} \\\\\n\\midrule\nLightGCN & 51.11 & 38.22 & \\underline{85.36} & \\textbf{91.32} & 69.08 & 77.21 \\\\\nMMGCN & 57.34 & 61.31 & 61.69 & 68.08 & 64.09 & 71.26 \\\\\nGRCN & \\underline{74.35} & \\underline{82.47} & 57.31 & 61.23 & 57.60 & 61.74 \\\\ \\midrule\nGWM-T &  70.84 & 75.08  & 84.29 & 88.60   & \\underline{71.73} & 74.26    \\\\\nGWM-E  & \\textbf{76.72} & \\textbf{84.74}  & \\textbf{88.78} & \\underline{90.32}   & \\textbf{75.27} & \\textbf{84.06}  \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{footnotesize}\n\\end{table}\n\\setlength\\tabcolsep{3pt}\n\\centering\n\n\\vspace{-1mm}\n\\caption{\\textbf{Recommendation on Baby, Sports, and Clothing}. Compared with three classical graph baselines, GWM achieved state-of-the-art results on most metrics.} \\label{tab:Recommender systems}\n\\resizebox{0.48\\textwidth}0.48\\textwidth{!}!{\n\\begin{tabular}{cccccccccc}\n\\toprule\n& \\multicolumn{2}{c}{\\texttt{Baby}} & \\multicolumn{2}{c}{\\texttt{Sports}} & \\multicolumn{2}{c}{\\texttt{Clothing}} \\\\ \\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-7}\n\\textbf{Model}   & Recall  & F1 Score   & Recall  & F1 Score & Recall  & F1 Score \\\\ \\midrule\nFREEDOM & 60.35 & 66.16 & 63.47 & 70.53 & 70.20 & \\underline{78.40} \\\\\n\\midrule\nLightGCN & 51.11 & 38.22 & \\underline{85.36} & \\textbf{91.32} & 69.08 & 77.21 \\\\\nMMGCN & 57.34 & 61.31 & 61.69 & 68.08 & 64.09 & 71.26 \\\\\nGRCN & \\underline{74.35} & \\underline{82.47} & 57.31 & 61.23 & 57.60 & 61.74 \\\\ \\midrule\nGWM-T &  70.84 & 75.08  & 84.29 & 88.60   & \\underline{71.73} & 74.26    \\\\\nGWM-E  & \\textbf{76.72} & \\textbf{84.74}  & \\textbf{88.78} & \\underline{90.32}   & \\textbf{75.27} & \\textbf{84.06}  \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\toprule\n& \\multicolumn{2}2{c}c{\\texttt{Baby}}\\texttt{Baby} & \\multicolumn{2}2{c}c{\\texttt{Sports}}\\texttt{Sports} & \\multicolumn{2}2{c}c{\\texttt{Clothing}}\\texttt{Clothing} \\\\ \\cmidrule(lr){2-3}2-3 \\cmidrule(lr){4-5}4-5 \\cmidrule(lr){6-7}6-7\n\\textbf{Model}   & Recall  & F1 Score   & Recall  & F1 Score & Recall  & F1 Score \\\\ \\midrule\nFREEDOM & 60.35 & 66.16 & 63.47 & 70.53 & 70.20 & \\underline{78.40} \\\\\n\\midrule\nLightGCN & 51.11 & 38.22 & \\underline{85.36} & \\textbf{91.32} & 69.08 & 77.21 \\\\\nMMGCN & 57.34 & 61.31 & 61.69 & 68.08 & 64.09 & 71.26 \\\\\nGRCN & \\underline{74.35} & \\underline{82.47} & 57.31 & 61.23 & 57.60 & 61.74 \\\\ \\midrule\nGWM-T &  70.84 & 75.08  & 84.29 & 88.60   & \\underline{71.73} & 74.26    \\\\\nGWM-E  & \\textbf{76.72} & \\textbf{84.74}  & \\textbf{88.78} & \\underline{90.32}   & \\textbf{75.27} & \\textbf{84.06}  \\\\\n\\bottomrule\n\n\n\n\n\n\n\\begin{table}[!tbp]\n\\setlength\\tabcolsep{5pt} % \u8bbe\u7f6e\u5217\u95f4\u8ddd\n\\centering\n\\begin{footnotesize}\n% \\vspace{-1mm}\n\\caption{\\textbf{Traditional graph prediction results on Cora, PubMed, and HIV}. It covers representative tasks at the node-level, edge-level, and graph-level. Compared to classic graph baselines and GFM methods, our GWM can match their performance with one unified model.}%It covers representative tasks at the node-level, edge-level, and graph-level. Compared to classic graph baselines and GFM methods, our GWM can match their performance with one unified model. \n\\label{tab:Traditional graph prediction}\n\\resizebox{0.4\\textwidth}{!}{\n\\begin{tabular}{lccccccc}\n\\toprule\n\\textbf{Model} & \\multicolumn{2}{c}{\\texttt{Cora}} & \\multicolumn{2}{c}{\\texttt{PubMed}} & \\multicolumn{1}{c}{\\texttt{HIV}} \\\\ \\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-6}\nTask Type & Node   & Link  & Node   & Link  & Graph  \\\\\\midrule\n\nGCN & 78.86  & 90.40  &74.49  &91.10  &86.72  \\\\\nGAT & 82.76  & \\underline{93.70}  &75.24  &91.20  &87.84   \\\\ \\midrule\nLLAGA &\\textbf{89.22}   & 89.18   &\\textbf{95.03}   &89.18   & 85.42  \\\\ \nOFA &73.21  & 93.12  &77.80  &\\textbf{96.39}  &92.04  \\\\ \\midrule\nGWM-T &81.92  &88.24  &\\underline{92.91}  &91.88  &\\underline{92.20}  \\\\\nGWM-E &\\underline{83.03}  & \\textbf{94.31}  & 84.22  &\\underline{94.01}  &\\textbf{93.86}  \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{footnotesize}\n% \\vspace{-6mm}\n\\end{table}\n\\setlength\\tabcolsep{5pt} \\centering\n\n\\caption{\\textbf{Traditional graph prediction results on Cora, PubMed, and HIV}. It covers representative tasks at the node-level, edge-level, and graph-level. Compared to classic graph baselines and GFM methods, our GWM can match their performance with one unified model.}\\label{tab:Traditional graph prediction}\n\\resizebox{0.4\\textwidth}0.4\\textwidth{!}!{\n\\begin{tabular}{lccccccc}\n\\toprule\n\\textbf{Model} & \\multicolumn{2}{c}{\\texttt{Cora}} & \\multicolumn{2}{c}{\\texttt{PubMed}} & \\multicolumn{1}{c}{\\texttt{HIV}} \\\\ \\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-6}\nTask Type & Node   & Link  & Node   & Link  & Graph  \\\\\\midrule\n\nGCN & 78.86  & 90.40  &74.49  &91.10  &86.72  \\\\\nGAT & 82.76  & \\underline{93.70}  &75.24  &91.20  &87.84   \\\\ \\midrule\nLLAGA &\\textbf{89.22}   & 89.18   &\\textbf{95.03}   &89.18   & 85.42  \\\\ \nOFA &73.21  & 93.12  &77.80  &\\textbf{96.39}  &92.04  \\\\ \\midrule\nGWM-T &81.92  &88.24  &\\underline{92.91}  &91.88  &\\underline{92.20}  \\\\\nGWM-E &\\underline{83.03}  & \\textbf{94.31}  & 84.22  &\\underline{94.01}  &\\textbf{93.86}  \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\toprule\n\\textbf{Model} & \\multicolumn{2}2{c}c{\\texttt{Cora}}\\texttt{Cora} & \\multicolumn{2}2{c}c{\\texttt{PubMed}}\\texttt{PubMed} & \\multicolumn{1}1{c}c{\\texttt{HIV}}\\texttt{HIV} \\\\ \\cmidrule(lr){2-3}2-3 \\cmidrule(lr){4-5}4-5 \\cmidrule(lr){6-6}6-6\nTask Type & Node   & Link  & Node   & Link  & Graph  \\\\\\midrule\n\nGCN & 78.86  & 90.40  &74.49  &91.10  &86.72  \\\\\nGAT & 82.76  & \\underline{93.70}  &75.24  &91.20  &87.84   \\\\ \\midrule\nLLAGA &\\textbf{89.22}   & 89.18   &\\textbf{95.03}   &89.18   & 85.42  \\\\ \nOFA &73.21  & 93.12  &77.80  &\\textbf{96.39}  &92.04  \\\\ \\midrule\nGWM-T &81.92  &88.24  &\\underline{92.91}  &91.88  &\\underline{92.20}  \\\\\nGWM-E &\\underline{83.03}  & \\textbf{94.31}  & 84.22  &\\underline{94.01}  &\\textbf{93.86}  \\\\\n\\bottomrule\n\n\n\n  \n\n\n\n\n\\xhdr{Implementation details}Implementation details We train and test \\textit{a single GWM} on all tasks, comparing it with domain-specific baselines for each task. Specifically, for the LLM module, we uniformly use Llama-3-8B, and for stable diffusion, we use SD-v1-5. For the image-to-text model used in GWM-T, we use LLaVA-1.5-7B. The image encoder and text decoder used in GWM-E are CLIP and BERT models, respectively. In addition, our multi-hop projector uses an n-hop MLP to aggregate features from different hops, where n-hop refers to the number of neighborhood hops of the graph nodes used. To ensure the training efficiency of the models, we set the maximum token length for all models at 2k. We use Adam optimizer \\citep{diederik2014adam} for model training and gradually decay the learning rate with LambdaLR scheduler. All the experiments are conducted on NVIDIA A6000 GPUs. Please refer to Appendix \\ref{sec:hyper} for other implementation details.\n\n\n\\begin{table}[ht]\n\\setlength\\tabcolsep{3pt}\n\\centering\n\\begin{footnotesize}\n% \\vspace{-6mm}\n\\caption{\\textbf{Multi-agent collaboration results on AgentClinic}. This task is to answer medical questions by leveraging interactions between agents and external knowledge sources. Compared to classic LLM baselines, GWM-T achieves state-of-the-art results.} %This task is to answer medical questions by leveraging interactions between agents and external knowledge sources. Compared to classic LLM baselines, GWM-T achieves state-of-the-art results.\n\\label{tab:Multi-agent collaboration}\n\\resizebox{0.35\\textwidth}{!}{\n\\begin{tabular}{cccc}\n\\toprule\n\\textbf{Model} & Accuracy  & Recall  & F1 Score \\\\ \\midrule\nCOT & \\underline{45.00} & 33.42 & 32.25 \\\\\nTOT &  35.00  & 33.71 & 29.71 \\\\ \nFew-shots & 40.00 & 40.63 & 29.37 \\\\\\midrule\nLongformer & 25.00 & 20.20 & 14.00 \\\\ \nFT & \\underline{45.00} & \\underline{45.40} & \\underline{44.00} \\\\ \n\\midrule\nGWM-T & \\textbf{50.00}   &\\textbf{46.42}  &\\textbf{48.20}  \\\\\nGWM-E &\\underline{45.00}  &39.57  & 35.56  \\\\\n\\bottomrule\n\\end{tabular}\n}\n%\\vspace{-3.5mm}\n\\end{footnotesize}\n\\end{table}\n\\setlength\\tabcolsep{3pt}\n\\centering\n\n\\caption{\\textbf{Multi-agent collaboration results on AgentClinic}. This task is to answer medical questions by leveraging interactions between agents and external knowledge sources. Compared to classic LLM baselines, GWM-T achieves state-of-the-art results.} \\label{tab:Multi-agent collaboration}\n\\resizebox{0.35\\textwidth}0.35\\textwidth{!}!{\n\\begin{tabular}{cccc}\n\\toprule\n\\textbf{Model} & Accuracy  & Recall  & F1 Score \\\\ \\midrule\nCOT & \\underline{45.00} & 33.42 & 32.25 \\\\\nTOT &  35.00  & 33.71 & 29.71 \\\\ \nFew-shots & 40.00 & 40.63 & 29.37 \\\\\\midrule\nLongformer & 25.00 & 20.20 & 14.00 \\\\ \nFT & \\underline{45.00} & \\underline{45.40} & \\underline{44.00} \\\\ \n\\midrule\nGWM-T & \\textbf{50.00}   &\\textbf{46.42}  &\\textbf{48.20}  \\\\\nGWM-E &\\underline{45.00}  &39.57  & 35.56  \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\toprule\n\\textbf{Model} & Accuracy  & Recall  & F1 Score \\\\ \\midrule\nCOT & \\underline{45.00} & 33.42 & 32.25 \\\\\nTOT &  35.00  & 33.71 & 29.71 \\\\ \nFew-shots & 40.00 & 40.63 & 29.37 \\\\\\midrule\nLongformer & 25.00 & 20.20 & 14.00 \\\\ \nFT & \\underline{45.00} & \\underline{45.40} & \\underline{44.00} \\\\ \n\\midrule\nGWM-T & \\textbf{50.00}   &\\textbf{46.42}  &\\textbf{48.20}  \\\\\nGWM-E &\\underline{45.00}  &39.57  & 35.56  \\\\\n\\bottomrule\n\n\n\n\n\n\\begin{table}[!tbp]\n\\setlength\\tabcolsep{3pt}\n\\centering\n\\begin{footnotesize}\n% \\vspace{-4mm}\n\\caption{\\textbf{Retrieval-augmented generation on LongBench v2}. It is a challenging long-context question-answering task that is categorized into easy and hard levels. Compared to classic RAG baselines and LLM models with extended contexts, GWM with limited context length achieved the best results. The result also demonstrates the superiority of GWM-E over GWM-T in tasks involving long contexts.} %It is a challenging long-context question-answering task that is categorized into easy and hard levels. Compared to classic RAG baselines and LLM models with extended contexts, GWM with limited context length achieved the best results. The result also demonstrates the superiority of GWM-E over GWM-T in tasks involving long contexts.\n\\label{tab:Retrieval-augmented generation}\n\\resizebox{0.39\\textwidth}{!}{\n\\begin{tabular}{cccc}\n\\toprule\n\\textbf{Model} & Overall  & Easy  & Hard    \\\\ \\midrule\n\nBM25 (2k) & 27.45 & \\textbf{41.18} & 20.59 \\\\ \nDragon (2k) & 23.53 & 35.29 & 17.65 \\\\ \\midrule\n% Full Context (left) & 27.45 & 35.29 & 23.53 \\\\\n% Full Context (right)  & 31.37 & 41.18 & 26.47 \\\\ \\midrule\n\nMistral Large 2 (128k) & 26.31 & 29.42  & 24.45    \\\\\nCommand R+ (128k) & 27.43 & 30.19  & 26.32    \\\\\nGPT-4o mini (128k) &29.01  &30.23  & \\underline{28.03}  \\\\ \\midrule\nGWM-T (2k) & \\underline{29.40}  &35.71  &21.74  \\\\\nGWM-E (2k) & \\textbf{33.32}  & \\underline{39.16}  & \\textbf{29.52}  \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{footnotesize}\n\\end{table}\n\\setlength\\tabcolsep{3pt}\n\\centering\n\n\\caption{\\textbf{Retrieval-augmented generation on LongBench v2}. It is a challenging long-context question-answering task that is categorized into easy and hard levels. Compared to classic RAG baselines and LLM models with extended contexts, GWM with limited context length achieved the best results. The result also demonstrates the superiority of GWM-E over GWM-T in tasks involving long contexts.} \\label{tab:Retrieval-augmented generation}\n\\resizebox{0.39\\textwidth}0.39\\textwidth{!}!{\n\\begin{tabular}{cccc}\n\\toprule\n\\textbf{Model} & Overall  & Easy  & Hard    \\\\ \\midrule\n\nBM25 (2k) & 27.45 & \\textbf{41.18} & 20.59 \\\\ \nDragon (2k) & 23.53 & 35.29 & 17.65 \\\\ \\midrule\n% Full Context (left) & 27.45 & 35.29 & 23.53 \\\\\n% Full Context (right)  & 31.37 & 41.18 & 26.47 \\\\ \\midrule\n\nMistral Large 2 (128k) & 26.31 & 29.42  & 24.45    \\\\\nCommand R+ (128k) & 27.43 & 30.19  & 26.32    \\\\\nGPT-4o mini (128k) &29.01  &30.23  & \\underline{28.03}  \\\\ \\midrule\nGWM-T (2k) & \\underline{29.40}  &35.71  &21.74  \\\\\nGWM-E (2k) & \\textbf{33.32}  & \\underline{39.16}  & \\textbf{29.52}  \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\toprule\n\\textbf{Model} & Overall  & Easy  & Hard    \\\\ \\midrule\n\nBM25 (2k) & 27.45 & \\textbf{41.18} & 20.59 \\\\ \nDragon (2k) & 23.53 & 35.29 & 17.65 \\\\ \\midrule\n\n\nMistral Large 2 (128k) & 26.31 & 29.42  & 24.45    \\\\\nCommand R+ (128k) & 27.43 & 30.19  & 26.32    \\\\\nGPT-4o mini (128k) &29.01  &30.23  & \\underline{28.03}  \\\\ \\midrule\nGWM-T (2k) & \\underline{29.40}  &35.71  &21.74  \\\\\nGWM-E (2k) & \\textbf{33.32}  & \\underline{39.16}  & \\textbf{29.52}  \\\\\n\\bottomrule\n\n\n\n  \n\n\n\n\\subsection{A single GWM matches the performance of domain-specific methods across multiple tasks}\n\nWe train a unified GWM on all tasks and test it across all tasks without further fine-tuning, compared with domain-specific baselines under each task. Specifically, for world prediction, we first report the multi-modal generation and matching results  in Table \\ref{tab:multi-modal generation} and  Table \\ref{tab:Multi-modal matching}. Subsequently, we report the recommendation results in Table \\ref{tab:Recommender systems}, and the traditional graph prediction results in Table \\ref{tab:Traditional graph prediction}. As for world generation, we report the multi-agent collaboration results in Table \\ref{tab:Multi-agent collaboration} and the retrieval-augmented generation results in Table \\ref{tab:Retrieval-augmented generation}. For world optimization, we report the results in Table \\ref{tab:Planning and optimization}.\n\nWe can observe that: (1) \\textit{A single GWM} achieves SOTA results in multi-modal generation (Multi-Modal-Paper), multi-agent collaboration, retrieval-augmented generation, as well as planning and optimization, and also performs comparably to domain-specific baselines in other tasks. This demonstrates GWM's ability to generalize and its applicability across a broad range of tasks. (2) GWM demonstrates promising capabilities in some highly challenging tasks, such as long-context RAG (shown in Table \\ref{tab:Retrieval-augmented generation}). GWM with a context length of 2k can outperform LLM models with a context length of 128k in RAG tasks, showcasing GWM's potential in understanding and reasoning with long texts. (3) The design of the latent embedding enables GWM-E to outperform GWM-T in five out of seven tasks with approximately 5-10 times fewer token costs. This demonstrates the efficiency and effectiveness of embedding-based message passing.\n\n\n\n\n\n\n\\begin{table}[!tbp]\n\\setlength\\tabcolsep{3pt}\n\\centering\n\\begin{footnotesize}\n% \\vspace{-4mm}\n\\caption{\\textbf{Planning and optimization results on ALFWorld}. It is to evaluate how well the methods can imitate the trajectory of expert strategies to effectively assist in solving optimization problems. Compared to classic LLM baselines and text generation baselines, GWM-E has achieved the best results.} %It is to evaluate how well the methods can imitate the trajectory of expert strategies to effectively assist in solving optimization problems. Compared to classic LLM baselines and text generation baselines, GWM-E has achieved the best results.\n\\label{tab:Planning and optimization}\n\\resizebox{0.32\\textwidth}{!}{\n\\begin{tabular}{cccc}\n\\toprule\n\\textbf{Model} & Precision  & Recall  & F1 Score \\\\ \\midrule\n\nNormal & 89.62 & 88.86 & 89.21 \\\\ \n% Few-shots & 99.30 & 99.21 & 99.25 \\\\\nCOT & 86.87 & 87.74 & 87.27 \\\\  \\midrule\nT5 FT & \\underline{92.06} & \\underline{91.52} & \\underline{91.82} \\\\ \\midrule\nGWM-T &88.10   & 87.05  & 87.42  \\\\\nGWM-E &\\textbf{93.27}  &\\textbf{92.36}  & \\textbf{92.13} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\\end{footnotesize}\n% \\vspace{-4mm}\n\\end{table}\n\\setlength\\tabcolsep{3pt}\n\\centering\n\n\\caption{\\textbf{Planning and optimization results on ALFWorld}. It is to evaluate how well the methods can imitate the trajectory of expert strategies to effectively assist in solving optimization problems. Compared to classic LLM baselines and text generation baselines, GWM-E has achieved the best results.} \\label{tab:Planning and optimization}\n\\resizebox{0.32\\textwidth}0.32\\textwidth{!}!{\n\\begin{tabular}{cccc}\n\\toprule\n\\textbf{Model} & Precision  & Recall  & F1 Score \\\\ \\midrule\n\nNormal & 89.62 & 88.86 & 89.21 \\\\ \n% Few-shots & 99.30 & 99.21 & 99.25 \\\\\nCOT & 86.87 & 87.74 & 87.27 \\\\  \\midrule\nT5 FT & \\underline{92.06} & \\underline{91.52} & \\underline{91.82} \\\\ \\midrule\nGWM-T &88.10   & 87.05  & 87.42  \\\\\nGWM-E &\\textbf{93.27}  &\\textbf{92.36}  & \\textbf{92.13} \\\\\n\\bottomrule\n\\end{tabular}\n}\n\n\\toprule\n\\textbf{Model} & Precision  & Recall  & F1 Score \\\\ \\midrule\n\nNormal & 89.62 & 88.86 & 89.21 \\\\ \nCOT & 86.87 & 87.74 & 87.27 \\\\  \\midrule\nT5 FT & \\underline{92.06} & \\underline{91.52} & \\underline{91.82} \\\\ \\midrule\nGWM-T &88.10   & 87.05  & 87.42  \\\\\nGWM-E &\\textbf{93.27}  &\\textbf{92.36}  & \\textbf{92.13} \\\\\n\\bottomrule\n\n\n\n\n\n\n\n\n\\subsection{GWM benefits from multi-hop graphs} \n\n\nTo explore whether multi-hop graphs can enhance the performance of GWM, we compared the effectiveness of four different hop settings with a no-graph baseline using GWM-E on six tasks, as illustrated in Figure \\ref{fig:performance_hops}. Specifically, we measured the average performance across five settings for all tasks. For Multi-modal tasks, we use DINOv2 and F1 Score to calculate average performance, while for Rec, Agent, and Optimization tasks, we exclusively use the F1 Score. Accuracy metrics were employed for the remaining tasks. Graphs have consistently enhanced GWM-E performance across all tasks, showing a minimum relative gain of 20\\% on graph-related tasks. However, an increased hop number does not always lead to better performance since it can cause over-smoothing and introduce redundant information.\n\n\\begin{figure}[ht]\n\\centering\n\\includegraphics[width=\\linewidth]{figs/hops_radar.pdf} \n\\vspace{-4mm}\n\\caption{\\textbf{Multi-hop graphs enhance GWM's performance across representative tasks in six domains}. We can observe that the introduction of graphs has benefited GWM-E across all tasks compared to no graph. Moreover, excessive hops can lead to over-smoothing, thereby decreasing performance.} %We can observe that the introduction of graphs has benefited GWM-E across all tasks compared to no graph. Moreover, excessive hops can lead to over-smoothing, thereby decreasing performance.\n\\label{fig:performance_hops}\n\\vspace{-3mm}\n\\end{figure}\n\\centering\n\\includegraphics[width=\\linewidth]{figs/hops_radar.pdf} \n\\vspace{-4mm}\n\\caption{\\textbf{Multi-hop graphs enhance GWM's performance across representative tasks in six domains}. We can observe that the introduction of graphs has benefited GWM-E across all tasks compared to no graph. Moreover, excessive hops can lead to over-smoothing, thereby decreasing performance.} \\label{fig:performance_hops}\n\\vspace{-3mm}\n\n\n\n\n\n\n\n\n\n\n\\subsection{GWM boosts zero-shot/few-shot performance}\n\nTo validate the zero-shot/few-shot capabilities of GWM, we conduct experiments with GWM-E and GWM-T on the Agent and RAG tasks, as shown in Figure \\ref{fig:zero_few_shot}  (``-T'' and ``-E'' respectively represent the experimental results of GWM-T and GWM-E). Here, Single Data refers to training GWM solely on the Agent or RAG task. Zero-shot refers to training GWM on tasks other than Agent or RAG and testing it on Agent or RAG. Fine-tuned GWM refers to training GWM on tasks other than Agent or RAG, followed by few-shot fine-tuning with 10\\% of the data from Agent or RAG. We can observe that GWM adapts effectively to new tasks using only a small amount of domain-specific training data. Moreover, we observe that the zero-shot results of GWM on the RAG task are even better than those from Single Data, indicating that GWM's strong generalization ability greatly benefits tasks with limited training data like Agent or RAG.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "appendix": false}, "Additional Related Work": {"content": "\n\n\n\n\n\n\n\\xhdr{Graph for Modelling Relations}Graph for Modelling Relations Graphs are highly effective in modeling complex relationships \\citep{fey2023relational,cao2023relational,gao2020ci,chen2022gndan,wu2022graph,yang2021consisrec}, extracting nodes and edges to model relational data with embeddings. Graph Neural Networks (GNNs) \\citep{Thomas2017GCN,hamilton2017graphsage,velivckovic2017gat,schlichtkrull2017modeling} have emerged as a dominant approach, particularly in recommendation systems \\citep{Erxue2022recommender} and social networks \\citep{wu2020social}. To further address the vast array of tasks and data, scholars have proposed the GFM \\citep{chen2024llaga,liu2023one} to explore GNNs' zero-shot or few-shot capabilities \\citep{fey2023relational,cao2023relational,gao2020ci,chen2022gndan} to tackle challenges such as the cold start problem in recommendations. \n\n\\begin{figure}[t]\n\\centering\n\\includegraphics[width=\\linewidth]{figs/zero_few_shot.pdf} \n\\caption{\\textbf{GWM boosts zero-shot/few-shot performance on multi-agent collaboration (Agent) and retrieval-augmented generation (RAG) tasks}. Note that ``-T'' and ``-E'' respectively represent the experimental results of GWM-T and GWM-E. It can be observed that  GWM can quickly adapt to new tasks with a small amount of domain-specific training data. Moreover, GWM's strong generalization ability can boost the performance of Agent and RAG.} %It can be observed that  GWM can quickly adapt to new tasks with a small amount of domain-specific training data. Moreover, GWM's strong generalization ability can boost the performance of Agent and RAG.\n\\label{fig:zero_few_shot}\n\\end{figure}\n\\centering\n\\includegraphics[width=\\linewidth]{figs/zero_few_shot.pdf} \n\\caption{\\textbf{GWM boosts zero-shot/few-shot performance on multi-agent collaboration (Agent) and retrieval-augmented generation (RAG) tasks}. Note that ``-T'' and ``-E'' respectively represent the experimental results of GWM-T and GWM-E. It can be observed that  GWM can quickly adapt to new tasks with a small amount of domain-specific training data. Moreover, GWM's strong generalization ability can boost the performance of Agent and RAG.} \\label{fig:zero_few_shot}\n\n\n\n\\xhdr{World Model}World Model The WM \\citep{ha2018recurrent} is to construct the world observations as states and predict future states based on given actions. Existing WMs \\citep{wu2024ivideogpt,bruce2024genie} primarily focus on how to utilize unstructured data to predict state transitions, thereby enhancing the effectiveness of sequence generation tasks. Genie \\citep{bruce2024genie} trained a foundation world model using a massive amount of unlabelled, serialized internet videos, which has provided benefits for the planning outcomes of downstream tasks. Additionally, some WMs \\citep{zhang2021world,zhu2022value} have attempted to integrate structured data with GWM. $L^3P$L^3P \\citep{zhang2021world} uses graphs to model each step of the agent's decision-making process and their connections, thus enhancing scalable planning in reinforcement learning. However, they are still largely confined to planning and optimization scenarios, which limits their potential for task generalization as WMs. Thus we  develop GWM that integrates the capabilities of graphs with WM to generalize across diverse tasks.\n\n\n\n\n\n\n\n\n\n\n\n\\vspace{-1mm}\n", "appendix": false}, "Conclusion": {"content": "\nWe propose GWM, a unified framework that uses a graph world state to tackle diverse prediction, generation, and planning tasks. Across six benchmarks, GWM matches domain-specific baselines while benefiting from multi-hop graph structures, showing strong generality and flexibility. It also improves zero-shot and few-shot performance, indicating strong cross-task generalization. GWM currently supports text, table, and image modalities, with plans to extend to more. While the current implementation focuses on homophilous graphs, we aim to expand it to support both homophilous and non-homophilous structures for broader applicability. Its modular design also makes it a flexible base for future multi-modal graph reasoning tasks.\n\n\n\n\n", "appendix": false}, "Impact Statement": {"content": "\nThe WM serves as a unified framework for prediction, generation, and decision-making across various applications. While traditional WMs are constrained to single-modality and unstructured data, our proposed  GWM enhances them by embedding-level message passing and aggregation to integrate structured and multi-modal data, bridging the gap between unstructured and structured processing. GWM demonstrates significant potential as a foundational graph-based model for real-world multi-modal tasks; however, its current scope is limited by the number of supported modalities and the simplicity of its graph architecture, necessitating further advancements for broader applicability and enhanced relational modeling. Future applications should also prioritize ethical considerations, recognizing that efforts are needed to ensure that GWM's responses are reliable, unbiased, and safe in real-world deployments, thereby preventing potential harm to users. In addition, the data utilized in this work are collected in compliance with applicable laws and licensing agreements. Their usage is also transparent and harmless.\n\nThe security of Large Language Models (LLMs) has always been a concern. Unfortunately, current LLMs sometimes produce harmful and biased information unexpectedly. Our proposed method uses LLMs to generate simulated queries and summary responses, which are only used to construct a graph of records and connect text chunks from long documents. However, more work is needed in real-world applications to ensure that LLMs' responses are reliable and harmless, so that they do not harm users. \n\n\n\n\\bibliography{paper}\n\\bibliographystyle{icml2025}icml2025\n\\newpage\n", "appendix": true}, "More on GWM Task": {"content": " \\label{app:dataset}\n\n\n\n\n\n\\begin{table*}[t]\n\\small\n \\setlength{\\tabcolsep}{7pt}\n \\centering\n \\renewcommand{\\arraystretch}{1}\n \\setlength\\tabcolsep{1pt}\n    \\caption{\\textbf{Detailed summarization of all collected datasets in GWM.} We summarize the dataset names, tasks, action level, multi-modality, nodes, and edges in the table.}\n    \\label{tab:dataset_all}\n    \\centering\n    \\begin{tabular}{l|ccccc}\n        \\toprule\n        \\textbf{Dataset}  & \\textbf{Task} & \\textbf{Action Level} &  \\textbf{Multi-modality} & \\textbf{Nodes} & \\textbf{Edges}    \\\\ \\midrule\n        Goodreads & Multi-modal generation/matching & Node level & Text/image & Text/image nodes &  Similar-book semantic   \\\\\n        Multi-Modal-Paper & Multi-modal generation/matching & Node level & Text/image/table & Text/image/table nodes & References \\\\\n        Baby & Recommendation & Link level & Text/table/image & User/item nodes & User-item interactions  \\\\\n        Sports & Recommendation & Link level & Text/table/image & User/item nodes & User-item interactions  \\\\\n        Clothing & Recommendation & Link level & Text/table/image & User/item nodes & User-item interactions  \\\\\n        Cora & Traditional graph prediction  & Node/edge level & Text & Research paper nodes & Citation  \\\\\n        PubMed & Traditional graph prediction  & Node/edge level & Text & Research paper nodes & Citation  \\\\\n        HIV & Traditional graph prediction & Graph level & Text & Atoms nodes & Atoms bonds \\\\\n        AgentClinic&  Multi-agent collaboration  & Graph level & Text/image & Agent/image/text nodes& Agent-agent/image/text \\\\\n        LongBench v2& Retrieval-augmented generation & Unintended action  & Text & Chunk nodes& Chunk-chunk similarity \\\\\n        ALFWorld& Planning and optimization & Graph level & Text/image & State  nodes &  State images similarity \\\\ \n        \\bottomrule\n    \\end{tabular}\n% }\n\n\\end{table*}\n\\small\n \\setlength{\\tabcolsep}{7pt}\n \\centering\n \\renewcommand{\\arraystretch}{1}\n \\setlength\\tabcolsep{1pt}\n    \\caption{\\textbf{Detailed summarization of all collected datasets in GWM.} We summarize the dataset names, tasks, action level, multi-modality, nodes, and edges in the table.}\n    \\label{tab:dataset_all}\n    \\centering\n    \n        \\toprule\n        \\textbf{Dataset}  & \\textbf{Task} & \\textbf{Action Level} &  \\textbf{Multi-modality} & \\textbf{Nodes} & \\textbf{Edges}    \\\\ \\midrule\n        Goodreads & Multi-modal generation/matching & Node level & Text/image & Text/image nodes &  Similar-book semantic   \\\\\n        Multi-Modal-Paper & Multi-modal generation/matching & Node level & Text/image/table & Text/image/table nodes & References \\\\\n        Baby & Recommendation & Link level & Text/table/image & User/item nodes & User-item interactions  \\\\\n        Sports & Recommendation & Link level & Text/table/image & User/item nodes & User-item interactions  \\\\\n        Clothing & Recommendation & Link level & Text/table/image & User/item nodes & User-item interactions  \\\\\n        Cora & Traditional graph prediction  & Node/edge level & Text & Research paper nodes & Citation  \\\\\n        PubMed & Traditional graph prediction  & Node/edge level & Text & Research paper nodes & Citation  \\\\\n        HIV & Traditional graph prediction & Graph level & Text & Atoms nodes & Atoms bonds \\\\\n        AgentClinic&  Multi-agent collaboration  & Graph level & Text/image & Agent/image/text nodes& Agent-agent/image/text \\\\\n        LongBench v2& Retrieval-augmented generation & Unintended action  & Text & Chunk nodes& Chunk-chunk similarity \\\\\n        ALFWorld& Planning and optimization & Graph level & Text/image & State  nodes &  State images similarity \\\\ \n        \\bottomrule\n    \n\n\n\n\n\nThis section discusses the detailed processing procedure for each dataset collected in GWM. We summarize their general information in Table \\ref{tab:dataset_all}.\n\n\\subsection{Multi-modal generation and matching} \\label{ap:multi-modal generation and matching g}\n\n\\xhdr{Dataset descriptions}Dataset descriptions In this study, we utilize two datasets for multi-modal generation and matching: the Goodreads dataset \\cite{wan-etal-2019-fine} and our curated Multi-Modal-Paper dataset. \n\n\\textbf{(1) Goodreads}: The Goodreads dataset is a large-scale collection of book-related metadata, textual descriptions, and cover images, widely used in prior multi-modal research~\\cite{jin2024instructg2i}. \nThe Goodreads dataset is structured as a graph, where each book is represented as a node, and edges signify similar-book semantics.\n\n\\textbf{(2) Multi-Model-Paper}: Multi-Modal-Paper dataset is a curated dataset of academic papers, incorporating textual content, figures, tables, and metadata to facilitate research on scholarly document analysis. The raw LaTeX files of the papers were collected from ArXiv\\footnote{\\href{https://arxiv.org/}{https://arxiv.org/} We thank ArXiv for providing open access interoperability.} using the ArXiv API, in accordance with the papers' licenses, which are primarily CC0 or CC-BY 4.0, permitting redistribution and sharing. Using carefully selected survey papers from various domains of Artificial Intelligence (AI), including Natural Language Processing (NLP), Computer Vision (CV), Bio-informatics, and Robotics, as seed papers, we employ a breadth-first search (BFS) algorithm to gather cited papers. Then, we traverse through the abstract syntax tree built from the LaTeX file to extract graph-structured multi-modal data for each gathered paper. \n\nFor the multi-modal generation task, we sample figure-caption pairs for training and evaluation. GWM and baseline models are tasked with reconstructing the figures from the provided captions. For the multi-modal matching task, we sample cited-citing pairs using reference relationship, namely the macro command \"$\\backslash$\\backslashref\". The cited-citing pairs are usually figure-text or table-text pairs.\n\nA detailed statistical overview of the Goodreads and Multi-Modal-Paper datasets is presented in Table~\\ref{tab:multimodal_data}.\n\n\\begin{table}[ht]\n\t\\centering\n    \\caption{\\textbf{Data statistics for multi-modal generation and matching.} In Multi-Modal-Paper, there are 58565 text-nodes, 7380 figure-nodes, and 6792 table-nodes.}\n\t\\begin{tabular}{lrrr}\n\t\t\\toprule\n\t\t\\textbf{Dataset} & \\textbf{\\#Node} & \\textbf{\\#Edges} \\\\\n\t\t\\midrule\n\t\t\\textbf{Goodreads} &  93,475 & 637,210\\\\\n\t\t\\textbf{Multi-Modal-Paper} & 72,737 &51,840\\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t\\label{tab:multimodal_data}%\n\\end{table}\n\t\\centering\n    \\caption{\\textbf{Data statistics for multi-modal generation and matching.} In Multi-Modal-Paper, there are 58565 text-nodes, 7380 figure-nodes, and 6792 table-nodes.}\n\t\n\t\t\\toprule\n\t\t\\textbf{Dataset} & \\textbf{\\#Node} & \\textbf{\\#Edges} \\\\\n\t\t\\midrule\n\t\t\\textbf{Goodreads} &  93,475 & 637,210\\\\\n\t\t\\textbf{Multi-Modal-Paper} & 72,737 &51,840\\\\\n\t\t\\bottomrule\n\t\n\t\\label{tab:multimodal_data}\n\n\\xhdr{Baselines details}Baselines details\nFor multi-modal generation tasks, we employ two text-to-image baseline models, SD-1.5 and SD-1.5 FT, along with an image-to-image baseline model, ControlNet.\n\n\n \\begin{itemize}[leftmargin=1em, itemsep=0pt] \n \\item \\textbf{SD-1.5}: A pre-trained Stable Diffusion v1.5 model~\\cite{rombach2022high} used for text-to-image generation without task-specific fine-tuning.\n\\item  \\textbf{SD-1.5 FT}: Stable Diffusion v1.5 models, each fine-tuned separately on the training splits of the Goodreads and Multi-Modal-Paper datasets.\n\\item \\textbf{ControlNet}: An extension of Stable Diffusion that incorporates structural guidance, such as edge maps or depth maps, to enhance control over generated images~\\cite{zhang2023adding}.\n \\end{itemize}\\begin{itemize}[leftmargin=1em, itemsep=0pt] \n \\item \\textbf{SD-1.5}: A pre-trained Stable Diffusion v1.5 model~\\cite{rombach2022high} used for text-to-image generation without task-specific fine-tuning.\n\\item  \\textbf{SD-1.5 FT}: Stable Diffusion v1.5 models, each fine-tuned separately on the training splits of the Goodreads and Multi-Modal-Paper datasets.\n\\item \\textbf{ControlNet}: An extension of Stable Diffusion that incorporates structural guidance, such as edge maps or depth maps, to enhance control over generated images~\\cite{zhang2023adding}.\n \\end{itemize} \n \\item \\textbf{SD-1.5}: A pre-trained Stable Diffusion v1.5 model~\\cite{rombach2022high} used for text-to-image generation without task-specific fine-tuning.\n\\item  \\textbf{SD-1.5 FT}: Stable Diffusion v1.5 models, each fine-tuned separately on the training splits of the Goodreads and Multi-Modal-Paper datasets.\n\\item \\textbf{ControlNet}: An extension of Stable Diffusion that incorporates structural guidance, such as edge maps or depth maps, to enhance control over generated images~\\cite{zhang2023adding}.\n  \n\nWe use three baselines for multi-modal matching: Contrastive MLP, CLIP, and CLIP FT. Since the vertices of the edge in the Multi-Modal-Paper dataset can include tables, namely the text-table pair that are not suitable for the CLIP model, we exclusively use Contrastive MLP for this dataset.\n \\begin{itemize}[leftmargin=1em, itemsep=0pt] \n \\item \\textbf{Contrastive MLP} \\citep{liu2022mlp}: A multi-layer perceptron trained to predict multi-modal matching by processing embeddings from different modalities. The text and table embeddings are encoded by BERT while the image embedding is encoded by CLIP. Embeddings from different modalities are padding to the same dimension.\n\\item  \\textbf{CLIP}: A pre-trained vision-language model designed for image-text alignment, using contrastive learning to map corresponding image and text embeddings into a shared space~\\cite{radford2021learning}.\n\\item \\textbf{CLIP FT}: A fine-tuned version of CLIP, adapted to the specific dataset to enhance multi-modal matching performance.\n \\end{itemize}\\begin{itemize}[leftmargin=1em, itemsep=0pt] \n \\item \\textbf{Contrastive MLP} \\citep{liu2022mlp}: A multi-layer perceptron trained to predict multi-modal matching by processing embeddings from different modalities. The text and table embeddings are encoded by BERT while the image embedding is encoded by CLIP. Embeddings from different modalities are padding to the same dimension.\n\\item  \\textbf{CLIP}: A pre-trained vision-language model designed for image-text alignment, using contrastive learning to map corresponding image and text embeddings into a shared space~\\cite{radford2021learning}.\n\\item \\textbf{CLIP FT}: A fine-tuned version of CLIP, adapted to the specific dataset to enhance multi-modal matching performance.\n \\end{itemize} \n \\item \\textbf{Contrastive MLP} \\citep{liu2022mlp}: A multi-layer perceptron trained to predict multi-modal matching by processing embeddings from different modalities. The text and table embeddings are encoded by BERT while the image embedding is encoded by CLIP. Embeddings from different modalities are padding to the same dimension.\n\\item  \\textbf{CLIP}: A pre-trained vision-language model designed for image-text alignment, using contrastive learning to map corresponding image and text embeddings into a shared space~\\cite{radford2021learning}.\n\\item \\textbf{CLIP FT}: A fine-tuned version of CLIP, adapted to the specific dataset to enhance multi-modal matching performance.\n  \n\n\n\\subsection{Recommendation} \\label{apd:rec}\n\n\\begin{table}[ht]\n\t\\centering\n    \\caption{\\textbf{Data statistics for recommendation.} It includes three datasets of different scales, with the sizes ranging from small to large as follows: Baby, Sports, and Clothing.}\n\t\\begin{tabular}{lrrrr}\n\t\t\\toprule\n\t\t\\textbf{Dataset} & \\textbf{\\#User} & \\textbf{\\#Item} & \\textbf{\\#Edges} & \\textbf{Sparsity}\\\\\n\t\t\\midrule\n\t\t\\textbf{Baby} & 19,445 & 7,050 & 160,792  & 99.883\\% \\\\\n\t\t\\textbf{Sports} & 35,598 & 18,357 & 296,337  & 99.955\\% \\\\\n\t\t\\textbf{Clothing} & 39,387 & 23,033 & 278,677  & 99.969\\% \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\t\\label{tab:rec_data}%\n\\end{table}\n\t\\centering\n    \\caption{\\textbf{Data statistics for recommendation.} It includes three datasets of different scales, with the sizes ranging from small to large as follows: Baby, Sports, and Clothing.}\n\t\n\t\t\\toprule\n\t\t\\textbf{Dataset} & \\textbf{\\#User} & \\textbf{\\#Item} & \\textbf{\\#Edges} & \\textbf{Sparsity}\\\\\n\t\t\\midrule\n\t\t\\textbf{Baby} & 19,445 & 7,050 & 160,792  & 99.883\\% \\\\\n\t\t\\textbf{Sports} & 35,598 & 18,357 & 296,337  & 99.955\\% \\\\\n\t\t\\textbf{Clothing} & 39,387 & 23,033 & 278,677  & 99.969\\% \\\\\n\t\t\\bottomrule\n\t\n\t\\label{tab:rec_data}\n\n\\xhdr{Dataset descriptions}Dataset descriptions In the recommendation task, we conduct extensive evaluations using three Amazon datasets extensively recognized in prior research~\\citep{mcauley2015image}, specifically: Baby, Sports, and Outdoors, as well as Clothing Shoes, and Jewelry. For simplicity, these datasets are hereafter referred to as Baby, Sports, and Clothing, respectively. Utilizing the 5-core setting, we filter inactive users and items with less than five interactions. Each dataset encompasses both visual and textual modalities and we use the extracted visual and textual features from existing work~\\citep{zhou2023mmrec}. Here the visual modality is the product image and the textual modality is the product description. The characteristics of these datasets are shown in Table~\\ref{tab:rec_data}.\n\n\n\\xhdr{Baselines details}Baselines details We compare GWM with three representative GNN baselines.\n\n \\begin{itemize}[leftmargin=1em, itemsep=0pt] \n \\item \\textbf{LightGCN}: Employs a simplified graph convolutional network to learn user and item interaction graph~\\citep{lightgcn}.\n\\item  \\textbf{MMGCN}: Learns user preferences across multiple modalities via message-passing on modality-specific user-item graphs, improving recommendations in multimedia contexts~\\citep{mmgcn}.\n\n\\item \\textbf{GRCN}: Refines interaction graphs using multimedia content to identify and remove noisy edges, thereby sharpening the recommendation process~\\citep{grcn}.\n\n \\end{itemize}\\begin{itemize}[leftmargin=1em, itemsep=0pt] \n \\item \\textbf{LightGCN}: Employs a simplified graph convolutional network to learn user and item interaction graph~\\citep{lightgcn}.\n\\item  \\textbf{MMGCN}: Learns user preferences across multiple modalities via message-passing on modality-specific user-item graphs, improving recommendations in multimedia contexts~\\citep{mmgcn}.\n\n\\item \\textbf{GRCN}: Refines interaction graphs using multimedia content to identify and remove noisy edges, thereby sharpening the recommendation process~\\citep{grcn}.\n\n \\end{itemize} \n \\item \\textbf{LightGCN}: Employs a simplified graph convolutional network to learn user and item interaction graph~\\citep{lightgcn}.\n\\item  \\textbf{MMGCN}: Learns user preferences across multiple modalities via message-passing on modality-specific user-item graphs, improving recommendations in multimedia contexts~\\citep{mmgcn}.\n\n\\item \\textbf{GRCN}: Refines interaction graphs using multimedia content to identify and remove noisy edges, thereby sharpening the recommendation process~\\citep{grcn}.\n\n  \n \n\n\n\n\n\n\n\\subsection{Traditional graph prediction} \\label{apd:graph}\n\\xhdr{Dataset descriptions}Dataset descriptions In the traditional graph prediction task, we evaluate GWM on Cora, PubMed, and HIV datasets.\n(1) Cora~\\cite{chen2024exploring}: Cora is a citation network in the computer science domain, where nodes represent research papers and edges denote citation relationships. Each node includes the paper's title and abstract as text features, with labels indicating paper categories. Tasks on Cora include category prediction (node level) and citation link identification (link level).\n(2) PubMed~\\cite{chen2024exploring}: PubMed is a biomedical citation network, similar to Cora, with nodes representing papers and edges indicating citation relationships.\n(3) HIV~\\cite{liu2023one}: HIV is a molecular dataset constructed from MOLHIV dataset~\\cite{wu2018moleculenet} that contains over 40,000 compounds annotated for their ability to inhibit HIV replication. Molecular structures and graph representations are generated from SMILES strings, with atoms (nodes) and bonds (edges) described using natural language.\n\n\\xhdr{Baselines details}Baselines details The settings for the baselines primarily follow LLAGA \\citep{chen2024llaga} and OFA \\citep{liu2023one}. We convert all nodes and labels in the Cora, PubMed, and HIV datasets into text. For all methods, we use BERT to obtain text embedding. We divide all datasets into training, validation, and test sets in an 8:1:1 ratio. \n\n \\begin{itemize}[leftmargin=1em, itemsep=0pt] \n \\item \\textbf{GCN} ~\\cite{Thomas2017GCN}: Applies spectral-based convolution operations to capture local graph structures and propagate information across nodes, serving as a fundamental baseline for graph-based learning.\n\\item  \\textbf{GAT} ~\\cite{velivckovic2017gat}: Enhances node representation learning by incorporating attention mechanisms, allowing adaptive weighting of neighboring nodes to improve feature aggregation.\n\\item \\textbf{LLAGA} ~\\cite{chen2024llaga}: Integrates LLM with graph structures to enhance reasoning and information retrieval in multi-modal and structured data scenarios.\n\\item \\textbf{OFA} ~\\cite{liu2023one}: Unifies vision, language, and multi-modal learning tasks within a single framework, leveraging pre-trained knowledge to facilitate cross-modal understanding and adaptation.\n \\end{itemize}\\begin{itemize}[leftmargin=1em, itemsep=0pt] \n \\item \\textbf{GCN} ~\\cite{Thomas2017GCN}: Applies spectral-based convolution operations to capture local graph structures and propagate information across nodes, serving as a fundamental baseline for graph-based learning.\n\\item  \\textbf{GAT} ~\\cite{velivckovic2017gat}: Enhances node representation learning by incorporating attention mechanisms, allowing adaptive weighting of neighboring nodes to improve feature aggregation.\n\\item \\textbf{LLAGA} ~\\cite{chen2024llaga}: Integrates LLM with graph structures to enhance reasoning and information retrieval in multi-modal and structured data scenarios.\n\\item \\textbf{OFA} ~\\cite{liu2023one}: Unifies vision, language, and multi-modal learning tasks within a single framework, leveraging pre-trained knowledge to facilitate cross-modal understanding and adaptation.\n \\end{itemize} \n \\item \\textbf{GCN} ~\\cite{Thomas2017GCN}: Applies spectral-based convolution operations to capture local graph structures and propagate information across nodes, serving as a fundamental baseline for graph-based learning.\n\\item  \\textbf{GAT} ~\\cite{velivckovic2017gat}: Enhances node representation learning by incorporating attention mechanisms, allowing adaptive weighting of neighboring nodes to improve feature aggregation.\n\\item \\textbf{LLAGA} ~\\cite{chen2024llaga}: Integrates LLM with graph structures to enhance reasoning and information retrieval in multi-modal and structured data scenarios.\n\\item \\textbf{OFA} ~\\cite{liu2023one}: Unifies vision, language, and multi-modal learning tasks within a single framework, leveraging pre-trained knowledge to facilitate cross-modal understanding and adaptation.\n  \n\n \n\\subsection{Multi-agent collaboration} \\label{apd:agent}\n\n\\xhdr{Dataset descriptions}Dataset descriptions In the multi-agent collaboration task, we evaluate GWM on AgentClinic~\\cite{schmidgall2024agentclinic} benchmark, specifically AgentClinic-NEJM collected from the New England Journal of Medicine (NEJM) case challenges. Each case in AgentClinic-NEJM is multimodal, comprising a case description, patient profile, clinical photograph, measurement results, and five candidate diagnoses. We partition AgentClinic-NEJM into training, validation, and test sets using a 4:1:1 split ratio. To simulate real-world clinical procedures, we employ the simulated clinical environments from AgentClinic to gather dialogues between the patient and doctor, along with physical examination results. This environment is modeled as a graph, where nodes represent various profile-based agents and edges capture the interactions between agents and their engagement with knowledge resources. We utilize Meta-Llama-3-70B-Instruct\\footnote{\\href{https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct}{https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct}} as backbone model for simulation. In the final diagnosis procedure, we apply both GWM and LLM baselines for comparison, where the graph information is converted into textual format before being processed by LLM baselines.\n\n\\xhdr{Baselines details}Baselines details We compare GWM with three classic LLM baselines adopting different reasoning strategies. We use Meta-Llama-3-8B-Instruct\\footnote{\\href{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}} as the backbone model to align with GWM.\n \\begin{itemize}[leftmargin=1em, itemsep=0pt] \n \\item \\textbf{CoT}: Adopts Chain-of-Thought~\\cite{wei2022chain} prompting, which enhances reasoning by decomposing complex problems into intermediate steps, improving performance on multi-step reasoning tasks.\n\\item  \\textbf{ToT}: Adopts Tree-of-Thought~\\cite{yao2024tree} prompting, which explores multiple reasoning paths in a tree-like structure, enabling iterative evaluation and refinement for more robust decision-making.\n\\item \\textbf{Few-shots}: Adopts Few-shot~\\cite{brown2020languagemodelsfewshotlearners} prompting, where the model is provided with a limited number of in-context examples to guide task-specific reasoning without requiring fine-tuning.\n \\end{itemize}\\begin{itemize}[leftmargin=1em, itemsep=0pt] \n \\item \\textbf{CoT}: Adopts Chain-of-Thought~\\cite{wei2022chain} prompting, which enhances reasoning by decomposing complex problems into intermediate steps, improving performance on multi-step reasoning tasks.\n\\item  \\textbf{ToT}: Adopts Tree-of-Thought~\\cite{yao2024tree} prompting, which explores multiple reasoning paths in a tree-like structure, enabling iterative evaluation and refinement for more robust decision-making.\n\\item \\textbf{Few-shots}: Adopts Few-shot~\\cite{brown2020languagemodelsfewshotlearners} prompting, where the model is provided with a limited number of in-context examples to guide task-specific reasoning without requiring fine-tuning.\n \\end{itemize} \n \\item \\textbf{CoT}: Adopts Chain-of-Thought~\\cite{wei2022chain} prompting, which enhances reasoning by decomposing complex problems into intermediate steps, improving performance on multi-step reasoning tasks.\n\\item  \\textbf{ToT}: Adopts Tree-of-Thought~\\cite{yao2024tree} prompting, which explores multiple reasoning paths in a tree-like structure, enabling iterative evaluation and refinement for more robust decision-making.\n\\item \\textbf{Few-shots}: Adopts Few-shot~\\cite{brown2020languagemodelsfewshotlearners} prompting, where the model is provided with a limited number of in-context examples to guide task-specific reasoning without requiring fine-tuning.\n \n\n\n\\subsection{Retrieval-augmented generation} \\label{apd:RAG}\n\nThe purpose of Retrieval-Augmented Generation (RAG) is to enhance the generation capabilities of Large Language Models (LLMs) by retrieving information from external knowledge \\citep{lewis2020retrieval,gao2023retrieval,zhao2024retrieval}. We introduce its dataset and baselines as follows:\n\n\n\\xhdr{Dataset descriptions}Dataset descriptions We employ LongBench v2 \\citep{bai2024longbench}, a benchmark specifically designed to test long-context understanding and reasoning. This benchmark comprises 503 challenging multiple-choice questions, with contextual lengths ranging from 8,000 to 2 million words, spanning six major task categories: Single-Doc QA, Multi-Doc QA, Long In-context Learning, Long-dialogue History Understanding, Code Repository Understanding, and Long Structured Data Understanding. The questions are stratified into easy and hard levels based on the difficulty encountered by human experts and models during their resolution. In alignment with methodologies from previous studies such as GraphRAG \\citep{edge2024local}, we segment long contexts into chunks that serve as graph nodes, with edges defined by the similarity of their BERT embeddings. Building on this, we select the Top-k (k=5 in our setting) chunks with the highest similarity to the question's embedding to feed into the GWM. For this task, we divided the dataset into training, validation, and test sets in an 8:1:1 ratio.\n\n\\xhdr{Baselines details}Baselines details We conduct comparisons with two RAG-based baselines\u2014BM25 \\citep{robertson2009probabilistic} and Dragon \\citep{lin2023train}\u2014and three long-context LLMs ($128k$128k), including Mistral Large 2\\footnote{\\href{https://huggingface.co/mistralai/Mistral-Large-Instruct-2407}{https://huggingface.co/mistralai/Mistral-Large-Instruct-2407}}, Command R+\\footnote{\\href{https://huggingface.co/CohereForAI/c4ai-command-r-plus}{https://huggingface.co/CohereForAI/c4ai-command-r-plus}}, and GPT-4o mini\\footnote{\\href{https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}{https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/}}. Their details are as follows:\n\n \\begin{itemize}[leftmargin=1em, itemsep=0pt] \n \n\\item \\textbf{BM25}:  A widely-used ranking function in information sparse retrieval. It inputs the retrieved context along with the question into the Llama-3-8B model to generate a response.\n\n\\item  \\textbf{Dragon}: It employs contrastive learning and other training tricks to finetune its ability to retrieve memory chunks. Using the Llama-3-8B model, it processes the retrieved context and the question to produce a response.\n\n\\item \\textbf{Mistral Large 2}: Mistral Large 2 from Mistral AI boasts 123 billion parameters, with a context limit of 128 k tokens. This model is one of the largest currently available, offering exceptional depth in language understanding and generation capabilities, suited for tackling the most demanding NLP tasks across various domains. \n\n\\item \\textbf{Command R+}: Command R+ by Cohere is a massive language model with 104 billion parameters, also supporting a context size of up to 128 k tokens. It is optimized for understanding and executing complex commands, making it particularly effective in interactive applications where precise and nuanced language comprehension is critical.\n \n\\item  \\textbf{GPT-4o mini}: GPT-4o mini, developed by OpenAI, is a variant of the GPT-4 series. Unlike its larger counterparts, specific details about the model's size in terms of parameters are not provided, but it is designed to handle a maximum context size of 128k tokens. This model is geared towards applications requiring high-quality text generation with potentially limited computational resources.\n\n\\end{itemize}\\begin{itemize}[leftmargin=1em, itemsep=0pt] \n \n\\item \\textbf{BM25}:  A widely-used ranking function in information sparse retrieval. It inputs the retrieved context along with the question into the Llama-3-8B model to generate a response.\n\n\\item  \\textbf{Dragon}: It employs contrastive learning and other training tricks to finetune its ability to retrieve memory chunks. Using the Llama-3-8B model, it processes the retrieved context and the question to produce a response.\n\n\\item \\textbf{Mistral Large 2}: Mistral Large 2 from Mistral AI boasts 123 billion parameters, with a context limit of 128 k tokens. This model is one of the largest currently available, offering exceptional depth in language understanding and generation capabilities, suited for tackling the most demanding NLP tasks across various domains. \n\n\\item \\textbf{Command R+}: Command R+ by Cohere is a massive language model with 104 billion parameters, also supporting a context size of up to 128 k tokens. It is optimized for understanding and executing complex commands, making it particularly effective in interactive applications where precise and nuanced language comprehension is critical.\n \n\\item  \\textbf{GPT-4o mini}: GPT-4o mini, developed by OpenAI, is a variant of the GPT-4 series. Unlike its larger counterparts, specific details about the model's size in terms of parameters are not provided, but it is designed to handle a maximum context size of 128k tokens. This model is geared towards applications requiring high-quality text generation with potentially limited computational resources.\n\n\\end{itemize} \n \n\\item \\textbf{BM25}:  A widely-used ranking function in information sparse retrieval. It inputs the retrieved context along with the question into the Llama-3-8B model to generate a response.\n\n\\item  \\textbf{Dragon}: It employs contrastive learning and other training tricks to finetune its ability to retrieve memory chunks. Using the Llama-3-8B model, it processes the retrieved context and the question to produce a response.\n\n\\item \\textbf{Mistral Large 2}: Mistral Large 2 from Mistral AI boasts 123 billion parameters, with a context limit of 128 k tokens. This model is one of the largest currently available, offering exceptional depth in language understanding and generation capabilities, suited for tackling the most demanding NLP tasks across various domains. \n\n\\item \\textbf{Command R+}: Command R+ by Cohere is a massive language model with 104 billion parameters, also supporting a context size of up to 128 k tokens. It is optimized for understanding and executing complex commands, making it particularly effective in interactive applications where precise and nuanced language comprehension is critical.\n \n\\item  \\textbf{GPT-4o mini}: GPT-4o mini, developed by OpenAI, is a variant of the GPT-4 series. Unlike its larger counterparts, specific details about the model's size in terms of parameters are not provided, but it is designed to handle a maximum context size of 128k tokens. This model is geared towards applications requiring high-quality text generation with potentially limited computational resources.\n\n\n\n\n\n\\subsection{Planning and optimization} \\label{apd:opt}\n\nThis task is designed to measure how effectively different methods can imitate the trajectory of expert strategies, which is very helpful for planning and optimization tasks.\n\n\\xhdr{Dataset descriptions}Dataset descriptions We employ the expert strategy dataset from the text-based embodied task framework, ALFWorld \\citep{shridhar2020alfworld,yang2024embodied}. This dataset provides detailed descriptions of the expert's strategic state at each decision point, incorporating both images and text, along with the corresponding decisions made in text format. In our approach, we represent each decision state as nodes within a graph and establish edges between these nodes based on the similarity of the state images linked to each decision. Our total sample size is 10,000, and it is divided into training, validation, and test sets in an 8:1:1 ratio.\n\n\\xhdr{Baselines details}Baselines details For all baselines, we first use LLaVA-1.5-7B to convert the image of each state into a text description. \n\n \\begin{itemize}[leftmargin=1em, itemsep=0pt] \n \\item \\textbf{Normal}: It directly inputs the text description of the current state into Llama-3-8B to get the response.  \n \n\\item  \\textbf{COT}: It adopts Chain-of-Thought~\\cite{wei2022chain} prompting into baseline Normal to enhance reasoning ability when predicting.\n\n\\item \\textbf{T5 FT \\citep{raffel2020exploring}}:  It is a versatile language model designed by Google Research, which treats every language problem as a text-to-text task, enhancing its adaptability across a broad range of NLP applications. Here we finetune it on the dataset of this task.\n\n \\end{itemize}\\begin{itemize}[leftmargin=1em, itemsep=0pt] \n \\item \\textbf{Normal}: It directly inputs the text description of the current state into Llama-3-8B to get the response.  \n \n\\item  \\textbf{COT}: It adopts Chain-of-Thought~\\cite{wei2022chain} prompting into baseline Normal to enhance reasoning ability when predicting.\n\n\\item \\textbf{T5 FT \\citep{raffel2020exploring}}:  It is a versatile language model designed by Google Research, which treats every language problem as a text-to-text task, enhancing its adaptability across a broad range of NLP applications. Here we finetune it on the dataset of this task.\n\n \\end{itemize} \n \\item \\textbf{Normal}: It directly inputs the text description of the current state into Llama-3-8B to get the response.  \n \n\\item  \\textbf{COT}: It adopts Chain-of-Thought~\\cite{wei2022chain} prompting into baseline Normal to enhance reasoning ability when predicting.\n\n\\item \\textbf{T5 FT \\citep{raffel2020exploring}}:  It is a versatile language model designed by Google Research, which treats every language problem as a text-to-text task, enhancing its adaptability across a broad range of NLP applications. Here we finetune it on the dataset of this task.\n\n \n\n\n\n\n", "appendix": true}, "Hyper-parameters": {"content": " \\label{sec:hyper}\n\nFor the GWM-E, we employ an n-hop MLP, where for the LLM decoder each MLP has dimensions of 2048*4096, and for the SD decoder, each MLP has dimensions of 2048*768. We fix the parameters of LLM and only fine-tune the parameters of MLP. For GWM-T, we select a maximum of 2k token-limited hops for each task to ensure a balance between efficiency and performance. We apply Lora (Lora rank = 8) \\citep{hu2021lora} for efficient training. We have summarized the hyperparameters for training different models in Table \\ref{apx:tab:hyper}.\n\n\n\n\\begin{table}[ht]\n\\caption{Hyper-parameter configuration for model training.}\\label{apx:tab:hyper}\n\\centering\n\\scalebox{0.63}{\n\\begin{tabular}{ccccc}\n\\toprule\nParameter & GWM-T LLM & GWM-T SD & GWM-E LLM & GWM-E SD \\\\\n\\midrule\nOptimizer & AdamW & AdamW & AdamW & AdamW \\\\\n% \\midrule\n% \\midrule\nAdam $\\epsilon$ & 1e-8  & 1e-8 & 1e-8  & 1e-8 \\\\\nAdam $(\\beta_1, \\beta_2)$ & (0.9, 0.999) & (0.9, 0.999) & (0.9, 0.999)  & (0.9, 0.999)  \\\\\nWeight decay & 1e-2 & 1e-2 & 1e-2  & 1e-2 \\\\\n\nBatch size per GPU & 4 & 1 & 10 & 16\\\\\nGradient Accumulation & 8 & 4 & 1 & 4\\\\\n\nEpochs & 4 & 5 & 1  & 30 \\\\\nResolution & - & 512 & - & 256 \\\\\n% \\midrule\nLearning rate & 3e-4 & 1e-5 &  3e-4  & 1e-5 \\\\\nBackbone SD &  Llama-3-8B & SD-v1-5 &  Llama-3-8B & SD-v1-5   \\\\\n\\bottomrule            \n\\end{tabular}\n}\n\\end{table}\n\\caption{Hyper-parameter configuration for model training.}\\label{apx:tab:hyper}\n\\centering\n\\scalebox{0.63}0.63{\n\\begin{tabular}{ccccc}\n\\toprule\nParameter & GWM-T LLM & GWM-T SD & GWM-E LLM & GWM-E SD \\\\\n\\midrule\nOptimizer & AdamW & AdamW & AdamW & AdamW \\\\\n% \\midrule\n% \\midrule\nAdam $\\epsilon$ & 1e-8  & 1e-8 & 1e-8  & 1e-8 \\\\\nAdam $(\\beta_1, \\beta_2)$ & (0.9, 0.999) & (0.9, 0.999) & (0.9, 0.999)  & (0.9, 0.999)  \\\\\nWeight decay & 1e-2 & 1e-2 & 1e-2  & 1e-2 \\\\\n\nBatch size per GPU & 4 & 1 & 10 & 16\\\\\nGradient Accumulation & 8 & 4 & 1 & 4\\\\\n\nEpochs & 4 & 5 & 1  & 30 \\\\\nResolution & - & 512 & - & 256 \\\\\n% \\midrule\nLearning rate & 3e-4 & 1e-5 &  3e-4  & 1e-5 \\\\\nBackbone SD &  Llama-3-8B & SD-v1-5 &  Llama-3-8B & SD-v1-5   \\\\\n\\bottomrule            \n\\end{tabular}\n}\n\n\\toprule\nParameter & GWM-T LLM & GWM-T SD & GWM-E LLM & GWM-E SD \\\\\n\\midrule\nOptimizer & AdamW & AdamW & AdamW & AdamW \\\\\nAdam $\\epsilon$\\epsilon & 1e-8  & 1e-8 & 1e-8  & 1e-8 \\\\\nAdam $(\\beta_1, \\beta_2)$(\\beta_1, \\beta_2) & (0.9, 0.999) & (0.9, 0.999) & (0.9, 0.999)  & (0.9, 0.999)  \\\\\nWeight decay & 1e-2 & 1e-2 & 1e-2  & 1e-2 \\\\\n\nBatch size per GPU & 4 & 1 & 10 & 16\\\\\nGradient Accumulation & 8 & 4 & 1 & 4\\\\\n\nEpochs & 4 & 5 & 1  & 30 \\\\\nResolution & - & 512 & - & 256 \\\\\nLearning rate & 3e-4 & 1e-5 &  3e-4  & 1e-5 \\\\\nBackbone SD &  Llama-3-8B & SD-v1-5 &  Llama-3-8B & SD-v1-5   \\\\\n\\bottomrule            \n\n\n\n\n \n", "appendix": true}, "Prompt Usage of GWM": {"content": " \\label{sec:prompt}\n\nWe summarize all the prompts we used in GWM in this section. We first introduce the action prompts in GWM. Specifically, we have summarized the action prompts for multi-modal generation and matching in Tables \\ref{prompt-template:multi-modal generation} and \\ref{prompt-template:multi-modal matching}. The action prompts for recommendations are summarized in Table \\ref{prompt-template:recommendation}. Additionally, the action prompts for traditional graph prediction are outlined in Tables \\ref{prompt-template:Cora_node}, \\ref{prompt-template:PubMed_node}, \\ref{prompt-template:link prediction}, and \\ref{prompt-template:HIV}. Moreover, we have also summarized the action prompts for multi-agent collaboration, retrieval-augmented generation, and planning and optimization in Tables \\ref{prompt-template:multi-agent collaboration}, \\ref{prompt-template:retrieval-augmented generation}, and \\ref{prompt-template:planning and optimization}.  Then, we introduce prompts used in GWM-T. We summarize the prompt $P_{u}$P_{u}u of multi-modality as tokens in GWM-T in Table \\ref{prompt-template:multi-modality as token}. Moreover, we introduce the prompt $f_{v}(\\cdot)$f_{v}v(\\cdot) of aggregating central node and neighbor nodes in GWM-T in Table \\ref{prompt-template:aggregating}.\n\n\n\n\n\n\n\n\n\n\n\n\n\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Action prompt of multi-modal generation}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    This is a multi-modal generation task. Please predict the missing modality based on the given modality: \\{modality\\}.\\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:multi-modal generation}\n\\end{tabular}\n\\end{table*}\n\\centering\n\\caption{\\textbf{Action prompt of multi-modal generation}.}\n\n\\toprule[1.1pt]\n    This is a multi-modal generation task. Please predict the missing modality based on the given modality: \\{modality\\}.\\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:multi-modal generation}\n\n\n\n\n\n\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Action prompt of multi-modal matching}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    This task involves matching multi-modal information. Given two modalities: \\{modality 1\\} and \\{modality 2\\}, please determine whether they correspond with each other. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:multi-modal matching}\n\\end{tabular}\n\\end{table*}\n\\centering\n\\caption{\\textbf{Action prompt of multi-modal matching}.}\n\n\\toprule[1.1pt]\n    This task involves matching multi-modal information. Given two modalities: \\{modality 1\\} and \\{modality 2\\}, please determine whether they correspond with each other. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:multi-modal matching}\n\n\n\n\n\n\n\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Action prompt of recommendation}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    This is a recommendation task. Given the user node and item node: \\{user node\\} and \\{item node\\}, please tell me whether these two nodes should connect to each other.\\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:recommendation}\n\\end{tabular}\n\\end{table*}\n\\centering\n\\caption{\\textbf{Action prompt of recommendation}.}\n\n\\toprule[1.1pt]\n    This is a recommendation task. Given the user node and item node: \\{user node\\} and \\{item node\\}, please tell me whether these two nodes should connect to each other.\\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:recommendation}\n\n\n\n\n\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Action prompt of node classification of Cora}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    Given a node-centered graph: \\{node\\}, each node represents a paper, we need to classify the center node into 7 classes: Case Based, Genetic Algorithms, Neural Networks, Probabilistic Methods, Reinforcement Learning, Rule Learning, Theory, please tell me which class the center node belongs to?\\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:Cora_node}\n\\end{tabular}\n\\end{table*}\n\\centering\n\\caption{\\textbf{Action prompt of node classification of Cora}.}\n\n\\toprule[1.1pt]\n    Given a node-centered graph: \\{node\\}, each node represents a paper, we need to classify the center node into 7 classes: Case Based, Genetic Algorithms, Neural Networks, Probabilistic Methods, Reinforcement Learning, Rule Learning, Theory, please tell me which class the center node belongs to?\\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:Cora_node}\n\n\n\n\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Action prompt of node classification of PubMed}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    Given a node-centered graph: \\{node\\}, each node represents a paper about Diabetes, we need to classify the center node into 3 classes: Diabetes Mellitus Experimental, Diabetes Mellitus Type 1, and Diabetes Mellitus Type 2, please tell me which class the center node belongs to?\\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:PubMed_node}\n\\end{tabular}\n\\end{table*}\n\\centering\n\\caption{\\textbf{Action prompt of node classification of PubMed}.}\n\n\\toprule[1.1pt]\n    Given a node-centered graph: \\{node\\}, each node represents a paper about Diabetes, we need to classify the center node into 3 classes: Diabetes Mellitus Experimental, Diabetes Mellitus Type 1, and Diabetes Mellitus Type 2, please tell me which class the center node belongs to?\\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:PubMed_node}\n\n\n\n\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Action prompt of link prediction of  Cora and PubMed}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    Given two nodes information: \\{node 1\\} and \\{node 2\\}, please tell me whether two center nodes in the subgraphs should connect to each other. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:link prediction}\n\\end{tabular}\n\\end{table*}\n\\centering\n\\caption{\\textbf{Action prompt of link prediction of  Cora and PubMed}.}\n\n\\toprule[1.1pt]\n    Given two nodes information: \\{node 1\\} and \\{node 2\\}, please tell me whether two center nodes in the subgraphs should connect to each other. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:link prediction}\n\n\n\n\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Action prompt of graph classification of  HIV}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    Human immunodeficiency viruses (HIV) are a type of retrovirus, which induces acquired immune deficiency syndrome (AIDs). Please determine whether this molecule \\{molecule\\} is effective for this assay. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:HIV}\n\\end{tabular}\n\\end{table*}\n\\centering\n\\caption{\\textbf{Action prompt of graph classification of  HIV}.}\n\n\\toprule[1.1pt]\n    Human immunodeficiency viruses (HIV) are a type of retrovirus, which induces acquired immune deficiency syndrome (AIDs). Please determine whether this molecule \\{molecule\\} is effective for this assay. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:HIV}\n\n\n\n\n\n\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Action prompt of multi-agent collaboration}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    This is a Multi-Agent Collaborative Generation task for creating dynamic conversational interactions. Given a user query: \\{user query\\} and context of three distinct agents: \\{Patient Agent Context\\}, \\{Measurement Agent Context\\}, and \\{Moderato Agent Context\\}, Please generate a well-rounded response to the user's question.\\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:multi-agent collaboration}\n\\end{tabular}\n\\end{table*}\n\\centering\n\\caption{\\textbf{Action prompt of multi-agent collaboration}.}\n\n\\toprule[1.1pt]\n    This is a Multi-Agent Collaborative Generation task for creating dynamic conversational interactions. Given a user query: \\{user query\\} and context of three distinct agents: \\{Patient Agent Context\\}, \\{Measurement Agent Context\\}, and \\{Moderato Agent Context\\}, Please generate a well-rounded response to the user's question.\\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:multi-agent collaboration}\n\n\n\n\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Action prompt of retrieval-augmented generation}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    This is a Retrieval-Augmented Generation task for improving response quality in dialogue systems. Given a user query: \\{user query\\} and a set of retrieved documents: \\{retrieved documents\\}, the goal is to generate a coherent and contextually relevant response. Please generate a response that integrates information from the retrieved documents to accurately address the user's query. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:retrieval-augmented generation}\n\\end{tabular}\n\\end{table*}\n\\centering\n\\caption{\\textbf{Action prompt of retrieval-augmented generation}.}\n\n\\toprule[1.1pt]\n    This is a Retrieval-Augmented Generation task for improving response quality in dialogue systems. Given a user query: \\{user query\\} and a set of retrieved documents: \\{retrieved documents\\}, the goal is to generate a coherent and contextually relevant response. Please generate a response that integrates information from the retrieved documents to accurately address the user's query. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:retrieval-augmented generation}\n\n\n\n\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Action prompt of planning and optimization}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    This is an embodied household task, please predict the next decision-making behavior based on multimodal historical information: \\{historical information\\}. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:planning and optimization}\n\\end{tabular}\n\\end{table*}\n\\centering\n\\caption{\\textbf{Action prompt of planning and optimization}.}\n\n\\toprule[1.1pt]\n    This is an embodied household task, please predict the next decision-making behavior based on multimodal historical information: \\{historical information\\}. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:planning and optimization}\n\n\n\n\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Prompt $P_{u}$ of multi-modality as token in GWM-T}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    The image's text description is: \\{image's text description\\}, original text is: \\{original text\\}, table description is: \\{table description\\}. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:multi-modality as token}\n\\end{tabular}\n\\end{table*}\n\\centering\n\\caption{\\textbf{Prompt $P_{u}$ of multi-modality as token in GWM-T}.}\n\n\\toprule[1.1pt]\n    The image's text description is: \\{image's text description\\}, original text is: \\{original text\\}, table description is: \\{table description\\}. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:multi-modality as token}\n\n\n\n\n\\begin{table*}[h]\n\\centering\n\\caption{\\textbf{Prompt $f_{v}(\\cdot)$ of aggregating central node and neighbor nodes in GWM-T}.}\n\\begin{tabular}{p{13cm}}\n\\toprule[1.1pt]\n    The text description of the central node is: \\{center node\\}, and the text descriptions of the neighboring nodes are: \\{neighbor nodes\\}. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:aggregating}\n\\end{tabular}\n\\end{table*}\n\\centering\n\\caption{\\textbf{Prompt $f_{v}(\\cdot)$ of aggregating central node and neighbor nodes in GWM-T}.}\n\n\\toprule[1.1pt]\n    The text description of the central node is: \\{center node\\}, and the text descriptions of the neighboring nodes are: \\{neighbor nodes\\}. \\\\\n\\bottomrule[1.1pt]\n\\label{prompt-template:aggregating}\n\n\n\n\n\n", "appendix": true}, "Qualitative Comparisons for All Tasks of GWM": {"content": " \\label{app:Qualitative Comparisons}\n\n\nThese tables present comprehensive qualitative comparisons across all task categories evaluated in our GWM framework study. Each table demonstrates the superior performance of our Graph World Model (GWM) variants compared to state-of-the-art baselines through concrete examples. Table \\ref{tab:multimodal_generation} showcases multi-modal generation capabilities where GWM-T successfully predicts missing modalities from given inputs. Table \\ref{tab:multimodal_matching} illustrates multi-modal matching tasks where GWM-E accurately determines correspondence between different modalities. Table \\ref{tab:recommendation} demonstrates recommendation performance where GWM-E correctly predicts user-item connections. Table \\ref{tab:graph_prediction} highlights traditional graph prediction tasks where GWM-E excels in node classification. Table \\ref{tab:multiagent_collaboration} presents multi-agent collaboration scenarios where GWM-T integrates multiple agent contexts for medical diagnosis. Table \\ref{tab:rag} shows retrieval-augmented generation capabilities where GWM-E effectively combines retrieved documents with user queries. Finally, Table \\ref{tab:planning_optimization} demonstrates planning and optimization tasks where GWM-E predicts optimal decision-making behaviors in embodied environments. These qualitative results consistently validate the effectiveness of our approach across diverse task domains.\n\n\\begin{table*}[h]\n    \\centering\n    \\caption{\\textbf{Task description and output comparison of multi-modal generation.} This task is to predict the missing modality based on the given\nmodality. Here we utilize one case of Goodreads dataset as examples. We show the output results of GWM-T, the best performing GWM, and ControlNet , the strongest baseline.}\n    \\label{tab:multimodal_generation}\n    \n    \\renewcommand{\\arraystretch}{1.2} % Slightly increase row height\n    \n    \\begin{tabular}{|m{3cm}|P{3cm}|P{3cm}|P{3cm}|}\n        \\toprule\n        \\multicolumn{4}{|c|}{\\textbf{Task Description}} \\\\\n        \\midrule\n        \\centering \\textbf{Task Name} & \\multicolumn{3}{c|}{Multi-modal generation} \\\\\n        \\midrule\n         \\centering \\textbf{Given modality} & \\textbf{Action prompt} & \\multicolumn{2}{c|}{\\textbf{Ground truth}} \\\\\n        \\midrule\n         \\parbox{3cm}{\\centering Title: The Shark-Infested Custard}& \n         \\parbox{3cm}{\\centering\\arraybackslash This is a multi-modal generation task. Please predict the missing modality based on the given modality: \\{modality\\}.} & \n        \\multicolumn{2}{m{4cm}|}{\\includegraphics[width=2.8cm]{figs/Goodreads_gt_shark.jpg}} \\\\\n        \\midrule\n        \\centering \\textbf{Method} & \\multicolumn{3}{c|}{\\textbf{Output Results}} \\\\\n        \\midrule\n        \\centering\\arraybackslash\n        ControlNet  & \\multicolumn{3}{m{8.5cm}|}{\\centering\\arraybackslash\\includegraphics[width=6cm]{figs/Goodreads_controlnet_shark.png}} \\\\\n        \\midrule\n        \\centering\\arraybackslash\n        GWM-T & \\multicolumn{3}{m{8.5cm}|}{\\centering\\arraybackslash\\includegraphics[width=6cm]{figs/Goodreads_GWMT_shark.jpg}} \\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table*}\n    \\centering\n    \\caption{\\textbf{Task description and output comparison of multi-modal generation.} This task is to predict the missing modality based on the given\nmodality. Here we utilize one case of Goodreads dataset as examples. We show the output results of GWM-T, the best performing GWM, and ControlNet , the strongest baseline.}\n    \\label{tab:multimodal_generation}\n    \n    \\renewcommand{\\arraystretch}{1.2} \n        \\toprule\n        \\multicolumn{4}4{|c|}|c|{\\textbf{Task Description}}\\textbf{Task Description} \\\\\n        \\midrule\n        \\centering \\textbf{Task Name} & \\multicolumn{3}3{c|}c|{Multi-modal generation}Multi-modal generation \\\\\n        \\midrule\n         \\centering \\textbf{Given modality} & \\textbf{Action prompt} & \\multicolumn{2}2{c|}c|{\\textbf{Ground truth}}\\textbf{Ground truth} \\\\\n        \\midrule\n         \\parbox{3cm}3cm{\\centering Title: The Shark-Infested Custard}\\centering Title: The Shark-Infested Custard& \n         \\parbox{3cm}3cm{\\centering\\arraybackslash This is a multi-modal generation task. Please predict the missing modality based on the given modality: \\{modality\\}.}\\centering\\arraybackslash This is a multi-modal generation task. Please predict the missing modality based on the given modality: \\{modality\\}. & \n        \\multicolumn{2}2{m{4cm}|}m{4cm}4cm|{\\includegraphics[width=2.8cm]{figs/Goodreads_gt_shark.jpg}}\\includegraphics[width=2.8cm]{figs/Goodreads_gt_shark.jpg} \\\\\n        \\midrule\n        \\centering \\textbf{Method} & \\multicolumn{3}3{c|}c|{\\textbf{Output Results}}\\textbf{Output Results} \\\\\n        \\midrule\n        \\centering\\arraybackslash\n        ControlNet  & \\multicolumn{3}3{m{8.5cm}|}m{8.5cm}8.5cm|{\\centering\\arraybackslash\\includegraphics[width=6cm]{figs/Goodreads_controlnet_shark.png}}\\centering\\arraybackslash\\includegraphics[width=6cm]{figs/Goodreads_controlnet_shark.png} \\\\\n        \\midrule\n        \\centering\\arraybackslash\n        GWM-T & \\multicolumn{3}3{m{8.5cm}|}m{8.5cm}8.5cm|{\\centering\\arraybackslash\\includegraphics[width=6cm]{figs/Goodreads_GWMT_shark.jpg}}\\centering\\arraybackslash\\includegraphics[width=6cm]{figs/Goodreads_GWMT_shark.jpg} \\\\\n        \\bottomrule\n    \n\n\n\\begin{table*}[h]\n    \\centering\n    \\caption{\\textbf{Task description and output comparison of multi-modal matching.} This task is to predict whether two modalities correspond with each other. Here we utilize one case of Multi-Modal-Paper dataset as examples. We show the output results of GWM-E, the best performing GWM, and Contrastive MLP, the strongest baseline. Note that modality can be an image, a table, or a text.}\n    \\label{tab:multimodal_matching}\n    \n    \\renewcommand{\\arraystretch}{1.2} % Slightly increase row height\n    \n    % Define a new column type for centered text in m columns\n    \\newcolumntype{C}[1]{>{\\centering\\arraybackslash}m{#1}}\n    \n    \\begin{tabular}{|C{5cm}|C{3cm}|C{5cm}|C{3cm}|C{3cm}|}\n        \\toprule\n        \\multicolumn{5}{|c|}{\\textbf{Task Description}} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{\\textbf{Task Name}} & \\multicolumn{3}{c|}{Multi-modal matching} \\\\\n        \\midrule\n        \\textbf{Modality 1: figure} & \\textbf{Modality 2: text} & \\textbf{Action prompt} & \\multicolumn{2}{c|}{\\textbf{Ground truth}} \\\\\n        \\midrule\n        \\includegraphics[width=5cm]{figs/figures_749.png} & \n        Illustration of different formats of STL expressions. (a) Different expression formats of the same STL. (b) The binary tree representation of STL. & \n        This task involves matching multi-modal information. Given two modalities: \\{modality 1\\} and \\{modality 2\\}, please determine whether they correspond with each other. & \n        \\multicolumn{2}{c|}{yes} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{\\textbf{Method}} & \\multicolumn{3}{c|}{\\textbf{Output Results}} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{Contrastive MLP} & \\multicolumn{3}{c|}{no} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{GWM-E} & \\multicolumn{3}{c|}{yes} \\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table*}\n    \\centering\n    \\caption{\\textbf{Task description and output comparison of multi-modal matching.} This task is to predict whether two modalities correspond with each other. Here we utilize one case of Multi-Modal-Paper dataset as examples. We show the output results of GWM-E, the best performing GWM, and Contrastive MLP, the strongest baseline. Note that modality can be an image, a table, or a text.}\n    \\label{tab:multimodal_matching}\n    \n    \\renewcommand{\\arraystretch}{1.2} \\newcolumntype{C}C[1]{>{\\centering\\arraybackslash}m{#1}}>{\\centering\\arraybackslash}\\centering\\arraybackslashm{#1}#1\n    \n    \n        \\toprule\n        \\multicolumn{5}5{|c|}|c|{\\textbf{Task Description}}\\textbf{Task Description} \\\\\n        \\midrule\n        \\multicolumn{2}2{|c|}|c|{\\textbf{Task Name}}\\textbf{Task Name} & \\multicolumn{3}3{c|}c|{Multi-modal matching}Multi-modal matching \\\\\n        \\midrule\n        \\textbf{Modality 1: figure} & \\textbf{Modality 2: text} & \\textbf{Action prompt} & \\multicolumn{2}2{c|}c|{\\textbf{Ground truth}}\\textbf{Ground truth} \\\\\n        \\midrule\n        \\includegraphics[width=5cm]{figs/figures_749.png} & \n        Illustration of different formats of STL expressions. (a) Different expression formats of the same STL. (b) The binary tree representation of STL. & \n        This task involves matching multi-modal information. Given two modalities: \\{modality 1\\} and \\{modality 2\\}, please determine whether they correspond with each other. & \n        \\multicolumn{2}2{c|}c|{yes}yes \\\\\n        \\midrule\n        \\multicolumn{2}2{|c|}|c|{\\textbf{Method}}\\textbf{Method} & \\multicolumn{3}3{c|}c|{\\textbf{Output Results}}\\textbf{Output Results} \\\\\n        \\midrule\n        \\multicolumn{2}2{|c|}|c|{Contrastive MLP}Contrastive MLP & \\multicolumn{3}3{c|}c|{no}no \\\\\n        \\midrule\n        \\multicolumn{2}2{|c|}|c|{GWM-E}GWM-E & \\multicolumn{3}3{c|}c|{yes}yes \\\\\n        \\bottomrule\n    \n\n\n\\begin{table*}[h]\n    \\centering\n    \\caption{\\textbf{Task description and output comparison of recommendation.} This task is to predict whether the user node and the item node are connected. Here we utilize one case of Baby dataset as examples. We show the output results of GWM-E, the best performing GWM, and LightGCN, the strongest baseline.}\n    \\label{tab:recommendation}\n    \n    \\renewcommand{\\arraystretch}{1.2} % Slightly increase row height\n    \n    % Define a new column type for centered text in m columns\n    \\newcolumntype{C}[1]{>{\\centering\\arraybackslash}m{#1}}\n    \n    \\begin{tabular}{|C{4cm}|C{4cm}|C{4cm}|C{3cm}|C{3cm}|}\n        \\toprule\n        \\multicolumn{5}{|c|}{\\textbf{Task Description}} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{\\textbf{Task Name}} & \\multicolumn{3}{c|}{Recommendation} \\\\\n        \\midrule\n        \\textbf{User node} & \\textbf{Item node} & \\textbf{Action prompt} & \\multicolumn{2}{c|}{\\textbf{Ground truth}} \\\\\n        \\midrule\n        User online product reviews series: I struggled to find full slips, especially larger ones. The first one was too small; the second fit well and was affordable. The beads looked stunning, perfect for beadwork. The earrings broke immediately due to poor quality. I love the 3 flower sister Hawaii glass beads on my Pandora bracelet. They're pretty and large, and my eight-year-old finds them comfy enough to sleep in ...  \n          & \\includegraphics[width=4cm]{figs/B004R1REP2.jpg} & \n        This is a recommendation task. Given the user node and item node: \\{user node\\} and \\{item node\\}, please tell me whether these two nodes should connect to each other. & \n        \\multicolumn{2}{c|}{no} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{\\textbf{Method}} & \\multicolumn{3}{c|}{\\textbf{Output Results}} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{LightGCN} & \\multicolumn{3}{c|}{yes} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{GWM-E} & \\multicolumn{3}{c|}{no} \\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table*}\n    \\centering\n    \\caption{\\textbf{Task description and output comparison of recommendation.} This task is to predict whether the user node and the item node are connected. Here we utilize one case of Baby dataset as examples. We show the output results of GWM-E, the best performing GWM, and LightGCN, the strongest baseline.}\n    \\label{tab:recommendation}\n    \n    \\renewcommand{\\arraystretch}{1.2} \\newcolumntype{C}C[1]{>{\\centering\\arraybackslash}m{#1}}>{\\centering\\arraybackslash}\\centering\\arraybackslashm{#1}#1\n    \n    \n        \\toprule\n        \\multicolumn{5}5{|c|}|c|{\\textbf{Task Description}}\\textbf{Task Description} \\\\\n        \\midrule\n        \\multicolumn{2}2{|c|}|c|{\\textbf{Task Name}}\\textbf{Task Name} & \\multicolumn{3}3{c|}c|{Recommendation}Recommendation \\\\\n        \\midrule\n        \\textbf{User node} & \\textbf{Item node} & \\textbf{Action prompt} & \\multicolumn{2}2{c|}c|{\\textbf{Ground truth}}\\textbf{Ground truth} \\\\\n        \\midrule\n        User online product reviews series: I struggled to find full slips, especially larger ones. The first one was too small; the second fit well and was affordable. The beads looked stunning, perfect for beadwork. The earrings broke immediately due to poor quality. I love the 3 flower sister Hawaii glass beads on my Pandora bracelet. They're pretty and large, and my eight-year-old finds them comfy enough to sleep in ...  \n          & \\includegraphics[width=4cm]{figs/B004R1REP2.jpg} & \n        This is a recommendation task. Given the user node and item node: \\{user node\\} and \\{item node\\}, please tell me whether these two nodes should connect to each other. & \n        \\multicolumn{2}2{c|}c|{no}no \\\\\n        \\midrule\n        \\multicolumn{2}2{|c|}|c|{\\textbf{Method}}\\textbf{Method} & \\multicolumn{3}3{c|}c|{\\textbf{Output Results}}\\textbf{Output Results} \\\\\n        \\midrule\n        \\multicolumn{2}2{|c|}|c|{LightGCN}LightGCN & \\multicolumn{3}3{c|}c|{yes}yes \\\\\n        \\midrule\n        \\multicolumn{2}2{|c|}|c|{GWM-E}GWM-E & \\multicolumn{3}3{c|}c|{no}no \\\\\n        \\bottomrule\n    \n\n\n\\begin{table*}[h]\n    \\centering\n    \\caption{\\textbf{Task description and output comparison of traditional graph prediction.} This task aims to perform predictions at three different levels: node, link, and graph. Here we utilize one case of Cora's node prediction. We show the output results of GWM-E, the best performing GWM, and LLAGA, the strongest baseline.}\n    \\label{tab:graph_prediction}\n    \n    \\renewcommand{\\arraystretch}{1.2} % Slightly increase row height\n    \n    % Define a new column type for centered text in m columns\n    \\newcolumntype{C}[1]{>{\\centering\\arraybackslash}m{#1}}\n    \n    \\begin{tabular}{|C{7cm}|C{5cm}|C{3cm}|}\n        \\hline\n        \\multicolumn{3}{|c|}{\\textbf{Task Description}} \\\\\n        \\hline\n        \\textbf{Task Name} & \\multicolumn{2}{c|}{Traditional graph prediction} \\\\\n        \\hline\n        \\textbf{Node} & \\textbf{Action prompt} & \\textbf{Ground truth} \\\\\n        \\hline\n        Learning under persistent drift: In this paper we study learning algorithms for environments which are changing over time. Unlike most previous work, we are interested in the case where the changes might be rapid but their \"direction\" is relatively constant. We model this type of change by assuming that the target distribution is changing continuously at a constant rate from one extreme distribution to another. We show in this case how to use a simple weighting scheme to estimate the error of an hypothesis, and using this estimate, to minimize the error of the prediction. & \n        Given a node-centered graph: \\{node\\}, each node represents a paper, we need to classify the center node into 7 classes: Case Based, Genetic Algorithms, Neural Networks, Probabilistic Methods, Reinforcement Learning, Rule Learning, Theory, please tell me which class the center node belongs to? & \n        Theory \\\\\n        \\hline\n        \\textbf{Method} & \\multicolumn{2}{c|}{\\textbf{Output Results}} \\\\\n        \\hline\n        LLAGA & \\multicolumn{2}{c|}{Neural Networks} \\\\\n        \\hline\n        GWM-E & \\multicolumn{2}{c|}{Theory} \\\\\n        \\hline\n    \\end{tabular}\n\\end{table*}\n    \\centering\n    \\caption{\\textbf{Task description and output comparison of traditional graph prediction.} This task aims to perform predictions at three different levels: node, link, and graph. Here we utilize one case of Cora's node prediction. We show the output results of GWM-E, the best performing GWM, and LLAGA, the strongest baseline.}\n    \\label{tab:graph_prediction}\n    \n    \\renewcommand{\\arraystretch}{1.2} \\newcolumntype{C}C[1]{>{\\centering\\arraybackslash}m{#1}}>{\\centering\\arraybackslash}\\centering\\arraybackslashm{#1}#1\n    \n    \n        \\hline\n        \\multicolumn{3}3{|c|}|c|{\\textbf{Task Description}}\\textbf{Task Description} \\\\\n        \\hline\n        \\textbf{Task Name} & \\multicolumn{2}2{c|}c|{Traditional graph prediction}Traditional graph prediction \\\\\n        \\hline\n        \\textbf{Node} & \\textbf{Action prompt} & \\textbf{Ground truth} \\\\\n        \\hline\n        Learning under persistent drift: In this paper we study learning algorithms for environments which are changing over time. Unlike most previous work, we are interested in the case where the changes might be rapid but their \"direction\" is relatively constant. We model this type of change by assuming that the target distribution is changing continuously at a constant rate from one extreme distribution to another. We show in this case how to use a simple weighting scheme to estimate the error of an hypothesis, and using this estimate, to minimize the error of the prediction. & \n        Given a node-centered graph: \\{node\\}, each node represents a paper, we need to classify the center node into 7 classes: Case Based, Genetic Algorithms, Neural Networks, Probabilistic Methods, Reinforcement Learning, Rule Learning, Theory, please tell me which class the center node belongs to? & \n        Theory \\\\\n        \\hline\n        \\textbf{Method} & \\multicolumn{2}2{c|}c|{\\textbf{Output Results}}\\textbf{Output Results} \\\\\n        \\hline\n        LLAGA & \\multicolumn{2}2{c|}c|{Neural Networks}Neural Networks \\\\\n        \\hline\n        GWM-E & \\multicolumn{2}2{c|}c|{Theory}Theory \\\\\n        \\hline\n    \n\n\n\n\n\n\n\\begin{table*}[h]\n    \\centering\n    \\caption{\\textbf{Multi-agent collaboration task and output comparison.} Disease-related query answering through agent interaction and external knowledge. Results show GWM-T vs COT baseline. \\textbf{Note: Medical images may cause discomfort but are from real datasets.}}\n    \\label{tab:multiagent_collaboration}\n    \n    \\renewcommand{\\arraystretch}{1.0}\n    \\small % \u4f7f\u7528\u5c0f\u5b57\u4f53\n    \n    \\begin{tabular}{|>{\\centering\\arraybackslash}m{4cm}|>{\\centering\\arraybackslash}m{5cm}|>{\\centering\\arraybackslash}m{5.5cm}|}\n        \\hline\n        \\multicolumn{3}{|c|}{\\textbf{Task Description}} \\\\\n        \\hline\n        \\textbf{Task} & \\multicolumn{2}{c|}{Multi-agent collaboration} \\\\\n        \\hline\n        \\textbf{User Query} & \\textbf{Patient Agent} & \\textbf{Moderator Agent} \\\\\n        \\hline\n        \n        53-year-old man, 3-year history: itchy rash, Raynaud's, dysphagia, burning hands. Exam: firm papules on forehead with glabellar grooves, waxy papules on hands with thickening and contractures. Similar changes on nose, lips, ears, trunk, feet. No telangiectasia/calcinosis. Sensory neuropathy in hands/arms/face. Normal thyroid. IgG-monoclonal gammopathy, normal bone marrow.\n        \n        Choices: (A) AL amyloidosis (B) Multiple myeloma (C) Scleredema (D) Scleromyxedema (E) Systemic sclerosis\n        & \n        Role: 53-year-old patient with 3-year symptoms including itchy rash, firm forehead papules causing brow grooves, waxy hand papules with thickening and finger contractures. Experience Raynaud's phenomenon, dysphagia, burning hands, and numbness in hands/arms/face. Aware of normal thyroid tests and abnormal blood protein but unaware of diagnosis implications.\n        & \n        Moderator organizing case information:\n        \n        Test results: Normal thyroid function, IgG-monoclonal gammopathy detected, normal bone marrow biopsy.\n        \n        Key findings: Extracellular yellow-brown deposits in dermis on skin biopsy.\n        \\\\\n        \\hline\n        \\textbf{Measurement Agent} & \\textbf{Action Prompt} & \\textbf{Ground Truth} \\\\\n        \\hline\n        \\centering\n        \\includegraphics[width=3.5cm]{figs/6.png} \n        & \n        Multi-Agent Collaborative Generation task for dynamic conversational interactions. Generate response using Patient, Measurement, and Moderator agent contexts for the given user query.\n        & \n        D\n        \\\\\n        \\hline\n        \\textbf{Method} & \\multicolumn{2}{c|}{\\textbf{Output Results}} \\\\\n        \\hline\n        COT & \\multicolumn{2}{m{10.5cm}|}{\\centering Analysis of patient symptoms and test results suggests monoclonal gammopathy with characteristic skin findings. DIAGNOSIS: Multiple myeloma (B)} \\\\\n        \\hline\n        GWM-T & \\multicolumn{2}{m{10.5cm}|}{\\centering D} \\\\\n        \\hline\n    \\end{tabular}\n\\end{table*}\n    \\centering\n    \\caption{\\textbf{Multi-agent collaboration task and output comparison.} Disease-related query answering through agent interaction and external knowledge. Results show GWM-T vs COT baseline. \\textbf{Note: Medical images may cause discomfort but are from real datasets.}}\n    \\label{tab:multiagent_collaboration}\n    \n    \\renewcommand{\\arraystretch}{1.0}\n    \\small \n        \\hline\n        \\multicolumn{3}3{|c|}|c|{\\textbf{Task Description}}\\textbf{Task Description} \\\\\n        \\hline\n        \\textbf{Task} & \\multicolumn{2}2{c|}c|{Multi-agent collaboration}Multi-agent collaboration \\\\\n        \\hline\n        \\textbf{User Query} & \\textbf{Patient Agent} & \\textbf{Moderator Agent} \\\\\n        \\hline\n        \n        53-year-old man, 3-year history: itchy rash, Raynaud's, dysphagia, burning hands. Exam: firm papules on forehead with glabellar grooves, waxy papules on hands with thickening and contractures. Similar changes on nose, lips, ears, trunk, feet. No telangiectasia/calcinosis. Sensory neuropathy in hands/arms/face. Normal thyroid. IgG-monoclonal gammopathy, normal bone marrow.\n        \n        Choices: (A) AL amyloidosis (B) Multiple myeloma (C) Scleredema (D) Scleromyxedema (E) Systemic sclerosis\n        & \n        Role: 53-year-old patient with 3-year symptoms including itchy rash, firm forehead papules causing brow grooves, waxy hand papules with thickening and finger contractures. Experience Raynaud's phenomenon, dysphagia, burning hands, and numbness in hands/arms/face. Aware of normal thyroid tests and abnormal blood protein but unaware of diagnosis implications.\n        & \n        Moderator organizing case information:\n        \n        Test results: Normal thyroid function, IgG-monoclonal gammopathy detected, normal bone marrow biopsy.\n        \n        Key findings: Extracellular yellow-brown deposits in dermis on skin biopsy.\n        \\\\\n        \\hline\n        \\textbf{Measurement Agent} & \\textbf{Action Prompt} & \\textbf{Ground Truth} \\\\\n        \\hline\n        \\centering\n        \\includegraphics[width=3.5cm]{figs/6.png} \n        & \n        Multi-Agent Collaborative Generation task for dynamic conversational interactions. Generate response using Patient, Measurement, and Moderator agent contexts for the given user query.\n        & \n        D\n        \\\\\n        \\hline\n        \\textbf{Method} & \\multicolumn{2}2{c|}c|{\\textbf{Output Results}}\\textbf{Output Results} \\\\\n        \\hline\n        COT & \\multicolumn{2}2{m{10.5cm}|}m{10.5cm}10.5cm|{\\centering Analysis of patient symptoms and test results suggests monoclonal gammopathy with characteristic skin findings. DIAGNOSIS: Multiple myeloma (B)}\\centering Analysis of patient symptoms and test results suggests monoclonal gammopathy with characteristic skin findings. DIAGNOSIS: Multiple myeloma (B) \\\\\n        \\hline\n        GWM-T & \\multicolumn{2}2{m{10.5cm}|}m{10.5cm}10.5cm|{\\centering D}\\centering D \\\\\n        \\hline\n    \n\n\\begin{table*}[h]\n    \\centering\n    \\caption{\\textbf{Task description and output comparison of retrieval-augmented generation.} This task is to generate a response that integrates information from the retrieved documents to accurately address the user's query. Here we utilize one case of LongBench v2 dataset as examples. We show the output results of GWM-E, the best performing GWM, and GPT-4o mini, the strongest baseline.}\n    \\label{tab:rag}\n    \n    \\renewcommand{\\arraystretch}{1.2} % Slightly increase row height\n    \n    % Define a new column type for centered text in m columns\n    \\newcolumntype{C}[1]{>{\\centering\\arraybackslash}m{#1}}\n    \n    \\begin{tabular}{|C{4.5cm}|C{4.5cm}|C{4cm}|C{2cm}|C{1cm}|}\n        \\toprule\n        \\multicolumn{5}{|c|}{\\textbf{Task Description}} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{\\textbf{Task Name}} & \\multicolumn{3}{c|}{Retrieval-augmented generation} \\\\\n        \\midrule\n        \\textbf{User query} & \\textbf{Document} & \\textbf{Action prompt} & \\multicolumn{2}{c|}{\\textbf{Ground truth}} \\\\\n        \\midrule\n        What is the correct answer to this question: You are given a grammar book of Kalamang language, now translate the following Kalamang sentence into English: Faisal emun me mindi don bolonet me ma he kademor.\nChoices:\n(A) Faisal's mother is still angry at him for a little thing like that.\n(B) Faisal's mother turns furious at him for a big thing like that.\n(C) Faisal's mother gets frustrated at him for a big thing like this.\n(D) Faisal's mother gets angry at him for a little thing like that.\n\nFormat your response as follows: \"The correct answer is (insert answer here)\". & \n        There are very few households with two fluent Kalamang-speaking parents and children born after 1990, but even in those households the children are not raised in Kalamang. As indicated above, non-fluent speakers have a good passive command of Kalamang ... Fluent Kalamang speakers do not necessarily shift to Papuan Malay when they join the conversation, but they are not expected to actively contribute, although they can express themselves in a simple way in Kalamang ... & \n        This is a Retrieval-Augmented Generation task for improving response quality in dialogue systems. Given a user query: \\{user query\\} and a set of retrieved documents: \\{retrieved documents\\}, the goal is to generate a coherent and contextually relevant response. Please generate a response that integrates information from the retrieved documents to accurately address the user's query. & \n        \\multicolumn{2}{c|}{D} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{\\textbf{Method}} & \\multicolumn{3}{c|}{\\textbf{Output Results}} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{GPT-4o mini} & \\multicolumn{3}{c|}{A} \\\\\n        \\midrule\n        \\multicolumn{2}{|c|}{GWM-E} & \\multicolumn{3}{c|}{D} \\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table*}\n    \\centering\n    \\caption{\\textbf{Task description and output comparison of retrieval-augmented generation.} This task is to generate a response that integrates information from the retrieved documents to accurately address the user's query. Here we utilize one case of LongBench v2 dataset as examples. We show the output results of GWM-E, the best performing GWM, and GPT-4o mini, the strongest baseline.}\n    \\label{tab:rag}\n    \n    \\renewcommand{\\arraystretch}{1.2} \\newcolumntype{C}C[1]{>{\\centering\\arraybackslash}m{#1}}>{\\centering\\arraybackslash}\\centering\\arraybackslashm{#1}#1\n    \n    \n        \\toprule\n        \\multicolumn{5}5{|c|}|c|{\\textbf{Task Description}}\\textbf{Task Description} \\\\\n        \\midrule\n        \\multicolumn{2}2{|c|}|c|{\\textbf{Task Name}}\\textbf{Task Name} & \\multicolumn{3}3{c|}c|{Retrieval-augmented generation}Retrieval-augmented generation \\\\\n        \\midrule\n        \\textbf{User query} & \\textbf{Document} & \\textbf{Action prompt} & \\multicolumn{2}2{c|}c|{\\textbf{Ground truth}}\\textbf{Ground truth} \\\\\n        \\midrule\n        What is the correct answer to this question: You are given a grammar book of Kalamang language, now translate the following Kalamang sentence into English: Faisal emun me mindi don bolonet me ma he kademor.\nChoices:\n(A) Faisal's mother is still angry at him for a little thing like that.\n(B) Faisal's mother turns furious at him for a big thing like that.\n(C) Faisal's mother gets frustrated at him for a big thing like this.\n(D) Faisal's mother gets angry at him for a little thing like that.\n\nFormat your response as follows: \"The correct answer is (insert answer here)\". & \n        There are very few households with two fluent Kalamang-speaking parents and children born after 1990, but even in those households the children are not raised in Kalamang. As indicated above, non-fluent speakers have a good passive command of Kalamang ... Fluent Kalamang speakers do not necessarily shift to Papuan Malay when they join the conversation, but they are not expected to actively contribute, although they can express themselves in a simple way in Kalamang ... & \n        This is a Retrieval-Augmented Generation task for improving response quality in dialogue systems. Given a user query: \\{user query\\} and a set of retrieved documents: \\{retrieved documents\\}, the goal is to generate a coherent and contextually relevant response. Please generate a response that integrates information from the retrieved documents to accurately address the user's query. & \n        \\multicolumn{2}2{c|}c|{D}D \\\\\n        \\midrule\n        \\multicolumn{2}2{|c|}|c|{\\textbf{Method}}\\textbf{Method} & \\multicolumn{3}3{c|}c|{\\textbf{Output Results}}\\textbf{Output Results} \\\\\n        \\midrule\n        \\multicolumn{2}2{|c|}|c|{GPT-4o mini}GPT-4o mini & \\multicolumn{3}3{c|}c|{A}A \\\\\n        \\midrule\n        \\multicolumn{2}2{|c|}|c|{GWM-E}GWM-E & \\multicolumn{3}3{c|}c|{D}D \\\\\n        \\bottomrule\n    \n\n\n\n\n\n\n\n\n\\begin{table*}[h]\n    \\centering\n    \\caption{\\textbf{Planning and optimization task and output comparison.} Agent decision-making prediction using ALFWorld dataset. Results show GWM-E vs T5 FT baseline.}\n    \\label{tab:planning_optimization}\n    \\renewcommand{\\arraystretch}{1.0}\n    \\small % \u4f7f\u7528\u5c0f\u5b57\u4f53\n    \\begin{tabular}{|>{\\centering\\arraybackslash}m{3cm}|>{\\centering\\arraybackslash}m{2.5cm}|>{\\centering\\arraybackslash}m{4cm}|>{\\centering\\arraybackslash}m{2cm}|}\n        \\hline\n        \\multicolumn{4}{|c|}{\\textbf{Task Description}} \\\\\n        \\hline\n        \\multicolumn{2}{|c|}{\\textbf{Task}} & \\multicolumn{2}{c|}{Planning and optimization} \\\\\n        \\hline\n        \\textbf{Image} & \\textbf{Text} & \\textbf{Action Prompt} & \\textbf{Ground Truth} \\\\\n        \\hline\n        \\includegraphics[width=2.8cm]{figs/000000000.jpg} & \n        Task: put a potato in countertop & \n        Embodied household task: predict next decision-making behavior based on multimodal information. & \n        go to garbagecan 1 \\\\\n        \\hline\n        \\multicolumn{2}{|c|}{\\textbf{Method}} & \\multicolumn{2}{c|}{\\textbf{Output Results}} \\\\\n        \\hline\n        \\multicolumn{2}{|c|}{T5 FT} & \\multicolumn{2}{m{6cm}|}{\\centering go to microwave 1} \\\\\n        \\hline\n        \\multicolumn{2}{|c|}{GWM-E} & \\multicolumn{2}{m{6cm}|}{\\centering go to garbagecan 1} \\\\\n        \\hline\n    \\end{tabular}\n\\end{table*}\n    \\centering\n    \\caption{\\textbf{Planning and optimization task and output comparison.} Agent decision-making prediction using ALFWorld dataset. Results show GWM-E vs T5 FT baseline.}\n    \\label{tab:planning_optimization}\n    \\renewcommand{\\arraystretch}{1.0}\n    \\small \n        \\hline\n        \\multicolumn{4}4{|c|}|c|{\\textbf{Task Description}}\\textbf{Task Description} \\\\\n        \\hline\n        \\multicolumn{2}2{|c|}|c|{\\textbf{Task}}\\textbf{Task} & \\multicolumn{2}2{c|}c|{Planning and optimization}Planning and optimization \\\\\n        \\hline\n        \\textbf{Image} & \\textbf{Text} & \\textbf{Action Prompt} & \\textbf{Ground Truth} \\\\\n        \\hline\n        \\includegraphics[width=2.8cm]{figs/000000000.jpg} & \n        Task: put a potato in countertop & \n        Embodied household task: predict next decision-making behavior based on multimodal information. & \n        go to garbagecan 1 \\\\\n        \\hline\n        \\multicolumn{2}2{|c|}|c|{\\textbf{Method}}\\textbf{Method} & \\multicolumn{2}2{c|}c|{\\textbf{Output Results}}\\textbf{Output Results} \\\\\n        \\hline\n        \\multicolumn{2}2{|c|}|c|{T5 FT}T5 FT & \\multicolumn{2}2{m{6cm}|}m{6cm}6cm|{\\centering go to microwave 1}\\centering go to microwave 1 \\\\\n        \\hline\n        \\multicolumn{2}2{|c|}|c|{GWM-E}GWM-E & \\multicolumn{2}2{m{6cm}|}m{6cm}6cm|{\\centering go to garbagecan 1}\\centering go to garbagecan 1 \\\\\n        \\hline\n    \n\n\n", "appendix": true}, "Training and Inference Efficiency": {"content": " \\label{app:Training and Inference Efficiency}\n\nAccurately comparing the training and inference efficiency of GWM with other FMs is highly challenging because many FMs are not designed to address multimodal problems and utilize various architectures. \nWe can only compare the efficiency between GWM and LLM-based FMs from principles. For GWM-T, its efficiency shows no fundamental difference from other LLM-based FMs, as both are based on the standard instruction tuning. \nFor GWM-E, its training process only requires fine-tuning the projector as stated in section \\ref{sec:Projector tuning}, and its embedding-based method also saves a significant amount of token cost, making it more efficient. \nFor GWM-E, it takes approximately 7 hours (\\(\\sim \\frac{1}{4}\\)\\sim \\frac{1}{4} of GWM-T) of training time on four NVIDIA A6000 GPUs described in implementation details of section \\ref{sec:exps}, and the inference time per case averages 0.213s (similar to GWM-T).  Moreover, GWM-E significantly reduces memory usage with a shorter token length of 140.23 (\\(\\sim \\frac{1}{14}\\)\\sim \\frac{1}{14} of GWM-T).\n", "appendix": false}}, "categories": ["cs.LG"], "published": "2025-07-14 17:57:45+00:00", "primary_category": "cs.LG", "summary": "World models (WMs) demonstrate strong capabilities in prediction, generation,\nand planning tasks. Existing WMs primarily focus on unstructured data and\ncannot leverage the ubiquitous structured data, often represented as graphs, in\nthe digital world. While multiple graph foundation models have been proposed,\nthey focus on graph learning tasks and cannot extend to diverse multi-modal\ndata and interdisciplinary tasks. To address these challenges, we propose the\nGraph World Model (GWM), a world model that supports both unstructured and\ngraph-structured states with multi-modal information and represents diverse\ntasks as actions. The core of a GWM is a generic message-passing algorithm to\naggregate structured information, either over a unified multi-modal token space\nby converting multi-modal data into text (GWM-T) or a unified multi-modal\nembedding space by modality-specific encoders (GWM-E). Notably, GWM introduces\naction nodes to support diverse tasks, where action nodes are linked to other\nnodes via direct reference or similarity computation. Extensive experiments on\nsix tasks from diverse domains, including multi-modal generation and matching,\nrecommendation, graph prediction, multi-agent, retrieval-augmented generation,\nand planning and optimization, show that the same GWM outperforms or matches\ndomain-specific baselines' performance, benefits from multi-hop structures, and\ndemonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our\ncode for GWM is released at https://github.com/ulab-uiuc/GWM."}