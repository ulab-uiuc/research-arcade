{"title": "RLVMR: Reinforcement Learning with Verifiable Meta-Reasoning Rewards for Robust Long-Horizon Agents", "author": "Zijing Zhang", "abstract": "\\begin{abstract}\nThe development of autonomous agents for complex, long-horizon tasks is a central goal in AI. However, dominant training paradigms face a critical limitation: reinforcement learning (RL) methods that optimize solely for final task success often reinforce flawed or inefficient reasoning paths, a problem we term {\\bf inefficient exploration}. This leads to agents that are brittle and fail to generalize, as they learn to find solutions without learning {\\em how} to reason coherently. To address this, we introduce {\\bf \\method{}}, a novel framework that integrates dense, process-level supervision into end-to-end RL by rewarding verifiable, meta-reasoning behaviors. \\method{} equips an agent to explicitly tag its cognitive steps\u2014such as planning, exploration, and reflection\u2014and provides programmatic, rule-based rewards for actions that contribute to effective problem-solving. These process-centric rewards are combined with the final outcome signal and optimized using a critic-free policy\ngradient method. On the challenging ALFWorld and ScienceWorld benchmarks, \\method{} achieves new state-of-the-art results, with our 7B model reaching an 83.6\\% success rate on the most difficult unseen task split. Our analysis confirms these gains stem from improved reasoning quality, including significant reductions in redundant actions and enhanced error recovery, leading to more robust, efficient, and interpretable agents.\n\\end{abstract}", "citations": {"zeng2024agenttuning": {"bib_key": "zeng2024agenttuning", "bib_title": "AgentTuning: Enabling Generalized Agent Abilities for LLMs", "bib_author ": "Zeng, Aohan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The quest to build autonomous agents capable of solving complex, long-horizon tasks has gained significant momentum with the rise of Large Language Models (LLMs)~\\citep{zeng2024agenttuning, wang2022scienceworld, bai2024digirl}", "next_context": "."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "LLM AgentsLarge language models (LLMs) are now widely utilized as the core of agentic systems across diverse domains, including code generation~\\citep{huang2023agentcoder, zhang2024codeagent}, web interaction~\\citep{bai2024digirl, agashe2024agents, abuelsaad2024agente}, embodied intelligence~\\citep{zeng2024agenttuning, qiao2024WKM, fu2025agentrefine}", "next_context": ", and emotional intelligence~\\citep{sage}."}], "importance_score": 0.6666666666666666}, "wang2022scienceworld": {"bib_key": "wang2022scienceworld", "bib_title": "Scienceworld: Is your agent smarter than a 5th grader?", "bib_author ": "Wang, Ruoyao", "arxiv_id": null, "short_id": "2203.07540", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The quest to build autonomous agents capable of solving complex, long-horizon tasks has gained significant momentum with the rise of Large Language Models (LLMs)~\\citep{zeng2024agenttuning, wang2022scienceworld, bai2024digirl}", "next_context": "."}, {"section": "Experiment", "subsection": "Main Results", "subsubsection": null, "prev_context": "In addition to\\bfALFWorld, we also conduct experiments on\\bfScienceWorld~\\citep{wang2022scienceworld}", "next_context": ", which focuses on text-based scientific experimentation."}], "importance_score": 1.3333333333333333}, "bai2024digirl": {"bib_key": "bai2024digirl", "bib_title": "Digirl: Training in-the-wild device-control agents with autonomous reinforcement learning", "bib_author ": "Bai, Hao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The quest to build autonomous agents capable of solving complex, long-horizon tasks has gained significant momentum with the rise of Large Language Models (LLMs)~\\citep{zeng2024agenttuning, wang2022scienceworld, bai2024digirl}", "next_context": "."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "LLM AgentsLarge language models (LLMs) are now widely utilized as the core of agentic systems across diverse domains, including code generation~\\citep{huang2023agentcoder, zhang2024codeagent}, web interaction~\\citep{bai2024digirl, agashe2024agents, abuelsaad2024agente}", "next_context": ", embodied intelligence~\\citep{zeng2024agenttuning, qiao2024WKM, fu2025agentrefine}, and emotional intelligence~\\citep{sage}."}], "importance_score": 0.6666666666666666}, "chu2025sftrl": {"bib_key": "chu2025sftrl", "bib_title": "Sft memorizes, rl generalizes: A comparative study of foundation model post-training", "bib_author ": "Chu, Tianzhe", "arxiv_id": null, "short_id": "2501.17161", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "On one hand, Supervised Fine-Tuning (SFT) on expert trajectories can teach agents efficient behaviors, but these policies are often brittle and fail to generalize to novel situations~\\citep{chu2025sftrl}", "next_context": "."}], "importance_score": 1.0}, "martinez2006metacognition": {"bib_key": "martinez2006metacognition", "bib_title": "What is metacognition?", "bib_author ": "Martinez, Michael E", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Drawing inspiration from metacognitive theory~\\citep{martinez2006metacognition}", "next_context": ", which posits that effective problem-solving depends on``thinking about thinking'', we propose to directly reward beneficial cognitive behaviors."}, {"section": "Methodology", "subsection": "Operationalizing Meta-Reasoning in LLM Agents", "subsubsection": null, "prev_context": "Our approach is grounded in metacognitive theory~\\citep{martinez2006metacognition, lai2011metacognition}", "next_context": ", which emphasizes``thinking about thinking''."}], "importance_score": 1.5}, "shridhar2020alfworld": {"bib_key": "shridhar2020alfworld", "bib_title": "Alfworld: Aligning text and embodied environments for interactive learning", "bib_author ": "Shridhar, Mohit", "arxiv_id": null, "short_id": "2010.03768", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Inefficient Exploration in Long-Horizon Agents", "subsection": "Experimental Setup", "subsubsection": null, "prev_context": "BenchmarksTo evaluate foundational capabilities and generalization, we conduct experiments on the widely-used and challenging\\bfALFWorldbenchmark~\\citep{shridhar2020alfworld}", "next_context": ", which comprises embodied household tasks."}], "importance_score": 1.0}, "yao2023react": {"bib_key": "yao2023react", "bib_title": "React: Synergizing reasoning and acting in language models", "bib_author ": "Yao, Shunyu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Inefficient Exploration in Long-Horizon Agents", "subsection": "Experimental Setup", "subsubsection": null, "prev_context": "Training ParadigmsWe experiment with Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct models using the\\textbf{ReAct}~\\citep{yao2023react}", "next_context": "framework, which alternates between reasoning and acting steps."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Early approaches primarily leveraged existing pretrained models, employing sophisticated prompting strategies and external tools to enhance performance on complex tasks, as exemplified by methods like ReAct~\\citep{yao2023react, shinn2023reflexion}", "next_context": "."}], "importance_score": 1.5}, "yang2023gpt4tools": {"bib_key": "yang2023gpt4tools", "bib_title": "Gpt4tools: Teaching large language model to use tools via self-instruction", "bib_author ": "Yang, Rui", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Inefficient Exploration in Long-Horizon Agents", "subsection": "Experimental Setup", "subsubsection": null, "prev_context": "We evaluate two dominant training paradigms:\\item\\textbf{SFT}~\\citep{yang2023gpt4tools, tang2023toolalpaca, xi2024agentgym}", "next_context": ": A widely adopted paradigm that applies supervised fine-tuning on high-quality expert trajectories."}], "importance_score": 0.3333333333333333}, "tang2023toolalpaca": {"bib_key": "tang2023toolalpaca", "bib_title": "Toolalpaca: Generalized tool learning for language models with 3000 simulated cases", "bib_author ": "Tang, Qiaoyu", "arxiv_id": null, "short_id": "2306.05301", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Inefficient Exploration in Long-Horizon Agents", "subsection": "Experimental Setup", "subsubsection": null, "prev_context": "We evaluate two dominant training paradigms:\\item\\textbf{SFT}~\\citep{yang2023gpt4tools, tang2023toolalpaca, xi2024agentgym}", "next_context": ": A widely adopted paradigm that applies supervised fine-tuning on high-quality expert trajectories."}], "importance_score": 0.3333333333333333}, "xi2024agentgym": {"bib_key": "xi2024agentgym", "bib_title": "Agentgym: Evolving large language model-based agents across diverse environments", "bib_author ": "Xi, Zhiheng", "arxiv_id": null, "short_id": "2406.04151", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Inefficient Exploration in Long-Horizon Agents", "subsection": "Experimental Setup", "subsubsection": null, "prev_context": "We evaluate two dominant training paradigms:\\item\\textbf{SFT}~\\citep{yang2023gpt4tools, tang2023toolalpaca, xi2024agentgym}", "next_context": ": A widely adopted paradigm that applies supervised fine-tuning on high-quality expert trajectories."}, {"section": "Experiment", "subsection": "Main Results", "subsubsection": null, "prev_context": "For broader comparison, we also report the performance of GPT-4o, DeepSeek-V3/R1 and AgentGym~\\citep{xi2024agentgym}", "next_context": "."}, {"section": "Experiment", "subsection": "Main Results", "subsubsection": null, "prev_context": "AgentGym is trained on Llama-2-Chat-7B, first with behavior cloning on the AgentTraj~\\citep{xi2024agentgym}", "next_context": "dataset from multiple environments, and then further improved via exploration and self-evolution on a broader instruction set."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "To address this limitation, some studies employ supervised fine-tuning (SFT) to enhance the models' decision-making abilities~\\citep{zhang2024you, xi2024agentgym, qin2024toolllm}", "next_context": "."}], "importance_score": 2.6666666666666665}, "feng2025GRPO": {"bib_key": "feng2025GRPO", "bib_title": "Group-in-group policy optimization for llm agent training", "bib_author ": "Feng, Lang", "arxiv_id": null, "short_id": "2505.10978", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Inefficient Exploration in Long-Horizon Agents", "subsection": "Experimental Setup", "subsubsection": null, "prev_context": "\\item\\textbf{GRPO}~\\citep{feng2025GRPO, wang2025ragen}", "next_context": ": An end-to-end RL method that optimizes the policy by comparing the final rewards of multiple trajectories sampled from the same initial state."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Recently, group-based RL algorithms have emerged as a promising alternative, with methods like GRPO~\\citep{feng2025GRPO}", "next_context": ", Dr.GRPO~\\citep{liu2025DrGRPO}, and DAPO~\\citep{yu2025DAPO}estimating advantages by using batches of samples generated from the same prompt."}], "importance_score": 1.5}, "wang2025ragen": {"bib_key": "wang2025ragen", "bib_title": "Ragen: Understanding self-evolution in llm agents via multi-turn reinforcement learning", "bib_author ": "Wang, Zihan", "arxiv_id": null, "short_id": "2504.20073", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Inefficient Exploration in Long-Horizon Agents", "subsection": "Experimental Setup", "subsubsection": null, "prev_context": "\\item\\textbf{GRPO}~\\citep{feng2025GRPO, wang2025ragen}", "next_context": ": An end-to-end RL method that optimizes the policy by comparing the final rewards of multiple trajectories sampled from the same initial state."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "However, applying these RL methods to multi-turn, long-horizon tasks remains a significant challenge, primarily due to issues of sparse and delayed rewards~\\citep{wang2025ragen}", "next_context": ", which is the focus of our work."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "More recently, there has been a growing focus on end-to-end reinforcement learning for agents~\\citep{wang2025ragen, feng2025gigpo}", "next_context": ", which learn through direct, adaptive online interaction with an environment, thereby obviating the need for complex data preparation or step-level reward models."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Nevertheless, these approaches still grapple with challenges in fine-grained credit assignment and generalization~\\citep{wang2025ragen}", "next_context": "."}], "importance_score": 3.0}, "yuan2025agentr": {"bib_key": "yuan2025agentr", "bib_title": "Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training", "bib_author ": "Yuan, Siyu", "arxiv_id": null, "short_id": "2501.11425", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Inefficient Exploration in Long-Horizon Agents", "subsection": "Experimental Setup", "subsubsection": null, "prev_context": "\\item\\bfRepetitive Action Rate (\\%,\\downarrow):The percentage of steps where the agent executes a\\bfmeaningless repeated action, as defined in prior work~\\citep{yuan2025agentr, fu2025agentrefine, feng2025gigpo}", "next_context": "."}, {"section": "Experiment", "subsection": "Exploration Efficiency", "subsubsection": null, "prev_context": "Low rates of repetitive and invalid actions imply that when the agent encounters an error or a dead end, it is less likely to get stuck in a loop~\\citep{yuan2025agentr}", "next_context": "."}], "importance_score": 1.3333333333333333}, "fu2025agentrefine": {"bib_key": "fu2025agentrefine", "bib_title": "Agentrefine: Enhancing agent generalization through refinement tuning", "bib_author ": "Fu, Dayuan", "arxiv_id": null, "short_id": "2501.01702", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Inefficient Exploration in Long-Horizon Agents", "subsection": "Experimental Setup", "subsubsection": null, "prev_context": "\\item\\bfRepetitive Action Rate (\\%,\\downarrow):The percentage of steps where the agent executes a\\bfmeaningless repeated action, as defined in prior work~\\citep{yuan2025agentr, fu2025agentrefine, feng2025gigpo}", "next_context": "."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "LLM AgentsLarge language models (LLMs) are now widely utilized as the core of agentic systems across diverse domains, including code generation~\\citep{huang2023agentcoder, zhang2024codeagent}, web interaction~\\citep{bai2024digirl, agashe2024agents, abuelsaad2024agente}, embodied intelligence~\\citep{zeng2024agenttuning, qiao2024WKM, fu2025agentrefine}", "next_context": ", and emotional intelligence~\\citep{sage}."}], "importance_score": 0.6666666666666666}, "feng2025gigpo": {"bib_key": "feng2025gigpo", "bib_title": "Group-in-group policy optimization for llm agent training", "bib_author ": "Feng, Lang", "arxiv_id": null, "short_id": "2505.10978", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Inefficient Exploration in Long-Horizon Agents", "subsection": "Experimental Setup", "subsubsection": null, "prev_context": "\\item\\bfRepetitive Action Rate (\\%,\\downarrow):The percentage of steps where the agent executes a\\bfmeaningless repeated action, as defined in prior work~\\citep{yuan2025agentr, fu2025agentrefine, feng2025gigpo}", "next_context": "."}, {"section": "Experiment", "subsection": "Main Results", "subsubsection": null, "prev_context": "\\item\\textbf{GiGPO}~\\citep{feng2025gigpo}", "next_context": ": A competitive method that introduces a two-level structure for finer-grained credit assignment."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "More recently, there has been a growing focus on end-to-end reinforcement learning for agents~\\citep{wang2025ragen, feng2025gigpo}", "next_context": ", which learn through direct, adaptive online interaction with an environment, thereby obviating the need for complex data preparation or step-level reward models."}], "importance_score": 1.8333333333333333}, "lai2011metacognition": {"bib_key": "lai2011metacognition", "bib_title": "Metacognition: A literature review", "bib_author ": "Lai, Emily R", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methodology", "subsection": "Operationalizing Meta-Reasoning in LLM Agents", "subsubsection": null, "prev_context": "Our approach is grounded in metacognitive theory~\\citep{martinez2006metacognition, lai2011metacognition}", "next_context": ", which emphasizes``thinking about thinking''."}], "importance_score": 0.5}, "song2024ETO": {"bib_key": "song2024ETO", "bib_title": "Trial and error: Exploration-based trajectory optimization for llm agents", "bib_author ": "Song, Yifan", "arxiv_id": null, "short_id": "2403.02502", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiment", "subsection": "Main Results", "subsubsection": null, "prev_context": "We evaluate our approach against two more commonly-used training paradigms in addition to SFT and GRPO:\\item\\textbf{ETO}~\\citep{song2024ETO}", "next_context": ": A RL method that iteratively refines actions using step-level feedback along trajectories."}], "importance_score": 1.0}, "ouyang2022RLHF": {"bib_key": "ouyang2022RLHF", "bib_title": "Training language models to follow instructions with human feedback", "bib_author ": "Ouyang, Long", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "~\\citep{ouyang2022RLHF}", "next_context": "and Direct Preference Optimization (DPO)~\\citep{rafailov2023DPO}."}], "importance_score": 1.0}, "rafailov2023DPO": {"bib_key": "rafailov2023DPO", "bib_title": "Direct preference optimization: Your language model is secretly a reward model", "bib_author ": "Rafailov, Rafael", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "~\\citep{ouyang2022RLHF}and Direct Preference Optimization (DPO)~\\citep{rafailov2023DPO}", "next_context": "."}], "importance_score": 1.0}, "hu2025open": {"bib_key": "hu2025open", "bib_title": "Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model", "bib_author ": "Hu, Jingcheng", "arxiv_id": null, "short_id": "2503.24290", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Beyond alignment, recent work has also leveraged RL to enhance other crucial LLM capabilities, such as reasoning~\\citep{hu2025open, muennighoff2025s1}", "next_context": "and emotional intelligence~\\citep{rlver}."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "These approaches have demonstrated significant effectiveness in tasks such as mathematical reasoning, search, and tool use~\\citep{yu2025DAPO, hu2025open}", "next_context": "."}], "importance_score": 1.0}, "muennighoff2025s1": {"bib_key": "muennighoff2025s1", "bib_title": "s1: Simple test-time scaling", "bib_author ": "Muennighoff, Niklas", "arxiv_id": null, "short_id": "2501.19393", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Beyond alignment, recent work has also leveraged RL to enhance other crucial LLM capabilities, such as reasoning~\\citep{hu2025open, muennighoff2025s1}", "next_context": "and emotional intelligence~\\citep{rlver}."}], "importance_score": 0.5}, "rlver": {"bib_key": "rlver", "bib_title": "RLVER: Reinforcement Learning with Verifiable Emotion Rewards for Empathetic Agents", "bib_author ": "Peisong Wang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Beyond alignment, recent work has also leveraged RL to enhance other crucial LLM capabilities, such as reasoning~\\citep{hu2025open, muennighoff2025s1}and emotional intelligence~\\citep{rlver}", "next_context": "."}], "importance_score": 1.0}, "liu2025DrGRPO": {"bib_key": "liu2025DrGRPO", "bib_title": "Understanding r1-zero-like training: A critical perspective", "bib_author ": "Liu, Zichen", "arxiv_id": null, "short_id": "2503.20783", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Recently, group-based RL algorithms have emerged as a promising alternative, with methods like GRPO~\\citep{feng2025GRPO}, Dr.GRPO~\\citep{liu2025DrGRPO}", "next_context": ", and DAPO~\\citep{yu2025DAPO}estimating advantages by using batches of samples generated from the same prompt."}], "importance_score": 1.0}, "yu2025DAPO": {"bib_key": "yu2025DAPO", "bib_title": "Dapo: An open-source llm reinforcement learning system at scale", "bib_author ": "Yu, Qiying", "arxiv_id": null, "short_id": "2503.14476", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Recently, group-based RL algorithms have emerged as a promising alternative, with methods like GRPO~\\citep{feng2025GRPO}, Dr.GRPO~\\citep{liu2025DrGRPO}, and DAPO~\\citep{yu2025DAPO}", "next_context": "estimating advantages by using batches of samples generated from the same prompt."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "These approaches have demonstrated significant effectiveness in tasks such as mathematical reasoning, search, and tool use~\\citep{yu2025DAPO, hu2025open}", "next_context": "."}], "importance_score": 1.5}, "schulman2017PPO": {"bib_key": "schulman2017PPO", "bib_title": "Proximal policy optimization algorithms", "bib_author ": "Schulman, John", "arxiv_id": null, "short_id": "1707.06347", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "In contrast to actor-critic methods like PPO~\\citep{schulman2017PPO}", "next_context": ", this approach to advantage estimation does not require an additional critic model, making large-scale RL training for LLMs more computationally efficient and practical."}], "importance_score": 1.0}, "huang2023agentcoder": {"bib_key": "huang2023agentcoder", "bib_title": "Agentcoder: Multi-agent-based code generation with iterative testing and optimisation", "bib_author ": "Huang, Dong", "arxiv_id": null, "short_id": "2312.13010", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "LLM AgentsLarge language models (LLMs) are now widely utilized as the core of agentic systems across diverse domains, including code generation~\\citep{huang2023agentcoder, zhang2024codeagent}", "next_context": ", web interaction~\\citep{bai2024digirl, agashe2024agents, abuelsaad2024agente}, embodied intelligence~\\citep{zeng2024agenttuning, qiao2024WKM, fu2025agentrefine}, and emotional intelligence~\\citep{sage}."}], "importance_score": 0.5}, "zhang2024codeagent": {"bib_key": "zhang2024codeagent", "bib_title": "CodeAgent: Enhancing Code Generation with Tool-Integrated Agent Systems\nfor Real-World Repo-level Coding Challenges", "bib_author ": "Kechi Zhang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "LLM AgentsLarge language models (LLMs) are now widely utilized as the core of agentic systems across diverse domains, including code generation~\\citep{huang2023agentcoder, zhang2024codeagent}", "next_context": ", web interaction~\\citep{bai2024digirl, agashe2024agents, abuelsaad2024agente}, embodied intelligence~\\citep{zeng2024agenttuning, qiao2024WKM, fu2025agentrefine}, and emotional intelligence~\\citep{sage}."}], "importance_score": 0.5}, "agashe2024agents": {"bib_key": "agashe2024agents", "bib_title": "Agent s: An open agentic framework that uses computers like a human", "bib_author ": "Agashe, Saaket", "arxiv_id": null, "short_id": "2410.08164", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "LLM AgentsLarge language models (LLMs) are now widely utilized as the core of agentic systems across diverse domains, including code generation~\\citep{huang2023agentcoder, zhang2024codeagent}, web interaction~\\citep{bai2024digirl, agashe2024agents, abuelsaad2024agente}", "next_context": ", embodied intelligence~\\citep{zeng2024agenttuning, qiao2024WKM, fu2025agentrefine}, and emotional intelligence~\\citep{sage}."}], "importance_score": 0.3333333333333333}, "abuelsaad2024agente": {"bib_key": "abuelsaad2024agente", "bib_title": "Agent-e: From autonomous web navigation to foundational design principles in agentic systems", "bib_author ": "Abuelsaad, Tamer", "arxiv_id": null, "short_id": "2407.13032", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "LLM AgentsLarge language models (LLMs) are now widely utilized as the core of agentic systems across diverse domains, including code generation~\\citep{huang2023agentcoder, zhang2024codeagent}, web interaction~\\citep{bai2024digirl, agashe2024agents, abuelsaad2024agente}", "next_context": ", embodied intelligence~\\citep{zeng2024agenttuning, qiao2024WKM, fu2025agentrefine}, and emotional intelligence~\\citep{sage}."}], "importance_score": 0.3333333333333333}, "qiao2024WKM": {"bib_key": "qiao2024WKM", "bib_title": "Agent planning with world knowledge model", "bib_author ": "Qiao, Shuofei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "LLM AgentsLarge language models (LLMs) are now widely utilized as the core of agentic systems across diverse domains, including code generation~\\citep{huang2023agentcoder, zhang2024codeagent}, web interaction~\\citep{bai2024digirl, agashe2024agents, abuelsaad2024agente}, embodied intelligence~\\citep{zeng2024agenttuning, qiao2024WKM, fu2025agentrefine}", "next_context": ", and emotional intelligence~\\citep{sage}."}], "importance_score": 0.3333333333333333}, "sage": {"bib_key": "sage", "bib_title": "Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models", "bib_author ": "Bang Zhang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "LLM AgentsLarge language models (LLMs) are now widely utilized as the core of agentic systems across diverse domains, including code generation~\\citep{huang2023agentcoder, zhang2024codeagent}, web interaction~\\citep{bai2024digirl, agashe2024agents, abuelsaad2024agente}, embodied intelligence~\\citep{zeng2024agenttuning, qiao2024WKM, fu2025agentrefine}, and emotional intelligence~\\citep{sage}", "next_context": "."}], "importance_score": 1.0}, "shinn2023reflexion": {"bib_key": "shinn2023reflexion", "bib_title": "Reflexion: Language agents with verbal reinforcement learning", "bib_author ": "Shinn, Noah", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Early approaches primarily leveraged existing pretrained models, employing sophisticated prompting strategies and external tools to enhance performance on complex tasks, as exemplified by methods like ReAct~\\citep{yao2023react, shinn2023reflexion}", "next_context": "."}], "importance_score": 0.5}, "zhang2024you": {"bib_key": "zhang2024you", "bib_title": "You Only Look at Screens: Multimodal Chain-of-Action Agents", "bib_author ": "Zhang, Zhuosheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "To address this limitation, some studies employ supervised fine-tuning (SFT) to enhance the models' decision-making abilities~\\citep{zhang2024you, xi2024agentgym, qin2024toolllm}", "next_context": "."}], "importance_score": 0.3333333333333333}, "qin2024toolllm": {"bib_key": "qin2024toolllm", "bib_title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs", "bib_author ": "Qin, Yujia", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "To address this limitation, some studies employ supervised fine-tuning (SFT) to enhance the models' decision-making abilities~\\citep{zhang2024you, xi2024agentgym, qin2024toolllm}", "next_context": "."}], "importance_score": 0.3333333333333333}, "yu2024steptool": {"bib_key": "yu2024steptool", "bib_title": "Steptool: A step-grained reinforcement learning framework for tool learning in llms", "bib_author ": "Yu, Yuanqing", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Other approaches explore single-step or offline reinforcement learning to further augment agent performance~\\citep{yu2024steptool,xiong2024watch,zhou2024archer}", "next_context": "."}], "importance_score": 0.3333333333333333}, "xiong2024watch": {"bib_key": "xiong2024watch", "bib_title": "Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement", "bib_author ": "Xiong, Weimin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Other approaches explore single-step or offline reinforcement learning to further augment agent performance~\\citep{yu2024steptool,xiong2024watch,zhou2024archer}", "next_context": "."}], "importance_score": 0.3333333333333333}, "zhou2024archer": {"bib_key": "zhou2024archer", "bib_title": "ArCHer: training language model agents via hierarchical multi-turn RL", "bib_author ": "Zhou, Yifei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Other approaches explore single-step or offline reinforcement learning to further augment agent performance~\\citep{yu2024steptool,xiong2024watch,zhou2024archer}", "next_context": "."}], "importance_score": 0.3333333333333333}}, "refs": [], "table": [{"original": "\\begin{table*}[t]\n\\centering\n\\setlength{\\tabcolsep}{8pt}\n\\caption{Performance comparison on the ALFWorld and ScienceWorld benchmarks. We report the success rate (\\%) on seen (L0: {\\color{ngreen} seen task variants and categories}) and unseen (L1: {\\color{nred} unseen task variants} but {\\color{ngreen} seen task categories}; L2: {\\color{nred} unseen task variants and categories}) task variations. Our method, RLVMR, consistently outperforms all baselines across both benchmarks and model sizes.}\n\\begin{tabular}{llrrrrrr}\n\\toprule\n\\multirow{2}{*}{\\textbf{Model}}\n& \\multirow{2}{*}{\\textbf{Method}}\n& \\multicolumn{3}{c}{\\textbf{ALFWorld}}\n& \\multicolumn{3}{c}{\\textbf{ScienceWorld}} \\\\\n\\cmidrule(lr){3-5} \\cmidrule(lr){6-8}\n& & \\bf L0 & \\bf L1 & \\bf L2 & \\bf L0 & \\bf L1 & \\bf L2\\\\\n\\midrule\nAgentGym & SFT+RL & 76.6 & 63.3 & -- & 46.9 & 33.6 & -- \\\\\n\\hdashline\nGPT-4o   & \\multirow{3}{*}{ReAct} & 57.3 & 66.0 & 68.8 & 45.4 & 49.2 & 41.0 \\\\\nDeepSeek-V3 & & 60.2 & 65.9 & 53.9 & 27.3 & 35.2 & 26.5 \\\\\nDeepSeek-R1 & & 68.8 & 70.2 & 67.3 & 22.2 & 31.4 & 29.1  \\\\\n\\midrule\n\\multirow{6}{*}{Qwen-1.5B}\n & ReAct & 11.3 & 13.7 & 10.2 & 1.2 & 0.8 & 0.8 \\\\\n & +~SFT   & 43.0 & 38.7 & 17.6 & 20.3 & 18.0 & 12.5 \\\\\n & +~ETO  & 64.1 & 66.4 & 25.8 & 39.1 & 22.7 & 15.6 \\\\\n & +~GRPO  & 76.6 & 71.1 & 29.7 & 21.1 & 13.7 & 10.9 \\\\\n & +~GiGPO & 86.7 & 83.2 & 48.0 & 25.8 & 15.2 & 4.7 \\\\\n & +~RLVMR  & \\textbf{89.1} & \\textbf{87.9} & \\textbf{56.3} & \\textbf{46.9} & \\textbf{34.4} & \\textbf{26.5} \\\\\n\\midrule\n\\multirow{6}{*}{Qwen-7B}\n & ReAct & 23.1 & 28.5 & 27.0 & 7.8 & 11.3 & 6.3 \\\\\n & +~SFT   & 63.3 & 57.0 & 37.5 & 36.7 & 32.0 & 23.4 \\\\\n & +~ETO  & 70.3 & 74.2 & 51.6 & 62.5 & 40.6 & 28.1 \\\\\n & +~GRPO  & 79.3 & 77.3 & 52.3 & 49.1 & 30.1 & 26.6 \\\\\n & +~GiGPO & 89.5 & 90.2 & 67.2 & 53.4 & 35.2 & 25.8 \\\\\n & +~RLVMR  & \\textbf{91.4} & \\textbf{91.8} & \\textbf{83.6} & \\textbf{67.2} & \\textbf{43.0} & \\textbf{32.2} \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:main-results}\n\\end{table*}", "caption": "\\caption{Performance comparison on the ALFWorld and ScienceWorld benchmarks. We report the success rate (\\%) on seen (L0: {\\color{ngreen} seen task variants and categories}) and unseen (L1: {\\color{nred} unseen task variants} but {\\color{ngreen} seen task categories}; L2: {\\color{nred} unseen task variants and categories}) task variations. Our method, RLVMR, consistently outperforms all baselines across both benchmarks and model sizes.}", "label": "\\label{tab:main-results}", "tabular": "\\begin{tabular}{llrrrrrr}\n\\toprule\n\\multirow{2}{*}{\\textbf{Model}}\n& \\multirow{2}{*}{\\textbf{Method}}\n& \\multicolumn{3}{c}{\\textbf{ALFWorld}}\n& \\multicolumn{3}{c}{\\textbf{ScienceWorld}} \\\\\n\\cmidrule(lr){3-5} \\cmidrule(lr){6-8}\n& & \\bf L0 & \\bf L1 & \\bf L2 & \\bf L0 & \\bf L1 & \\bf L2\\\\\n\\midrule\nAgentGym & SFT+RL & 76.6 & 63.3 & -- & 46.9 & 33.6 & -- \\\\\n\\hdashline\nGPT-4o   & \\multirow{3}{*}{ReAct} & 57.3 & 66.0 & 68.8 & 45.4 & 49.2 & 41.0 \\\\\nDeepSeek-V3 & & 60.2 & 65.9 & 53.9 & 27.3 & 35.2 & 26.5 \\\\\nDeepSeek-R1 & & 68.8 & 70.2 & 67.3 & 22.2 & 31.4 & 29.1  \\\\\n\\midrule\n\\multirow{6}{*}{Qwen-1.5B}\n & ReAct & 11.3 & 13.7 & 10.2 & 1.2 & 0.8 & 0.8 \\\\\n & +~SFT   & 43.0 & 38.7 & 17.6 & 20.3 & 18.0 & 12.5 \\\\\n & +~ETO  & 64.1 & 66.4 & 25.8 & 39.1 & 22.7 & 15.6 \\\\\n & +~GRPO  & 76.6 & 71.1 & 29.7 & 21.1 & 13.7 & 10.9 \\\\\n & +~GiGPO & 86.7 & 83.2 & 48.0 & 25.8 & 15.2 & 4.7 \\\\\n & +~RLVMR  & \\textbf{89.1} & \\textbf{87.9} & \\textbf{56.3} & \\textbf{46.9} & \\textbf{34.4} & \\textbf{26.5} \\\\\n\\midrule\n\\multirow{6}{*}{Qwen-7B}\n & ReAct & 23.1 & 28.5 & 27.0 & 7.8 & 11.3 & 6.3 \\\\\n & +~SFT   & 63.3 & 57.0 & 37.5 & 36.7 & 32.0 & 23.4 \\\\\n & +~ETO  & 70.3 & 74.2 & 51.6 & 62.5 & 40.6 & 28.1 \\\\\n & +~GRPO  & 79.3 & 77.3 & 52.3 & 49.1 & 30.1 & 26.6 \\\\\n & +~GiGPO & 89.5 & 90.2 & 67.2 & 53.4 & 35.2 & 25.8 \\\\\n & +~RLVMR  & \\textbf{91.4} & \\textbf{91.8} & \\textbf{83.6} & \\textbf{67.2} & \\textbf{43.0} & \\textbf{32.2} \\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[t]\n\\centering\n\\caption{Exploration efficiency of our approach.}\n\\begin{tabular}{l rrr rrr rrr}\n\\toprule\n\\multirow{2}{*}{\\textbf{Method}}\n& \\multicolumn{3}{c}{\\textbf{Success Rate ($\\uparrow$)}} & \\multicolumn{3}{c}{\\textbf{\\# Actions ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{Redudance ($\\downarrow$)}}\\\\\n\\cmidrule(lr){2-4} \\cmidrule(lr){5-7}  \\cmidrule(lr){8-10}\n& \\bf L0 & \\bf L1 & \\bf L2 & \\bf L0 & \\bf L1 & \\bf L2 & \\bf L0 & \\bf L1 & \\bf L2\\\\\n\\midrule\n\\multicolumn{10}{c}{\\bf \\em Qwen2.5-1.5B-Instruct}\\\\\n\\bf ReAct   & 11.3 & 13.7 & 10.2 & 76.3 & 79.3 & 82.5 & 10.3 & 11.3 & 16.6\\\\\n\\bf +~SFT   & 43.0 & 38.7 & 17.6 & 8.0 & 11.8 & 24.9 & 10.7 & 20.2 & 21.4\\\\\n\\bf +~GRPO  & 76.6 & 71.1 & 29.7 & 18.8 & 17.3 & 21.6 & 18.4 & 17.6 & 27.1\\\\\n\\bf +~\\method{}  & 89.1 & 87.9 & 56.3 & 11.1 & 14.0 & 12.5 & 6.1 & 4.4 & 5.7\\\\\n\\midrule\n\\multicolumn{10}{c}{\\bf \\em Qwen2.5-7B-Instruct}\\\\\n\\bf ReAct   & 23.1 & 28.5 & 27.0 & 13.0 & 14.7 & 13.3 & 49.0 & 53.3 & 49.6\\\\\n\\bf +~SFT   & 63.3 & 57.0 & 37.5 & 6.2 & 5.5 & 6.3 & 13.9 & 24.5 & 14.4\\\\\n\\bf +~GRPO  & 79.3 & 77.3 & 52.3 & 14.8 & 25.4 & 13.5 & 21.5 & 20.3 & 31.2\\\\\n\\bf +~\\method{}  & 91.4 & 91.8 & 83.6 & 5.6 & 6.5 & 7.1 & 2.3 & 7.8 & 11.7\\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:main-results}\n\\end{table*}", "caption": "\\caption{Exploration efficiency of our approach.}", "label": "\\label{tab:main-results}", "tabular": "\\begin{tabular}{l rrr rrr rrr}\n\\toprule\n\\multirow{2}{*}{\\textbf{Method}}\n& \\multicolumn{3}{c}{\\textbf{Success Rate ($\\uparrow$)}} & \\multicolumn{3}{c}{\\textbf{\\# Actions ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{Redudance ($\\downarrow$)}}\\\\\n\\cmidrule(lr){2-4} \\cmidrule(lr){5-7}  \\cmidrule(lr){8-10}\n& \\bf L0 & \\bf L1 & \\bf L2 & \\bf L0 & \\bf L1 & \\bf L2 & \\bf L0 & \\bf L1 & \\bf L2\\\\\n\\midrule\n\\multicolumn{10}{c}{\\bf \\em Qwen2.5-1.5B-Instruct}\\\\\n\\bf ReAct   & 11.3 & 13.7 & 10.2 & 76.3 & 79.3 & 82.5 & 10.3 & 11.3 & 16.6\\\\\n\\bf +~SFT   & 43.0 & 38.7 & 17.6 & 8.0 & 11.8 & 24.9 & 10.7 & 20.2 & 21.4\\\\\n\\bf +~GRPO  & 76.6 & 71.1 & 29.7 & 18.8 & 17.3 & 21.6 & 18.4 & 17.6 & 27.1\\\\\n\\bf +~\\method{}  & 89.1 & 87.9 & 56.3 & 11.1 & 14.0 & 12.5 & 6.1 & 4.4 & 5.7\\\\\n\\midrule\n\\multicolumn{10}{c}{\\bf \\em Qwen2.5-7B-Instruct}\\\\\n\\bf ReAct   & 23.1 & 28.5 & 27.0 & 13.0 & 14.7 & 13.3 & 49.0 & 53.3 & 49.6\\\\\n\\bf +~SFT   & 63.3 & 57.0 & 37.5 & 6.2 & 5.5 & 6.3 & 13.9 & 24.5 & 14.4\\\\\n\\bf +~GRPO  & 79.3 & 77.3 & 52.3 & 14.8 & 25.4 & 13.5 & 21.5 & 20.3 & 31.2\\\\\n\\bf +~\\method{}  & 91.4 & 91.8 & 83.6 & 5.6 & 6.5 & 7.1 & 2.3 & 7.8 & 11.7\\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[t]\n\\centering\n%\\renewcommand{\\arraystretch}{1.2}\n\\caption{Comparison of the average lengths of action sequences of different methods across various scenarios. A shorter length (number of actions) indicates higher efficiency.}\n\\begin{tabular}{ll ccccc}\n\\toprule\n\\textbf{Environment} & \\textbf{Level}   &   \\bf Vanilla &   \\bf SFT & \\textbf{GRPO} & \\textbf{GiGPO} & \\textbf{RLVMR} \\\\\n\\midrule\n\\multirow{3}{*}{ALFWorld}    \n    & Seen-L0       & 28.8 & 23.2 & 15.8 & 12.6 & \\textbf{10.8}\\\\\n    & Unseen-L1     & 29.1 & 24.5 & 18.1 & 14.7 & \\textbf{11.6}\\\\\n    & Unseen-L2     & 28.9 & 27.5 & 21.7 & 19.4 & \\textbf{15.4}\\\\\n\\midrule\n\\multirow{3}{*}{ScienceWorld}\n    & Seen-L0       & 25.8 & 22.9 & 15.4 & 14.7 & \\textbf{12.5}\\\\\n    & Unseen-L1     & 27.9 & 25.7 & 26.7 & 25.2 & \\textbf{18.8}\\\\\n    & Unseen-L2     & 26.8 & 26.2 & 27.6 & 26.3 & \\textbf{20.5}\\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:len_comparison}\n\\end{table}", "caption": "\\caption{Comparison of the average lengths of action sequences of different methods across various scenarios. A shorter length (number of actions) indicates higher efficiency.}", "label": "\\label{tab:len_comparison}", "tabular": "\\begin{tabular}{ll ccccc}\n\\toprule\n\\textbf{Environment} & \\textbf{Level}   &   \\bf Vanilla &   \\bf SFT & \\textbf{GRPO} & \\textbf{GiGPO} & \\textbf{RLVMR} \\\\\n\\midrule\n\\multirow{3}{*}{ALFWorld}    \n    & Seen-L0       & 28.8 & 23.2 & 15.8 & 12.6 & \\textbf{10.8}\\\\\n    & Unseen-L1     & 29.1 & 24.5 & 18.1 & 14.7 & \\textbf{11.6}\\\\\n    & Unseen-L2     & 28.9 & 27.5 & 21.7 & 19.4 & \\textbf{15.4}\\\\\n\\midrule\n\\multirow{3}{*}{ScienceWorld}\n    & Seen-L0       & 25.8 & 22.9 & 15.4 & 14.7 & \\textbf{12.5}\\\\\n    & Unseen-L1     & 27.9 & 25.7 & 26.7 & 25.2 & \\textbf{18.8}\\\\\n    & Unseen-L2     & 26.8 & 26.2 & 27.6 & 26.3 & \\textbf{20.5}\\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[t]\n\\centering\n\\caption{Ablation results on ALFWorld and ScienceWorld (success rates (\\%) on L2 variant).}\n\\begin{tabular}{lrr}\n\\toprule\n\\textbf{Variant} & \\textbf{ALFWorld} & \\textbf{ScienceWorld} \\\\\n\\midrule\n\\method{} (Full) & \\textbf{56.3} & \\textbf{26.5} \\\\\n\\quad w/o $A^{\\text{T}}$ (Outcome Reward) & 12.5 & 7.8 \\\\\n\\quad w/o $A^{\\text{MC}}$ (Meta-Reasoning Reward) & 45.3 & 20.3 \\\\\n\\quad w/o CS (Cold-Start) & 40.6 & 18.8 \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:ablation}\n\\end{table}", "caption": "\\caption{Ablation results on ALFWorld and ScienceWorld (success rates (\\%) on L2 variant).}", "label": "\\label{tab:ablation}", "tabular": "\\begin{tabular}{lrr}\n\\toprule\n\\textbf{Variant} & \\textbf{ALFWorld} & \\textbf{ScienceWorld} \\\\\n\\midrule\n\\method{} (Full) & \\textbf{56.3} & \\textbf{26.5} \\\\\n\\quad w/o $A^{\\text{T}}$ (Outcome Reward) & 12.5 & 7.8 \\\\\n\\quad w/o $A^{\\text{MC}}$ (Meta-Reasoning Reward) & 45.3 & 20.3 \\\\\n\\quad w/o CS (Cold-Start) & 40.6 & 18.8 \\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n\\centering\n\\begin{tabular}{lccccccc}\n\\toprule\n\\textbf{Method} & \\textbf{Pick} & \\textbf{Look} & \\textbf{Clean} & \\textbf{Heat} & \\textbf{Cool} & \\textbf{Pick2} & \\textbf{All} \\\\\n\\midrule\nReAct   & 23.1 & 18.3 & 10.8 &  8.7 &  3.5 &  0.0 & 13.7 \\\\\n+SFT    & 43.2 & 42.0 & 35.9 & 33.2 & 29.4 & 29.7 & 38.7 \\\\\n+ETO    & 73.6 & 46.3 & 66.2 & 68.3 & 62.8 & 55.6 & 66.4 \\\\\n+GRPO   & 80.3 & 55.6 & 88.1 & 76.2 & 62.0 & 72.1 & 71.1 \\\\\n+GiGPO  & 92.8 & 66.5 & 90.7 & 90.9 & 80.2 & 73.8 & 83.2 \\\\\n+RLVMR  & 95.2 & 78.8 & 91.2 & 90.2 & 83.9 & 77.6 & 87.9 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Success rates on ALFWorld with Qwen2.5-1.5B model.}\n\\label{tab:alfworld-1.5B}\n\\end{table*}", "caption": "\\caption{Success rates on ALFWorld with Qwen2.5-1.5B model.}", "label": "\\label{tab:alfworld-1.5B}", "tabular": "\\begin{tabular}{lccccccc}\n\\toprule\n\\textbf{Method} & \\textbf{Pick} & \\textbf{Look} & \\textbf{Clean} & \\textbf{Heat} & \\textbf{Cool} & \\textbf{Pick2} & \\textbf{All} \\\\\n\\midrule\nReAct   & 23.1 & 18.3 & 10.8 &  8.7 &  3.5 &  0.0 & 13.7 \\\\\n+SFT    & 43.2 & 42.0 & 35.9 & 33.2 & 29.4 & 29.7 & 38.7 \\\\\n+ETO    & 73.6 & 46.3 & 66.2 & 68.3 & 62.8 & 55.6 & 66.4 \\\\\n+GRPO   & 80.3 & 55.6 & 88.1 & 76.2 & 62.0 & 72.1 & 71.1 \\\\\n+GiGPO  & 92.8 & 66.5 & 90.7 & 90.9 & 80.2 & 73.8 & 83.2 \\\\\n+RLVMR  & 95.2 & 78.8 & 91.2 & 90.2 & 83.9 & 77.6 & 87.9 \\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[h]\n\\centering\n\\begin{tabular}{lccccccc}\n\\toprule\n\\textbf{Method} & \\textbf{Pick} & \\textbf{Look} & \\textbf{Clean} & \\textbf{Heat} & \\textbf{Cool} & \\textbf{Pick2} & \\textbf{All} \\\\\n\\midrule\nReAct   & 43.1 & 33.2 & 18.7 & 16.4 & 20.2 & 12.8 & 28.5 \\\\\n+SFT    & 70.8 & 63.0 & 61.1 & 46.3 & 49.7 & 33.2 & 57.0 \\\\\n+ETO    & 88.2 & 70.5 & 82.3 & 83.6 & 71.0 & 51.2 & 74.2 \\\\\n+GRPO   & 90.2 & 76.7 & 86.0 & 80.1 & 68.3 & 56.4 & 77.3 \\\\\n+GiGPO  & 91.7 & 85.9 & 93.3 & 90.3 & 89.0 & 83.6 & 90.2 \\\\\n+RLVMR  & 95.3 & 88.2 & 90.1 & 92.4 & 89.8 & 86.7 & 91.8 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Success rates on ALFWorld with Qwen2.5-7B model.}\n\\label{tab:alfworld-7B}\n\\end{table*}", "caption": "\\caption{Success rates on ALFWorld with Qwen2.5-7B model.}", "label": "\\label{tab:alfworld-7B}", "tabular": "\\begin{tabular}{lccccccc}\n\\toprule\n\\textbf{Method} & \\textbf{Pick} & \\textbf{Look} & \\textbf{Clean} & \\textbf{Heat} & \\textbf{Cool} & \\textbf{Pick2} & \\textbf{All} \\\\\n\\midrule\nReAct   & 43.1 & 33.2 & 18.7 & 16.4 & 20.2 & 12.8 & 28.5 \\\\\n+SFT    & 70.8 & 63.0 & 61.1 & 46.3 & 49.7 & 33.2 & 57.0 \\\\\n+ETO    & 88.2 & 70.5 & 82.3 & 83.6 & 71.0 & 51.2 & 74.2 \\\\\n+GRPO   & 90.2 & 76.7 & 86.0 & 80.1 & 68.3 & 56.4 & 77.3 \\\\\n+GiGPO  & 91.7 & 85.9 & 93.3 & 90.3 & 89.0 & 83.6 & 90.2 \\\\\n+RLVMR  & 95.3 & 88.2 & 90.1 & 92.4 & 89.8 & 86.7 & 91.8 \\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}], "figure": [{"original": "\\begin{figure}[ht]\n  \\centering\n  \\subfloat[Model Performance on ALFWorld]{\\includegraphics[width=0.48\\linewidth]{figures/front_page_performance.pdf}} \\hfill\n  \\subfloat[Repetitive Action Rate]{\\includegraphics[width=0.48\\linewidth]{figures/front_page_repetitive.pdf}}\\\\\n  \\caption{Reinforcement learning with outcome-only rewards (e.g., GRPO) improves performance over vanilla models but fosters inefficient exploration, characterized by high rates of repetitive actions that hinder generalization to unseen tasks. In contrast, our proposed \\method{} significantly improves success rates and generalization by directly mitigating this inefficient exploration.}\n  \\label{fig:front_page}\n\\end{figure}", "caption": "\\caption{Reinforcement learning with outcome-only rewards (e.g., GRPO) improves performance over vanilla models but fosters inefficient exploration, characterized by high rates of repetitive actions that hinder generalization to unseen tasks. In contrast, our proposed \\method{} significantly improves success rates and generalization by directly mitigating this inefficient exploration.}", "label": "\\label{fig:front_page}", "subfigures": [], "figure_paths": ["figures/front_page_performance.pdf", "figures/front_page_repetitive.pdf"]}, {"original": "\\begin{figure}[ht]\n  \\centering\n  \\subfloat[Standard RLVR (GRPO)]{\\includegraphics[width=0.45\\linewidth]{figures/RLVR.pdf}} \\hspace{0.05\\linewidth}\n  \\subfloat[\\method{} (Ours)]{\\includegraphics[width=0.45\\linewidth]{figures/RLVMR.pdf}}\\\\\n  \\caption{Comparison of LLM agent RL training paradigms: (a) Standard RL with outcome-only rewards (e.g., GRPO) inadvertently reinforces trajectories with inefficient or illogical intermediate reasoning steps. (b) Our \\method{} approach provides dense, verifiable rewards for beneficial meta-reasoning behaviors (e.g., T1-T4), directly shaping a more robust and coherent reasoning process.}\n  \\label{fig:intro}\n\\end{figure}", "caption": "\\caption{Comparison of LLM agent RL training paradigms: (a) Standard RL with outcome-only rewards (e.g., GRPO) inadvertently reinforces trajectories with inefficient or illogical intermediate reasoning steps. (b) Our \\method{} approach provides dense, verifiable rewards for beneficial meta-reasoning behaviors (e.g., T1-T4), directly shaping a more robust and coherent reasoning process.}", "label": "\\label{fig:intro}", "subfigures": [], "figure_paths": ["figures/RLVR.pdf", "figures/RLVMR.pdf"]}, {"original": "\\begin{figure}[t]\n  \\centering\n  \\subfloat[Success Rate (1.5B)]{\\includegraphics[width=0.3\\linewidth]{figures/1.5B_success.pdf}} \\hfill\n  \\subfloat[Invalid Actions (1.5B)]{\\includegraphics[width=0.3\\linewidth]{figures/1.5B_invalid.pdf}} \\hfill\n  \\subfloat[Repetitive Actions (1.5B)]{\\includegraphics[width=0.3\\linewidth]{figures/1.5B_repetitive.pdf}}\\\\\\vspace{5pt}\n  \\subfloat[Success Rate (7B)]{\\includegraphics[width=0.3\\linewidth]{figures/7B_success.pdf}} \\hfill\n  \\subfloat[Invlaid Actions (7B)]{\\includegraphics[width=0.3\\linewidth]{figures/7B_invalid.pdf}} \\hfill\n  \\subfloat[Repetitive Actions (7B)]{\\includegraphics[width=0.3\\linewidth]{figures/7B_repetitive.pdf}}\n\\caption{Performance of SFT and GRPO on ALFWorld. While SFT excels on seen tasks (L0) but fails to generalize, GRPO achieves better generalization at the cost of significant inefficiency (high action counts and redundancy). This highlights a fundamental trade-off between brittle efficiency and inefficient generalization.}\n  \\label{fig:inefficient-exploration}\n\\end{figure}", "caption": "\\caption{Performance of SFT and GRPO on ALFWorld. While SFT excels on seen tasks (L0) but fails to generalize, GRPO achieves better generalization at the cost of significant inefficiency (high action counts and redundancy). This highlights a fundamental trade-off between brittle efficiency and inefficient generalization.}", "label": "\\label{fig:inefficient-exploration}", "subfigures": [], "figure_paths": ["figures/1.5B_success.pdf", "figures/1.5B_invalid.pdf", "figures/1.5B_repetitive.pdf", "figures/7B_success.pdf", "figures/7B_invalid.pdf", "figures/7B_repetitive.pdf"]}, {"original": "\\begin{figure*}[t]\n  \\centering\n    \\includegraphics[width=\\linewidth]{figures/method.pdf}\n    \\caption{A schematic diagram of the RLVMR framework, which consists of two training phases: cold start and reinforcement learning. Our method provides rule-verifiable feedback signals based on the final outcome and the relative advantages of different types of meta-reasoning behaviors.\n    }\n    \\label{fig:framework}\n\\end{figure*}", "caption": "\\caption{A schematic diagram of the RLVMR framework, which consists of two training phases: cold start and reinforcement learning. Our method provides rule-verifiable feedback signals based on the final outcome and the relative advantages of different types of meta-reasoning behaviors.\n    }", "label": "\\label{fig:framework}", "subfigures": [], "figure_paths": ["figures/method.pdf"]}, {"original": "\\begin{figure}[t]\n  \\centering\n  \\subfloat[Invalid Actions (1.5B)]{\\includegraphics[width=0.4\\linewidth]{figures/1.5B_ours_invalid.pdf}} \\hspace{0.1\\linewidth}\n  \\subfloat[Repetitive Actions (1.5B)]{\\includegraphics[width=0.4\\linewidth]{figures/1.5B_ours_repetitive.pdf}}\\\\\\vspace{5pt}\n  \\subfloat[Invlaid Actions (7B)]{\\includegraphics[width=0.4\\linewidth]{figures/7B_ours_invalid.pdf}} \\hspace{0.1\\linewidth}\n  \\subfloat[Repetitive Actions (7B)]{\\includegraphics[width=0.4\\linewidth]{figures/7B_ours_repetitive.pdf}}\n  \\caption{Exploration efficiency of \\method{} compared to SFT and GRPO baselines on ALFWorld. RLVMR consistently and significantly reduces both invalid and repetitive actions across all generalization levels and model sizes, demonstrating its effectiveness at mitigating inefficient exploration.}  \\label{fig:exploration-efficiency}\n\\end{figure}", "caption": "\\caption{Exploration efficiency of \\method{} compared to SFT and GRPO baselines on ALFWorld. RLVMR consistently and significantly reduces both invalid and repetitive actions across all generalization levels and model sizes, demonstrating its effectiveness at mitigating inefficient exploration.}", "label": "\\label{fig:exploration-efficiency}", "subfigures": [], "figure_paths": ["figures/1.5B_ours_invalid.pdf", "figures/1.5B_ours_repetitive.pdf", "figures/7B_ours_invalid.pdf", "figures/7B_ours_repetitive.pdf"]}, {"original": "\\begin{figure}[t]\n  \\centering\n    \\includegraphics[width=0.8\\linewidth]{figures/comparison_train.pdf}\n    \\caption{The step count curves of different methods on two datasets during the RL training process.}\n    \\label{fig:traj_length}\n\\end{figure}", "caption": "\\caption{The step count curves of different methods on two datasets during the RL training process.}", "label": "\\label{fig:traj_length}", "subfigures": [], "figure_paths": ["figures/comparison_train.pdf"]}], "equations": ["\\begin{equation}\n    \\max_\\theta\\ \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ R(\\tau) \\right].\n\\end{equation}", "\\begin{equation}\n    A^{\\mathrm{traj}}_k = \\frac{R(\\tau_k) - \\mu_R}{\\sigma_R},\n\\end{equation}", "\\begin{equation}\n    A^{\\mathrm{MR}}_{t,\\mathrm{tag}} = \\frac{r^{\\mathrm{MR}}_{t,\\mathrm{tag}} - \\mu_{\\mathrm{tag}}}{\\sigma_{\\mathrm{tag}}},\n\\end{equation}", "\\begin{equation}\n    A_t = \\alpha \\cdot A^{\\text{traj}}_k + (1 - \\alpha) \\cdot A^{\\text{MR}}_{t, \\text{tag}},\n\\end{equation}", "\\begin{equation}\n\\mathcal{L}_{\\text{final}} = \\mathbb{E}_t \\left[ \\min \\left( r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t \\right) \\right] - \\lambda_{\\text{KL}} D_{\\text{KL}}(\\pi_\\theta \\Vert \\pi_{\\text{ref}}),\n\\end{equation}"], "algorithm": [], "sections": {"Introduction": {"content": "\n\n\n\\begin{figure}[ht]\n  \\centering\n  \\subfloat[Standard RLVR (GRPO)]{\\includegraphics[width=0.45\\linewidth]{figures/RLVR.pdf}} \\hspace{0.05\\linewidth}\n  \\subfloat[\\method{} (Ours)]{\\includegraphics[width=0.45\\linewidth]{figures/RLVMR.pdf}}\\\\\n  \\caption{Comparison of LLM agent RL training paradigms: (a) Standard RL with outcome-only rewards (e.g., GRPO) inadvertently reinforces trajectories with inefficient or illogical intermediate reasoning steps. (b) Our \\method{} approach provides dense, verifiable rewards for beneficial meta-reasoning behaviors (e.g., T1-T4), directly shaping a more robust and coherent reasoning process.}\n  \\label{fig:intro}\n\\end{figure}\n  \\centering\n  \\subfloat[Standard RLVR (GRPO)]{\\includegraphics[width=0.45\\linewidth]{figures/RLVR.pdf}}\\includegraphics[width=0.45\\linewidth]{figures/RLVR.pdf} \\hspace{0.05\\linewidth}\n  \\subfloat[\\method{} (Ours)]{\\includegraphics[width=0.45\\linewidth]{figures/RLVMR.pdf}}\\includegraphics[width=0.45\\linewidth]{figures/RLVMR.pdf}\\\\\n  \\caption{Comparison of LLM agent RL training paradigms: (a) Standard RL with outcome-only rewards (e.g., GRPO) inadvertently reinforces trajectories with inefficient or illogical intermediate reasoning steps. (b) Our \\method{} approach provides dense, verifiable rewards for beneficial meta-reasoning behaviors (e.g., T1-T4), directly shaping a more robust and coherent reasoning process.}\n  \\label{fig:intro}\n\n\nThe quest to build autonomous agents capable of solving complex, long-horizon tasks has gained significant momentum with the rise of Large Language Models (LLMs)~\\citep{zeng2024agenttuning, wang2022scienceworld, bai2024digirl}. However, dominant training paradigms face a fundamental trade-off. On one hand, Supervised Fine-Tuning (SFT) on expert trajectories can teach agents efficient behaviors, but these policies are often brittle and fail to generalize to novel situations~\\citep{chu2025sftrl}. On the other hand, Reinforcement Learning (RL) from environmental feedback encourages exploration and can lead to better generalization, but it typically optimizes for a single, sparse reward signal: final task success.\n\nThis reliance on outcome-only rewards raises a critical, yet underexplored question: {\\bf Are agents learning to reason coherently, or are they just finding brittle shortcuts to success?}\\bf Are agents learning to reason coherently, or are they just finding brittle shortcuts to success? Our work investigates a pervasive issue we term {\\bf inefficient exploration}\\bf inefficient exploration, where agents are rewarded for successful outcomes even when their path to success is built on flawed, illogical, or redundant reasoning. As illustrated in Figure~\\ref{fig:front_page}, this leads to agents that exhibit high rates of repetitive actions and struggle to adapt to unseen tasks, because their underlying problem-solving process is unsound. Standard RL inadvertently reinforces any successful trajectory, failing to distinguish between robust and flawed reasoning processes. This deficiency undermines agent reliability, interpretability, and generalization, especially as tasks grow in complexity.\n\nWe argue that to build truly robust and generalizable agents, we must move beyond rewarding only the final outcome and begin to supervise the reasoning {\\em process}\\em process itself. Drawing inspiration from metacognitive theory~\\citep{martinez2006metacognition}, which posits that effective problem-solving depends on ``thinking about thinking'', we propose to directly reward beneficial cognitive behaviors. Our key insight is that high-level skills like planning, monitoring progress, exploring alternatives, and reflecting on errors can be operationalized as distinct, verifiable steps within an agent's reasoning process.\n\nTo this end, we introduce {\\bf Reinforcement Learning with Verifiable Meta-Reasoning Rewards (\\method{})}\\bf Reinforcement Learning with Verifiable Meta-Reasoning Rewards (\\method{}), a novel framework that integrates dense, process-level supervision into end-to-end RL. \nAs illustrated in Figure~\\ref{fig:intro}, \\method{} contrasts with standard RL by rewarding not only the final outcome but also the intermediate reasoning steps. \nOur framework defines a set of core meta-reasoning behaviors \u2014 {\\em planning}\\em planning, {\\em exploration}\\em exploration, and {\\em reflection/monitoring}\\em reflection/monitoring \u2014 and enables the agent to articulate its cognitive state through special tags. During online interaction, we use lightweight, programmatic rules to grant verifiable rewards for these behaviors. For example, an `exploration' tag is rewarded when the agent discovers a new state, while a `reflection' tag is rewarded when it leads to the correction of a prior mistake. These process-centric rewards are combined with the global outcome reward and optimized using a policy gradient method. After a brief ``cold-start'' supervised fine-tuning (SFT) phase on only 200 trajectories to learn the tag syntax, the agent is trained entirely through environmental interaction.\n\nWe demonstrate the effectiveness of \\method{} on two challenging long-horizon benchmarks, ALFWorld and ScienceWorld. Our experiments show that \\method{} achieves new state-of-the-art results across all settings. Notably, on the hardest unseen task split (L2), our 7B model achieves an 83.6\\% success rate, and surpasses the performance of the much larger models. In-depth analysis reveals that these gains are driven by a tangible improvement in reasoning quality: \\method{}-trained agents exhibit significant reductions in repetitive and invalid actions. This confirms that by rewarding the {\\em process}\\em process of good reasoning, we create agents that are not only more successful but also more robust, efficient, and generalizable.\n\n\nIn summary, our contributions are as follows:\n\\begin{enumerate}[leftmargin=12pt]\n    \\item We identify and formulate the {\\bf inefficient exploration} problem in long-horizon agents, showing how optimizing for final outcomes alone reinforces flawed reasoning and leads to brittle policies that fail to generalize.\n    \\item We propose {\\bf \\method{}}, a novel RL framework that provides dense, process-level supervision by rewarding verifiable meta-reasoning behaviors (e.g., planning, exploration, reflection) using lightweight, programmatic rules.\n    \\item We achieve {\\bf state-of-the-art performance} on the challenging ALFWorld and ScienceWorld benchmarks, with significant improvements in generalization to unseen tasks. %, even enabling a 7B model to surpass much larger models such as GPT-4o and DeepSeek-V3/R1.\n    \\item We provide {\\bf in-depth analysis} confirming that \\method{}'s gains stem directly from improved reasoning quality, evidenced by measurable reductions in repetitive actions and enhanced error recovery, thereby improving both agent robustness and efficiency.\n\\end{enumerate}\\begin{enumerate}[leftmargin=12pt]\n    \\item We identify and formulate the {\\bf inefficient exploration} problem in long-horizon agents, showing how optimizing for final outcomes alone reinforces flawed reasoning and leads to brittle policies that fail to generalize.\n    \\item We propose {\\bf \\method{}}, a novel RL framework that provides dense, process-level supervision by rewarding verifiable meta-reasoning behaviors (e.g., planning, exploration, reflection) using lightweight, programmatic rules.\n    \\item We achieve {\\bf state-of-the-art performance} on the challenging ALFWorld and ScienceWorld benchmarks, with significant improvements in generalization to unseen tasks. %, even enabling a 7B model to surpass much larger models such as GPT-4o and DeepSeek-V3/R1.\n    \\item We provide {\\bf in-depth analysis} confirming that \\method{}'s gains stem directly from improved reasoning quality, evidenced by measurable reductions in repetitive actions and enhanced error recovery, thereby improving both agent robustness and efficiency.\n\\end{enumerate}\n    \\item We identify and formulate the {\\bf inefficient exploration}\\bf inefficient exploration problem in long-horizon agents, showing how optimizing for final outcomes alone reinforces flawed reasoning and leads to brittle policies that fail to generalize.\n    \\item We propose {\\bf \\method{}}\\bf \\method{}, a novel RL framework that provides dense, process-level supervision by rewarding verifiable meta-reasoning behaviors (e.g., planning, exploration, reflection) using lightweight, programmatic rules.\n    \\item We achieve {\\bf state-of-the-art performance}\\bf state-of-the-art performance on the challenging ALFWorld and ScienceWorld benchmarks, with significant improvements in generalization to unseen tasks. \\item We provide {\\bf in-depth analysis}\\bf in-depth analysis confirming that \\method{}'s gains stem directly from improved reasoning quality, evidenced by measurable reductions in repetitive actions and enhanced error recovery, thereby improving both agent robustness and efficiency.\n\n\n\n\n\n", "appendix": false}, "Inefficient Exploration in Long-Horizon Agents": {"content": "\n\nThis section investigates the phenomenon of ``inefficient exploration'' in agents designed for long-horizon tasks. We analyze its detrimental effects on performance, which manifest as {\\bf brittle efficiency}\\bf brittle efficiency on previously seen tasks and {\\bf poor generalization}\\bf poor generalization to unseen ones.\n\n\\subsection{Experimental Setup}\n\n\\paragraph{Benchmarks}Benchmarks\nTo evaluate foundational capabilities and generalization, we conduct experiments on the widely-used and challenging {\\bf ALFWorld}\\bf ALFWorld benchmark~\\citep{shridhar2020alfworld}, which comprises embodied household tasks. To systematically measure generalization, we define three evaluation splits based on the original benchmark:\n\\begin{itemize}[leftmargin=12pt]\n    \\item \\textbf{L0} (\\emph{seen-L0}): {\\color{ngreen} seen task variants and seen task categories}; \n    \\item \\textbf{L1} (\\emph{unseen-L1}): {\\color{nred} unseen held-out task variants} but {\\color{ngreen} seen task categories};\n    \\item \\textbf{L2} (\\emph{unseen-L2}): {\\color{nred} unseen held-out task variants and unseen task categories}.\n\\end{itemize}\\begin{itemize}[leftmargin=12pt]\n    \\item \\textbf{L0} (\\emph{seen-L0}): {\\color{ngreen} seen task variants and seen task categories}; \n    \\item \\textbf{L1} (\\emph{unseen-L1}): {\\color{nred} unseen held-out task variants} but {\\color{ngreen} seen task categories};\n    \\item \\textbf{L2} (\\emph{unseen-L2}): {\\color{nred} unseen held-out task variants and unseen task categories}.\n\\end{itemize}\n    \\item \\textbf{L0} (\\emph{seen-L0}): {\\color{ngreen} seen task variants and seen task categories}\\color{ngreen} seen task variants and seen task categories; \n    \\item \\textbf{L1} (\\emph{unseen-L1}): {\\color{nred} unseen held-out task variants}\\color{nred} unseen held-out task variants but {\\color{ngreen} seen task categories}\\color{ngreen} seen task categories;\n    \\item \\textbf{L2} (\\emph{unseen-L2}): {\\color{nred} unseen held-out task variants and unseen task categories}\\color{nred} unseen held-out task variants and unseen task categories.\n\nL0 and L1 follow the official benchmark splits. For L2, we further partition ALFWorld by task category, holding out entire categories from training for exclusive use in evaluation.\n\n\\paragraph{Training Paradigms}Training Paradigms\nWe experiment with Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct models using the \\textbf{ReAct}~\\citep{yao2023react} framework, which alternates between reasoning and acting steps. We evaluate two dominant training paradigms:\n\\begin{itemize}[leftmargin=12pt]\n    \\item \\textbf{SFT}~\\citep{yang2023gpt4tools, tang2023toolalpaca, xi2024agentgym}: A widely adopted paradigm that applies supervised fine-tuning on high-quality expert trajectories.\n    \\item \\textbf{GRPO}~\\citep{feng2025GRPO, wang2025ragen}: An end-to-end RL method that optimizes the policy by comparing the final rewards of multiple trajectories sampled from the same initial state.\n\\end{itemize}\\begin{itemize}[leftmargin=12pt]\n    \\item \\textbf{SFT}~\\citep{yang2023gpt4tools, tang2023toolalpaca, xi2024agentgym}: A widely adopted paradigm that applies supervised fine-tuning on high-quality expert trajectories.\n    \\item \\textbf{GRPO}~\\citep{feng2025GRPO, wang2025ragen}: An end-to-end RL method that optimizes the policy by comparing the final rewards of multiple trajectories sampled from the same initial state.\n\\end{itemize}\n    \\item \\textbf{SFT}~\\citep{yang2023gpt4tools, tang2023toolalpaca, xi2024agentgym}: A widely adopted paradigm that applies supervised fine-tuning on high-quality expert trajectories.\n    \\item \\textbf{GRPO}~\\citep{feng2025GRPO, wang2025ragen}: An end-to-end RL method that optimizes the policy by comparing the final rewards of multiple trajectories sampled from the same initial state.\n\n\n\\paragraph{Evaluation Metrics}Evaluation Metrics\nWe assess performance using the following metrics:\n\\begin{itemize}[leftmargin=12pt]\n    \\item {\\bf Success Rate (\\%, $\\uparrow$):} The percentage of tasks successfully completed by the agent on each evaluation split.\n    % \\item {\\bf Number of Actions ($\\downarrow$):} The average number of actions taken to complete a task. A lower count signifies higher step-efficiency.\n    \\item \\textbf{Invalid Action Rate (\\%, $\\downarrow$):} The proportion of generated actions that are invalid in the current state, reflecting basic comprehension and error frequency.\n    \\item {\\bf Repetitive Action Rate (\\%, $\\downarrow$):} The percentage of steps where the agent executes a {\\bf meaningless repeated action}, as defined in prior work~\\citep{yuan2025agentr, fu2025agentrefine, feng2025gigpo}. This metric quantifies inefficient exploration, indicating that the agent's policy may be overfitting to familiar action sequences rather than being guided by robust reasoning.\n\\end{itemize}\\begin{itemize}[leftmargin=12pt]\n    \\item {\\bf Success Rate (\\%, $\\uparrow$):} The percentage of tasks successfully completed by the agent on each evaluation split.\n    % \\item {\\bf Number of Actions ($\\downarrow$):} The average number of actions taken to complete a task. A lower count signifies higher step-efficiency.\n    \\item \\textbf{Invalid Action Rate (\\%, $\\downarrow$):} The proportion of generated actions that are invalid in the current state, reflecting basic comprehension and error frequency.\n    \\item {\\bf Repetitive Action Rate (\\%, $\\downarrow$):} The percentage of steps where the agent executes a {\\bf meaningless repeated action}, as defined in prior work~\\citep{yuan2025agentr, fu2025agentrefine, feng2025gigpo}. This metric quantifies inefficient exploration, indicating that the agent's policy may be overfitting to familiar action sequences rather than being guided by robust reasoning.\n\\end{itemize}\n    \\item {\\bf Success Rate (\\%, $\\uparrow$):}\\bf Success Rate (\\%, $\\uparrow$\\uparrow): The percentage of tasks successfully completed by the agent on each evaluation split.\n    \\item \\textbf{Invalid Action Rate (\\%, $\\downarrow$):} The proportion of generated actions that are invalid in the current state, reflecting basic comprehension and error frequency.\n    \\item {\\bf Repetitive Action Rate (\\%, $\\downarrow$):}\\bf Repetitive Action Rate (\\%, $\\downarrow$\\downarrow): The percentage of steps where the agent executes a {\\bf meaningless repeated action}\\bf meaningless repeated action, as defined in prior work~\\citep{yuan2025agentr, fu2025agentrefine, feng2025gigpo}. This metric quantifies inefficient exploration, indicating that the agent's policy may be overfitting to familiar action sequences rather than being guided by robust reasoning.\n\n\n\n\\subsection{Observing the Inefficient Exploration Problem}\n\nBefore presenting aggregate statistics, we first examine individual trajectories to understand how inefficient exploration manifests at the step level. Such microscopic inspection reveals the cognitive patterns that lead to brittle or wasteful behaviors. We observe that, although various methods (e.g., GRPO) can improve an agent's success rate on specific tasks, such improvement is often due to reinforcement of state-action mappings associated with correct reasoning. However, the agent's self-reflection and understanding of its own reasoning process are frequently overlooked.\n\n\nThe following is a trajectory segment of a vanilla GRPO-trained agent performing a novel task\u2014-- \\textit{put two keychains in the safe} --- in ALFWorld (corresponding to our L2 split). By step 7, the agent has already arrived at dresser 1. However, we observe that in the subsequent steps, the agent falls into a sequence of inefficient decisions: its next intention is to find the second keychain, yet it persistently tries to go to dresser 1 for several steps, disregarding the fact that it is already there. This indicates that its policy mainly reflects the action distribution present in the training data, rather than allowing the reasoning process itself to truly regulate decision-making. Although the agent can form relatively effective action strategies for completing tasks, its capacity for critically evaluating its own behavior and understanding the underlying task requirements remains limited. This suggests that the agent has not truly acquired the reasoning patterns necessary for robust task-solving.\n\n\\begin{tcolorbox}[title = {A trajectory segment of an agent trained with GRPO}, breakable]\n\\textbf{Step 7}\n\n\\textbf{Observation:} You arrive at dresser 1. On the dresser 1, you see a box 1, a creditcard 1, a keychain 2, a keychain 1, a laptop 1, a statue 1, and a tissuebox 2.\\\\\n\\textbf{Reasoning:} The current situation is that I am at dresser 1, which has multiple items including a keychain 1, a keychain 2, a creditcard 1, etc. To complete the task of putting two keychains in the safe, I need to select one of these keychains from the dresser.\\\\\n\\textbf{Action:} take keychain 1 from dresser 1\\\\\n\n\\textbf{Step 8}\n\n\\textbf{Observation:} You pick up the keychain 1 from the dresser 1.\\\\\n\\textbf{Reasoning:} The current situation is that I am at dresser 1 with the keychain 1 in my possession. To put two keychains in the safe, I need to find another keychain and then move both keychains to the safe.\\\\\n\\textbf{Action:} go to dresser 1\\\\\n\n\\textbf{Step 9}\n\n\\textbf{Observation:} Nothing happens.\\\\\n\\textbf{Reasoning:} The player has been in the room for several steps but has not yet interacted with the safe or taken any keychains. The safe is closed initially, which might need interaction to open it.\\\\\n\\textbf{Action:} go to dresser 1\\\\\n\n\\textbf{Step 10}\n\n\\textbf{Observation:} Nothing happens.\\\\\n\\textbf{Reasoning:} The previous steps were not successful in opening the safe or taking the keychains. Let's try another approach.\\\\\n\\textbf{Action:} go to dresser 1\\\\\n\n\\textbf{Step 11}\n\n\\textbf{Observation:} Nothing happens.\\\\\n\\textbf{Reasoning:} The user has taken all the steps necessary to obtain both keychains but has yet to actually put them into the safe. The last known location of the keychains was on the dresser before the attempt to open the safe failed (in Observation 8). To complete the task, the user needs to move one or both keychains to the safe before attempting to open it again.\\\\\n\\textbf{Action:} move keychain 1 to safe 1\\\\\n\n\\textbf{Observation:} Nothing happens.\\\\\n\\end{tcolorbox}\\begin{tcolorbox}[title = {A trajectory segment of an agent trained with GRPO}, breakable]\n\\textbf{Step 7}\n\n\\textbf{Observation:} You arrive at dresser 1. On the dresser 1, you see a box 1, a creditcard 1, a keychain 2, a keychain 1, a laptop 1, a statue 1, and a tissuebox 2.\\\\\n\\textbf{Reasoning:} The current situation is that I am at dresser 1, which has multiple items including a keychain 1, a keychain 2, a creditcard 1, etc. To complete the task of putting two keychains in the safe, I need to select one of these keychains from the dresser.\\\\\n\\textbf{Action:} take keychain 1 from dresser 1\\\\\n\n\\textbf{Step 8}\n\n\\textbf{Observation:} You pick up the keychain 1 from the dresser 1.\\\\\n\\textbf{Reasoning:} The current situation is that I am at dresser 1 with the keychain 1 in my possession. To put two keychains in the safe, I need to find another keychain and then move both keychains to the safe.\\\\\n\\textbf{Action:} go to dresser 1\\\\\n\n\\textbf{Step 9}\n\n\\textbf{Observation:} Nothing happens.\\\\\n\\textbf{Reasoning:} The player has been in the room for several steps but has not yet interacted with the safe or taken any keychains. The safe is closed initially, which might need interaction to open it.\\\\\n\\textbf{Action:} go to dresser 1\\\\\n\n\\textbf{Step 10}\n\n\\textbf{Observation:} Nothing happens.\\\\\n\\textbf{Reasoning:} The previous steps were not successful in opening the safe or taking the keychains. Let's try another approach.\\\\\n\\textbf{Action:} go to dresser 1\\\\\n\n\\textbf{Step 11}\n\n\\textbf{Observation:} Nothing happens.\\\\\n\\textbf{Reasoning:} The user has taken all the steps necessary to obtain both keychains but has yet to actually put them into the safe. The last known location of the keychains was on the dresser before the attempt to open the safe failed (in Observation 8). To complete the task, the user needs to move one or both keychains to the safe before attempting to open it again.\\\\\n\\textbf{Action:} move keychain 1 to safe 1\\\\\n\n\\textbf{Observation:} Nothing happens.\\\\\n\\end{tcolorbox}[title = {A trajectory segment of an agent trained with GRPO}A trajectory segment of an agent trained with GRPO, breakable]\n\\textbf{Step 7}\n\n\\textbf{Observation:} You arrive at dresser 1. On the dresser 1, you see a box 1, a creditcard 1, a keychain 2, a keychain 1, a laptop 1, a statue 1, and a tissuebox 2.\\\\\n\\textbf{Reasoning:} The current situation is that I am at dresser 1, which has multiple items including a keychain 1, a keychain 2, a creditcard 1, etc. To complete the task of putting two keychains in the safe, I need to select one of these keychains from the dresser.\\\\\n\\textbf{Action:} take keychain 1 from dresser 1\\\\\n\n\\textbf{Step 8}\n\n\\textbf{Observation:} You pick up the keychain 1 from the dresser 1.\\\\\n\\textbf{Reasoning:} The current situation is that I am at dresser 1 with the keychain 1 in my possession. To put two keychains in the safe, I need to find another keychain and then move both keychains to the safe.\\\\\n\\textbf{Action:} go to dresser 1\\\\\n\n\\textbf{Step 9}\n\n\\textbf{Observation:} Nothing happens.\\\\\n\\textbf{Reasoning:} The player has been in the room for several steps but has not yet interacted with the safe or taken any keychains. The safe is closed initially, which might need interaction to open it.\\\\\n\\textbf{Action:} go to dresser 1\\\\\n\n\\textbf{Step 10}\n\n\\textbf{Observation:} Nothing happens.\\\\\n\\textbf{Reasoning:} The previous steps were not successful in opening the safe or taking the keychains. Let's try another approach.\\\\\n\\textbf{Action:} go to dresser 1\\\\\n\n\\textbf{Step 11}\n\n\\textbf{Observation:} Nothing happens.\\\\\n\\textbf{Reasoning:} The user has taken all the steps necessary to obtain both keychains but has yet to actually put them into the safe. The last known location of the keychains was on the dresser before the attempt to open the safe failed (in Observation 8). To complete the task, the user needs to move one or both keychains to the safe before attempting to open it again.\\\\\n\\textbf{Action:} move keychain 1 to safe 1\\\\\n\n\\textbf{Observation:} Nothing happens.\\\\\n\n\nThis qualitative example highlights how an ostensibly ``successful'' policy can still waste a large number of steps on redundant or illogical moves.  We next quantify how pervasive this phenomenon is across models and training paradigms.\n\n\n\\subsection{The Problem of Inefficient Exploration}\n\nWe now present large\u2010scale empirical results that corroborate the anecdotal evidence above. Figure~\\ref{fig:inefficient-exploration} compares SFT and GRPO across success, invalid\u2010action, and repetitive\u2010action metrics.\n\n\\paragraph{SFT creates efficient but brittle policies that fail to generalize.}SFT creates efficient but brittle policies that fail to generalize.\nAs seen, SFT significantly boosts performance on seen tasks (L0) compared to the ReAct baseline. For instance, the 7B model's success rate jumps from 23.1\\% to 63.3\\%. This approach also yields highly efficient policies with low invalid action rates (e.g., 6.2\\% on L0 for the 7B model). However, this efficiency is brittle. On the most challenging unseen split (L2), the 7B model's success rate plummets to 37.5\\%. Furthermore, its repetitive action rate nearly doubles from 13.9\\% on L0 to 24.5\\% on L1, revealing a critical flaw: when faced with novel situations not covered by expert data, the agent falls back on non-productive loops. This demonstrates that SFT teaches agents to mimic actions without instilling a robust, generalizable reasoning process.\n\n\\paragraph{RL with outcome-only rewards (GRPO) improves generalization but fosters inefficient and flawed reasoning.}RL with outcome-only rewards (GRPO) improves generalization but fosters inefficient and flawed reasoning.\nIn contrast, GRPO achieves substantially better generalization. The 7B GRPO model attains success rates of 77.3\\% on L1 and 52.3\\% on L2, significantly outperforming SFT. This success, however, validates our core hypothesis about the {\\bf inefficient exploration problem}\\bf inefficient exploration problem. The agent's performance is undermined by severe inefficiency, as evidenced by high invalid and repetitive action rates across all splits. For example, the 7B model's repetitive action rate on the most difficult L2 tasks is a staggering 31.2\\%. By optimizing solely for final task success, GRPO reinforces any path that leads to a positive outcome, even those built on illogical steps, redundant actions, and inefficient exploration.\n\n\\paragraph{Scaling the model size improves baseline capabilities but does not fix the underlying reasoning deficiencies.}Scaling the model size improves baseline capabilities but does not fix the underlying reasoning deficiencies.\nWhile scaling from a 1.5B to a 7B model improves overall success rates for both SFT and GRPO, it does not resolve the fundamental issue. The 7B GRPO model, despite its higher success rate on L2 (52.3\\% vs. 29.7\\% for 1.5B), also exhibits a higher repetitive action rate (31.2\\% vs. 27.1\\%). This indicates that a larger model's enhanced capabilities can sometimes be misdirected to more effectively exploit flawed strategies, rather than to reason more coherently. This finding underscores that the limitations are rooted in the training objective itself, not merely in the model's capacity. Simply increasing model size is not a panacea for the inefficient exploration problem.\n\n\\paragraph{Current training paradigms force a trade-off between brittle efficiency and inefficient generalization.}Current training paradigms force a trade-off between brittle efficiency and inefficient generalization.\nOur analysis reveals a fundamental dilemma in agent training. SFT produces policies that are efficient on familiar tasks but brittle and unable to generalize, as they lack robust problem-solving skills. Conversely, GRPO fosters better generalization through exploration but at the cost of reinforcing inefficient and logically flawed reasoning paths. Neither paradigm effectively teaches the agent {\\em how}\\em how to reason well. This establishes a clear need for a new framework that moves beyond sparse, outcome-only signals to provide direct, {\\bf process-level supervision}\\bf process-level supervision. By rewarding coherent and efficient reasoning steps, we can guide agents to not only find solutions but to do so robustly and intelligently, which is the precise goal of our RLVMR framework.\n\n\n\\begin{figure}[t]\n  \\centering\n  \\subfloat[Success Rate (1.5B)]{\\includegraphics[width=0.3\\linewidth]{figures/1.5B_success.pdf}} \\hfill\n  \\subfloat[Invalid Actions (1.5B)]{\\includegraphics[width=0.3\\linewidth]{figures/1.5B_invalid.pdf}} \\hfill\n  \\subfloat[Repetitive Actions (1.5B)]{\\includegraphics[width=0.3\\linewidth]{figures/1.5B_repetitive.pdf}}\\\\\\vspace{5pt}\n  \\subfloat[Success Rate (7B)]{\\includegraphics[width=0.3\\linewidth]{figures/7B_success.pdf}} \\hfill\n  \\subfloat[Invlaid Actions (7B)]{\\includegraphics[width=0.3\\linewidth]{figures/7B_invalid.pdf}} \\hfill\n  \\subfloat[Repetitive Actions (7B)]{\\includegraphics[width=0.3\\linewidth]{figures/7B_repetitive.pdf}}\n\\caption{Performance of SFT and GRPO on ALFWorld. While SFT excels on seen tasks (L0) but fails to generalize, GRPO achieves better generalization at the cost of significant inefficiency (high action counts and redundancy). This highlights a fundamental trade-off between brittle efficiency and inefficient generalization.}\n  \\label{fig:inefficient-exploration}\n\\end{figure}\n  \\centering\n  \\subfloat[Success Rate (1.5B)]{\\includegraphics[width=0.3\\linewidth]{figures/1.5B_success.pdf}}\\includegraphics[width=0.3\\linewidth]{figures/1.5B_success.pdf} \\hfill\n  \\subfloat[Invalid Actions (1.5B)]{\\includegraphics[width=0.3\\linewidth]{figures/1.5B_invalid.pdf}}\\includegraphics[width=0.3\\linewidth]{figures/1.5B_invalid.pdf} \\hfill\n  \\subfloat[Repetitive Actions (1.5B)]{\\includegraphics[width=0.3\\linewidth]{figures/1.5B_repetitive.pdf}}\\includegraphics[width=0.3\\linewidth]{figures/1.5B_repetitive.pdf}\\\\\\vspace{5pt}\n  \\subfloat[Success Rate (7B)]{\\includegraphics[width=0.3\\linewidth]{figures/7B_success.pdf}}\\includegraphics[width=0.3\\linewidth]{figures/7B_success.pdf} \\hfill\n  \\subfloat[Invlaid Actions (7B)]{\\includegraphics[width=0.3\\linewidth]{figures/7B_invalid.pdf}}\\includegraphics[width=0.3\\linewidth]{figures/7B_invalid.pdf} \\hfill\n  \\subfloat[Repetitive Actions (7B)]{\\includegraphics[width=0.3\\linewidth]{figures/7B_repetitive.pdf}}\\includegraphics[width=0.3\\linewidth]{figures/7B_repetitive.pdf}\n\\caption{Performance of SFT and GRPO on ALFWorld. While SFT excels on seen tasks (L0) but fails to generalize, GRPO achieves better generalization at the cost of significant inefficiency (high action counts and redundancy). This highlights a fundamental trade-off between brittle efficiency and inefficient generalization.}\n  \\label{fig:inefficient-exploration}\n\n\n\n\n", "appendix": false}, "Methodology": {"content": "\n\n\\begin{figure*}[t]\n  \\centering\n    \\includegraphics[width=\\linewidth]{figures/method.pdf}\n    \\caption{A schematic diagram of the RLVMR framework, which consists of two training phases: cold start and reinforcement learning. Our method provides rule-verifiable feedback signals based on the final outcome and the relative advantages of different types of meta-reasoning behaviors.\n    }\n    \\label{fig:framework}\n\\end{figure*}\n  \\centering\n    \\includegraphics[width=\\linewidth]{figures/method.pdf}\n    \\caption{A schematic diagram of the RLVMR framework, which consists of two training phases: cold start and reinforcement learning. Our method provides rule-verifiable feedback signals based on the final outcome and the relative advantages of different types of meta-reasoning behaviors.\n    }\n    \\label{fig:framework}\n\n\nOur methodology equips LLM agents with an explicit meta-reasoning framework to mitigate inefficient exploration in complex tasks. We begin by formalizing the agent-environment interaction as a Markov Decision Process (\\S~\\ref{sec:mdp}). We then introduce a novel meta-reasoning framework that extends existing agent architectures by operationalizing principles from cognitive science (\\S~\\ref{sec:operations}). As illustrated in Figure~\\ref{fig:framework}, the agent is trained in two phases: an initial SFT stage to bootstrap the agent's meta-reasoning capabilities (\\S~\\ref{sec:cold_start}), followed by a reinforcement learning phase that uses a custom policy optimization algorithm to refine these skills based on task outcomes and process-centric rewards (\\S~\\ref{sec:rlvmr}).\n\n\n\\subsection{Task Formulation as a Markov Decision Process}\n\\label{sec:mdp}\n\nWe formalize the interaction between an agent and its environment in long-horizon tasks as a Markov Decision Process (MDP). An MDP is defined by a tuple $(S, A, O, F, R)$(S, A, O, F, R), where $S$S is the set of environment states, $A$A is the action space, $O$O is the observation space, $F: S \\times A \\rightarrow S$F: S \\times A \\rightarrow S is the state transition function, and $R: S \\times A \\rightarrow \\mathbb{R}$R: S \\times A \\rightarrow \\mathbb{R} is the reward function. In our setting, which is tailored for LLM agents, the state, action, and observation spaces ($S, A, O$S, A, O) are all represented as natural language sequences over a finite token vocabulary.\n\nAt each timestep $t$t, the agent's policy $\\pi_\\theta$\\pi_\\theta generates a thought process $th_t$th_t and an action $a_t$a_t based on the current state $s_t$s_t: $(th_t, a_t) \\sim \\pi_\\theta(\\cdot \\mid s_t)$(th_t, a_t) \\sim \\pi_\\theta(\\cdot \\mid s_t). The agent's interaction with the environment produces a trajectory $\\tau = \\{(o_1, th_1, a_1), (o_2, th_2, a_2), \\ldots, (o_n, th_n, a_n)\\}$\\tau = \\{(o_1, th_1, a_1), (o_2, th_2, a_2), \\ldots, (o_n, th_n, a_n)\\}. In many long-horizon tasks, reward signals are sparse, typically provided only as a final outcome reward $R(\\tau)$R(\\tau) at the end of an episode. This sparsity poses significant challenges for credit assignment. The agent's objective is to learn an optimal policy $\\pi_\\theta$\\pi_\\theta that maximizes the expected cumulative reward:\n\\begin{equation}\n    \\max_\\theta\\ \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ R(\\tau) \\right].\n\\end{equation}\\begin{equation}\n    \\max_\\theta\\ \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ R(\\tau) \\right].\n\\end{equation}\n    \\max_\\theta\\ \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\tau \\sim \\pi_\\theta \\left[ R(\\tau) \\right].\n\n\n\\subsection{Operationalizing Meta-Reasoning in LLM Agents}\n\\label{sec:operations}\n\nOur approach is grounded in metacognitive theory~\\citep{martinez2006metacognition, lai2011metacognition}, which emphasizes ``thinking about thinking''. Metacognition comprises two key components: \\textit{metacognitive knowledge} (an agent's self-awareness of its own reasoning strategies) and \\textit{metacognitive regulation} (the active control of these processes, including planning, monitoring, and adaptive revision). This theoretical lens suggests that for LLM agents to solve complex tasks, they require not just domain knowledge but also the capacity for dynamic planning, self-monitoring, and creative exploration.\n\nTo operationalize these principles, we extend the ReAct framework. While ReAct interleaves reasoning and actions (e.g., ``Think: ..., Act: ...''), it treats reasoning as a monolithic process. We refine this by introducing a structured set of meta-reasoning tags to explicitly represent distinct cognitive functions. This decouples reasoning from actions and enables fine-grained analysis and supervision. Specifically, we define four meta-reasoning tags, each enclosed in XML-style tags (e.g., \\texttt{<planning>}), while all actions are contained within the \\texttt{<action>} tag.\n\n\\begin{itemize}\n    \\item \\textbf{Planning (\\texttt{<planning>}):} Decomposes the task into high-level steps to formulate an overall strategy. Used at the start of a task or when replanning is needed.\n    \\item \\textbf{Exploration (\\texttt{<explore>}):} Generates hypotheses or options to navigate uncertainty or bottlenecks, encouraging creative problem-solving.\n    \\item \\textbf{Reflection (\\texttt{<reflection>}):} Reviews history to analyze errors and formulate corrective actions. Typically triggered after unsuccessful attempts.\n    \\item \\textbf{Monitoring (\\texttt{<monitor>}):} Tracks task progress against the overall plan, ensuring actions remain aligned with subgoals. Applied during routine execution.\n\\end{itemize}\\begin{itemize}\n    \\item \\textbf{Planning (\\texttt{<planning>}):} Decomposes the task into high-level steps to formulate an overall strategy. Used at the start of a task or when replanning is needed.\n    \\item \\textbf{Exploration (\\texttt{<explore>}):} Generates hypotheses or options to navigate uncertainty or bottlenecks, encouraging creative problem-solving.\n    \\item \\textbf{Reflection (\\texttt{<reflection>}):} Reviews history to analyze errors and formulate corrective actions. Typically triggered after unsuccessful attempts.\n    \\item \\textbf{Monitoring (\\texttt{<monitor>}):} Tracks task progress against the overall plan, ensuring actions remain aligned with subgoals. Applied during routine execution.\n\\end{itemize}\n    \\item \\textbf{Planning (\\texttt{<planning>}):} Decomposes the task into high-level steps to formulate an overall strategy. Used at the start of a task or when replanning is needed.\n    \\item \\textbf{Exploration (\\texttt{<explore>}):} Generates hypotheses or options to navigate uncertainty or bottlenecks, encouraging creative problem-solving.\n    \\item \\textbf{Reflection (\\texttt{<reflection>}):} Reviews history to analyze errors and formulate corrective actions. Typically triggered after unsuccessful attempts.\n    \\item \\textbf{Monitoring (\\texttt{<monitor>}):} Tracks task progress against the overall plan, ensuring actions remain aligned with subgoals. Applied during routine execution.\n\n\n\\subsection{Cold Start: Initial Meta-Reasoning Acquisition via SFT}\n\\label{sec:cold_start}\n\nTo equip the base LLM with the foundational ability to generate structured meta-reasoning, we begin with a supervised fine-tuning phase. This step is crucial, as reasoning patterns learned during subsequent reinforcement learning are heavily influenced by the base model's capabilities. The SFT data is constructed as follows:\n\\begin{enumerate}\n    \\item We collect a dataset of successful task trajectories containing only observation-action pairs.\n    \\item We employ a more powerful teacher model (e.g., GPT-4) to annotate these trajectories with our meta-reasoning tags, inferring the most likely cognitive step preceding each action. This process creates synthetic, reasoning-rich expert demonstrations.\n    \\item The target LLM is fine-tuned on these annotated trajectories, learning to imitate the expert's meta-reasoning and action generation patterns.\n\\end{enumerate}\\begin{enumerate}\n    \\item We collect a dataset of successful task trajectories containing only observation-action pairs.\n    \\item We employ a more powerful teacher model (e.g., GPT-4) to annotate these trajectories with our meta-reasoning tags, inferring the most likely cognitive step preceding each action. This process creates synthetic, reasoning-rich expert demonstrations.\n    \\item The target LLM is fine-tuned on these annotated trajectories, learning to imitate the expert's meta-reasoning and action generation patterns.\n\\end{enumerate}\n    \\item We collect a dataset of successful task trajectories containing only observation-action pairs.\n    \\item We employ a more powerful teacher model (e.g., GPT-4) to annotate these trajectories with our meta-reasoning tags, inferring the most likely cognitive step preceding each action. This process creates synthetic, reasoning-rich expert demonstrations.\n    \\item The target LLM is fine-tuned on these annotated trajectories, learning to imitate the expert's meta-reasoning and action generation patterns.\n\n\n\\subsection{\\method}\n\\label{sec:rlvmr}\n\n\\subsubsection{Meta-Reasoning-Aware Reward Shaping}\n\\label{sec:reward_shaping}\n\nDuring reinforcement learning, we guide the agent with a composite reward signal that combines task completion with the quality of the reasoning process. This signal comprises a sparse outcome reward and a dense, process-based meta-reasoning reward.\n\n\\textbf{Outcome Reward ($R(\\tau)$):} A binary signal awarded at the end of a trajectory: $R(\\tau) = r_s$R(\\tau) = r_s for task success and $0$0 otherwise, where $r_s$r_s is a positive constant.\n\n\\textbf{Meta-Reasoning Reward ($r_t^{\\mathrm{MR}}$):} A dense reward assigned at each step $t$t to incentivize locally beneficial behaviors.\n\\begin{itemize}\n    \\item \\textbf{Planning Reward ($r_{\\mathrm{planning}}$):} Awarded for a \\texttt{<planning>} step if the trajectory ultimately succeeds.\n    \\item \\textbf{Exploration Reward ($r_{\\mathrm{explore}}$):} Awarded if the current action targets a new object or location, discouraging redundancy.\n    \\item \\textbf{Reflection Reward ($r_{\\mathrm{reflection}}$):} Awarded if a \\texttt{<reflection>} step is followed by a corrective action after a sequence of failures.\n\\end{itemize}\\begin{itemize}\n    \\item \\textbf{Planning Reward ($r_{\\mathrm{planning}}$):} Awarded for a \\texttt{<planning>} step if the trajectory ultimately succeeds.\n    \\item \\textbf{Exploration Reward ($r_{\\mathrm{explore}}$):} Awarded if the current action targets a new object or location, discouraging redundancy.\n    \\item \\textbf{Reflection Reward ($r_{\\mathrm{reflection}}$):} Awarded if a \\texttt{<reflection>} step is followed by a corrective action after a sequence of failures.\n\\end{itemize}\n    \\item \\textbf{Planning Reward ($r_{\\mathrm{planning}}$):} Awarded for a \\texttt{<planning>} step if the trajectory ultimately succeeds.\n    \\item \\textbf{Exploration Reward ($r_{\\mathrm{explore}}$):} Awarded if the current action targets a new object or location, discouraging redundancy.\n    \\item \\textbf{Reflection Reward ($r_{\\mathrm{reflection}}$):} Awarded if a \\texttt{<reflection>} step is followed by a corrective action after a sequence of failures.\n\n\n\\textbf{Format Reward ($r_t^{\\mathrm{format}}$):} A penalty, $-\\lambda_{\\mathrm{format}}$-\\lambda_{\\mathrm{format}}\\mathrm{format}, is applied if the model's output at step $t$t does not conform to the expected \\texttt{<tag>...<action>...</action>} structure.\n\nThe total step-level reward is the sum of the process-based rewards: $r_t = r_t^{\\mathrm{MR}} + r_t^{\\mathrm{format}}$r_t = r_t^{\\mathrm{MR}}\\mathrm{MR} + r_t^{\\mathrm{format}}\\mathrm{format}.\n\n\\subsubsection{Group Relative Policy Optimization with Meta-Reasoning (GRPO-MR)}\n\\label{sec:GRPO_MR}\n\nTo effectively leverage our composite reward signal, we introduce Meta-Reasoning Group Policy Optimization (GRPO-MR), an algorithm adapted from PPO. GRPO-MR computes a step-level advantage by combining global trajectory performance with local, context-aware reasoning quality.\n\n\\textbf{Trajectory-level Relative Advantage:} For a batch of $K$K trajectories, we first calculate a normalized trajectory-level advantage to capture overall performance:\n\\begin{equation}\n    A^{\\mathrm{traj}}_k = \\frac{R(\\tau_k) - \\mu_R}{\\sigma_R},\n\\end{equation}\\begin{equation}\n    A^{\\mathrm{traj}}_k = \\frac{R(\\tau_k) - \\mu_R}{\\sigma_R},\n\\end{equation}\n    A^{\\mathrm{traj}}\\mathrm{traj}_k = \\frac{R(\\tau_k) - \\mu_R}{\\sigma_R},\n\nwhere $\\mu_R$\\mu_R and $\\sigma_R$\\sigma_R are the mean and standard deviation of outcome rewards across the batch.\n\n\\textbf{Meta-reasoning Level Relative Advantage:} The core of GRPO-MR is the computation of a context-aware advantage. We group all steps within a batch that share the same meta-reasoning tag (e.g., all \\texttt{<explore>} steps) and normalize their rewards \\textit{within} that group:\n\\begin{equation}\n    A^{\\mathrm{MR}}_{t,\\mathrm{tag}} = \\frac{r^{\\mathrm{MR}}_{t,\\mathrm{tag}} - \\mu_{\\mathrm{tag}}}{\\sigma_{\\mathrm{tag}}},\n\\end{equation}\\begin{equation}\n    A^{\\mathrm{MR}}_{t,\\mathrm{tag}} = \\frac{r^{\\mathrm{MR}}_{t,\\mathrm{tag}} - \\mu_{\\mathrm{tag}}}{\\sigma_{\\mathrm{tag}}},\n\\end{equation}\n    A^{\\mathrm{MR}}\\mathrm{MR}_{t,\\mathrm{tag}}t,\\mathrm{tag} = \\frac{r^{\\mathrm{MR}}_{t,\\mathrm{tag}} - \\mu_{\\mathrm{tag}}}{\\sigma_{\\mathrm{tag}}},\n\nwhere $\\mu_{\\mathrm{tag}}$\\mu_{\\mathrm{tag}}\\mathrm{tag} and $\\sigma_{\\mathrm{tag}}$\\sigma_{\\mathrm{tag}}\\mathrm{tag} are the mean and standard deviation of meta-reasoning rewards for all steps with that specific `tag`.\n\nThe final step-level advantage $A_t$A_t is a weighted combination of these two signals:\n\\begin{equation}\n    A_t = \\alpha \\cdot A^{\\text{traj}}_k + (1 - \\alpha) \\cdot A^{\\text{MR}}_{t, \\text{tag}},\n\\end{equation}\\begin{equation}\n    A_t = \\alpha \\cdot A^{\\text{traj}}_k + (1 - \\alpha) \\cdot A^{\\text{MR}}_{t, \\text{tag}},\n\\end{equation}\n    A_t = \\alpha \\cdot A^{\\text{traj}}\\text{traj}_k + (1 - \\alpha) \\cdot A^{\\text{MR}}\\text{MR}_{t, \\text{tag}}t, \\text{tag},\n\nwhere $\\alpha \\in [0, 1]$\\alpha \\in [0, 1] is a hyperparameter balancing the influence of the global outcome and local reasoning quality.\n\nFinally, we optimize the policy $\\pi_\\theta$\\pi_\\theta using a clipped surrogate objective with KL divergence regularization:\n\\begin{equation}\n\\mathcal{L}_{\\text{final}} = \\mathbb{E}_t \\left[ \\min \\left( r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t \\right) \\right] - \\lambda_{\\text{KL}} D_{\\text{KL}}(\\pi_\\theta \\Vert \\pi_{\\text{ref}}),\n\\end{equation}\\begin{equation}\n\\mathcal{L}_{\\text{final}} = \\mathbb{E}_t \\left[ \\min \\left( r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t \\right) \\right] - \\lambda_{\\text{KL}} D_{\\text{KL}}(\\pi_\\theta \\Vert \\pi_{\\text{ref}}),\n\\end{equation}\n\\mathcal{L}_{\\text{final}}\\text{final} = \\mathbb{E}_t \\left[ \\min \\left( r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t \\right) \\right] - \\lambda_{\\text{KL}}\\text{KL} D_{\\text{KL}}\\text{KL}(\\pi_\\theta \\Vert \\pi_{\\text{ref}}\\text{ref}),\n\nwhere $r_t(\\theta)$r_t(\\theta) is the importance sampling ratio, $\\epsilon$\\epsilon is the clipping hyperparameter, and $\\lambda_{\\text{KL}}$\\lambda_{\\text{KL}}\\text{KL} controls the KL penalty against a reference policy $\\pi_{\\text{ref}}$\\pi_{\\text{ref}}\\text{ref}.\n\n\n\n\n", "appendix": false}, "Experiment": {"content": "\n\n\\begin{table*}[t]\n\\centering\n\\setlength{\\tabcolsep}{8pt}\n\\caption{Performance comparison on the ALFWorld and ScienceWorld benchmarks. We report the success rate (\\%) on seen (L0: {\\color{ngreen} seen task variants and categories}) and unseen (L1: {\\color{nred} unseen task variants} but {\\color{ngreen} seen task categories}; L2: {\\color{nred} unseen task variants and categories}) task variations. Our method, RLVMR, consistently outperforms all baselines across both benchmarks and model sizes.}\n\\begin{tabular}{llrrrrrr}\n\\toprule\n\\multirow{2}{*}{\\textbf{Model}}\n& \\multirow{2}{*}{\\textbf{Method}}\n& \\multicolumn{3}{c}{\\textbf{ALFWorld}}\n& \\multicolumn{3}{c}{\\textbf{ScienceWorld}} \\\\\n\\cmidrule(lr){3-5} \\cmidrule(lr){6-8}\n& & \\bf L0 & \\bf L1 & \\bf L2 & \\bf L0 & \\bf L1 & \\bf L2\\\\\n\\midrule\nAgentGym & SFT+RL & 76.6 & 63.3 & -- & 46.9 & 33.6 & -- \\\\\n\\hdashline\nGPT-4o   & \\multirow{3}{*}{ReAct} & 57.3 & 66.0 & 68.8 & 45.4 & 49.2 & 41.0 \\\\\nDeepSeek-V3 & & 60.2 & 65.9 & 53.9 & 27.3 & 35.2 & 26.5 \\\\\nDeepSeek-R1 & & 68.8 & 70.2 & 67.3 & 22.2 & 31.4 & 29.1  \\\\\n\\midrule\n\\multirow{6}{*}{Qwen-1.5B}\n & ReAct & 11.3 & 13.7 & 10.2 & 1.2 & 0.8 & 0.8 \\\\\n & +~SFT   & 43.0 & 38.7 & 17.6 & 20.3 & 18.0 & 12.5 \\\\\n & +~ETO  & 64.1 & 66.4 & 25.8 & 39.1 & 22.7 & 15.6 \\\\\n & +~GRPO  & 76.6 & 71.1 & 29.7 & 21.1 & 13.7 & 10.9 \\\\\n & +~GiGPO & 86.7 & 83.2 & 48.0 & 25.8 & 15.2 & 4.7 \\\\\n & +~RLVMR  & \\textbf{89.1} & \\textbf{87.9} & \\textbf{56.3} & \\textbf{46.9} & \\textbf{34.4} & \\textbf{26.5} \\\\\n\\midrule\n\\multirow{6}{*}{Qwen-7B}\n & ReAct & 23.1 & 28.5 & 27.0 & 7.8 & 11.3 & 6.3 \\\\\n & +~SFT   & 63.3 & 57.0 & 37.5 & 36.7 & 32.0 & 23.4 \\\\\n & +~ETO  & 70.3 & 74.2 & 51.6 & 62.5 & 40.6 & 28.1 \\\\\n & +~GRPO  & 79.3 & 77.3 & 52.3 & 49.1 & 30.1 & 26.6 \\\\\n & +~GiGPO & 89.5 & 90.2 & 67.2 & 53.4 & 35.2 & 25.8 \\\\\n & +~RLVMR  & \\textbf{91.4} & \\textbf{91.8} & \\textbf{83.6} & \\textbf{67.2} & \\textbf{43.0} & \\textbf{32.2} \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:main-results}\n\\end{table*}\n\\centering\n\\setlength{\\tabcolsep}{8pt}\n\\caption{Performance comparison on the ALFWorld and ScienceWorld benchmarks. We report the success rate (\\%) on seen (L0: {\\color{ngreen} seen task variants and categories}) and unseen (L1: {\\color{nred} unseen task variants} but {\\color{ngreen} seen task categories}; L2: {\\color{nred} unseen task variants and categories}) task variations. Our method, RLVMR, consistently outperforms all baselines across both benchmarks and model sizes.}\n\n\\toprule\n\\multirow{2}2{*}*{\\textbf{Model}}\\textbf{Model}\n& \\multirow{2}2{*}*{\\textbf{Method}}\\textbf{Method}\n& \\multicolumn{3}3{c}c{\\textbf{ALFWorld}}\\textbf{ALFWorld}\n& \\multicolumn{3}3{c}c{\\textbf{ScienceWorld}}\\textbf{ScienceWorld} \\\\\n\\cmidrule(lr){3-5}3-5 \\cmidrule(lr){6-8}6-8\n& & \\bf L0 & \\bf L1 & \\bf L2 & \\bf L0 & \\bf L1 & \\bf L2\\\\\n\\midrule\nAgentGym & SFT+RL & 76.6 & 63.3 & -- & 46.9 & 33.6 & -- \\\\\n\\hdashline\nGPT-4o   & \\multirow{3}3{*}*{ReAct}ReAct & 57.3 & 66.0 & 68.8 & 45.4 & 49.2 & 41.0 \\\\\nDeepSeek-V3 & & 60.2 & 65.9 & 53.9 & 27.3 & 35.2 & 26.5 \\\\\nDeepSeek-R1 & & 68.8 & 70.2 & 67.3 & 22.2 & 31.4 & 29.1  \\\\\n\\midrule\n\\multirow{6}6{*}*{Qwen-1.5B}Qwen-1.5B\n & ReAct & 11.3 & 13.7 & 10.2 & 1.2 & 0.8 & 0.8 \\\\\n & +~SFT   & 43.0 & 38.7 & 17.6 & 20.3 & 18.0 & 12.5 \\\\\n & +~ETO  & 64.1 & 66.4 & 25.8 & 39.1 & 22.7 & 15.6 \\\\\n & +~GRPO  & 76.6 & 71.1 & 29.7 & 21.1 & 13.7 & 10.9 \\\\\n & +~GiGPO & 86.7 & 83.2 & 48.0 & 25.8 & 15.2 & 4.7 \\\\\n & +~RLVMR  & \\textbf{89.1} & \\textbf{87.9} & \\textbf{56.3} & \\textbf{46.9} & \\textbf{34.4} & \\textbf{26.5} \\\\\n\\midrule\n\\multirow{6}6{*}*{Qwen-7B}Qwen-7B\n & ReAct & 23.1 & 28.5 & 27.0 & 7.8 & 11.3 & 6.3 \\\\\n & +~SFT   & 63.3 & 57.0 & 37.5 & 36.7 & 32.0 & 23.4 \\\\\n & +~ETO  & 70.3 & 74.2 & 51.6 & 62.5 & 40.6 & 28.1 \\\\\n & +~GRPO  & 79.3 & 77.3 & 52.3 & 49.1 & 30.1 & 26.6 \\\\\n & +~GiGPO & 89.5 & 90.2 & 67.2 & 53.4 & 35.2 & 25.8 \\\\\n & +~RLVMR  & \\textbf{91.4} & \\textbf{91.8} & \\textbf{83.6} & \\textbf{67.2} & \\textbf{43.0} & \\textbf{32.2} \\\\\n\\bottomrule\n\n\\label{tab:main-results}\n\n\n\n\\subsection{Main Results}\n\nIn this section, we present the core experimental results to evaluate the effectiveness of our proposed \\method.\nIn addition to {\\bf ALFWorld}\\bf ALFWorld, we also conduct experiments on {\\bf ScienceWorld}\\bf ScienceWorld~\\citep{wang2022scienceworld}, which focuses on text-based scientific experimentation.\nWe evaluate our approach against two more commonly-used training paradigms in addition to SFT and GRPO:\n\\begin{enumerate}[leftmargin=12pt]\n    \\item \\textbf{ETO}~\\citep{song2024ETO}: A RL method that iteratively refines actions using step-level feedback along trajectories.\n    \\item \\textbf{GiGPO}~\\citep{feng2025gigpo}: A competitive method that introduces a two-level structure for finer-grained credit assignment.\n\\end{enumerate}\\begin{enumerate}[leftmargin=12pt]\n    \\item \\textbf{ETO}~\\citep{song2024ETO}: A RL method that iteratively refines actions using step-level feedback along trajectories.\n    \\item \\textbf{GiGPO}~\\citep{feng2025gigpo}: A competitive method that introduces a two-level structure for finer-grained credit assignment.\n\\end{enumerate}\n    \\item \\textbf{ETO}~\\citep{song2024ETO}: A RL method that iteratively refines actions using step-level feedback along trajectories.\n    \\item \\textbf{GiGPO}~\\citep{feng2025gigpo}: A competitive method that introduces a two-level structure for finer-grained credit assignment.\n\nFor broader comparison, we also report the performance of GPT-4o, DeepSeek-V3/R1 and AgentGym~\\citep{xi2024agentgym}. AgentGym is trained on Llama-2-Chat-7B, first with behavior cloning on the AgentTraj~\\citep{xi2024agentgym} dataset from multiple environments, and then further improved via exploration and self-evolution on a broader instruction set.\nFor the cold-start phase, we perform supervised fine-tuning on 200 trajectories for 5 epochs. In the RL phase, we set the weighting parameter to $\\alpha = 0.5$\\alpha = 0.5 and apply a penalty of $-0.1$-0.1 to the reward for format violations. The maximum number of steps per episode is fixed at 30 for both benchmarks. Our method is trained for 100 epochs in the RL stage, whereas RL-based baselines are trained for 150 epochs.\nDetailed information is provided in Appendix~\\ref{app:setup}.\n\nTable~\\ref{tab:main-results} presents the results, where we have several findings.\n\n\\paragraph{\\method{} achieves new SOTA performance across all benchmarks and model sizes.}\\method{} achieves new SOTA performance across all benchmarks and model sizes.\nOur proposed \\method{} framework consistently sets a new standard for performance, outperforming all baseline methods on both ALFWorld and ScienceWorld. With the Qwen-7B model, \\method{} achieves success rates of 91.4\\% on seen ALFWorld tasks and 67.2\\% on seen ScienceWorld tasks, surpassing the next-best method, GiGPO. This consistent superiority highlights the broad applicability and effectiveness of integrating verifiable meta-reasoning rewards into the RL training loop, leading to more capable and successful agents.\n\n\\paragraph{Rewarding meta-reasoning significantly enhances generalization to unseen tasks.}Rewarding meta-reasoning significantly enhances generalization to unseen tasks.\nA primary contribution of this work is addressing the inefficient exploration issue to improve generalization. Our results validate this claim, showing that \\method{} excels in novel scenarios, especially on the most challenging Unseen-L2 split, which involves entirely new task categories. On ALFWorld's L2 split, our 7B model reaches an impressive 83.6\\% success rate, a substantial 16.4 percentage point improvement over the strongest baseline (GiGPO). Similarly, on ScienceWorld's L2 split, \\method{} outperforms all other methods. This demonstrates that by learning {\\bf how}\\bf how to reason effectively\u2014rather than just memorizing solutions\u2014our agent develops more robust and transferable problem-solving skills, leading to superior performance on unfamiliar challenges.\n\n\\paragraph{\\method{} enables smaller models to outperform much larger ones like GPT-4o and DeepSeek-V3/R1.}\\method{} enables smaller models to outperform much larger ones like GPT-4o and DeepSeek-V3/R1.\nThe efficiency of our approach is underscored by the performance of our smaller models. The Qwen-1.5B model, when trained with \\method{}, achieves a success rate of 87.9\\% on the Unseen-L1 split of ALFWorld, decisively outperforming the much larger and more powerful GPT-4o, which scored 66.0\\% using a standard ReAct prompting strategy. This result powerfully illustrates that targeted, process-level supervision through our meta-reasoning rewards is a more effective and efficient path to high performance than relying solely on the scale capabilities of massive pre-trained models.\n\n\\paragraph{The performance gains are directly attributable to the novel verifiable meta-reasoning rewards.}The performance gains are directly attributable to the novel verifiable meta-reasoning rewards.\nTo isolate the impact of our core contribution, we compare \\method{} against Vanilla-GRPO and GiGPO, which also use trajectory-level optimization but lack process-level rewards. The performance gap is stark. On the ALFWorld L2 split with the 1.5B model, \\method{} (56.3\\%) achieves nearly double the success rate of Vanilla-GRPO (29.7\\%) and significantly surpasses GiGPO (48.1\\%). Since \\method{} builds upon the same GRPO foundation, this substantial improvement can be directly attributed to the dense, verifiable rewards for beneficial reasoning behaviors. This confirms our central hypothesis: explicitly rewarding the {\\bf process}\\bf process of good reasoning, not just the final {\\bf outcome}\\bf outcome, is the key driver of \\method{}'s superior performance and robustness.\n\n\n\\subsection{Exploration Efficiency}\n\n\\begin{figure}[t]\n  \\centering\n  \\subfloat[Invalid Actions (1.5B)]{\\includegraphics[width=0.4\\linewidth]{figures/1.5B_ours_invalid.pdf}} \\hspace{0.1\\linewidth}\n  \\subfloat[Repetitive Actions (1.5B)]{\\includegraphics[width=0.4\\linewidth]{figures/1.5B_ours_repetitive.pdf}}\\\\\\vspace{5pt}\n  \\subfloat[Invlaid Actions (7B)]{\\includegraphics[width=0.4\\linewidth]{figures/7B_ours_invalid.pdf}} \\hspace{0.1\\linewidth}\n  \\subfloat[Repetitive Actions (7B)]{\\includegraphics[width=0.4\\linewidth]{figures/7B_ours_repetitive.pdf}}\n  \\caption{Exploration efficiency of \\method{} compared to SFT and GRPO baselines on ALFWorld. RLVMR consistently and significantly reduces both invalid and repetitive actions across all generalization levels and model sizes, demonstrating its effectiveness at mitigating inefficient exploration.}  \\label{fig:exploration-efficiency}\n\\end{figure}\n  \\centering\n  \\subfloat[Invalid Actions (1.5B)]{\\includegraphics[width=0.4\\linewidth]{figures/1.5B_ours_invalid.pdf}}\\includegraphics[width=0.4\\linewidth]{figures/1.5B_ours_invalid.pdf} \\hspace{0.1\\linewidth}\n  \\subfloat[Repetitive Actions (1.5B)]{\\includegraphics[width=0.4\\linewidth]{figures/1.5B_ours_repetitive.pdf}}\\includegraphics[width=0.4\\linewidth]{figures/1.5B_ours_repetitive.pdf}\\\\\\vspace{5pt}\n  \\subfloat[Invlaid Actions (7B)]{\\includegraphics[width=0.4\\linewidth]{figures/7B_ours_invalid.pdf}}\\includegraphics[width=0.4\\linewidth]{figures/7B_ours_invalid.pdf} \\hspace{0.1\\linewidth}\n  \\subfloat[Repetitive Actions (7B)]{\\includegraphics[width=0.4\\linewidth]{figures/7B_ours_repetitive.pdf}}\\includegraphics[width=0.4\\linewidth]{figures/7B_ours_repetitive.pdf}\n  \\caption{Exploration efficiency of \\method{} compared to SFT and GRPO baselines on ALFWorld. RLVMR consistently and significantly reduces both invalid and repetitive actions across all generalization levels and model sizes, demonstrating its effectiveness at mitigating inefficient exploration.}  \\label{fig:exploration-efficiency}\n\n\n\\iffalse\n\\begin{table*}[t]\n\\centering\n\\caption{Exploration efficiency of our approach.}\n\\begin{tabular}{l rrr rrr rrr}\n\\toprule\n\\multirow{2}{*}{\\textbf{Method}}\n& \\multicolumn{3}{c}{\\textbf{Success Rate ($\\uparrow$)}} & \\multicolumn{3}{c}{\\textbf{\\# Actions ($\\downarrow$)}} & \\multicolumn{3}{c}{\\textbf{Redudance ($\\downarrow$)}}\\\\\n\\cmidrule(lr){2-4} \\cmidrule(lr){5-7}  \\cmidrule(lr){8-10}\n& \\bf L0 & \\bf L1 & \\bf L2 & \\bf L0 & \\bf L1 & \\bf L2 & \\bf L0 & \\bf L1 & \\bf L2\\\\\n\\midrule\n\\multicolumn{10}{c}{\\bf \\em Qwen2.5-1.5B-Instruct}\\\\\n\\bf ReAct   & 11.3 & 13.7 & 10.2 & 76.3 & 79.3 & 82.5 & 10.3 & 11.3 & 16.6\\\\\n\\bf +~SFT   & 43.0 & 38.7 & 17.6 & 8.0 & 11.8 & 24.9 & 10.7 & 20.2 & 21.4\\\\\n\\bf +~GRPO  & 76.6 & 71.1 & 29.7 & 18.8 & 17.3 & 21.6 & 18.4 & 17.6 & 27.1\\\\\n\\bf +~\\method{}  & 89.1 & 87.9 & 56.3 & 11.1 & 14.0 & 12.5 & 6.1 & 4.4 & 5.7\\\\\n\\midrule\n\\multicolumn{10}{c}{\\bf \\em Qwen2.5-7B-Instruct}\\\\\n\\bf ReAct   & 23.1 & 28.5 & 27.0 & 13.0 & 14.7 & 13.3 & 49.0 & 53.3 & 49.6\\\\\n\\bf +~SFT   & 63.3 & 57.0 & 37.5 & 6.2 & 5.5 & 6.3 & 13.9 & 24.5 & 14.4\\\\\n\\bf +~GRPO  & 79.3 & 77.3 & 52.3 & 14.8 & 25.4 & 13.5 & 21.5 & 20.3 & 31.2\\\\\n\\bf +~\\method{}  & 91.4 & 91.8 & 83.6 & 5.6 & 6.5 & 7.1 & 2.3 & 7.8 & 11.7\\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:main-results}\n\\end{table*}\n\\centering\n\\caption{Exploration efficiency of our approach.}\n\n\\toprule\n\\multirow{2}2{*}*{\\textbf{Method}}\\textbf{Method}\n& \\multicolumn{3}3{c}c{\\textbf{Success Rate ($\\uparrow$)}}\\textbf{Success Rate ($\\uparrow$)} & \\multicolumn{3}3{c}c{\\textbf{\\# Actions ($\\downarrow$)}}\\textbf{\\# Actions ($\\downarrow$)} & \\multicolumn{3}3{c}c{\\textbf{Redudance ($\\downarrow$)}}\\textbf{Redudance ($\\downarrow$)}\\\\\n\\cmidrule(lr){2-4}2-4 \\cmidrule(lr){5-7}5-7  \\cmidrule(lr){8-10}8-10\n& \\bf L0 & \\bf L1 & \\bf L2 & \\bf L0 & \\bf L1 & \\bf L2 & \\bf L0 & \\bf L1 & \\bf L2\\\\\n\\midrule\n\\multicolumn{10}10{c}c{\\bf \\em Qwen2.5-1.5B-Instruct}\\bf \\em Qwen2.5-1.5B-Instruct\\\\\n\\bf ReAct   & 11.3 & 13.7 & 10.2 & 76.3 & 79.3 & 82.5 & 10.3 & 11.3 & 16.6\\\\\n\\bf +~SFT   & 43.0 & 38.7 & 17.6 & 8.0 & 11.8 & 24.9 & 10.7 & 20.2 & 21.4\\\\\n\\bf +~GRPO  & 76.6 & 71.1 & 29.7 & 18.8 & 17.3 & 21.6 & 18.4 & 17.6 & 27.1\\\\\n\\bf +~\\method{}  & 89.1 & 87.9 & 56.3 & 11.1 & 14.0 & 12.5 & 6.1 & 4.4 & 5.7\\\\\n\\midrule\n\\multicolumn{10}10{c}c{\\bf \\em Qwen2.5-7B-Instruct}\\bf \\em Qwen2.5-7B-Instruct\\\\\n\\bf ReAct   & 23.1 & 28.5 & 27.0 & 13.0 & 14.7 & 13.3 & 49.0 & 53.3 & 49.6\\\\\n\\bf +~SFT   & 63.3 & 57.0 & 37.5 & 6.2 & 5.5 & 6.3 & 13.9 & 24.5 & 14.4\\\\\n\\bf +~GRPO  & 79.3 & 77.3 & 52.3 & 14.8 & 25.4 & 13.5 & 21.5 & 20.3 & 31.2\\\\\n\\bf +~\\method{}  & 91.4 & 91.8 & 83.6 & 5.6 & 6.5 & 7.1 & 2.3 & 7.8 & 11.7\\\\\n\\bottomrule\n\n\\label{tab:main-results}\n\n\\fi\n\nIn this section, we analyze the agent's behavior during task execution to quantify its exploration efficiency, as shown in Figure~\\ref{fig:exploration-efficiency}. By comparing RLVMR against strong baselines, we provide direct evidence that our verifiable meta-reasoning rewards successfully cultivate more purposeful and efficient problem-solving strategies.\n\n\\paragraph{\\method{} directly mitigates the inefficient exploration problem by cultivating highly efficient and purposeful behavior.}\\method{} directly mitigates the inefficient exploration problem by cultivating highly efficient and purposeful behavior.\nOur approach drastically improves exploration efficiency, providing a direct solution to the problem identified in our contributions. RLVMR significantly reduces both invalid and repetitive actions compared to GRPO, which suffers from inefficient exploration despite its high success rate. For instance, our 7B model reduces the repetitive action rate to just 2.3\\% on seen tasks (L0), a nearly tenfold improvement over GRPO (21.5\\%). Similarly, the invalid action rate is more than halved (5.6\\% vs. 14.8\\%). This confirms that our verifiable meta-reasoning rewards\u2014such as the format penalty and the reward for exploring new states\u2014successfully guide the agent away from flawed or redundant steps, leading to more direct and effective problem-solving.\n\n\\paragraph{The efficiency gains from RLVMR are robust and generalize to unseen tasks, demonstrating superior reasoning quality.}The efficiency gains from RLVMR are robust and generalize to unseen tasks, demonstrating superior reasoning quality.\nA key measure of robustness is whether an agent maintains its efficiency when facing novel challenges. Our analysis confirms that RLVMR excels here. While the GRPO-trained 7B agent's inefficiency worsens on unseen tasks\u2014with its repetitive action rate ballooning from 21.5\\% on L0 to 31.2\\% on the hardest L2 split\u2014our RLVMR-trained agent remains highly efficient, with its rate staying controlled at 11.7\\%. This trend holds for invalid actions as well. This result provides strong evidence for our claim that RLVMR instills a more robust and generalizable reasoning process. Instead of overfitting to familiar paths, our agent learns core problem-solving principles that remain effective in new situations.\n\n\\paragraph{Reduced inefficiency serves as strong evidence of enhanced self-correction and error recovery capabilities.}Reduced inefficiency serves as strong evidence of enhanced self-correction and error recovery capabilities.\nThe marked reduction in flawed behaviors is a direct indicator of the agent's improved ability to self-correct, a cornerstone of robust intelligence. Low rates of repetitive and invalid actions imply that when the agent encounters an error or a dead end, it is less likely to get stuck in a loop~\\citep{yuan2025agentr}. This behavior is a direct outcome of our process-level rewards. The `exploration` reward discourages repeating past actions, while the `reflection` reward (as described in our methodology) incentivizes analyzing failures to find a new, corrective path. These results validate our central hypothesis: by explicitly rewarding the {\\em process}\\em process of good reasoning, RLVMR produces agents that not only find correct answers but also demonstrate the intelligent, adaptive, and robust behavior required to solve complex, long-horizon tasks.\n\n\n\n\\paragraph{Case Study}Case Study Below is a trajectory segment of an agent trained with RLVMR on ALFWorld, performing a novel task: \\textit{find two soapbars and put them in the cabinet}. We observe that, guided by meta-cognitive rewards, the agent demonstrates a more comprehensive and meticulous reasoning process, exhibiting significantly more adaptive reasoning patterns. At step 13, the agent is already carrying one soapbar and is actively searching for the second. Lacking explicit information about the soapbars' location, the agent engages in structured and efficient exploration by systematically hypothesizing possible locations via exploratory actions. Concurrently, it keeps track of its current sub-goal, thereby avoiding the task confusion commonly seen in SFT-trained agents.\n\nNotably, in step 14, the agent experiences a brief hallucination, leading to an ineffective action. However, at step 15, rather than falling into an error loop or blindly continuing, the agent immediately triggers reflective behavior, proposing to \"double-check my inventory\" to collect more information and guard against the possibility of having already acquired both soapbars. This demonstrates a more rigorous and robust reasoning and decision-making process. Experimental results show that such meta-cognitively driven behavioral patterns enhance agent performance and generalization on previously unseen tasks compared to baselines.\n\n\\begin{tcolorbox}[title = {A trajectory segment of an agent trained with RLVMR}, breakable]\n\\textbf{Step 13}\n\n\\textbf{Observation:} You arrive at sinkbasin 2. On the sinkbasin 2, you see nothing.\\\\\n\\textbf{Reasoning:} \\textless explore\\textgreater Since there are no soapbars at sinkbasin 2, I should broaden my search to other potential locations. The next logical area to check is the toilet, as soapbars could potentially be found there.\\textless/explore\\textgreater\\\\\n\\textbf{Action:} go to toilet 1\\\\\n\n\\textbf{Step 14}\n\n\\textbf{Observation:} You arrive at toilet 1. On the toilet 1, you see a candle 1 and a spraybottle 3.\\\\\n\\textbf{Reasoning:} \\textless monitor\\textgreater My current sub-goal is to obtain the second soapbar. Since there are no soapbars visible here, I will attempt to examine the environment further, just in case the second soapbar is present but not immediately listed in the observation.\\textless/monitor\\textgreater\\\\\n\\textbf{Action:} examine soapbar 2\\\\\n\n\\textbf{Step 15}\n\n\\textbf{Observation:} Nothing happens.\\\\\n\\textbf{Reasoning:} \\textless reflection\\textgreater Reflecting on my recent actions, I realize I failed to place the second soapbar in the cabinet earlier. To proceed efficiently, I need to double-check my inventory and ensure I\u2019m actually carrying both soapbars before trying again.\\textless/reflection\\textgreater\\\\\n\\textbf{Action:} inventory\\\\\n\n\\textbf{Step 16}\n\n\\textbf{Observation:} You are carrying: a soapbar 1.\\\\\n\\textbf{Reasoning:} \\textless monitor\\textgreater At this point, my sub-goal is still to locate the second soapbar. Since I am currently only carrying soapbar 1, I need to continue searching for the other one.\\textless/monitor\\textgreater\\\\\n\\textbf{Action:} go to countertop 1\\\\\n\n\\textbf{Step 17}\n\n\\textbf{Observation:} You arrive at countertop 1. On the countertop 1, you see a soapbottle 1, a spraybottle 1, and a toiletpaper 1.\\\\\n\\textbf{Reasoning:} \\textless explore\\textgreater No soapbar is found on the countertop either, so I should expand my search to nearby areas. Checking places like the toilet paper hanger may help ensure I\u2019m not missing any possible locations for the second soapbar.\\textless/explore\\textgreater\\\\\n\\textbf{Action:} go to toiletpaperhanger 1\\\\\n\n\\textbf{Step 18}\n\n\\textbf{Observation:} On the toiletpaperhanger 1, you see a toiletpaper 3.\\\\\n\\textbf{Reasoning:} \\textless explore\\textgreater Since I still haven\u2019t found a new soapbar, I should continue exploring other potential areas, such as the hand towel holders, to make sure I don\u2019t overlook any possible spots where a soapbar could be located.\\textless/explore\\textgreater\\\\\n\\textbf{Action:} go to handtowelholder 1\\\\\n\\end{tcolorbox}\\begin{tcolorbox}[title = {A trajectory segment of an agent trained with RLVMR}, breakable]\n\\textbf{Step 13}\n\n\\textbf{Observation:} You arrive at sinkbasin 2. On the sinkbasin 2, you see nothing.\\\\\n\\textbf{Reasoning:} \\textless explore\\textgreater Since there are no soapbars at sinkbasin 2, I should broaden my search to other potential locations. The next logical area to check is the toilet, as soapbars could potentially be found there.\\textless/explore\\textgreater\\\\\n\\textbf{Action:} go to toilet 1\\\\\n\n\\textbf{Step 14}\n\n\\textbf{Observation:} You arrive at toilet 1. On the toilet 1, you see a candle 1 and a spraybottle 3.\\\\\n\\textbf{Reasoning:} \\textless monitor\\textgreater My current sub-goal is to obtain the second soapbar. Since there are no soapbars visible here, I will attempt to examine the environment further, just in case the second soapbar is present but not immediately listed in the observation.\\textless/monitor\\textgreater\\\\\n\\textbf{Action:} examine soapbar 2\\\\\n\n\\textbf{Step 15}\n\n\\textbf{Observation:} Nothing happens.\\\\\n\\textbf{Reasoning:} \\textless reflection\\textgreater Reflecting on my recent actions, I realize I failed to place the second soapbar in the cabinet earlier. To proceed efficiently, I need to double-check my inventory and ensure I\u2019m actually carrying both soapbars before trying again.\\textless/reflection\\textgreater\\\\\n\\textbf{Action:} inventory\\\\\n\n\\textbf{Step 16}\n\n\\textbf{Observation:} You are carrying: a soapbar 1.\\\\\n\\textbf{Reasoning:} \\textless monitor\\textgreater At this point, my sub-goal is still to locate the second soapbar. Since I am currently only carrying soapbar 1, I need to continue searching for the other one.\\textless/monitor\\textgreater\\\\\n\\textbf{Action:} go to countertop 1\\\\\n\n\\textbf{Step 17}\n\n\\textbf{Observation:} You arrive at countertop 1. On the countertop 1, you see a soapbottle 1, a spraybottle 1, and a toiletpaper 1.\\\\\n\\textbf{Reasoning:} \\textless explore\\textgreater No soapbar is found on the countertop either, so I should expand my search to nearby areas. Checking places like the toilet paper hanger may help ensure I\u2019m not missing any possible locations for the second soapbar.\\textless/explore\\textgreater\\\\\n\\textbf{Action:} go to toiletpaperhanger 1\\\\\n\n\\textbf{Step 18}\n\n\\textbf{Observation:} On the toiletpaperhanger 1, you see a toiletpaper 3.\\\\\n\\textbf{Reasoning:} \\textless explore\\textgreater Since I still haven\u2019t found a new soapbar, I should continue exploring other potential areas, such as the hand towel holders, to make sure I don\u2019t overlook any possible spots where a soapbar could be located.\\textless/explore\\textgreater\\\\\n\\textbf{Action:} go to handtowelholder 1\\\\\n\\end{tcolorbox}[title = {A trajectory segment of an agent trained with RLVMR}A trajectory segment of an agent trained with RLVMR, breakable]\n\\textbf{Step 13}\n\n\\textbf{Observation:} You arrive at sinkbasin 2. On the sinkbasin 2, you see nothing.\\\\\n\\textbf{Reasoning:} \\textless explore\\textgreater Since there are no soapbars at sinkbasin 2, I should broaden my search to other potential locations. The next logical area to check is the toilet, as soapbars could potentially be found there.\\textless/explore\\textgreater\\\\\n\\textbf{Action:} go to toilet 1\\\\\n\n\\textbf{Step 14}\n\n\\textbf{Observation:} You arrive at toilet 1. On the toilet 1, you see a candle 1 and a spraybottle 3.\\\\\n\\textbf{Reasoning:} \\textless monitor\\textgreater My current sub-goal is to obtain the second soapbar. Since there are no soapbars visible here, I will attempt to examine the environment further, just in case the second soapbar is present but not immediately listed in the observation.\\textless/monitor\\textgreater\\\\\n\\textbf{Action:} examine soapbar 2\\\\\n\n\\textbf{Step 15}\n\n\\textbf{Observation:} Nothing happens.\\\\\n\\textbf{Reasoning:} \\textless reflection\\textgreater Reflecting on my recent actions, I realize I failed to place the second soapbar in the cabinet earlier. To proceed efficiently, I need to double-check my inventory and ensure I\u2019m actually carrying both soapbars before trying again.\\textless/reflection\\textgreater\\\\\n\\textbf{Action:} inventory\\\\\n\n\\textbf{Step 16}\n\n\\textbf{Observation:} You are carrying: a soapbar 1.\\\\\n\\textbf{Reasoning:} \\textless monitor\\textgreater At this point, my sub-goal is still to locate the second soapbar. Since I am currently only carrying soapbar 1, I need to continue searching for the other one.\\textless/monitor\\textgreater\\\\\n\\textbf{Action:} go to countertop 1\\\\\n\n\\textbf{Step 17}\n\n\\textbf{Observation:} You arrive at countertop 1. On the countertop 1, you see a soapbottle 1, a spraybottle 1, and a toiletpaper 1.\\\\\n\\textbf{Reasoning:} \\textless explore\\textgreater No soapbar is found on the countertop either, so I should expand my search to nearby areas. Checking places like the toilet paper hanger may help ensure I\u2019m not missing any possible locations for the second soapbar.\\textless/explore\\textgreater\\\\\n\\textbf{Action:} go to toiletpaperhanger 1\\\\\n\n\\textbf{Step 18}\n\n\\textbf{Observation:} On the toiletpaperhanger 1, you see a toiletpaper 3.\\\\\n\\textbf{Reasoning:} \\textless explore\\textgreater Since I still haven\u2019t found a new soapbar, I should continue exploring other potential areas, such as the hand towel holders, to make sure I don\u2019t overlook any possible spots where a soapbar could be located.\\textless/explore\\textgreater\\\\\n\\textbf{Action:} go to handtowelholder 1\\\\\n\n\n\n\n\n\\subsection{Training Efficiency}\n\n\n\\begin{table}[t]\n\\centering\n%\\renewcommand{\\arraystretch}{1.2}\n\\caption{Comparison of the average lengths of action sequences of different methods across various scenarios. A shorter length (number of actions) indicates higher efficiency.}\n\\begin{tabular}{ll ccccc}\n\\toprule\n\\textbf{Environment} & \\textbf{Level}   &   \\bf Vanilla &   \\bf SFT & \\textbf{GRPO} & \\textbf{GiGPO} & \\textbf{RLVMR} \\\\\n\\midrule\n\\multirow{3}{*}{ALFWorld}    \n    & Seen-L0       & 28.8 & 23.2 & 15.8 & 12.6 & \\textbf{10.8}\\\\\n    & Unseen-L1     & 29.1 & 24.5 & 18.1 & 14.7 & \\textbf{11.6}\\\\\n    & Unseen-L2     & 28.9 & 27.5 & 21.7 & 19.4 & \\textbf{15.4}\\\\\n\\midrule\n\\multirow{3}{*}{ScienceWorld}\n    & Seen-L0       & 25.8 & 22.9 & 15.4 & 14.7 & \\textbf{12.5}\\\\\n    & Unseen-L1     & 27.9 & 25.7 & 26.7 & 25.2 & \\textbf{18.8}\\\\\n    & Unseen-L2     & 26.8 & 26.2 & 27.6 & 26.3 & \\textbf{20.5}\\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:len_comparison}\n\\end{table}\n\\centering\n\\caption{Comparison of the average lengths of action sequences of different methods across various scenarios. A shorter length (number of actions) indicates higher efficiency.}\n\n\\toprule\n\\textbf{Environment} & \\textbf{Level}   &   \\bf Vanilla &   \\bf SFT & \\textbf{GRPO} & \\textbf{GiGPO} & \\textbf{RLVMR} \\\\\n\\midrule\n\\multirow{3}3{*}*{ALFWorld}ALFWorld    \n    & Seen-L0       & 28.8 & 23.2 & 15.8 & 12.6 & \\textbf{10.8}\\\\\n    & Unseen-L1     & 29.1 & 24.5 & 18.1 & 14.7 & \\textbf{11.6}\\\\\n    & Unseen-L2     & 28.9 & 27.5 & 21.7 & 19.4 & \\textbf{15.4}\\\\\n\\midrule\n\\multirow{3}3{*}*{ScienceWorld}ScienceWorld\n    & Seen-L0       & 25.8 & 22.9 & 15.4 & 14.7 & \\textbf{12.5}\\\\\n    & Unseen-L1     & 27.9 & 25.7 & 26.7 & 25.2 & \\textbf{18.8}\\\\\n    & Unseen-L2     & 26.8 & 26.2 & 27.6 & 26.3 & \\textbf{20.5}\\\\\n\\bottomrule\n\n\\label{tab:len_comparison}\n\n\n\\begin{figure}[t]\n  \\centering\n    \\includegraphics[width=0.8\\linewidth]{figures/comparison_train.pdf}\n    \\caption{The step count curves of different methods on two datasets during the RL training process.}\n    \\label{fig:traj_length}\n\\end{figure}\n  \\centering\n    \\includegraphics[width=0.8\\linewidth]{figures/comparison_train.pdf}\n    \\caption{The step count curves of different methods on two datasets during the RL training process.}\n    \\label{fig:traj_length}\n\n\nBeyond achieving higher success rates and more efficient exploration, it is crucial to evaluate how efficiently a method learns. In this section, we analyze the training efficiency of \\method{} from two perspectives: the quality of the learned policy, measured by the average length of action trajectories (Table~\\ref{tab:len_comparison}), and the stability of the learning process itself (Figure~\\ref{fig:traj_length}). We demonstrate that agents trained with \\method{} not only find more direct solutions to tasks but also exhibit a more stable and rapid convergence during training compared to baseline methods. This highlights that our process-level rewards provide a clearer and more consistent learning signal, leading to faster and more robust policy optimization.\n\n\\paragraph{\\method{} learns significantly more efficient policies, reducing the number of actions required to solve tasks.}\\method{} learns significantly more efficient policies, reducing the number of actions required to solve tasks.\nAs shown in Table~\\ref{tab:len_comparison}, agents trained with \\method{} consistently find shorter solution paths compared to those trained with GRPO and GiGPO. For instance, on the challenging Unseen-L2 split of ALFWorld, \\method{} requires only 15.4 actions on average, a 28.1\\% reduction from GRPO (21.7 actions) and a 20.6\\% reduction from GiGPO (19.4 actions). This superior efficiency directly addresses the ``inefficient exploration issue'' by penalizing redundant or invalid actions. We attribute this improvement to the verifiable meta-reasoning rewards: the exploration reward discourages revisiting states, while the reflection reward encourages escaping unproductive loops. This mechanism cultivates more direct and purposeful reasoning, enhancing agent robustness as claimed in our contributions.\n\n\\paragraph{\\method{} demonstrates superior training stability and faster convergence to efficient solutions.}\\method{} demonstrates superior training stability and faster convergence to efficient solutions.\nFigure~\\ref{fig:traj_length} illustrates the training dynamics, showing that \\method{} not only achieves shorter final trajectory lengths but also converges much faster. While baseline methods (e.g., GRPO and GiGPO) exhibit unstable or even increasing action counts during training, \\method{}'s action count curve shows a stable and consistent decline. This is particularly evident on ScienceWorld, where baselines struggle. Their action counts increase as they learn to attempt more complex actions without a clear reasoning strategy. In contrast, \\method{}'s cold-start phase provides foundational knowledge, and its dense meta-reasoning rewards offer a stable, process-level learning signal. This prevents the agent from developing inefficient, looping behaviors and validates that our framework cultivates robust reasoning processes rather than just optimizing for final outcomes.\n\n\n\n\\subsection{Ablation Study}\n\\label{sec:ablation}\n\n\\begin{table}[t]\n\\centering\n\\caption{Ablation results on ALFWorld and ScienceWorld (success rates (\\%) on L2 variant).}\n\\begin{tabular}{lrr}\n\\toprule\n\\textbf{Variant} & \\textbf{ALFWorld} & \\textbf{ScienceWorld} \\\\\n\\midrule\n\\method{} (Full) & \\textbf{56.3} & \\textbf{26.5} \\\\\n\\quad w/o $A^{\\text{T}}$ (Outcome Reward) & 12.5 & 7.8 \\\\\n\\quad w/o $A^{\\text{MC}}$ (Meta-Reasoning Reward) & 45.3 & 20.3 \\\\\n\\quad w/o CS (Cold-Start) & 40.6 & 18.8 \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:ablation}\n\\end{table}\n\\centering\n\\caption{Ablation results on ALFWorld and ScienceWorld (success rates (\\%) on L2 variant).}\n\n\\toprule\n\\textbf{Variant} & \\textbf{ALFWorld} & \\textbf{ScienceWorld} \\\\\n\\midrule\n\\method{} (Full) & \\textbf{56.3} & \\textbf{26.5} \\\\\n\\quad w/o $A^{\\text{T}}$A^{\\text{T}}\\text{T} (Outcome Reward) & 12.5 & 7.8 \\\\\n\\quad w/o $A^{\\text{MC}}$A^{\\text{MC}}\\text{MC} (Meta-Reasoning Reward) & 45.3 & 20.3 \\\\\n\\quad w/o CS (Cold-Start) & 40.6 & 18.8 \\\\\n\\bottomrule\n\n\\label{tab:ablation}\n\n\nWe conduct ablation studies on the Unseen-L2 split using Qwen2.5-1.5B-Instruct to analyze the impact of our framework's key components: the trajectory-level outcome advantage signal ($A^{\\text{T}}$A^{\\text{T}}\\text{T}), the meta-reasoning advantage signal ($A^{\\text{MC}}$A^{\\text{MC}}\\text{MC}), and the cold-start process (CS). The results in Table~\\ref{tab:ablation} confirm that each component is critical for achieving optimal performance.\n\n\\paragraph{Verifiable meta-reasoning rewards are essential for tackling complex, unseen tasks.}Verifiable meta-reasoning rewards are essential for tackling complex, unseen tasks.\nRemoving the meta-reasoning advantage signal ($A^{\\text{MC}}$A^{\\text{MC}}\\text{MC}) causes a significant performance drop, with the success rate on ALFWorld falling by 11.0 percentage points (from 56.3\\% to 45.3\\%) and on ScienceWorld by 6.2 points. This variant is equivalent to a standard GRPO agent fine-tuned from the cold-start model. The sharp decline validates our central hypothesis: directly rewarding beneficial reasoning processes is crucial for developing robust problem-solving skills. This component directly addresses the ``inefficient exploration issue'' by providing dense, process-level signals that guide the agent toward more efficient and logical behaviors, a benefit that outcome-only rewards ($A^{\\text{T}}$A^{\\text{T}}\\text{T}) cannot provide alone.\n\n\\paragraph{Outcome-based rewards remain indispensable for guiding the agent toward final task success.}Outcome-based rewards remain indispensable for guiding the agent toward final task success.\nEliminating the trajectory-level outcome advantage ($A^{\\text{T}}$A^{\\text{T}}\\text{T}) results in a catastrophic performance collapse, with the success rate plummeting to just 12.5\\% on ALFWorld and 7.8\\% on ScienceWorld. This demonstrates that while meta-reasoning rewards effectively shape the {\\bf process}\\bf process, the global signal of task success is vital for orienting the agent toward the ultimate goal. The meta-reasoning rewards are locally effective\u2014for instance, rewarding non-repetitive exploration\u2014but without the final outcome signal, the agent cannot learn which explorations ultimately lead to a successful trajectory. This confirms that the synergy between process-level and outcome-level rewards is a key strength of the \\method{} framework.\n\n\\paragraph{A lightweight cold-start phase is critical for bootstrapping the agent's reasoning capabilities.}A lightweight cold-start phase is critical for bootstrapping the agent's reasoning capabilities.\nTraining the agent without the supervised fine-tuning cold-start (CS) phase leads to a substantial performance decrease on both ALFWorld (down 15.7 points) and ScienceWorld (down 7.7 points). The cold-start phase, which uses only 200 trajectories, is not intended to solve the tasks but to equip the model with the basic ability to generate syntactically correct meta-reasoning tags and follow instructions. For smaller models (e.g., 1.5B), this initial grounding is vital; without it, the agent often fails to produce parsable outputs during RL, leading to training instability and policy collapse. This finding underscores the efficiency of our approach: a brief, low-data cold-start is sufficient to unlock the model's capacity for complex reasoning, which is then honed by the RL phase.\n\n\n\n", "appendix": false}, "Related Work": {"content": "\n\n\\paragraph{LLM Reinforcement Learning}LLM Reinforcement Learning\n\nReinforcement learning (RL) has been instrumental in aligning large language models (LLMs) with human preferences. Prominent examples include Reinforcement Learning from Human Feedback (RLHF)~\\citep{ouyang2022RLHF} and Direct Preference Optimization (DPO)~\\citep{rafailov2023DPO}. \nBeyond alignment, recent work has also leveraged RL to enhance other crucial LLM capabilities, such as reasoning~\\citep{hu2025open, muennighoff2025s1} and emotional intelligence~\\citep{rlver}.\nRecently, group-based RL algorithms have emerged as a promising alternative, with methods like GRPO~\\citep{feng2025GRPO}, Dr.GRPO~\\citep{liu2025DrGRPO}, and DAPO~\\citep{yu2025DAPO} estimating advantages by using batches of samples generated from the same prompt. In contrast to actor-critic methods like PPO~\\citep{schulman2017PPO}, this approach to advantage estimation does not require an additional critic model, making large-scale RL training for LLMs more computationally efficient and practical. These approaches have demonstrated significant effectiveness in tasks such as mathematical reasoning, search, and tool use~\\citep{yu2025DAPO, hu2025open}. However, applying these RL methods to multi-turn, long-horizon tasks remains a significant challenge, primarily due to issues of sparse and delayed rewards~\\citep{wang2025ragen}, which is the focus of our work.\n\n\\paragraph{LLM Agents}LLM Agents\nLarge language models (LLMs) are now widely utilized as the core of agentic systems across diverse domains, including code generation~\\citep{huang2023agentcoder, zhang2024codeagent}, web interaction~\\citep{bai2024digirl, agashe2024agents, abuelsaad2024agente}, embodied intelligence~\\citep{zeng2024agenttuning, qiao2024WKM, fu2025agentrefine}, and emotional intelligence~\\citep{sage}. Early approaches primarily leveraged existing pretrained models, employing sophisticated prompting strategies and external tools to enhance performance on complex tasks, as exemplified by methods like ReAct~\\citep{yao2023react, shinn2023reflexion}. However, models with smaller parameter counts often lack the requisite foundational capabilities for such complex reasoning. To address this limitation, some studies employ supervised fine-tuning (SFT) to enhance the models' decision-making abilities~\\citep{zhang2024you, xi2024agentgym, qin2024toolllm}.\nOther approaches explore single-step or offline reinforcement learning to further augment agent performance~\\citep{yu2024steptool,xiong2024watch,zhou2024archer}. More recently, there has been a growing focus on end-to-end reinforcement learning for agents~\\citep{wang2025ragen, feng2025gigpo}, which learn through direct, adaptive online interaction with an environment, thereby obviating the need for complex data preparation or step-level reward models. Nevertheless, these approaches still grapple with challenges in fine-grained credit assignment and generalization~\\citep{wang2025ragen}. In this work, we introduce a novel approach to reward shaping that draws from the perspective of verifiable meta-cognitive behaviors. By explicitly encouraging effective reasoning patterns, our method enhances agent performance and robustness.\n\n\n", "appendix": false}, "Conclusion": {"content": "\n\nIn this work, we addressed the critical problem of {\\bf inefficient exploration}\\bf inefficient exploration in long-horizon agents, where standard RL rewards successful outcomes without ensuring the coherence of the underlying reasoning process. We introduced {\\bf \\method{}}\\bf \\method{}, a novel framework that integrates process-level supervision by providing dense, verifiable rewards for explicit meta-reasoning behaviors such as planning, exploration, and reflection. Our approach, which combines a lightweight cold-start phase with end-to-end policy optimization, effectively shapes the agent's reasoning process to be more robust, efficient, and adaptive.\n\nExtensive experiments on ALFWorld and ScienceWorld demonstrate that \\method{} establishes a new state of the art, significantly improving success rates and generalization to unseen tasks. Crucially, we showed that these performance gains are driven by tangible improvements in reasoning quality, including fewer redundant actions and a markedly improved ability to recover from errors. This work underscores the importance of supervising the reasoning process itself and offers a scalable, effective method for building more reliable and generalizable autonomous agents. Future work could extend \\method{} to multi-modal domains, explore more sophisticated adaptive reward mechanisms, and apply the framework to complex, real-world scenarios such as robotics and software engineering, advancing the development of more capable and trustworthy AI systems.\n\n\n\n\\bibliography{main}\n\\bibliographystyle{colm2024_conference}colm2024_conference\n\n\\clearpage\n\n\\clearpage\n\n\n", "appendix": true}, "Setup Details": {"content": "\n\\label{app:setup}\n\n\\subsection{Dataset Details}\n\\label{app:dataset}\n\n\\textbf{ALFWorld} is a household task environment that requires agents to explore rooms and employ common-sense reasoning to accomplish tasks, such as \"put the pencil on the desk.\" The environment provides feedback on whether the agent successfully completes the task within a given number of steps. ALFWorld describes the environment in purely textual form and supplies a reward signal indicating only the final task completion status.\n\n\\textbf{ScienceWorld} is a text-based virtual environment designed as a comprehensive testbed for evaluating and enhancing scientific reasoning abilities in AI systems. It features tasks spanning 10 scientific domains and 30 subcategories, simulating a broad range of experiments found in elementary science curricula, including state changes of matter, measurement, electricity, life sciences, plant growth, chemical reactions, genetics, among others. Each task involves multiple subgoals, and the final reward is computed based on the completion of these subgoals. However, to better reflect real-world scenarios, we only use the final reward and disregard intermediate rewards. Notably, some tasks in ScienceWorld require agents to make conclusive judgments based on experimental outcomes or common sense; a task is considered successful only if the agent provides the correct final answer.\n\nBoth ALFWorld and ScienceWorld offer ``seen'' and ``unseen'' variants for evaluating generalization capabilities. To further assess the agents\u2019 robustness and generalization, we define three difficulty levels (L0, L1, L2), with L2 comprising entirely held-out task types. Specifically, for ALFWorld, we designate \\textit{Cool \\& Place} and \\textit{Pick Two \\& Place} as held-out tasks; for ScienceWorld, the final task type of each topic is reserved for unseen evaluation.\n\n\\subsection{Implementation Details}\\label{app:implement-details}\n\nWe conducted experiments on both the Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct models. During the cold start phase, we set the batch size per GPU to 16, used a learning rate of $1 \\times 10^{-5}$1 \\times 10^{-5}-5, and trained for 5 epochs. For the RL phase, we adopted the veRL framework with necessary modifications. The batch size per GPU was also set to 16. At each training step, we sampled from 16 distinct environments, with each environment rolling out 8 trajectories. The weights for outcome advantage and meta-reasoning advantage were both set to 0.5 by default. To penalize outputs that did not adhere to the required format, we applied a reward penalty of -0.1. Specifically, an output was considered valid only if it included at least one meta-reasoning tag (e.g., $\\langle \\text{reflection} \\rangle$\\langle \\text{reflection} \\rangle) and one action tag (e.g., $\\langle \\text{action} \\rangle$\\langle \\text{action} \\rangle). The KL regularization coefficient was set to 0.01. For all environments, we allowed a maximum of 30 steps per episode by default.\n\n\n", "appendix": true}, "Detailed Experiment Results": {"content": "\\label{app:detailed-results}\nWe further report the success rates of different methods on various tasks in ALFWorld. Table~\\ref{tab:alfworld-1.5B} provides the results using the Qwen2.5-1.5B model as the base model, while Table~\\ref{tab:alfworld-7B} presents the results using the Qwen2.5-7B model. As shown in the tables, RLVMR generally outperforms other methods across all tasks, and particularly exhibits strong performance in more complex tasks. This demonstrates that RLVMR, by rewarding high-quality reasoning behaviors, significantly enhances the robustness and adaptability of agents in multi-step interactions.\n\n\\begin{table*}[h]\n\\centering\n\\begin{tabular}{lccccccc}\n\\toprule\n\\textbf{Method} & \\textbf{Pick} & \\textbf{Look} & \\textbf{Clean} & \\textbf{Heat} & \\textbf{Cool} & \\textbf{Pick2} & \\textbf{All} \\\\\n\\midrule\nReAct   & 23.1 & 18.3 & 10.8 &  8.7 &  3.5 &  0.0 & 13.7 \\\\\n+SFT    & 43.2 & 42.0 & 35.9 & 33.2 & 29.4 & 29.7 & 38.7 \\\\\n+ETO    & 73.6 & 46.3 & 66.2 & 68.3 & 62.8 & 55.6 & 66.4 \\\\\n+GRPO   & 80.3 & 55.6 & 88.1 & 76.2 & 62.0 & 72.1 & 71.1 \\\\\n+GiGPO  & 92.8 & 66.5 & 90.7 & 90.9 & 80.2 & 73.8 & 83.2 \\\\\n+RLVMR  & 95.2 & 78.8 & 91.2 & 90.2 & 83.9 & 77.6 & 87.9 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Success rates on ALFWorld with Qwen2.5-1.5B model.}\n\\label{tab:alfworld-1.5B}\n\\end{table*}\n\\centering\n\n\\toprule\n\\textbf{Method} & \\textbf{Pick} & \\textbf{Look} & \\textbf{Clean} & \\textbf{Heat} & \\textbf{Cool} & \\textbf{Pick2} & \\textbf{All} \\\\\n\\midrule\nReAct   & 23.1 & 18.3 & 10.8 &  8.7 &  3.5 &  0.0 & 13.7 \\\\\n+SFT    & 43.2 & 42.0 & 35.9 & 33.2 & 29.4 & 29.7 & 38.7 \\\\\n+ETO    & 73.6 & 46.3 & 66.2 & 68.3 & 62.8 & 55.6 & 66.4 \\\\\n+GRPO   & 80.3 & 55.6 & 88.1 & 76.2 & 62.0 & 72.1 & 71.1 \\\\\n+GiGPO  & 92.8 & 66.5 & 90.7 & 90.9 & 80.2 & 73.8 & 83.2 \\\\\n+RLVMR  & 95.2 & 78.8 & 91.2 & 90.2 & 83.9 & 77.6 & 87.9 \\\\\n\\bottomrule\n\n\\caption{Success rates on ALFWorld with Qwen2.5-1.5B model.}\n\\label{tab:alfworld-1.5B}\n\n\n\\begin{table*}[h]\n\\centering\n\\begin{tabular}{lccccccc}\n\\toprule\n\\textbf{Method} & \\textbf{Pick} & \\textbf{Look} & \\textbf{Clean} & \\textbf{Heat} & \\textbf{Cool} & \\textbf{Pick2} & \\textbf{All} \\\\\n\\midrule\nReAct   & 43.1 & 33.2 & 18.7 & 16.4 & 20.2 & 12.8 & 28.5 \\\\\n+SFT    & 70.8 & 63.0 & 61.1 & 46.3 & 49.7 & 33.2 & 57.0 \\\\\n+ETO    & 88.2 & 70.5 & 82.3 & 83.6 & 71.0 & 51.2 & 74.2 \\\\\n+GRPO   & 90.2 & 76.7 & 86.0 & 80.1 & 68.3 & 56.4 & 77.3 \\\\\n+GiGPO  & 91.7 & 85.9 & 93.3 & 90.3 & 89.0 & 83.6 & 90.2 \\\\\n+RLVMR  & 95.3 & 88.2 & 90.1 & 92.4 & 89.8 & 86.7 & 91.8 \\\\\n\\bottomrule\n\\end{tabular}\n\\caption{Success rates on ALFWorld with Qwen2.5-7B model.}\n\\label{tab:alfworld-7B}\n\\end{table*}\n\\centering\n\n\\toprule\n\\textbf{Method} & \\textbf{Pick} & \\textbf{Look} & \\textbf{Clean} & \\textbf{Heat} & \\textbf{Cool} & \\textbf{Pick2} & \\textbf{All} \\\\\n\\midrule\nReAct   & 43.1 & 33.2 & 18.7 & 16.4 & 20.2 & 12.8 & 28.5 \\\\\n+SFT    & 70.8 & 63.0 & 61.1 & 46.3 & 49.7 & 33.2 & 57.0 \\\\\n+ETO    & 88.2 & 70.5 & 82.3 & 83.6 & 71.0 & 51.2 & 74.2 \\\\\n+GRPO   & 90.2 & 76.7 & 86.0 & 80.1 & 68.3 & 56.4 & 77.3 \\\\\n+GiGPO  & 91.7 & 85.9 & 93.3 & 90.3 & 89.0 & 83.6 & 90.2 \\\\\n+RLVMR  & 95.3 & 88.2 & 90.1 & 92.4 & 89.8 & 86.7 & 91.8 \\\\\n\\bottomrule\n\n\\caption{Success rates on ALFWorld with Qwen2.5-7B model.}\n\\label{tab:alfworld-7B}\n\n\n", "appendix": true}, "Prompts": {"content": "\\label{app:prompts}\n\nBelow are the prompts we used in the ALFWorld and ScienceWorld environments.\n\n\n\n\\begin{tcolorbox}[title = {Prompt Template for ALFWorld Enviroment}, breakable]\nYou are an expert agent operating in the \\textbf{ALFRED Embodied Environment}. Your task is to: \\texttt{\\{task\\_description\\}}\n\n\\vspace{0.2em}\n\n\\textbf{Prior to this step, you have already taken \\texttt{\\{step\\_count\\}} step(s).}\n\nBelow are the most recent \\texttt{\\{history\\_length\\}} observations and the corresponding actions you took: \\texttt{\\{action\\_history\\}}\n\nYou are now at step \\texttt{\\{current\\_step\\}} and your current observation is: \\texttt{\\{current\\_observation\\}}\n\n\\vspace{0.5em}\n\n\\textbf{Your admissible actions of the current situation are:} \\texttt{\\{admissible\\_actions\\}}.\n\n\\vspace{0.5em}\n\n\\textbf{Your previous overall plan is:} \\texttt{\\{planning\\}}. Please strictly adhere to your plan.\n\n\\vspace{0.8em}\n\nNow it's your turn to take an action, following these steps:\n\n\\begin{enumerate}\n    \\item \\textbf{First, reason using \\emph{ONLY ONE} tag pair and express your reasoning in \\emph{one concise, brief sentence}:}\n    \n    \\begin{itemize}\n      \\item \\texttt{<planning>} Plan or replan the entire task by breaking it down into high-level steps. Focus on outlining the full sequence required to complete the overall task, not just the immediate next action. Use this at the beginning of complex tasks or whenever the previous plan is incorrect or insufficient. It is necessary to list all the points separately. eg, step 1: xxx, step 2: xxx, step 3: xxx, etc.\n      \\item \\texttt{<explore>} When results are unexpected or information is lacking, use current observations to think outside the box and list as many possible locations, items, or actions as possible. Use this approach when facing obstacles that require creative and innovative thinking.\n      \\item \\texttt{<reflection>} Analyze the reasons for errors in task execution and correct them by exploring alternative approaches. 'No known action matches that input.' indicates the action is invalid. This is typically used when several consecutive actions yield no substantial progress.\n      \\item \\texttt{<monitor>} Continuously track the current progress and history of reasoning and execution throughout the task. Recall the current subgoal and consider the next concrete action, ensuring agent alignment with the overall plan. Typically used when task outcomes are as expected and no other mode of reasoning is required.\n    \\end{itemize}\n\n    \\item \\textbf{After your reasoning, you \\emph{MUST} select and present an admissible action for the current step within \\texttt{<action>} \\ldots \\texttt{</action>} tags.}\n\n    Specify the next action the agent should take to progress toward the task goal, following these guidelines:\n    \\begin{enumerate}\n        \\item \\textbf{Object and Receptacle References:} Use specific identifiers:\n        \\begin{itemize}\n            \\item \\texttt{[obj id]} for objects (e.g., apple 1).\n            \\item \\texttt{[recep id]} for receptacles (e.g., countertop 1).\n        \\end{itemize}\n        \\item \\textbf{Action Validity:} Follow the exact format below. Any deviation renders the action invalid:\n        \\begin{itemize}\n            \\item Valid actions: \\texttt{go to [recep id]}, \\texttt{take [obj id] from [recep id]}, \\texttt{put [obj id] in/on [recep id]}, \\texttt{open/close [recep id]}, \\texttt{use [obj id]}, \\texttt{heat/cool/clean [obj id] with [recep id]}.\n        \\end{itemize}\n    \\end{enumerate}\n\\end{enumerate}\n\n\\end{tcolorbox}\\begin{tcolorbox}[title = {Prompt Template for ALFWorld Enviroment}, breakable]\nYou are an expert agent operating in the \\textbf{ALFRED Embodied Environment}. Your task is to: \\texttt{\\{task\\_description\\}}\n\n\\vspace{0.2em}\n\n\\textbf{Prior to this step, you have already taken \\texttt{\\{step\\_count\\}} step(s).}\n\nBelow are the most recent \\texttt{\\{history\\_length\\}} observations and the corresponding actions you took: \\texttt{\\{action\\_history\\}}\n\nYou are now at step \\texttt{\\{current\\_step\\}} and your current observation is: \\texttt{\\{current\\_observation\\}}\n\n\\vspace{0.5em}\n\n\\textbf{Your admissible actions of the current situation are:} \\texttt{\\{admissible\\_actions\\}}.\n\n\\vspace{0.5em}\n\n\\textbf{Your previous overall plan is:} \\texttt{\\{planning\\}}. Please strictly adhere to your plan.\n\n\\vspace{0.8em}\n\nNow it's your turn to take an action, following these steps:\n\n\\begin{enumerate}\n    \\item \\textbf{First, reason using \\emph{ONLY ONE} tag pair and express your reasoning in \\emph{one concise, brief sentence}:}\n    \n    \\begin{itemize}\n      \\item \\texttt{<planning>} Plan or replan the entire task by breaking it down into high-level steps. Focus on outlining the full sequence required to complete the overall task, not just the immediate next action. Use this at the beginning of complex tasks or whenever the previous plan is incorrect or insufficient. It is necessary to list all the points separately. eg, step 1: xxx, step 2: xxx, step 3: xxx, etc.\n      \\item \\texttt{<explore>} When results are unexpected or information is lacking, use current observations to think outside the box and list as many possible locations, items, or actions as possible. Use this approach when facing obstacles that require creative and innovative thinking.\n      \\item \\texttt{<reflection>} Analyze the reasons for errors in task execution and correct them by exploring alternative approaches. 'No known action matches that input.' indicates the action is invalid. This is typically used when several consecutive actions yield no substantial progress.\n      \\item \\texttt{<monitor>} Continuously track the current progress and history of reasoning and execution throughout the task. Recall the current subgoal and consider the next concrete action, ensuring agent alignment with the overall plan. Typically used when task outcomes are as expected and no other mode of reasoning is required.\n    \\end{itemize}\n\n    \\item \\textbf{After your reasoning, you \\emph{MUST} select and present an admissible action for the current step within \\texttt{<action>} \\ldots \\texttt{</action>} tags.}\n\n    Specify the next action the agent should take to progress toward the task goal, following these guidelines:\n    \\begin{enumerate}\n        \\item \\textbf{Object and Receptacle References:} Use specific identifiers:\n        \\begin{itemize}\n            \\item \\texttt{[obj id]} for objects (e.g., apple 1).\n            \\item \\texttt{[recep id]} for receptacles (e.g., countertop 1).\n        \\end{itemize}\n        \\item \\textbf{Action Validity:} Follow the exact format below. Any deviation renders the action invalid:\n        \\begin{itemize}\n            \\item Valid actions: \\texttt{go to [recep id]}, \\texttt{take [obj id] from [recep id]}, \\texttt{put [obj id] in/on [recep id]}, \\texttt{open/close [recep id]}, \\texttt{use [obj id]}, \\texttt{heat/cool/clean [obj id] with [recep id]}.\n        \\end{itemize}\n    \\end{enumerate}\n\\end{enumerate}\n\n\\end{tcolorbox}[title = {Prompt Template for ALFWorld Enviroment}Prompt Template for ALFWorld Enviroment, breakable]\nYou are an expert agent operating in the \\textbf{ALFRED Embodied Environment}. Your task is to: \\texttt{\\{task\\_description\\}}\n\n\\vspace{0.2em}\n\n\\textbf{Prior to this step, you have already taken \\texttt{\\{step\\_count\\}} step(s).}\n\nBelow are the most recent \\texttt{\\{history\\_length\\}} observations and the corresponding actions you took: \\texttt{\\{action\\_history\\}}\n\nYou are now at step \\texttt{\\{current\\_step\\}} and your current observation is: \\texttt{\\{current\\_observation\\}}\n\n\\vspace{0.5em}\n\n\\textbf{Your admissible actions of the current situation are:} \\texttt{\\{admissible\\_actions\\}}.\n\n\\vspace{0.5em}\n\n\\textbf{Your previous overall plan is:} \\texttt{\\{planning\\}}. Please strictly adhere to your plan.\n\n\\vspace{0.8em}\n\nNow it's your turn to take an action, following these steps:\n\n\n    \\item \\textbf{First, reason using \\emph{ONLY ONE} tag pair and express your reasoning in \\emph{one concise, brief sentence}:}\n    \n    \n      \\item \\texttt{<planning>} Plan or replan the entire task by breaking it down into high-level steps. Focus on outlining the full sequence required to complete the overall task, not just the immediate next action. Use this at the beginning of complex tasks or whenever the previous plan is incorrect or insufficient. It is necessary to list all the points separately. eg, step 1: xxx, step 2: xxx, step 3: xxx, etc.\n      \\item \\texttt{<explore>} When results are unexpected or information is lacking, use current observations to think outside the box and list as many possible locations, items, or actions as possible. Use this approach when facing obstacles that require creative and innovative thinking.\n      \\item \\texttt{<reflection>} Analyze the reasons for errors in task execution and correct them by exploring alternative approaches. 'No known action matches that input.' indicates the action is invalid. This is typically used when several consecutive actions yield no substantial progress.\n      \\item \\texttt{<monitor>} Continuously track the current progress and history of reasoning and execution throughout the task. Recall the current subgoal and consider the next concrete action, ensuring agent alignment with the overall plan. Typically used when task outcomes are as expected and no other mode of reasoning is required.\n    \n\n    \\item \\textbf{After your reasoning, you \\emph{MUST} select and present an admissible action for the current step within \\texttt{<action>} \\ldots \\texttt{</action>} tags.}\n\n    Specify the next action the agent should take to progress toward the task goal, following these guidelines:\n    \n        \\item \\textbf{Object and Receptacle References:} Use specific identifiers:\n        \n            \\item \\texttt{[obj id]} for objects (e.g., apple 1).\n            \\item \\texttt{[recep id]} for receptacles (e.g., countertop 1).\n        \n        \\item \\textbf{Action Validity:} Follow the exact format below. Any deviation renders the action invalid:\n        \n            \\item Valid actions: \\texttt{go to [recep id]}, \\texttt{take [obj id] from [recep id]}, \\texttt{put [obj id] in/on [recep id]}, \\texttt{open/close [recep id]}, \\texttt{use [obj id]}, \\texttt{heat/cool/clean [obj id] with [recep id]}.\n        \n    \n\n\n\n\n\\begin{tcolorbox}[title = {Prompt Template for ScienceWorld Environment}, breakable]\nYou are an expert agent operating in the \\textbf{ScienceWorld} environment, which is a text-based virtual environment centered around accomplishing tasks from the elementary science curriculum.\n\n\\vspace{0.2em}\n\n\\textbf{Your current task is:} \\texttt{\\{task\\_description\\}}\n\n\\vspace{0.5em}\n\n\\textbf{Prior to this step, you have already taken \\texttt{\\{step\\_count\\}} step(s).} \n\nBelow are the most recent \\texttt{\\{history\\_length\\}} observations and the corresponding actions you took: \\texttt{\\{action\\_history\\}}\n\nYou are now at step \\texttt{\\{current\\_step\\}} and your current observation is: \\texttt{\\{current\\_observation\\}}\n\n\\vspace{0.5em}\n\n\\textbf{Here are the actions you may take:}\n\\begin{itemize}\n    \\item \\texttt{\\{\"action\": \"open OBJ\", \"description\": \"open a container\"\\}}\n    \\item \\texttt{\\{\"action\": \"close OBJ\", \"description\": \"close a container\"\\}}\n    \\item \\texttt{\\{\"action\": \"activate OBJ\", \"description\": \"activate a device\"\\}}\n    \\item \\texttt{\\{\"action\": \"deactivate OBJ\", \"description\": \"deactivate a device\"\\}}\n    \\item \\texttt{\\{\"action\": \"connect OBJ to OBJ\", \"description\": \"connect electrical components\"\\}}\n    \\item \\texttt{\\{\"action\": \"disconnect OBJ\", \"description\": \"disconnect electrical components\"\\}}\n    \\item \\texttt{\\{\"action\": \"use OBJ [on OBJ]\", \"description\": \"use a device/item\"\\}}\n    \\item \\texttt{\\{\"action\": \"look around\", \"description\": \"describe the current room\"\\}}\n    \\item \\texttt{\\{\"action\": \"look at OBJ\", \"description\": \"describe an object in detail\"\\}}\n    \\item \\texttt{\\{\"action\": \"look in OBJ\", \"description\": \"describe a container's contents\"\\}}\n    \\item \\texttt{\\{\"action\": \"read OBJ\", \"description\": \"read a note or book\"\\}}\n    \\item \\texttt{\\{\"action\": \"move OBJ to OBJ\", \"description\": \"move an object to a container\"\\}}\n    \\item \\texttt{\\{\"action\": \"pick up OBJ\", \"description\": \"move an object to the inventory\"\\}}\n    \\item \\texttt{\\{\"action\": \"put down OBJ\", \"description\": \"drop an inventory item\"\\}}\n    \\item \\texttt{\\{\"action\": \"pour OBJ into OBJ\", \"description\": \"pour a liquid into a container\"\\}}\n    \\item \\texttt{\\{\"action\": \"dunk OBJ into OBJ\", \"description\": \"dunk a container into a liquid\"\\}}\n    \\item \\texttt{\\{\"action\": \"mix OBJ\", \"description\": \"chemically mix a container\"\\}}\n    \\item \\texttt{\\{\"action\": \"go to LOC\", \"description\": \"move to a new location\"\\}}\n    \\item \\texttt{\\{\"action\": \"eat OBJ\", \"description\": \"eat a food\"\\}}\n    \\item \\texttt{\\{\"action\": \"flush OBJ\", \"description\": \"flush a toilet\"\\}}\n    \\item \\texttt{\\{\"action\": \"focus on OBJ\", \"description\": \"signal intent on a task object\"\\}}\n    \\item \\texttt{\\{\"action\": \"wait\", \"description\": \"take no action for 10 iterations\"\\}}\n    \\item \\texttt{\\{\"action\": \"wait1\", \"description\": \"take no action for 1 iteration\"\\}}\n    \\item \\texttt{\\{\"action\": \"task\", \"description\": \"describe current task\"\\}}\n    \\item \\texttt{\\{\"action\": \"inventory\", \"description\": \"list your inventory\"\\}}\n\\end{itemize}\n\n\\vspace{0.5em}\n\n\\textbf{Your previous overall plan is:} \\texttt{\\{planning\\}}.\n\nPlease strictly adhere to your plan.\n\n\\vspace{0.8em}\n\nNow it's your turn to take an action, following these steps:\n\n\\begin{enumerate}\n    \\item \\textbf{First, reason using \\emph{ONLY ONE} tag pair and express your reasoning in \\emph{one concise, brief sentence}:}\n    \n    \\begin{itemize}\n      \\item \\texttt{<planning>} \\\\\n        Plan or replan the entire task by breaking it down into high-level steps. Focus on outlining the full sequence required to complete the overall task, not just the immediate next action. \\\\\n        Use this at the beginning of complex tasks or whenever the previous plan is incorrect or insufficient. \\\\\n        It is necessary to list all the points separately. eg, step 1: xxx, step 2: xxx, step 3: xxx, etc.\n      \\item \\texttt{<explore>} \\\\\n        When results are unexpected or information is lacking, use current observations to think outside the box and list as many possible locations, items, or actions as possible. \\\\\n        Use this approach when facing obstacles that require creative and innovative thinking.\n      \\item \\texttt{<reflection>} \\\\\n        Analyze the reasons for errors in task execution and correct them by exploring alternative approaches. 'No known action matches that input.' indicates the action is invalid. \\\\\n        This is typically used when several consecutive actions yield no substantial progress.\n      \\item \\texttt{<monitor>} \\\\\n        Continuously track the current progress and history of reasoning and execution throughout the task. Recall the current subgoal and consider the next concrete action, ensuring agent alignment with the overall plan. \\\\\n        Typically used when task outcomes are as expected and no other mode of reasoning is required.\n    \\end{itemize}\n\n    \\item \\textbf{After your reasoning, you \\emph{MUST} select and present an appropriate action for the current step within \\texttt{<action>} \\ldots \\texttt{</action>} tags.}\n\\end{enumerate}\n\n\\end{tcolorbox}\\begin{tcolorbox}[title = {Prompt Template for ScienceWorld Environment}, breakable]\nYou are an expert agent operating in the \\textbf{ScienceWorld} environment, which is a text-based virtual environment centered around accomplishing tasks from the elementary science curriculum.\n\n\\vspace{0.2em}\n\n\\textbf{Your current task is:} \\texttt{\\{task\\_description\\}}\n\n\\vspace{0.5em}\n\n\\textbf{Prior to this step, you have already taken \\texttt{\\{step\\_count\\}} step(s).} \n\nBelow are the most recent \\texttt{\\{history\\_length\\}} observations and the corresponding actions you took: \\texttt{\\{action\\_history\\}}\n\nYou are now at step \\texttt{\\{current\\_step\\}} and your current observation is: \\texttt{\\{current\\_observation\\}}\n\n\\vspace{0.5em}\n\n\\textbf{Here are the actions you may take:}\n\\begin{itemize}\n    \\item \\texttt{\\{\"action\": \"open OBJ\", \"description\": \"open a container\"\\}}\n    \\item \\texttt{\\{\"action\": \"close OBJ\", \"description\": \"close a container\"\\}}\n    \\item \\texttt{\\{\"action\": \"activate OBJ\", \"description\": \"activate a device\"\\}}\n    \\item \\texttt{\\{\"action\": \"deactivate OBJ\", \"description\": \"deactivate a device\"\\}}\n    \\item \\texttt{\\{\"action\": \"connect OBJ to OBJ\", \"description\": \"connect electrical components\"\\}}\n    \\item \\texttt{\\{\"action\": \"disconnect OBJ\", \"description\": \"disconnect electrical components\"\\}}\n    \\item \\texttt{\\{\"action\": \"use OBJ [on OBJ]\", \"description\": \"use a device/item\"\\}}\n    \\item \\texttt{\\{\"action\": \"look around\", \"description\": \"describe the current room\"\\}}\n    \\item \\texttt{\\{\"action\": \"look at OBJ\", \"description\": \"describe an object in detail\"\\}}\n    \\item \\texttt{\\{\"action\": \"look in OBJ\", \"description\": \"describe a container's contents\"\\}}\n    \\item \\texttt{\\{\"action\": \"read OBJ\", \"description\": \"read a note or book\"\\}}\n    \\item \\texttt{\\{\"action\": \"move OBJ to OBJ\", \"description\": \"move an object to a container\"\\}}\n    \\item \\texttt{\\{\"action\": \"pick up OBJ\", \"description\": \"move an object to the inventory\"\\}}\n    \\item \\texttt{\\{\"action\": \"put down OBJ\", \"description\": \"drop an inventory item\"\\}}\n    \\item \\texttt{\\{\"action\": \"pour OBJ into OBJ\", \"description\": \"pour a liquid into a container\"\\}}\n    \\item \\texttt{\\{\"action\": \"dunk OBJ into OBJ\", \"description\": \"dunk a container into a liquid\"\\}}\n    \\item \\texttt{\\{\"action\": \"mix OBJ\", \"description\": \"chemically mix a container\"\\}}\n    \\item \\texttt{\\{\"action\": \"go to LOC\", \"description\": \"move to a new location\"\\}}\n    \\item \\texttt{\\{\"action\": \"eat OBJ\", \"description\": \"eat a food\"\\}}\n    \\item \\texttt{\\{\"action\": \"flush OBJ\", \"description\": \"flush a toilet\"\\}}\n    \\item \\texttt{\\{\"action\": \"focus on OBJ\", \"description\": \"signal intent on a task object\"\\}}\n    \\item \\texttt{\\{\"action\": \"wait\", \"description\": \"take no action for 10 iterations\"\\}}\n    \\item \\texttt{\\{\"action\": \"wait1\", \"description\": \"take no action for 1 iteration\"\\}}\n    \\item \\texttt{\\{\"action\": \"task\", \"description\": \"describe current task\"\\}}\n    \\item \\texttt{\\{\"action\": \"inventory\", \"description\": \"list your inventory\"\\}}\n\\end{itemize}\n\n\\vspace{0.5em}\n\n\\textbf{Your previous overall plan is:} \\texttt{\\{planning\\}}.\n\nPlease strictly adhere to your plan.\n\n\\vspace{0.8em}\n\nNow it's your turn to take an action, following these steps:\n\n\\begin{enumerate}\n    \\item \\textbf{First, reason using \\emph{ONLY ONE} tag pair and express your reasoning in \\emph{one concise, brief sentence}:}\n    \n    \\begin{itemize}\n      \\item \\texttt{<planning>} \\\\\n        Plan or replan the entire task by breaking it down into high-level steps. Focus on outlining the full sequence required to complete the overall task, not just the immediate next action. \\\\\n        Use this at the beginning of complex tasks or whenever the previous plan is incorrect or insufficient. \\\\\n        It is necessary to list all the points separately. eg, step 1: xxx, step 2: xxx, step 3: xxx, etc.\n      \\item \\texttt{<explore>} \\\\\n        When results are unexpected or information is lacking, use current observations to think outside the box and list as many possible locations, items, or actions as possible. \\\\\n        Use this approach when facing obstacles that require creative and innovative thinking.\n      \\item \\texttt{<reflection>} \\\\\n        Analyze the reasons for errors in task execution and correct them by exploring alternative approaches. 'No known action matches that input.' indicates the action is invalid. \\\\\n        This is typically used when several consecutive actions yield no substantial progress.\n      \\item \\texttt{<monitor>} \\\\\n        Continuously track the current progress and history of reasoning and execution throughout the task. Recall the current subgoal and consider the next concrete action, ensuring agent alignment with the overall plan. \\\\\n        Typically used when task outcomes are as expected and no other mode of reasoning is required.\n    \\end{itemize}\n\n    \\item \\textbf{After your reasoning, you \\emph{MUST} select and present an appropriate action for the current step within \\texttt{<action>} \\ldots \\texttt{</action>} tags.}\n\\end{enumerate}\n\n\\end{tcolorbox}[title = {Prompt Template for ScienceWorld Environment}Prompt Template for ScienceWorld Environment, breakable]\nYou are an expert agent operating in the \\textbf{ScienceWorld} environment, which is a text-based virtual environment centered around accomplishing tasks from the elementary science curriculum.\n\n\\vspace{0.2em}\n\n\\textbf{Your current task is:} \\texttt{\\{task\\_description\\}}\n\n\\vspace{0.5em}\n\n\\textbf{Prior to this step, you have already taken \\texttt{\\{step\\_count\\}} step(s).} \n\nBelow are the most recent \\texttt{\\{history\\_length\\}} observations and the corresponding actions you took: \\texttt{\\{action\\_history\\}}\n\nYou are now at step \\texttt{\\{current\\_step\\}} and your current observation is: \\texttt{\\{current\\_observation\\}}\n\n\\vspace{0.5em}\n\n\\textbf{Here are the actions you may take:}\n\n    \\item \\texttt{\\{\"action\": \"open OBJ\", \"description\": \"open a container\"\\}}\n    \\item \\texttt{\\{\"action\": \"close OBJ\", \"description\": \"close a container\"\\}}\n    \\item \\texttt{\\{\"action\": \"activate OBJ\", \"description\": \"activate a device\"\\}}\n    \\item \\texttt{\\{\"action\": \"deactivate OBJ\", \"description\": \"deactivate a device\"\\}}\n    \\item \\texttt{\\{\"action\": \"connect OBJ to OBJ\", \"description\": \"connect electrical components\"\\}}\n    \\item \\texttt{\\{\"action\": \"disconnect OBJ\", \"description\": \"disconnect electrical components\"\\}}\n    \\item \\texttt{\\{\"action\": \"use OBJ [on OBJ]\", \"description\": \"use a device/item\"\\}}\n    \\item \\texttt{\\{\"action\": \"look around\", \"description\": \"describe the current room\"\\}}\n    \\item \\texttt{\\{\"action\": \"look at OBJ\", \"description\": \"describe an object in detail\"\\}}\n    \\item \\texttt{\\{\"action\": \"look in OBJ\", \"description\": \"describe a container's contents\"\\}}\n    \\item \\texttt{\\{\"action\": \"read OBJ\", \"description\": \"read a note or book\"\\}}\n    \\item \\texttt{\\{\"action\": \"move OBJ to OBJ\", \"description\": \"move an object to a container\"\\}}\n    \\item \\texttt{\\{\"action\": \"pick up OBJ\", \"description\": \"move an object to the inventory\"\\}}\n    \\item \\texttt{\\{\"action\": \"put down OBJ\", \"description\": \"drop an inventory item\"\\}}\n    \\item \\texttt{\\{\"action\": \"pour OBJ into OBJ\", \"description\": \"pour a liquid into a container\"\\}}\n    \\item \\texttt{\\{\"action\": \"dunk OBJ into OBJ\", \"description\": \"dunk a container into a liquid\"\\}}\n    \\item \\texttt{\\{\"action\": \"mix OBJ\", \"description\": \"chemically mix a container\"\\}}\n    \\item \\texttt{\\{\"action\": \"go to LOC\", \"description\": \"move to a new location\"\\}}\n    \\item \\texttt{\\{\"action\": \"eat OBJ\", \"description\": \"eat a food\"\\}}\n    \\item \\texttt{\\{\"action\": \"flush OBJ\", \"description\": \"flush a toilet\"\\}}\n    \\item \\texttt{\\{\"action\": \"focus on OBJ\", \"description\": \"signal intent on a task object\"\\}}\n    \\item \\texttt{\\{\"action\": \"wait\", \"description\": \"take no action for 10 iterations\"\\}}\n    \\item \\texttt{\\{\"action\": \"wait1\", \"description\": \"take no action for 1 iteration\"\\}}\n    \\item \\texttt{\\{\"action\": \"task\", \"description\": \"describe current task\"\\}}\n    \\item \\texttt{\\{\"action\": \"inventory\", \"description\": \"list your inventory\"\\}}\n\n\n\\vspace{0.5em}\n\n\\textbf{Your previous overall plan is:} \\texttt{\\{planning\\}}.\n\nPlease strictly adhere to your plan.\n\n\\vspace{0.8em}\n\nNow it's your turn to take an action, following these steps:\n\n\n    \\item \\textbf{First, reason using \\emph{ONLY ONE} tag pair and express your reasoning in \\emph{one concise, brief sentence}:}\n    \n    \n      \\item \\texttt{<planning>} \\\\\n        Plan or replan the entire task by breaking it down into high-level steps. Focus on outlining the full sequence required to complete the overall task, not just the immediate next action. \\\\\n        Use this at the beginning of complex tasks or whenever the previous plan is incorrect or insufficient. \\\\\n        It is necessary to list all the points separately. eg, step 1: xxx, step 2: xxx, step 3: xxx, etc.\n      \\item \\texttt{<explore>} \\\\\n        When results are unexpected or information is lacking, use current observations to think outside the box and list as many possible locations, items, or actions as possible. \\\\\n        Use this approach when facing obstacles that require creative and innovative thinking.\n      \\item \\texttt{<reflection>} \\\\\n        Analyze the reasons for errors in task execution and correct them by exploring alternative approaches. 'No known action matches that input.' indicates the action is invalid. \\\\\n        This is typically used when several consecutive actions yield no substantial progress.\n      \\item \\texttt{<monitor>} \\\\\n        Continuously track the current progress and history of reasoning and execution throughout the task. Recall the current subgoal and consider the next concrete action, ensuring agent alignment with the overall plan. \\\\\n        Typically used when task outcomes are as expected and no other mode of reasoning is required.\n    \n\n    \\item \\textbf{After your reasoning, you \\emph{MUST} select and present an appropriate action for the current step within \\texttt{<action>} \\ldots \\texttt{</action>} tags.}\n\n\n\n\n\n\n\n\n", "appendix": false}}, "categories": ["cs.LG", "cs.AI"], "published": "2025-07-30 17:00:48+00:00", "primary_category": "cs.LG", "summary": "The development of autonomous agents for complex, long-horizon tasks is a\ncentral goal in AI. However, dominant training paradigms face a critical\nlimitation: reinforcement learning (RL) methods that optimize solely for final\ntask success often reinforce flawed or inefficient reasoning paths, a problem\nwe term inefficient exploration. This leads to agents that are brittle and fail\nto generalize, as they learn to find solutions without learning how to reason\ncoherently. To address this, we introduce RLVMR, a novel framework that\nintegrates dense, process-level supervision into end-to-end RL by rewarding\nverifiable, meta-reasoning behaviors. RLVMR equips an agent to explicitly tag\nits cognitive steps, such as planning, exploration, and reflection, and\nprovides programmatic, rule-based rewards for actions that contribute to\neffective problem-solving. These process-centric rewards are combined with the\nfinal outcome signal and optimized using a critic-free policy gradient method.\nOn the challenging ALFWorld and ScienceWorld benchmarks, RLVMR achieves new\nstate-of-the-art results, with our 7B model reaching an 83.6% success rate on\nthe most difficult unseen task split. Our analysis confirms these gains stem\nfrom improved reasoning quality, including significant reductions in redundant\nactions and enhanced error recovery, leading to more robust, efficient, and\ninterpretable agents."}