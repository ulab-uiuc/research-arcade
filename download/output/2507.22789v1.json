{"title": "G-Core: A Simple, Scalable and Balanced RLHF Trainer", "author": "Junyu Wu", "abstract": "\\begin{abstract}\n\nReinforcement Learning from Human Feedback (RLHF) has become an increasingly popular paradigm for training large language models (LLMs) and diffusion models. While existing RLHF training systems have enabled significant progress, they often face challenges in scaling to multi-modal and diffusion workflows and adapting to dynamic workloads. In particular, current approaches may encounter limitations in controller scalability, flexible resource placement, and efficient orchestration when handling complex RLHF pipelines, especially in scenarios involving dynamic sampling or generative reward modeling.\nIn this paper, we present \\textbf{G-Core}, a simple, scalable, and balanced RLHF training framework designed to address these challenges. G-Core introduces a parallel controller programming model, enabling flexible and efficient orchestration of complex RLHF workflows without the bottlenecks of a single centralized controller. Furthermore, we propose a dynamic placement schema that adaptively partitions resources and schedules workloads, significantly reducing hardware idle time and improving utilization, even under highly variable training conditions.\nG-Core has successfully trained models that support WeChat product features serving a large-scale user base, demonstrating its effectiveness and robustness in real-world scenarios. Our results show that G-Core advances the state of the art in RLHF training, providing a solid foundation for future research and deployment of large-scale, human-aligned models.\n\n\\end{abstract}", "citations": {"gpt4": {"bib_key": "gpt4", "bib_title": "GPT-4 Technical Report", "bib_author ": "OpenAI", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Transformer-based large models and their multimodal extensions have achieved remarkable progress, demonstrating state-of-the-art performance across a wide range of natural language processing, understanding, and generation tasks\\cite{gpt4, llama3, dseekv3, googlegemini, flux, gpt3survey, sd, kimi, attn}", "next_context": "."}], "importance_score": 0.1111111111111111}, "llama3": {"bib_key": "llama3", "bib_title": "The Llama 3 Herd of Models", "bib_author ": "Aaron Grattafiori", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Transformer-based large models and their multimodal extensions have achieved remarkable progress, demonstrating state-of-the-art performance across a wide range of natural language processing, understanding, and generation tasks\\cite{gpt4, llama3, dseekv3, googlegemini, flux, gpt3survey, sd, kimi, attn}", "next_context": "."}, {"section": "Background", "subsection": "Distributed Training of Deep Models", "subsubsection": null, "prev_context": "\\cite{llama3, ringattn, ulysess, fang2024uspunifiedsequenceparallelism}", "next_context": "."}, {"section": "Implementation", "subsection": "Distributed Attention", "subsubsection": null, "prev_context": "To support more complex attention masks (e.g., Gemma-3\\cite{gemma3}), inspired by Llama-3\\cite{llama3}", "next_context": "and open source community contributors, we implemented distributed attention based on CCL all-gather, where we first all-gather the key (K) and value (V) tensors, and then compute attention output for the local query (Q) tensor chunk."}], "importance_score": 1.3611111111111112}, "dseekv3": {"bib_key": "dseekv3", "bib_title": "DeepSeek-V3 Technical Report", "bib_author ": "DeepSeek-AI", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Transformer-based large models and their multimodal extensions have achieved remarkable progress, demonstrating state-of-the-art performance across a wide range of natural language processing, understanding, and generation tasks\\cite{gpt4, llama3, dseekv3, googlegemini, flux, gpt3survey, sd, kimi, attn}", "next_context": "."}], "importance_score": 0.1111111111111111}, "googlegemini": {"bib_key": "googlegemini", "bib_title": "Gemini: A Family of Highly Capable Multimodal Models", "bib_author ": "Gemini Team", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Transformer-based large models and their multimodal extensions have achieved remarkable progress, demonstrating state-of-the-art performance across a wide range of natural language processing, understanding, and generation tasks\\cite{gpt4, llama3, dseekv3, googlegemini, flux, gpt3survey, sd, kimi, attn}", "next_context": "."}], "importance_score": 0.1111111111111111}, "flux": {"bib_key": "flux", "bib_title": "FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space", "bib_author ": "Black Forest Labs", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Transformer-based large models and their multimodal extensions have achieved remarkable progress, demonstrating state-of-the-art performance across a wide range of natural language processing, understanding, and generation tasks\\cite{gpt4, llama3, dseekv3, googlegemini, flux, gpt3survey, sd, kimi, attn}", "next_context": "."}, {"section": "Methods", "subsection": "Parallel Controllers", "subsubsection": null, "prev_context": "Firstly, if we need to transfer large features (such as a large number of images or videos, especially in image generation tasks\\cite{flux, sd, dancegrpo}", "next_context": ") within the control flow, the memory and RPC network bandwidth of a single controller can become bottlenecks."}], "importance_score": 0.4444444444444444}, "gpt3survey": {"bib_key": "gpt3survey", "bib_title": "A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models", "bib_author ": "Junjie Ye", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Transformer-based large models and their multimodal extensions have achieved remarkable progress, demonstrating state-of-the-art performance across a wide range of natural language processing, understanding, and generation tasks\\cite{gpt4, llama3, dseekv3, googlegemini, flux, gpt3survey, sd, kimi, attn}", "next_context": "."}], "importance_score": 0.1111111111111111}, "sd": {"bib_key": "sd", "bib_title": "High-Resolution Image Synthesis with Latent Diffusion Models", "bib_author ": "Robin Rombach", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Transformer-based large models and their multimodal extensions have achieved remarkable progress, demonstrating state-of-the-art performance across a wide range of natural language processing, understanding, and generation tasks\\cite{gpt4, llama3, dseekv3, googlegemini, flux, gpt3survey, sd, kimi, attn}", "next_context": "."}, {"section": "Methods", "subsection": "Parallel Controllers", "subsubsection": null, "prev_context": "Firstly, if we need to transfer large features (such as a large number of images or videos, especially in image generation tasks\\cite{flux, sd, dancegrpo}", "next_context": ") within the control flow, the memory and RPC network bandwidth of a single controller can become bottlenecks."}], "importance_score": 0.4444444444444444}, "kimi": {"bib_key": "kimi", "bib_title": "Kimi k1.5: Scaling Reinforcement Learning with LLMs", "bib_author ": "Kimi Team", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Transformer-based large models and their multimodal extensions have achieved remarkable progress, demonstrating state-of-the-art performance across a wide range of natural language processing, understanding, and generation tasks\\cite{gpt4, llama3, dseekv3, googlegemini, flux, gpt3survey, sd, kimi, attn}", "next_context": "."}], "importance_score": 0.1111111111111111}, "attn": {"bib_key": "attn", "bib_title": "Attention Is All You Need", "bib_author ": "Ashish Vaswani", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Transformer-based large models and their multimodal extensions have achieved remarkable progress, demonstrating state-of-the-art performance across a wide range of natural language processing, understanding, and generation tasks\\cite{gpt4, llama3, dseekv3, googlegemini, flux, gpt3survey, sd, kimi, attn}", "next_context": "."}], "importance_score": 0.1111111111111111}, "mlm1": {"bib_key": "mlm1", "bib_title": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism", "bib_author ": "Mohammad Shoeybi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "To address these challenges, the research community has developed a variety of hybrid parallelism techniques\\cite{mlm1, mlm2, mlm3, mlm4, ringattn}", "next_context": "."}, {"section": "Background", "subsection": "Distributed Training of Deep Models", "subsubsection": null, "prev_context": "Each device is responsible for computing a portion of the operations within a layer, enabling the training of very large models that cannot fit into a single device\u2019s memory\\cite{mlm1}", "next_context": "."}, {"section": "Implementation", "subsection": null, "subsubsection": null, "prev_context": "Our system combines vLLM\\cite{vllm}and SGlang\\cite{sglang}for generation serving with Megatron-Core\\cite{mlm1, mlm2, mlm3, mlm4}", "next_context": "as the training backend."}], "importance_score": 1.45}, "mlm2": {"bib_key": "mlm2", "bib_title": "Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM", "bib_author ": "Deepak Narayanan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "To address these challenges, the research community has developed a variety of hybrid parallelism techniques\\cite{mlm1, mlm2, mlm3, mlm4, ringattn}", "next_context": "."}, {"section": "Background", "subsection": "Distributed Training of Deep Models", "subsubsection": null, "prev_context": "\\cite{mlm2, gpipe, pipedream}", "next_context": "."}, {"section": "Implementation", "subsection": null, "subsubsection": null, "prev_context": "Our system combines vLLM\\cite{vllm}and SGlang\\cite{sglang}for generation serving with Megatron-Core\\cite{mlm1, mlm2, mlm3, mlm4}", "next_context": "as the training backend."}], "importance_score": 0.7833333333333333}, "mlm3": {"bib_key": "mlm3", "bib_title": "Reducing Activation Recomputation in Large Transformer Models", "bib_author ": "Vijay Korthikanti", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "To address these challenges, the research community has developed a variety of hybrid parallelism techniques\\cite{mlm1, mlm2, mlm3, mlm4, ringattn}", "next_context": "."}, {"section": "Implementation", "subsection": null, "subsubsection": null, "prev_context": "Our system combines vLLM\\cite{vllm}and SGlang\\cite{sglang}for generation serving with Megatron-Core\\cite{mlm1, mlm2, mlm3, mlm4}", "next_context": "as the training backend."}], "importance_score": 0.45}, "mlm4": {"bib_key": "mlm4", "bib_title": "MoE Parallel Folding: Heterogeneous Parallelism Mappings for Efficient Large-Scale MoE Model Training with Megatron Core", "bib_author ": "Dennis Liu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "To address these challenges, the research community has developed a variety of hybrid parallelism techniques\\cite{mlm1, mlm2, mlm3, mlm4, ringattn}", "next_context": "."}, {"section": "Background", "subsection": "Distributed Training of Deep Models", "subsubsection": null, "prev_context": "\\cite{mlm4, tutel}", "next_context": "."}, {"section": "Implementation", "subsection": null, "subsubsection": null, "prev_context": "Our system combines vLLM\\cite{vllm}and SGlang\\cite{sglang}for generation serving with Megatron-Core\\cite{mlm1, mlm2, mlm3, mlm4}", "next_context": "as the training backend."}], "importance_score": 0.95}, "ringattn": {"bib_key": "ringattn", "bib_title": "Ring Attention with Blockwise Transformers for Near-Infinite Context", "bib_author ": "Hao Liu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "To address these challenges, the research community has developed a variety of hybrid parallelism techniques\\cite{mlm1, mlm2, mlm3, mlm4, ringattn}", "next_context": "."}, {"section": "Background", "subsection": "Distributed Training of Deep Models", "subsubsection": null, "prev_context": "\\cite{llama3, ringattn, ulysess, fang2024uspunifiedsequenceparallelism}", "next_context": "."}], "importance_score": 0.45}, "qwen25math": {"bib_key": "qwen25math", "bib_title": "Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement", "bib_author ": "An Yang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "With the advent of Reinforcement Learning from Human Feedback (RLHF), the training paradigm for LLMs has evolved further\\cite{qwen25math, dseekr1, dspo}", "next_context": "."}, {"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "Most RLHF workflows largely follow this 4-stage workflow\\cite{ppo, grpo, dapo, dseekr1, qwen25math, qwen3, remax}", "next_context": "(Sometimes, the first three stages are referred to as \"rollout\":\\itemStage 1(\\textbf{Generation}): The actor produces responses from a batch of prompts using auto-regressive generation."}, {"section": "Evaluation", "subsection": "Bradley-Terry Reward Model", "subsubsection": null, "prev_context": "We employ the Qwen 2.5 model series\\cite{qwen25math}", "next_context": "as base models (Qwen2.5-Math-1.5B as the base model and Qwen2.5-Math-RM-72B as the reward model)."}, {"section": "Evaluation", "subsection": "Generative Reward Model", "subsubsection": null, "prev_context": "We employ the Qwen 2.5 model series\\cite{qwen25math}", "next_context": "as base models."}], "importance_score": 2.4761904761904763}, "dseekr1": {"bib_key": "dseekr1", "bib_title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning", "bib_author ": "DeepSeek-AI", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "With the advent of Reinforcement Learning from Human Feedback (RLHF), the training paradigm for LLMs has evolved further\\cite{qwen25math, dseekr1, dspo}", "next_context": "."}, {"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "Most RLHF workflows largely follow this 4-stage workflow\\cite{ppo, grpo, dapo, dseekr1, qwen25math, qwen3, remax}", "next_context": "(Sometimes, the first three stages are referred to as \"rollout\":\\itemStage 1(\\textbf{Generation}): The actor produces responses from a batch of prompts using auto-regressive generation."}, {"section": "Methods", "subsection": "Dynamic Placement", "subsubsection": null, "prev_context": "However, this approach overlooks the fact that the system load can change from an algorithmic perspective\\cite{dseekr1}", "next_context": "."}], "importance_score": 1.4761904761904763}, "dspo": {"bib_key": "dspo", "bib_title": "DSPO: Direct Semantic Preference Optimization for Real-World Image Super-Resolution", "bib_author ": "Miaomiao Cai", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "With the advent of Reinforcement Learning from Human Feedback (RLHF), the training paradigm for LLMs has evolved further\\cite{qwen25math, dseekr1, dspo}", "next_context": "."}], "importance_score": 0.3333333333333333}, "ppo": {"bib_key": "ppo", "bib_title": "Proximal Policy Optimization Algorithms", "bib_author ": "John Schulman", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "RLHF introduces additional complexity by incorporating human preferences into the training loop, typically requiring the orchestration of multiple models within a single training workflow\\cite{ppo, grpo, remax}", "next_context": "."}, {"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "RLHF aligns the linguistic space of LLMs with human preferences, using a set of human-ranked candidates of given prompts\\cite{ppo, bai2022traininghelpfulharmlessassistant}", "next_context": "."}, {"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "Most RLHF workflows largely follow this 4-stage workflow\\cite{ppo, grpo, dapo, dseekr1, qwen25math, qwen3, remax}", "next_context": "(Sometimes, the first three stages are referred to as \"rollout\":\\itemStage 1(\\textbf{Generation}): The actor produces responses from a batch of prompts using auto-regressive generation."}], "importance_score": 0.976190476190476}, "grpo": {"bib_key": "grpo", "bib_title": "Group Robust Preference Optimization in Reward-free RLHF", "bib_author ": "Shyam Sundhar Ramesh", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "RLHF introduces additional complexity by incorporating human preferences into the training loop, typically requiring the orchestration of multiple models within a single training workflow\\cite{ppo, grpo, remax}", "next_context": "."}, {"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "Most RLHF workflows largely follow this 4-stage workflow\\cite{ppo, grpo, dapo, dseekr1, qwen25math, qwen3, remax}", "next_context": "(Sometimes, the first three stages are referred to as \"rollout\":\\itemStage 1(\\textbf{Generation}): The actor produces responses from a batch of prompts using auto-regressive generation."}], "importance_score": 0.47619047619047616}, "remax": {"bib_key": "remax", "bib_title": "ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models", "bib_author ": "Ziniu Li", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "RLHF introduces additional complexity by incorporating human preferences into the training loop, typically requiring the orchestration of multiple models within a single training workflow\\cite{ppo, grpo, remax}", "next_context": "."}, {"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "Most RLHF workflows largely follow this 4-stage workflow\\cite{ppo, grpo, dapo, dseekr1, qwen25math, qwen3, remax}", "next_context": "(Sometimes, the first three stages are referred to as \"rollout\":\\itemStage 1(\\textbf{Generation}): The actor produces responses from a batch of prompts using auto-regressive generation."}], "importance_score": 0.47619047619047616}, "nemoaligner": {"bib_key": "nemoaligner", "bib_title": "NeMo-Aligner: Scalable Toolkit for Efficient Model Alignment", "bib_author ": "Gerald Shen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "To mitigate these issues,\\cite{nemoaligner}", "next_context": "models the RLHF training process as a control flow, leveraging multi-cluster, multi-model co-location, and swapping strategies to reduce peak GPU memory usage."}, {"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "In engineering practice, the generation step is typically implemented using a specialized language model inference engine (e.g., vLLM\\cite{vllm}, sglang\\cite{sglang})\\cite{openrlhf, nemoaligner, verl}", "next_context": "."}], "importance_score": 1.3333333333333333}, "openrlhf": {"bib_key": "openrlhf", "bib_title": "OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework", "bib_author ": "Jian Hu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Other systems\\cite{openrlhf, verl}", "next_context": "have adopted similar approaches, with\\cite{verl}further improving throughput by combining single and multi-controller and statically planned placement schemes."}, {"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "In engineering practice, the generation step is typically implemented using a specialized language model inference engine (e.g., vLLM\\cite{vllm}, sglang\\cite{sglang})\\cite{openrlhf, nemoaligner, verl}", "next_context": "."}, {"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "To simplify the implementation, most systems use a single controller to manage data transfer, intermediate results, and workflow orchestration through a central point\\cite{openrlhf, verl}", "next_context": "."}, {"section": "Background", "subsection": "RLHF Placement", "subsubsection": null, "prev_context": "Although the\\cite{verl}proposed methods for calculating placement based on workload, both the open-source implementations of\\cite{openrlhf}", "next_context": "and\\cite{verl}adopt a co-locate all models strategy."}], "importance_score": 2.333333333333333}, "verl": {"bib_key": "verl", "bib_title": "HybridFlow: A Flexible and Efficient RLHF Framework", "bib_author ": "Sheng, Guangming", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Other systems\\cite{openrlhf, verl}", "next_context": "have adopted similar approaches, with\\cite{verl}further improving throughput by combining single and multi-controller and statically planned placement schemes."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Other systems\\cite{openrlhf, verl}have adopted similar approaches, with\\cite{verl}", "next_context": "further improving throughput by combining single and multi-controller and statically planned placement schemes."}, {"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "In engineering practice, the generation step is typically implemented using a specialized language model inference engine (e.g., vLLM\\cite{vllm}, sglang\\cite{sglang})\\cite{openrlhf, nemoaligner, verl}", "next_context": "."}, {"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "To simplify the implementation, most systems use a single controller to manage data transfer, intermediate results, and workflow orchestration through a central point\\cite{openrlhf, verl}", "next_context": "."}, {"section": "Background", "subsection": "RLHF Placement", "subsubsection": null, "prev_context": "Although the\\cite{verl}", "next_context": "proposed methods for calculating placement based on workload, both the open-source implementations of\\cite{openrlhf}and\\cite{verl}adopt a co-locate all models strategy."}, {"section": "Background", "subsection": "RLHF Placement", "subsubsection": null, "prev_context": "Although the\\cite{verl}proposed methods for calculating placement based on workload, both the open-source implementations of\\cite{openrlhf}and\\cite{verl}", "next_context": "adopt a co-locate all models strategy."}, {"section": "Methods", "subsection": "Parallel Controllers", "subsubsection": null, "prev_context": "Similar to the hybrid controller\\cite{verl}", "next_context": ", each actor internally adopts a multi-controller structure for higher efficiency\\cite{evalgpuinter, vllm}."}, {"section": "Methods", "subsection": "Parallel Controllers", "subsubsection": null, "prev_context": "Contrary to the claims in\\cite{verl}", "next_context": ", we believe that for experienced engineers, adopting a multi-controller architecture does not introduce significant programming challenges."}, {"section": "Methods", "subsection": "Dynamic Placement", "subsubsection": null, "prev_context": "If the workload is dynamic, it is not possible to use the static estimation method from\\cite{verl}", "next_context": "to determine placement."}], "importance_score": 7.333333333333333}, "torchddp": {"bib_key": "torchddp", "bib_title": "PyTorch Distributed: Experiences on Accelerating Data Parallel Training", "bib_author ": "Shen Li", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "Distributed Training of Deep Models", "subsubsection": null, "prev_context": "Gradients are averaged (or summed) across devices to update the model parameters synchronously\\cite{torchddp, horovod}", "next_context": "."}], "importance_score": 0.5}, "horovod": {"bib_key": "horovod", "bib_title": "Horovod: fast and easy distributed deep learning in TensorFlow", "bib_author ": "Alex", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "Distributed Training of Deep Models", "subsubsection": null, "prev_context": "Gradients are averaged (or summed) across devices to update the model parameters synchronously\\cite{torchddp, horovod}", "next_context": "."}], "importance_score": 0.5}, "gpipe": {"bib_key": "gpipe", "bib_title": "GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism", "bib_author ": "Yanping Huang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "Distributed Training of Deep Models", "subsubsection": null, "prev_context": "\\cite{mlm2, gpipe, pipedream}", "next_context": "."}], "importance_score": 0.3333333333333333}, "pipedream": {"bib_key": "pipedream", "bib_title": "PipeDream: Fast and Efficient Pipeline Parallel DNN Training", "bib_author ": "Aaron Harlap", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "Distributed Training of Deep Models", "subsubsection": null, "prev_context": "\\cite{mlm2, gpipe, pipedream}", "next_context": "."}], "importance_score": 0.3333333333333333}, "zero3": {"bib_key": "zero3", "bib_title": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models", "bib_author ": "Samyam Rajbh", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "Distributed Training of Deep Models", "subsubsection": null, "prev_context": "This enables the training of much larger models by reducing memory consumption per device\\cite{zero3, zero1}", "next_context": "."}], "importance_score": 0.5}, "zero1": {"bib_key": "zero1", "bib_title": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models", "bib_author ": "Samyam Rajbh", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "Distributed Training of Deep Models", "subsubsection": null, "prev_context": "This enables the training of much larger models by reducing memory consumption per device\\cite{zero3, zero1}", "next_context": "."}], "importance_score": 0.5}, "ulysess": {"bib_key": "ulysess", "bib_title": "DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models", "bib_author ": "Sam Ade Jacobs", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "Distributed Training of Deep Models", "subsubsection": null, "prev_context": "\\cite{llama3, ringattn, ulysess, fang2024uspunifiedsequenceparallelism}", "next_context": "."}], "importance_score": 0.25}, "fang2024uspunifiedsequenceparallelism": {"bib_key": "fang2024uspunifiedsequenceparallelism", "bib_title": "USP: A Unified Sequence Parallelism Approach for Long Context Generative AI", "bib_author ": "Jiarui Fang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "Distributed Training of Deep Models", "subsubsection": null, "prev_context": "\\cite{llama3, ringattn, ulysess, fang2024uspunifiedsequenceparallelism}", "next_context": "."}], "importance_score": 0.25}, "tutel": {"bib_key": "tutel", "bib_title": "Tutel: Adaptive Mixture-of-Experts at Scale", "bib_author ": "Changho Hwang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "Distributed Training of Deep Models", "subsubsection": null, "prev_context": "\\cite{mlm4, tutel}", "next_context": "."}], "importance_score": 0.5}, "bai2022traininghelpfulharmlessassistant": {"bib_key": "bai2022traininghelpfulharmlessassistant", "bib_title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback", "bib_author ": "Yuntao Bai", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "RLHF aligns the linguistic space of LLMs with human preferences, using a set of human-ranked candidates of given prompts\\cite{ppo, bai2022traininghelpfulharmlessassistant}", "next_context": "."}, {"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "The critic and reward models are usually models tuned in the human preference dataset, with the language modeling head replaced by a numerical output head (Bradley-Terry reward model)\\cite{bai2022traininghelpfulharmlessassistant, ouyang2022traininglanguagemodelsfollow, btrm, rethinkbt}", "next_context": "."}], "importance_score": 0.75}, "ouyang2022traininglanguagemodelsfollow": {"bib_key": "ouyang2022traininglanguagemodelsfollow", "bib_title": "Training language models to follow instructions with human feedback", "bib_author ": "Long Ouyang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "The critic and reward models are usually models tuned in the human preference dataset, with the language modeling head replaced by a numerical output head (Bradley-Terry reward model)\\cite{bai2022traininghelpfulharmlessassistant, ouyang2022traininglanguagemodelsfollow, btrm, rethinkbt}", "next_context": "."}], "importance_score": 0.25}, "btrm": {"bib_key": "btrm", "bib_title": "Rank analysis of incomplete block designs: I. the method of paired comparisons", "bib_author ": "Bradley, Ralph Allan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "The critic and reward models are usually models tuned in the human preference dataset, with the language modeling head replaced by a numerical output head (Bradley-Terry reward model)\\cite{bai2022traininghelpfulharmlessassistant, ouyang2022traininglanguagemodelsfollow, btrm, rethinkbt}", "next_context": "."}], "importance_score": 0.25}, "rethinkbt": {"bib_key": "rethinkbt", "bib_title": "Rethinking Bradley-Terry Models in Preference-Based Reward Modeling: Foundations, Theory, and Alternatives", "bib_author ": "Hao Sun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "The critic and reward models are usually models tuned in the human preference dataset, with the language modeling head replaced by a numerical output head (Bradley-Terry reward model)\\cite{bai2022traininghelpfulharmlessassistant, ouyang2022traininglanguagemodelsfollow, btrm, rethinkbt}", "next_context": "."}], "importance_score": 0.25}, "genrm": {"bib_key": "genrm", "bib_title": "Generative Reward Models", "bib_author ": "Dakota Mahan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "Bradley-Terry reward models can be replaced with generative models, which obtain reward scores through token prediction; furthermore, leveraging chain-of-thought reasoning, the accuracy and final performance of the actor model can be further improved\\cite{genrm, cot}", "next_context": "."}, {"section": "Methods", "subsection": "Dynamic Placement", "subsubsection": null, "prev_context": "The generative rewarding method has been proven effective\\cite{genrm, genverifier}", "next_context": "."}], "importance_score": 1.0}, "cot": {"bib_key": "cot", "bib_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models", "bib_author ": "Jason Wei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "Bradley-Terry reward models can be replaced with generative models, which obtain reward scores through token prediction; furthermore, leveraging chain-of-thought reasoning, the accuracy and final performance of the actor model can be further improved\\cite{genrm, cot}", "next_context": "."}], "importance_score": 0.5}, "dapo": {"bib_key": "dapo", "bib_title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale", "bib_author ": "Qiying Yu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "Most RLHF workflows largely follow this 4-stage workflow\\cite{ppo, grpo, dapo, dseekr1, qwen25math, qwen3, remax}", "next_context": "(Sometimes, the first three stages are referred to as \"rollout\":\\itemStage 1(\\textbf{Generation}): The actor produces responses from a batch of prompts using auto-regressive generation."}, {"section": "Methods", "subsection": "Parallel Controllers", "subsubsection": null, "prev_context": "If we could implement local state transitions, we would be able to support certain special sampling processes, such as dynamic sampling or reward-augmented generation\\cite{dapo, rad}", "next_context": "."}, {"section": "Methods", "subsection": "Dynamic Placement", "subsubsection": null, "prev_context": "\\cite{dapo}", "next_context": "proposes to filter out prompts with the accuracy equal to 1 and 0 (the overall reward for a batch of samples is too high or too low), and trigger re-sampling."}], "importance_score": 1.6428571428571428}, "qwen3": {"bib_key": "qwen3", "bib_title": "Qwen3 Technical Report", "bib_author ": "An Yang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "Most RLHF workflows largely follow this 4-stage workflow\\cite{ppo, grpo, dapo, dseekr1, qwen25math, qwen3, remax}", "next_context": "(Sometimes, the first three stages are referred to as \"rollout\":\\itemStage 1(\\textbf{Generation}): The actor produces responses from a batch of prompts using auto-regressive generation."}], "importance_score": 0.14285714285714285}, "vllm": {"bib_key": "vllm", "bib_title": "Efficient Memory Management for Large Language Model Serving with PagedAttention", "bib_author ": "Woosuk Kwon", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "In engineering practice, the generation step is typically implemented using a specialized language model inference engine (e.g., vLLM\\cite{vllm}", "next_context": ", sglang\\cite{sglang})\\cite{openrlhf, nemoaligner, verl}."}, {"section": "Methods", "subsection": "Parallel Controllers", "subsubsection": null, "prev_context": "Similar to the hybrid controller\\cite{verl}, each actor internally adopts a multi-controller structure for higher efficiency\\cite{evalgpuinter, vllm}", "next_context": "."}, {"section": "Implementation", "subsection": null, "subsubsection": null, "prev_context": "Our system combines vLLM\\cite{vllm}", "next_context": "and SGlang\\cite{sglang}for generation serving with Megatron-Core\\cite{mlm1, mlm2, mlm3, mlm4}as the training backend."}], "importance_score": 2.5}, "sglang": {"bib_key": "sglang", "bib_title": "SGLang: Efficient Execution of Structured Language Model Programs", "bib_author ": "Lianmin Zheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "RLHF Workflow", "subsubsection": null, "prev_context": "In engineering practice, the generation step is typically implemented using a specialized language model inference engine (e.g., vLLM\\cite{vllm}, sglang\\cite{sglang}", "next_context": ")\\cite{openrlhf, nemoaligner, verl}."}, {"section": "Implementation", "subsection": null, "subsubsection": null, "prev_context": "Our system combines vLLM\\cite{vllm}and SGlang\\cite{sglang}", "next_context": "for generation serving with Megatron-Core\\cite{mlm1, mlm2, mlm3, mlm4}as the training backend."}], "importance_score": 2.0}, "areal": {"bib_key": "areal", "bib_title": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning", "bib_author ": "Wei Fu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "RLHF Placement", "subsubsection": null, "prev_context": "Some works\\cite{areal, flexrlhf}", "next_context": "have adopted a co-existing approach, which effectively avoids the overhead of model swapping."}, {"section": "Background", "subsection": "RLHF Placement", "subsubsection": null, "prev_context": "This can lead to suboptimal model convergence; for example,\\cite{areal}", "next_context": "introduces staleness-aware training to address this issue."}], "importance_score": 1.5}, "flexrlhf": {"bib_key": "flexrlhf", "bib_title": "An adaptive placement and parallelism framework for accelerating rlhf training", "bib_author ": "Xiao, Youshao", "arxiv_id": null, "short_id": "2312.11819", "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Background", "subsection": "RLHF Placement", "subsubsection": null, "prev_context": "Some works\\cite{areal, flexrlhf}", "next_context": "have adopted a co-existing approach, which effectively avoids the overhead of model swapping."}], "importance_score": 0.5}, "dancegrpo": {"bib_key": "dancegrpo", "bib_title": "DanceGRPO: Unleashing GRPO on Visual Generation", "bib_author ": "Zeyue Xue", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methods", "subsection": "Parallel Controllers", "subsubsection": null, "prev_context": "Firstly, if we need to transfer large features (such as a large number of images or videos, especially in image generation tasks\\cite{flux, sd, dancegrpo}", "next_context": ") within the control flow, the memory and RPC network bandwidth of a single controller can become bottlenecks."}], "importance_score": 0.3333333333333333}, "rad": {"bib_key": "rad", "bib_title": "Reward-Augmented Decoding: Efficient Controlled Text Generation With a Unidirectional Reward Model", "bib_author ": "Deng, Haikang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methods", "subsection": "Parallel Controllers", "subsubsection": null, "prev_context": "If we could implement local state transitions, we would be able to support certain special sampling processes, such as dynamic sampling or reward-augmented generation\\cite{dapo, rad}", "next_context": "."}], "importance_score": 0.5}, "evalgpuinter": {"bib_key": "evalgpuinter", "bib_title": "Evaluating Modern GPU Interconnect: PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect", "bib_author ": "Li, Ang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methods", "subsection": "Parallel Controllers", "subsubsection": null, "prev_context": "Similar to the hybrid controller\\cite{verl}, each actor internally adopts a multi-controller structure for higher efficiency\\cite{evalgpuinter, vllm}", "next_context": "."}], "importance_score": 0.5}, "genverifier": {"bib_key": "genverifier", "bib_title": "Generative Verifiers: Reward Modeling as Next-Token Prediction", "bib_author ": "Lunjun Zhang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Methods", "subsection": "Dynamic Placement", "subsubsection": null, "prev_context": "The generative rewarding method has been proven effective\\cite{genrm, genverifier}", "next_context": "."}, {"section": "Methods", "subsection": "Dynamic Placement", "subsubsection": null, "prev_context": "This keeps the generation abilities of language model intact as the verification decision is just another token, while also enabling several advantages that come for \u201cfree\u201d with LLMs, such as unified training for solution generation and verification, chain-of-thought reasoning, and inference-time computation\\cite{genverifier}", "next_context": "."}, {"section": "Methods", "subsection": "Dynamic Placement", "subsubsection": null, "prev_context": "After generation is completed, we offload the policy model to cpu memory and load the generative reward model onto gpu memory, and then use this model to generate reward scores through generation and regex matching\\cite{genverifier}", "next_context": "."}], "importance_score": 2.5}, "torch": {"bib_key": "torch", "bib_title": "PyTorch: An Imperative Style, High-Performance Deep Learning Library", "bib_author ": "Adam Paszke", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Implementation", "subsection": null, "subsubsection": null, "prev_context": "We implement G-Core using Python and PyTorch\\cite{torch}", "next_context": "."}], "importance_score": 1.0}, "ray": {"bib_key": "ray", "bib_title": "Ray: A Distributed Framework for Emerging AI Applications", "bib_author ": "Philipp Moritz", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Implementation", "subsection": "Placement and RPC", "subsubsection": null, "prev_context": "To keep the placement scheme as simple and predictable as possible (e.g., forming communication groups according to the GPU switch topology) and to minimize interference with component orchestration, we did not use Ray\\cite{ray}", "next_context": "for placement and RPC."}], "importance_score": 1.0}, "openmpi": {"bib_key": "openmpi", "bib_title": "Open MPI: A flexible high performance MPI", "bib_author ": "Graham, Richard L", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Implementation", "subsection": "Placement and RPC", "subsubsection": null, "prev_context": "Nevertheless, our code is designed to be loosely coupled, and we believe it can interoperate well with systems such as MPI\\cite{openmpi}", "next_context": "and SLURM\\cite{slurm}."}], "importance_score": 1.0}, "slurm": {"bib_key": "slurm", "bib_title": "SLURM Heterogeneous Jobs for Hybrid Classical-Quantum Workflows", "bib_author ": "Aniello Esposito", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Implementation", "subsection": "Placement and RPC", "subsubsection": null, "prev_context": "Nevertheless, our code is designed to be loosely coupled, and we believe it can interoperate well with systems such as MPI\\cite{openmpi}and SLURM\\cite{slurm}", "next_context": "."}], "importance_score": 1.0}, "megascale": {"bib_key": "megascale", "bib_title": "MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs", "bib_author ": "Ziheng Jiang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Implementation", "subsection": "Workload Balancing", "subsubsection": null, "prev_context": "As noted in\\cite{megascale,wlbllm}", "next_context": ", workload imbalance can create significant bubbles during training, becoming a major bottleneck."}], "importance_score": 0.5}, "wlbllm": {"bib_key": "wlbllm", "bib_title": "WLB-LLM: Workload-Balanced 4D Parallelism for Large Language Model Training", "bib_author ": "Zheng Wang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Implementation", "subsection": "Workload Balancing", "subsubsection": null, "prev_context": "As noted in\\cite{megascale,wlbllm}", "next_context": ", workload imbalance can create significant bubbles during training, becoming a major bottleneck."}], "importance_score": 0.5}, "gemma3": {"bib_key": "gemma3", "bib_title": "Gemma 3 Technical Report", "bib_author ": "Gemma Team", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Implementation", "subsection": "Distributed Attention", "subsubsection": null, "prev_context": "To support more complex attention masks (e.g., Gemma-3\\cite{gemma3}", "next_context": "), inspired by Llama-3\\cite{llama3}and open source community contributors, we implemented distributed attention based on CCL all-gather, where we first all-gather the key (K) and value (V) tensors, and then compute attention output for the local query (Q) tensor chunk."}, {"section": "Implementation", "subsection": "Train Data Storage", "subsubsection": null, "prev_context": "Multimodal data is often extremely large, both in quantity and size\\cite{qwen25vl, gemma3}", "next_context": "."}], "importance_score": 1.5}, "qwen25vl": {"bib_key": "qwen25vl", "bib_title": "Qwen2.5-VL Technical Report", "bib_author ": "Shuai Bai", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Implementation", "subsection": "Train Data Storage", "subsubsection": null, "prev_context": "Multimodal data is often extremely large, both in quantity and size\\cite{qwen25vl, gemma3}", "next_context": "."}], "importance_score": 0.5}, "wxpaxos": {"bib_key": "wxpaxos", "bib_title": "PaxosStore: high-availability storage made practical in WeChat", "bib_author ": "Zheng, Jianjun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Implementation", "subsection": "Train Data Storage", "subsubsection": null, "prev_context": "FeatureKV and UnionDB), on top of our private service discovery and distributed file system (Wechat File System)\\cite{wxpaxos}", "next_context": ", to serve training data."}], "importance_score": 1.0}, "li2024numinamath": {"bib_key": "li2024numinamath", "bib_title": "Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions", "bib_author ": "Li, Jia", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Evaluation", "subsection": "Bradley-Terry Reward Model", "subsubsection": null, "prev_context": "First, we perform SFT (supervised fine-tuning) using the open-source English dataset NuminaMath-CoT\\cite{li2024numinamath}", "next_context": "."}], "importance_score": 1.0}, "gsm8k": {"bib_key": "gsm8k", "bib_title": "Training Verifiers to Solve Math Word Problems", "bib_author ": "Karl Cobbe", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Evaluation", "subsection": "Bradley-Terry Reward Model", "subsubsection": null, "prev_context": "We use Qwen2.5-Math-RM-72B as the reward model and the deduplicated GSM-8k\\cite{gsm8k}", "next_context": "dataset as the set of questions."}], "importance_score": 1.0}}, "refs": [], "table": [{"original": "\\begin{table}[]\n\\centering\n\\begin{tabular}{|c|c|c|c|c|c|c|}\n\\hline\nTrainer & Policy Model & Reward Model & Format & Accuracy & Training Hours \\\\\n\\hline\nVeRL   & Qwen 2.5 1.5B & Qwen 2.5 72B Math RM & 0.990 & 0.845 & xx \\\\\n\\hline\nG-Core & Qwen 2.5 1.5B & Qwen 2.5 72B Math RM & 0.983 & 0.849 & xx \\\\\n\\hline\nVeRL   & Qwen 2.5 7B & Qwen 2.5 72B Math RM & - & - & - \\\\\n\\hline\nG-Core & Qwen 2.5 7B & Qwen 2.5 72B Math RM & - & - & - \\\\\n\\hline\n\\end{tabular}\n\\caption{Comparison with Bradley-Terry reward model}\n\\label{table:cmp-math}\n\\end{table}", "caption": "\\caption{Comparison with Bradley-Terry reward model}", "label": "\\label{table:cmp-math}", "tabular": "\\begin{tabular}{|c|c|c|c|c|c|c|}\n\\hline\nTrainer & Policy Model & Reward Model & Format & Accuracy & Training Hours \\\\\n\\hline\nVeRL   & Qwen 2.5 1.5B & Qwen 2.5 72B Math RM & 0.990 & 0.845 & xx \\\\\n\\hline\nG-Core & Qwen 2.5 1.5B & Qwen 2.5 72B Math RM & 0.983 & 0.849 & xx \\\\\n\\hline\nVeRL   & Qwen 2.5 7B & Qwen 2.5 72B Math RM & - & - & - \\\\\n\\hline\nG-Core & Qwen 2.5 7B & Qwen 2.5 72B Math RM & - & - & - \\\\\n\\hline\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[]\n\\centering\n\\begin{tabular}{|c|c|c|c|c|c|}\n\\hline\nTrainer & Policy Model & Reward Model & Num GPUs of Policy & Num GPUs of Reward & Training Hours \\\\\n\\hline\nVeRL   & Qwen 2.5 1.5B & Qwen 2.5 72B Math & first 32 & last 32 & xx \\\\\n\\hline\nG-Core & Qwen 2.5 1.5B & Qwen 2.5 72B Math & 64 & 64 & xx \\\\\n\\hline\n\\end{tabular}\n\\caption{Comparison with generative reward model}\n\\label{table:cmp-math-genrm}\n\\end{table}", "caption": "\\caption{Comparison with generative reward model}", "label": "\\label{table:cmp-math-genrm}", "tabular": "\\begin{tabular}{|c|c|c|c|c|c|}\n\\hline\nTrainer & Policy Model & Reward Model & Num GPUs of Policy & Num GPUs of Reward & Training Hours \\\\\n\\hline\nVeRL   & Qwen 2.5 1.5B & Qwen 2.5 72B Math & first 32 & last 32 & xx \\\\\n\\hline\nG-Core & Qwen 2.5 1.5B & Qwen 2.5 72B Math & 64 & 64 & xx \\\\\n\\hline\n\\end{tabular}", "subtables": []}], "figure": [{"original": "\\begin{figure}\n    \\begin{subfigure}{.5\\textwidth}\n      \\centering\n      \\includegraphics[width=.8\\linewidth]{pic/hybrid-controller.png}\n      \\caption{Single Controller}\n      \\label{fig:sfig1}\n    \\end{subfigure}%\n    \\begin{subfigure}{.5\\textwidth}\n      \\centering\n      \\includegraphics[width=.8\\linewidth]{pic/federal-controller.png}\n      \\caption{Parallel Controllers}\n      \\label{fig:sfig2}\n    \\end{subfigure}\n\\caption{Parallel controllers avoids overwhelming of controller bottleneck resources}\n\\label{fig:3l-controller}\n\\end{figure}", "caption": "\\caption{Parallel controllers avoids overwhelming of controller bottleneck resources}", "label": "\\label{fig:3l-controller}", "subfigures": [{"caption": "\\caption{Single Controller}", "label": "\\label{fig:sfig1}", "figure_paths": ["pic/hybrid-controller.png"], "subfigures": []}, {"caption": "\\caption{Parallel Controllers}", "label": "\\label{fig:sfig2}", "figure_paths": ["pic/federal-controller.png"], "subfigures": []}], "figure_paths": ["pic/hybrid-controller.png", "pic/federal-controller.png"]}, {"original": "\\begin{subfigure}{.5\\textwidth}\n      \\centering\n      \\includegraphics[width=.8\\linewidth]{pic/hybrid-controller.png}\n      \\caption{Single Controller}\n      \\label{fig:sfig1}\n    \\end{subfigure}", "caption": "\\caption{Single Controller}", "label": "\\label{fig:sfig1}", "subfigures": [], "figure_paths": ["pic/hybrid-controller.png"]}, {"original": "\\begin{subfigure}{.5\\textwidth}\n      \\centering\n      \\includegraphics[width=.8\\linewidth]{pic/federal-controller.png}\n      \\caption{Parallel Controllers}\n      \\label{fig:sfig2}\n    \\end{subfigure}", "caption": "\\caption{Parallel Controllers}", "label": "\\label{fig:sfig2}", "subfigures": [], "figure_paths": ["pic/federal-controller.png"]}], "equations": [], "algorithm": [], "sections": {"Introduction": {"content": "\n\nTransformer-based large models and their multimodal extensions have achieved remarkable progress, demonstrating state-of-the-art performance across a wide range of natural language processing, understanding, and generation tasks \\cite{gpt4, llama3, dseekv3, googlegemini, flux, gpt3survey, sd, kimi, attn}. These models, characterized by their massive parameter counts and ability to process extremely long context sequences, have become the foundation for many advanced AI applications. However, the unprecedented scale of LLMs has introduced significant challenges for efficient training, traditional data-parallel training strategies are often insufficient to handle the memory and computational demands of such large models. To address these challenges, the research community has developed a variety of hybrid parallelism techniques \\cite{mlm1, mlm2, mlm3, mlm4, ringattn}. These approaches enable the training of models that would otherwise exceed the memory and compute capabilities of individual devices, by partitioning both the model and the data across multiple GPUs or nodes.\n\nWith the advent of Reinforcement Learning from Human Feedback (RLHF), the training paradigm for LLMs has evolved further \\cite{qwen25math, dseekr1, dspo}. RLHF introduces additional complexity by incorporating human preferences into the training loop, typically requiring the orchestration of multiple models within a single training workflow \\cite{ppo, grpo, remax}. This multi-model setup dramatically increases the total number of parameters and intermediate activations that must be managed during training, pushing the limits of existing distributed training frameworks.\n\nAlthough hybrid parallelism theoretically offers near-infinite scalability, in practice, increasing the degree of parallelism leads to substantial communication overhead. As the number of models and the length of context sequences grow, the cost of synchronizing parameters and activations across devices can become prohibitive, undermining the benefits of parallelization. To mitigate these issues, \\cite{nemoaligner} models the RLHF training process as a control flow, leveraging multi-cluster, multi-model co-location, and swapping strategies to reduce peak GPU memory usage. Other systems \\cite{openrlhf, verl} have adopted similar approaches, with \\cite{verl} further improving throughput by combining single and multi-controller and statically planned placement schemes.\n\nDespite these advances, several limitations remain in current RLHF training systems:\n\n\\begin{enumerate}\n    \\item \\textbf{Scalability bottlenecks in large-scale multimodal and diffusion scenarios:} When handling large-scale multimodal data (e.g., images, videos), a single controller could become a communication or memory bottleneck, limiting overall system throughput, or cause the system to fail to operate.\n    \\item \\textbf{Inefficiency under dynamic sampling and generative reward modeling:} In workflows that require frequent dynamic sampling or generative reward computation, the overhead of model swapping and long-tail effects (i.e., straggler tasks) can significantly reduce hardware utilization and training efficiency.    \n\\end{enumerate}\\begin{enumerate}\n    \\item \\textbf{Scalability bottlenecks in large-scale multimodal and diffusion scenarios:} When handling large-scale multimodal data (e.g., images, videos), a single controller could become a communication or memory bottleneck, limiting overall system throughput, or cause the system to fail to operate.\n    \\item \\textbf{Inefficiency under dynamic sampling and generative reward modeling:} In workflows that require frequent dynamic sampling or generative reward computation, the overhead of model swapping and long-tail effects (i.e., straggler tasks) can significantly reduce hardware utilization and training efficiency.    \n\\end{enumerate}\n    \\item \\textbf{Scalability bottlenecks in large-scale multimodal and diffusion scenarios:} When handling large-scale multimodal data (e.g., images, videos), a single controller could become a communication or memory bottleneck, limiting overall system throughput, or cause the system to fail to operate.\n    \\item \\textbf{Inefficiency under dynamic sampling and generative reward modeling:} In workflows that require frequent dynamic sampling or generative reward computation, the overhead of model swapping and long-tail effects (i.e., straggler tasks) can significantly reduce hardware utilization and training efficiency.    \n\n\nTo address these challenges, we present \\textbf{G-Core}, a novel RLHF training framework designed to maximize scalability, flexibility, and efficiency in large-scale, multi-model training environments. Our main contributions are as follows:\n\n\\begin{enumerate}\n    \\item \\textbf{Parallel Controller Programming Model:} We introduce a parallel controller programming model that enables users to conveniently construct complex RLHF workflows. This model supports efficient distributed execution of intra-node computation and flexible control flow, while avoiding the bottlenecks associated with a single centralized controller.\n    \\item \\textbf{Dynamic Scaling Placement Schema:} We propose a dynamic scaling placement schema that supports efficient generative rewarding and dynamic sampling. By adaptively partitioning resources and enabling fine-grained scheduling, our approach minimizes device idle time (\u201cbubbles\u201d) and improves hardware utilization, even in the presence of highly variable workloads.\n\\end{enumerate}\\begin{enumerate}\n    \\item \\textbf{Parallel Controller Programming Model:} We introduce a parallel controller programming model that enables users to conveniently construct complex RLHF workflows. This model supports efficient distributed execution of intra-node computation and flexible control flow, while avoiding the bottlenecks associated with a single centralized controller.\n    \\item \\textbf{Dynamic Scaling Placement Schema:} We propose a dynamic scaling placement schema that supports efficient generative rewarding and dynamic sampling. By adaptively partitioning resources and enabling fine-grained scheduling, our approach minimizes device idle time (\u201cbubbles\u201d) and improves hardware utilization, even in the presence of highly variable workloads.\n\\end{enumerate}\n    \\item \\textbf{Parallel Controller Programming Model:} We introduce a parallel controller programming model that enables users to conveniently construct complex RLHF workflows. This model supports efficient distributed execution of intra-node computation and flexible control flow, while avoiding the bottlenecks associated with a single centralized controller.\n    \\item \\textbf{Dynamic Scaling Placement Schema:} We propose a dynamic scaling placement schema that supports efficient generative rewarding and dynamic sampling. By adaptively partitioning resources and enabling fine-grained scheduling, our approach minimizes device idle time (\u201cbubbles\u201d) and improves hardware utilization, even in the presence of highly variable workloads.\n\n\nIn summary, G-Core advances the state of the art in RLHF training by addressing the critical bottlenecks of controller scalability and resource placement. Our design provides a practical and flexible approach for orchestrating complex, multi-model workflows, which can facilitate the development of large-scale, human-aligned models.\n\n\n", "appendix": false}, "Background": {"content": "\n\n\n\\subsection{Distributed Training of Deep Models}\n\nData Parallelism involves splitting the training data across multiple devices (such as GPUs), where each device holds a copy of the model and processes a different mini-batch of data. Gradients are averaged (or summed) across devices to update the model parameters synchronously \\cite{torchddp, horovod}. Tensor Parallelism splits individual layers or tensors of the model across multiple devices. Each device is responsible for computing a portion of the operations within a layer, enabling the training of very large models that cannot fit into a single device\u2019s memory \\cite{mlm1}. Pipeline Parallelism divides the model into sequential stages, with each stage placed on a different device. Mini-batches are split into micro-batches and passed through the pipeline, allowing different devices to work on different parts of the input simultaneously, improving hardware utilization \\cite{mlm2, gpipe, pipedream}.\n\nZeRO is an optimization technique that partitions model states (such as optimizer states, gradients, and parameters) across devices to minimize memory redundancy. This enables the training of much larger models by reducing memory consumption per device \\cite{zero3, zero1}.\n\nContext Parallelism is a parallelization strategy where input context is distributed across multiple devices. This allows for efficient processing of long sequences or large batches by splitting the workload. Techniques like ring attention are examples of context parallelism, enabling scalable and memory-efficient attention computation across devices \\cite{llama3, ringattn, ulysess, fang2024uspunifiedsequenceparallelism}.\n\nExpert Parallelism is used in Mixture-of-Experts (MoE) models, where different subsets of the model (experts) are distributed across devices. During training and inference, only a subset of experts is activated for each input, enabling efficient scaling and utilization of resources \\cite{mlm4, tutel}.\n\n\n\\subsection{RLHF Workflow}\n\nRLHF aligns the linguistic space of LLMs with human preferences, using a set of human-ranked candidates of given prompts \\cite{ppo, bai2022traininghelpfulharmlessassistant}. An typical reinforcement learning trainer consists of multiple models, e.g., an actor, a critic, a reference policy, and one or multiple reward models. The critic and reward models are usually models tuned in the human preference dataset, with the language modeling head replaced by a numerical output head (Bradley-Terry reward model) \\cite{bai2022traininghelpfulharmlessassistant, ouyang2022traininglanguagemodelsfollow, btrm, rethinkbt}. Bradley-Terry reward models can be replaced with generative models, which obtain reward scores through token prediction; furthermore, leveraging chain-of-thought reasoning, the accuracy and final performance of the actor model can be further improved \\cite{genrm, cot}.\n\nMost RLHF workflows largely follow this 4-stage workflow \\cite{ppo, grpo, dapo, dseekr1, qwen25math, qwen3, remax}(Sometimes, the first three stages are referred to as \"rollout\":\n\n\\begin{enumerate}\n  \\item Stage 1(\\textbf{Generation}): The actor produces responses from a batch of prompts using auto-regressive generation.\n  \\item Stage 2(\\textbf{Rewarding}): The reward model computes rewards for generated responses.\n  \\item Stage 3(\\textbf{Preparation}): Using generated responses and rewards, the critic computes their values, the policy and reference policy computes their reference log probabilities.\n  \\item Stage 4(\\textbf{Training}): The actor and the critic are trained on the batch of data produced by previous stages and the loss function.\n\\end{enumerate}\\begin{enumerate}\n  \\item Stage 1(\\textbf{Generation}): The actor produces responses from a batch of prompts using auto-regressive generation.\n  \\item Stage 2(\\textbf{Rewarding}): The reward model computes rewards for generated responses.\n  \\item Stage 3(\\textbf{Preparation}): Using generated responses and rewards, the critic computes their values, the policy and reference policy computes their reference log probabilities.\n  \\item Stage 4(\\textbf{Training}): The actor and the critic are trained on the batch of data produced by previous stages and the loss function.\n\\end{enumerate}\n  \\item Stage 1(\\textbf{Generation}): The actor produces responses from a batch of prompts using auto-regressive generation.\n  \\item Stage 2(\\textbf{Rewarding}): The reward model computes rewards for generated responses.\n  \\item Stage 3(\\textbf{Preparation}): Using generated responses and rewards, the critic computes their values, the policy and reference policy computes their reference log probabilities.\n  \\item Stage 4(\\textbf{Training}): The actor and the critic are trained on the batch of data produced by previous stages and the loss function.\n\n\n\n\nIn engineering practice, the generation step is typically implemented using a specialized language model inference engine (e.g., vLLM \\cite{vllm}, sglang \\cite{sglang}) \\cite{openrlhf, nemoaligner, verl}.\n\nTo simplify the implementation, most systems use a single controller to manage data transfer, intermediate results, and workflow orchestration through a central point \\cite{openrlhf, verl}. However, this centralized approach can sometimes become a bottleneck\u2014especially when the trainer itself has high memory requirements.\n\n\n\\subsection{RLHF Placement}\n\nWorkflow is at the core of RLHF training, and model placement is a crucial component of the RLHF workflow. Currently, there are at least two main approaches:\n\\begin{itemize}\n    \\item \\textbf{Co-exist:} Different models are assigned to separate groups of devices, allowing them to run in parallel as long as there are no data dependencies between them.\n    \\item \\textbf{Co-locate:} Multiple models are placed on the same set of GPUs, sharing GPU memory. To avoid out-of-memory (OOM) errors, which can easily occur if large models run simultaneously, these colocated models are executed one after another in a time-sharing fashion.\n\\end{itemize}\\begin{itemize}\n    \\item \\textbf{Co-exist:} Different models are assigned to separate groups of devices, allowing them to run in parallel as long as there are no data dependencies between them.\n    \\item \\textbf{Co-locate:} Multiple models are placed on the same set of GPUs, sharing GPU memory. To avoid out-of-memory (OOM) errors, which can easily occur if large models run simultaneously, these colocated models are executed one after another in a time-sharing fashion.\n\\end{itemize}\n    \\item \\textbf{Co-exist:} Different models are assigned to separate groups of devices, allowing them to run in parallel as long as there are no data dependencies between them.\n    \\item \\textbf{Co-locate:} Multiple models are placed on the same set of GPUs, sharing GPU memory. To avoid out-of-memory (OOM) errors, which can easily occur if large models run simultaneously, these colocated models are executed one after another in a time-sharing fashion.\n\n\nAlthough the \\cite{verl} proposed methods for calculating placement based on workload, both the open-source implementations of \\cite{openrlhf} and \\cite{verl} adopt a co-locate all models strategy. The co-location approach introduces overhead from swapping among multiple models. For example, swapping a 32B model could take nearly a minute, and updating weights may take tens of seconds. In typical GRPO training, this overhead is not a major issue, since the primary bottlenecks in RLHF training are the generation and the forward/backward passes of modeling. Compared to the tens of minutes spent on rollout and training, model swapping is not the system bottleneck.\n\nSome works \\cite{areal, flexrlhf} have adopted a co-existing approach, which effectively avoids the overhead of model swapping. However, to mask device idle time during workflow state transitions, these methods introduce varying degrees of asynchronous overlap between the rollout and training stages. This can lead to suboptimal model convergence; for example, \\cite{areal} introduces staleness-aware training to address this issue. In our experience, many algorithm engineers find that this tradeoff adds extra cognitive burden.\n\n", "appendix": false}, "Methods": {"content": "\n\n\\subsection{Parallel Controllers}\n\nAlthough a hybrid controller offers significant convenience in programming, it may not provide sufficient flexibility or could become a bottleneck for the system in certain scenarios.\n\n\n\nFirstly, if we need to transfer large features (such as a large number of images or videos, especially in image generation tasks \\cite{flux, sd, dancegrpo}) within the control flow, the memory and RPC network bandwidth of a single controller can become bottlenecks. Similarly, if there are complex procedures that needs to be performed in the controller, it may also lead to a CPU bottleneck. In certain specialized algorithmic scenarios, even if the controller is equipped with 2048 GB of memory, and setting aside the fact that, in practice, model and optimizer swap buffers typically consume half of that, performing a rollout of 1024 samples, each containing 32 2k resolution images, would already at least occupy 768 GB of memory. If there are additional operations such as copying, more video frames, or communication buffer overhead, it is most likely to encounter out-of-memory (OOM) issues. Utilizing shallow copies can help save a significant amount of memory by avoiding the recreation of image tensors during repeated sampling. However, if the sampling algorithm incorporates special data augmentation strategies, the memory footprint will spike again.\n\n\n\nSecondly, from a top-level perspective, the hybrid controller is essentially a single controller that manages proxy objects for the GPU resource pools of multiple roles (e.g., policy, reward, ref, critic). The hybrid controller can only switch from stage to stage as a whole, making it difficult to achieve local state transitions. If we could implement local state transitions, we would be able to support certain special sampling processes, such as dynamic sampling or reward-augmented generation \\cite{dapo, rad}.\n\nTo address these issues, we propose the parallel controller architecture. As shown in Figure \\ref{fig:3l-controller}, we partition RL tasks using the traditional SPMD (Single Program Multiple Data) approach, where multiple controllers manage different subsets of data. From a statistical perspective, as the batch size increases, the workload distribution among multiple controllers tends to become more balanced. This is because the law of large numbers ensures that, with a large enough sample size, the amount of data assigned to each controller approaches its expected value, resulting in balanced load distribution.\n\n\n\nEach controller in the parallel controllers architecture is only responsible for managing a portion of the resources, but resources may be controlled by a single controller or by multiple controllers. This allows multiple stages to co-exist in the system at the same time, enabling specific state transitions tailored to particular reinforcement learning algorithms.\n\n\\begin{figure}\n    \\begin{subfigure}{.5\\textwidth}\n      \\centering\n      \\includegraphics[width=.8\\linewidth]{pic/hybrid-controller.png}\n      \\caption{Single Controller}\n      \\label{fig:sfig1}\n    \\end{subfigure}%\n    \\begin{subfigure}{.5\\textwidth}\n      \\centering\n      \\includegraphics[width=.8\\linewidth]{pic/federal-controller.png}\n      \\caption{Parallel Controllers}\n      \\label{fig:sfig2}\n    \\end{subfigure}\n\\caption{Parallel controllers avoids overwhelming of controller bottleneck resources}\n\\label{fig:3l-controller}\n\\end{figure}\n    {.5\\textwidth}.5\\textwidth\n      \\centering\n      \\includegraphics[width=.8\\linewidth]{pic/hybrid-controller.png}\n      \\caption{Single Controller}\n      \\label{fig:sfig1}\n    {.5\\textwidth}.5\\textwidth\n      \\centering\n      \\includegraphics[width=.8\\linewidth]{pic/federal-controller.png}\n      \\caption{Parallel Controllers}\n      \\label{fig:sfig2}\n    \n\\caption{Parallel controllers avoids overwhelming of controller bottleneck resources}\n\\label{fig:3l-controller}\n\n\nWhile it is generally challenging to use multiple controllers to coordinate multiple RL roles, our approach assigns multiple controllers to manage different roles. Similar to the hybrid controller \\cite{verl}, each actor internally adopts a multi-controller structure for higher efficiency \\cite{evalgpuinter, vllm}.\n\nYou can view the parallel controllers as a compromise towards a multi-controller design of hybrid controller for more controller resources and flexible resource scheduling. Contrary to the claims in \\cite{verl}, we believe that for experienced engineers, adopting a multi-controller architecture does not introduce significant programming challenges. In our design, we further decompose the top-level controller and use collective communication to coordinate among controllers. Within each worker cluster managed by a controller, we retain a control pattern similar to the hybrid controller.\n\n\n\\subsection{Dynamic Placement}\n\n\n\nThe generative rewarding method has been proven effective \\cite{genrm, genverifier}. Discriminative LLM-based verifiers do not utilize the text generation capabilities of pretrained LLMs. Generative rewarding using standard next-token prediction, represents solution correctness using the LLM\u2019s probability distribution over tokens, instead of predicting a separate numerical score. This keeps the generation abilities of language model intact as the verification decision is just another token, while also enabling several advantages that come for \u201cfree\u201d with LLMs, such as unified training for solution generation and verification, chain-of-thought reasoning, and inference-time computation \\cite{genverifier}.\n\nWe adopt a co-location strategy: As we mentioned earlier, co-location is a relatively efficient implementation without introducing additional algorithmic trade-offs. We use a causal text generation inference engine to replace the traditional regression-based rewarding model. After generation is completed, we offload the policy model to cpu memory and load the generative reward model onto gpu memory, and then use this model to generate reward scores through generation and regex matching \\cite{genverifier}.\n\nThe co-location approach has proven to be highly effective in practice, resulting in minimal hardware idle time. Compared to the relatively time-consuming processes of text generation, reward generation, and training, the overhead of swapping models in and out of GPU memory is negligible. For instance, swapping a 32B model typically takes only 30\u201360 seconds, which is minor relative to the total computation time.\n\nHowever, in dynamic sampling scenarios, the situation becomes more complex. \\cite{dapo} proposes to filter out prompts with the accuracy equal to 1 and 0 (the overall reward for a batch of samples is too high or too low), and trigger re-sampling. Re-sampling requiring additional rounds of generation and reward computation.\n\n\\begin{enumerate}\n    \\item As training progresses and the model\u2019s capabilities improve, the acceptance rate of generated samples gradually decreases. This leads to more frequent resampling and, consequently, more frequent model swaps between the policy and reward models. In such cases, the previously negligible model swapping overhead can accumulate and become a bottleneck in the training pipeline, potentially limiting overall system efficiency.\n    \\item In the generation stage, it is inevitable that there will be some degree of long-tail outputs. These long-tail outputs can reduce the overall utilization of the GPU cluster. Moreover, in co-location solutions, where frequent swapping between multiple stages is required, the long-tail effect can be further amplified. This is because the uneven completion times of different tasks can lead to increased resource fragmentation and idle GPU time, making it even more difficult to maintain high cluster efficiency.\n\\end{enumerate}\\begin{enumerate}\n    \\item As training progresses and the model\u2019s capabilities improve, the acceptance rate of generated samples gradually decreases. This leads to more frequent resampling and, consequently, more frequent model swaps between the policy and reward models. In such cases, the previously negligible model swapping overhead can accumulate and become a bottleneck in the training pipeline, potentially limiting overall system efficiency.\n    \\item In the generation stage, it is inevitable that there will be some degree of long-tail outputs. These long-tail outputs can reduce the overall utilization of the GPU cluster. Moreover, in co-location solutions, where frequent swapping between multiple stages is required, the long-tail effect can be further amplified. This is because the uneven completion times of different tasks can lead to increased resource fragmentation and idle GPU time, making it even more difficult to maintain high cluster efficiency.\n\\end{enumerate}\n    \\item As training progresses and the model\u2019s capabilities improve, the acceptance rate of generated samples gradually decreases. This leads to more frequent resampling and, consequently, more frequent model swaps between the policy and reward models. In such cases, the previously negligible model swapping overhead can accumulate and become a bottleneck in the training pipeline, potentially limiting overall system efficiency.\n    \\item In the generation stage, it is inevitable that there will be some degree of long-tail outputs. These long-tail outputs can reduce the overall utilization of the GPU cluster. Moreover, in co-location solutions, where frequent swapping between multiple stages is required, the long-tail effect can be further amplified. This is because the uneven completion times of different tasks can lead to increased resource fragmentation and idle GPU time, making it even more difficult to maintain high cluster efficiency.\n\n\n\n\n\n\nTo address the swapping overhead and the amplified long-tail effect mentioned above, we have integrated both Co-existing (asynchronous workflow) and Co-location (synchronous workflow) placement strategies into what we call dynamic placement. Specifically:\n\\begin{enumerate}\n    \\item we partition the GPU cluster according to workload, allocating separate portions for actor generation and reward model generation., allowing the generation (stage 1) and rewarding (stage 2) models to co-exist on the same devices.\n    \\item For the preparation (stage 3) and training (stage 4) stages, we retain the co-location approach, use all GPUs for training and rollout log probability calculations, allowing the devices to operate with minimal idle time.\n\\end{enumerate}\\begin{enumerate}\n    \\item we partition the GPU cluster according to workload, allocating separate portions for actor generation and reward model generation., allowing the generation (stage 1) and rewarding (stage 2) models to co-exist on the same devices.\n    \\item For the preparation (stage 3) and training (stage 4) stages, we retain the co-location approach, use all GPUs for training and rollout log probability calculations, allowing the devices to operate with minimal idle time.\n\\end{enumerate}\n    \\item we partition the GPU cluster according to workload, allocating separate portions for actor generation and reward model generation., allowing the generation (stage 1) and rewarding (stage 2) models to co-exist on the same devices.\n    \\item For the preparation (stage 3) and training (stage 4) stages, we retain the co-location approach, use all GPUs for training and rollout log probability calculations, allowing the devices to operate with minimal idle time.\n\n\n\n\nFor stage 1 and 2, since both the policy generation model and the rewarding model reside in GPU memory simultaneously, there is no need to swap models in and out, which avoids the memory copy and graph capturing overhead caused by frequent model swapping during dynamic sampling. Additionally, because the policy model remains in memory after the rewarding computation, we can immediately start the next round of generation, this finer-grained control allows for minimizing idle periods (\"bubbles\") in the long-tail phase of the system.\n\n\n\nFrom an engineering perspective, it is technically feasible to adjust the GPU cluster partitioning based on the workload. However, this approach overlooks the fact that the system load can change from an algorithmic perspective \\cite{dseekr1}. The average response length of models on the training set during the RL process, naturally learns to solve reasoning tasks with more thinking time. The thinking time of RL shows consistent improvement throughout the training process. Models naturally acquires the ability to solve increasingly complex reasoning tasks by leveraging extended test-time computation. This computation ranges from generating hundreds to thousands of reasoning tokens,\nallowing the model to explore and refine its thought processes in greater depth. In many of our simulation experiments, we observed phenomena similar to those reports.\n\n\n\nIf the workload is dynamic, it is not possible to use the static estimation method from \\cite{verl} to determine placement. It is difficult to predict how the output length of the model will change after a period of training. Even worse, as the model's thinking responses become longer, it is also impossible to determine how the generation length for rewarding will vary.\n\n\n\nTo address this issue, we introduced a dynamic placement strategy. At the start of training, we use simple heuristic strategies (such as the number of activated parameters in the model) to set an initial ratio between the policy model and the generative rewarding model. As training progresses, we continuously monitor hardware utilization and gradually reduce the resource allocation for roles with low utilization, reallocating those resources to other roles. Ultimately, the workload across training roles becomes balanced, and hardware utilization approaches full capacity.\n\n\n", "appendix": false}, "Implementation": {"content": "\n\n\nWe implement G-Core using Python and PyTorch \\cite{torch}. Our system combines vLLM \\cite{vllm} and SGlang \\cite{sglang} for generation serving with Megatron-Core \\cite{mlm1, mlm2, mlm3, mlm4} as the training backend.\n\n\n\\subsection{Multi-processing}\n\nNo software is perfect; every piece of software inevitably has some flaws or limitations. Some programs may create conflicting global objects, or rely on global state that makes multi-tenancy impossible. These kinds of issue can lead to unexpected behavior, or difficulties when integrating different components. This can cause subtle or hard-to-detect issues when running multiple models in a single process. For example, some distributed training libraries use a global communication process group, making it impossible to assign different hybrid parallel plans to different models for optimal throughput. Due to the internal scheduling of enterprises, it is unrealistic to expect the open-source community to deliver bug fixes or undertake large-scale refactoring within a specific timeframe. This reality necessitates that enterprises plan ahead and develop their own solutions, ensuring that their projects can proceed according to their own schedules without being hindered by external dependencies.\n\nWe distribute all modules, whether for generation, training, or rewarding, across different processes, and enable them to collaborate via Remote Procedure Call. At the same time, we strive to minimize interference with the orchestration mechanisms used by these modules. By running each component in a separate process, we can avoid issues where multiple instances of certain components interfere with each other, which could otherwise impact throughput or introduce subtle bugs. Furthermore, by respecting the original design of each component and minimizing our interference with their orchestration, we allow the software to operate along well-tested code paths as much as possible. This approach not only improves stability and reliability, but also makes it easier to diagnose and resolve issues when they arise.\n\n\\subsection{Placement and RPC}\n\n\n\nTo keep the placement scheme as simple and predictable as possible (e.g., forming communication groups according to the GPU switch topology) and to minimize interference with component orchestration, we did not use Ray \\cite{ray} for placement and RPC. Instead, we launch tasks via WeChat\u2019s internal job scheduling system. Nevertheless, our code is designed to be loosely coupled, and we believe it can interoperate well with systems such as MPI \\cite{openmpi} and SLURM \\cite{slurm}. We do not doubt that Ray could also accomplish similar tasks.\n\n\n\nTo ensure RPC delivery and exactly-once semantics, we designed a simple and generic RPC mechanism: each RPC request is assigned a unique ID, and the result is cached on the server side until the client successfully retrieves it. The client then sends a request to clean up the cached RPC result. Since deep learning training systems typically only consider complete success (with all other cases treated as complete failure), error handling is greatly simplified. If the RPC returns an unexpected or undesired result, the controller simply terminates all processes.\n\n\n\nSince it is impossible to completely avoid rare and untested code paths in components that may cause the system to hang, or low-probability scenarios within the internal cluster that may result in extremely slow CPU execution, we monitor the training progress to ensure it meets expectations. If the training progress falls below the expected threshold, the job is terminated, resources are reallocated, and the job is restarted.\n\n\\subsection{Idle Resources and Checkpointing}\n\nMost WeChat services exhibit strong temporal fluctuations in resource usage, resulting in a large amount of idle resources during off-peak hours. We leverage these idle resources for non-urgent model training tasks.\n\nTo minimize training progress loss due to interruptions, we employ asynchronous checkpointing to improve checkpoint efficiency and increase checkpoint frequency. Additionally, when online services detect increased load and request resources from the training cluster, we attempt to save an on-demand checkpoint. If the checkpoint cannot be completed within the specified time, we abandon the current progress and release resources to prioritize online service needs.\n\nTo accommodate elastic resource scaling, we utilize distributed checkpointing and design the dataloader consumption state such that checkpoints can be reused across GPU clusters of varying sizes.\n\n\n\\subsection{Workload Balancing}\n\nAs noted in \\cite{megascale,wlbllm}, workload imbalance can create significant bubbles during training, becoming a major bottleneck. When sequence lengths are long, the main training bottleneck is attention computation, whose complexity is $s^2$s^2 for a sequence of length $s$s, while the cost of packing is much lower. For example, if a sequence is formed by concatenating two sequences of length $\\frac s 2$\\frac s 2, the total computation is $\\frac {s^2} 2$\\frac {s^2} 2, leading to severe load imbalance and device underutilization. This issue is even more pronounced with post-training data, which often varies greatly in length.\n\nCompared to complex combinatorial optimization approaches, we found a much simpler solution for workload balancing: instead of sequence packing, we simply sort data by simulated workload, greatly simplifying the process and even improving computational efficiency.\n\nWe can easily show that the proportion of wasted compute is less than 10\\%. Furthermore, using non-uniform bucket splitting can reduce this waste even further.\n\n\n\nNaively sorting data by length may introduce distribution bias. To address this, we first bucket data according to the global batch size, then shuffle the buckets to ensure data is randomly distributed. Our experiments show that this method has almost no impact on model accuracy.\n\n\n\\subsection{Distributed Attention}\n\n\n\nCurrently, most implementations achieve context parallelism using ring attention for both causal and full attention. To support more complex attention masks (e.g., Gemma-3 \\cite{gemma3}), inspired by Llama-3 \\cite{llama3} and open source community contributors, we implemented distributed attention based on CCL all-gather, where we first all-gather the key (K) and value (V) tensors, and then compute attention output for the local query (Q) tensor chunk.\n\nTo reduce the memory footprint of all-gathered key-value pairs, we process only a subset of attention heads at a time and overlap KV communication with attention computation. This makes it feasible to train sequences up to 1 million tokens in length.\n\n\n\\subsection{Train Data Storage}\n\nMultimodal data is often extremely large, both in quantity and size \\cite{qwen25vl, gemma3}. While algorithm developers typically use JSONL files for flexible storage of text data, images and videos cannot be easily stored in this format.\n\nStoring massive numbers of images directly in a distributed file system can easily exceed file number quota, putting pressure on the distributed file system name nodes. To address this, we adopt private key-value storage solutions (e.g. FeatureKV and UnionDB), on top of our private service discovery and distributed file system (Wechat File System) \\cite{wxpaxos}, to serve training data. By leveraging more efficient storage engine data structures, we achieve more efficient and convenient storage and access for training data.\n\n\n", "appendix": false}, "Evaluation": {"content": "\n\nOur evaluation comprises 2 components: 1. comparisons on several tasks with traditional Bradley-Terry reward model, 2. comparision on serveral tasks with generative reward model.\n\nWe conduct this experiment on a cluster of 8 machines (64 GPUs). Each machine is equipped with 8 NVIDIA H20-96GB GPUs inter-connected with NVLink. The inter-machine RDMA bandwidth is 200Gbps. Our experiments use the following software versions: CUDA 12.4, PyTorch 2.5, Megatron-core 0.12, vLLM 0.8.5.\n\nAlthough we used at most 128 GPUs in the evaluation section, we have also validated scenarios with more than 512 GPUs in the production environment.\n\n\\iffalse\n\n\\subsection{Bradley-Terry Reward Model}\n\nWe evaluate G-Core with Bradley-Terry reward model on math task. We employ the Qwen 2.5 model series \\cite{qwen25math} as base models (Qwen2.5-Math-1.5B as the base model and Qwen2.5-Math-RM-72B as the reward model).\n\nWe conduct this experiment on a cluster of 8 machines (64 GPUs). Each machine is equipped with 8 NVIDIA H20-96GB GPUs inter-connected with NVLink. The inter-machine RDMA bandwidth is 200Gbps. Our experiments use the following software versions: CUDA 12.4, PyTorch 2.5, Megatron-core 0.12, vLLM 0.8.5.\n\n\n\nFor GRPO training with the Bradley-Terry reward model, there is no fundamental difference between G-Core and VeRL. Both use a fully co-located placement scheme, and both employ megatron-core together with vllm or sglang as the backend for training and generation. We tend to believe that there is no essential difference in methodology between the two; the only differences are in their implementations.\n\n\n\nFirst, we perform SFT (supervised fine-tuning) using the open-source English dataset NuminaMath-CoT \\cite{li2024numinamath}. During the fine-tuning stage, we train for only one epoch with 859K samples, resulting in an SFT model.\n\n\n\nNext, we conduct GRPO reinforcement learning experiments. We use Qwen2.5-Math-RM-72B as the reward model and the deduplicated GSM-8k \\cite{gsm8k} dataset as the set of questions. The SFT model from the previous stage is further trained with GRPO to obtain the GRPO model. We train for a global batch size of 256 for both rollout and training, repeat generation for 32 times, sequence length of 4k. The policy model uses no model parallelism, while the reward model uses a pipeline parallel size of 4. Since this is only a comparative experiment, we only verified convergence and did not deliberately tune the learning rate or other hyperparameters.\n\nThe evaluation is conducted on the GSM-8K test split, focusing on the format of the generated outputs and accuracy. \"Format\" refers to the percentage of outputs that meet the format requirement of the matching pattern. \"Accuracy\" refers to the percentage of cases where the model's output matches the ground truth result. We train for a fixed number of PPO updates and evaluate the final checkpoint.\n\n\n\\begin{table}[]\n\\centering\n\\begin{tabular}{|c|c|c|c|c|c|c|}\n\\hline\nTrainer & Policy Model & Reward Model & Format & Accuracy & Training Hours \\\\\n\\hline\nVeRL   & Qwen 2.5 1.5B & Qwen 2.5 72B Math RM & 0.990 & 0.845 & xx \\\\\n\\hline\nG-Core & Qwen 2.5 1.5B & Qwen 2.5 72B Math RM & 0.983 & 0.849 & xx \\\\\n\\hline\nVeRL   & Qwen 2.5 7B & Qwen 2.5 72B Math RM & - & - & - \\\\\n\\hline\nG-Core & Qwen 2.5 7B & Qwen 2.5 72B Math RM & - & - & - \\\\\n\\hline\n\\end{tabular}\n\\caption{Comparison with Bradley-Terry reward model}\n\\label{table:cmp-math}\n\\end{table}\n\\centering\n\n\\hline\nTrainer & Policy Model & Reward Model & Format & Accuracy & Training Hours \\\\\n\\hline\nVeRL   & Qwen 2.5 1.5B & Qwen 2.5 72B Math RM & 0.990 & 0.845 & xx \\\\\n\\hline\nG-Core & Qwen 2.5 1.5B & Qwen 2.5 72B Math RM & 0.983 & 0.849 & xx \\\\\n\\hline\nVeRL   & Qwen 2.5 7B & Qwen 2.5 72B Math RM & - & - & - \\\\\n\\hline\nG-Core & Qwen 2.5 7B & Qwen 2.5 72B Math RM & - & - & - \\\\\n\\hline\n\n\\caption{Comparison with Bradley-Terry reward model}\n\\label{table:cmp-math}\n\n\n\n\nFrom Table \\ref{table:cmp-math}, we can see that the convergence of G-Core is as expected, and we consider the difference in the format and accuracy metrics to be within the margin of error. In this experiment, G-Core is slightly faster than VeRL. Theoretically, in this scenario, the throughput of the two should be similar. Based on our rough observation, we believe that verl may have unintentionally reduced the throughput of the policy model because multiple Megatron models actually used the same distributed topology. This experiment was conducted in March 2025, we are not sure whether this issue has been fixed in recent versions of Megatron.\n\n\n\\subsection{Generative Reward Model}\n\nSimilarly, we evaluate G-Core with generative reward model on math task. We employ the Qwen 2.5 model series \\cite{qwen25math} as base models.\n\n\n\nThe comparative experiments for the Generative Reward model are divided into two parts. First, we compare the throughput of the generative reward model under a simple fully co-located setup. Then, we demonstrate how dynamic sampling in certain scenarios leads to frequent swapping, resulting in overhead and a long-tail amplification effect, and show how dynamic placement can address this issue.\n\n\n\nSince VeRL currently can only call external generative reward modeling, we set the resource allocation to 50\\% during our tests, following the example provided by VeRL.\n\n\\begin{table}[]\n\\centering\n\\begin{tabular}{|c|c|c|c|c|c|}\n\\hline\nTrainer & Policy Model & Reward Model & Num GPUs of Policy & Num GPUs of Reward & Training Hours \\\\\n\\hline\nVeRL   & Qwen 2.5 1.5B & Qwen 2.5 72B Math & first 32 & last 32 & xx \\\\\n\\hline\nG-Core & Qwen 2.5 1.5B & Qwen 2.5 72B Math & 64 & 64 & xx \\\\\n\\hline\n\\end{tabular}\n\\caption{Comparison with generative reward model}\n\\label{table:cmp-math-genrm}\n\\end{table}\n\\centering\n\n\\hline\nTrainer & Policy Model & Reward Model & Num GPUs of Policy & Num GPUs of Reward & Training Hours \\\\\n\\hline\nVeRL   & Qwen 2.5 1.5B & Qwen 2.5 72B Math & first 32 & last 32 & xx \\\\\n\\hline\nG-Core & Qwen 2.5 1.5B & Qwen 2.5 72B Math & 64 & 64 & xx \\\\\n\\hline\n\n\\caption{Comparison with generative reward model}\n\\label{table:cmp-math-genrm}\n\n\nFrom Table \\ref{table:cmp-math-genrm}, we can see that during the rollout phase, VeRL can only use a portion of the GPU resources for reward computation, whereas G-Core can utilize all GPUs. In the training phase, the GPU resources allocated to the reward model in VeRL remain idle, while G-Core can use all resources for training. Because G-Core can utilize GPU resources more efficiently, it can easily achieve higher throughput.\n\nAlthough in this experiment the accuracy of the generative reward is not as high as that of the Bradley-Terry reward model, this is only because we did not perform detailed tuning of the algorithm. This does not indicate anything about the nature of the model or the training itself.\n\n\n\\subsection{Dynamic Placement}\n\nBuilding on the experiments in table \\ref{table:cmp-math-genrm}, we introduced dynamic sampling by rejecting cases where accuracy equals 1 or 0. As expected, we observed that as training progresses, the reject rate gradually increases, throughput gradually decreases, and swapping overhead gradually increases.\n\n\n\nAfter enabling dynamic placement, as training progresses, the reject rate gradually increases. However, since both the policy and generative reward models co-exist in GPU memory, the swapping overhead is eliminated. In addition, the long-tail effect is also significantly reduced. As shown in the figure, although the reject rate still increases and throughput continues to decrease, dynamic placement overall achieves higher throughput. It is unrealistic to expect throughput not to decrease when more computation is required; the goal can only be to minimize overhead as much as possible.\n\n\\fi\n\n\n", "appendix": false}, "Conclusion": {"content": "\n\nIn this work, we presented G-Core, a simple, scalable, and balanced RLHF training framework designed to address key bottlenecks in large-scale, multi-model reinforcement learning workflows. By introducing a parallel controller programming model and a dynamic scaling placement schema, G-Core effectively mitigates memory and communication bottlenecks, and adapts to dynamic workloads with improved hardware utilization. Our design emphasizes practicality and flexibility, aiming to reduce engineering complexity while supporting efficient orchestration of complex RLHF pipelines.\n\nG-Core has already produced model that deployed in WeChat\u2019s production environment, supporting real-world business models that serve the entire user base. This practical deployment demonstrates the framework\u2019s robustness and effectiveness at scale. While there remain open challenges in further optimizing resource allocation and supporting increasingly diverse model architectures, we believe G-Core provides a solid foundation for future research and practical deployment of large-scale, human-aligned models.\n\n\n\\bibliographystyle{unsrt}unsrt  \n\\bibliography{refs}  \n\n\n", "appendix": false}}, "categories": ["cs.LG", "cs.AI"], "published": "2025-07-30 15:55:08+00:00", "primary_category": "cs.LG", "summary": "Reinforcement Learning from Human Feedback (RLHF) has become an increasingly\npopular paradigm for training large language models (LLMs) and diffusion\nmodels. While existing RLHF training systems have enabled significant progress,\nthey often face challenges in scaling to multi-modal and diffusion workflows\nand adapting to dynamic workloads. In particular, current approaches may\nencounter limitations in controller scalability, flexible resource placement,\nand efficient orchestration when handling complex RLHF pipelines, especially in\nscenarios involving dynamic sampling or generative reward modeling. In this\npaper, we present \\textbf{G-Core}, a simple, scalable, and balanced RLHF\ntraining framework designed to address these challenges. G-Core introduces a\nparallel controller programming model, enabling flexible and efficient\norchestration of complex RLHF workflows without the bottlenecks of a single\ncentralized controller. Furthermore, we propose a dynamic placement schema that\nadaptively partitions resources and schedules workloads, significantly reducing\nhardware idle time and improving utilization, even under highly variable\ntraining conditions. G-Core has successfully trained models that support WeChat\nproduct features serving a large-scale user base, demonstrating its\neffectiveness and robustness in real-world scenarios. Our results show that\nG-Core advances the state of the art in RLHF training, providing a solid\nfoundation for future research and deployment of large-scale, human-aligned\nmodels."}