seed: ['2506.21506v1']
cnt: 0
BFS_que.qsize(): 1
current paper: 2506.21506v1
Thread 13092106240 Processing 2506.21506v1
Downloading source
Downloaded source for 2506.21506v1 to download/2506.21506v1/2506.21506v1.tar.gz
Thread 13092106240 Processing file main.tex
bib_names: ['ref.bib']
bbl_nodes: []
Processing input sections/0-abstract
Processing input sections/1-intro
Processing input sections/2-related
Processing input sections/3-benchmark
Processing input tables/cmp_existing
Processing input tables/statistics
Processing input sections/4-experiments
Processing input tables/main_results
Processing input sections/5-analysis
Processing input sections/6-conclusion
Processing input app_sections/app_overview
Processing input app_sections/limitation
Processing input app_sections/broader_impacts
Processing input app_sections/details_exp
Processing input app_sections/details_rubric_scripts
Processing input app_sections/raw_instructions
numbmer of citations in node info method: 0
The bib is loaded as below:
<_io.TextIOWrapper name='download/working_folder/13092106240/ref.bib' mode='r' encoding='UTF-8'>
doc_node: LatexEnvironmentNode(parsing_state=<parsing state 4691586928>, pos=8073, len=1950, environmentname='document', nodelist=[LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8089, len=3, chars='\n\n\n'), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=8092, len=10, macroname='maketitle', nodeargd=ParsedMacroArgs(argspec='', argnlist=[]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8102, len=2, chars='\n\n'), LatexEnvironmentNode(parsing_state=<parsing state 4691586928>, pos=8104, len=378, environmentname='figure', nodelist=[LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8121, len=3, chars='\n  '), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=8124, len=13, macroname='centering', nodeargd=ParsedMacroArgs(argspec='', argnlist=[]), macro_post_space='\n  '), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=8137, len=15, macroname='vspace', nodeargd=ParsedMacroArgs(argspec='*{', argnlist=[None, LatexGroupNode(parsing_state=<parsing state 4691586928>, pos=8144, len=8, nodelist=[LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8145, len=6, chars='-2.5em')], delimiters=('{', '}'))]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8152, len=3, chars='\n  '), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=8155, len=94, macroname='includegraphics', nodeargd=ParsedMacroArgs(argspec='[{', argnlist=[LatexGroupNode(parsing_state=<parsing state 4691586928>, pos=8171, len=49, nodelist=[LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8172, len=10, chars='width=0.72'), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=8182, len=10, macroname='textwidth', nodeargd=ParsedMacroArgs(argspec='', argnlist=[]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8192, len=27, chars=',trim=0cm 0cm 0cm 0cm, clip')], delimiters=('[', ']')), LatexGroupNode(parsing_state=<parsing state 4691586928>, pos=8220, len=29, nodelist=[LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8221, len=27, chars='figures/m2w2_figure1_v8.pdf')], delimiters=('{', '}'))]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8249, len=3, chars='\n  '), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=8252, len=179, macroname='caption', nodeargd=ParsedMacroArgs(argspec='{', argnlist=[LatexGroupNode(parsing_state=<parsing state 4691586928>, pos=8260, len=171, nodelist=[LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=8261, len=6, macroname='mind', nodeargd=ParsedMacroArgs(argspec='', argnlist=[]), macro_post_space=' '), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8267, len=163, chars='features realistic and diverse long-horizon web search tasks and a novel Agent-as-a-Judge framework to evaluate complex, time-varying, and citation-backed answers.')], delimiters=('{', '}'))]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8431, len=3, chars='\n  '), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=8434, len=14, macroname='vspace', nodeargd=ParsedMacroArgs(argspec='*{', argnlist=[None, LatexGroupNode(parsing_state=<parsing state 4691586928>, pos=8441, len=7, nodelist=[LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8442, len=5, chars='-.8em')], delimiters=('{', '}'))]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8448, len=3, chars='\n  '), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=8451, len=18, macroname='label', nodeargd=ParsedMacroArgs(argspec='{', argnlist=[LatexGroupNode(parsing_state=<parsing state 4691586928>, pos=8457, len=12, nodelist=[LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8458, len=10, chars='fig:teaser')], delimiters=('{', '}'))]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8469, len=1, chars='\n')], nodeargd=ParsedMacroArgs(argspec='[', argnlist=[LatexGroupNode(parsing_state=<parsing state 4691586928>, pos=8118, len=3, nodelist=[LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8119, len=1, chars='h')], delimiters=('[', ']'))])), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8482, len=2, chars='\n\n'), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=8484, len=27, macroname='input', nodeargd=ParsedMacroArgs(argspec='{', argnlist=[LatexGroupNode(parsing_state=<parsing state 4691586928>, pos=8490, len=21, nodelist=[LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8491, len=19, chars='sections/0-abstract')], delimiters=('{', '}'))]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8511, len=5, chars='\n\n\n\n\n'), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=8516, len=24, macroname='input', nodeargd=ParsedMacroArgs(argspec='{', argnlist=[LatexGroupNode(parsing_state=<parsing state 4691586928>, pos=8522, len=18, nodelist=[LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8523, len=16, chars='sections/1-intro')], delimiters=('{', '}'))]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8540, len=3, chars='\n\n\n'), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=8543, len=26, macroname='input', nodeargd=ParsedMacroArgs(argspec='{', argnlist=[LatexGroupNode(parsing_state=<parsing state 4691586928>, pos=8549, len=20, nodelist=[LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8550, len=18, chars='sections/2-related')], delimiters=('{', '}'))]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8569, len=2, chars='\n\n'), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=8571, len=28, macroname='input', nodeargd=ParsedMacroArgs(argspec='{', argnlist=[LatexGroupNode(parsing_state=<parsing state 4691586928>, pos=8577, len=22, nodelist=[LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8578, len=20, chars='sections/3-benchmark')], delimiters=('{', '}'))]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8599, len=2, chars='\n\n'), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=8601, len=30, macroname='input', nodeargd=ParsedMacroArgs(argspec='{', argnlist=[LatexGroupNode(parsing_state=<parsing state 4691586928>, pos=8607, len=24, nodelist=[LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8608, len=22, chars='sections/4-experiments')], delimiters=('{', '}'))]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8631, len=2, chars='\n\n'), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=8633, len=27, macroname='input', nodeargd=ParsedMacroArgs(argspec='{', argnlist=[LatexGroupNode(parsing_state=<parsing state 4691586928>, pos=8639, len=21, nodelist=[LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8640, len=19, chars='sections/5-analysis')], delimiters=('{', '}'))]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8660, len=6, chars='\n\n\n\n\n\n'), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=8666, len=29, macroname='input', nodeargd=ParsedMacroArgs(argspec='{', argnlist=[LatexGroupNode(parsing_state=<parsing state 4691586928>, pos=8672, len=23, nodelist=[LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8673, len=21, chars='sections/6-conclusion')], delimiters=('{', '}'))]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8695, len=3, chars='\n\n\n'), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=8698, len=26, macroname='section', nodeargd=ParsedMacroArgs(argspec='*[{', argnlist=[LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8706, len=1, chars='*'), None, LatexGroupNode(parsing_state=<parsing state 4691586928>, pos=8707, len=17, nodelist=[LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8708, len=15, chars='Acknowledgments')], delimiters=('{', '}'))]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=8724, len=792, chars='\nThe authors would like to thank colleagues from the OSU NLP group and Amazon AGI for constructive discussions and generous help, Zishuo Zheng for his exploration of developing long-horizon agentic search agents, Akshay Anand and Scott Salisbury for their help on benchmark construction, the HuggingFace team (Amir Mahla, Aymeric Roucher, Aksel Joonas Reedi, and Thomas Wolf) for their assistance with the evaluation of HuggingFace Open Deep Research as well as covering the inference costs, the Grok team (Piaoyang Cui, Hexiang Hu) for their assistance with the evaluation of Grok DeepResearch and DeeperResearch, and the Amazon AGI team for their valuable feedback and contribution to task collection. This research is sponsored in part by a gift from Amazon, ARL W911NF2220144, NSF CAREER '), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=9516, len=2, macroname='#', nodeargd=ParsedMacroArgs(argspec='', argnlist=[]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=9518, len=361, chars='1942980, and NSF OAC 2112606. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. government. The U.S. government is authorized to reproduce and distribute reprints for government purposes notwithstanding any copyright notice herein.\n\n'), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=9879, len=18, macroname='bibliographystyle', nodeargd=ParsedMacroArgs(argspec='', argnlist=[]), macro_post_space=''), LatexGroupNode(parsing_state=<parsing state 4691586928>, pos=9897, len=10, nodelist=[LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=9898, len=8, chars='plainnat')], delimiters=('{', '}')), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=9907, len=1, chars='\n'), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=9908, len=22, macroname='bibliography', nodeargd=ParsedMacroArgs(argspec='{', argnlist=[LatexGroupNode(parsing_state=<parsing state 4691586928>, pos=9921, len=9, nodelist=[LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=9922, len=7, chars='ref.bib')], delimiters=('{', '}'))]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=9930, len=3, chars='\n\n\n'), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=9933, len=8, macroname='newpage', nodeargd=ParsedMacroArgs(argspec='', argnlist=[]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=9941, len=2, chars='\n\n'), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=9943, len=8, macroname='newpage', nodeargd=ParsedMacroArgs(argspec='', argnlist=[]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=9951, len=2, chars='\n\n'), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=9953, len=9, macroname='newpage', nodeargd=ParsedMacroArgs(argspec='', argnlist=[]), macro_post_space='\n'), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=9962, len=9, macroname='appendix', nodeargd=ParsedMacroArgs(argspec='', argnlist=[]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=9971, len=2, chars='\n\n'), LatexMacroNode(parsing_state=<parsing state 4691586928>, pos=9973, len=33, macroname='input', nodeargd=ParsedMacroArgs(argspec='{', argnlist=[LatexGroupNode(parsing_state=<parsing state 4691586928>, pos=9979, len=27, nodelist=[LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=9980, len=25, chars='app_sections/app_overview')], delimiters=('{', '}'))]), macro_post_space=''), LatexCharsNode(parsing_state=<parsing state 4691586928>, pos=10006, len=3, chars='\n\n\n')], nodeargd=ParsedMacroArgs(argspec='', argnlist=[]))
Extracting citation information
Processing input sections/0-abstract
Processing input sections/1-intro
Processing input sections/2-related
Processing input sections/3-benchmark
Processing input tables/cmp_existing
Processing input tables/statistics
Processing input sections/4-experiments
Processing input tables/main_results
Processing input sections/5-analysis
Processing input sections/6-conclusion
Processing input app_sections/app_overview
Processing input app_sections/limitation
Processing input app_sections/broader_impacts
Processing input app_sections/details_exp
Processing input app_sections/details_rubric_scripts
Processing input app_sections/raw_instructions
structured_data: {'title': 'Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge', 'author': 'Boyu Gou', 'abstract': '\\begin{abstract}\n\nAgentic search such as Deep Research systems—where large language models autonomously browse the web, synthesize information, and return comprehensive citation-backed answers—represents a major shift in how users interact with web-scale information. \nWhile promising greater efficiency and cognitive offloading, the growing complexity and open-endedness of agentic search have outpaced existing evaluation benchmarks and methodologies, which largely assume short search horizons and static answers. \nIn this paper, we introduce \\mind, a benchmark of \\ntasks realistic, high-quality, and long-horizon tasks that require real-time web browsing and extensive information synthesis, constructed with over 1,000 hours of human labor.\nTo address the challenge of evaluating time-varying and complex answers, we propose a novel Agent-as-a-Judge framework. \nOur method constructs \\textit{task-specific judge agents} based on a tree-structured rubric design to automatically assess both answer correctness and source attribution. \nWe conduct a comprehensive evaluation of nine frontier agentic search systems and human performance, along with a detailed error analysis to draw insights for future development.\nThe best-performing system, OpenAI Deep Research, can already achieve 50-70\\% of human performance while spending half the time, showing a great potential.\nAltogether, \\mind provides a rigorous foundation for developing and benchmarking the next generation of agentic search systems.\n\n\n\n\n\\end{abstract}', 'citations': {'salton1975vector': {'bib_key': 'salton1975vector', 'bib_title': 'A vector space model for automatic indexing', 'bib_author ': 'Salton, Gerard', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'The core techniques supporting web search have undergone constant evolution in the past decades, from TF-IDF~\\cite{salton1975vector}', 'next_context': 'for term statistics to PageRank~\\cite{brin1998anatomy}for network analysis and learning to rank~\\cite{liu2009learning,burges2005learning}for supervised learning.'}], 'importance_score': 1.0}, 'brin1998anatomy': {'bib_key': 'brin1998anatomy', 'bib_title': 'The anatomy of a large-scale hypertextual web search engine', 'bib_author ': 'Brin, Sergey', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'The core techniques supporting web search have undergone constant evolution in the past decades, from TF-IDF~\\cite{salton1975vector}for term statistics to PageRank~\\cite{brin1998anatomy}', 'next_context': 'for network analysis and learning to rank~\\cite{liu2009learning,burges2005learning}for supervised learning.'}], 'importance_score': 1.0}, 'liu2009learning': {'bib_key': 'liu2009learning', 'bib_title': 'Learning to rank for information retrieval', 'bib_author ': 'Liu, Tie-Yan', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'The core techniques supporting web search have undergone constant evolution in the past decades, from TF-IDF~\\cite{salton1975vector}for term statistics to PageRank~\\cite{brin1998anatomy}for network analysis and learning to rank~\\cite{liu2009learning,burges2005learning}', 'next_context': 'for supervised learning.'}], 'importance_score': 0.5}, 'burges2005learning': {'bib_key': 'burges2005learning', 'bib_title': 'Learning to rank using gradient descent', 'bib_author ': 'Burges, Chris', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'The core techniques supporting web search have undergone constant evolution in the past decades, from TF-IDF~\\cite{salton1975vector}for term statistics to PageRank~\\cite{brin1998anatomy}for network analysis and learning to rank~\\cite{liu2009learning,burges2005learning}', 'next_context': 'for supervised learning.'}], 'importance_score': 0.5}, 'webgpt': {'bib_key': 'webgpt', 'bib_title': 'WebGPT: Browser-assisted question-answering with human feedback', 'bib_author ': 'Reiichiro Nakano', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'In recent years, agentic search has quickly progressed from\\textit{search-augmented LLMs}(e.g., ChatGPT and Perplexity Search) to LLM-based\\textit{autonomous web agents}~\\cite{webgpt,deng2023mind2web,zhou2024webarena,zheng2024seeact,claude_computer_use,openai2025operator}', 'next_context': 'and recent\\textit{Deep Research}systems~\\cite{gemini_deep_research,openai2025deepresearch}specifically optimized for long-horizon browsing and search behavior.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Autonomous web agents~\\cite{webgpt,deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena}', 'next_context': ', especially those with visual perception of the web~\\cite{zheng2024seeact,koh2024visualwebarena,gou2025uground,qin2025ui}, have emerged to browse the real-time web as humans do.'}], 'importance_score': 0.41666666666666663}, 'deng2023mind2web': {'bib_key': 'deng2023mind2web', 'bib_title': 'Mind2web: Towards a generalist agent for the web', 'bib_author ': 'Deng, Xiang', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'In recent years, agentic search has quickly progressed from\\textit{search-augmented LLMs}(e.g., ChatGPT and Perplexity Search) to LLM-based\\textit{autonomous web agents}~\\cite{webgpt,deng2023mind2web,zhou2024webarena,zheng2024seeact,claude_computer_use,openai2025operator}', 'next_context': 'and recent\\textit{Deep Research}systems~\\cite{gemini_deep_research,openai2025deepresearch}specifically optimized for long-horizon browsing and search behavior.'}, {'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'Many benchmarks have been proposed for autonomous web agents~\\cite{deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena,lu2024weblinx,xue2025onlinemind2web}', 'next_context': 'but they primarily focus on tasks of a moderate horizon (e.g., up to 10 actions) that can be completed on a single website.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Autonomous web agents~\\cite{webgpt,deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena}', 'next_context': ', especially those with visual perception of the web~\\cite{zheng2024seeact,koh2024visualwebarena,gou2025uground,qin2025ui}, have emerged to browse the real-time web as humans do.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Most existing benchmarks for web agents focus on evaluating whether an agent can autonomously perform certain processes on a single website~\\cite{deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena,lu2024weblinx,he2024webvoyager,xue2025onlinemind2web,koh2024visualwebarena}', 'next_context': '.'}, {'section': 'Details of Task Construction', 'subsection': 'Domain Distribution', 'subsubsection': None, 'prev_context': 'During task collection, proposers are provided an initial set of fine-grained domains derived from prior work~\\citep{deng2023mind2web}', 'next_context': 'and further expanded using\\texttt{GPT-4o}.'}], 'importance_score': 1.7595238095238095}, 'zhou2024webarena': {'bib_key': 'zhou2024webarena', 'bib_title': 'WebArena: A Realistic Web Environment for Building Autonomous Agents', 'bib_author ': 'Zhou, Shuyan', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'In recent years, agentic search has quickly progressed from\\textit{search-augmented LLMs}(e.g., ChatGPT and Perplexity Search) to LLM-based\\textit{autonomous web agents}~\\cite{webgpt,deng2023mind2web,zhou2024webarena,zheng2024seeact,claude_computer_use,openai2025operator}', 'next_context': 'and recent\\textit{Deep Research}systems~\\cite{gemini_deep_research,openai2025deepresearch}specifically optimized for long-horizon browsing and search behavior.'}, {'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'Many benchmarks have been proposed for autonomous web agents~\\cite{deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena,lu2024weblinx,xue2025onlinemind2web}', 'next_context': 'but they primarily focus on tasks of a moderate horizon (e.g., up to 10 actions) that can be completed on a single website.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Autonomous web agents~\\cite{webgpt,deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena}', 'next_context': ', especially those with visual perception of the web~\\cite{zheng2024seeact,koh2024visualwebarena,gou2025uground,qin2025ui}, have emerged to browse the real-time web as humans do.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Most existing benchmarks for web agents focus on evaluating whether an agent can autonomously perform certain processes on a single website~\\cite{deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena,lu2024weblinx,he2024webvoyager,xue2025onlinemind2web,koh2024visualwebarena}', 'next_context': '.'}], 'importance_score': 0.7595238095238095}, 'zheng2024seeact': {'bib_key': 'zheng2024seeact', 'bib_title': '{GPT-4V}(ision) is a Generalist Web Agent, if Grounded', 'bib_author ': 'Boyuan Zheng', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'In recent years, agentic search has quickly progressed from\\textit{search-augmented LLMs}(e.g., ChatGPT and Perplexity Search) to LLM-based\\textit{autonomous web agents}~\\cite{webgpt,deng2023mind2web,zhou2024webarena,zheng2024seeact,claude_computer_use,openai2025operator}', 'next_context': 'and recent\\textit{Deep Research}systems~\\cite{gemini_deep_research,openai2025deepresearch}specifically optimized for long-horizon browsing and search behavior.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Autonomous web agents~\\cite{webgpt,deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena}, especially those with visual perception of the web~\\cite{zheng2024seeact,koh2024visualwebarena,gou2025uground,qin2025ui}', 'next_context': ', have emerged to browse the real-time web as humans do.'}], 'importance_score': 0.41666666666666663}, 'claude_computer_use': {'bib_key': 'claude_computer_use', 'bib_title': 'Claude Computer Use', 'bib_author ': '{Anthropic}', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'In recent years, agentic search has quickly progressed from\\textit{search-augmented LLMs}(e.g., ChatGPT and Perplexity Search) to LLM-based\\textit{autonomous web agents}~\\cite{webgpt,deng2023mind2web,zhou2024webarena,zheng2024seeact,claude_computer_use,openai2025operator}', 'next_context': 'and recent\\textit{Deep Research}systems~\\cite{gemini_deep_research,openai2025deepresearch}specifically optimized for long-horizon browsing and search behavior.'}], 'importance_score': 0.16666666666666666}, 'openai2025operator': {'bib_key': 'openai2025operator', 'bib_title': 'Operator System Card', 'bib_author ': '{OpenAI}', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'In recent years, agentic search has quickly progressed from\\textit{search-augmented LLMs}(e.g., ChatGPT and Perplexity Search) to LLM-based\\textit{autonomous web agents}~\\cite{webgpt,deng2023mind2web,zhou2024webarena,zheng2024seeact,claude_computer_use,openai2025operator}', 'next_context': 'and recent\\textit{Deep Research}systems~\\cite{gemini_deep_research,openai2025deepresearch}specifically optimized for long-horizon browsing and search behavior.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': "OpenAI's Operator~\\cite{openai2025operator}", 'next_context': ', with specialized reinforcement learning training, represents the current frontier~\\cite{xue2025onlinemind2web}.'}, {'section': 'Experiments', 'subsection': 'Experimental Setup', 'subsubsection': None, 'prev_context': 'Lastly, we assess OpenAI Operator~\\cite{openai2025operator}', 'next_context': ', one of the most advanced web agents currently available, which perform tasks through direct browser interactions.'}], 'importance_score': 2.166666666666667}, 'gemini_deep_research': {'bib_key': 'gemini_deep_research', 'bib_title': 'Gemini Deep Research', 'bib_author ': '{Google}', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'In recent years, agentic search has quickly progressed from\\textit{search-augmented LLMs}(e.g., ChatGPT and Perplexity Search) to LLM-based\\textit{autonomous web agents}~\\cite{webgpt,deng2023mind2web,zhou2024webarena,zheng2024seeact,claude_computer_use,openai2025operator}and recent\\textit{Deep Research}systems~\\cite{gemini_deep_research,openai2025deepresearch}', 'next_context': 'specifically optimized for long-horizon browsing and search behavior.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Recent advances in reasoning models~\\cite{jaech2024openai, guo2025deepseek}have enabled the development of Deep Research systems\\citep{openai2025deepresearch,gemini_deep_research,huggingface_open_deep_research}', 'next_context': 'that leverage a suite of advanced tools, including search APIs and web browsing, '}, {'section': 'Experiments', 'subsection': 'Experimental Setup', 'subsubsection': None, 'prev_context': 'Additionally, we systematically evaluate a suite of Deep Research systems~\\cite{huggingface_open_deep_research, perplexityai, grok3, gemini_deep_research, openai2025deepresearch}', 'next_context': ', which are explicitly optimized for extensive information gathering and comprehensive report generation, many of which can sustain continuous running for extended periods (e.g., beyond 30 minutes per query).'}], 'importance_score': 1.0333333333333332}, 'openai2025deepresearch': {'bib_key': 'openai2025deepresearch', 'bib_title': 'Deep Research System Card', 'bib_author ': '{OpenAI}', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'In recent years, agentic search has quickly progressed from\\textit{search-augmented LLMs}(e.g., ChatGPT and Perplexity Search) to LLM-based\\textit{autonomous web agents}~\\cite{webgpt,deng2023mind2web,zhou2024webarena,zheng2024seeact,claude_computer_use,openai2025operator}and recent\\textit{Deep Research}systems~\\cite{gemini_deep_research,openai2025deepresearch}', 'next_context': 'specifically optimized for long-horizon browsing and search behavior.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Recent advances in reasoning models~\\cite{jaech2024openai, guo2025deepseek}have enabled the development of Deep Research systems\\citep{openai2025deepresearch,gemini_deep_research,huggingface_open_deep_research}', 'next_context': 'that leverage a suite of advanced tools, including search APIs and web browsing, '}, {'section': 'Experiments', 'subsection': 'Experimental Setup', 'subsubsection': None, 'prev_context': 'Additionally, we systematically evaluate a suite of Deep Research systems~\\cite{huggingface_open_deep_research, perplexityai, grok3, gemini_deep_research, openai2025deepresearch}', 'next_context': ', which are explicitly optimized for extensive information gathering and comprehensive report generation, many of which can sustain continuous running for extended periods (e.g., beyond 30 minutes per query).'}], 'importance_score': 1.0333333333333332}, 'hendrycksmeasuring': {'bib_key': 'hendrycksmeasuring', 'bib_title': 'Measuring Massive Multitask Language Understanding', 'bib_author ': 'Hendrycks, Dan', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'Meanwhile, automated and reliable evaluation has proven crucial for the iterative development of AI technologies, especially in the early stages~\\cite{hendrycksmeasuring,chiang2024chatbot,yue2024mmmu}', 'next_context': '.'}], 'importance_score': 0.3333333333333333}, 'chiang2024chatbot': {'bib_key': 'chiang2024chatbot', 'bib_title': 'Chatbot arena: An open platform for evaluating llms by human preference', 'bib_author ': 'Chiang, Wei-Lin', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'Meanwhile, automated and reliable evaluation has proven crucial for the iterative development of AI technologies, especially in the early stages~\\cite{hendrycksmeasuring,chiang2024chatbot,yue2024mmmu}', 'next_context': '.'}], 'importance_score': 0.3333333333333333}, 'yue2024mmmu': {'bib_key': 'yue2024mmmu', 'bib_title': 'Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi', 'bib_author ': 'Yue, Xiang', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'Meanwhile, automated and reliable evaluation has proven crucial for the iterative development of AI technologies, especially in the early stages~\\cite{hendrycksmeasuring,chiang2024chatbot,yue2024mmmu}', 'next_context': '.'}], 'importance_score': 0.3333333333333333}, 'yaoWebShopScalableRealWorld2022': {'bib_key': 'yaoWebShopScalableRealWorld2022', 'bib_title': 'Webshop: Towards scalable real-world web interaction with grounded language agents', 'bib_author ': 'Yao, Shunyu', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'Many benchmarks have been proposed for autonomous web agents~\\cite{deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena,lu2024weblinx,xue2025onlinemind2web}', 'next_context': 'but they primarily focus on tasks of a moderate horizon (e.g., up to 10 actions) that can be completed on a single website.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Autonomous web agents~\\cite{webgpt,deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena}', 'next_context': ', especially those with visual perception of the web~\\cite{zheng2024seeact,koh2024visualwebarena,gou2025uground,qin2025ui}, have emerged to browse the real-time web as humans do.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Most existing benchmarks for web agents focus on evaluating whether an agent can autonomously perform certain processes on a single website~\\cite{deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena,lu2024weblinx,he2024webvoyager,xue2025onlinemind2web,koh2024visualwebarena}', 'next_context': '.'}], 'importance_score': 0.5928571428571429}, 'lu2024weblinx': {'bib_key': 'lu2024weblinx', 'bib_title': 'WebLINX: Real-World Website Navigation with Multi-Turn Dialogue', 'bib_author ': 'Lu, Xing Han', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'Many benchmarks have been proposed for autonomous web agents~\\cite{deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena,lu2024weblinx,xue2025onlinemind2web}', 'next_context': 'but they primarily focus on tasks of a moderate horizon (e.g., up to 10 actions) that can be completed on a single website.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Most existing benchmarks for web agents focus on evaluating whether an agent can autonomously perform certain processes on a single website~\\cite{deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena,lu2024weblinx,he2024webvoyager,xue2025onlinemind2web,koh2024visualwebarena}', 'next_context': '.'}], 'importance_score': 0.34285714285714286}, 'xue2025onlinemind2web': {'bib_key': 'xue2025onlinemind2web', 'bib_title': 'An illusion of progress? assessing the current state of web agents', 'bib_author ': 'Xue, Tianci', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'Many benchmarks have been proposed for autonomous web agents~\\cite{deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena,lu2024weblinx,xue2025onlinemind2web}', 'next_context': 'but they primarily focus on tasks of a moderate horizon (e.g., up to 10 actions) that can be completed on a single website.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': "OpenAI's Operator~\\cite{openai2025operator}, with specialized reinforcement learning training, represents the current frontier~\\cite{xue2025onlinemind2web}", 'next_context': '.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Most existing benchmarks for web agents focus on evaluating whether an agent can autonomously perform certain processes on a single website~\\cite{deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena,lu2024weblinx,he2024webvoyager,xue2025onlinemind2web,koh2024visualwebarena}', 'next_context': '.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': '~\\cite{zheng2023judging}has been widely used in evaluating complex tasks, including for web agents~\\cite{pan2024autonomous,he2024webvoyager,xue2025onlinemind2web}', 'next_context': '.'}, {'section': '', 'subsection': 'Rubric-based Judge Agent', 'subsubsection': None, 'prev_context': '\\midruleOnline-Mind2Web~\\cite{xue2025onlinemind2web}', 'next_context': ''}, {'section': 'Experiments', 'subsection': 'Human Evaluation of Judge Agents', 'subsubsection': None, 'prev_context': 'This demonstrates remarkable reliability, particularly when compared to recent automated evaluation approaches for relatively simpler web tasks\\citep{xue2025onlinemind2web}', 'next_context': ', where reported correctness rates of the automated evaluation methods typically fall below 90\\%.'}, {'section': 'Details of Task Construction', 'subsection': 'Future Maintenance of the Benchmark', 'subsubsection': None, 'prev_context': 'Similar to previous benchmarks that rely on live web environments~\\citep{pan2024webcanvas,xue2025onlinemind2web}', 'next_context': ', tasks in\\mind~may be affected by changes or updates to websites over time.'}], 'importance_score': 4.176190476190476}, 'mialon2023gaia': {'bib_key': 'mialon2023gaia', 'bib_title': 'Gaia: a benchmark for general ai assistants', 'bib_author ': "Mialon, Gr{\\'e}goire", 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'Several benchmarks cover cross-website search tasks~\\cite{mialon2023gaia,yoran2024assistantbenchwebagentssolve,song2025bearcubs}', 'next_context': ', including most recently BrowseComp~\\cite{wei2025browsecomp}from OpenAI.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Several recent benchmarks have a stronger focus on search over the open web~\\cite{mialon2023gaia,yoran2024assistantbenchwebagentssolve,wu2025webwalker,song2025bearcubs,wei2025browsecomp}', 'next_context': '.'}, {'section': '', 'subsection': 'Rubric-based Judge Agent', 'subsubsection': None, 'prev_context': 'GAIA~\\cite{mialon2023gaia}', 'next_context': ''}], 'importance_score': 1.5333333333333332}, 'yoran2024assistantbenchwebagentssolve': {'bib_key': 'yoran2024assistantbenchwebagentssolve', 'bib_title': '{AssistantBench}: Can Web Agents Solve Realistic and Time-Consuming Tasks?', 'bib_author ': 'Ori Yoran', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'Several benchmarks cover cross-website search tasks~\\cite{mialon2023gaia,yoran2024assistantbenchwebagentssolve,song2025bearcubs}', 'next_context': ', including most recently BrowseComp~\\cite{wei2025browsecomp}from OpenAI.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Several recent benchmarks have a stronger focus on search over the open web~\\cite{mialon2023gaia,yoran2024assistantbenchwebagentssolve,wu2025webwalker,song2025bearcubs,wei2025browsecomp}', 'next_context': '.'}, {'section': '', 'subsection': 'Rubric-based Judge Agent', 'subsubsection': None, 'prev_context': 'WebWalkerQA~\\cite{yoran2024assistantbenchwebagentssolve}', 'next_context': ''}, {'section': '', 'subsection': 'Rubric-based Judge Agent', 'subsubsection': None, 'prev_context': 'AssistantBench~\\cite{yoran2024assistantbenchwebagentssolve}', 'next_context': ''}], 'importance_score': 2.533333333333333}, 'song2025bearcubs': {'bib_key': 'song2025bearcubs', 'bib_title': 'BEARCUBS: A benchmark for computer-using web agents', 'bib_author ': 'Song, Yixiao', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'Several benchmarks cover cross-website search tasks~\\cite{mialon2023gaia,yoran2024assistantbenchwebagentssolve,song2025bearcubs}', 'next_context': ', including most recently BrowseComp~\\cite{wei2025browsecomp}from OpenAI.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Several recent benchmarks have a stronger focus on search over the open web~\\cite{mialon2023gaia,yoran2024assistantbenchwebagentssolve,wu2025webwalker,song2025bearcubs,wei2025browsecomp}', 'next_context': '.'}, {'section': '', 'subsection': 'Rubric-based Judge Agent', 'subsubsection': None, 'prev_context': 'BEARCUBS~\\cite{song2025bearcubs}', 'next_context': ''}], 'importance_score': 1.5333333333333332}, 'wei2025browsecomp': {'bib_key': 'wei2025browsecomp', 'bib_title': 'Browsecomp: A simple yet challenging benchmark for browsing agents', 'bib_author ': 'Wei, Jason', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'Several benchmarks cover cross-website search tasks~\\cite{mialon2023gaia,yoran2024assistantbenchwebagentssolve,song2025bearcubs}, including most recently BrowseComp~\\cite{wei2025browsecomp}', 'next_context': 'from OpenAI.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Several recent benchmarks have a stronger focus on search over the open web~\\cite{mialon2023gaia,yoran2024assistantbenchwebagentssolve,wu2025webwalker,song2025bearcubs,wei2025browsecomp}', 'next_context': '.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'The BrowseComp benchmark~\\cite{wei2025browsecomp}', 'next_context': 'from OpenAI, a concurrent work to ours, is representative of this evaluation methodology.'}, {'section': '', 'subsection': 'Rubric-based Judge Agent', 'subsubsection': None, 'prev_context': 'BrowseComp~\\cite{wei2025browsecomp}', 'next_context': ''}], 'importance_score': 3.2}, 'zheng2023judging': {'bib_key': 'zheng2023judging', 'bib_title': 'Judging llm-as-a-judge with mt-bench and chatbot arena', 'bib_author ': 'Zheng, Lianmin', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Introduction', 'subsection': None, 'subsubsection': None, 'prev_context': 'The complexity is far beyond what conventional LLM-as-a-Judge~\\cite{zheng2023judging}', 'next_context': 'methods are used for.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': '~\\cite{zheng2023judging}', 'next_context': 'has been widely used in evaluating complex tasks, including for web agents~\\cite{pan2024autonomous,he2024webvoyager,xue2025onlinemind2web}.'}], 'importance_score': 2.0}, 'chen2024mindsearch': {'bib_key': 'chen2024mindsearch', 'bib_title': 'Mindsearch: Mimicking human minds elicits deep ai searcher', 'bib_author ': 'Chen, Zehui', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Early systems like MindSearch~\\cite{chen2024mindsearch}', 'next_context': ', ChatGPT and Perplexity Search augment LLMs with search APIs to iteratively search for up-to-date information.'}], 'importance_score': 1.0}, 'koh2024visualwebarena': {'bib_key': 'koh2024visualwebarena', 'bib_title': 'VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks', 'bib_author ': 'Koh, Jing Yu', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Autonomous web agents~\\cite{webgpt,deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena}, especially those with visual perception of the web~\\cite{zheng2024seeact,koh2024visualwebarena,gou2025uground,qin2025ui}', 'next_context': ', have emerged to browse the real-time web as humans do.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Most existing benchmarks for web agents focus on evaluating whether an agent can autonomously perform certain processes on a single website~\\cite{deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena,lu2024weblinx,he2024webvoyager,xue2025onlinemind2web,koh2024visualwebarena}', 'next_context': '.'}], 'importance_score': 0.39285714285714285}, 'gou2025uground': {'bib_key': 'gou2025uground', 'bib_title': 'Navigating the Digital World as Humans Do: Universal Visual Grounding for {GUI} Agents', 'bib_author ': 'Boyu Gou', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Autonomous web agents~\\cite{webgpt,deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena}, especially those with visual perception of the web~\\cite{zheng2024seeact,koh2024visualwebarena,gou2025uground,qin2025ui}', 'next_context': ', have emerged to browse the real-time web as humans do.'}], 'importance_score': 0.25}, 'qin2025ui': {'bib_key': 'qin2025ui', 'bib_title': 'UI-TARS: Pioneering Automated GUI Interaction with Native Agents', 'bib_author ': 'Qin, Yujia', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Autonomous web agents~\\cite{webgpt,deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena}, especially those with visual perception of the web~\\cite{zheng2024seeact,koh2024visualwebarena,gou2025uground,qin2025ui}', 'next_context': ', have emerged to browse the real-time web as humans do.'}], 'importance_score': 0.25}, 'jaech2024openai': {'bib_key': 'jaech2024openai', 'bib_title': 'Openai o1 system card', 'bib_author ': 'Jaech, Aaron', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Recent advances in reasoning models~\\cite{jaech2024openai, guo2025deepseek}', 'next_context': 'have enabled the development of Deep Research systems\\citep{openai2025deepresearch,gemini_deep_research,huggingface_open_deep_research}that leverage a suite of advanced tools, including search APIs and web browsing, '}], 'importance_score': 0.5}, 'guo2025deepseek': {'bib_key': 'guo2025deepseek', 'bib_title': 'Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning', 'bib_author ': 'Guo, Daya', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Recent advances in reasoning models~\\cite{jaech2024openai, guo2025deepseek}', 'next_context': 'have enabled the development of Deep Research systems\\citep{openai2025deepresearch,gemini_deep_research,huggingface_open_deep_research}that leverage a suite of advanced tools, including search APIs and web browsing, '}], 'importance_score': 0.5}, 'huggingface_open_deep_research': {'bib_key': 'huggingface_open_deep_research', 'bib_title': 'Open Deep Research', 'bib_author ': '{Hugging Face}', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Recent advances in reasoning models~\\cite{jaech2024openai, guo2025deepseek}have enabled the development of Deep Research systems\\citep{openai2025deepresearch,gemini_deep_research,huggingface_open_deep_research}', 'next_context': 'that leverage a suite of advanced tools, including search APIs and web browsing, '}, {'section': 'Experiments', 'subsection': 'Experimental Setup', 'subsubsection': None, 'prev_context': 'Additionally, we systematically evaluate a suite of Deep Research systems~\\cite{huggingface_open_deep_research, perplexityai, grok3, gemini_deep_research, openai2025deepresearch}', 'next_context': ', which are explicitly optimized for extensive information gathering and comprehensive report generation, many of which can sustain continuous running for extended periods (e.g., beyond 30 minutes per query).'}, {'section': 'Experiments', 'subsection': 'Experimental Setup', 'subsubsection': None, 'prev_context': 'HF Open Deep Research~\\cite{huggingface_open_deep_research}', 'next_context': 'is the only open-source system that we find to yield reasonable results at the time of this evaluation; all the other sufficiently capable systems are closed-source.'}], 'importance_score': 1.5333333333333332}, 'he2024webvoyager': {'bib_key': 'he2024webvoyager', 'bib_title': 'WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models', 'bib_author ': 'He, Hongliang', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Most existing benchmarks for web agents focus on evaluating whether an agent can autonomously perform certain processes on a single website~\\cite{deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena,lu2024weblinx,he2024webvoyager,xue2025onlinemind2web,koh2024visualwebarena}', 'next_context': '.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': '~\\cite{zheng2023judging}has been widely used in evaluating complex tasks, including for web agents~\\cite{pan2024autonomous,he2024webvoyager,xue2025onlinemind2web}', 'next_context': '.'}, {'section': '', 'subsection': 'Rubric-based Judge Agent', 'subsubsection': None, 'prev_context': 'WebVoyager~\\cite{he2024webvoyager}', 'next_context': ''}], 'importance_score': 1.4761904761904763}, 'wu2025webwalker': {'bib_key': 'wu2025webwalker', 'bib_title': 'WebWalker: Benchmarking LLMs in Web Traversal', 'bib_author ': 'Wu, Jialong', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Several recent benchmarks have a stronger focus on search over the open web~\\cite{mialon2023gaia,yoran2024assistantbenchwebagentssolve,wu2025webwalker,song2025bearcubs,wei2025browsecomp}', 'next_context': '.'}], 'importance_score': 0.2}, 'pan2024autonomous': {'bib_key': 'pan2024autonomous', 'bib_title': 'Autonomous Evaluation and Refinement of Digital Agents', 'bib_author ': 'Pan, Jiayi', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': '~\\cite{zheng2023judging}has been widely used in evaluating complex tasks, including for web agents~\\cite{pan2024autonomous,he2024webvoyager,xue2025onlinemind2web}', 'next_context': '.'}], 'importance_score': 0.3333333333333333}, 'zhuge2024agent': {'bib_key': 'zhuge2024agent', 'bib_title': 'Agent-as-a-judge: Evaluate agents with agents', 'bib_author ': 'Zhuge, Mingchen', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'However, the complexity of agentic search is far beyond what a few LLM calls can evaluate, necessitating an Agent-as-a-Judge approach~\\cite{zhuge2024agent,starace2025paperbench}', 'next_context': '.'}], 'importance_score': 0.5}, 'starace2025paperbench': {'bib_key': 'starace2025paperbench', 'bib_title': "PaperBench: Evaluating AI's Ability to Replicate AI Research", 'bib_author ': 'Starace, Giulio', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'However, the complexity of agentic search is far beyond what a few LLM calls can evaluate, necessitating an Agent-as-a-Judge approach~\\cite{zhuge2024agent,starace2025paperbench}', 'next_context': '.'}, {'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'PaperBench~\\cite{starace2025paperbench}', 'next_context': '(a concurrent work) is most related to ours in that it also adopts a tree-structured rubric, though it is manually written by human experts and used to evaluate replication of AI research.'}], 'importance_score': 1.5}, 'yue2023automatic': {'bib_key': 'yue2023automatic', 'bib_title': 'Automatic Evaluation of Attribution by Large Language Models', 'bib_author ': 'Yue, Xiang', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Finally, our attribution evaluation is also related to the attribution literature~\\cite{yue2023automatic,gao2023enabling,li2024attributionbench,liu2023evaluating}', 'next_context': '.'}], 'importance_score': 0.25}, 'gao2023enabling': {'bib_key': 'gao2023enabling', 'bib_title': 'Enabling Large Language Models to Generate Text with Citations', 'bib_author ': 'Gao, Tianyu', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Finally, our attribution evaluation is also related to the attribution literature~\\cite{yue2023automatic,gao2023enabling,li2024attributionbench,liu2023evaluating}', 'next_context': '.'}], 'importance_score': 0.25}, 'li2024attributionbench': {'bib_key': 'li2024attributionbench', 'bib_title': 'AttributionBench: How Hard is Automatic Attribution Evaluation?', 'bib_author ': 'Li, Yifei', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Finally, our attribution evaluation is also related to the attribution literature~\\cite{yue2023automatic,gao2023enabling,li2024attributionbench,liu2023evaluating}', 'next_context': '.'}], 'importance_score': 0.25}, 'liu2023evaluating': {'bib_key': 'liu2023evaluating', 'bib_title': 'Evaluating Verifiability in Generative Search Engines', 'bib_author ': 'Liu, Nelson F', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Related Work', 'subsection': None, 'subsubsection': None, 'prev_context': 'Finally, our attribution evaluation is also related to the attribution literature~\\cite{yue2023automatic,gao2023enabling,li2024attributionbench,liu2023evaluating}', 'next_context': '.'}], 'importance_score': 0.25}, 'chen2023teaching': {'bib_key': 'chen2023teaching', 'bib_title': 'Teaching Large Language Models to Self-Debug', 'bib_author ': 'Xinyun Chen', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': '', 'subsection': 'Rubric-based Judge Agent', 'subsubsection': None, 'prev_context': 'The generated scripts undergo iterative autonomous refinements (including self-debug\\citep{chen2023teaching}', 'next_context': 'and self-reflection\\citep{shinn2023reflexion, madaan2023self}) to auto-correct minor or common errors.'}], 'importance_score': 1.0}, 'shinn2023reflexion': {'bib_key': 'shinn2023reflexion', 'bib_title': 'Reflexion: Language agents with verbal reinforcement learning', 'bib_author ': 'Shinn, Noah', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': '', 'subsection': 'Rubric-based Judge Agent', 'subsubsection': None, 'prev_context': 'The generated scripts undergo iterative autonomous refinements (including self-debug\\citep{chen2023teaching}and self-reflection\\citep{shinn2023reflexion, madaan2023self}', 'next_context': ') to auto-correct minor or common errors.'}], 'importance_score': 0.5}, 'madaan2023self': {'bib_key': 'madaan2023self', 'bib_title': 'Self-refine: Iterative refinement with self-feedback', 'bib_author ': 'Madaan, Aman', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': '', 'subsection': 'Rubric-based Judge Agent', 'subsubsection': None, 'prev_context': 'The generated scripts undergo iterative autonomous refinements (including self-debug\\citep{chen2023teaching}and self-reflection\\citep{shinn2023reflexion, madaan2023self}', 'next_context': ') to auto-correct minor or common errors.'}], 'importance_score': 0.5}, 'pan2024webcanvas': {'bib_key': 'pan2024webcanvas', 'bib_title': 'Webcanvas: Benchmarking web agents in online environments', 'bib_author ': 'Pan, Yichen', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': '', 'subsection': 'Rubric-based Judge Agent', 'subsubsection': None, 'prev_context': 'Mind2Web-Live~\\cite{pan2024webcanvas}', 'next_context': ''}, {'section': 'Details of Task Construction', 'subsection': 'Future Maintenance of the Benchmark', 'subsubsection': None, 'prev_context': 'Similar to previous benchmarks that rely on live web environments~\\citep{pan2024webcanvas,xue2025onlinemind2web}', 'next_context': ', tasks in\\mind~may be affected by changes or updates to websites over time.'}], 'importance_score': 1.5}, 'chatgptsearch': {'bib_key': 'chatgptsearch', 'bib_title': 'Introducing {ChatGPT} search', 'bib_author ': '{OpenAI}', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Experiments', 'subsection': 'Experimental Setup', 'subsubsection': None, 'prev_context': '\\citep{chatgptsearch}', 'next_context': 'and Perplexity Pro Search\\citep{perplexityai}, which augment LLMs with search capabilities, delivering rapid responses with a limited number of agentic search steps.'}], 'importance_score': 1.0}, 'perplexityai': {'bib_key': 'perplexityai', 'bib_title': 'Perplexity AI', 'bib_author ': '{Perplexity AI}', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Experiments', 'subsection': 'Experimental Setup', 'subsubsection': None, 'prev_context': '\\citep{chatgptsearch}and Perplexity Pro Search\\citep{perplexityai}', 'next_context': ', which augment LLMs with search capabilities, delivering rapid responses with a limited number of agentic search steps.'}, {'section': 'Experiments', 'subsection': 'Experimental Setup', 'subsubsection': None, 'prev_context': 'Additionally, we systematically evaluate a suite of Deep Research systems~\\cite{huggingface_open_deep_research, perplexityai, grok3, gemini_deep_research, openai2025deepresearch}', 'next_context': ', which are explicitly optimized for extensive information gathering and comprehensive report generation, many of which can sustain continuous running for extended periods (e.g., beyond 30 minutes per query).'}], 'importance_score': 1.2}, 'grok3': {'bib_key': 'grok3', 'bib_title': 'Grok 3 Beta — The Age of Reasoning Agents', 'bib_author ': '{xAI}', 'arxiv_id': None, 'short_id': None, 'title': None, 'author': None, 'published': None, 'similar_score': None, 'context': [{'section': 'Experiments', 'subsection': 'Experimental Setup', 'subsubsection': None, 'prev_context': 'Additionally, we systematically evaluate a suite of Deep Research systems~\\cite{huggingface_open_deep_research, perplexityai, grok3, gemini_deep_research, openai2025deepresearch}', 'next_context': ', which are explicitly optimized for extensive information gathering and comprehensive report generation, many of which can sustain continuous running for extended periods (e.g., beyond 30 minutes per query).'}], 'importance_score': 0.2}}, 'refs': [], 'table': [{'original': '\\begin{table}\n    \\centering\n    \\small\n    \\caption{Comparison with existing benchmarks for web browsing or search on live websites. \\textbf{Horizon}: the average number of required actions per task, grouped into Short ($<$ 10), Medium (10$-$50), Long ($>$ 50). \\textbf{Time-Varying}: whether the answer can change over time.}\n    {\n    \\begin{tabular}{lcccc}\n    \\toprule\n    &\n    \\textbf{Horizon} &\n    \\textbf{\\# of Tasks} &\n    \\textbf{Time-Varying} &\n    \\textbf{Evaluation} \\\\\n    \\midrule\n       Online-Mind2Web~\\cite{xue2025onlinemind2web} & Short &300  & \\ding{51}  & LLM-as-a-Judge  \\\\\n       WebVoyager~\\cite{he2024webvoyager} &Short   &643   & \\ding{51} & LLM-as-a-Judge \\\\\n       Mind2Web-Live~\\cite{pan2024webcanvas} & Short  & 542 & \\ding{51}  & Rule \\\\\n       BEARCUBS~\\cite{song2025bearcubs} & Short  & 111 & \\ding{55}  & Manual Evaluation \\\\\n       WebWalkerQA~\\cite{yoran2024assistantbenchwebagentssolve} & Short & 680  & \\ding{55} & Answer Match  \\\\\n       GAIA~\\cite{mialon2023gaia} & Medium & 466 & \\ding{55} & Answer Match \\\\\n       AssistantBench~\\cite{yoran2024assistantbenchwebagentssolve} & Medium & 214 & \\ding{55} & Answer Match  \\\\\n       \n       BrowseComp~\\cite{wei2025browsecomp} & Long & 1,266 & \\ding{55}  & Answer Match \\\\\n       \\midrule\n       \\mind & Long & \\ntasks & \\ding{51} & Agent-as-a-Judge \\\\\n        \\bottomrule\n    \\end{tabular}\n    }\n    \\label{tab:comparison}\n\\end{table}', 'caption': '\\caption{Comparison with existing benchmarks for web browsing or search on live websites. \\textbf{Horizon}: the average number of required actions per task, grouped into Short ($<$ 10), Medium (10$-$50), Long ($>$ 50). \\textbf{Time-Varying}: whether the answer can change over time.}', 'label': '\\label{tab:comparison}', 'tabular': '\\begin{tabular}{lcccc}\n    \\toprule\n    &\n    \\textbf{Horizon} &\n    \\textbf{\\# of Tasks} &\n    \\textbf{Time-Varying} &\n    \\textbf{Evaluation} \\\\\n    \\midrule\n       Online-Mind2Web~\\cite{xue2025onlinemind2web} & Short &300  & \\ding{51}  & LLM-as-a-Judge  \\\\\n       WebVoyager~\\cite{he2024webvoyager} &Short   &643   & \\ding{51} & LLM-as-a-Judge \\\\\n       Mind2Web-Live~\\cite{pan2024webcanvas} & Short  & 542 & \\ding{51}  & Rule \\\\\n       BEARCUBS~\\cite{song2025bearcubs} & Short  & 111 & \\ding{55}  & Manual Evaluation \\\\\n       WebWalkerQA~\\cite{yoran2024assistantbenchwebagentssolve} & Short & 680  & \\ding{55} & Answer Match  \\\\\n       GAIA~\\cite{mialon2023gaia} & Medium & 466 & \\ding{55} & Answer Match \\\\\n       AssistantBench~\\cite{yoran2024assistantbenchwebagentssolve} & Medium & 214 & \\ding{55} & Answer Match  \\\\\n       \n       BrowseComp~\\cite{wei2025browsecomp} & Long & 1,266 & \\ding{55}  & Answer Match \\\\\n       \\midrule\n       \\mind & Long & \\ntasks & \\ding{51} & Agent-as-a-Judge \\\\\n        \\bottomrule\n    \\end{tabular}', 'subtables': []}, {'original': '\\begin{wraptable}[18]{r}{0.4\\linewidth}\n\\centering\n\\small\n\\captionsetup{width=0.8\\linewidth}\n\\caption{Benchmark statistics.} \n\\label{tab:stats}\n\n\\caption*{(a) Rubric complexity. }\\par\n\\begin{tabular}{@{}lccc@{}}\n\\toprule\n & Avg & Min & Max \\\\\n\\midrule\n\\# Leaf nodes     & 34 & 3  & 357 \\\\\n\\# Total nodes    & 50 & 4  & 603 \\\\\nDepth      & 4  & 2  & 6   \\\\\n\\bottomrule\n\\end{tabular}\n\\vspace{0.6em}\n\n\\caption*{(b) Human effort required per task (Subset-\\nsub).}\\par\n\\begin{tabular}{@{}lccc@{}}\n\\toprule\n & Avg & Min &   Max\\\\\n\\midrule\nTime (min)     & 18 & {8} & {44} \\\\\n\\# Websites    & 8     & 3  & 31   \\\\\n\\# Webpages    & 110  & {38} & {375}   \\\\\n\\bottomrule\n\\end{tabular}\n\n\\vspace{1em}\n\n\\end{wraptable}', 'caption': '\\caption*', 'label': '\\label{tab:stats}', 'tabular': '\\begin{tabular}{@{}lccc@{}}\n\\toprule\n & Avg & Min &   Max\\\\\n\\midrule\nTime (min)     & 18 & {8} & {44} \\\\\n\\# Websites    & 8     & 3  & 31   \\\\\n\\# Webpages    & 110  & {38} & {375}   \\\\\n\\bottomrule\n\\end{tabular}', 'subtables': []}, {'original': '\\begin{table}\n  \\caption{Main evaluation results. We report the partial completion score, full-task success rate, Pass@3, average time (in minutes), average answer length (in words), and their standard deviation. *: To reduce human workload, the human study is conducted on \\sub as described in \\S\\ref{sec:statistics}.\n  }\n  \\label{tab:main}\n  \n  \\centering\n  \\small\n  \\resizebox{\\textwidth}{!}{\n  \\begin{tabular}{lccccc}\n    \\toprule\n             & \\textbf{Partial Completion} & \\textbf{Success Rate} & \\textbf{Pass@3} & \\textbf{Time (min)} & \\textbf{Answer Length} \\\\\n    \\midrule\n    \\rowcolor{mypink} ChatGPT Search            & 0.26$_{\\pm 0.01}$ & 0.06$_{\\pm 0.01}$ & 0.11 & $<1$  & 314$_{\\pm 4}$  \\\\\n    \\rowcolor{mypink} Perplexity Pro Search     & 0.28$_{\\pm 0.02}$ & 0.08$_{\\pm 0.01}$ & 0.12 & $<1$  & 408$_{\\pm 13}$  \\\\\n    \\midrule\n    \\rowcolor{myblue} OpenAI Operator           & 0.26$_{\\pm 0.01}$ & 0.10$_{\\pm 0.01}$ & 0.17 & 9.74$_{\\pm 0.21}$     & 160$_{\\pm 1}$  \\\\\n    \\midrule\n    \\rowcolor{myyellow} HF Open Deep Research     & 0.26$_{\\pm 0.01}$ & 0.11$_{\\pm 0.01}$ & 0.18 & 13.65$_{\\pm 0.07}$     & 209$_{\\pm 3}$  \\\\\n    \\rowcolor{myyellow} Grok DeepSearch           & 0.40$_{\\pm 0.04}$ & 0.18$_{\\pm 0.02}$ & 0.36 & 2.58$_{\\pm 0.14}$     & 1,428$_{\\pm 16}$ \\\\\n    \\rowcolor{myyellow} Perplexity Deep Research  & 0.42$_{\\pm 0.03}$ & 0.15$_{\\pm 0.03}$ & 0.26 & 5.67$_{\\pm 0.13}$     & 585$_{\\pm 13}$  \\\\\n    \\rowcolor{myyellow} Gemini Deep Research      & 0.45$_{\\pm 0.03}$ & 0.18$_{\\pm 0.02}$ & 0.30 & 7.38$_{\\pm 0.58}$      & 3,357$_{\\pm 49}$ \\\\\n    \\rowcolor{myyellow} Grok DeeperSearch         & \\underline{0.52}$_{\\pm 0.02}$ & \\underline{0.27}$_{\\pm 0.03}$ & \\textbf{0.40} & 5.72$_{\\pm 0.27}$     & 1,362$_{\\pm 24}$ \\\\\n    \\rowcolor{myyellow} OpenAI Deep Research      & \\textbf{0.54}$_{\\pm 0.04}$ & \\textbf{0.28}$_{\\pm 0.04}$ & \\textbf{0.40} & 8.40$_{\\pm 0.71}$    & 559$_{\\pm 19}$  \\\\\n    \\midrule\n    Human*                    & {0.79}$_{\\pm 0.01}$ & {0.54}$_{\\pm 0.07}$ & {0.83} & 18.40$_{\\pm 1.61}$    & 186$_{\\pm 27}$  \\\\\n    \\bottomrule\n\\end{tabular}\n\n\n  }\n\\end{table}', 'caption': '\\caption{Main evaluation results. We report the partial completion score, full-task success rate, Pass@3, average time (in minutes), average answer length (in words), and their standard deviation. *: To reduce human workload, the human study is conducted on \\sub as described in \\S\\ref{sec:statistics}.\n  }', 'label': '\\label{tab:main}', 'tabular': '\\begin{tabular}{lccccc}\n    \\toprule\n             & \\textbf{Partial Completion} & \\textbf{Success Rate} & \\textbf{Pass@3} & \\textbf{Time (min)} & \\textbf{Answer Length} \\\\\n    \\midrule\n    \\rowcolor{mypink} ChatGPT Search            & 0.26$_{\\pm 0.01}$ & 0.06$_{\\pm 0.01}$ & 0.11 & $<1$  & 314$_{\\pm 4}$  \\\\\n    \\rowcolor{mypink} Perplexity Pro Search     & 0.28$_{\\pm 0.02}$ & 0.08$_{\\pm 0.01}$ & 0.12 & $<1$  & 408$_{\\pm 13}$  \\\\\n    \\midrule\n    \\rowcolor{myblue} OpenAI Operator           & 0.26$_{\\pm 0.01}$ & 0.10$_{\\pm 0.01}$ & 0.17 & 9.74$_{\\pm 0.21}$     & 160$_{\\pm 1}$  \\\\\n    \\midrule\n    \\rowcolor{myyellow} HF Open Deep Research     & 0.26$_{\\pm 0.01}$ & 0.11$_{\\pm 0.01}$ & 0.18 & 13.65$_{\\pm 0.07}$     & 209$_{\\pm 3}$  \\\\\n    \\rowcolor{myyellow} Grok DeepSearch           & 0.40$_{\\pm 0.04}$ & 0.18$_{\\pm 0.02}$ & 0.36 & 2.58$_{\\pm 0.14}$     & 1,428$_{\\pm 16}$ \\\\\n    \\rowcolor{myyellow} Perplexity Deep Research  & 0.42$_{\\pm 0.03}$ & 0.15$_{\\pm 0.03}$ & 0.26 & 5.67$_{\\pm 0.13}$     & 585$_{\\pm 13}$  \\\\\n    \\rowcolor{myyellow} Gemini Deep Research      & 0.45$_{\\pm 0.03}$ & 0.18$_{\\pm 0.02}$ & 0.30 & 7.38$_{\\pm 0.58}$      & 3,357$_{\\pm 49}$ \\\\\n    \\rowcolor{myyellow} Grok DeeperSearch         & \\underline{0.52}$_{\\pm 0.02}$ & \\underline{0.27}$_{\\pm 0.03}$ & \\textbf{0.40} & 5.72$_{\\pm 0.27}$     & 1,362$_{\\pm 24}$ \\\\\n    \\rowcolor{myyellow} OpenAI Deep Research      & \\textbf{0.54}$_{\\pm 0.04}$ & \\textbf{0.28}$_{\\pm 0.04}$ & \\textbf{0.40} & 8.40$_{\\pm 0.71}$    & 559$_{\\pm 19}$  \\\\\n    \\midrule\n    Human*                    & {0.79}$_{\\pm 0.01}$ & {0.54}$_{\\pm 0.07}$ & {0.83} & 18.40$_{\\pm 1.61}$    & 186$_{\\pm 27}$  \\\\\n    \\bottomrule\n\\end{tabular}', 'subtables': []}], 'figure': [{'original': '\\begin{figure}[h]\n  \\centering\n  \\vspace{-2.5em}\n  \\includegraphics[width=0.72\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/m2w2_figure1_v8.pdf}\n  \\caption{\\mind features realistic and diverse long-horizon web search tasks and a novel Agent-as-a-Judge framework to evaluate complex, time-varying, and citation-backed answers.}\n  \\vspace{-.8em}\n  \\label{fig:teaser}\n\\end{figure}', 'caption': '\\caption{\\mind features realistic and diverse long-horizon web search tasks and a novel Agent-as-a-Judge framework to evaluate complex, time-varying, and citation-backed answers.}', 'label': '\\label{fig:teaser}', 'subfigures': [], 'figure_paths': ['figures/m2w2_figure1_v8.pdf']}, {'original': '\\begin{figure}[t]\n  \\centering\n  \\includegraphics[width=\\textwidth]{figures/m2w2_figure2_v2.pdf}\n  \\caption{Example of tree-structured rubrics. Top-down, task goals are decomposed into a tree structure; bottom-up, binary scores from leaf nodes are aggregated into the overall task score. The leaf nodes are verification of low-level criteria, implemented by various functions of judge agents (e.g., \\texttt{simple\\_verify}: verify a simple factual or logical statement; \\texttt{verify\\_by\\_url}: verify whether a statement in the answer is backed by a cited webpage). See more discussion in \\S\\ref{sec:rubric_tree} and \\S\\ref{sec:judge_agent}.}\n  \\label{fig:rubric}\n\\end{figure}', 'caption': '\\caption{Example of tree-structured rubrics. Top-down, task goals are decomposed into a tree structure; bottom-up, binary scores from leaf nodes are aggregated into the overall task score. The leaf nodes are verification of low-level criteria, implemented by various functions of judge agents (e.g., \\texttt{simple\\_verify}: verify a simple factual or logical statement; \\texttt{verify\\_by\\_url}: verify whether a statement in the answer is backed by a cited webpage). See more discussion in \\S\\ref{sec:rubric_tree} and \\S\\ref{sec:judge_agent}.}', 'label': '\\label{fig:rubric}', 'subfigures': [], 'figure_paths': ['figures/m2w2_figure2_v2.pdf']}, {'original': '\\begin{figure}[h]\n  \\centering\n  \\begin{minipage}{0.48\\textwidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/m2w2_partial_vs_time_error_0626.pdf}\n    \\captionof{figure}{Average Partial Completion against average task completion time.}\n    \\label{fig:inference_time}\n  \\end{minipage}\n  \\hfill\n  \\begin{minipage}{0.48\\textwidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/mean_avg_explicit_vs_other_0626.pdf}\n    \\captionof{figure}{Average Partial Completion on explicitly time-varying tasks compared to all other tasks.}\n    \\label{fig:time_varying}\n  \\end{minipage}\n\\end{figure}', 'caption': '', 'label': '\\label{fig:time_varying}', 'subfigures': [], 'figure_paths': ['figures/m2w2_partial_vs_time_error_0626.pdf', 'figures/mean_avg_explicit_vs_other_0626.pdf']}, {'original': '\\begin{minipage}{0.48\\textwidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/m2w2_partial_vs_time_error_0626.pdf}\n    \\captionof{figure}{Average Partial Completion against average task completion time.}\n    \\label{fig:inference_time}\n  \\end{minipage}', 'caption': '', 'label': '\\label{fig:inference_time}', 'subfigures': [], 'figure_paths': ['figures/m2w2_partial_vs_time_error_0626.pdf']}, {'original': '\\begin{minipage}{0.48\\textwidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/mean_avg_explicit_vs_other_0626.pdf}\n    \\captionof{figure}{Average Partial Completion on explicitly time-varying tasks compared to all other tasks.}\n    \\label{fig:time_varying}\n  \\end{minipage}', 'caption': '', 'label': '\\label{fig:time_varying}', 'subfigures': [], 'figure_paths': ['figures/mean_avg_explicit_vs_other_0626.pdf']}, {'original': '\\begin{figure}\n  \\centering\n  \\includegraphics[width=\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_analysis_distribution_alt_with_human.pdf}\n  \\caption{Errors across agents and humans.\n  The bars indicate the percentage of tasks exhibiting each error type.\n  We include results from five agentic search systems and humans.}\n  \\label{fig:error_analysis}\n\\end{figure}', 'caption': '\\caption{Errors across agents and humans.\n  The bars indicate the percentage of tasks exhibiting each error type.\n  We include results from five agentic search systems and humans.}', 'label': '\\label{fig:error_analysis}', 'subfigures': [], 'figure_paths': ['figures/error_analysis_distribution_alt_with_human.pdf']}, {'original': '\\begin{figure}[h]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/m2w2_domain_plot_v2.pdf}\n  \\caption{\\mind contains \\ntasks diverse tasks covering 6 broad domains and 24 sub-domains.}\n  \\label{fig:domain}\n\\end{figure}', 'caption': '\\caption{\\mind contains \\ntasks diverse tasks covering 6 broad domains and 24 sub-domains.}', 'label': '\\label{fig:domain}', 'subfigures': [], 'figure_paths': ['figures/m2w2_domain_plot_v2.pdf']}, {'original': '\\begin{figure}[h]\n  \\centering\n  \\includegraphics[width=0.9\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_analysis_workflow.pdf}\n  \\caption{Workflow of categorizing errors in error analysis.}\n  \\label{fig:error_workflow}\n\\end{figure}', 'caption': '\\caption{Workflow of categorizing errors in error analysis.}', 'label': '\\label{fig:error_workflow}', 'subfigures': [], 'figure_paths': ['figures/error_analysis_workflow.pdf']}, {'original': '\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_case_not_found.jpg}\n  \\caption{An example of \\textit{Information Not Found}, where Perplexity Pro Search explicitly states that it cannot retrieve the requested information, thus failing to fully address the task.}\n  \\label{fig:error_case_not_found}\n\\end{figure}', 'caption': '\\caption{An example of \\textit{Information Not Found}, where Perplexity Pro Search explicitly states that it cannot retrieve the requested information, thus failing to fully address the task.}', 'label': '\\label{fig:error_case_not_found}', 'subfigures': [], 'figure_paths': ['figures/error_case_not_found.jpg']}, {'original': "\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_case_partial_missing.jpg}\n  \\caption{An example of \\textit{Partial Missing}, where ChatGPT Search provides the Nobel Prize winners' information only for a subset of the requested years (2004–2014), failing to fully complete the task (2004–2024).}\n  \\label{fig:error_case_partial_missing}\n\\end{figure}", 'caption': "\\caption{An example of \\textit{Partial Missing}, where ChatGPT Search provides the Nobel Prize winners' information only for a subset of the requested years (2004–2014), failing to fully complete the task (2004–2024).}", 'label': '\\label{fig:error_case_partial_missing}', 'subfigures': [], 'figure_paths': ['figures/error_case_partial_missing.jpg']}, {'original': '\\begin{figure}[H]\n  \\centering\n\\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_case_cv.jpg}\n  \\caption{An example of \\textit{Criteria Violation}, where OpenAI Operator explicitly violates the specified budget constraint (\\$200–\\$600) by providing a shopping list totaling \\$1,277.97.}\n  \\label{fig:error_case_criteria_violation}\n\\end{figure}', 'caption': '\\caption{An example of \\textit{Criteria Violation}, where OpenAI Operator explicitly violates the specified budget constraint (\\$200–\\$600) by providing a shopping list totaling \\$1,277.97.}', 'label': '\\label{fig:error_case_criteria_violation}', 'subfigures': [], 'figure_paths': ['figures/error_case_cv.jpg']}, {'original': '\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_hallucination_invalid_attr.jpg}\n  \\caption{An example of \\textit{Invalid Attribution}, where OpenAI Operator fabricates three links that mimic the URL patterns of Federal Reserve official speech pages and Reuters articles, resulting in an entirely hallucinated response.}\n  \\label{fig:error_case_invalid_attr}\n\\end{figure}', 'caption': '\\caption{An example of \\textit{Invalid Attribution}, where OpenAI Operator fabricates three links that mimic the URL patterns of Federal Reserve official speech pages and Reuters articles, resulting in an entirely hallucinated response.}', 'label': '\\label{fig:error_case_invalid_attr}', 'subfigures': [], 'figure_paths': ['figures/case_hallucination_invalid_attr.jpg']}, {'original': '\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_case_missing_attr.jpg}\n  \\caption{An example of \\textit{Missing Attribution}, where OpenAI Operator provides birthplace details for Nobel Prize winners without supplying URLs or sources to support these claims.}\n  \\label{fig:error_case_missing_attr}\n\\end{figure}', 'caption': '\\caption{An example of \\textit{Missing Attribution}, where OpenAI Operator provides birthplace details for Nobel Prize winners without supplying URLs or sources to support these claims.}', 'label': '\\label{fig:error_case_missing_attr}', 'subfigures': [], 'figure_paths': ['figures/error_case_missing_attr.jpg']}, {'original': "\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_case_retrieval.jpg}\n  \\caption{An example of \\textit{Retrieval Error}, where the provided URL from ChatGPT Search contains irrelevant information and cannot support the claims about characters' abilities in the answer.}\n  \\label{fig:error_case_retrieval_err}\n\\end{figure}", 'caption': "\\caption{An example of \\textit{Retrieval Error}, where the provided URL from ChatGPT Search contains irrelevant information and cannot support the claims about characters' abilities in the answer.}", 'label': '\\label{fig:error_case_retrieval_err}', 'subfigures': [], 'figure_paths': ['figures/error_case_retrieval.jpg']}, {'original': '\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_hallucination_synthesis.jpg}\n  \\caption{An example of \\textit{Synthesis Error}, where inaccurate details in answers provided by Perplexity Pro Search ultimately lead to incorrect responses.}\n  \\label{fig:error_case_synthesis_err}\n\\end{figure}', 'caption': '\\caption{An example of \\textit{Synthesis Error}, where inaccurate details in answers provided by Perplexity Pro Search ultimately lead to incorrect responses.}', 'label': '\\label{fig:error_case_synthesis_err}', 'subfigures': [], 'figure_paths': ['figures/case_hallucination_synthesis.jpg']}, {'original': '\\begin{figure}\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_human_cri_vio.jpg}\n  \\caption{A case of a human annotator making a \\textit{Criteria Violation} by carelessly categorizing the University of Waterloo as a U.S. university.}\n  \\label{fig:error_case_human_cri_vio}\n\\end{figure}', 'caption': '\\caption{A case of a human annotator making a \\textit{Criteria Violation} by carelessly categorizing the University of Waterloo as a U.S. university.}', 'label': '\\label{fig:error_case_human_cri_vio}', 'subfigures': [], 'figure_paths': ['figures/case_human_cri_vio.jpg']}, {'original': '\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_human_synthesis.jpg}\n  \\caption{A case of a human annotator making a \\textit{Synthesis Error} by carelessly misspelling the name of the most recent Pritzker Prize winner.}\n  \\label{fig:error_case_human_synthesis}\n\\end{figure}', 'caption': '\\caption{A case of a human annotator making a \\textit{Synthesis Error} by carelessly misspelling the name of the most recent Pritzker Prize winner.}', 'label': '\\label{fig:error_case_human_synthesis}', 'subfigures': [], 'figure_paths': ['figures/case_human_synthesis.jpg']}, {'original': '\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_hf_odr_not_found.jpg}\n  \\caption{A case of HF Open Deep Research committing an \\textit{Information Not Found} error due to a system failure to properly follow the system prompt to invoke the search tool.}\n  \\label{fig:error_case_hf_odr_not_found}\n\\end{figure}', 'caption': '\\caption{A case of HF Open Deep Research committing an \\textit{Information Not Found} error due to a system failure to properly follow the system prompt to invoke the search tool.}', 'label': '\\label{fig:error_case_hf_odr_not_found}', 'subfigures': [], 'figure_paths': ['figures/case_hf_odr_not_found.jpg']}, {'original': '\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_operator_invalid_attr.jpg}\n  \\caption{A case where the OpenAI Operator reports an invalid attribution, differing by a few words from the actual website URL, despite having visited the correct source.}\n  \\label{fig:error_case_operator_invalid_attr}\n\\end{figure}', 'caption': '\\caption{A case where the OpenAI Operator reports an invalid attribution, differing by a few words from the actual website URL, despite having visited the correct source.}', 'label': '\\label{fig:error_case_operator_invalid_attr}', 'subfigures': [], 'figure_paths': ['figures/case_operator_invalid_attr.jpg']}, {'original': '\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_oai_dr_missing_attr.jpg}\n  \\caption{A case where OpenAI Deep Research presents claims without verifiable attribution, likely resulting from direct generation rather than conducting a real-time search.}\n  \\label{fig:error_case_oai_dr_missing_attr}\n\\end{figure}', 'caption': '\\caption{A case where OpenAI Deep Research presents claims without verifiable attribution, likely resulting from direct generation rather than conducting a real-time search.}', 'label': '\\label{fig:error_case_oai_dr_missing_attr}', 'subfigures': [], 'figure_paths': ['figures/case_oai_dr_missing_attr.jpg']}, {'original': '\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_perplexity_not_found.jpg}\n  \\caption{A case where Perplexity Pro Search fails to find the required information within limited search steps, despite the known ground-truth answer being available and discoverable.}\n  \\label{fig:error_case_perplexity_not_found}\n\\end{figure}', 'caption': '\\caption{A case where Perplexity Pro Search fails to find the required information within limited search steps, despite the known ground-truth answer being available and discoverable.}', 'label': '\\label{fig:error_case_perplexity_not_found}', 'subfigures': [], 'figure_paths': ['figures/case_perplexity_not_found.jpg']}, {'original': '\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_chatgpt_search_synthesis_err.jpg}\n  \\caption{A case where ChatGPT Search retrieves several relevant webpages but fails to synthesize a correct answer with accurate attribution in a task requiring extensive information across 20 years.}\n  \\label{fig:error_case_chatgpt_search_synthesis_err}\n\\end{figure}', 'caption': '\\caption{A case where ChatGPT Search retrieves several relevant webpages but fails to synthesize a correct answer with accurate attribution in a task requiring extensive information across 20 years.}', 'label': '\\label{fig:error_case_chatgpt_search_synthesis_err}', 'subfigures': [], 'figure_paths': ['figures/case_chatgpt_search_synthesis_err.jpg']}, {'original': '\\begin{figure}[h]\n  \\centering\n  \\includegraphics[width=0.95\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/m2w2_annotation_tool.png}\n  \\caption{A screenshot of the GUI tool for visualizing agent answers, pre-cached webpages, rubrics, and judge-agent evaluation outcomes.}\n  \\label{fig:annotation_tool}\n\\end{figure}', 'caption': '\\caption{A screenshot of the GUI tool for visualizing agent answers, pre-cached webpages, rubrics, and judge-agent evaluation outcomes.}', 'label': '\\label{fig:annotation_tool}', 'subfigures': [], 'figure_paths': ['figures/m2w2_annotation_tool.png']}], 'equations': [], 'algorithm': [], 'sections': {'Introduction': {'content': '\n\nWeb search has long been the gateway to the world’s knowledge, underpinning everything from everyday fact‑checking to frontier scientific discovery. \nThe core techniques supporting web search have undergone constant evolution in the past decades, from TF-IDF~\\cite{salton1975vector} for term statistics to PageRank~\\cite{brin1998anatomy} for network analysis and learning to rank~\\cite{liu2009learning,burges2005learning} for supervised learning. \nYet the core interaction model has remained essentially unchanged: users issue a query, receive a ranked list of URLs, and must manually open, read, and synthesize multiple webpages to answer complex questions.\nCurrent web search is inherently \\textit{user-driven}: it retrieves pieces of information but relies on users to interpret and assemble those pieces.\nThat places a significant cognitive load on users, especially as the complexity of the digital world grows.\n\nRecent advances in large language models (LLMs) have sparked the development of \\textit{agentic search} systems. \nRather than taking keyword queries and returning lists of links, agentic search systems decompose and plan for complex queries, iteratively search the web and interact with dynamic websites, and synthesize information into a citation-backed response. \nIn recent years, agentic search has quickly progressed from \\textit{search-augmented LLMs} (e.g., ChatGPT and Perplexity Search) to LLM-based \\textit{autonomous web agents}~\\cite{webgpt,deng2023mind2web,zhou2024webarena,zheng2024seeact,claude_computer_use,openai2025operator} and recent \\textit{Deep Research} systems~\\cite{gemini_deep_research,openai2025deepresearch} specifically optimized for long-horizon browsing and search behavior. \nBy off‑loading many low-level tasks, such as query decomposition and reformulation, web browsing, and basic analytics, to a tireless AI agent, agentic search promises to empower human users to focus their cognitive capacity on more important matters like oversight and critical decisions, improving both search efficiency and quality.\n\nHowever, the rapidly growing complexity of agentic search systems and their tasks is leading to an \\textit{evaluation crisis}: how to evaluate the result of a long-horizon task that an AI agent or human produces after taking possibly an hour and hundreds of actions across dozens of websites?\nMeanwhile, automated and reliable evaluation has proven crucial for the iterative development of AI technologies, especially in the early stages~\\cite{hendrycksmeasuring,chiang2024chatbot,yue2024mmmu}. \nFor agentic search, evaluation is also critical for establishing its \\textit{trustworthiness}––while traditional search requires the user to read original documents and verify information, an agent that synthesizes answers must be relied on to be correct and unbiased.\nAutomated evaluation serves as the first line of defense to detect whether an agent is just hallucinating plausible-sounding answers or the cited sources verifiably back them.\n\nExisting benchmarks and evaluation methodologies struggle to keep up with the growing complexity of agentic search. \nMany benchmarks have been proposed for autonomous web agents~\\cite{deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena,lu2024weblinx,xue2025onlinemind2web} but they primarily focus on tasks of a moderate horizon (e.g., up to 10 actions) that can be completed on a single website.\nSeveral benchmarks cover cross-website search tasks~\\cite{mialon2023gaia,yoran2024assistantbenchwebagentssolve,song2025bearcubs}, including most recently BrowseComp~\\cite{wei2025browsecomp} from OpenAI.\nHowever, to facilitate automated evaluation, a common compromise was made: they focus on tasks with \\textit{predefined, time-invariant answers}, oftentimes just a single answer string. \nWhile these benchmarks still provide valuable signals for evaluating certain aspects of agentic search systems, they are far from the full spectrum of tasks that current and future systems are facing.\nConsider an everyday task already within reach of current Deep Research systems, shown in~\\autoref{fig:teaser}.\nIt does not have a predefined answer but requires interacting with live websites to get real-time information.\nA corresponding agent trajectory may span dozens to hundreds of actions on the IKEA website, let alone more complex tasks that span many websites.\nWe need new evaluation methodologies and benchmarks for such \\textit{long-horizon, time-varying} tasks. \n\nIn response to these challenges, we propose \\mind, a new benchmark designed to rigorously evaluate agentic search systems on realistic and long-horizon tasks involving real-time web search and browsing. \nIt consists of \\ntasks high-quality tasks across diverse practical domains.\nEach task has undergone multiple stages and hours of expert labor for polishing and validation to ensure its realism, complexity, and verifiability. \nApproximately, at least 1,000 hours of human labor is spent to construct the benchmark, including the tasks and their evaluation scripts.\n\nAgentic search systems typically produce long, time-varying answers (e.g., the product catalog of a shopping website constantly changes) ranging from hundreds to thousands of words on these tasks. \nThe complexity is far beyond what conventional LLM-as-a-Judge~\\cite{zheng2023judging} methods are used for.\nTherefore, we propose a novel \\textit{Agent-as-a-Judge} framework to automatically yet reliably evaluate such complex answers.\nThe key insight behind our evaluation methodology lies in the \\textit{generation-verification asymmetry}: while the generated answers can vary substantially across agents, search strategies, or query times, we know \\textit{a priori} what each task is looking for and can design a \\textit{task-specific rubric} to specify the evaluation logic. At a high level, a rubric evaluates two main aspects of an answer: \\textit{correctness} (i.e., whether the answer satisfies all the requirements of the task) and \\textit{attribution} (i.e., whether every statement in the answer can be attributed to the cited sources).\nAt the operational level,\na rubric is structured as a tree that breaks down the evaluation into hierarchical evaluation nodes, where each leaf node conforms to a binary judgment and the internal nodes aggregate and propagate the results toward the root following various aggregation logic. Given a task, we develop a \\textit{task-specific judge agent}, an agentic workflow interleaving LLM-based information extraction, LLM-as-a-Judge, and tool calls following our unified rubric design, to automatically evaluate complex answers from agentic search systems (see \\autoref{fig:teaser} for illustration). \nDue to the complexity of our tasks, the rubric trees are also highly complex, with an average of 50 nodes and a max of 603 nodes (\\autoref{tab:stats} (a)).\nYet, rigorous human evaluation of our judge agents shows a 99\\% correctness rate, demonstrating their exceptional reliability (\\S\\ref{app:human_agreement}).\n\n\n\n\nWe evaluate nine frontier agentic search systems on \\mind and also compare them with human performance. \nOverall, the results show a clear advantage of Deep Research systems over search-augmented LLMs and web agents like Operator, owing to their ability in effectively leveraging advanced tools and staying focused over a long horizon. \nOur results also reveal that current systems still struggle with time-varying tasks that require real-time information and highlight the need for agentic search systems to integrate the ability of interacting with live websites.\nFinally, even though current systems still underperform humans, the best-performing system, OpenAI Deep Research, can already achieve 50-70\\% of human performance while spending half the time. \nIt also outperforms humans on some tasks that require a great attention to details and exhaustiveness in the search.\nAfter all, humans are subject to cognitive fatigue and a limited working memory.\nAgentic search presents a substantial potential in augmenting human cognition by automating away the legwork and allowing us to focus our limited cognitive capacity on things that matter more, such as critical decisions and oversight.\n\n\n\n\n\n', 'appendix': False}, 'Related Work': {'content': "\n\\label{sec:related}\n\n\\begin{remark} [Agentic Search.]\n\nWe define \\textit{agentic search} as systems that iteratively and autonomously tackle complex search tasks using a combination of tools (e.g., search APIs, retrievers, or web browsing). \nThe autonomy is typically powered by LLMs that decompose the initial search task, dynamically reason and plan based on the accumulating information, or interact with live websites.\nEarly systems like MindSearch~\\cite{chen2024mindsearch}, ChatGPT and Perplexity Search augment LLMs with search APIs to iteratively search for up-to-date information.\nHowever, solely relying on conventional web search inherits its limitations.\nFor example, many websites dynamically render information not indexed by search engines based on user interaction. \nAutonomous web agents~\\cite{webgpt,deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena}, especially those with visual perception of the web~\\cite{zheng2024seeact,koh2024visualwebarena,gou2025uground,qin2025ui}, have emerged to browse the real-time web as humans do. \nOpenAI's Operator~\\cite{openai2025operator}, with specialized reinforcement learning training, represents the current frontier~\\cite{xue2025onlinemind2web}.\nRecent advances in reasoning models~\\cite{jaech2024openai, guo2025deepseek} have enabled the development of Deep Research systems \\citep{openai2025deepresearch,gemini_deep_research,huggingface_open_deep_research} that leverage a suite of advanced tools, including search APIs and web browsing, \nto conduct substantially longer-horizon and deeper research on complex topics.  \nHowever, there is yet a benchmark designed to simultaneously evaluate this broad spectrum of agentic search systems, a gap that our work aims to bridge.\n\n\\end{remark}\\begin{remark} [Agentic Search.]\n\nWe define \\textit{agentic search} as systems that iteratively and autonomously tackle complex search tasks using a combination of tools (e.g., search APIs, retrievers, or web browsing). \nThe autonomy is typically powered by LLMs that decompose the initial search task, dynamically reason and plan based on the accumulating information, or interact with live websites.\nEarly systems like MindSearch~\\cite{chen2024mindsearch}, ChatGPT and Perplexity Search augment LLMs with search APIs to iteratively search for up-to-date information.\nHowever, solely relying on conventional web search inherits its limitations.\nFor example, many websites dynamically render information not indexed by search engines based on user interaction. \nAutonomous web agents~\\cite{webgpt,deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena}, especially those with visual perception of the web~\\cite{zheng2024seeact,koh2024visualwebarena,gou2025uground,qin2025ui}, have emerged to browse the real-time web as humans do. \nOpenAI's Operator~\\cite{openai2025operator}, with specialized reinforcement learning training, represents the current frontier~\\cite{xue2025onlinemind2web}.\nRecent advances in reasoning models~\\cite{jaech2024openai, guo2025deepseek} have enabled the development of Deep Research systems \\citep{openai2025deepresearch,gemini_deep_research,huggingface_open_deep_research} that leverage a suite of advanced tools, including search APIs and web browsing, \nto conduct substantially longer-horizon and deeper research on complex topics.  \nHowever, there is yet a benchmark designed to simultaneously evaluate this broad spectrum of agentic search systems, a gap that our work aims to bridge.\n\n\\end{remark}\n\nWe define \\textit{agentic search} as systems that iteratively and autonomously tackle complex search tasks using a combination of tools (e.g., search APIs, retrievers, or web browsing). \nThe autonomy is typically powered by LLMs that decompose the initial search task, dynamically reason and plan based on the accumulating information, or interact with live websites.\nEarly systems like MindSearch~\\cite{chen2024mindsearch}, ChatGPT and Perplexity Search augment LLMs with search APIs to iteratively search for up-to-date information.\nHowever, solely relying on conventional web search inherits its limitations.\nFor example, many websites dynamically render information not indexed by search engines based on user interaction. \nAutonomous web agents~\\cite{webgpt,deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena}, especially those with visual perception of the web~\\cite{zheng2024seeact,koh2024visualwebarena,gou2025uground,qin2025ui}, have emerged to browse the real-time web as humans do. \nOpenAI's Operator~\\cite{openai2025operator}, with specialized reinforcement learning training, represents the current frontier~\\cite{xue2025onlinemind2web}.\nRecent advances in reasoning models~\\cite{jaech2024openai, guo2025deepseek} have enabled the development of Deep Research systems \\citep{openai2025deepresearch,gemini_deep_research,huggingface_open_deep_research} that leverage a suite of advanced tools, including search APIs and web browsing, \nto conduct substantially longer-horizon and deeper research on complex topics.  \nHowever, there is yet a benchmark designed to simultaneously evaluate this broad spectrum of agentic search systems, a gap that our work aims to bridge.\n\n\n\n\n\\begin{remark} [Benchmarks and Evaluation Methodologies.]\n\nMost existing benchmarks for web agents focus on evaluating whether an agent can autonomously perform certain processes on a single website~\\cite{deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena,lu2024weblinx,he2024webvoyager,xue2025onlinemind2web,koh2024visualwebarena}.\nThe tasks tend to be short (e.g., less than 10 actions) and transactional (e.g., purchasing a flight ticket).\nTherefore, they can be useful for evaluating the web browsing aspect of agentic search but not the whole systems.\nSeveral recent benchmarks have a stronger focus on search over the open web~\\cite{mialon2023gaia,yoran2024assistantbenchwebagentssolve,wu2025webwalker,song2025bearcubs,wei2025browsecomp}.\nHowever, for the feasibility of automated evaluation, these benchmarks have made a common compromise: they limit the benchmark to tasks with \\textit{predefined, time-invariant answers}, oftentimes just a single answer string. \nThe BrowseComp benchmark~\\cite{wei2025browsecomp} from OpenAI, a concurrent work to ours, is representative of this evaluation methodology. \nSimilar to ours, it also leverages the generation-verification asymmetry. \nIt specifically targets tasks that are \\textit{hard to solve but easy to verify} (e.g., the answer is often a unique unambiguous string but may require combing through hundreds of webpages to find it). \nThis strategy is adopted to sidestep the challenge of automatically evaluating complex, time-varying answers, but at the cost of systematically deviating from the true user query distribution.\nIn contrast, we take this challenge head-on with a novel Agent-as-a-Judge methodology. \nThat allows our benchmark to include more realistic and complex tasks that expect a comprehensive answer with real-time information.\n\nLLM-as-a-Judge~\\cite{zheng2023judging} has been widely used in evaluating complex tasks, including for web agents~\\cite{pan2024autonomous,he2024webvoyager,xue2025onlinemind2web}. \nHowever, the complexity of agentic search is far beyond what a few LLM calls can evaluate, necessitating an Agent-as-a-Judge approach~\\cite{zhuge2024agent,starace2025paperbench}.\nPaperBench~\\cite{starace2025paperbench} (a concurrent work) is most related to ours in that it also adopts a tree-structured rubric, though it is manually written by human experts and used to evaluate replication of AI research.\nOur work goes further by largely automating the generation of rubrics.\nWe also have more sophisticated score aggregation methods beyond simple weighted averaging due to the diversity of our tasks.\nFinally, our attribution evaluation is also related to the attribution literature~\\cite{yue2023automatic,gao2023enabling,li2024attributionbench,liu2023evaluating}.\n\n\n\\end{remark}\\begin{remark} [Benchmarks and Evaluation Methodologies.]\n\nMost existing benchmarks for web agents focus on evaluating whether an agent can autonomously perform certain processes on a single website~\\cite{deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena,lu2024weblinx,he2024webvoyager,xue2025onlinemind2web,koh2024visualwebarena}.\nThe tasks tend to be short (e.g., less than 10 actions) and transactional (e.g., purchasing a flight ticket).\nTherefore, they can be useful for evaluating the web browsing aspect of agentic search but not the whole systems.\nSeveral recent benchmarks have a stronger focus on search over the open web~\\cite{mialon2023gaia,yoran2024assistantbenchwebagentssolve,wu2025webwalker,song2025bearcubs,wei2025browsecomp}.\nHowever, for the feasibility of automated evaluation, these benchmarks have made a common compromise: they limit the benchmark to tasks with \\textit{predefined, time-invariant answers}, oftentimes just a single answer string. \nThe BrowseComp benchmark~\\cite{wei2025browsecomp} from OpenAI, a concurrent work to ours, is representative of this evaluation methodology. \nSimilar to ours, it also leverages the generation-verification asymmetry. \nIt specifically targets tasks that are \\textit{hard to solve but easy to verify} (e.g., the answer is often a unique unambiguous string but may require combing through hundreds of webpages to find it). \nThis strategy is adopted to sidestep the challenge of automatically evaluating complex, time-varying answers, but at the cost of systematically deviating from the true user query distribution.\nIn contrast, we take this challenge head-on with a novel Agent-as-a-Judge methodology. \nThat allows our benchmark to include more realistic and complex tasks that expect a comprehensive answer with real-time information.\n\nLLM-as-a-Judge~\\cite{zheng2023judging} has been widely used in evaluating complex tasks, including for web agents~\\cite{pan2024autonomous,he2024webvoyager,xue2025onlinemind2web}. \nHowever, the complexity of agentic search is far beyond what a few LLM calls can evaluate, necessitating an Agent-as-a-Judge approach~\\cite{zhuge2024agent,starace2025paperbench}.\nPaperBench~\\cite{starace2025paperbench} (a concurrent work) is most related to ours in that it also adopts a tree-structured rubric, though it is manually written by human experts and used to evaluate replication of AI research.\nOur work goes further by largely automating the generation of rubrics.\nWe also have more sophisticated score aggregation methods beyond simple weighted averaging due to the diversity of our tasks.\nFinally, our attribution evaluation is also related to the attribution literature~\\cite{yue2023automatic,gao2023enabling,li2024attributionbench,liu2023evaluating}.\n\n\n\\end{remark}\n\nMost existing benchmarks for web agents focus on evaluating whether an agent can autonomously perform certain processes on a single website~\\cite{deng2023mind2web,yaoWebShopScalableRealWorld2022,zhou2024webarena,lu2024weblinx,he2024webvoyager,xue2025onlinemind2web,koh2024visualwebarena}.\nThe tasks tend to be short (e.g., less than 10 actions) and transactional (e.g., purchasing a flight ticket).\nTherefore, they can be useful for evaluating the web browsing aspect of agentic search but not the whole systems.\nSeveral recent benchmarks have a stronger focus on search over the open web~\\cite{mialon2023gaia,yoran2024assistantbenchwebagentssolve,wu2025webwalker,song2025bearcubs,wei2025browsecomp}.\nHowever, for the feasibility of automated evaluation, these benchmarks have made a common compromise: they limit the benchmark to tasks with \\textit{predefined, time-invariant answers}, oftentimes just a single answer string. \nThe BrowseComp benchmark~\\cite{wei2025browsecomp} from OpenAI, a concurrent work to ours, is representative of this evaluation methodology. \nSimilar to ours, it also leverages the generation-verification asymmetry. \nIt specifically targets tasks that are \\textit{hard to solve but easy to verify} (e.g., the answer is often a unique unambiguous string but may require combing through hundreds of webpages to find it). \nThis strategy is adopted to sidestep the challenge of automatically evaluating complex, time-varying answers, but at the cost of systematically deviating from the true user query distribution.\nIn contrast, we take this challenge head-on with a novel Agent-as-a-Judge methodology. \nThat allows our benchmark to include more realistic and complex tasks that expect a comprehensive answer with real-time information.\n\nLLM-as-a-Judge~\\cite{zheng2023judging} has been widely used in evaluating complex tasks, including for web agents~\\cite{pan2024autonomous,he2024webvoyager,xue2025onlinemind2web}. \nHowever, the complexity of agentic search is far beyond what a few LLM calls can evaluate, necessitating an Agent-as-a-Judge approach~\\cite{zhuge2024agent,starace2025paperbench}.\nPaperBench~\\cite{starace2025paperbench} (a concurrent work) is most related to ours in that it also adopts a tree-structured rubric, though it is manually written by human experts and used to evaluate replication of AI research.\nOur work goes further by largely automating the generation of rubrics.\nWe also have more sophisticated score aggregation methods beyond simple weighted averaging due to the diversity of our tasks.\nFinally, our attribution evaluation is also related to the attribution literature~\\cite{yue2023automatic,gao2023enabling,li2024attributionbench,liu2023evaluating}.\n\n\n\n\n\n", 'appendix': False}, 'Experiments': {'content': "\n\\label{sec:exp}\n\n\n\\subsection{Experimental Setup}\n\nWe evaluate agentic search systems of various types on \\mind. Given the complexity of our tasks, we focus on frontier systems capable of yielding meaningful results, namely, those exhibit sufficient long-horizon search capability and can consistently provide source attributions. We report two primary metrics: \\textbf{Partial Completion} and \\textbf{Success Rate}, as defined in \\S\\ref{sec:rubric_tree}. We run and evaluate each system independently over three runs per task, and we present the averaged metrics along with their standard deviations. Additionally, we introduce \\textbf{Pass@3}, indicating whether at least one of the three attempts for a task is successful. To further contextualize system performance, we also report behavioral aspects influencing user experience, including the average task completion time and average answer length.\\footnote{We use the self-reported completion time whenever available; otherwise, we manually record the completion time. Manual recording is limited to \\sub to reduce human workload.} We report results on the private test set, reserving the public development set for unrestricted exploration.\\footnote{Note that \\sub is guaranteed to be a subset of the private test set.}\n\n\nWe include two prominent commercial search products, ChatGPT Search \\citep{chatgptsearch} and Perplexity Pro Search \\citep{perplexityai}, which augment LLMs with search capabilities, delivering rapid responses with a limited number of agentic search steps. \nAdditionally, we systematically evaluate a suite of Deep Research systems~\\cite{huggingface_open_deep_research, perplexityai, grok3, gemini_deep_research, openai2025deepresearch}, which are explicitly optimized for extensive information gathering and comprehensive report generation, many of which can sustain continuous running for extended periods (e.g., beyond 30 minutes per query). \nLastly, we assess OpenAI Operator~\\cite{openai2025operator}, one of the most advanced web agents currently available, which perform tasks through direct browser interactions.\nHF Open Deep Research~\\cite{huggingface_open_deep_research} is the only open-source system that we find to yield reasonable results at the time of this evaluation; all the other sufficiently capable systems are closed-source.  \n\nTo provide deeper insights into the practical values of these systems, \nwe further include a human performance study (previously detailed in \\S\\ref{sec:statistics}), wherein human participants undertake tasks in \\sub under fair settings (further elaborated in \\autoref{app:answer}).\n\n\n\\subsection{Main Results}\n\n\\begin{table}\n  \\caption{Main evaluation results. We report the partial completion score, full-task success rate, Pass@3, average time (in minutes), average answer length (in words), and their standard deviation. *: To reduce human workload, the human study is conducted on \\sub as described in \\S\\ref{sec:statistics}.\n  }\n  \\label{tab:main}\n  \n  \\centering\n  \\small\n  \\resizebox{\\textwidth}{!}{\n  \\begin{tabular}{lccccc}\n    \\toprule\n             & \\textbf{Partial Completion} & \\textbf{Success Rate} & \\textbf{Pass@3} & \\textbf{Time (min)} & \\textbf{Answer Length} \\\\\n    \\midrule\n    \\rowcolor{mypink} ChatGPT Search            & 0.26$_{\\pm 0.01}$ & 0.06$_{\\pm 0.01}$ & 0.11 & $<1$  & 314$_{\\pm 4}$  \\\\\n    \\rowcolor{mypink} Perplexity Pro Search     & 0.28$_{\\pm 0.02}$ & 0.08$_{\\pm 0.01}$ & 0.12 & $<1$  & 408$_{\\pm 13}$  \\\\\n    \\midrule\n    \\rowcolor{myblue} OpenAI Operator           & 0.26$_{\\pm 0.01}$ & 0.10$_{\\pm 0.01}$ & 0.17 & 9.74$_{\\pm 0.21}$     & 160$_{\\pm 1}$  \\\\\n    \\midrule\n    \\rowcolor{myyellow} HF Open Deep Research     & 0.26$_{\\pm 0.01}$ & 0.11$_{\\pm 0.01}$ & 0.18 & 13.65$_{\\pm 0.07}$     & 209$_{\\pm 3}$  \\\\\n    \\rowcolor{myyellow} Grok DeepSearch           & 0.40$_{\\pm 0.04}$ & 0.18$_{\\pm 0.02}$ & 0.36 & 2.58$_{\\pm 0.14}$     & 1,428$_{\\pm 16}$ \\\\\n    \\rowcolor{myyellow} Perplexity Deep Research  & 0.42$_{\\pm 0.03}$ & 0.15$_{\\pm 0.03}$ & 0.26 & 5.67$_{\\pm 0.13}$     & 585$_{\\pm 13}$  \\\\\n    \\rowcolor{myyellow} Gemini Deep Research      & 0.45$_{\\pm 0.03}$ & 0.18$_{\\pm 0.02}$ & 0.30 & 7.38$_{\\pm 0.58}$      & 3,357$_{\\pm 49}$ \\\\\n    \\rowcolor{myyellow} Grok DeeperSearch         & \\underline{0.52}$_{\\pm 0.02}$ & \\underline{0.27}$_{\\pm 0.03}$ & \\textbf{0.40} & 5.72$_{\\pm 0.27}$     & 1,362$_{\\pm 24}$ \\\\\n    \\rowcolor{myyellow} OpenAI Deep Research      & \\textbf{0.54}$_{\\pm 0.04}$ & \\textbf{0.28}$_{\\pm 0.04}$ & \\textbf{0.40} & 8.40$_{\\pm 0.71}$    & 559$_{\\pm 19}$  \\\\\n    \\midrule\n    Human*                    & {0.79}$_{\\pm 0.01}$ & {0.54}$_{\\pm 0.07}$ & {0.83} & 18.40$_{\\pm 1.61}$    & 186$_{\\pm 27}$  \\\\\n    \\bottomrule\n\\end{tabular}\n\n\n  }\n\\end{table}\n  \\caption{Main evaluation results. We report the partial completion score, full-task success rate, Pass@3, average time (in minutes), average answer length (in words), and their standard deviation. *: To reduce human workload, the human study is conducted on \\sub as described in \\S\\ref{sec:statistics}.\n  }\n  \\label{tab:main}\n  \n  \\centering\n  \\small\n  \\resizebox{\\textwidth}\\textwidth{!}!{\n  \\begin{tabular}{lccccc}\n    \\toprule\n             & \\textbf{Partial Completion} & \\textbf{Success Rate} & \\textbf{Pass@3} & \\textbf{Time (min)} & \\textbf{Answer Length} \\\\\n    \\midrule\n    \\rowcolor{mypink} ChatGPT Search            & 0.26$_{\\pm 0.01}$ & 0.06$_{\\pm 0.01}$ & 0.11 & $<1$  & 314$_{\\pm 4}$  \\\\\n    \\rowcolor{mypink} Perplexity Pro Search     & 0.28$_{\\pm 0.02}$ & 0.08$_{\\pm 0.01}$ & 0.12 & $<1$  & 408$_{\\pm 13}$  \\\\\n    \\midrule\n    \\rowcolor{myblue} OpenAI Operator           & 0.26$_{\\pm 0.01}$ & 0.10$_{\\pm 0.01}$ & 0.17 & 9.74$_{\\pm 0.21}$     & 160$_{\\pm 1}$  \\\\\n    \\midrule\n    \\rowcolor{myyellow} HF Open Deep Research     & 0.26$_{\\pm 0.01}$ & 0.11$_{\\pm 0.01}$ & 0.18 & 13.65$_{\\pm 0.07}$     & 209$_{\\pm 3}$  \\\\\n    \\rowcolor{myyellow} Grok DeepSearch           & 0.40$_{\\pm 0.04}$ & 0.18$_{\\pm 0.02}$ & 0.36 & 2.58$_{\\pm 0.14}$     & 1,428$_{\\pm 16}$ \\\\\n    \\rowcolor{myyellow} Perplexity Deep Research  & 0.42$_{\\pm 0.03}$ & 0.15$_{\\pm 0.03}$ & 0.26 & 5.67$_{\\pm 0.13}$     & 585$_{\\pm 13}$  \\\\\n    \\rowcolor{myyellow} Gemini Deep Research      & 0.45$_{\\pm 0.03}$ & 0.18$_{\\pm 0.02}$ & 0.30 & 7.38$_{\\pm 0.58}$      & 3,357$_{\\pm 49}$ \\\\\n    \\rowcolor{myyellow} Grok DeeperSearch         & \\underline{0.52}$_{\\pm 0.02}$ & \\underline{0.27}$_{\\pm 0.03}$ & \\textbf{0.40} & 5.72$_{\\pm 0.27}$     & 1,362$_{\\pm 24}$ \\\\\n    \\rowcolor{myyellow} OpenAI Deep Research      & \\textbf{0.54}$_{\\pm 0.04}$ & \\textbf{0.28}$_{\\pm 0.04}$ & \\textbf{0.40} & 8.40$_{\\pm 0.71}$    & 559$_{\\pm 19}$  \\\\\n    \\midrule\n    Human*                    & {0.79}$_{\\pm 0.01}$ & {0.54}$_{\\pm 0.07}$ & {0.83} & 18.40$_{\\pm 1.61}$    & 186$_{\\pm 27}$  \\\\\n    \\bottomrule\n\\end{tabular}\n\n\n  }\n  \n    \\toprule\n             & \\textbf{Partial Completion} & \\textbf{Success Rate} & \\textbf{Pass@3} & \\textbf{Time (min)} & \\textbf{Answer Length} \\\\\n    \\midrule\n    \\rowcolor{mypink}mypink ChatGPT Search            & 0.26$_{\\pm 0.01}$_{\\pm 0.01}\\pm 0.01 & 0.06$_{\\pm 0.01}$_{\\pm 0.01}\\pm 0.01 & 0.11 & $<1$<1  & 314$_{\\pm 4}$_{\\pm 4}\\pm 4  \\\\\n    \\rowcolor{mypink}mypink Perplexity Pro Search     & 0.28$_{\\pm 0.02}$_{\\pm 0.02}\\pm 0.02 & 0.08$_{\\pm 0.01}$_{\\pm 0.01}\\pm 0.01 & 0.12 & $<1$<1  & 408$_{\\pm 13}$_{\\pm 13}\\pm 13  \\\\\n    \\midrule\n    \\rowcolor{myblue}myblue OpenAI Operator           & 0.26$_{\\pm 0.01}$_{\\pm 0.01}\\pm 0.01 & 0.10$_{\\pm 0.01}$_{\\pm 0.01}\\pm 0.01 & 0.17 & 9.74$_{\\pm 0.21}$_{\\pm 0.21}\\pm 0.21     & 160$_{\\pm 1}$_{\\pm 1}\\pm 1  \\\\\n    \\midrule\n    \\rowcolor{myyellow}myyellow HF Open Deep Research     & 0.26$_{\\pm 0.01}$_{\\pm 0.01}\\pm 0.01 & 0.11$_{\\pm 0.01}$_{\\pm 0.01}\\pm 0.01 & 0.18 & 13.65$_{\\pm 0.07}$_{\\pm 0.07}\\pm 0.07     & 209$_{\\pm 3}$_{\\pm 3}\\pm 3  \\\\\n    \\rowcolor{myyellow}myyellow Grok DeepSearch           & 0.40$_{\\pm 0.04}$_{\\pm 0.04}\\pm 0.04 & 0.18$_{\\pm 0.02}$_{\\pm 0.02}\\pm 0.02 & 0.36 & 2.58$_{\\pm 0.14}$_{\\pm 0.14}\\pm 0.14     & 1,428$_{\\pm 16}$_{\\pm 16}\\pm 16 \\\\\n    \\rowcolor{myyellow}myyellow Perplexity Deep Research  & 0.42$_{\\pm 0.03}$_{\\pm 0.03}\\pm 0.03 & 0.15$_{\\pm 0.03}$_{\\pm 0.03}\\pm 0.03 & 0.26 & 5.67$_{\\pm 0.13}$_{\\pm 0.13}\\pm 0.13     & 585$_{\\pm 13}$_{\\pm 13}\\pm 13  \\\\\n    \\rowcolor{myyellow}myyellow Gemini Deep Research      & 0.45$_{\\pm 0.03}$_{\\pm 0.03}\\pm 0.03 & 0.18$_{\\pm 0.02}$_{\\pm 0.02}\\pm 0.02 & 0.30 & 7.38$_{\\pm 0.58}$_{\\pm 0.58}\\pm 0.58      & 3,357$_{\\pm 49}$_{\\pm 49}\\pm 49 \\\\\n    \\rowcolor{myyellow}myyellow Grok DeeperSearch         & \\underline{0.52}$_{\\pm 0.02}$_{\\pm 0.02}\\pm 0.02 & \\underline{0.27}$_{\\pm 0.03}$_{\\pm 0.03}\\pm 0.03 & \\textbf{0.40} & 5.72$_{\\pm 0.27}$_{\\pm 0.27}\\pm 0.27     & 1,362$_{\\pm 24}$_{\\pm 24}\\pm 24 \\\\\n    \\rowcolor{myyellow}myyellow OpenAI Deep Research      & \\textbf{0.54}$_{\\pm 0.04}$_{\\pm 0.04}\\pm 0.04 & \\textbf{0.28}$_{\\pm 0.04}$_{\\pm 0.04}\\pm 0.04 & \\textbf{0.40} & 8.40$_{\\pm 0.71}$_{\\pm 0.71}\\pm 0.71    & 559$_{\\pm 19}$_{\\pm 19}\\pm 19  \\\\\n    \\midrule\n    Human*                    & {0.79}0.79$_{\\pm 0.01}$_{\\pm 0.01}\\pm 0.01 & {0.54}0.54$_{\\pm 0.07}$_{\\pm 0.07}\\pm 0.07 & {0.83}0.83 & 18.40$_{\\pm 1.61}$_{\\pm 1.61}\\pm 1.61    & 186$_{\\pm 27}$_{\\pm 27}\\pm 27  \\\\\n    \\bottomrule\n\n\n\n  \n\n\n\n\nAs shown in \\autoref{tab:main}, while most tasks in \\mind  are conceptually straightforward, their tedious nature poses substantial challenges not only for the agent systems but also for human participants, resulting in low success rates (up to {28\\%}28\\% for agents and {$54\\%$}$54\\%$54\\% for humans). \nMoreover, the substantial gap between partial completions and success rates highlights that current systems often demonstrate initial competence but struggle to fully complete tasks accurately. \n\n\n\\begin{figure}[h]\n  \\centering\n  \\begin{minipage}{0.48\\textwidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/m2w2_partial_vs_time_error_0626.pdf}\n    \\captionof{figure}{Average Partial Completion against average task completion time.}\n    \\label{fig:inference_time}\n  \\end{minipage}\n  \\hfill\n  \\begin{minipage}{0.48\\textwidth}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/mean_avg_explicit_vs_other_0626.pdf}\n    \\captionof{figure}{Average Partial Completion on explicitly time-varying tasks compared to all other tasks.}\n    \\label{fig:time_varying}\n  \\end{minipage}\n\\end{figure}\n  \\centering\n  {0.48\\textwidth}0.48\\textwidth\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/m2w2_partial_vs_time_error_0626.pdf}\n    \\captionof{figure}figure{Average Partial Completion against average task completion time.}Average Partial Completion against average task completion time.\n    \\label{fig:inference_time}\n  \n  \\hfill\n  {0.48\\textwidth}0.48\\textwidth\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/mean_avg_explicit_vs_other_0626.pdf}\n    \\captionof{figure}figure{Average Partial Completion on explicitly time-varying tasks compared to all other tasks.}Average Partial Completion on explicitly time-varying tasks compared to all other tasks.\n    \\label{fig:time_varying}\n  \n\n\n\\textbf{Comparison Between Agent Types.} Unsurprisingly, ChatGPT Search and Perplexity Pro Search emerge as the weakest systems, primarily limited by their restricted search horizon and relatively shallow information synthesis abilities inherent to the LLMs. \nIn contrast, most Deep Research systems achieve superior performance. \nThese systems are explicitly designed, trained, or prompted for extensive information gathering and sophisticated synthesis tasks, enabling sustained, detailed task engagement. \nAdditionally, several Deep Research systems integrate capabilities of text-only or multimodal web browsing (clicking, scrolling), alongside coding tools (e.g., dedicated virtual environments, Python interpreters), enabling real-time search on live websites as well as advanced reasoning and information synthesis. \nOperator exhibits notably lower performance compared to Deep Research systems. \nCompared to agents primarily leveraging search APIs, web agents navigate more complex and noisier environments, manage complex action spaces, and handle substantially longer and more intricate context. These pose substantial challenges to robust long-term reasoning, planning, and memory management, and these challenges are especially amplified and highlighted by the extensive, long-horizon tasks included in \\mind. \nMoreover, unlike web agents that sequentially interact with browsers, recent search agents have begun leveraging parallelized retrieval strategies, offering clear advantages in locating information from the vast online content landscape.\n\n\n\\textbf{Different Behaviors of Deep Research.} We observe two distinct behaviors among Deep Research systems in terms of their response style and output length. The first type, exemplified by OpenAI's and HuggingFace's systems, produces relatively concise and precise answers similar to those of conventional LLM-based search products, occasionally accompanied by supplementary contextual information. In contrast, other systems such as Gemini and Grok consistently generate substantially longer responses organized into structured sections (e.g., introduction, main findings, summary, conclusion), frequently exceeding thousands of words. However, despite the apparent comprehensiveness of these extensive reports, our evaluation reveals that their increased length does not necessarily result in better task completion. Moreover, excessively lengthy reports can be cognitively burdensome and suboptimal for users seeking concise and targeted information.\n\n\n\\textbf{Test-Time Scaling.} As illustrated in \\autoref{fig:inference_time}, we observe clear performance improvements resulting from increased inference time. The benefit is especially evidenced when comparing systems within the same family (e.g., Grok and Perplexity), given that they presumably share the same underlying models. \nThis observation aligns intuitively with the complexity of our tasks, which inherently demands prolonged searches and sophisticated synthesis: extending inference time enables agents to more thoroughly retrieve, process, and integrate the necessary information. \nAdditionally, performing multiple independent trials for each system substantially enhances the likelihood of task success, as indicated by the improved Pass@3 scores. This further underscores the potential of current agentic search systems to benefit from increased computational resources and inference attempts.\n\n\\textbf{Struggle with Time-varying Tasks.} We hypothesize that agentic search systems equipped with no or only limited browsing features might perform worse on time-varying tasks compared to time-invariant tasks. \nMany of those tasks inherently require live web interactions, for instance, verifying hotel room availability on a specific date. \nWithout real-time browsing, agents often provide outdated or hallucinated information.  \nWe identify 57 tasks that are explicitly time-varying (e.g., tasks explicitly associated with certain dates/times or requiring information like product prices that change over time).\nAs shown in \\autoref{fig:time_varying}, most of the evaluated systems perform worse on this subset than on the remaining tasks, which supports our hypothesis.\nInterestingly, OpenAI Operator and human participants, both excelling at interacting with live websites, achieve relatively on-par or superior performance on time-varying tasks. \nIn addition to real-time information, some tasks, such as those requiring advanced filters or visual understanding, also favor browser interaction over search APIs. These collectively highlight the importance of integrating web browsing into agentic search systems, likely contributing substantially to OpenAI Deep Research's superior performance over the other Deep Research systems.\n\n\\textbf{Promises of Agentic Search.} \nDespite current limitations, our evaluation already demonstrates early promise of agentic search systems.  \nThe best-performing system, OpenAI Deep Research, already achieves 50-70\\% of human performance while spending less than half the time. \nHumans are not perfect at many of such complex tasks because we are subject to cognitive fatigue and limited working memory.\nFor instance, in a task that requires retrieval of news articles with nuanced constraints, all the human participants exhibit various forms of oversight or carelessness regarding subtle details or overall task requirements, resulting in task failures. In contrast, most agent systems accurately interpret the task and articles and achieve better performance. \nAgentic search has a substantial potential in augmenting human cognition by automating away the legwork and allowing us to focus our limited cognitive capacity on things that matter more, such as critical decisions and active oversight.\n\n\n\\subsection{Error Analysis}\n\\label{subsec:error}\n\nWe conduct a detailed error analysis over current agentic search systems to gain insights for future development. \nWe ask human annotators to manually label error types in the answers for \\sub.\nWe define seven common and easy-to-identify error categories related to \\textit{correctness} and \\textit{attribution}. Results are shown in \\autoref{fig:error_analysis}, noting that a single answer may contain multiple types of error. Details of definitions and examples of these error types are provided in Appendix~\\ref{app:error_analysis}, with case studies in Appendix~\\ref{app:case_study}.\n\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_analysis_distribution_alt_with_human.pdf}\n  \\caption{Errors across agents and humans.\n  The bars indicate the percentage of tasks exhibiting each error type.\n  We include results from five agentic search systems and humans.}\n  \\label{fig:error_analysis}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_analysis_distribution_alt_with_human.pdf}\n  \\caption{Errors across agents and humans.\n  The bars indicate the percentage of tasks exhibiting each error type.\n  We include results from five agentic search systems and humans.}\n  \\label{fig:error_analysis}\n\n\n\\textbf{Incompleteness}: We observe a notable gap between human and agent performance regarding task completion. We further divide the errors into two subtypes: 1) \\textit{Info. Not Found} (Ex.~\\ref{fig:error_case_not_found}), i.e., the agent explicitly states failure in retrieving the requested information. 2) \\textit{Partial Missing} (Ex.~\\ref{fig:error_case_partial_missing}), i.e., the agent provides fewer items or fewer procedural steps than explicitly required by the task. On the one hand, systems not optimized for `Deep Research'  often exhibit early termination behaviors, showing a degree of \\textit{laziness}. \nOn the other hand, they inherently lack sufficient capabilities to complete these long-horizon tasks. \nFor example, ChatGPT Search executes only limited search steps and applies simple information synthesis by LLMs without using advanced tools (e.g., Python interpreters in many deep research systems), making it challenging for it to find and integrate all necessary information. Systems such as HF Open Deep Research and Operator frequently exhibit total failures at some tasks. Upon closer examination, we find many issues of HF Open Deep Research regarding system errors (e.g., failures in adhering to system prompts to generate valid code to invoke the search tool), which may also apply to other open-source agent systems that simply leverage off-the-shelf models to build deep research systems without post-training the underlying models.\n\n\n\\textbf{Criteria Violation} (Ex.~\\ref{fig:error_case_criteria_violation}):\nWe identify explicit violations to task criteria or factually wrong information directly identifiable from the answer text. \nSuch errors are prevalent among all the evaluated systems, including humans. Notably, this is the most common error type for humans, primarily due to the tedious and demanding nature of the tasks, where humans appear to struggle to remain patient and careful. For instance, one annotator mistakenly lists the University of Waterloo as a U.S. institution. \nInterestingly, deep research systems (e.g., OpenAI Deep Research) have already surpassed humans in this regard, as they are designed to perform exhaustive searches and analysis to meet users' requirements. \n\n\n\\textbf{Invalid Attribution} (Ex.~\\ref{fig:error_case_invalid_attr}): We often observe expired and fabricated URLs from the answers. One potential reason could be that agents generate URLs directly without actually accessing the webpages. \nFor instance, in a task requiring an Amazon purchase link, HF Open Deep Research \ndirectly fabricates a link without accessing Amazon. \nSurprisingly, Operator also has a high percentage of this error type, even though it actually accesses websites as humans do. \nFrom the analysis of the trajectories, we find that it often mistakenly reports incorrect URLs in its final responses even thought it has successfully accessed the correct webpages, which may be partially due to the challenge of generating answers grounded in a long context. For example, in a fellowship identification task, Operator navigates correctly to the correct page but ultimately reports a link that differs by a few words from the correct link.\n\n\\textbf{Missing Attribution} (Ex.~\\ref{fig:error_case_missing_attr}): Claims made in the responses often lack source attribution. \nWeb agents are usually designed or trained on web navigation or citation-free information seeking tasks. Therefore, in contrast to AI search systems, Operator struggles to follow our instructions to provide attribution. \nMoreover, LLMs with massive parametric memory sometimes tend to directly produce or hallucinate information without conducting actual searches, even though most of our tasks do require them to search online in order to provide up-to-date information and attribution. \n\n\n\n\\textbf{Unsupported Answer}: The information in the answer can be different from the sources even when valid attribution is provided. We further divide this issue into two subtypes: (1) \\textit{Synthesis Error} (Ex.~\\ref{fig:error_case_synthesis_err}), i.e., the agent synthesizes information incorrectly from correct webpages (e.g., distorting the price listed on a product page). (2) \\textit{Retrieval Error} (Ex.~\\ref{fig:error_case_retrieval_err}), i.e., the provided source is totally irrelevant to the information. Synthesis errors are pronounced in ChatGPT Search and Perplexity Pro Search, which struggle to accurately synthesize answers from massive sources without advanced tools (e.g., Python interpreters). Humans also sometimes commit synthesis errors due to carelessness when overwhelmed by large volumes of information. \nRetrieval errors can result from the failure to find task-relevant information. For example, an agent may retrieve webpages similar but not precisely aligned with the task requirements, subsequently causing the agent to hallucinate seemingly relevant but unsupported details. \n\n\n\n\\subsection{Human Evaluation of Judge Agents}\n\\label{app:human_agreement}\n\nEmpirically, we have validated the reliability of our judge agents through validation processes. Nonetheless, it remains possible that some evaluation inaccuracies persist. Therefore, to rigorously assess the reliability of our judge agents, \nwe conducted an additional human evaluation for the judge agents. \nWe involve a human evaluator who has no prior experience in judge-agent development, but possesses a deep understanding of the task criteria gained from participating in error analysis, thus ensuring unbiased and accurate assessments.\n\nSpecifically, this evaluation consists of three phases on 15 randomly sampled tasks. \nFor each task, we involve evaluation results of two held-out answers from two different agent systems.\\footnote{During sampling, we exclude trivial total-failure cases to enhance informativeness.} Further details are provided in Appendix~\\ref{app:human_eval}.\n\nIn the \\textbf{Rubric-Level Assessment}, the evaluator assesses the overall rubrics of judge agents independently (without viewing answers or automated evaluation results), rating their validity and comprehensiveness using a three-point scale (\\textit{Strongly Agree}, \\textit{Agree with Reservations}, \\textit{Disagree}). \nIn the subsequent \\textbf{Node-Level Assessment}, the evaluator acts as the Verifier, manually annotating the leaf-node binary judgments, which are then compared against automated judge-agent results to identify discrepancies. \nTo further validate and confirm accuracy, we subsequently perform a \\textbf{Validation of Human Annotation}, wherein an experienced judge-agent developer examines all identified discrepancies from the Node-Level Assessment and communicates directly with the evaluator to confirm potential human errors.\n\n\\textbf{Results and Analysis.}\nThe evaluator fully agrees with all the 15 rubrics. \nHowever, for two rubrics, the evaluator offers minor suggestions regarding the strictness of partial scoring. \nFor example, in one case the evaluator recommends removing partial scoring from a particular node to enforce stricter evaluation, although acknowledging that the existing partial scoring remains reasonable. \nAt the leaf-node level, we identify a total of 35 discrepancies out of 720 verifications. Upon further validation, we discover that 27 of the discrepancies arise from human evaluator errors; the original judgments are correct. \nThis highlights the high complexity and cognitive demand involved in accurately evaluating claims within lengthy answers from agentic search, and reaffirms the reliability of our automated judge agents relative to even well-informed human evaluators. \n\nOf the remaining eight discrepancies, we find:\n\n\\begin{itemize}[nosep, leftmargin=2em, labelsep=0.5em, parsep=0.5em]\n    \\item Three cases result from mistakes by the Verifier, due to overly strict or lenient judgments.\n    \\item Four cases occur because critical information for attribution evaluation is hidden within collapsed content sections of webpages, making it inaccessible during automated retrieval—a known limitation that we have sought to avoid during task validation.\n    \\item One case is due to inconsistent information across multiple sources. Specifically, the agent provides two sources for a year number (2016), where one source shows 2016 while the other one shows 2017. The human evaluator bases their judgment on the incorrect year and deems the response incorrect. Meanwhile, under our current assumption, it suffices to have at least one valid supporting source.\n    \n\\end{itemize}\\begin{itemize}[nosep, leftmargin=2em, labelsep=0.5em, parsep=0.5em]\n    \\item Three cases result from mistakes by the Verifier, due to overly strict or lenient judgments.\n    \\item Four cases occur because critical information for attribution evaluation is hidden within collapsed content sections of webpages, making it inaccessible during automated retrieval—a known limitation that we have sought to avoid during task validation.\n    \\item One case is due to inconsistent information across multiple sources. Specifically, the agent provides two sources for a year number (2016), where one source shows 2016 while the other one shows 2017. The human evaluator bases their judgment on the incorrect year and deems the response incorrect. Meanwhile, under our current assumption, it suffices to have at least one valid supporting source.\n    \n\\end{itemize}\n    \\item Three cases result from mistakes by the Verifier, due to overly strict or lenient judgments.\n    \\item Four cases occur because critical information for attribution evaluation is hidden within collapsed content sections of webpages, making it inaccessible during automated retrieval—a known limitation that we have sought to avoid during task validation.\n    \\item One case is due to inconsistent information across multiple sources. Specifically, the agent provides two sources for a year number (2016), where one source shows 2016 while the other one shows 2017. The human evaluator bases their judgment on the incorrect year and deems the response incorrect. Meanwhile, under our current assumption, it suffices to have at least one valid supporting source.\n    \n\n\nExcluding human mistakes and the source inconsistency case, only 7 out of 720 nodes reflect actual Verifier errors, achieving an exceptional correctness rate of 99.03\\%. This demonstrates remarkable reliability, particularly when compared to recent automated evaluation approaches for relatively simpler web tasks \\citep{xue2025onlinemind2web}, where reported correctness rates of the automated evaluation methods typically fall below 90\\%.\nWe attribute this success to our tree-structured rubric design that cleanly decomposes the complex evaluation, the agentic code generation pipeline for generating judge agents, as well as the rigorous human refinement process.\n\n\n\n\n\n\n", 'appendix': False}, 'Conclusions': {'content': '\n\nIn this work, we introduced \\mind, a novel benchmark specifically designed for comprehensively evaluating agentic search systems on long-horizon information-gathering tasks and time-varying answers. We proposed a flexible, reliable, automated, and scalable evaluation framework based on Agents-as-a-Judge that systematically assesses agent performance on open-ended long-horizon search tasks. Our comprehensive empirical analysis, spanning AI-based search engines, deep research systems, and web agents, reveals both their potentials and significant current limitations. \\mind\\ serves as a valuable resource and rigorous assessment platform for better advancing agentic search systems.\n\n\n\n\n\n', 'appendix': False}, 'Acknowledgments': {'content': '\nThe authors would like to thank colleagues from the OSU NLP group and Amazon AGI for constructive discussions and generous help, Zishuo Zheng for his exploration of developing long-horizon agentic search agents, Akshay Anand and Scott Salisbury for their help on benchmark construction, the HuggingFace team (Amir Mahla, Aymeric Roucher, Aksel Joonas Reedi, and Thomas Wolf) for their assistance with the evaluation of HuggingFace Open Deep Research as well as covering the inference costs, the Grok team (Piaoyang Cui, Hexiang Hu) for their assistance with the evaluation of Grok DeepResearch and DeeperResearch, and the Amazon AGI team for their valuable feedback and contribution to task collection. This research is sponsored in part by a gift from Amazon, ARL W911NF2220144, NSF CAREER \\#1942980, and NSF OAC 2112606. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. government. The U.S. government is authorized to reproduce and distribute reprints for government purposes notwithstanding any copyright notice herein.\n\n\\bibliographystyle{plainnat}plainnat\n\\bibliography{ref.bib}\n\n\n\\newpage\n\n\\newpage\n\n\\newpage\n\n\n\\counterwithin{figure}figure{section}section  \n\\counterwithin{table}table{section}section   \n\n\\renewcommand{\\thefigure}{\\thesection.\\arabic{figure}}\n\\renewcommand{\\thetable}{\\thesection.\\arabic{table}}\n\n\n\n\\DoToC\n\n\\newpage\n\n', 'appendix': True}, 'Limitations': {'content': '\n\\label{app:limitations}\n\n\nWe acknowledge and discuss several limitations in our benchmark design and evaluation methodology:\n\n\\textbf{Task Coverage and Scope.}\nWhile \\mind comprises 130 carefully curated tasks spanning diverse practical domains, it cannot encompass all possible real-world information-seeking scenarios. Certain task categories (e.g., vague or highly subjective queries) are excluded due to our focus on realistic, tedious information-gathering tasks and practical considerations for evaluation. Nevertheless, the extensive diversity of included domains, websites, and realistic scenarios still ensures a reasonable coverage. Thus, we believe these exclusions do not significantly diminish the benchmark’s utility for evaluating and advancing agentic search systems.\n\n\\textbf{Evaluation Framework Assumptions.}\nOur evaluation framework relies on URL-based attribution, presupposing that cited URLs provide truthful and credible information, despite potential misinformation on the web. Evaluating the credibility and truthfulness of individual sources is beyond the scope of this work. Additionally, our evaluation and task design assume critical information can be attributed to individual webpages, which may not always hold true for all possible tasks. However, this constraint has not prevented us from developing a large, diverse, and meaningful benchmark.\n\n\\textbf{Reliance on LLM-based Judgments.}\nOur evaluation employs LLM-based extractions and verifications. While powerful, LLMs may occasionally introduce extraction errors or incorrect judgments. Empirically, we find the base model (OpenAI \\texttt{o4-mini}) sufficiently capable for the extraction and verification tasks in this benchmark. Moreover, to mitigate potential inaccuracies and maintain evaluation reliability, we employ multi-stage validation processes, including rigorous human validation and refinements of evaluation scripts. We further conduct human evaluations of judge-agent outputs, systematically assessing and confirming the overall reliability of LLM-based judgments.\n\n\\textbf{Limited Analysis on Black-Box Systems.}\nOur benchmark primarily evaluates state-of-the-art commercial and research-grade agentic search systems. To ensure informative comparisons, we exclude weak systems incapable of meaningful performance, consequently focusing mainly on proprietary or closed-source solutions. This limits our ability to fully interpret performance differences or estimate precise inference costs (e.g., token usage). Nonetheless, our answer-based evaluation framework effectively assesses the capabilities and common failure modes (e.g., pervasive hallucinations) of these black-box systems, offering valuable insights. To partially compensate for limited access, we report metrics such as task completion time and generated answer length, providing relative references for practical efficiency.\n\n\n\n\n\n', 'appendix': True}, 'Broader Impacts and Ethical Considerations': {'content': '\n\\label{app:impacts}\n\nIn this section, we discuss broader impacts from two interconnected perspectives: the broader implications of agentic search systems, and the impacts associated with the release and use of the \\mind benchmark.\n\n\\textbf{Agentic Search Systems.} Advanced agentic search systems promise a transformation in how users interact with the web, shifting from manual, multi-step information gathering to streamlined, automated information synthesis. This change could significantly reduce cognitive load, improve efficiency, democratize sophisticated search capabilities, and support informed decision-making across diverse fields including education, healthcare, commerce, and policy-making.\n\nDespite benefits, enhanced agentic search may exacerbate misinformation by generating seemingly credible yet incorrect or unsupported information. Malicious actors could exploit such systems for large-scale disinformation or unauthorized data extraction. Additionally, agentic systems risk perpetuating existing biases found in web content, raising fairness concerns and potentially leading to discriminatory outcomes without careful oversight and transparency.\nReliable and scalable evaluation serves as the first line of defense to detect and mitigate such issues.\n\n\\textbf{\\mind Benchmark.} By emphasizing rigorous evaluation through structured rubrics and explicit verification of source attribution, \\mind facilitates the development of transparent and accountable agentic search systems. Establishing standardized, robust evaluation practices helps accelerate trustworthy system development and promotes clarity in capability assessments across the research and industry communities.\n\nHowever, wide adoption of our rubric-based evaluation could lead to automated mass-production of training data via reinforcement learning, particularly by resourceful organizations. While this may improve agent capabilities, it also risks overfitting to benchmark-specific tasks and amplifying biases inherent in rubrics or evaluation methods. Consequently, agents might perform poorly in broader, unstructured real-world scenarios or inadvertently introduce systematic biases.\nTo mitigate this, we maintain a private test set and keep the rubric and evaluation script of the test tasks as well as the script generation pipeline hidden.\n\n\n\n\n', 'appendix': True}, 'Details of Task Construction': {'content': "\n\\label{app:task_construction}\n\n\n\n\\subsection{Domain Distribution}\n\\label{app:domain_distribution}\n\n\\begin{figure}[h]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/m2w2_domain_plot_v2.pdf}\n  \\caption{\\mind contains \\ntasks diverse tasks covering 6 broad domains and 24 sub-domains.}\n  \\label{fig:domain}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/m2w2_domain_plot_v2.pdf}\n  \\caption{\\mind contains \\ntasks diverse tasks covering 6 broad domains and 24 sub-domains.}\n  \\label{fig:domain}\n\n\nDuring task collection, proposers are provided an initial set of fine-grained domains derived from prior work~\\citep{deng2023mind2web} and further expanded using \\texttt{GPT-4o}. Proposers categorize each new task into the most suitable domain, adding new domains as needed. In the subsequent refinement and validation stages, domain assignments are reviewed and adjusted by expert annotators to ensure accuracy and minimize redundancy. Finally, after collecting all \\ntasks tasks, we further refine and consolidate domain categorizations to minimize overlap and redundancy, resulting in the final domain structure presented in~\\autoref{fig:domain}.\n\n\n\n\n\\subsection{Design Principles of Tasks}\\label{app: detail_task_principles}\n\nTo ensure tasks align with the goals of our benchmark and are compatible with our rubric-based evaluation framework, we define and follow these task-design principles:\n\n\n\\textbf{Realism.}\nTasks should represent authentic and practical user needs. Each task must have clear real-world applicability, avoiding artificial combinations of unrelated steps just for complexity or to challenge AI systems.\n\n\\textbf{Tediousness (Long-Horizon).}\nTasks must require sustained effort due to extensive web search, exploration, and information synthesis. Simple tasks solvable within a few queries are explicitly avoided. Human annotators validate tediousness by confirming each task requires at least five minutes of human effort.\nNote that it is just the minimum; most tasks in \\mind take humans much longer to complete (see statistics in \\autoref{tab:stats}).\n\n\\textbf{Clarity and Objectivity.}\nTask descriptions must be explicit, precise, grammatically correct, and unambiguous. Answer criteria must be clearly stated, avoiding vague or subjective terms (e.g., ``\\textit{good},'' ``\\textit{effective},'' or ``\\textit{better}''). When domain-specific knowledge is required, it must be clearly defined or explained in the task description. To ensure clarity, tasks undergo ambiguity checks via both manual and LLM-assisted inspection.\n\n\\textbf{Verifiability.}\nTasks must have clearly defined and practically verifiable criteria. The criteria should be verifiable primarily through the answer text itself as well as the expected URL-based provenance. Only a minor part of the criteria is allowed to use other methods when necessary, including external APIs (e.g., Google Maps for distance measurement) and fixed ground-truth answers (or ground-truth answers from fixed URLs).\n\n\\textbf{Additional Constraints and Exclusions.}\nTo ensure practicality and our focus on web search instead of other intelligent capabilities as well as the reliability of evaluation, the following constraints apply:\n\n\\begin{itemize}[nosep, leftmargin=1em, labelsep=0.5em, parsep=0.5em]\n    \\item Tasks involving video understanding or non-English websites are excluded from this study.\n    \\item Tasks \\textit{explicitly} requiring complex reasoning (e.g., summarize a complex research paper) or external tools (e.g., Python interpreters or calculators) are avoided. However, we do not constrain the evaluated systems on how they complete the tasks. They can use whatever tools deemed necessary or helpful.\n    \\item Tasks whose answers constantly change (e.g., currency exchange rates which change within a very short period) are excluded to ensure stable evaluation.\n    \\item  Tasks should avoid reliance on global or overly general qualifiers (e.g., ``cheapest,'' ``list all,'' or ``top-$k$'') unless these conditions are verifiable (e.g., by a fixed set of URL sources or fixed ground-truth answers).\n    \\item We currently assume each verification of attribution can be conducted on a single webpage. Tasks requiring simultaneous verification across multiple webpages, where verification cannot be decomposed into independent single-page validations, are beyond the scope of this benchmark.\n\\end{itemize}\\begin{itemize}[nosep, leftmargin=1em, labelsep=0.5em, parsep=0.5em]\n    \\item Tasks involving video understanding or non-English websites are excluded from this study.\n    \\item Tasks \\textit{explicitly} requiring complex reasoning (e.g., summarize a complex research paper) or external tools (e.g., Python interpreters or calculators) are avoided. However, we do not constrain the evaluated systems on how they complete the tasks. They can use whatever tools deemed necessary or helpful.\n    \\item Tasks whose answers constantly change (e.g., currency exchange rates which change within a very short period) are excluded to ensure stable evaluation.\n    \\item  Tasks should avoid reliance on global or overly general qualifiers (e.g., ``cheapest,'' ``list all,'' or ``top-$k$'') unless these conditions are verifiable (e.g., by a fixed set of URL sources or fixed ground-truth answers).\n    \\item We currently assume each verification of attribution can be conducted on a single webpage. Tasks requiring simultaneous verification across multiple webpages, where verification cannot be decomposed into independent single-page validations, are beyond the scope of this benchmark.\n\\end{itemize}\n    \\item Tasks involving video understanding or non-English websites are excluded from this study.\n    \\item Tasks \\textit{explicitly} requiring complex reasoning (e.g., summarize a complex research paper) or external tools (e.g., Python interpreters or calculators) are avoided. However, we do not constrain the evaluated systems on how they complete the tasks. They can use whatever tools deemed necessary or helpful.\n    \\item Tasks whose answers constantly change (e.g., currency exchange rates which change within a very short period) are excluded to ensure stable evaluation.\n    \\item  Tasks should avoid reliance on global or overly general qualifiers (e.g., ``cheapest,'' ``list all,'' or ``top-$k$k'') unless these conditions are verifiable (e.g., by a fixed set of URL sources or fixed ground-truth answers).\n    \\item We currently assume each verification of attribution can be conducted on a single webpage. Tasks requiring simultaneous verification across multiple webpages, where verification cannot be decomposed into independent single-page validations, are beyond the scope of this benchmark.\n\n\nThese principles are documented and illustrated with concrete examples, serving as guidelines for human annotators. Each task is carefully validated and iteratively refined by initial proposers, refiner experts and validation experts to ensure full compliance before final inclusion into \\mind.\n\n\n\\subsection{Task Collection Details}\n\nWe collect and refine tasks for \\mind under a three-stage pipeline: \\textit{Proposal}, \\textit{Refinement}, and \\textit{Validation}, ensuring adherence to the task principles as well as evaluation practicability.\n\n\\textbf{Task Proposal.}\nInitial task proposers independently generate task ideas aligned with the defined principles. At this stage, proposers conduct self-checks covering major task principles (e.g., realism, tediousness, clarity, verifiability) as well as minor aspects such as grammatical correctness and clarity. Proposers also provide initial draft answers or relevant URLs to facilitate the following  refinement and validation phases.\n\n\\textbf{Task Refinement.}\nExpert annotators further review and iteratively refine each proposed task together with the initial proposers. During refinement, experts carefully evaluate tasks for practicality, clarity, and adherence to the defined principles, suggesting necessary adjustments to task descriptions, verification criteria, or expected answers. Refinement ensures that tasks remain realistic and challenging yet clearly defined and objectively verifiable.\n\n\\textbf{Task Validation.}\nFinally, each task undergoes validation by two more independent annotators. Validators verify task feasibility by fully completing the task as well as carefully checking for potential ambiguities, overlooked edge cases, or any violations of the URL-based evaluation assumptions. Tasks failing validation criteria (e.g., too ambiguous, infeasible, or impractical to verify) are further revised or rejected. Only tasks successfully passing validation are included in the final benchmark.\n\n\n\n\\subsection{Future Maintenance of the Benchmark}\n\nSimilar to previous benchmarks that rely on live web environments~\\citep{pan2024webcanvas,xue2025onlinemind2web}, tasks in \\mind~may be affected by changes or updates to websites over time. However, unlike prior works that explicitly tie tasks to specific websites, our benchmark primarily involves broad information-seeking goals, allowing flexibility for agents in selecting sources. \nMoreover, our evaluation focuses exclusively on verifying the final retrieved information rather than intermediate web interactions, and our Agent-as-a-Judge evaluation can reliably evaluate time-varying answers. Collectively, these designs substantially reduce our sensitivity to website changes compared to prior benchmarks.\n\nNevertheless, we commit to long-term maintenance of our benchmark. We will periodically review tasks and actively solicit feedback from benchmark users. If substantial website changes or unavailability significantly alter task difficulty or solvability, we will update affected tasks or replace them with new ones of similar complexity and scope, thereby maintaining the integrity and intended challenge level of our benchmark.\n\n\n\n", 'appendix': True}, 'Experimental Details': {'content': "\n\\label{app:answer}\n\n\n\\subsection{System Selection and Settings}\n\n\\textbf{System Selection.} \nWe aim to evaluate a broad spectrum of agentic search systems, encompassing systems based on search APIs, web agents interacting directly with browsers, hybrid systems integrating both paradigms, and potentially agents of some other forms.\n\nWe exclude systems incapable of reliably providing source attribution, as accurate attribution is integral to our evaluation. Additionally, we omit weak systems that are unlikely to demonstrate meaningful performance within our benchmark context.\n\n\n\n\n\\textbf{Settings.} \nTo test the variability in outputs, we independently run and evaluate each agent system three times per task. As certain agent systems (e.g., Perplexity Pro Deep Research and Gemini Deep Research) do not report the completion time, we manually measure their completion time on the \\sub. For a fair comparison, the reported timing for all systems in~\\autoref{tab:main} are calculated only using the \\sub.\n\nWe note that many of these systems are continuously improving. Therefore, to clarify, all answers in this study are collected between April and June, 2025. Additionally, for Hugging Face Open Deep Research, we use OpenAI's \\texttt{o3} model as its base model.\n\n\n\\textbf{Prompts.} \nFor most of the agents we evaluate, we use a unified prompt as follows:\n\n\\begin{tcolorbox}[colframe=green!50!black, colback=green!10!white, title=System Prompt for Agent Inference, breakable]\n\\small\n\nYou are an expert assistant specializing in solving information-seeking tasks.\n\n\\medskip\n\nIMPORTANT:\n\n\\smallskip\n\n1. Do not ask for additional information or follow-up questions. All necessary requirements are provided in the task description — please strictly adhere to it to complete the task.\n\n\\smallskip\n\n2. To solve the task, you should search the web for online sources and use them to support all your claims and the information in your final answer. Do not provide critical information without actual searching.\n\n\\smallskip\n\n3. Every claim and piece of information you provide must be supported by a source. In your answer, please include relevant links for each claim and piece of information.\n\n\n\\end{tcolorbox}\\begin{tcolorbox}[colframe=green!50!black, colback=green!10!white, title=System Prompt for Agent Inference, breakable]\n\\small\n\nYou are an expert assistant specializing in solving information-seeking tasks.\n\n\\medskip\n\nIMPORTANT:\n\n\\smallskip\n\n1. Do not ask for additional information or follow-up questions. All necessary requirements are provided in the task description — please strictly adhere to it to complete the task.\n\n\\smallskip\n\n2. To solve the task, you should search the web for online sources and use them to support all your claims and the information in your final answer. Do not provide critical information without actual searching.\n\n\\smallskip\n\n3. Every claim and piece of information you provide must be supported by a source. In your answer, please include relevant links for each claim and piece of information.\n\n\n\\end{tcolorbox}[colframe=green!50!black, colback=green!10!white, title=System Prompt for Agent Inference, breakable]\n\\small\n\nYou are an expert assistant specializing in solving information-seeking tasks.\n\n\\medskip\n\nIMPORTANT:\n\n\\smallskip\n\n1. Do not ask for additional information or follow-up questions. All necessary requirements are provided in the task description — please strictly adhere to it to complete the task.\n\n\\smallskip\n\n2. To solve the task, you should search the web for online sources and use them to support all your claims and the information in your final answer. Do not provide critical information without actual searching.\n\n\\smallskip\n\n3. Every claim and piece of information you provide must be supported by a source. In your answer, please include relevant links for each claim and piece of information.\n\n\n\n\nEmpirically, we find OpenAI Operator and OpenAI Deep Research occasionally neglect the requirements to provide sources for all information retrieved. Therefore, we slightly modify the prompts for them to mitigate this issue:\n\n\n\\begin{tcolorbox}[colframe=blue!50!black, colback=blue!10!white, title=System Prompt for OpenAI Operator, breakable]\n\\small\n\nYou are an expert assistant specializing in solving information-seeking tasks.\n\n\\medskip\n\nIMPORTANT:\n\n\\smallskip\n\n1. Do not ask for additional information or follow-up questions. All necessary requirements are provided in the task description—please strictly adhere to it to complete the task.\n\n\\smallskip\n\n2. To solve the task, you should search the web for online sources and use them to support all your claims and the information in your final answer. Do not provide critical information without actual searching.\n\n\\smallskip\n\n3. Every claim and piece of information you provide must be supported by a source. In your answer, please include relevant links for each claim and piece of information. If the task requires a list of items (e.g., names, emails, affiliations, products), each item in the list must be supported by its own unique source URL that directly confirms the item.\n\n\n\\end{tcolorbox}\\begin{tcolorbox}[colframe=blue!50!black, colback=blue!10!white, title=System Prompt for OpenAI Operator, breakable]\n\\small\n\nYou are an expert assistant specializing in solving information-seeking tasks.\n\n\\medskip\n\nIMPORTANT:\n\n\\smallskip\n\n1. Do not ask for additional information or follow-up questions. All necessary requirements are provided in the task description—please strictly adhere to it to complete the task.\n\n\\smallskip\n\n2. To solve the task, you should search the web for online sources and use them to support all your claims and the information in your final answer. Do not provide critical information without actual searching.\n\n\\smallskip\n\n3. Every claim and piece of information you provide must be supported by a source. In your answer, please include relevant links for each claim and piece of information. If the task requires a list of items (e.g., names, emails, affiliations, products), each item in the list must be supported by its own unique source URL that directly confirms the item.\n\n\n\\end{tcolorbox}[colframe=blue!50!black, colback=blue!10!white, title=System Prompt for OpenAI Operator, breakable]\n\\small\n\nYou are an expert assistant specializing in solving information-seeking tasks.\n\n\\medskip\n\nIMPORTANT:\n\n\\smallskip\n\n1. Do not ask for additional information or follow-up questions. All necessary requirements are provided in the task description—please strictly adhere to it to complete the task.\n\n\\smallskip\n\n2. To solve the task, you should search the web for online sources and use them to support all your claims and the information in your final answer. Do not provide critical information without actual searching.\n\n\\smallskip\n\n3. Every claim and piece of information you provide must be supported by a source. In your answer, please include relevant links for each claim and piece of information. If the task requires a list of items (e.g., names, emails, affiliations, products), each item in the list must be supported by its own unique source URL that directly confirms the item.\n\n\n\n\n\n\\begin{tcolorbox}[colframe=blue!50!black, colback=blue!10!white, title=System Prompt for Gemini Deep Research, breakable]\n\\small\n\nYou are an expert assistant specializing in solving information-seeking tasks.\n\n\\medskip\n\nIMPORTANT:\n\n\\smallskip\n\n1. Do not ask for additional information or follow-up questions. All necessary requirements are provided in the task description—please strictly adhere to it to complete the task.\n\n\\smallskip\n\n2. To solve the task, you should search the web for online sources and use them to support all your claims and the information in your final answer. Do not provide critical information without actual searching.\n\n\\smallskip\n\n3. Every claim and piece of information you provide must be supported by a source. In your answer, please include relevant links for each claim and piece of information. Even if the task explicitly requests some specific links, you must still provide URL sources for all the other information included.\n\n% \\medskip\n\n% TASK:\n\n\\end{tcolorbox}\\begin{tcolorbox}[colframe=blue!50!black, colback=blue!10!white, title=System Prompt for Gemini Deep Research, breakable]\n\\small\n\nYou are an expert assistant specializing in solving information-seeking tasks.\n\n\\medskip\n\nIMPORTANT:\n\n\\smallskip\n\n1. Do not ask for additional information or follow-up questions. All necessary requirements are provided in the task description—please strictly adhere to it to complete the task.\n\n\\smallskip\n\n2. To solve the task, you should search the web for online sources and use them to support all your claims and the information in your final answer. Do not provide critical information without actual searching.\n\n\\smallskip\n\n3. Every claim and piece of information you provide must be supported by a source. In your answer, please include relevant links for each claim and piece of information. Even if the task explicitly requests some specific links, you must still provide URL sources for all the other information included.\n\n% \\medskip\n\n% TASK:\n\n\\end{tcolorbox}[colframe=blue!50!black, colback=blue!10!white, title=System Prompt for Gemini Deep Research, breakable]\n\\small\n\nYou are an expert assistant specializing in solving information-seeking tasks.\n\n\\medskip\n\nIMPORTANT:\n\n\\smallskip\n\n1. Do not ask for additional information or follow-up questions. All necessary requirements are provided in the task description—please strictly adhere to it to complete the task.\n\n\\smallskip\n\n2. To solve the task, you should search the web for online sources and use them to support all your claims and the information in your final answer. Do not provide critical information without actual searching.\n\n\\smallskip\n\n3. Every claim and piece of information you provide must be supported by a source. In your answer, please include relevant links for each claim and piece of information. Even if the task explicitly requests some specific links, you must still provide URL sources for all the other information included.\n\n\n\n\n\n\n\n\n\n\\subsection{Webpage Pre-caching for Evaluation}\n\n\nThe verification of attribution is critical for our evaluation. To ensure stable evaluation, we employ a pipeline to pre-fetch and cache webpage contents referenced in agent-generated answers. This caching process ensures consistent, reliable, and efficient access to webpage screenshots and textual content for subsequent verification steps. We apply this strategy to all the tasks prior to their evaluation by the judge agents.\n\n\\textbf{Webpage Loading and Caching.} For each task, we first aggregate the URLs from agent answers. We load and cache webpage content of each unique URL using Playwright. Additionally, our script distinguishes and supports handling PDF documents besides normal webpages.\n\nGiven that webpage contents may evolve, especially for time-sensitive tasks (e.g., fluctuating product prices), this caching step is essential for establishing a stable reference for evaluation, reflecting the exact state of online sources at the time answers are generated.\n\n\\textbf{Manual Intervention for Blocked Webpages.} A small number of websites block automated visits, preventing automatic content retrieval. Since attribution is crucial for verification, we provide an additional manual review and replacement script. Human annotators manually visit these blocked websites using standard browsers, collect the correct webpage content (\\texttt{MHTML}), and replace the cached version.\n\n\n\n\\subsection{Human Performance on \\sub}\n\n\nTo establish a clear reference point for evaluating agent performance, we conduct a study on human performance using \\sub.\n\nHuman completers are tasked to independently complete each assigned task by searching and browsing relevant websites, providing answers with explicit URL-based sources for each claim or statement.\n\nEach task is assigned to three completers without prior knowledge of the task (excluding creators or reviewers). \nCompleters are instructed not to give up on a task unless they still have not landed on a clear path to the solution after 30 minutes. \nSome tasks may be easy to find a path to solution but exceedingly tedious to execute on that path (e.g., it may require visiting hundreds of different webpages to collect information). \nCompleters are allowed to give up after continuing efforts exceeding one hour.\n\nDuring task completion, completers utilize an open-source Chrome extension to log time and webpages visited,\\footnote{Web Activity Time Tracker: https://github.com/Stigmatoz/web-activity-time-tracker.} exporting these records for subsequent analysis. This data collection provides critical benchmark statistics regarding task complexity and human effort.\n\nTo ensure the quality of human performance, completers first undertake two simplified trial tasks from \\mind. Only completers who have successfully followed instructions and met quality expectations in these trials can participate in the formal human study. \n\n\\subsection{Error Analysis and Examples}\n\\label{app:error_analysis}\n\\begin{figure}[h]\n  \\centering\n  \\includegraphics[width=0.9\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_analysis_workflow.pdf}\n  \\caption{Workflow of categorizing errors in error analysis.}\n  \\label{fig:error_workflow}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.9\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_analysis_workflow.pdf}\n  \\caption{Workflow of categorizing errors in error analysis.}\n  \\label{fig:error_workflow}\n\n\n\n\nTo gain deeper insights into the failure modes of both agent systems and human performance, we perform an error analysis using the \\sub. We first categorize common failure patterns along two dimensions, {\\textit{correctness} and \\textit{attribution}}\\textit{correctness} and \\textit{attribution}: \n\n\\paragraph{Correctness.}Correctness.  We evaluate the {textual}textual correctness of an answer based on the following aspects:\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Incompleteness}: The answer fails to fully satisfy the task needs, with two subcategories: (1) \\textit{Information Not Found}: The agent explicitly states it cannot find the requested information. (2) \\textit{Partial Completion}: The answer contains fewer items or steps than explicitly requested by the task.\n    \\item \\textbf{Criteria Violation}: The answer explicitly contradicts the clearly stated task criteria or provides incorrect factual information, identifiable directly from the answer text itself. Examples include providing an item priced higher than the user-given threshold or incorrectly identifying the user-specified research paper.\n\\end{itemize}\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Incompleteness}: The answer fails to fully satisfy the task needs, with two subcategories: (1) \\textit{Information Not Found}: The agent explicitly states it cannot find the requested information. (2) \\textit{Partial Completion}: The answer contains fewer items or steps than explicitly requested by the task.\n    \\item \\textbf{Criteria Violation}: The answer explicitly contradicts the clearly stated task criteria or provides incorrect factual information, identifiable directly from the answer text itself. Examples include providing an item priced higher than the user-given threshold or incorrectly identifying the user-specified research paper.\n\\end{itemize}\n    \\item \\textbf{Incompleteness}: The answer fails to fully satisfy the task needs, with two subcategories: (1) \\textit{Information Not Found}: The agent explicitly states it cannot find the requested information. (2) \\textit{Partial Completion}: The answer contains fewer items or steps than explicitly requested by the task.\n    \\item \\textbf{Criteria Violation}: The answer explicitly contradicts the clearly stated task criteria or provides incorrect factual information, identifiable directly from the answer text itself. Examples include providing an item priced higher than the user-given threshold or incorrectly identifying the user-specified research paper.\n\n\n\\paragraph{Attribution.}Attribution. Independently from the \\textit{correctness} criterion based on the answer text only, we verify whether the provided URL sources support the key information stated in the answer. Attribution errors are often related to hallucinations in LLM-based agent systems.\n\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Invalid Attribution}: URLs provided by the agent are expired, incorrectly formatted, or fabricated.\n    \\item \\textbf{Missing Attribution}: No URL is provided to support the claims made.\n    \\item \\textbf{Unsupported Answer}: URLs do not support the claims. This category can be further divided into: (1) \\textit{Synthesis Error}: The URL contains useful information required for the task, but the agent misrepresents or incorrectly extracts this information from the URL in the generated text. (2) \\textit{Retrieval Error}: The provided URLs are irrelevant to the task and thus do not match the claims made in the answer.\n\\end{itemize}\\begin{itemize}[leftmargin=*]\n    \\item \\textbf{Invalid Attribution}: URLs provided by the agent are expired, incorrectly formatted, or fabricated.\n    \\item \\textbf{Missing Attribution}: No URL is provided to support the claims made.\n    \\item \\textbf{Unsupported Answer}: URLs do not support the claims. This category can be further divided into: (1) \\textit{Synthesis Error}: The URL contains useful information required for the task, but the agent misrepresents or incorrectly extracts this information from the URL in the generated text. (2) \\textit{Retrieval Error}: The provided URLs are irrelevant to the task and thus do not match the claims made in the answer.\n\\end{itemize}\n    \\item \\textbf{Invalid Attribution}: URLs provided by the agent are expired, incorrectly formatted, or fabricated.\n    \\item \\textbf{Missing Attribution}: No URL is provided to support the claims made.\n    \\item \\textbf{Unsupported Answer}: URLs do not support the claims. This category can be further divided into: (1) \\textit{Synthesis Error}: The URL contains useful information required for the task, but the agent misrepresents or incorrectly extracts this information from the URL in the generated text. (2) \\textit{Retrieval Error}: The provided URLs are irrelevant to the task and thus do not match the claims made in the answer.\n\n\nThen, human annotators examine answers from {five}five representative agent systems (ChatGPT Search, Perplexity Pro Search, {HF Open Deep Research}HF Open Deep Research, OpenAI Deep Research, and OpenAI Operator), as well as human answers. For each task, we randomly select one answer per system. As shown in \\autoref{fig:error_workflow}, we provide a workflow figure to help human annotators categorize and identify errors.\n\n\nTo better illustrate the error types, we present the following examples for each type.\n\n\n\\textbf{Summary on Error Examples.} \nHallucination plays a significant role in the errors across all agent systems. \nSpecifically, two error types can be directly attributed to hallucination: \\textit{Invalid Attribution} and \\textit{Unsupported Answer}, while other errors may arise from issues such as instruction-following inabilities. \\textit{Invalid Attribution} refers to cases where the provided sources are fabricated or non-existent. \\textit{Unsupported Answer} describes situations where the provided sources do not substantiate the claims made. Both indicate that the answers are hallucinated rather than accurately extracted from retrieved sources. Accordingly, we calculate a hallucination rate, defined as the proportion of tasks exhibiting either \\textit{Invalid Attribution} or \\textit{Unsupported Answer}, across all systems examined in our error analysis. Even OpenAI Deep Research, the best-performing system on \\mind, reaches a hallucination rate of 23\\%. Other systems exhibit a hallucination rate of at least 50\\%. Note that this hallucination rate is likely underestimated, as it only considers two specific error types, while some of the other errors might also be influenced by hallucination. For example, \\autoref{fig:error_case_invalid_attr}, \\autoref{fig:error_case_synthesis_err} and \\autoref{fig:error_case_retrieval_err} illustrate specific cases where hallucination manifests as invalid attribution, synthesis error, and retrieval error, respectively.\n\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_case_not_found.jpg}\n  \\caption{An example of \\textit{Information Not Found}, where Perplexity Pro Search explicitly states that it cannot retrieve the requested information, thus failing to fully address the task.}\n  \\label{fig:error_case_not_found}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_case_not_found.jpg}\n  \\caption{An example of \\textit{Information Not Found}, where Perplexity Pro Search explicitly states that it cannot retrieve the requested information, thus failing to fully address the task.}\n  \\label{fig:error_case_not_found}\n\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_case_partial_missing.jpg}\n  \\caption{An example of \\textit{Partial Missing}, where ChatGPT Search provides the Nobel Prize winners' information only for a subset of the requested years (2004–2014), failing to fully complete the task (2004–2024).}\n  \\label{fig:error_case_partial_missing}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_case_partial_missing.jpg}\n  \\caption{An example of \\textit{Partial Missing}, where ChatGPT Search provides the Nobel Prize winners' information only for a subset of the requested years (2004–2014), failing to fully complete the task (2004–2024).}\n  \\label{fig:error_case_partial_missing}\n\n\n\n\n\\begin{figure}[H]\n  \\centering\n\\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_case_cv.jpg}\n  \\caption{An example of \\textit{Criteria Violation}, where OpenAI Operator explicitly violates the specified budget constraint (\\$200–\\$600) by providing a shopping list totaling \\$1,277.97.}\n  \\label{fig:error_case_criteria_violation}\n\\end{figure}\n  \\centering\n\\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_case_cv.jpg}\n  \\caption{An example of \\textit{Criteria Violation}, where OpenAI Operator explicitly violates the specified budget constraint (\\$200–\\$600) by providing a shopping list totaling \\$1,277.97.}\n  \\label{fig:error_case_criteria_violation}\n\n\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_hallucination_invalid_attr.jpg}\n  \\caption{An example of \\textit{Invalid Attribution}, where OpenAI Operator fabricates three links that mimic the URL patterns of Federal Reserve official speech pages and Reuters articles, resulting in an entirely hallucinated response.}\n  \\label{fig:error_case_invalid_attr}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_hallucination_invalid_attr.jpg}\n  \\caption{An example of \\textit{Invalid Attribution}, where OpenAI Operator fabricates three links that mimic the URL patterns of Federal Reserve official speech pages and Reuters articles, resulting in an entirely hallucinated response.}\n  \\label{fig:error_case_invalid_attr}\n\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_case_missing_attr.jpg}\n  \\caption{An example of \\textit{Missing Attribution}, where OpenAI Operator provides birthplace details for Nobel Prize winners without supplying URLs or sources to support these claims.}\n  \\label{fig:error_case_missing_attr}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_case_missing_attr.jpg}\n  \\caption{An example of \\textit{Missing Attribution}, where OpenAI Operator provides birthplace details for Nobel Prize winners without supplying URLs or sources to support these claims.}\n  \\label{fig:error_case_missing_attr}\n\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_case_retrieval.jpg}\n  \\caption{An example of \\textit{Retrieval Error}, where the provided URL from ChatGPT Search contains irrelevant information and cannot support the claims about characters' abilities in the answer.}\n  \\label{fig:error_case_retrieval_err}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/error_case_retrieval.jpg}\n  \\caption{An example of \\textit{Retrieval Error}, where the provided URL from ChatGPT Search contains irrelevant information and cannot support the claims about characters' abilities in the answer.}\n  \\label{fig:error_case_retrieval_err}\n\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_hallucination_synthesis.jpg}\n  \\caption{An example of \\textit{Synthesis Error}, where inaccurate details in answers provided by Perplexity Pro Search ultimately lead to incorrect responses.}\n  \\label{fig:error_case_synthesis_err}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_hallucination_synthesis.jpg}\n  \\caption{An example of \\textit{Synthesis Error}, where inaccurate details in answers provided by Perplexity Pro Search ultimately lead to incorrect responses.}\n  \\label{fig:error_case_synthesis_err}\n\n\n\n\\newpage\n\\clearpage\n\\subsection{Additional Case Studies}\n\\label{app:case_study}\n\nWe have presented some error examples in Appendix~\\ref{app:error_analysis}. Here, we further include a few cases of common patterns in different systems, and include several case studies below.\n\n\\textbf{Humans Mistakes due to Carelessness.} Tasks in \\mind are intentionally designed to be tedious while ensuring complete feasibility for human participants. Intuitively and empirically, we find that humans indeed have no issue with completeness: all human answers fully fulfill task requirements without omission, and without hallucinations of webpage URLs. These errors include, but are not limited to: overlooking the overall or detailed constraints explicitly stated in the task description; misreading or incorrectly extracting information from webpages; significant spelling mistakes; and errors related to common-sense knowledge. Notably, some of these human mistakes are unlikely to occur in answers from capable agents. We include two examples in \\autoref{fig:error_case_human_cri_vio} and \\autoref{fig:error_case_human_synthesis}.\n\n\\textbf{System Errors from HuggingFace Open Deep Research.} In our error analysis, we observed a substantial number of \\textit{Information Not Found} errors from the HuggingFace Open Deep Research (HF ODR). Upon closely examining its execution trajectories and logs, we discover that many of these errors result from improper tool usage (e.g., incorrect input formats) or mistakes in generated code. Such mistakes prematurely terminated the agent's execution, leading it to incorrectly conclude the requested information was unattainable. We include an example in \\autoref{fig:error_case_hf_odr_not_found}\n\nNotably, HF ODR is the only open-source solution included in our experiments. It entirely relies on off-the-shelf models connected mainly through prompting, without further fine-tuning. This likely contributes substantially to the frequent occurrence of these system errors, which may also apply to other open-source agents that utilize current off-the-shelf models.\n\nOverall, these suggest that directly leveraging current off-the-shelf models without additional tailoring or training may not suffice for developing robust, reliable deep research systems.\n\n\n\\textbf{Web Agents for Long-Horizon Information Seeking} During our evaluation, Operator frequently exhibit poor performance. Several challenges likely contributed to this issue, including insufficient long-term reasoning, planning, and potentially grounding failures. Notably, we also clearly observed that Operator is inadequately optimized for comprehensive information seeking tasks. In particular, it lacks optimized long-term memory mechanisms for managing retrieved information and associated sources. We include an example in \\autoref{fig:error_case_operator_invalid_attr}.\n\nWe also include examples of OpenAI Deep Research (\\autoref{fig:error_case_oai_dr_missing_attr}), Perplexity Pro Search (\\autoref{fig:error_case_perplexity_not_found}) and ChatGPT Search (\\autoref{fig:error_case_chatgpt_search_synthesis_err}) in this section.\n\n\\newpage\n\n\\begin{figure}\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_human_cri_vio.jpg}\n  \\caption{A case of a human annotator making a \\textit{Criteria Violation} by carelessly categorizing the University of Waterloo as a U.S. university.}\n  \\label{fig:error_case_human_cri_vio}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_human_cri_vio.jpg}\n  \\caption{A case of a human annotator making a \\textit{Criteria Violation} by carelessly categorizing the University of Waterloo as a U.S. university.}\n  \\label{fig:error_case_human_cri_vio}\n\n\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_human_synthesis.jpg}\n  \\caption{A case of a human annotator making a \\textit{Synthesis Error} by carelessly misspelling the name of the most recent Pritzker Prize winner.}\n  \\label{fig:error_case_human_synthesis}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_human_synthesis.jpg}\n  \\caption{A case of a human annotator making a \\textit{Synthesis Error} by carelessly misspelling the name of the most recent Pritzker Prize winner.}\n  \\label{fig:error_case_human_synthesis}\n\n\n\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_hf_odr_not_found.jpg}\n  \\caption{A case of HF Open Deep Research committing an \\textit{Information Not Found} error due to a system failure to properly follow the system prompt to invoke the search tool.}\n  \\label{fig:error_case_hf_odr_not_found}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_hf_odr_not_found.jpg}\n  \\caption{A case of HF Open Deep Research committing an \\textit{Information Not Found} error due to a system failure to properly follow the system prompt to invoke the search tool.}\n  \\label{fig:error_case_hf_odr_not_found}\n\n\n\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_operator_invalid_attr.jpg}\n  \\caption{A case where the OpenAI Operator reports an invalid attribution, differing by a few words from the actual website URL, despite having visited the correct source.}\n  \\label{fig:error_case_operator_invalid_attr}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_operator_invalid_attr.jpg}\n  \\caption{A case where the OpenAI Operator reports an invalid attribution, differing by a few words from the actual website URL, despite having visited the correct source.}\n  \\label{fig:error_case_operator_invalid_attr}\n\n\n\n\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_oai_dr_missing_attr.jpg}\n  \\caption{A case where OpenAI Deep Research presents claims without verifiable attribution, likely resulting from direct generation rather than conducting a real-time search.}\n  \\label{fig:error_case_oai_dr_missing_attr}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_oai_dr_missing_attr.jpg}\n  \\caption{A case where OpenAI Deep Research presents claims without verifiable attribution, likely resulting from direct generation rather than conducting a real-time search.}\n  \\label{fig:error_case_oai_dr_missing_attr}\n\n\n\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_perplexity_not_found.jpg}\n  \\caption{A case where Perplexity Pro Search fails to find the required information within limited search steps, despite the known ground-truth answer being available and discoverable.}\n  \\label{fig:error_case_perplexity_not_found}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_perplexity_not_found.jpg}\n  \\caption{A case where Perplexity Pro Search fails to find the required information within limited search steps, despite the known ground-truth answer being available and discoverable.}\n  \\label{fig:error_case_perplexity_not_found}\n\n\n\n\\begin{figure}[H]\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_chatgpt_search_synthesis_err.jpg}\n  \\caption{A case where ChatGPT Search retrieves several relevant webpages but fails to synthesize a correct answer with accurate attribution in a task requiring extensive information across 20 years.}\n  \\label{fig:error_case_chatgpt_search_synthesis_err}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.85\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/case_chatgpt_search_synthesis_err.jpg}\n  \\caption{A case where ChatGPT Search retrieves several relevant webpages but fails to synthesize a correct answer with accurate attribution in a task requiring extensive information across 20 years.}\n  \\label{fig:error_case_chatgpt_search_synthesis_err}\n\n\n\n\n\n\n\\subsection{Human Evaluation of Judge Agents}\n\\label{app:human_eval}\nEmpirically we have found \\texttt{o4-mini} capable of serving as the Extractor and Verifier. In addition, for each judge agent, we have done a two-stage careful validation and refinement. Nonetheless, to further validate the reliability of our judge agents, we conduct this human evaluation study. We involve one human evaluator, who is familiar with our tasks but has never reviewed the judge agents, to conduct a human evaluation of judge agents with 15 sampled tasks. The evaluator has engaged in the error analysis, and gained abundant knowledge and experience with the criteria of the tasks.  Specifically, the human annotator will first conduct a \\textbf{rubric-level assessment} about the overall rubrics of these judge agents, confirming whether they agree with the overall rubrics. It's possible that different people may have different understanding about the optimal rubric for a task. Then, the human annotator conducts a \\textbf{node-level assessment}, manually assigning binary scores to leaf nodes (i.e., the fine-grained judgments.) \n\nWe include the full instructions to the human evaluator in Appendix~\\ref{app:instr_doc_human_eval}.\n\n\n\n\\newpage\n", 'appendix': True}, 'Details on Rubrics and Judge Agents': {'content': '\n\\label{app:rubric}\n\n\n\\subsection{Rubric Design}\n\n\nOur primary objective in designing the rubric-based evaluation framework is to create a unified, scalable, and practical scoring method applicable across all tasks for \\mind, as well as potential future tasks. We emphasize practicality in verification processes and a meaningful assignment of partial scores, intended to clearly reflect incremental progress and practical utility to users. Moreover, we emphasize: (1) Partial scoring is permitted only when it meaningfully represents incremental progress and offers genuine utility. For example, in tasks involving the identification of items meeting several criteria, partial satisfaction typically yields no practical benefit to the user, hence such cases receive no partial score. (2) For attribution verification, if it is reasonable and practical to expect URL-based source citations for a statement, the corresponding verification node must be set as \\textit{critical} rather than optional. This ensures strict adherence to proper attribution standards, thus reinforcing trustworthiness and factual accuracy.\n\nThrough these principles, we aim to ensure the rubrics are both rigorous and practically useful, providing reliable and meaningful evaluations across varied and complex agentic search tasks.\n\n\n\\subsection{Details for Judge Agents}\n\nTo build judge agents aligned with our rubric design, we first develop a comprehensive and reusable codebase. This codebase includes implementations of rubric tree structures, scoring mechanisms, \\textit{Verifier}, \\textit{Extractor}, and necessary auxiliary components. Leveraging this carefully constructed codebase, judge-agent development primarily focuses on designing rubric tree structures, extraction pipelines, and leaf-node verification processes (including prompts when LLM-based verification is involved). Each of these components has corresponding helper functions and classes, enabling straightforward implementation.\n\nAdditionally, during judge-agent evaluation, we employ a default short-circuit mechanism for evaluation efficiency in terms of inference time as well as the cost. Specifically, verification at any given node is skipped if it is blocked by any critical node failure, or a preceding node failure within a sequential parent node. However, when conducting human evaluation of the judge agents, we disable this short-circuit mechanism to ensure all nodes are evaluated comprehensively, facilitating a complete comparison against human annotations.\n\nTo provide further understanding for the \\textit{Extractor} and \\textit{Verifier}, we present below the main prompts used by these components in our judge agents. Additional implementation details and complete code are available in our open-source repository.\n\\begin{tcolorbox}[colframe=green!50!black, colback=green!10!white, title=Prompt for Extractor, breakable]\n\\small\n\nYou are responsible for extracting specific information of interest from the provided answer text for a task. For context, we are evaluating the correctness of an answer to a web information-gathering task. This extraction step helps us identify relevant information for subsequent validation. You must carefully follow the provided extraction instructions to accurately extract information from the answer.\n\n\\medskip\n\\textbf{GENERAL RULES:}\n\n\\smallskip\n\n1. Do not add, omit, or invent any information. Extract only information explicitly mentioned in the provided answer exactly as it appears.\n\n2. If any required information is missing from the answer, explicitly return \\texttt{null} as the JSON value.\n\n3. You will also receive the original task description as context. Understand it clearly, as it provides essential background for the extraction. You may apply common-sense reasoning to assist your extraction, but your final result must be accurately extracted from the answer text provided.\n\n4. Occasionally, additional instructions might be provided to aid your extraction. Carefully follow those instructions when available.\n\n\\medskip\n\\textbf{SPECIAL RULES FOR URL EXTRACTION:}\n\n– These rules apply only when URL fields are required in the extraction.\n\n\\smallskip\n\n1. Extract only URLs explicitly present in the answer text. Do not create or infer any URLs.\n\n2. Extract only valid URLs. Ignore obviously invalid or malformed URLs.\n\n3. If a URL is missing a protocol (\\texttt{http\\://} or \\texttt{https\\://}), prepend \\texttt{http\\://}.\n\n\\medskip\n\\textbf{Instruction for Extraction:}\n\\begin{quote}\n\\{extraction\\_prompt\\}\n\\end{quote}\n\n\\textbf{Original Task Description:}\n\\begin{quote}\n\\{task\\_description\\}\n\\end{quote}\n\n\\textbf{Complete Answer to the Task:}\n\\begin{quote}\n\\{answer\\}\n\\end{quote}\n\n\\textbf{Additional Instructions (if any):}\n\\begin{quote}\n\\{additional\\_instruction\\}\n\\end{quote}\n\n\\end{tcolorbox}\\begin{tcolorbox}[colframe=green!50!black, colback=green!10!white, title=Prompt for Extractor, breakable]\n\\small\n\nYou are responsible for extracting specific information of interest from the provided answer text for a task. For context, we are evaluating the correctness of an answer to a web information-gathering task. This extraction step helps us identify relevant information for subsequent validation. You must carefully follow the provided extraction instructions to accurately extract information from the answer.\n\n\\medskip\n\\textbf{GENERAL RULES:}\n\n\\smallskip\n\n1. Do not add, omit, or invent any information. Extract only information explicitly mentioned in the provided answer exactly as it appears.\n\n2. If any required information is missing from the answer, explicitly return \\texttt{null} as the JSON value.\n\n3. You will also receive the original task description as context. Understand it clearly, as it provides essential background for the extraction. You may apply common-sense reasoning to assist your extraction, but your final result must be accurately extracted from the answer text provided.\n\n4. Occasionally, additional instructions might be provided to aid your extraction. Carefully follow those instructions when available.\n\n\\medskip\n\\textbf{SPECIAL RULES FOR URL EXTRACTION:}\n\n– These rules apply only when URL fields are required in the extraction.\n\n\\smallskip\n\n1. Extract only URLs explicitly present in the answer text. Do not create or infer any URLs.\n\n2. Extract only valid URLs. Ignore obviously invalid or malformed URLs.\n\n3. If a URL is missing a protocol (\\texttt{http\\://} or \\texttt{https\\://}), prepend \\texttt{http\\://}.\n\n\\medskip\n\\textbf{Instruction for Extraction:}\n\\begin{quote}\n\\{extraction\\_prompt\\}\n\\end{quote}\n\n\\textbf{Original Task Description:}\n\\begin{quote}\n\\{task\\_description\\}\n\\end{quote}\n\n\\textbf{Complete Answer to the Task:}\n\\begin{quote}\n\\{answer\\}\n\\end{quote}\n\n\\textbf{Additional Instructions (if any):}\n\\begin{quote}\n\\{additional\\_instruction\\}\n\\end{quote}\n\n\\end{tcolorbox}[colframe=green!50!black, colback=green!10!white, title=Prompt for Extractor, breakable]\n\\small\n\nYou are responsible for extracting specific information of interest from the provided answer text for a task. For context, we are evaluating the correctness of an answer to a web information-gathering task. This extraction step helps us identify relevant information for subsequent validation. You must carefully follow the provided extraction instructions to accurately extract information from the answer.\n\n\\medskip\n\\textbf{GENERAL RULES:}\n\n\\smallskip\n\n1. Do not add, omit, or invent any information. Extract only information explicitly mentioned in the provided answer exactly as it appears.\n\n2. If any required information is missing from the answer, explicitly return \\texttt{null} as the JSON value.\n\n3. You will also receive the original task description as context. Understand it clearly, as it provides essential background for the extraction. You may apply common-sense reasoning to assist your extraction, but your final result must be accurately extracted from the answer text provided.\n\n4. Occasionally, additional instructions might be provided to aid your extraction. Carefully follow those instructions when available.\n\n\\medskip\n\\textbf{SPECIAL RULES FOR URL EXTRACTION:}\n\n– These rules apply only when URL fields are required in the extraction.\n\n\\smallskip\n\n1. Extract only URLs explicitly present in the answer text. Do not create or infer any URLs.\n\n2. Extract only valid URLs. Ignore obviously invalid or malformed URLs.\n\n3. If a URL is missing a protocol (\\texttt{http\\://} or \\texttt{https\\://}), prepend \\texttt{http\\://}.\n\n\\medskip\n\\textbf{Instruction for Extraction:}\n\n\\{extraction\\_prompt\\}\n\n\n\\textbf{Original Task Description:}\n\n\\{task\\_description\\}\n\n\n\\textbf{Complete Answer to the Task:}\n\n\\{answer\\}\n\n\n\\textbf{Additional Instructions (if any):}\n\n\\{additional\\_instruction\\}\n\n\n\n\n\n\n\\begin{tcolorbox}[colframe=blue!50!black, colback=blue!10!white, title=Prompt for Verifier (Simple Verification), breakable]\n\\small\n\nYou are responsible for verifying whether a given claim or simple statement is correct and accurate. Typically, this verification involves straightforward factual judgments or logical checks (e.g., "1+1=2", or verifying if a given name matches exactly another given name). For context, we are evaluating the correctness of an answer to a web information-gathering task. This verification step helps us determine part of the answer’s accuracy. Your task is to provide a binary judgment ("Correct" or "Incorrect") along with clear and detailed reasoning supporting your decision.\n\n\\medskip\nTo assist your judgment, you will receive:\n\\begin{itemize}\n\\small\n\\item The original task description (as context).\n\\item The complete answer to the task (as context).\n\\item Additional instructions (occasionally provided to guide your verification).\n\\end{itemize}\n\n\\medskip\n\\textbf{GENERAL RULES:}\n\n\\smallskip\n\n1. Carefully examine the provided claim or statement. Use logic, basic factual knowledge, or simple reasoning to determine its accuracy.\n\n2. Clearly understand the provided task description and complete answer, as they offer important context and may influence your decision.\n\n3. Your reasoning must be explicit, concise, and directly support your binary judgment.\n\n4. Carefully follow any additional instructions provided. If none are provided, you may ignore this.\n\n\\medskip\n\\textbf{Original Task Description:}\n\\begin{quote}\n\\{task\\_description\\}\n\\end{quote}\n\n\\textbf{Complete Answer to the Task:}\n\\begin{quote}\n\\{answer\\}\n\\end{quote}\n\n\\textbf{Additional Instructions (if any):}\n\\begin{quote}\n\\{additional\\_instruction\\}\n\\end{quote}\n\n\\textbf{Claim or Statement to Verify:}\n\\begin{quote}\n\\{claim\\}\n\\end{quote}\n\n\\end{tcolorbox}\\begin{tcolorbox}[colframe=blue!50!black, colback=blue!10!white, title=Prompt for Verifier (Simple Verification), breakable]\n\\small\n\nYou are responsible for verifying whether a given claim or simple statement is correct and accurate. Typically, this verification involves straightforward factual judgments or logical checks (e.g., "1+1=2", or verifying if a given name matches exactly another given name). For context, we are evaluating the correctness of an answer to a web information-gathering task. This verification step helps us determine part of the answer’s accuracy. Your task is to provide a binary judgment ("Correct" or "Incorrect") along with clear and detailed reasoning supporting your decision.\n\n\\medskip\nTo assist your judgment, you will receive:\n\\begin{itemize}\n\\small\n\\item The original task description (as context).\n\\item The complete answer to the task (as context).\n\\item Additional instructions (occasionally provided to guide your verification).\n\\end{itemize}\n\n\\medskip\n\\textbf{GENERAL RULES:}\n\n\\smallskip\n\n1. Carefully examine the provided claim or statement. Use logic, basic factual knowledge, or simple reasoning to determine its accuracy.\n\n2. Clearly understand the provided task description and complete answer, as they offer important context and may influence your decision.\n\n3. Your reasoning must be explicit, concise, and directly support your binary judgment.\n\n4. Carefully follow any additional instructions provided. If none are provided, you may ignore this.\n\n\\medskip\n\\textbf{Original Task Description:}\n\\begin{quote}\n\\{task\\_description\\}\n\\end{quote}\n\n\\textbf{Complete Answer to the Task:}\n\\begin{quote}\n\\{answer\\}\n\\end{quote}\n\n\\textbf{Additional Instructions (if any):}\n\\begin{quote}\n\\{additional\\_instruction\\}\n\\end{quote}\n\n\\textbf{Claim or Statement to Verify:}\n\\begin{quote}\n\\{claim\\}\n\\end{quote}\n\n\\end{tcolorbox}[colframe=blue!50!black, colback=blue!10!white, title=Prompt for Verifier (Simple Verification), breakable]\n\\small\n\nYou are responsible for verifying whether a given claim or simple statement is correct and accurate. Typically, this verification involves straightforward factual judgments or logical checks (e.g., "1+1=2", or verifying if a given name matches exactly another given name). For context, we are evaluating the correctness of an answer to a web information-gathering task. This verification step helps us determine part of the answer’s accuracy. Your task is to provide a binary judgment ("Correct" or "Incorrect") along with clear and detailed reasoning supporting your decision.\n\n\\medskip\nTo assist your judgment, you will receive:\n\n\\small\n\\item The original task description (as context).\n\\item The complete answer to the task (as context).\n\\item Additional instructions (occasionally provided to guide your verification).\n\n\n\\medskip\n\\textbf{GENERAL RULES:}\n\n\\smallskip\n\n1. Carefully examine the provided claim or statement. Use logic, basic factual knowledge, or simple reasoning to determine its accuracy.\n\n2. Clearly understand the provided task description and complete answer, as they offer important context and may influence your decision.\n\n3. Your reasoning must be explicit, concise, and directly support your binary judgment.\n\n4. Carefully follow any additional instructions provided. If none are provided, you may ignore this.\n\n\\medskip\n\\textbf{Original Task Description:}\n\n\\{task\\_description\\}\n\n\n\\textbf{Complete Answer to the Task:}\n\n\\{answer\\}\n\n\n\\textbf{Additional Instructions (if any):}\n\n\\{additional\\_instruction\\}\n\n\n\\textbf{Claim or Statement to Verify:}\n\n\\{claim\\}\n\n\n\n\n\n\n\\begin{tcolorbox}[colframe=blue!50!black, colback=blue!10!white, title=Prompt for Verifier (URL-based Verification), breakable]\n\\small\n\nYou are responsible for verifying whether a given claim or "fact" is fully supported by the actual content of a specified webpage (or a PDF file from a PDF webpage). For context, we are examining the correctness of an answer to a web information-gathering task. Typically, the claim or "fact" is extracted directly from the answer, and the webpage provided is the URL source referenced in the answer. This verification step helps us determine whether the claim or "fact" in the answer is accurate or hallucinated, a common issue in LLM-based systems. You will receive both the text content and a screenshot of the webpage for examination. Your task is to provide a binary judgment (i.e., supported or not supported) along with clear and detailed reasoning for your decision.\n\n\\medskip\n\\textbf{GENERAL RULES:}\n\n\\smallskip\n\n1. The provided webpage content may be lengthy. Carefully examine the relevant sections of both the webpage text and the screenshot. Determine clearly whether the claim or "fact" exactly matches or is explicitly supported by the webpage content. If the information appears to be not able to find from the text, but more likely from the screenshot, please check the screenshot carefully.\n\n2. You will also receive the original task description and the complete answer as context. Understand them clearly, as they provide essential background for evaluating the claim. You may apply common-sense reasoning (e.g., fuzzy matching for names differing only in letter casing or minor spelling variations) to assist your judgment, but your final decision must primarily rely on explicit evidence from the webpage content provided.\n\n3. If the provided webpage (the URL source mentioned in the answer) is entirely irrelevant, invalid, or inaccessible, you must conclude that the claim or "fact" is not supported.\n\n4. Occasionally, additional instructions might be provided to aid your judgment. Carefully follow those instructions when available.\n\n\\medskip\n\\textbf{Original Task Description:}\n\\begin{quote}\n\\{task\\_description\\}\n\\end{quote}\n\n\\textbf{Complete Answer to the Task:}\n\\begin{quote}\n\\{answer\\}\n\\end{quote}\n\n\\textbf{Claim or Fact to Verify:}\n\\begin{quote}\n\\{claim\\}\n\\end{quote}\n\n\\textbf{Additional Instructions (if any):}\n\\begin{quote}\n\\{additional\\_instruction\\}\n\\end{quote}\n\n\\textbf{Webpage URL:}\n\\begin{quote}\n\\{url\\}\n\\end{quote}\n\n\\textbf{Extracted Webpage Text (truncated if too long):}\n\\begin{quote}\n\\{web\\_text\\}\n\\end{quote}\n\n\n\\textbf{Rendered Screenshots (to provide non-textual context):}\n\\begin{quote}\n\\{screenshots\\}\n\\end{quote}\n\n\n\n\n\\end{tcolorbox}\\begin{tcolorbox}[colframe=blue!50!black, colback=blue!10!white, title=Prompt for Verifier (URL-based Verification), breakable]\n\\small\n\nYou are responsible for verifying whether a given claim or "fact" is fully supported by the actual content of a specified webpage (or a PDF file from a PDF webpage). For context, we are examining the correctness of an answer to a web information-gathering task. Typically, the claim or "fact" is extracted directly from the answer, and the webpage provided is the URL source referenced in the answer. This verification step helps us determine whether the claim or "fact" in the answer is accurate or hallucinated, a common issue in LLM-based systems. You will receive both the text content and a screenshot of the webpage for examination. Your task is to provide a binary judgment (i.e., supported or not supported) along with clear and detailed reasoning for your decision.\n\n\\medskip\n\\textbf{GENERAL RULES:}\n\n\\smallskip\n\n1. The provided webpage content may be lengthy. Carefully examine the relevant sections of both the webpage text and the screenshot. Determine clearly whether the claim or "fact" exactly matches or is explicitly supported by the webpage content. If the information appears to be not able to find from the text, but more likely from the screenshot, please check the screenshot carefully.\n\n2. You will also receive the original task description and the complete answer as context. Understand them clearly, as they provide essential background for evaluating the claim. You may apply common-sense reasoning (e.g., fuzzy matching for names differing only in letter casing or minor spelling variations) to assist your judgment, but your final decision must primarily rely on explicit evidence from the webpage content provided.\n\n3. If the provided webpage (the URL source mentioned in the answer) is entirely irrelevant, invalid, or inaccessible, you must conclude that the claim or "fact" is not supported.\n\n4. Occasionally, additional instructions might be provided to aid your judgment. Carefully follow those instructions when available.\n\n\\medskip\n\\textbf{Original Task Description:}\n\\begin{quote}\n\\{task\\_description\\}\n\\end{quote}\n\n\\textbf{Complete Answer to the Task:}\n\\begin{quote}\n\\{answer\\}\n\\end{quote}\n\n\\textbf{Claim or Fact to Verify:}\n\\begin{quote}\n\\{claim\\}\n\\end{quote}\n\n\\textbf{Additional Instructions (if any):}\n\\begin{quote}\n\\{additional\\_instruction\\}\n\\end{quote}\n\n\\textbf{Webpage URL:}\n\\begin{quote}\n\\{url\\}\n\\end{quote}\n\n\\textbf{Extracted Webpage Text (truncated if too long):}\n\\begin{quote}\n\\{web\\_text\\}\n\\end{quote}\n\n\n\\textbf{Rendered Screenshots (to provide non-textual context):}\n\\begin{quote}\n\\{screenshots\\}\n\\end{quote}\n\n\n\n\n\\end{tcolorbox}[colframe=blue!50!black, colback=blue!10!white, title=Prompt for Verifier (URL-based Verification), breakable]\n\\small\n\nYou are responsible for verifying whether a given claim or "fact" is fully supported by the actual content of a specified webpage (or a PDF file from a PDF webpage). For context, we are examining the correctness of an answer to a web information-gathering task. Typically, the claim or "fact" is extracted directly from the answer, and the webpage provided is the URL source referenced in the answer. This verification step helps us determine whether the claim or "fact" in the answer is accurate or hallucinated, a common issue in LLM-based systems. You will receive both the text content and a screenshot of the webpage for examination. Your task is to provide a binary judgment (i.e., supported or not supported) along with clear and detailed reasoning for your decision.\n\n\\medskip\n\\textbf{GENERAL RULES:}\n\n\\smallskip\n\n1. The provided webpage content may be lengthy. Carefully examine the relevant sections of both the webpage text and the screenshot. Determine clearly whether the claim or "fact" exactly matches or is explicitly supported by the webpage content. If the information appears to be not able to find from the text, but more likely from the screenshot, please check the screenshot carefully.\n\n2. You will also receive the original task description and the complete answer as context. Understand them clearly, as they provide essential background for evaluating the claim. You may apply common-sense reasoning (e.g., fuzzy matching for names differing only in letter casing or minor spelling variations) to assist your judgment, but your final decision must primarily rely on explicit evidence from the webpage content provided.\n\n3. If the provided webpage (the URL source mentioned in the answer) is entirely irrelevant, invalid, or inaccessible, you must conclude that the claim or "fact" is not supported.\n\n4. Occasionally, additional instructions might be provided to aid your judgment. Carefully follow those instructions when available.\n\n\\medskip\n\\textbf{Original Task Description:}\n\n\\{task\\_description\\}\n\n\n\\textbf{Complete Answer to the Task:}\n\n\\{answer\\}\n\n\n\\textbf{Claim or Fact to Verify:}\n\n\\{claim\\}\n\n\n\\textbf{Additional Instructions (if any):}\n\n\\{additional\\_instruction\\}\n\n\n\\textbf{Webpage URL:}\n\n\\{url\\}\n\n\n\\textbf{Extracted Webpage Text (truncated if too long):}\n\n\\{web\\_text\\}\n\n\n\n\\textbf{Rendered Screenshots (to provide non-textual context):}\n\n\\{screenshots\\}\n\n\n\n\n\n\n\n\\subsection{Rubric and Judge Agent Generation}\n\nGiven the complexity of our tasks and rubrics, manually developing rubric-based judge agents from scratch would be both time-consuming and cognitively demanding. Therefore, we employ an automated generation pipeline leveraging frontier LLMs (\\texttt{Claude-3.7-Sonnet}) to produce the initial version of the judge-agent scripts.\n\nSpecifically, we input the following content to the code LLM: the task description, along with detailed instructions covering our benchmark\'s overall goals, rubric design principles, evaluation strategies, and core evaluation toolkit functionalities (such as \\textit{Extractor} and \\textit{Verifier} functions as well as rubric tree management utilities). We also include examples of common mistakes and tips to guide the LLM towards producing practical and well-structured scripts.\n\nTo further improve code generation quality, we implement two autonomous debugging strategies:\n\n\\textbf{Self-Debug with System Feedback}: After script generation, the code is automatically executed, capturing runtime errors or execution issues. We by default use the answer from OpenAI Deep Research for providing information to the extractors, while omitting all the verification steps (returning all \\texttt{True}) to detect bugs in the code. System feedback (i.e., error messages) is then iteratively fed back into the model for script correction until there is no runtime errors.\n\n\\textbf{Self-Debug with Self-Reflection}: The scripts undergo another stage of autonomous review, which involves multiple rounds of self-reflection, guided by explicit quality checklists. \nThe LLM reflects on script correctness, logical coherence, rubric completeness, and potentially overlooked edge cases.\n\nEmpirically, we observe these iterative debugging and self-reflection stages to be indispensable and highly useful, as the initial scripts produced by LLMs often require multiple refinement rounds to achieve the desired level of correctness and completeness.\n\n\n\\subsection{Two-Stage Validation of Judge Agents}\n\n\n\nWe conduct a two-stage manual validation process to ensure the quality and robustness of the generated judge-agent scripts.\n\nIn the first stage, trained annotators independently inspect each generated judge-agent script. Annotators verify the rubric\'s correctness, completeness, and practical feasibility, ensuring that the rubric and prompts accurately reflect task criteria with reasonable scoring. Particularly complex rubrics, involving intricate combinations of sequential and parallel criteria, typically require careful manual adjustments beyond initial automated generation.\n\nIn the second stage, scripts undergo practical validation against real answers collected from various agent systems. Specifically, for each task, we randomly select a single answer from each of six randomly chosen agent systems after the initial evaluation runs. Annotators review the evaluation outcomes from these answers to identify subtle issues or edge cases. To maintain generalizability, annotators are instructed to adjust only critical errors or omissions, refining scripts with targeted logic or additional prompts without overfitting to specific answers. The remaining answers are held out as an additional set to further verify the generalization of the finalized evaluation scripts, as used in the human agreement study. Empirically, we often find necessary adjustments to the prompts of the Extractor and the Verifier, as well as making necessary changes to allow reasonable edge cases.\n\nTo facilitate the validation processes, we also develop a GUI tool that enables human annotators to easily visualize answers, rubrics and evaluation outcomes from the judge agents, as illustrated in \\autoref{fig:annotation_tool}.\n\n\n\\begin{figure}[h]\n  \\centering\n  \\includegraphics[width=0.95\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/m2w2_annotation_tool.png}\n  \\caption{A screenshot of the GUI tool for visualizing agent answers, pre-cached webpages, rubrics, and judge-agent evaluation outcomes.}\n  \\label{fig:annotation_tool}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.95\\textwidth,trim=0cm 0cm 0cm 0cm, clip]{figures/m2w2_annotation_tool.png}\n  \\caption{A screenshot of the GUI tool for visualizing agent answers, pre-cached webpages, rubrics, and judge-agent evaluation outcomes.}\n  \\label{fig:annotation_tool}\n\n\n\n\n\n\n\n\n\\newpage\n\n\n', 'appendix': True}, 'Example of Judge-Agent Scripts': {'content': '\\label{app:script_exp}\n\nWe provide a script of a task in the public development set, where the full task description can be find in the script below.\n\n\\begin{lstlisting}{python}\nimport asyncio\nimport logging\nfrom typing import Optional, List, Dict, Any\n\nfrom pydantic import BaseModel, Field\n\nfrom mind2web2 import CacheClass, Evaluator, VerificationNode, AggregationStrategy\n\nTASK_ID = "find_llava_commit"\nTASK_DESCRIPTION = """\nIdentify the first commit on the main branch of the official Hugging Face transformers repository that added support for the LLaVA model.\nPlease provide the following details about this commit: the short commit ID (first 7 characters), the date of the commit, a list of all contributors/authors involved in this commit. For each author, include a link to their GitHub profile page and the full real name displayed on their GitHub profile page.\n"""\n\nEVAL_NOTES = ""\nGROUND_TRUTH = {\n    "commit_id": "44b5506",\n    "date": "Dec 7, 2023",\n    "expected_authors": [\n        "Younes B",\n        "Arthur",  # Or Arthur Zucker\n        "Shauray Singh",\n        "Lysandre Debut",\n        "Haotian Liu"\n    ]\n}\n\n\nclass AuthorInfo(BaseModel):\n    """Data model for individual author information."""\n    name: Optional[str] = Field(default=None, description="Author name as provided in the answer")\n    profile_url: Optional[str] = Field(default=None, description="GitHub profile page URL")\n    real_name_from_profile: Optional[str] = Field(default=None, description="Real name extracted from profile page")\n\n\nclass CommitInfo(BaseModel):\n    """Data model for extracted commit information."""\n    commit_id: Optional[str] = Field(default=None, description="Short commit ID (7 characters)")\n    date: Optional[str] = Field(default=None, description="Date of the commit")\n    source_urls: Optional[List[str]] = Field(default_factory=list, description="Source URLs for commit verification")\n\n\nclass AuthorsInfo(BaseModel):\n    """Data model for extracted authors information."""\n    authors: Optional[List[AuthorInfo]] = Field(default_factory=list,\n                                                description="List of authors with their profile info")\n\n\ndef prompt_extract_commit_info() -> str:\n    """\n    Extraction prompt for getting basic commit information from the answer.\n    """\n    return """\n    Extract the basic commit information for the LLaVA model support from the answer.\n\n    Look for:\n    - commit_id: The short commit ID (typically 7 characters) \n    - date: The date when the commit was made\n    - source_urls: Any URLs that contain or reference this commit information (e.g., GitHub commit URLs, repository links)\n\n    Extract the information exactly as it appears in the text.\n    If any field is not mentioned, set it to null or empty list.\n    """\n\n\ndef prompt_extract_authors_info() -> str:\n    """\n    Extraction prompt for getting authors information from the answer.\n    """\n    return """\n    Extract all author information mentioned in the answer related to the LLaVA commit.\n\n    For each author, extract:\n    - name: The author\'s name as mentioned in the answer\n    - profile_url: Their GitHub profile page URL if provided\n    - real_name_from_profile: Their real name as stated to appear on their profile page\n\n    Extract information exactly as it appears in the text.\n    If any field is not mentioned for an author, set it to null.\n    Include all authors mentioned, even if some information is incomplete.\n    """\n\n\nasync def verify_commit_info(\n        evaluator: Evaluator,\n        parent_node: VerificationNode,\n        commit_info: CommitInfo,\n) -> None:\n    """\n    Verify commit information in sequential order.\n    This is the first node in the sequential chain.\n    """\n    # Create sequential node for commit verification steps\n    commit_verification = evaluator.add_sequential(\n        id_="commit_verification",\n        desc="Verify commit ID, date, and provenance in sequence",\n        parent=parent_node,\n        critical=False  # Non-critical to allow sequential partial scoring\n    )\n\n    # Step 1: Verify commit ID exists and is correct\n    await verify_commit_id(evaluator, commit_verification, commit_info)\n\n    # Step 2: Verify commit date\n    await verify_commit_date(evaluator, commit_verification, commit_info)\n\n\nasync def verify_commit_id(\n        evaluator: Evaluator,\n        parent_node: VerificationNode,\n        commit_info: CommitInfo,\n) -> None:\n    """\n    Verify commit ID existence, correctness and provenance.\n    """\n    # Create parallel node for commit ID checks\n    commit_id_node = evaluator.add_parallel(\n        id_="commit_id_verification",\n        desc="Verify commit ID existence, correctness and provenance",\n        parent=parent_node\n    )\n\n    # Check if commit ID exists\n    id_exists = evaluator.add_custom_node(\n        result=bool(commit_info.commit_id and commit_info.commit_id.strip() and commit_info.source_urls and commit_info.commit_id),\n        node_id="commit_id_exists",\n        description="Commit ID is provided in the answer",\n        parent=commit_id_node,\n        critical=True,  # Most critical - without ID, nothing else matters\n    )\n\n    # Verify commit ID correctness\n    id_correctness = evaluator.add_leaf(\n        id_="commit_id_correctness",\n        desc="Commit ID matches the expected value (44b5506)",\n        parent=commit_id_node,\n        critical=True,  # Critical - ID must be correct\n    )\n\n    # Always perform verification - short-circuit logic will skip if existence failed\n    claim = f"This ID (a github commit id) \'{commit_info.commit_id or \'N/A\'}\' matches this ID \'{GROUND_TRUTH[\'commit_id\']}\'"\n    await evaluator.verify(\n        claim=claim,\n        node=id_correctness,\n        sources=None,  # Simple comparison, no URL needed\n        additional_instruction="Allow minor formatting differences or extra descriptions but the core 7-character commit ID should exist and match exactly. Expected: 44b5506."\n    )\n\n    # Provenance check - verify the commit info is supported by sources\n    provenance_check = evaluator.add_leaf(\n        id_="commit_provenance",\n        desc="Commit information is supported by provided source URLs",\n        parent=commit_id_node,\n        critical=True,  # Critical - information must be substantiated\n    )\n\n    # Always perform verification - short-circuit logic will skip if needed\n    # if commit_info.source_urls and commit_info.commit_id:\n    claim = f"This page shows or mentioned the github commit ID: \'{commit_info.commit_id}\'. For example, if this is exactly the commit page"\n    await evaluator.verify(\n        claim=claim,\n        node=provenance_check,\n        sources=commit_info.source_urls,\n    )\n\n\nasync def verify_commit_date(\n        evaluator: Evaluator,\n        parent_node: VerificationNode,\n        commit_info: CommitInfo,\n) -> None:\n    """\n    Verify commit date accuracy.\n    """\n    # Check if date exists\n    date_exists = evaluator.add_custom_node(\n        result=bool(commit_info.date and commit_info.date.strip()),\n        node_id="commit_date_exists",\n        description="Commit date is provided in the answer",\n        parent=parent_node,\n        critical=True,  # Critical - date is required\n    )\n\n    # Verify date correctness\n    date_correctness = evaluator.add_leaf(\n        id_="commit_date_correctness",\n        desc="Commit date matches the expected value (Dec 7, 2023)",\n        parent=parent_node,\n        critical=True,  # Critical - date must be correct\n    )\n\n    # Always perform verification - short-circuit logic will skip if existence failed\n    claim = f"The provided commit date \'{commit_info.date or \'N/A\'}\' matches the expected date \'{GROUND_TRUTH[\'date\']}\'"\n    await evaluator.verify(\n        claim=claim,\n        node=date_correctness,\n        sources=commit_info.source_urls if commit_info.source_urls else None,\n        additional_instruction="Allow reasonable date format variations (e.g., \'Dec 7, 2023\', \'December 7, 2023\', \'2023-12-07\') but the core date should match. Expected: Dec 7, 2023."\n    )\n\n\nasync def verify_authors_info(\n        evaluator: Evaluator,\n        parent_node: VerificationNode,\n        authors_info: AuthorsInfo,\n) -> None:\n    """\n    Verify authors information in parallel.\n    This is the second node in the sequential chain.\n    """\n    # Create parallel node for authors verification\n    authors_verification = evaluator.add_parallel(\n        id_="authors_verification",\n        desc="Verify all authors information in parallel",\n        parent=parent_node,\n        critical=False  # Non-critical to allow partial scoring\n    )\n\n    # Extract first 5 authors, pad with empty authors if needed\n    provided_authors = authors_info.authors[:5] if authors_info.authors else []\n\n    # Pad with empty AuthorInfo objects for missing authors\n    while len(provided_authors) < 5:\n        provided_authors.append(AuthorInfo())  # Empty author info\n\n    # Create verification nodes for each author position\n    for i, author in enumerate(provided_authors):\n        await verify_single_author(evaluator, authors_verification, author, i + 1)\n\n\nasync def verify_single_author(\n        evaluator: Evaluator,\n        parent_node: VerificationNode,\n        author: AuthorInfo,\n        author_number: int,\n) -> None:\n    """\n    Verify a single author\'s information.\n    """\n    # Create parallel node for this author\n    author_node = evaluator.add_parallel(\n        id_=f"author_{author_number}",\n        desc=f"Author {author_number} information verification",\n        parent=parent_node,\n        critical=False  # Non-critical to allow partial scoring across authors\n    )\n\n    # Check if author information exists\n    author_exists = evaluator.add_custom_node(\n        result=bool(author.name and author.name.strip()),\n        node_id=f"author_{author_number}_exists",\n        description=f"Author {author_number} name is provided",\n        parent=author_node,\n        critical=True  # Critical - if no name, this author slot is meaningless\n    )\n\n    # Verify name matches expected contributors\n    name_match_node = evaluator.add_leaf(\n        id_=f"author_{author_number}_name_match",\n        desc=f"Author {author_number} name matches one of the expected contributors",\n        parent=author_node,\n        critical=True,  # Critical - must match an expected author\n    )\n\n    # Always perform verification - short-circuit logic will skip if existence failed\n    expected_authors_str = ", ".join(GROUND_TRUTH[\'expected_authors\'])\n    author_name = author.real_name_from_profile if author.real_name_from_profile else author.name\n    claim = f"The name \'{author_name or \'N/A\'}\' matches one of the names in the following list: {expected_authors_str}"\n    await evaluator.verify(\n        claim=claim,\n        node=name_match_node,\n        sources=None,  # Simple name matching\n        additional_instruction="Allow variations like \'Arthur\' matching \'Arthur Zucker\', or reasonable name format differences. Expected authors: Younes B, Arthur (or Arthur Zucker), Shauray Singh, Lysandre Debut, Haotian Liu."\n    )\n\n    # Verify profile page is provided (non-critical)\n    profile_provided_node = evaluator.add_leaf(\n        id_=f"author_{author_number}_profile_provided",\n        desc=f"Author {author_number} GitHub profile page URL is provided",\n        parent=author_node,\n        critical=False,  # Non-critical - nice to have but not essential\n    )\n\n    profile_claim = f"This is a GitHub profile page for \'{author.name or \'N/A\'}\'"\n    await evaluator.verify(\n        claim=profile_claim,\n        node=profile_provided_node,\n        sources=author.profile_url,\n    )\n\n\n# Main evaluation entry point                                         \nasync def evaluate_answer(\n        client,  # LLMClient type not imported, use generic annotation\n        answer: str,\n        agent_name: str,\n        answer_name: str,\n        cache: CacheClass,\n        semaphore: asyncio.Semaphore,\n        logger: logging.Logger,\n        model: str = "o4-mini"\n) -> Dict[str, Any]:\n    """\n    Main evaluation function for the LLaVA commit finding task.\n\n    This function implements a sequential verification strategy:\n    1. First verify commit information (ID, date, provenance)\n    2. Then verify authors information (only if commit info is correct)\n\n    The sequential design ensures that if commit information is wrong,\n    author verification is automatically skipped via short-circuit logic.\n    """\n\n    # -------- 1. Initialize evaluator ----------------------------- #\n    evaluator = Evaluator()\n    root = evaluator.initialize(\n        task_id=TASK_ID,\n        strategy=AggregationStrategy.SEQUENTIAL,  # Sequential\n        agent_name=agent_name,\n        answer_name=answer_name,\n        # Evaluator creation parameters\n        client=client,\n        task_description=TASK_DESCRIPTION,\n        answer=answer,\n        global_cache=cache,\n        global_semaphore=semaphore,\n        logger=logger,\n        default_model=model,\n    )\n\n    # Record ground truth information\n    evaluator.add_ground_truth(GROUND_TRUTH, "expected_commit_and_authors_info")\n\n    # -------- 2. Extract structured information ------------------- #\n\n    # Extract basic commit information\n    commit_info = await evaluator.extract(\n        prompt=prompt_extract_commit_info(),\n        template_class=CommitInfo,\n        extraction_name="commit_extraction",\n        source=None,  # Extract from answer text\n    )\n\n    # Extract authors information\n    authors_info = await evaluator.extract(\n        prompt=prompt_extract_authors_info(),\n        template_class=AuthorsInfo,\n        extraction_name="authors_extraction",\n        source=None,  # Extract from answer text\n    )\n\n    # -------- 3. Build verification tree (Sequential) ------------- #\n\n    # Step 1: Verify commit information (non-critical for sequential scoring)\n    await verify_commit_info(evaluator, root, commit_info)\n\n    # Step 2: Verify authors information (will be skipped if commit info fails)\n    await verify_authors_info(evaluator, root, authors_info)\n\n    # -------- 4. Return evaluation results ------------------------ #\n    return evaluator.get_summary()\n\\end{lstlisting}\\begin{lstlisting}{python}\nimport asyncio\nimport logging\nfrom typing import Optional, List, Dict, Any\n\nfrom pydantic import BaseModel, Field\n\nfrom mind2web2 import CacheClass, Evaluator, VerificationNode, AggregationStrategy\n\nTASK_ID = "find_llava_commit"\nTASK_DESCRIPTION = """\nIdentify the first commit on the main branch of the official Hugging Face transformers repository that added support for the LLaVA model.\nPlease provide the following details about this commit: the short commit ID (first 7 characters), the date of the commit, a list of all contributors/authors involved in this commit. For each author, include a link to their GitHub profile page and the full real name displayed on their GitHub profile page.\n"""\n\nEVAL_NOTES = ""\nGROUND_TRUTH = {\n    "commit_id": "44b5506",\n    "date": "Dec 7, 2023",\n    "expected_authors": [\n        "Younes B",\n        "Arthur",  # Or Arthur Zucker\n        "Shauray Singh",\n        "Lysandre Debut",\n        "Haotian Liu"\n    ]\n}\n\n\nclass AuthorInfo(BaseModel):\n    """Data model for individual author information."""\n    name: Optional[str] = Field(default=None, description="Author name as provided in the answer")\n    profile_url: Optional[str] = Field(default=None, description="GitHub profile page URL")\n    real_name_from_profile: Optional[str] = Field(default=None, description="Real name extracted from profile page")\n\n\nclass CommitInfo(BaseModel):\n    """Data model for extracted commit information."""\n    commit_id: Optional[str] = Field(default=None, description="Short commit ID (7 characters)")\n    date: Optional[str] = Field(default=None, description="Date of the commit")\n    source_urls: Optional[List[str]] = Field(default_factory=list, description="Source URLs for commit verification")\n\n\nclass AuthorsInfo(BaseModel):\n    """Data model for extracted authors information."""\n    authors: Optional[List[AuthorInfo]] = Field(default_factory=list,\n                                                description="List of authors with their profile info")\n\n\ndef prompt_extract_commit_info() -> str:\n    """\n    Extraction prompt for getting basic commit information from the answer.\n    """\n    return """\n    Extract the basic commit information for the LLaVA model support from the answer.\n\n    Look for:\n    - commit_id: The short commit ID (typically 7 characters) \n    - date: The date when the commit was made\n    - source_urls: Any URLs that contain or reference this commit information (e.g., GitHub commit URLs, repository links)\n\n    Extract the information exactly as it appears in the text.\n    If any field is not mentioned, set it to null or empty list.\n    """\n\n\ndef prompt_extract_authors_info() -> str:\n    """\n    Extraction prompt for getting authors information from the answer.\n    """\n    return """\n    Extract all author information mentioned in the answer related to the LLaVA commit.\n\n    For each author, extract:\n    - name: The author\'s name as mentioned in the answer\n    - profile_url: Their GitHub profile page URL if provided\n    - real_name_from_profile: Their real name as stated to appear on their profile page\n\n    Extract information exactly as it appears in the text.\n    If any field is not mentioned for an author, set it to null.\n    Include all authors mentioned, even if some information is incomplete.\n    """\n\n\nasync def verify_commit_info(\n        evaluator: Evaluator,\n        parent_node: VerificationNode,\n        commit_info: CommitInfo,\n) -> None:\n    """\n    Verify commit information in sequential order.\n    This is the first node in the sequential chain.\n    """\n    # Create sequential node for commit verification steps\n    commit_verification = evaluator.add_sequential(\n        id_="commit_verification",\n        desc="Verify commit ID, date, and provenance in sequence",\n        parent=parent_node,\n        critical=False  # Non-critical to allow sequential partial scoring\n    )\n\n    # Step 1: Verify commit ID exists and is correct\n    await verify_commit_id(evaluator, commit_verification, commit_info)\n\n    # Step 2: Verify commit date\n    await verify_commit_date(evaluator, commit_verification, commit_info)\n\n\nasync def verify_commit_id(\n        evaluator: Evaluator,\n        parent_node: VerificationNode,\n        commit_info: CommitInfo,\n) -> None:\n    """\n    Verify commit ID existence, correctness and provenance.\n    """\n    # Create parallel node for commit ID checks\n    commit_id_node = evaluator.add_parallel(\n        id_="commit_id_verification",\n        desc="Verify commit ID existence, correctness and provenance",\n        parent=parent_node\n    )\n\n    # Check if commit ID exists\n    id_exists = evaluator.add_custom_node(\n        result=bool(commit_info.commit_id and commit_info.commit_id.strip() and commit_info.source_urls and commit_info.commit_id),\n        node_id="commit_id_exists",\n        description="Commit ID is provided in the answer",\n        parent=commit_id_node,\n        critical=True,  # Most critical - without ID, nothing else matters\n    )\n\n    # Verify commit ID correctness\n    id_correctness = evaluator.add_leaf(\n        id_="commit_id_correctness",\n        desc="Commit ID matches the expected value (44b5506)",\n        parent=commit_id_node,\n        critical=True,  # Critical - ID must be correct\n    )\n\n    # Always perform verification - short-circuit logic will skip if existence failed\n    claim = f"This ID (a github commit id) \'{commit_info.commit_id or \'N/A\'}\' matches this ID \'{GROUND_TRUTH[\'commit_id\']}\'"\n    await evaluator.verify(\n        claim=claim,\n        node=id_correctness,\n        sources=None,  # Simple comparison, no URL needed\n        additional_instruction="Allow minor formatting differences or extra descriptions but the core 7-character commit ID should exist and match exactly. Expected: 44b5506."\n    )\n\n    # Provenance check - verify the commit info is supported by sources\n    provenance_check = evaluator.add_leaf(\n        id_="commit_provenance",\n        desc="Commit information is supported by provided source URLs",\n        parent=commit_id_node,\n        critical=True,  # Critical - information must be substantiated\n    )\n\n    # Always perform verification - short-circuit logic will skip if needed\n    # if commit_info.source_urls and commit_info.commit_id:\n    claim = f"This page shows or mentioned the github commit ID: \'{commit_info.commit_id}\'. For example, if this is exactly the commit page"\n    await evaluator.verify(\n        claim=claim,\n        node=provenance_check,\n        sources=commit_info.source_urls,\n    )\n\n\nasync def verify_commit_date(\n        evaluator: Evaluator,\n        parent_node: VerificationNode,\n        commit_info: CommitInfo,\n) -> None:\n    """\n    Verify commit date accuracy.\n    """\n    # Check if date exists\n    date_exists = evaluator.add_custom_node(\n        result=bool(commit_info.date and commit_info.date.strip()),\n        node_id="commit_date_exists",\n        description="Commit date is provided in the answer",\n        parent=parent_node,\n        critical=True,  # Critical - date is required\n    )\n\n    # Verify date correctness\n    date_correctness = evaluator.add_leaf(\n        id_="commit_date_correctness",\n        desc="Commit date matches the expected value (Dec 7, 2023)",\n        parent=parent_node,\n        critical=True,  # Critical - date must be correct\n    )\n\n    # Always perform verification - short-circuit logic will skip if existence failed\n    claim = f"The provided commit date \'{commit_info.date or \'N/A\'}\' matches the expected date \'{GROUND_TRUTH[\'date\']}\'"\n    await evaluator.verify(\n        claim=claim,\n        node=date_correctness,\n        sources=commit_info.source_urls if commit_info.source_urls else None,\n        additional_instruction="Allow reasonable date format variations (e.g., \'Dec 7, 2023\', \'December 7, 2023\', \'2023-12-07\') but the core date should match. Expected: Dec 7, 2023."\n    )\n\n\nasync def verify_authors_info(\n        evaluator: Evaluator,\n        parent_node: VerificationNode,\n        authors_info: AuthorsInfo,\n) -> None:\n    """\n    Verify authors information in parallel.\n    This is the second node in the sequential chain.\n    """\n    # Create parallel node for authors verification\n    authors_verification = evaluator.add_parallel(\n        id_="authors_verification",\n        desc="Verify all authors information in parallel",\n        parent=parent_node,\n        critical=False  # Non-critical to allow partial scoring\n    )\n\n    # Extract first 5 authors, pad with empty authors if needed\n    provided_authors = authors_info.authors[:5] if authors_info.authors else []\n\n    # Pad with empty AuthorInfo objects for missing authors\n    while len(provided_authors) < 5:\n        provided_authors.append(AuthorInfo())  # Empty author info\n\n    # Create verification nodes for each author position\n    for i, author in enumerate(provided_authors):\n        await verify_single_author(evaluator, authors_verification, author, i + 1)\n\n\nasync def verify_single_author(\n        evaluator: Evaluator,\n        parent_node: VerificationNode,\n        author: AuthorInfo,\n        author_number: int,\n) -> None:\n    """\n    Verify a single author\'s information.\n    """\n    # Create parallel node for this author\n    author_node = evaluator.add_parallel(\n        id_=f"author_{author_number}",\n        desc=f"Author {author_number} information verification",\n        parent=parent_node,\n        critical=False  # Non-critical to allow partial scoring across authors\n    )\n\n    # Check if author information exists\n    author_exists = evaluator.add_custom_node(\n        result=bool(author.name and author.name.strip()),\n        node_id=f"author_{author_number}_exists",\n        description=f"Author {author_number} name is provided",\n        parent=author_node,\n        critical=True  # Critical - if no name, this author slot is meaningless\n    )\n\n    # Verify name matches expected contributors\n    name_match_node = evaluator.add_leaf(\n        id_=f"author_{author_number}_name_match",\n        desc=f"Author {author_number} name matches one of the expected contributors",\n        parent=author_node,\n        critical=True,  # Critical - must match an expected author\n    )\n\n    # Always perform verification - short-circuit logic will skip if existence failed\n    expected_authors_str = ", ".join(GROUND_TRUTH[\'expected_authors\'])\n    author_name = author.real_name_from_profile if author.real_name_from_profile else author.name\n    claim = f"The name \'{author_name or \'N/A\'}\' matches one of the names in the following list: {expected_authors_str}"\n    await evaluator.verify(\n        claim=claim,\n        node=name_match_node,\n        sources=None,  # Simple name matching\n        additional_instruction="Allow variations like \'Arthur\' matching \'Arthur Zucker\', or reasonable name format differences. Expected authors: Younes B, Arthur (or Arthur Zucker), Shauray Singh, Lysandre Debut, Haotian Liu."\n    )\n\n    # Verify profile page is provided (non-critical)\n    profile_provided_node = evaluator.add_leaf(\n        id_=f"author_{author_number}_profile_provided",\n        desc=f"Author {author_number} GitHub profile page URL is provided",\n        parent=author_node,\n        critical=False,  # Non-critical - nice to have but not essential\n    )\n\n    profile_claim = f"This is a GitHub profile page for \'{author.name or \'N/A\'}\'"\n    await evaluator.verify(\n        claim=profile_claim,\n        node=profile_provided_node,\n        sources=author.profile_url,\n    )\n\n\n# Main evaluation entry point                                         \nasync def evaluate_answer(\n        client,  # LLMClient type not imported, use generic annotation\n        answer: str,\n        agent_name: str,\n        answer_name: str,\n        cache: CacheClass,\n        semaphore: asyncio.Semaphore,\n        logger: logging.Logger,\n        model: str = "o4-mini"\n) -> Dict[str, Any]:\n    """\n    Main evaluation function for the LLaVA commit finding task.\n\n    This function implements a sequential verification strategy:\n    1. First verify commit information (ID, date, provenance)\n    2. Then verify authors information (only if commit info is correct)\n\n    The sequential design ensures that if commit information is wrong,\n    author verification is automatically skipped via short-circuit logic.\n    """\n\n    # -------- 1. Initialize evaluator ----------------------------- #\n    evaluator = Evaluator()\n    root = evaluator.initialize(\n        task_id=TASK_ID,\n        strategy=AggregationStrategy.SEQUENTIAL,  # Sequential\n        agent_name=agent_name,\n        answer_name=answer_name,\n        # Evaluator creation parameters\n        client=client,\n        task_description=TASK_DESCRIPTION,\n        answer=answer,\n        global_cache=cache,\n        global_semaphore=semaphore,\n        logger=logger,\n        default_model=model,\n    )\n\n    # Record ground truth information\n    evaluator.add_ground_truth(GROUND_TRUTH, "expected_commit_and_authors_info")\n\n    # -------- 2. Extract structured information ------------------- #\n\n    # Extract basic commit information\n    commit_info = await evaluator.extract(\n        prompt=prompt_extract_commit_info(),\n        template_class=CommitInfo,\n        extraction_name="commit_extraction",\n        source=None,  # Extract from answer text\n    )\n\n    # Extract authors information\n    authors_info = await evaluator.extract(\n        prompt=prompt_extract_authors_info(),\n        template_class=AuthorsInfo,\n        extraction_name="authors_extraction",\n        source=None,  # Extract from answer text\n    )\n\n    # -------- 3. Build verification tree (Sequential) ------------- #\n\n    # Step 1: Verify commit information (non-critical for sequential scoring)\n    await verify_commit_info(evaluator, root, commit_info)\n\n    # Step 2: Verify authors information (will be skipped if commit info fails)\n    await verify_authors_info(evaluator, root, authors_info)\n\n    # -------- 4. Return evaluation results ------------------------ #\n    return evaluator.get_summary()\n\\end{lstlisting}{python}python\nimport asyncio\nimport logging\nfrom typing import Optional, List, Dict, Any\n\nfrom pydantic import BaseModel, Field\n\nfrom mind2web2 import CacheClass, Evaluator, VerificationNode, AggregationStrategy\n\nTASK_ID = "find_llava_commit"\nTASK_DESCRIPTION = """\nIdentify the first commit on the main branch of the official Hugging Face transformers repository that added support for the LLaVA model.\nPlease provide the following details about this commit: the short commit ID (first 7 characters), the date of the commit, a list of all contributors/authors involved in this commit. For each author, include a link to their GitHub profile page and the full real name displayed on their GitHub profile page.\n"""\n\nEVAL_NOTES = ""\nGROUND_TRUTH = {\n    "commit_id": "44b5506",\n    "date": "Dec 7, 2023",\n    "expected_authors": [\n        "Younes B",\n        "Arthur",  # Or Arthur Zucker\n        "Shauray Singh",\n        "Lysandre Debut",\n        "Haotian Liu"\n    ]\n}\n    "commit_id": "44b5506",\n    "date": "Dec 7, 2023",\n    "expected_authors": [\n        "Younes B",\n        "Arthur",  # Or Arthur Zucker\n        "Shauray Singh",\n        "Lysandre Debut",\n        "Haotian Liu"\n    ]\n\n\n\nclass AuthorInfo(BaseModel):\n    """Data model for individual author information."""\n    name: Optional[str] = Field(default=None, description="Author name as provided in the answer")\n    profile_url: Optional[str] = Field(default=None, description="GitHub profile page URL")\n    real_name_from_profile: Optional[str] = Field(default=None, description="Real name extracted from profile page")\n\n\nclass CommitInfo(BaseModel):\n    """Data model for extracted commit information."""\n    commit_id: Optional[str] = Field(default=None, description="Short commit ID (7 characters)")\n    date: Optional[str] = Field(default=None, description="Date of the commit")\n    source_urls: Optional[List[str]] = Field(default_factory=list, description="Source URLs for commit verification")\n\n\nclass AuthorsInfo(BaseModel):\n    """Data model for extracted authors information."""\n    authors: Optional[List[AuthorInfo]] = Field(default_factory=list,\n                                                description="List of authors with their profile info")\n\n\ndef prompt_extract_commit_info() -> str:\n    """\n    Extraction prompt for getting basic commit information from the answer.\n    """\n    return """\n    Extract the basic commit information for the LLaVA model support from the answer.\n\n    Look for:\n    - commit_id: The short commit ID (typically 7 characters) \n    - date: The date when the commit was made\n    - source_urls: Any URLs that contain or reference this commit information (e.g., GitHub commit URLs, repository links)\n\n    Extract the information exactly as it appears in the text.\n    If any field is not mentioned, set it to null or empty list.\n    """\n\n\ndef prompt_extract_authors_info() -> str:\n    """\n    Extraction prompt for getting authors information from the answer.\n    """\n    return """\n    Extract all author information mentioned in the answer related to the LLaVA commit.\n\n    For each author, extract:\n    - name: The author\'s name as mentioned in the answer\n    - profile_url: Their GitHub profile page URL if provided\n    - real_name_from_profile: Their real name as stated to appear on their profile page\n\n    Extract information exactly as it appears in the text.\n    If any field is not mentioned for an author, set it to null.\n    Include all authors mentioned, even if some information is incomplete.\n    """\n\n\nasync def verify_commit_info(\n        evaluator: Evaluator,\n        parent_node: VerificationNode,\n        commit_info: CommitInfo,\n) -> None:\n    """\n    Verify commit information in sequential order.\n    This is the first node in the sequential chain.\n    """\n    # Create sequential node for commit verification steps\n    commit_verification = evaluator.add_sequential(\n        id_="commit_verification",\n        desc="Verify commit ID, date, and provenance in sequence",\n        parent=parent_node,\n        critical=False  # Non-critical to allow sequential partial scoring\n    )\n\n    # Step 1: Verify commit ID exists and is correct\n    await verify_commit_id(evaluator, commit_verification, commit_info)\n\n    # Step 2: Verify commit date\n    await verify_commit_date(evaluator, commit_verification, commit_info)\n\n\nasync def verify_commit_id(\n        evaluator: Evaluator,\n        parent_node: VerificationNode,\n        commit_info: CommitInfo,\n) -> None:\n    """\n    Verify commit ID existence, correctness and provenance.\n    """\n    # Create parallel node for commit ID checks\n    commit_id_node = evaluator.add_parallel(\n        id_="commit_id_verification",\n        desc="Verify commit ID existence, correctness and provenance",\n        parent=parent_node\n    )\n\n    # Check if commit ID exists\n    id_exists = evaluator.add_custom_node(\n        result=bool(commit_info.commit_id and commit_info.commit_id.strip() and commit_info.source_urls and commit_info.commit_id),\n        node_id="commit_id_exists",\n        description="Commit ID is provided in the answer",\n        parent=commit_id_node,\n        critical=True,  # Most critical - without ID, nothing else matters\n    )\n\n    # Verify commit ID correctness\n    id_correctness = evaluator.add_leaf(\n        id_="commit_id_correctness",\n        desc="Commit ID matches the expected value (44b5506)",\n        parent=commit_id_node,\n        critical=True,  # Critical - ID must be correct\n    )\n\n    # Always perform verification - short-circuit logic will skip if existence failed\n    claim = f"This ID (a github commit id) \'{commit_info.commit_id or \'N/A\'}commit_info.commit_id or \'N/A\'\' matches this ID \'{GROUND_TRUTH[\'commit_id\']}GROUND_TRUTH[\'commit_id\']\'"\n    await evaluator.verify(\n        claim=claim,\n        node=id_correctness,\n        sources=None,  # Simple comparison, no URL needed\n        additional_instruction="Allow minor formatting differences or extra descriptions but the core 7-character commit ID should exist and match exactly. Expected: 44b5506."\n    )\n\n    # Provenance check - verify the commit info is supported by sources\n    provenance_check = evaluator.add_leaf(\n        id_="commit_provenance",\n        desc="Commit information is supported by provided source URLs",\n        parent=commit_id_node,\n        critical=True,  # Critical - information must be substantiated\n    )\n\n    # Always perform verification - short-circuit logic will skip if needed\n    # if commit_info.source_urls and commit_info.commit_id:\n    claim = f"This page shows or mentioned the github commit ID: \'{commit_info.commit_id}commit_info.commit_id\'. For example, if this is exactly the commit page"\n    await evaluator.verify(\n        claim=claim,\n        node=provenance_check,\n        sources=commit_info.source_urls,\n    )\n\n\nasync def verify_commit_date(\n        evaluator: Evaluator,\n        parent_node: VerificationNode,\n        commit_info: CommitInfo,\n) -> None:\n    """\n    Verify commit date accuracy.\n    """\n    # Check if date exists\n    date_exists = evaluator.add_custom_node(\n        result=bool(commit_info.date and commit_info.date.strip()),\n        node_id="commit_date_exists",\n        description="Commit date is provided in the answer",\n        parent=parent_node,\n        critical=True,  # Critical - date is required\n    )\n\n    # Verify date correctness\n    date_correctness = evaluator.add_leaf(\n        id_="commit_date_correctness",\n        desc="Commit date matches the expected value (Dec 7, 2023)",\n        parent=parent_node,\n        critical=True,  # Critical - date must be correct\n    )\n\n    # Always perform verification - short-circuit logic will skip if existence failed\n    claim = f"The provided commit date \'{commit_info.date or \'N/A\'}commit_info.date or \'N/A\'\' matches the expected date \'{GROUND_TRUTH[\'date\']}GROUND_TRUTH[\'date\']\'"\n    await evaluator.verify(\n        claim=claim,\n        node=date_correctness,\n        sources=commit_info.source_urls if commit_info.source_urls else None,\n        additional_instruction="Allow reasonable date format variations (e.g., \'Dec 7, 2023\', \'December 7, 2023\', \'2023-12-07\') but the core date should match. Expected: Dec 7, 2023."\n    )\n\n\nasync def verify_authors_info(\n        evaluator: Evaluator,\n        parent_node: VerificationNode,\n        authors_info: AuthorsInfo,\n) -> None:\n    """\n    Verify authors information in parallel.\n    This is the second node in the sequential chain.\n    """\n    # Create parallel node for authors verification\n    authors_verification = evaluator.add_parallel(\n        id_="authors_verification",\n        desc="Verify all authors information in parallel",\n        parent=parent_node,\n        critical=False  # Non-critical to allow partial scoring\n    )\n\n    # Extract first 5 authors, pad with empty authors if needed\n    provided_authors = authors_info.authors[:5] if authors_info.authors else []\n\n    # Pad with empty AuthorInfo objects for missing authors\n    while len(provided_authors) < 5:\n        provided_authors.append(AuthorInfo())  # Empty author info\n\n    # Create verification nodes for each author position\n    for i, author in enumerate(provided_authors):\n        await verify_single_author(evaluator, authors_verification, author, i + 1)\n\n\nasync def verify_single_author(\n        evaluator: Evaluator,\n        parent_node: VerificationNode,\n        author: AuthorInfo,\n        author_number: int,\n) -> None:\n    """\n    Verify a single author\'s information.\n    """\n    # Create parallel node for this author\n    author_node = evaluator.add_parallel(\n        id_=f"author_{author_number}author_number",\n        desc=f"Author {author_number}author_number information verification",\n        parent=parent_node,\n        critical=False  # Non-critical to allow partial scoring across authors\n    )\n\n    # Check if author information exists\n    author_exists = evaluator.add_custom_node(\n        result=bool(author.name and author.name.strip()),\n        node_id=f"author_{author_number}author_number_exists",\n        description=f"Author {author_number}author_number name is provided",\n        parent=author_node,\n        critical=True  # Critical - if no name, this author slot is meaningless\n    )\n\n    # Verify name matches expected contributors\n    name_match_node = evaluator.add_leaf(\n        id_=f"author_{author_number}author_number_name_match",\n        desc=f"Author {author_number}author_number name matches one of the expected contributors",\n        parent=author_node,\n        critical=True,  # Critical - must match an expected author\n    )\n\n    # Always perform verification - short-circuit logic will skip if existence failed\n    expected_authors_str = ", ".join(GROUND_TRUTH[\'expected_authors\'])\n    author_name = author.real_name_from_profile if author.real_name_from_profile else author.name\n    claim = f"The name \'{author_name or \'N/A\'}author_name or \'N/A\'\' matches one of the names in the following list: {expected_authors_str}expected_authors_str"\n    await evaluator.verify(\n        claim=claim,\n        node=name_match_node,\n        sources=None,  # Simple name matching\n        additional_instruction="Allow variations like \'Arthur\' matching \'Arthur Zucker\', or reasonable name format differences. Expected authors: Younes B, Arthur (or Arthur Zucker), Shauray Singh, Lysandre Debut, Haotian Liu."\n    )\n\n    # Verify profile page is provided (non-critical)\n    profile_provided_node = evaluator.add_leaf(\n        id_=f"author_{author_number}author_number_profile_provided",\n        desc=f"Author {author_number}author_number GitHub profile page URL is provided",\n        parent=author_node,\n        critical=False,  # Non-critical - nice to have but not essential\n    )\n\n    profile_claim = f"This is a GitHub profile page for \'{author.name or \'N/A\'}author.name or \'N/A\'\'"\n    await evaluator.verify(\n        claim=profile_claim,\n        node=profile_provided_node,\n        sources=author.profile_url,\n    )\n\n\n# Main evaluation entry point                                         \nasync def evaluate_answer(\n        client,  # LLMClient type not imported, use generic annotation\n        answer: str,\n        agent_name: str,\n        answer_name: str,\n        cache: CacheClass,\n        semaphore: asyncio.Semaphore,\n        logger: logging.Logger,\n        model: str = "o4-mini"\n) -> Dict[str, Any]:\n    """\n    Main evaluation function for the LLaVA commit finding task.\n\n    This function implements a sequential verification strategy:\n    1. First verify commit information (ID, date, provenance)\n    2. Then verify authors information (only if commit info is correct)\n\n    The sequential design ensures that if commit information is wrong,\n    author verification is automatically skipped via short-circuit logic.\n    """\n\n    # -------- 1. Initialize evaluator ----------------------------- #\n    evaluator = Evaluator()\n    root = evaluator.initialize(\n        task_id=TASK_ID,\n        strategy=AggregationStrategy.SEQUENTIAL,  # Sequential\n        agent_name=agent_name,\n        answer_name=answer_name,\n        # Evaluator creation parameters\n        client=client,\n        task_description=TASK_DESCRIPTION,\n        answer=answer,\n        global_cache=cache,\n        global_semaphore=semaphore,\n        logger=logger,\n        default_model=model,\n    )\n\n    # Record ground truth information\n    evaluator.add_ground_truth(GROUND_TRUTH, "expected_commit_and_authors_info")\n\n    # -------- 2. Extract structured information ------------------- #\n\n    # Extract basic commit information\n    commit_info = await evaluator.extract(\n        prompt=prompt_extract_commit_info(),\n        template_class=CommitInfo,\n        extraction_name="commit_extraction",\n        source=None,  # Extract from answer text\n    )\n\n    # Extract authors information\n    authors_info = await evaluator.extract(\n        prompt=prompt_extract_authors_info(),\n        template_class=AuthorsInfo,\n        extraction_name="authors_extraction",\n        source=None,  # Extract from answer text\n    )\n\n    # -------- 3. Build verification tree (Sequential) ------------- #\n\n    # Step 1: Verify commit information (non-critical for sequential scoring)\n    await verify_commit_info(evaluator, root, commit_info)\n\n    # Step 2: Verify authors information (will be skipped if commit info fails)\n    await verify_authors_info(evaluator, root, authors_info)\n\n    # -------- 4. Return evaluation results ------------------------ #\n    return evaluator.get_summary()\n\n\n\\newpage\n', 'appendix': True}, 'Instructions for Human Annotators': {'content': '\n\n\\subsection{Instructions for Task Collection}\n\n\\begin{tcolorbox}[colframe=gray!50!black, colback=gray!10!white, title=Task Proposal Instruction, breakable]\n\\small\n\n\nTraditional web searches often require carefully constructed queries, limiting the ability to address complex, natural language requests. Agentic search methods—such as PerplexityAI, ChatGPT Search, and Deep Research—enable users to pose more complex queries naturally. However, comprehensive benchmarks to evaluate these agentic search methods are currently lacking.\nOur objective is to develop the first extensive and rigorous benchmark for evaluating agentic search capabilities. Your contributions in proposing challenging, diverse, and verifiable tasks are crucial to achieving this goal.\n\n\\medskip\n \n\\textbf{Criteria for Task Proposals}\n\n\\begin{itemize}[leftmargin=1.5em]\n\\item Tasks should:\n    \\begin{itemize}\n        \\item Be information-seeking.\n        \\item Be realistic, reflecting genuine scenarios or previously encountered problems.\n        \\item Be tedious and sufficiently complex, requiring multiple intermediate steps and taking at least five minutes for a human to complete.\n        \\item Be verifiable. The agent’s response must include text and reference URLs or be verifiable through established ground truth.\n        \\item Be single-round tasks (no user clarification required); all necessary information must be clearly included in the description.\n    \\end{itemize}\n\n\n\\item Tasks to avoid:\n    \\begin{itemize}\n        \\item Tasks requiring user logins.\n        \\item Simple or quickly resolvable tasks.\n        \\item Tasks with global qualifiers (e.g., "cheapest," "best") that cannot be reliably verified.\n        \\item Tasks containing vague or subjective elements (e.g., "nice restaurant").\n    \\end{itemize}\n\\end{itemize}\n\n\\end{tcolorbox}\\begin{tcolorbox}[colframe=gray!50!black, colback=gray!10!white, title=Task Proposal Instruction, breakable]\n\\small\n\n\nTraditional web searches often require carefully constructed queries, limiting the ability to address complex, natural language requests. Agentic search methods—such as PerplexityAI, ChatGPT Search, and Deep Research—enable users to pose more complex queries naturally. However, comprehensive benchmarks to evaluate these agentic search methods are currently lacking.\nOur objective is to develop the first extensive and rigorous benchmark for evaluating agentic search capabilities. Your contributions in proposing challenging, diverse, and verifiable tasks are crucial to achieving this goal.\n\n\\medskip\n \n\\textbf{Criteria for Task Proposals}\n\n\\begin{itemize}[leftmargin=1.5em]\n\\item Tasks should:\n    \\begin{itemize}\n        \\item Be information-seeking.\n        \\item Be realistic, reflecting genuine scenarios or previously encountered problems.\n        \\item Be tedious and sufficiently complex, requiring multiple intermediate steps and taking at least five minutes for a human to complete.\n        \\item Be verifiable. The agent’s response must include text and reference URLs or be verifiable through established ground truth.\n        \\item Be single-round tasks (no user clarification required); all necessary information must be clearly included in the description.\n    \\end{itemize}\n\n\n\\item Tasks to avoid:\n    \\begin{itemize}\n        \\item Tasks requiring user logins.\n        \\item Simple or quickly resolvable tasks.\n        \\item Tasks with global qualifiers (e.g., "cheapest," "best") that cannot be reliably verified.\n        \\item Tasks containing vague or subjective elements (e.g., "nice restaurant").\n    \\end{itemize}\n\\end{itemize}\n\n\\end{tcolorbox}[colframe=gray!50!black, colback=gray!10!white, title=Task Proposal Instruction, breakable]\n\\small\n\n\nTraditional web searches often require carefully constructed queries, limiting the ability to address complex, natural language requests. Agentic search methods—such as PerplexityAI, ChatGPT Search, and Deep Research—enable users to pose more complex queries naturally. However, comprehensive benchmarks to evaluate these agentic search methods are currently lacking.\nOur objective is to develop the first extensive and rigorous benchmark for evaluating agentic search capabilities. Your contributions in proposing challenging, diverse, and verifiable tasks are crucial to achieving this goal.\n\n\\medskip\n \n\\textbf{Criteria for Task Proposals}\n\n\n\\item Tasks should:\n    \n        \\item Be information-seeking.\n        \\item Be realistic, reflecting genuine scenarios or previously encountered problems.\n        \\item Be tedious and sufficiently complex, requiring multiple intermediate steps and taking at least five minutes for a human to complete.\n        \\item Be verifiable. The agent’s response must include text and reference URLs or be verifiable through established ground truth.\n        \\item Be single-round tasks (no user clarification required); all necessary information must be clearly included in the description.\n    \n\n\n\\item Tasks to avoid:\n    \n        \\item Tasks requiring user logins.\n        \\item Simple or quickly resolvable tasks.\n        \\item Tasks with global qualifiers (e.g., "cheapest," "best") that cannot be reliably verified.\n        \\item Tasks containing vague or subjective elements (e.g., "nice restaurant").\n    \n\n\n\n\n\n\\begin{tcolorbox}[colframe=gray!50!black, colback=gray!10!white, title=Task Refinement Instruction, breakable]\n\\small\n\nWe are conducting task refinement to ensure that all proposed tasks consistently meet our quality standards. We provide a structured checklist to help you evaluate whether each task aligns with our specified criteria. Please systematically go through these checks to assess and refine the tasks accordingly.\n\n\\medskip\n\n\\textbf{Checklist}\n\n\\begin{enumerate}[leftmargin=1.5em]\n\n\\item Realism\n\n\\begin{enumerate}[leftmargin=1.5em, label=\\alph*.]\n    \\item Verify the task reflects real-world scenarios. Imagine yourself or someone you know performing this task in real-life.\n    \\item Verify the task is not artificially combining many simple steps to increase complexity or tediousness.\n\\end{enumerate}\n\n\\item[] \\emph{Note}:\n\\begin{itemize}[leftmargin=1.5em]\n    \\item Subjectivity is acceptable here since people have different practical needs.\n\\end{itemize}\n\n\n\\item Clarity and Objectivity\n\n\\begin{enumerate}[leftmargin=1.5em, label=\\alph*.]\n    \\item Ensure the task description is typo-free and grammatically correct.\n    \\item Verify that the description is clear and understandable.\n    \\item Ensure the task explicitly state necessary background knowledge needed to complete the task.\n    \\item Make sure the task criteria are objective and unambiguous:\n    \\begin{itemize}\n        \\item Avoid subjective terms (e.g., “nice”, “good”).\n        \\item Avoid vague terms (e.g., “effective”, “better”) and use precise, measurable language instead.\n    \\end{itemize}\n    \\item Ensure there are no alternative interpretations of the task. \n\n\\end{enumerate}\n\n\n\\item Tediousness and Feasibility\n\n\\begin{enumerate}[leftmargin=1.5em, label=\\alph*.]\n    \\item Confirm that the task takes more than 5 minutes to complete. Try performing the task, ensure it can\'t be quickly solved with only one or two simple searches.\n    \\item Confirm that the task required information can be found on publicly accessible websites where no login or paywall required.\n\\end{enumerate}\n\n\\item[] \\emph{Note}:\n\\begin{itemize}[leftmargin=1.5em]\n    \\item Familiarity with the task might cause you to underestimate the actual completion time.\n    \\item Tasks don\'t need to specifically challenge particular AI systems (e.g., Perplexity AI, ChatGPT Search, Deep Research).\n\\end{itemize}\n\n\n\\item Verifiability\n\n\\begin{enumerate}[leftmargin=1.5em, label=\\alph*.]\n    \\item Ensure task verifiability.\n\n    \\begin{enumerate}[leftmargin=1.5em, label=\\roman*.]\n        \\item Draft an outline of the expected answer as a sanity check, as well as to assist future task validation. The outline should include:\n        \\begin{itemize}\n            \\item All critical information explicitly required by the task, OR\n            \\item Information that, while not explicitly requested, should reasonably be included based on common sense.\n        \\end{itemize}\n        \\item Ensure that the information in the outline is sufficient to verify whether an answer satisfies the following principles by using our verification and helper tools:\n        \\begin{itemize}\n            \\item \\textit{Correctness}: The answer meets all task requirements.\n            \\item \\textit{Source Attribution}: Each fact is supported by at least one URL provenance.\n        \\end{itemize}\n\n    \\end{enumerate}\n\\end{enumerate}\n\n\\item[] \\emph{Verification and Helper Tools:}\n\n\\begin{itemize}[leftmargin=1.5em]\n    \\item \\textit{Simple LLM-as-a-Judge:}\n    \\begin{itemize}\n        \\item Use LLMs for verifying simple logic, common-sense reasoning, and universally known facts.\n    \\end{itemize}\n\n    \\item \\textit{URL-based LLM-as-a-Judge:}\n    \\begin{itemize}\n        \\item Given a statement and URL, use an LLM to confirm whether the statement is supported by the webpage content or a screenshot.\n    \\end{itemize}\n\n    \\item \\textit{Google Maps API:}\n    \\begin{itemize}\n        \\item Given an address, retrieve the city or sub-city name (Geocoding).\n        \\item Calculate travel time between two addresses (driving, walking, or public transit).\n        \\item Calculate travel distance between two addresses (driving, walking, or public transit).\n    \\end{itemize}\n\n    \\item \\textit{arXiv API:}\n    \\begin{itemize}\n        \\item Search for academic papers by title and retrieve their arXiv ID.\n        \\item Retrieve detailed information about a paper using its arXiv ID or URL.\n    \\end{itemize}\n\\end{itemize}\n\n\\item[] \\emph{Note:}\n\n\\begin{itemize}[leftmargin=1.5em]\n    \\item Be cautious with tasks involving global qualifiers (e.g., “list all”, “top-k”), you must ensure such answers can be verified.\n    \\item Please beware that our current URL-based LLM-as-a-Judge cannot obtain information from webpages that load content dynamically with additional clicks or interactions. Please avoid tasks that depends on such information for verification.   \n    \\item If verification seems challenging for a specific task, please discuss with us about approaches to assist verification, such as adding helper tools.\n\\end{itemize}\n\n\\item Additional Considerations\n\n\\begin{itemize}[leftmargin=1.5em]\n    \\item Tasks involving video understanding or non-English websites are currently not supported.\n    \\item Avoid tasks with rapidly changing answers (e.g., stock prices, exchange rates).\n    \\item Avoid tasks requiring extensive reasoning, complex calculations, or external tools (e.g., Python, calculators). The current focus is on information gathering via web browsing.\n    \\item If you find a task interesting but borderline according to these guidelines, discuss it with the team for further consideration.\n\\end{itemize}\n\n\n\\end{enumerate}\n\n\n\\end{tcolorbox}\\begin{tcolorbox}[colframe=gray!50!black, colback=gray!10!white, title=Task Refinement Instruction, breakable]\n\\small\n\nWe are conducting task refinement to ensure that all proposed tasks consistently meet our quality standards. We provide a structured checklist to help you evaluate whether each task aligns with our specified criteria. Please systematically go through these checks to assess and refine the tasks accordingly.\n\n\\medskip\n\n\\textbf{Checklist}\n\n\\begin{enumerate}[leftmargin=1.5em]\n\n\\item Realism\n\n\\begin{enumerate}[leftmargin=1.5em, label=\\alph*.]\n    \\item Verify the task reflects real-world scenarios. Imagine yourself or someone you know performing this task in real-life.\n    \\item Verify the task is not artificially combining many simple steps to increase complexity or tediousness.\n\\end{enumerate}\n\n\\item[] \\emph{Note}:\n\\begin{itemize}[leftmargin=1.5em]\n    \\item Subjectivity is acceptable here since people have different practical needs.\n\\end{itemize}\n\n\n\\item Clarity and Objectivity\n\n\\begin{enumerate}[leftmargin=1.5em, label=\\alph*.]\n    \\item Ensure the task description is typo-free and grammatically correct.\n    \\item Verify that the description is clear and understandable.\n    \\item Ensure the task explicitly state necessary background knowledge needed to complete the task.\n    \\item Make sure the task criteria are objective and unambiguous:\n    \\begin{itemize}\n        \\item Avoid subjective terms (e.g., “nice”, “good”).\n        \\item Avoid vague terms (e.g., “effective”, “better”) and use precise, measurable language instead.\n    \\end{itemize}\n    \\item Ensure there are no alternative interpretations of the task. \n\n\\end{enumerate}\n\n\n\\item Tediousness and Feasibility\n\n\\begin{enumerate}[leftmargin=1.5em, label=\\alph*.]\n    \\item Confirm that the task takes more than 5 minutes to complete. Try performing the task, ensure it can\'t be quickly solved with only one or two simple searches.\n    \\item Confirm that the task required information can be found on publicly accessible websites where no login or paywall required.\n\\end{enumerate}\n\n\\item[] \\emph{Note}:\n\\begin{itemize}[leftmargin=1.5em]\n    \\item Familiarity with the task might cause you to underestimate the actual completion time.\n    \\item Tasks don\'t need to specifically challenge particular AI systems (e.g., Perplexity AI, ChatGPT Search, Deep Research).\n\\end{itemize}\n\n\n\\item Verifiability\n\n\\begin{enumerate}[leftmargin=1.5em, label=\\alph*.]\n    \\item Ensure task verifiability.\n\n    \\begin{enumerate}[leftmargin=1.5em, label=\\roman*.]\n        \\item Draft an outline of the expected answer as a sanity check, as well as to assist future task validation. The outline should include:\n        \\begin{itemize}\n            \\item All critical information explicitly required by the task, OR\n            \\item Information that, while not explicitly requested, should reasonably be included based on common sense.\n        \\end{itemize}\n        \\item Ensure that the information in the outline is sufficient to verify whether an answer satisfies the following principles by using our verification and helper tools:\n        \\begin{itemize}\n            \\item \\textit{Correctness}: The answer meets all task requirements.\n            \\item \\textit{Source Attribution}: Each fact is supported by at least one URL provenance.\n        \\end{itemize}\n\n    \\end{enumerate}\n\\end{enumerate}\n\n\\item[] \\emph{Verification and Helper Tools:}\n\n\\begin{itemize}[leftmargin=1.5em]\n    \\item \\textit{Simple LLM-as-a-Judge:}\n    \\begin{itemize}\n        \\item Use LLMs for verifying simple logic, common-sense reasoning, and universally known facts.\n    \\end{itemize}\n\n    \\item \\textit{URL-based LLM-as-a-Judge:}\n    \\begin{itemize}\n        \\item Given a statement and URL, use an LLM to confirm whether the statement is supported by the webpage content or a screenshot.\n    \\end{itemize}\n\n    \\item \\textit{Google Maps API:}\n    \\begin{itemize}\n        \\item Given an address, retrieve the city or sub-city name (Geocoding).\n        \\item Calculate travel time between two addresses (driving, walking, or public transit).\n        \\item Calculate travel distance between two addresses (driving, walking, or public transit).\n    \\end{itemize}\n\n    \\item \\textit{arXiv API:}\n    \\begin{itemize}\n        \\item Search for academic papers by title and retrieve their arXiv ID.\n        \\item Retrieve detailed information about a paper using its arXiv ID or URL.\n    \\end{itemize}\n\\end{itemize}\n\n\\item[] \\emph{Note:}\n\n\\begin{itemize}[leftmargin=1.5em]\n    \\item Be cautious with tasks involving global qualifiers (e.g., “list all”, “top-k”), you must ensure such answers can be verified.\n    \\item Please beware that our current URL-based LLM-as-a-Judge cannot obtain information from webpages that load content dynamically with additional clicks or interactions. Please avoid tasks that depends on such information for verification.   \n    \\item If verification seems challenging for a specific task, please discuss with us about approaches to assist verification, such as adding helper tools.\n\\end{itemize}\n\n\\item Additional Considerations\n\n\\begin{itemize}[leftmargin=1.5em]\n    \\item Tasks involving video understanding or non-English websites are currently not supported.\n    \\item Avoid tasks with rapidly changing answers (e.g., stock prices, exchange rates).\n    \\item Avoid tasks requiring extensive reasoning, complex calculations, or external tools (e.g., Python, calculators). The current focus is on information gathering via web browsing.\n    \\item If you find a task interesting but borderline according to these guidelines, discuss it with the team for further consideration.\n\\end{itemize}\n\n\n\\end{enumerate}\n\n\n\\end{tcolorbox}[colframe=gray!50!black, colback=gray!10!white, title=Task Refinement Instruction, breakable]\n\\small\n\nWe are conducting task refinement to ensure that all proposed tasks consistently meet our quality standards. We provide a structured checklist to help you evaluate whether each task aligns with our specified criteria. Please systematically go through these checks to assess and refine the tasks accordingly.\n\n\\medskip\n\n\\textbf{Checklist}\n\n\n\n\\item Realism\n\n\n    \\item Verify the task reflects real-world scenarios. Imagine yourself or someone you know performing this task in real-life.\n    \\item Verify the task is not artificially combining many simple steps to increase complexity or tediousness.\n\n\n\\item[] \\emph{Note}:\n\n    \\item Subjectivity is acceptable here since people have different practical needs.\n\n\n\n\\item Clarity and Objectivity\n\n\n    \\item Ensure the task description is typo-free and grammatically correct.\n    \\item Verify that the description is clear and understandable.\n    \\item Ensure the task explicitly state necessary background knowledge needed to complete the task.\n    \\item Make sure the task criteria are objective and unambiguous:\n    \n        \\item Avoid subjective terms (e.g., “nice”, “good”).\n        \\item Avoid vague terms (e.g., “effective”, “better”) and use precise, measurable language instead.\n    \n    \\item Ensure there are no alternative interpretations of the task. \n\n\n\n\n\\item Tediousness and Feasibility\n\n\n    \\item Confirm that the task takes more than 5 minutes to complete. Try performing the task, ensure it can\'t be quickly solved with only one or two simple searches.\n    \\item Confirm that the task required information can be found on publicly accessible websites where no login or paywall required.\n\n\n\\item[] \\emph{Note}:\n\n    \\item Familiarity with the task might cause you to underestimate the actual completion time.\n    \\item Tasks don\'t need to specifically challenge particular AI systems (e.g., Perplexity AI, ChatGPT Search, Deep Research).\n\n\n\n\\item Verifiability\n\n\n    \\item Ensure task verifiability.\n\n    \n        \\item Draft an outline of the expected answer as a sanity check, as well as to assist future task validation. The outline should include:\n        \n            \\item All critical information explicitly required by the task, OR\n            \\item Information that, while not explicitly requested, should reasonably be included based on common sense.\n        \n        \\item Ensure that the information in the outline is sufficient to verify whether an answer satisfies the following principles by using our verification and helper tools:\n        \n            \\item \\textit{Correctness}: The answer meets all task requirements.\n            \\item \\textit{Source Attribution}: Each fact is supported by at least one URL provenance.\n        \n\n    \n\n\n\\item[] \\emph{Verification and Helper Tools:}\n\n\n    \\item \\textit{Simple LLM-as-a-Judge:}\n    \n        \\item Use LLMs for verifying simple logic, common-sense reasoning, and universally known facts.\n    \n\n    \\item \\textit{URL-based LLM-as-a-Judge:}\n    \n        \\item Given a statement and URL, use an LLM to confirm whether the statement is supported by the webpage content or a screenshot.\n    \n\n    \\item \\textit{Google Maps API:}\n    \n        \\item Given an address, retrieve the city or sub-city name (Geocoding).\n        \\item Calculate travel time between two addresses (driving, walking, or public transit).\n        \\item Calculate travel distance between two addresses (driving, walking, or public transit).\n    \n\n    \\item \\textit{arXiv API:}\n    \n        \\item Search for academic papers by title and retrieve their arXiv ID.\n        \\item Retrieve detailed information about a paper using its arXiv ID or URL.\n    \n\n\n\\item[] \\emph{Note:}\n\n\n    \\item Be cautious with tasks involving global qualifiers (e.g., “list all”, “top-k”), you must ensure such answers can be verified.\n    \\item Please beware that our current URL-based LLM-as-a-Judge cannot obtain information from webpages that load content dynamically with additional clicks or interactions. Please avoid tasks that depends on such information for verification.   \n    \\item If verification seems challenging for a specific task, please discuss with us about approaches to assist verification, such as adding helper tools.\n\n\n\\item Additional Considerations\n\n\n    \\item Tasks involving video understanding or non-English websites are currently not supported.\n    \\item Avoid tasks with rapidly changing answers (e.g., stock prices, exchange rates).\n    \\item Avoid tasks requiring extensive reasoning, complex calculations, or external tools (e.g., Python, calculators). The current focus is on information gathering via web browsing.\n    \\item If you find a task interesting but borderline according to these guidelines, discuss it with the team for further consideration.\n\n\n\n\n\n\n\n\n\n\\begin{tcolorbox}[colframe=gray!50!black, colback=gray!10!white, title=Task Validation Instruction, breakable]\n\\small\n\nWe are conducting task validation to rigorously ensure that all proposed tasks consistently meet our quality standards and are practically evaluable within our evaluation framework. During validation, please fully complete each task end-to-end, paying particular attention to potential ambiguities, overlooked edge cases, and the verifiability of all required information.\n\nYou should closely follow the structured checklist provided in the Task Refinement Instruction to evaluate realism, clarity, tediousness, and overall feasibility. Additionally, please specifically emphasize the following unique validation aspects:\n\n\\medskip\n\n\\textbf{Checklist Addendum for Validation}\n\n\\begin{enumerate}[leftmargin=1.5em]\n\n\\item \\textbf{Full Completion and End-to-End Testing}\n\\begin{itemize}[leftmargin=1.5em]\n\\item Fully perform the task yourself from start to finish. Ensure all or most critical information can be practically located and verified from the URL sources.\n\\end{itemize}\n\n\\item \\textbf{Feasibility of URL-based Verification}\n\\begin{itemize}[leftmargin=1.5em]\n\\item Verify that each URL provided as an information source uniquely and directly supports the expected statement.\n\\item Avoid scenarios that are beyond our evaluation framework capabilities, such as:\n\\begin{itemize}[leftmargin=1.5em]\n\\item Tasks requiring simultaneous verification from multiple distinct webpages, where the verification cannot be decomposed into independent single-page validations.\n\\item Tasks where the critical information is dynamically loaded, hidden, or collapsed on the webpage, making it inaccessible from the static HTML or initial page rendering.\n\\item Tasks that require additional webpage interactions beyond simple navigation (e.g., clicking multiple buttons, performing searches within the site, or scrolling to trigger dynamic loading).\n\\item Tasks where URLs provided are not unique or stable (e.g., search result URLs that frequently change).\n\\item Tasks requiring login credentials (verify this using your browser\'s incognito mode).\n\\end{itemize}\n\\end{itemize}\n\n\\item \\textbf{Explicit Ground Truth and Evaluation Notes}\n\\begin{itemize}[leftmargin=1.5em]\n\\item Clearly document the related sources and information that can be helpful for understanding the task. \n\n\\item If ground-truth information is necessary for certain criteria, note them down and carefully validate its correctness.\n\n\\item Provide explicit notes for potentially tricky evaluation scenarios or subtleties that might be easily overlooked or incorrectly handled during judge agent development.\n\n\\end{itemize}\n\n\\end{enumerate}\n\n\\medskip\n\nIf during validation you identify tasks that seem borderline or ambiguous with respect to our framework capabilities, promptly discuss these tasks within the team to determine their suitability or necessary adjustments.\n\n\\end{tcolorbox}\\begin{tcolorbox}[colframe=gray!50!black, colback=gray!10!white, title=Task Validation Instruction, breakable]\n\\small\n\nWe are conducting task validation to rigorously ensure that all proposed tasks consistently meet our quality standards and are practically evaluable within our evaluation framework. During validation, please fully complete each task end-to-end, paying particular attention to potential ambiguities, overlooked edge cases, and the verifiability of all required information.\n\nYou should closely follow the structured checklist provided in the Task Refinement Instruction to evaluate realism, clarity, tediousness, and overall feasibility. Additionally, please specifically emphasize the following unique validation aspects:\n\n\\medskip\n\n\\textbf{Checklist Addendum for Validation}\n\n\\begin{enumerate}[leftmargin=1.5em]\n\n\\item \\textbf{Full Completion and End-to-End Testing}\n\\begin{itemize}[leftmargin=1.5em]\n\\item Fully perform the task yourself from start to finish. Ensure all or most critical information can be practically located and verified from the URL sources.\n\\end{itemize}\n\n\\item \\textbf{Feasibility of URL-based Verification}\n\\begin{itemize}[leftmargin=1.5em]\n\\item Verify that each URL provided as an information source uniquely and directly supports the expected statement.\n\\item Avoid scenarios that are beyond our evaluation framework capabilities, such as:\n\\begin{itemize}[leftmargin=1.5em]\n\\item Tasks requiring simultaneous verification from multiple distinct webpages, where the verification cannot be decomposed into independent single-page validations.\n\\item Tasks where the critical information is dynamically loaded, hidden, or collapsed on the webpage, making it inaccessible from the static HTML or initial page rendering.\n\\item Tasks that require additional webpage interactions beyond simple navigation (e.g., clicking multiple buttons, performing searches within the site, or scrolling to trigger dynamic loading).\n\\item Tasks where URLs provided are not unique or stable (e.g., search result URLs that frequently change).\n\\item Tasks requiring login credentials (verify this using your browser\'s incognito mode).\n\\end{itemize}\n\\end{itemize}\n\n\\item \\textbf{Explicit Ground Truth and Evaluation Notes}\n\\begin{itemize}[leftmargin=1.5em]\n\\item Clearly document the related sources and information that can be helpful for understanding the task. \n\n\\item If ground-truth information is necessary for certain criteria, note them down and carefully validate its correctness.\n\n\\item Provide explicit notes for potentially tricky evaluation scenarios or subtleties that might be easily overlooked or incorrectly handled during judge agent development.\n\n\\end{itemize}\n\n\\end{enumerate}\n\n\\medskip\n\nIf during validation you identify tasks that seem borderline or ambiguous with respect to our framework capabilities, promptly discuss these tasks within the team to determine their suitability or necessary adjustments.\n\n\\end{tcolorbox}[colframe=gray!50!black, colback=gray!10!white, title=Task Validation Instruction, breakable]\n\\small\n\nWe are conducting task validation to rigorously ensure that all proposed tasks consistently meet our quality standards and are practically evaluable within our evaluation framework. During validation, please fully complete each task end-to-end, paying particular attention to potential ambiguities, overlooked edge cases, and the verifiability of all required information.\n\nYou should closely follow the structured checklist provided in the Task Refinement Instruction to evaluate realism, clarity, tediousness, and overall feasibility. Additionally, please specifically emphasize the following unique validation aspects:\n\n\\medskip\n\n\\textbf{Checklist Addendum for Validation}\n\n\n\n\\item \\textbf{Full Completion and End-to-End Testing}\n\n\\item Fully perform the task yourself from start to finish. Ensure all or most critical information can be practically located and verified from the URL sources.\n\n\n\\item \\textbf{Feasibility of URL-based Verification}\n\n\\item Verify that each URL provided as an information source uniquely and directly supports the expected statement.\n\\item Avoid scenarios that are beyond our evaluation framework capabilities, such as:\n\n\\item Tasks requiring simultaneous verification from multiple distinct webpages, where the verification cannot be decomposed into independent single-page validations.\n\\item Tasks where the critical information is dynamically loaded, hidden, or collapsed on the webpage, making it inaccessible from the static HTML or initial page rendering.\n\\item Tasks that require additional webpage interactions beyond simple navigation (e.g., clicking multiple buttons, performing searches within the site, or scrolling to trigger dynamic loading).\n\\item Tasks where URLs provided are not unique or stable (e.g., search result URLs that frequently change).\n\\item Tasks requiring login credentials (verify this using your browser\'s incognito mode).\n\n\n\n\\item \\textbf{Explicit Ground Truth and Evaluation Notes}\n\n\\item Clearly document the related sources and information that can be helpful for understanding the task. \n\n\\item If ground-truth information is necessary for certain criteria, note them down and carefully validate its correctness.\n\n\\item Provide explicit notes for potentially tricky evaluation scenarios or subtleties that might be easily overlooked or incorrectly handled during judge agent development.\n\n\n\n\n\n\\medskip\n\nIf during validation you identify tasks that seem borderline or ambiguous with respect to our framework capabilities, promptly discuss these tasks within the team to determine their suitability or necessary adjustments.\n\n\n\n\n\n\n\n\\subsection{Instructions for Human Performance Study}\n\nWe omit some examples from the instruction for clarity.\n\n\\begin{tcolorbox}[colframe=gray!50!black, colback=gray!10!white, title=Human Performance Study Instruction, breakable]\n\\small\n\n\\textbf{Objective}\n\nThe goal of this stage is for humans to complete tasks and provide answers, enabling us to compare human performance with AI agents.\nFor each task, please search and browse relevant websites to gather the necessary information. Use Google Docs as a text pad for your answer as you proceed, and include the URLs of your sources as provenance for each piece of information or claim made. Each URL (webpage or PDF) should allow others to easily verify the attribution of your answer. Additionally, record videos of your completion processes for review and potential publication purposes.\n\n\\medskip\n\\textbf{Setup Requirements}\n\\begin{itemize}[leftmargin=1.5em]\n    \\item A clean browser context only for this purpose\n    \\begin{itemize}\n        \\item Use a clean Chrome browser window (not incognito) without signing in, and avoid displaying sensitive personal information.\n        \\item You can only use the browser during task completion.\n    \\end{itemize}\n    \\item Chrome extension for timing: \\textit{Web Activity Time Tracker}\n    \\begin{itemize}\n        \\item This extension will track the websites you visit and the time you spend on them in the browser.\n        \\item Set \\textit{Stop the tracker if there is no action for} to \\textit{30 minutes} in the extension settings.\n    \\end{itemize}\n\\end{itemize}\n\n\\medskip\n\\textbf{Procedure for Each Task}\n\n\\begin{enumerate}[label=\\textbf{\\arabic*.}, leftmargin=*]\n    \\item Before you begin:\n    \\begin{enumerate}[label=\\textbf{\\alph*.}]\n        \\item Understand the task clearly\n        \\begin{enumerate}[label=\\roman*.]\n            \\item Carefully read the task description and ensure you clearly understand what is asked (e.g., no language barriers or a lack of domain-specific knowledge). It’s okay if you do not yet know how to solve the task; planning and figuring this out are part of the task-solving.\n            \\item Please let us know if you have prior knowledge about the assigned task (e.g., you already know the answer without searching) before starting to solve the task.\n        \\end{enumerate}\n        \n        \\item Reset time tracker\n        \\begin{enumerate}[label=\\roman*.]\n            \\item Clear previous data from the \\textit{Web Activity Time Tracker} extension. Specifically, go to \\texttt{Settings → Remove all data}. This ensures tracking statistics are only for the current task from now on.\n        \\end{enumerate}\n        \n        \\item Open a new Google Doc as a text pad for your answer\n    \\end{enumerate}\n\n    \\item Solving the task:\n    \\begin{enumerate}[label=\\textbf{\\alph*.}]\n        \\item Start recording\n        \\begin{enumerate}[label=\\roman*.]\n            \\item Begin screen recording before you start planning or researching for the task.\n            \\item Ensure all task-related activity is recorded, avoiding external screens.\n        \\end{enumerate}\n        \n        \\item Research and Answer\n        \\begin{enumerate}[label=\\roman*.]\n            \\item Search and browse to gather accurate and reliable information. DO NOT rely on prior knowledge; actively search to verify all information.\n            \\item Clearly document your response, explicitly linking each critical piece of information to its source.\n            \\begin{enumerate}[label=\\arabic*.]\n                \\item Our basic expectation for answers is: All critical points required by the task should be included, along with URLs to verify them (i.e., the URL where you find each information). You do not need to summarize every web page you visit.\n                \\item You could also write some intermediate thoughts or the reasoning process on Google Docs for yourself when completing the task, though only the critical information asked by the task is required for the final answer.\n            \\end{enumerate}\n            \\item For every piece of factual information in your answer, insert the link as attribution.\n            \\begin{enumerate}[label=\\arabic*.]\n                \\item Use either inline hyperlinks or numbered citations.\n                \\item Ensure all URLs start with \\texttt{http} or \\texttt{https}.\n            \\end{enumerate}\n        \\end{enumerate}\n    \\end{enumerate}\n    \n    \\item After completing the task:\n    \\begin{enumerate}[label=\\textbf{\\alph*.}]\n        \\item Export browsing data:\n        \\begin{enumerate}[label=\\roman*.]\n            \\item Export statistics from the time tracker extension to CSV immediately\n            \\item After exporting, you must stop gathering new information; only reformat your answer if you wish (e.g., if it is still cluttered). (Imagine you no longer have access to the Internet other than the answer text pad after this step.)\n        \\end{enumerate}\n        \\item Stop recording:\n        \\begin{enumerate}[label=\\roman*.]\n            \\item End your screen recording promptly after exporting data.\n        \\end{enumerate}\n    \\end{enumerate}\n\n    \\item Upload your results:\n    \\begin{enumerate}[label=\\textbf{\\alph*.}]\n        \\item Paste your final answer into the designated \\textit{Answer} column in the provided spreadsheet.\n        \\item Convert your recording to MP4 using \\textit{HandBrake} (preset: \\texttt{Fast 1080p30}) and upload to our OneDrive folder.\n        \\item Rename the CSV as \\texttt{taskID-yourName.csv}, upload it to OneDrive, and paste its URL into the spreadsheet\'s corresponding \\textit{Time} column.\n    \\end{enumerate}\n\\end{enumerate}\n\n\\medskip\n\n\\textbf{Notes}\n\\begin{itemize}[leftmargin=1.5em]\n    \\item Notify us if any task takes less than 5 minutes or more than 1 hour.\n    \\item You should NOT give up on a task unless you still have not landed on a clear path to the solution after 30 minutes. However, to conserve effort, you may stop working on tasks that exceed 60 minutes, even if a solution path is evident.\n    \\item AI tools (e.g., ChatGPT) are NOT allowed.\n    \\item Personal browser extensions and setups are permitted, but keep potential video publication in mind.\n\\end{itemize}\n\n\\medskip\n\n\\textbf{Trial Tasks}\n\n\\smallskip\n\nTo make sure you\'ve understood the task and to ensure the quality of our human performance, let\'s start with two simple trial tasks.\nWe will go through your answers and provide feedback. When you pass the two trial tasks, we will start assigning real tasks to you.\n\n\n\\end{tcolorbox}\\begin{tcolorbox}[colframe=gray!50!black, colback=gray!10!white, title=Human Performance Study Instruction, breakable]\n\\small\n\n\\textbf{Objective}\n\nThe goal of this stage is for humans to complete tasks and provide answers, enabling us to compare human performance with AI agents.\nFor each task, please search and browse relevant websites to gather the necessary information. Use Google Docs as a text pad for your answer as you proceed, and include the URLs of your sources as provenance for each piece of information or claim made. Each URL (webpage or PDF) should allow others to easily verify the attribution of your answer. Additionally, record videos of your completion processes for review and potential publication purposes.\n\n\\medskip\n\\textbf{Setup Requirements}\n\\begin{itemize}[leftmargin=1.5em]\n    \\item A clean browser context only for this purpose\n    \\begin{itemize}\n        \\item Use a clean Chrome browser window (not incognito) without signing in, and avoid displaying sensitive personal information.\n        \\item You can only use the browser during task completion.\n    \\end{itemize}\n    \\item Chrome extension for timing: \\textit{Web Activity Time Tracker}\n    \\begin{itemize}\n        \\item This extension will track the websites you visit and the time you spend on them in the browser.\n        \\item Set \\textit{Stop the tracker if there is no action for} to \\textit{30 minutes} in the extension settings.\n    \\end{itemize}\n\\end{itemize}\n\n\\medskip\n\\textbf{Procedure for Each Task}\n\n\\begin{enumerate}[label=\\textbf{\\arabic*.}, leftmargin=*]\n    \\item Before you begin:\n    \\begin{enumerate}[label=\\textbf{\\alph*.}]\n        \\item Understand the task clearly\n        \\begin{enumerate}[label=\\roman*.]\n            \\item Carefully read the task description and ensure you clearly understand what is asked (e.g., no language barriers or a lack of domain-specific knowledge). It’s okay if you do not yet know how to solve the task; planning and figuring this out are part of the task-solving.\n            \\item Please let us know if you have prior knowledge about the assigned task (e.g., you already know the answer without searching) before starting to solve the task.\n        \\end{enumerate}\n        \n        \\item Reset time tracker\n        \\begin{enumerate}[label=\\roman*.]\n            \\item Clear previous data from the \\textit{Web Activity Time Tracker} extension. Specifically, go to \\texttt{Settings → Remove all data}. This ensures tracking statistics are only for the current task from now on.\n        \\end{enumerate}\n        \n        \\item Open a new Google Doc as a text pad for your answer\n    \\end{enumerate}\n\n    \\item Solving the task:\n    \\begin{enumerate}[label=\\textbf{\\alph*.}]\n        \\item Start recording\n        \\begin{enumerate}[label=\\roman*.]\n            \\item Begin screen recording before you start planning or researching for the task.\n            \\item Ensure all task-related activity is recorded, avoiding external screens.\n        \\end{enumerate}\n        \n        \\item Research and Answer\n        \\begin{enumerate}[label=\\roman*.]\n            \\item Search and browse to gather accurate and reliable information. DO NOT rely on prior knowledge; actively search to verify all information.\n            \\item Clearly document your response, explicitly linking each critical piece of information to its source.\n            \\begin{enumerate}[label=\\arabic*.]\n                \\item Our basic expectation for answers is: All critical points required by the task should be included, along with URLs to verify them (i.e., the URL where you find each information). You do not need to summarize every web page you visit.\n                \\item You could also write some intermediate thoughts or the reasoning process on Google Docs for yourself when completing the task, though only the critical information asked by the task is required for the final answer.\n            \\end{enumerate}\n            \\item For every piece of factual information in your answer, insert the link as attribution.\n            \\begin{enumerate}[label=\\arabic*.]\n                \\item Use either inline hyperlinks or numbered citations.\n                \\item Ensure all URLs start with \\texttt{http} or \\texttt{https}.\n            \\end{enumerate}\n        \\end{enumerate}\n    \\end{enumerate}\n    \n    \\item After completing the task:\n    \\begin{enumerate}[label=\\textbf{\\alph*.}]\n        \\item Export browsing data:\n        \\begin{enumerate}[label=\\roman*.]\n            \\item Export statistics from the time tracker extension to CSV immediately\n            \\item After exporting, you must stop gathering new information; only reformat your answer if you wish (e.g., if it is still cluttered). (Imagine you no longer have access to the Internet other than the answer text pad after this step.)\n        \\end{enumerate}\n        \\item Stop recording:\n        \\begin{enumerate}[label=\\roman*.]\n            \\item End your screen recording promptly after exporting data.\n        \\end{enumerate}\n    \\end{enumerate}\n\n    \\item Upload your results:\n    \\begin{enumerate}[label=\\textbf{\\alph*.}]\n        \\item Paste your final answer into the designated \\textit{Answer} column in the provided spreadsheet.\n        \\item Convert your recording to MP4 using \\textit{HandBrake} (preset: \\texttt{Fast 1080p30}) and upload to our OneDrive folder.\n        \\item Rename the CSV as \\texttt{taskID-yourName.csv}, upload it to OneDrive, and paste its URL into the spreadsheet\'s corresponding \\textit{Time} column.\n    \\end{enumerate}\n\\end{enumerate}\n\n\\medskip\n\n\\textbf{Notes}\n\\begin{itemize}[leftmargin=1.5em]\n    \\item Notify us if any task takes less than 5 minutes or more than 1 hour.\n    \\item You should NOT give up on a task unless you still have not landed on a clear path to the solution after 30 minutes. However, to conserve effort, you may stop working on tasks that exceed 60 minutes, even if a solution path is evident.\n    \\item AI tools (e.g., ChatGPT) are NOT allowed.\n    \\item Personal browser extensions and setups are permitted, but keep potential video publication in mind.\n\\end{itemize}\n\n\\medskip\n\n\\textbf{Trial Tasks}\n\n\\smallskip\n\nTo make sure you\'ve understood the task and to ensure the quality of our human performance, let\'s start with two simple trial tasks.\nWe will go through your answers and provide feedback. When you pass the two trial tasks, we will start assigning real tasks to you.\n\n\n\\end{tcolorbox}[colframe=gray!50!black, colback=gray!10!white, title=Human Performance Study Instruction, breakable]\n\\small\n\n\\textbf{Objective}\n\nThe goal of this stage is for humans to complete tasks and provide answers, enabling us to compare human performance with AI agents.\nFor each task, please search and browse relevant websites to gather the necessary information. Use Google Docs as a text pad for your answer as you proceed, and include the URLs of your sources as provenance for each piece of information or claim made. Each URL (webpage or PDF) should allow others to easily verify the attribution of your answer. Additionally, record videos of your completion processes for review and potential publication purposes.\n\n\\medskip\n\\textbf{Setup Requirements}\n\n    \\item A clean browser context only for this purpose\n    \n        \\item Use a clean Chrome browser window (not incognito) without signing in, and avoid displaying sensitive personal information.\n        \\item You can only use the browser during task completion.\n    \n    \\item Chrome extension for timing: \\textit{Web Activity Time Tracker}\n    \n        \\item This extension will track the websites you visit and the time you spend on them in the browser.\n        \\item Set \\textit{Stop the tracker if there is no action for} to \\textit{30 minutes} in the extension settings.\n    \n\n\n\\medskip\n\\textbf{Procedure for Each Task}\n\n\n    \\item Before you begin:\n    \n        \\item Understand the task clearly\n        \n            \\item Carefully read the task description and ensure you clearly understand what is asked (e.g., no language barriers or a lack of domain-specific knowledge). It’s okay if you do not yet know how to solve the task; planning and figuring this out are part of the task-solving.\n            \\item Please let us know if you have prior knowledge about the assigned task (e.g., you already know the answer without searching) before starting to solve the task.\n        \n        \n        \\item Reset time tracker\n        \n            \\item Clear previous data from the \\textit{Web Activity Time Tracker} extension. Specifically, go to \\texttt{Settings → Remove all data}. This ensures tracking statistics are only for the current task from now on.\n        \n        \n        \\item Open a new Google Doc as a text pad for your answer\n    \n\n    \\item Solving the task:\n    \n        \\item Start recording\n        \n            \\item Begin screen recording before you start planning or researching for the task.\n            \\item Ensure all task-related activity is recorded, avoiding external screens.\n        \n        \n        \\item Research and Answer\n        \n            \\item Search and browse to gather accurate and reliable information. DO NOT rely on prior knowledge; actively search to verify all information.\n            \\item Clearly document your response, explicitly linking each critical piece of information to its source.\n            \n                \\item Our basic expectation for answers is: All critical points required by the task should be included, along with URLs to verify them (i.e., the URL where you find each information). You do not need to summarize every web page you visit.\n                \\item You could also write some intermediate thoughts or the reasoning process on Google Docs for yourself when completing the task, though only the critical information asked by the task is required for the final answer.\n            \n            \\item For every piece of factual information in your answer, insert the link as attribution.\n            \n                \\item Use either inline hyperlinks or numbered citations.\n                \\item Ensure all URLs start with \\texttt{http} or \\texttt{https}.\n            \n        \n    \n    \n    \\item After completing the task:\n    \n        \\item Export browsing data:\n        \n            \\item Export statistics from the time tracker extension to CSV immediately\n            \\item After exporting, you must stop gathering new information; only reformat your answer if you wish (e.g., if it is still cluttered). (Imagine you no longer have access to the Internet other than the answer text pad after this step.)\n        \n        \\item Stop recording:\n        \n            \\item End your screen recording promptly after exporting data.\n        \n    \n\n    \\item Upload your results:\n    \n        \\item Paste your final answer into the designated \\textit{Answer} column in the provided spreadsheet.\n        \\item Convert your recording to MP4 using \\textit{HandBrake} (preset: \\texttt{Fast 1080p30}) and upload to our OneDrive folder.\n        \\item Rename the CSV as \\texttt{taskID-yourName.csv}, upload it to OneDrive, and paste its URL into the spreadsheet\'s corresponding \\textit{Time} column.\n    \n\n\n\\medskip\n\n\\textbf{Notes}\n\n    \\item Notify us if any task takes less than 5 minutes or more than 1 hour.\n    \\item You should NOT give up on a task unless you still have not landed on a clear path to the solution after 30 minutes. However, to conserve effort, you may stop working on tasks that exceed 60 minutes, even if a solution path is evident.\n    \\item AI tools (e.g., ChatGPT) are NOT allowed.\n    \\item Personal browser extensions and setups are permitted, but keep potential video publication in mind.\n\n\n\\medskip\n\n\\textbf{Trial Tasks}\n\n\\smallskip\n\nTo make sure you\'ve understood the task and to ensure the quality of our human performance, let\'s start with two simple trial tasks.\nWe will go through your answers and provide feedback. When you pass the two trial tasks, we will start assigning real tasks to you.\n\n\n\n\n\\subsection{Instructions for Error Analysis}\n\nWe omit some examples from the instruction for clarity.\n\n\\begin{tcolorbox}[colframe=gray!50!black, colback=gray!10!white, title=Error Analysis Instruction, breakable]\n\\small\n\nFollowing the workflow in \\autoref{fig:error_workflow}, we start by evaluating the overall correctness of the agent’s entire response based on its text, and then move on to assess the attribution of each key information.\n\n\\medskip\n\nWe randomly select answers from selected agents, and each answer is presented to you with a Google Doc along with an annotation spreadsheet. Please follow these steps (Read, Evaluate, Comment, Collect) for error analysis:\n\\begin{enumerate}[leftmargin=1.5em]\n    \\item Read the task and the agent\'s answer carefully.\n    \\item Evaluate the response using the criteria detailed below.\n    \\item Whenever you identify an error, leave a comment in the Google Doc directly on the problematic part. Your comment should briefly explain what kind of error it is and why.\n    \\item Collect all error types you identified, and check the corresponding labels in the annotation sheet.\n\\end{enumerate}\n\n\\medskip\n\\textbf{Correctness Check}\n\n\\smallskip\n\nThis section is concerned only with the agent\'s full response based on the text.\n\n\\begin{enumerate}[label=\\textbf{\\arabic*.}, leftmargin=*]\n    \\item \\textit{Incompleteness:} Our definition of Incompleteness here is limited to immediately noticeable, surface-level omissions: The agent does not provide all content explicitly requested in the task. It does not cover more subtle or long-range reasoning failures. We further define two subtypes. You may better understand our intended scope by reviewing the examples provided.\n    \\begin{enumerate}[label=\\textbf{\\alph*.}]\n        \\item \\textit{Information Not Found: } The agent explicitly states it fails, such as:\n        \\begin{itemize}\n            \\item \\texttt{``I tried but failed to find....\'\'}\n            \\item \\texttt{``I provide another ..., ... is not available.\'\'}\n        \\end{itemize}\n        \n        \\item \\textit{Partial Completion:} The agent provides fewer items or steps than explicitly requested by the task, such as:\n        \\begin{itemize}\n            \\item \\texttt{Requested 3 items but only 1 provided.}\n            \\item \\texttt{Asked for 5 explicit steps but only completed 3 steps.}\n        \\end{itemize}\n    \\end{enumerate}\n\n    \\item \\textit{Criteria Violation:} This label is used when the agent’s answer breaks explicit constraints mentioned in the task. These constraints could be things like price ranges, required formats, word limits, or instructions such as “find all.” It also applies when the task comes with a ground-truth reference — for example, when the prompt clearly expects a specific answer—and the agent gives a different or incorrect one. Examples:\n    \\begin{itemize}\n        \\item \\texttt{``Find an item priced under \\$250.\'\' → Answer provides an item of ``\\$260\'\'.}\n        \\item \\texttt{``Find the specific follow-up work of GCG (AmpleGCG is the ground truth).\'\' → Answer gives a different, incorrect paper title.}\n    \\end{itemize}\n    \n\\end{enumerate}\n\n\n\\medskip\n\\textbf{Attribution Check}\n\n\\smallskip\n\nIn this section, we examine each individual key information in the answer. One key information refers to either a subtask in a multi-step instruction or an item in a list-based enumeration task. The evaluation follows a factual open-book QA pipeline: we verify 1) whether the attribution is invalid or missing, 2) whether the cited page is relevant, and 3) whether it genuinely supports the agent’s claim.\nSpecific types include:\n\n\\begin{enumerate}[label=\\textbf{\\arabic*.}, leftmargin=*]\n    \\item \\textit{Invalid Attribution:} URL is expired, incorrectly formatted, or obviously fabricated, such as\n    \\begin{itemize}\n        \\item \\texttt{The URL leads to a ``page not found\'\' error.}\n        \\item \\texttt{The provided arXiv link for a research paper (\\url{https://arxiv.org/abs/1234.56789}) is clearly fake.}\n    \\end{itemize}\n    \\item \\textit{Missing Attribution:} No source URL is provided for the claim. For example, in the following answer, a missing source and a valid one are presented.\n    \\begin{itemize}\n        \\item \\texttt{Totokaelo}\n            \\begin{itemize}\n                \\item \\texttt{Address: 913 Western Avenue, Seattle, WA 98104 → Missing Attribution: no URLs (including the following product page link) can substantiate the address information.}\n                \\item \\texttt{Product Page: https://totokaelo.com/collections/acne-studios}\n            \\end{itemize}\n        \\item \\texttt{DNA 2050}\n        \\begin{itemize}\n            \\item \\texttt{Address: 700 11th Avenue, Suite 160, The Bravern, Bellevue, WA 98004 [1] → Correct attribution: the link provides the address.}\n            \\item \\texttt{Product Page: https://www.dna2050.com/collections/acne-studios}\n        \\end{itemize}\n    \\end{itemize}\n    \\item \\textit{Unsupported Answer:} If a reachable attribution is provided, we need to examine whether it can support the claim. If not, there are 2 subtypes of errors:\n    \\begin{enumerate}[label=\\textbf{\\alph*.}]\n        \\item \\textit{Retrieval Error:} The provided sources are irrelevant to the task, such as:\n        \\begin{itemize}\n            \\item \\texttt{The task requests the list of K-pop songs in Just Dance, but the provided source is a discussion page without a tracklist.}\n            \\item \\texttt{The task requests a complete list of character abilities in Marvel Rivals, but the source only contains team-up abilities, unable to support the hallucinated ability list.}\n        \\end{itemize}\n        \\item \\textit{Synthesis Error:} The source contains useful information required by the task, but the answer misquotes or misinterprets it. Examples:\n        \\begin{itemize}\n            \\item \\texttt{The provided product page shows a price of \\$220, while the answer incorrectly states \\$230.}\n            \\item \\texttt{The source gives a correct tracklist of songs in Just Dance, but the answer identifies incorrect K-pop songs.}\n        \\end{itemize}\n    \\end{enumerate}\n\\end{enumerate}\n\n\n\\medskip\n\\textbf{Notes}\n\\begin{itemize}[leftmargin=1.5em]\n    \\item Please check our sample annotations to help you get started and use them as references during annotation.\n    \\item If you are unsure how to label an answer, please raise the issue in the group for discussion.\n\\end{itemize}\n\n\\end{tcolorbox}\\begin{tcolorbox}[colframe=gray!50!black, colback=gray!10!white, title=Error Analysis Instruction, breakable]\n\\small\n\nFollowing the workflow in \\autoref{fig:error_workflow}, we start by evaluating the overall correctness of the agent’s entire response based on its text, and then move on to assess the attribution of each key information.\n\n\\medskip\n\nWe randomly select answers from selected agents, and each answer is presented to you with a Google Doc along with an annotation spreadsheet. Please follow these steps (Read, Evaluate, Comment, Collect) for error analysis:\n\\begin{enumerate}[leftmargin=1.5em]\n    \\item Read the task and the agent\'s answer carefully.\n    \\item Evaluate the response using the criteria detailed below.\n    \\item Whenever you identify an error, leave a comment in the Google Doc directly on the problematic part. Your comment should briefly explain what kind of error it is and why.\n    \\item Collect all error types you identified, and check the corresponding labels in the annotation sheet.\n\\end{enumerate}\n\n\\medskip\n\\textbf{Correctness Check}\n\n\\smallskip\n\nThis section is concerned only with the agent\'s full response based on the text.\n\n\\begin{enumerate}[label=\\textbf{\\arabic*.}, leftmargin=*]\n    \\item \\textit{Incompleteness:} Our definition of Incompleteness here is limited to immediately noticeable, surface-level omissions: The agent does not provide all content explicitly requested in the task. It does not cover more subtle or long-range reasoning failures. We further define two subtypes. You may better understand our intended scope by reviewing the examples provided.\n    \\begin{enumerate}[label=\\textbf{\\alph*.}]\n        \\item \\textit{Information Not Found: } The agent explicitly states it fails, such as:\n        \\begin{itemize}\n            \\item \\texttt{``I tried but failed to find....\'\'}\n            \\item \\texttt{``I provide another ..., ... is not available.\'\'}\n        \\end{itemize}\n        \n        \\item \\textit{Partial Completion:} The agent provides fewer items or steps than explicitly requested by the task, such as:\n        \\begin{itemize}\n            \\item \\texttt{Requested 3 items but only 1 provided.}\n            \\item \\texttt{Asked for 5 explicit steps but only completed 3 steps.}\n        \\end{itemize}\n    \\end{enumerate}\n\n    \\item \\textit{Criteria Violation:} This label is used when the agent’s answer breaks explicit constraints mentioned in the task. These constraints could be things like price ranges, required formats, word limits, or instructions such as “find all.” It also applies when the task comes with a ground-truth reference — for example, when the prompt clearly expects a specific answer—and the agent gives a different or incorrect one. Examples:\n    \\begin{itemize}\n        \\item \\texttt{``Find an item priced under \\$250.\'\' → Answer provides an item of ``\\$260\'\'.}\n        \\item \\texttt{``Find the specific follow-up work of GCG (AmpleGCG is the ground truth).\'\' → Answer gives a different, incorrect paper title.}\n    \\end{itemize}\n    \n\\end{enumerate}\n\n\n\\medskip\n\\textbf{Attribution Check}\n\n\\smallskip\n\nIn this section, we examine each individual key information in the answer. One key information refers to either a subtask in a multi-step instruction or an item in a list-based enumeration task. The evaluation follows a factual open-book QA pipeline: we verify 1) whether the attribution is invalid or missing, 2) whether the cited page is relevant, and 3) whether it genuinely supports the agent’s claim.\nSpecific types include:\n\n\\begin{enumerate}[label=\\textbf{\\arabic*.}, leftmargin=*]\n    \\item \\textit{Invalid Attribution:} URL is expired, incorrectly formatted, or obviously fabricated, such as\n    \\begin{itemize}\n        \\item \\texttt{The URL leads to a ``page not found\'\' error.}\n        \\item \\texttt{The provided arXiv link for a research paper (\\url{https://arxiv.org/abs/1234.56789}) is clearly fake.}\n    \\end{itemize}\n    \\item \\textit{Missing Attribution:} No source URL is provided for the claim. For example, in the following answer, a missing source and a valid one are presented.\n    \\begin{itemize}\n        \\item \\texttt{Totokaelo}\n            \\begin{itemize}\n                \\item \\texttt{Address: 913 Western Avenue, Seattle, WA 98104 → Missing Attribution: no URLs (including the following product page link) can substantiate the address information.}\n                \\item \\texttt{Product Page: https://totokaelo.com/collections/acne-studios}\n            \\end{itemize}\n        \\item \\texttt{DNA 2050}\n        \\begin{itemize}\n            \\item \\texttt{Address: 700 11th Avenue, Suite 160, The Bravern, Bellevue, WA 98004 [1] → Correct attribution: the link provides the address.}\n            \\item \\texttt{Product Page: https://www.dna2050.com/collections/acne-studios}\n        \\end{itemize}\n    \\end{itemize}\n    \\item \\textit{Unsupported Answer:} If a reachable attribution is provided, we need to examine whether it can support the claim. If not, there are 2 subtypes of errors:\n    \\begin{enumerate}[label=\\textbf{\\alph*.}]\n        \\item \\textit{Retrieval Error:} The provided sources are irrelevant to the task, such as:\n        \\begin{itemize}\n            \\item \\texttt{The task requests the list of K-pop songs in Just Dance, but the provided source is a discussion page without a tracklist.}\n            \\item \\texttt{The task requests a complete list of character abilities in Marvel Rivals, but the source only contains team-up abilities, unable to support the hallucinated ability list.}\n        \\end{itemize}\n        \\item \\textit{Synthesis Error:} The source contains useful information required by the task, but the answer misquotes or misinterprets it. Examples:\n        \\begin{itemize}\n            \\item \\texttt{The provided product page shows a price of \\$220, while the answer incorrectly states \\$230.}\n            \\item \\texttt{The source gives a correct tracklist of songs in Just Dance, but the answer identifies incorrect K-pop songs.}\n        \\end{itemize}\n    \\end{enumerate}\n\\end{enumerate}\n\n\n\\medskip\n\\textbf{Notes}\n\\begin{itemize}[leftmargin=1.5em]\n    \\item Please check our sample annotations to help you get started and use them as references during annotation.\n    \\item If you are unsure how to label an answer, please raise the issue in the group for discussion.\n\\end{itemize}\n\n\\end{tcolorbox}[colframe=gray!50!black, colback=gray!10!white, title=Error Analysis Instruction, breakable]\n\\small\n\nFollowing the workflow in \\autoref{fig:error_workflow}, we start by evaluating the overall correctness of the agent’s entire response based on its text, and then move on to assess the attribution of each key information.\n\n\\medskip\n\nWe randomly select answers from selected agents, and each answer is presented to you with a Google Doc along with an annotation spreadsheet. Please follow these steps (Read, Evaluate, Comment, Collect) for error analysis:\n\n    \\item Read the task and the agent\'s answer carefully.\n    \\item Evaluate the response using the criteria detailed below.\n    \\item Whenever you identify an error, leave a comment in the Google Doc directly on the problematic part. Your comment should briefly explain what kind of error it is and why.\n    \\item Collect all error types you identified, and check the corresponding labels in the annotation sheet.\n\n\n\\medskip\n\\textbf{Correctness Check}\n\n\\smallskip\n\nThis section is concerned only with the agent\'s full response based on the text.\n\n\n    \\item \\textit{Incompleteness:} Our definition of Incompleteness here is limited to immediately noticeable, surface-level omissions: The agent does not provide all content explicitly requested in the task. It does not cover more subtle or long-range reasoning failures. We further define two subtypes. You may better understand our intended scope by reviewing the examples provided.\n    \n        \\item \\textit{Information Not Found: } The agent explicitly states it fails, such as:\n        \n            \\item \\texttt{``I tried but failed to find....\'\'}\n            \\item \\texttt{``I provide another ..., ... is not available.\'\'}\n        \n        \n        \\item \\textit{Partial Completion:} The agent provides fewer items or steps than explicitly requested by the task, such as:\n        \n            \\item \\texttt{Requested 3 items but only 1 provided.}\n            \\item \\texttt{Asked for 5 explicit steps but only completed 3 steps.}\n        \n    \n\n    \\item \\textit{Criteria Violation:} This label is used when the agent’s answer breaks explicit constraints mentioned in the task. These constraints could be things like price ranges, required formats, word limits, or instructions such as “find all.” It also applies when the task comes with a ground-truth reference — for example, when the prompt clearly expects a specific answer—and the agent gives a different or incorrect one. Examples:\n    \n        \\item \\texttt{``Find an item priced under \\$250.\'\' → Answer provides an item of ``\\$260\'\'.}\n        \\item \\texttt{``Find the specific follow-up work of GCG (AmpleGCG is the ground truth).\'\' → Answer gives a different, incorrect paper title.}\n    \n    \n\n\n\n\\medskip\n\\textbf{Attribution Check}\n\n\\smallskip\n\nIn this section, we examine each individual key information in the answer. One key information refers to either a subtask in a multi-step instruction or an item in a list-based enumeration task. The evaluation follows a factual open-book QA pipeline: we verify 1) whether the attribution is invalid or missing, 2) whether the cited page is relevant, and 3) whether it genuinely supports the agent’s claim.\nSpecific types include:\n\n\n    \\item \\textit{Invalid Attribution:} URL is expired, incorrectly formatted, or obviously fabricated, such as\n    \n        \\item \\texttt{The URL leads to a ``page not found\'\' error.}\n        \\item \\texttt{The provided arXiv link for a research paper (\\url{https://arxiv.org/abs/1234.56789}) is clearly fake.}\n    \n    \\item \\textit{Missing Attribution:} No source URL is provided for the claim. For example, in the following answer, a missing source and a valid one are presented.\n    \n        \\item \\texttt{Totokaelo}\n            \n                \\item \\texttt{Address: 913 Western Avenue, Seattle, WA 98104 → Missing Attribution: no URLs (including the following product page link) can substantiate the address information.}\n                \\item \\texttt{Product Page: https://totokaelo.com/collections/acne-studios}\n            \n        \\item \\texttt{DNA 2050}\n        \n            \\item \\texttt{Address: 700 11th Avenue, Suite 160, The Bravern, Bellevue, WA 98004 [1] → Correct attribution: the link provides the address.}\n            \\item \\texttt{Product Page: https://www.dna2050.com/collections/acne-studios}\n        \n    \n    \\item \\textit{Unsupported Answer:} If a reachable attribution is provided, we need to examine whether it can support the claim. If not, there are 2 subtypes of errors:\n    \n        \\item \\textit{Retrieval Error:} The provided sources are irrelevant to the task, such as:\n        \n            \\item \\texttt{The task requests the list of K-pop songs in Just Dance, but the provided source is a discussion page without a tracklist.}\n            \\item \\texttt{The task requests a complete list of character abilities in Marvel Rivals, but the source only contains team-up abilities, unable to support the hallucinated ability list.}\n        \n        \\item \\textit{Synthesis Error:} The source contains useful information required by the task, but the answer misquotes or misinterprets it. Examples:\n        \n            \\item \\texttt{The provided product page shows a price of \\$220, while the answer incorrectly states \\$230.}\n            \\item \\texttt{The source gives a correct tracklist of songs in Just Dance, but the answer identifies incorrect K-pop songs.}\n        \n    \n\n\n\n\\medskip\n\\textbf{Notes}\n\n    \\item Please check our sample annotations to help you get started and use them as references during annotation.\n    \\item If you are unsure how to label an answer, please raise the issue in the group for discussion.\n\n\n\n\n\\subsection{Instructions for Human Evaluation}\\label{app:instr_doc_human_eval}\n\n\\begin{tcolorbox}[colframe=gray!50!black, colback=gray!10!white, title=Human Evaluation Instruction, breakable]\n\\small\n\n\\textbf{Objective}\n\nThe goal of this study is to validate the quality and reliability of the rubrics and judge agent implementations. Your task is to independently evaluate them in two phases: rubric-level and node-level assessment.\n\nSpecifically, we have sampled 15 tasks and prepared their evaluation rubrics and evaluation results. In the rubric-level assessment phase, you will review these evaluation rubrics structured as trees, and record your feedback. In the node-level assessment phase, for tasks with rubrics agreed upon in the previous phase, you will independently score the leaf nodes of the rubric for two sampled answers per task, without reference to the judge agents’ evaluation results.\n\n\\medskip\n\\textbf{Phase 1: Rubric-Level Assessment}\n\nFor each task, you will be provided with an evaluation rubric in a tree structure. Each node in the tree includes an ID, a brief description, and its settings (e.g., sequential or parallel; critical or non-critical). Please refer to \\S\\ref{sec:rubric_tree} for a detailed explanation of our rubric design.\n\n\\smallskip\nPlease read the task description carefully to understand the intended evaluation criteria, examine the rubric carefully, and use the following scale to rate each rubric:\n\\begin{itemize}[leftmargin=1.5em]\n    \\item \\textit{Strongly Agree:} The rubric is clear, comprehensive, practical, and fully aligns with task requirements.\n    \\item \\textit{Agree with Reservations:} The rubric is generally acceptable, but minor adjustments (e.g., stricter or clearer node criteria) would enhance clarity or robustness.\n    \\item \\textit{Disagree:} The rubric is significantly flawed, impractical, or misaligned with the task criteria.\n\\end{itemize}\n\nWhen reviewing rubrics, pay close attention to: node decomposition and ordering, prompt formulation, score aggregation strategies, etc.\nIf the tree structure alone is not sufficient for understanding, please refer to the evaluation script to review the actual prompt wording and implementation details.\n\n\\medskip\nFor each rubric you evaluate, you must provide a clear written justification for your rating.\n\n\\smallskip\n\n\\emph{Notes:}\n\\begin{itemize}[leftmargin=1.5em]\n    \\item A task may allow for multiple reasonable rubric trees. As long as the provided rubric is logically sound and aligns with the task goals, it should be accepted.\n    \\item If you disagree with any rubric, please notify us ASAP. Any rubric receiving a "Disagree" rating must be revised before proceeding to Phase 2.\n\\end{itemize}\n\n\\medskip\n\\textbf{Phase 2: Node-Level Assessment}\n\nOnly rubrics that passed Phase 1 will proceed to this phase. For each task, you will be given two sampled agent answers. We have filtered out answers that are entirely failed. Your goal is to independently evaluate each answer at the leaf node level, strictly following the rubric. Your goal is to independently evaluate these answers at the leaf node level, strictly following the rubric.\n\n\\smallskip\n\nYou will be provided with a JSON file for each task. This file contains the structured evaluation rubric tree, where all scores are initially unset. Each leaf node will contain a \\texttt{score} field set to ``TODO\'\'. Your task is to replace each ``TODO\'\' with a judgment:\n\\begin{itemize}[leftmargin=1.5em]\n    \\item \\textit{1} if the answer satisfies the leaf node criterion (based on the provided answer and source URLs), or\n    \\item \\textit{0} if it does not.\n\\end{itemize}\nIn other words, you are substituting your own judgment in place of the LLM-based verification in each leaf node. Please strictly follow the rubric definitions when assigning scores. If anything is unclear, refer to the evaluation scripts.\n\nOnce you have completed all annotations for a task, upload your updated JSON file with all ``TODO\'\' values replaced. \n\n\\smallskip\n\nPlease note that human judgment is not always perfect, especially when working with a rubric that contains hundreds of leaf nodes. To help ensure the quality of this evaluation study, we will run a script that compares your annotations with the results from the judge agent. This will generate a mismatch report highlighting all leaf nodes where your judgment differs. Please review all mismatches, explain the rationale for your decision, and indicate whether the discrepancy may have resulted from an oversight on your part.\n\n\\smallskip\n\\emph{Notes:}\n\\begin{itemize}[leftmargin=1.5em]\n    \\item Internal node scores will be automatically calculated based on leaf evaluations; you do not need to judge them.\n\\end{itemize}\n\n\n\n\\end{tcolorbox}\\begin{tcolorbox}[colframe=gray!50!black, colback=gray!10!white, title=Human Evaluation Instruction, breakable]\n\\small\n\n\\textbf{Objective}\n\nThe goal of this study is to validate the quality and reliability of the rubrics and judge agent implementations. Your task is to independently evaluate them in two phases: rubric-level and node-level assessment.\n\nSpecifically, we have sampled 15 tasks and prepared their evaluation rubrics and evaluation results. In the rubric-level assessment phase, you will review these evaluation rubrics structured as trees, and record your feedback. In the node-level assessment phase, for tasks with rubrics agreed upon in the previous phase, you will independently score the leaf nodes of the rubric for two sampled answers per task, without reference to the judge agents’ evaluation results.\n\n\\medskip\n\\textbf{Phase 1: Rubric-Level Assessment}\n\nFor each task, you will be provided with an evaluation rubric in a tree structure. Each node in the tree includes an ID, a brief description, and its settings (e.g., sequential or parallel; critical or non-critical). Please refer to \\S\\ref{sec:rubric_tree} for a detailed explanation of our rubric design.\n\n\\smallskip\nPlease read the task description carefully to understand the intended evaluation criteria, examine the rubric carefully, and use the following scale to rate each rubric:\n\\begin{itemize}[leftmargin=1.5em]\n    \\item \\textit{Strongly Agree:} The rubric is clear, comprehensive, practical, and fully aligns with task requirements.\n    \\item \\textit{Agree with Reservations:} The rubric is generally acceptable, but minor adjustments (e.g., stricter or clearer node criteria) would enhance clarity or robustness.\n    \\item \\textit{Disagree:} The rubric is significantly flawed, impractical, or misaligned with the task criteria.\n\\end{itemize}\n\nWhen reviewing rubrics, pay close attention to: node decomposition and ordering, prompt formulation, score aggregation strategies, etc.\nIf the tree structure alone is not sufficient for understanding, please refer to the evaluation script to review the actual prompt wording and implementation details.\n\n\\medskip\nFor each rubric you evaluate, you must provide a clear written justification for your rating.\n\n\\smallskip\n\n\\emph{Notes:}\n\\begin{itemize}[leftmargin=1.5em]\n    \\item A task may allow for multiple reasonable rubric trees. As long as the provided rubric is logically sound and aligns with the task goals, it should be accepted.\n    \\item If you disagree with any rubric, please notify us ASAP. Any rubric receiving a "Disagree" rating must be revised before proceeding to Phase 2.\n\\end{itemize}\n\n\\medskip\n\\textbf{Phase 2: Node-Level Assessment}\n\nOnly rubrics that passed Phase 1 will proceed to this phase. For each task, you will be given two sampled agent answers. We have filtered out answers that are entirely failed. Your goal is to independently evaluate each answer at the leaf node level, strictly following the rubric. Your goal is to independently evaluate these answers at the leaf node level, strictly following the rubric.\n\n\\smallskip\n\nYou will be provided with a JSON file for each task. This file contains the structured evaluation rubric tree, where all scores are initially unset. Each leaf node will contain a \\texttt{score} field set to ``TODO\'\'. Your task is to replace each ``TODO\'\' with a judgment:\n\\begin{itemize}[leftmargin=1.5em]\n    \\item \\textit{1} if the answer satisfies the leaf node criterion (based on the provided answer and source URLs), or\n    \\item \\textit{0} if it does not.\n\\end{itemize}\nIn other words, you are substituting your own judgment in place of the LLM-based verification in each leaf node. Please strictly follow the rubric definitions when assigning scores. If anything is unclear, refer to the evaluation scripts.\n\nOnce you have completed all annotations for a task, upload your updated JSON file with all ``TODO\'\' values replaced. \n\n\\smallskip\n\nPlease note that human judgment is not always perfect, especially when working with a rubric that contains hundreds of leaf nodes. To help ensure the quality of this evaluation study, we will run a script that compares your annotations with the results from the judge agent. This will generate a mismatch report highlighting all leaf nodes where your judgment differs. Please review all mismatches, explain the rationale for your decision, and indicate whether the discrepancy may have resulted from an oversight on your part.\n\n\\smallskip\n\\emph{Notes:}\n\\begin{itemize}[leftmargin=1.5em]\n    \\item Internal node scores will be automatically calculated based on leaf evaluations; you do not need to judge them.\n\\end{itemize}\n\n\n\n\\end{tcolorbox}[colframe=gray!50!black, colback=gray!10!white, title=Human Evaluation Instruction, breakable]\n\\small\n\n\\textbf{Objective}\n\nThe goal of this study is to validate the quality and reliability of the rubrics and judge agent implementations. Your task is to independently evaluate them in two phases: rubric-level and node-level assessment.\n\nSpecifically, we have sampled 15 tasks and prepared their evaluation rubrics and evaluation results. In the rubric-level assessment phase, you will review these evaluation rubrics structured as trees, and record your feedback. In the node-level assessment phase, for tasks with rubrics agreed upon in the previous phase, you will independently score the leaf nodes of the rubric for two sampled answers per task, without reference to the judge agents’ evaluation results.\n\n\\medskip\n\\textbf{Phase 1: Rubric-Level Assessment}\n\nFor each task, you will be provided with an evaluation rubric in a tree structure. Each node in the tree includes an ID, a brief description, and its settings (e.g., sequential or parallel; critical or non-critical). Please refer to \\S\\ref{sec:rubric_tree} for a detailed explanation of our rubric design.\n\n\\smallskip\nPlease read the task description carefully to understand the intended evaluation criteria, examine the rubric carefully, and use the following scale to rate each rubric:\n\n    \\item \\textit{Strongly Agree:} The rubric is clear, comprehensive, practical, and fully aligns with task requirements.\n    \\item \\textit{Agree with Reservations:} The rubric is generally acceptable, but minor adjustments (e.g., stricter or clearer node criteria) would enhance clarity or robustness.\n    \\item \\textit{Disagree:} The rubric is significantly flawed, impractical, or misaligned with the task criteria.\n\n\nWhen reviewing rubrics, pay close attention to: node decomposition and ordering, prompt formulation, score aggregation strategies, etc.\nIf the tree structure alone is not sufficient for understanding, please refer to the evaluation script to review the actual prompt wording and implementation details.\n\n\\medskip\nFor each rubric you evaluate, you must provide a clear written justification for your rating.\n\n\\smallskip\n\n\\emph{Notes:}\n\n    \\item A task may allow for multiple reasonable rubric trees. As long as the provided rubric is logically sound and aligns with the task goals, it should be accepted.\n    \\item If you disagree with any rubric, please notify us ASAP. Any rubric receiving a "Disagree" rating must be revised before proceeding to Phase 2.\n\n\n\\medskip\n\\textbf{Phase 2: Node-Level Assessment}\n\nOnly rubrics that passed Phase 1 will proceed to this phase. For each task, you will be given two sampled agent answers. We have filtered out answers that are entirely failed. Your goal is to independently evaluate each answer at the leaf node level, strictly following the rubric. Your goal is to independently evaluate these answers at the leaf node level, strictly following the rubric.\n\n\\smallskip\n\nYou will be provided with a JSON file for each task. This file contains the structured evaluation rubric tree, where all scores are initially unset. Each leaf node will contain a \\texttt{score} field set to ``TODO\'\'. Your task is to replace each ``TODO\'\' with a judgment:\n\n    \\item \\textit{1} if the answer satisfies the leaf node criterion (based on the provided answer and source URLs), or\n    \\item \\textit{0} if it does not.\n\nIn other words, you are substituting your own judgment in place of the LLM-based verification in each leaf node. Please strictly follow the rubric definitions when assigning scores. If anything is unclear, refer to the evaluation scripts.\n\nOnce you have completed all annotations for a task, upload your updated JSON file with all ``TODO\'\' values replaced. \n\n\\smallskip\n\nPlease note that human judgment is not always perfect, especially when working with a rubric that contains hundreds of leaf nodes. To help ensure the quality of this evaluation study, we will run a script that compares your annotations with the results from the judge agent. This will generate a mismatch report highlighting all leaf nodes where your judgment differs. Please review all mismatches, explain the rationale for your decision, and indicate whether the discrepancy may have resulted from an oversight on your part.\n\n\\smallskip\n\\emph{Notes:}\n\n    \\item Internal node scores will be automatically calculated based on leaf evaluations; you do not need to judge them.\n\n\n\n\n\n\n\n', 'appendix': False}}, 'categories': ['cs.AI', 'cs.CL'], 'published': '2025-06-26 17:32:50+00:00', 'primary_category': 'cs.AI', 'summary': 'Agentic search such as Deep Research systems, where large language models\nautonomously browse the web, synthesize information, and return comprehensive\ncitation-backed answers, represents a major shift in how users interact with\nweb-scale information. While promising greater efficiency and cognitive\noffloading, the growing complexity and open-endedness of agentic search have\noutpaced existing evaluation benchmarks and methodologies, which largely assume\nshort search horizons and static answers. In this paper, we introduce Mind2Web\n2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that\nrequire real-time web browsing and extensive information synthesis, constructed\nwith over 1,000 hours of human labor. To address the challenge of evaluating\ntime-varying and complex answers, we propose a novel Agent-as-a-Judge\nframework. Our method constructs task-specific judge agents based on a\ntree-structured rubric design to automatically assess both answer correctness\nand source attribution. We conduct a comprehensive evaluation of nine frontier\nagentic search systems and human performance, along with a detailed error\nanalysis to draw insights for future development. The best-performing system,\nOpenAI Deep Research, can already achieve 50-70% of human performance while\nspending half the time, showing a great potential. Altogether, Mind2Web 2\nprovides a rigorous foundation for developing and benchmarking the next\ngeneration of agentic search systems.'}
number of citations collected: 46
Thread 13092106240 Finished processing 2506.21506v1 (1/1) Time elapsed: 2.02s
Thread 8680141376 Finished processing 1 papers
