Attempt 0
Retrieved data: {'offset': 0, 'citingPaperInfo': {'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1806.08804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'title': 'Hierarchical Graph Representation Learning with Differentiable Pooling', 'authors': [{'authorId': '83539859', 'name': 'Rex Ying'}, {'authorId': '145829303', 'name': 'Jiaxuan You'}, {'authorId': '143622465', 'name': 'Christopher Morris'}, {'authorId': '145201124', 'name': 'Xiang Ren'}, {'authorId': '49437682', 'name': 'William L. Hamilton'}, {'authorId': '1702139', 'name': 'J. Leskovec'}]}, 'data': [{'citedPaper': {'paperId': '1a8ee9382e79b92986f9d780a929fc2d2be2f47b', 'title': 'Graph Capsule Convolutional Neural Networks', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1805.08090, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '47383867', 'name': 'Saurabh Verma'}, {'authorId': '1708494', 'name': 'Zhi-Li Zhang'}], 'abstract': 'Graph Convolutional Neural Networks (GCNNs) are the most recent exciting advancement in deep learning field and their applications are quickly spreading in multi-cross-domains including bioinformatics, chemoinformatics, social networks, natural language processing and computer vision. In this paper, we expose and tackle some of the basic weaknesses of a GCNN model with a capsule idea presented in \\cite{hinton2011transforming} and propose our Graph Capsule Network (GCAPS-CNN) model. In addition, we design our GCAPS-CNN model to solve especially graph classification problem which current GCNN models find challenging. Through extensive experiments, we show that our proposed Graph Capsule Network can significantly outperforms both the existing state-of-art deep learning methods and graph kernels on graph classification benchmark datasets.'}}, {'citedPaper': {'paperId': 'd81fc968196e06ccafd7ea4c008b13e1cad1be64', 'title': 'An End-to-End Deep Learning Architecture for Graph Classification', 'openAccessPdf': {'url': 'https://ojs.aaai.org/index.php/AAAI/article/download/11782/11641', 'status': 'BRONZE', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v32i1.11782?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v32i1.11782, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '3098251', 'name': 'Muhan Zhang'}, {'authorId': '7217944', 'name': 'Zhicheng Cui'}, {'authorId': '40059761', 'name': 'Marion Neumann'}, {'authorId': '9527255', 'name': 'Yixin Chen'}], 'abstract': '\n \n Neural networks are typically designed to deal with data in tensor forms. In this paper, we propose a novel neural network architecture accepting graphs of arbitrary structure. Given a dataset containing graphs in the form of (G,y) where G is a graph and y is its class, we aim to develop neural networks that read the graphs directly and learn a classification function. There are two main challenges: 1) how to extract useful features characterizing the rich information encoded in a graph for classification purpose, and 2) how to sequentially read a graph in a meaningful and consistent order. To address the first challenge, we design a localized graph convolution model and show its connection with two graph kernels. To address the second challenge, we design a novel SortPooling layer which sorts graph vertices in a consistent order so that traditional neural networks can be trained on the graphs. Experiments on benchmark graph classification datasets demonstrate that the proposed architecture achieves highly competitive performance with state-of-the-art graph kernels and other graph neural network methods. Moreover, the architecture allows end-to-end gradient-based training with original graphs, without the need to first transform graphs into vectors.\n \n'}}, {'citedPaper': {'paperId': '4aa8e316bc1c5959537517ed16b4bf81b4bd73ed', 'title': 'Graph Partition Neural Networks for Semi-Supervised Classification', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1803.06272, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2246396', 'name': 'Renjie Liao'}, {'authorId': '2107692', 'name': 'Marc Brockschmidt'}, {'authorId': '1725299', 'name': 'Daniel Tarlow'}, {'authorId': '35058304', 'name': 'Alexander L. Gaunt'}, {'authorId': '2422559', 'name': 'R. Urtasun'}, {'authorId': '1804104', 'name': 'R. Zemel'}], 'abstract': 'We present graph partition neural networks (GPNN), an extension of graph neural networks (GNNs) able to handle extremely large graphs. GPNNs alternate between locally propagating information between nodes in small subgraphs and globally propagating information between the subgraphs. To efficiently partition graphs, we experiment with several partitioning algorithms and also propose a novel variant for fast processing of large scale graphs. We extensively test our model on a variety of semi-supervised node classification tasks. Experimental results indicate that GPNNs are either superior or comparable to state-of-the-art methods on a wide variety of datasets for graph-based semi-supervised classification. We also show that GPNNs can achieve similar performance as standard GNNs with fewer propagation steps.'}}, {'citedPaper': {'paperId': 'a73531abe4cafbccd5b3e949e84410a50016bd33', 'title': 'SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels', 'openAccessPdf': {'url': 'https://arxiv.org/pdf/1711.08920', 'status': 'GREEN', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1711.08920, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '3410500', 'name': 'Matthias Fey'}, {'authorId': '9572099', 'name': 'J. E. Lenssen'}, {'authorId': '2595376', 'name': 'F. Weichert'}, {'authorId': '2151194196', 'name': 'H. Müller'}], 'abstract': 'We present Spline-based Convolutional Neural Networks (SplineCNNs), a variant of deep neural networks for irregular structured and geometric input, e.g., graphs or meshes. Our main contribution is a novel convolution operator based on B-splines, that makes the computation time independent from the kernel size due to the local support property of the B-spline basis functions. As a result, we obtain a generalization of the traditional CNN convolution operator by using continuous kernel functions parametrized by a fixed number of trainable weights. In contrast to related approaches that filter in the spectral domain, the proposed method aggregates features purely in the spatial domain. In addition, SplineCNN allows entire end-to-end training of deep architectures, using only the geometric structure as input, instead of handcrafted feature descriptors. For validation, we apply our method on tasks from the fields of image graph classification, shape correspondence and graph node classification, and show that it outperforms or pars state-of-the-art approaches while being significantly faster and having favorable properties like domain-independence. Our source code is available on GitHub1.'}}, {'citedPaper': {'paperId': 'c751ab01aedc2888a7fe6e8b4f77ab1afa94072f', 'title': 'Protein Interface Prediction using Graph Convolutional Networks', 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'authors': [{'authorId': '29837788', 'name': 'A. Fout'}, {'authorId': '40205705', 'name': 'Jonathon Byrd'}, {'authorId': '11545747', 'name': 'Basir Shariat'}, {'authorId': '1399356737', 'name': 'Asa Ben-Hur'}], 'abstract': None}}, {'citedPaper': {'paperId': '33998aff64ce51df8dee45989cdca4b6b1329ec4', 'title': 'Graph Attention Networks', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1710.10903, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '3444569', 'name': 'Petar Velickovic'}, {'authorId': '7153363', 'name': 'Guillem Cucurull'}, {'authorId': '8742492', 'name': 'Arantxa Casanova'}, {'authorId': '144290131', 'name': 'Adriana Romero'}, {'authorId': '144269589', 'name': 'P. Lio’'}, {'authorId': '1751762', 'name': 'Yoshua Bengio'}], 'abstract': "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training)."}}, {'citedPaper': {'paperId': 'ecf6c42d84351f34e1625a6a2e4cc6526da45c74', 'title': 'Representation Learning on Graphs: Methods and Applications', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1709.05584, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '49437682', 'name': 'William L. Hamilton'}, {'authorId': '83539859', 'name': 'Rex Ying'}, {'authorId': '1702139', 'name': 'J. Leskovec'}], 'abstract': 'Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.'}}, {'citedPaper': {'paperId': 'aaf046c4da99ee6184f3fd31961a9967272152f9', 'title': 'Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1709.04555, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2400119', 'name': 'Wengong Jin'}, {'authorId': '13027820', 'name': 'Connor W. Coley'}, {'authorId': '1741283', 'name': 'R. Barzilay'}, {'authorId': '35132120', 'name': 'T. Jaakkola'}], 'abstract': 'The prediction of organic reaction outcomes is a fundamental problem in computational chemistry. Since a reaction may involve hundreds of atoms, fully exploring the space of possible transformations is intractable. The current solution utilizes reaction templates to limit the space, but it suffers from coverage and efficiency issues. In this paper, we propose a template-free approach to efficiently explore the space of product molecules by first pinpointing the reaction center -- the set of nodes and edges where graph edits occur. Since only a small number of atoms contribute to reaction center, we can directly enumerate candidate products. The generated candidates are scored by a Weisfeiler-Lehman Difference Network that models high-order interactions between changes occurring at nodes across the molecule. Our framework outperforms the top-performing template-based approach with a 10\\% margin, while running orders of magnitude faster. Finally, we demonstrate that the model accuracy rivals the performance of domain experts.'}}, {'citedPaper': {'paperId': '5bf31dc4bd54b623008c13f8bc8954dc7c9a2d80', 'title': 'SchNet: A continuous-filter convolutional neural network for modeling quantum interactions', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1706.08566, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '31961144', 'name': 'Kristof Schütt'}, {'authorId': '2113697', 'name': 'Pieter-Jan Kindermans'}, {'authorId': '29800712', 'name': 'Huziel Enoc Sauceda Felix'}, {'authorId': '7631063', 'name': 'Stefan Chmiela'}, {'authorId': '2462983', 'name': 'A. Tkatchenko'}, {'authorId': '145034054', 'name': 'K. Müller'}], 'abstract': 'Deep learning has the potential to revolutionize quantum chemistry as it is ideally suited to learn representations for structured data and speed up the exploration of chemical space. While convolutional neural networks have proven to be the first choice for images, audio and video data, the atoms in molecules are not restricted to a grid. Instead, their precise locations contain essential physical information, that would get lost if discretized. Thus, we propose to use continuous-filter convolutional layers to be able to model local correlations without requiring the data to lie on a grid. We apply those layers in SchNet: a novel deep learning architecture modeling quantum interactions in molecules. We obtain a joint model for the total energy and interatomic forces that follows fundamental quantum-chemical principles. This includes rotationally invariant energy predictions and a smooth, differentiable potential energy surface. Our architecture achieves state-of-the-art performance for benchmarks of equilibrium molecules and molecular dynamics trajectories. Finally, we introduce a more challenging benchmark with chemical and structural variations that suggests the path for further work.'}}, {'citedPaper': {'paperId': '6b7d6e6416343b2a122f8416e69059ce919026ef', 'title': 'Inductive Representation Learning on Large Graphs', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1706.02216, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '49437682', 'name': 'William L. Hamilton'}, {'authorId': '4058003', 'name': 'Z. Ying'}, {'authorId': '1702139', 'name': 'J. Leskovec'}], 'abstract': "Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions."}}, {'citedPaper': {'paperId': 'ee9c6aeb6e29cf3c9081df2cc100b8203ebf5cff', 'title': 'Deriving Neural Architectures from Sequence and Graph Kernels', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1705.09037, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '49986267', 'name': 'Tao Lei'}, {'authorId': '2400119', 'name': 'Wengong Jin'}, {'authorId': '1741283', 'name': 'R. Barzilay'}, {'authorId': '35132120', 'name': 'T. Jaakkola'}], 'abstract': 'The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects (cf. filters in CNN) via the kernels. Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training. We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art results across these applications.'}}, {'citedPaper': {'paperId': '1a39bb2caa151d15efd6718f3a80d9f4bff95af2', 'title': 'Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs', 'openAccessPdf': {'url': 'https://arxiv.org/pdf/1704.02901', 'status': 'GREEN', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1704.02901, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '3451689', 'name': 'M. Simonovsky'}, {'authorId': '2505902', 'name': 'N. Komodakis'}], 'abstract': 'A number of problems can be formulated as prediction on graph-structured data. In this work, we generalize the convolution operator from regular grids to arbitrary graphs while avoiding the spectral domain, which allows us to handle graphs of varying size and connectivity. To move beyond a simple diffusion, filter weights are conditioned on the specific edge labels in the neighborhood of a vertex. Together with the proper choice of graph coarsening, we explore constructing deep neural networks for graph classification. In particular, we demonstrate the generality of our formulation in point cloud classification, where we set the new state of the art, and on a graph classification dataset, where we outperform other deep learning approaches.'}}, {'citedPaper': {'paperId': 'e24cdf73b3e7e590c2fe5ecac9ae8aa983801367', 'title': 'Neural Message Passing for Quantum Chemistry', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1704.01212, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2058362', 'name': 'J. Gilmer'}, {'authorId': '2601641', 'name': 'S. Schoenholz'}, {'authorId': '119508204', 'name': 'Patrick F. Riley'}, {'authorId': '1689108', 'name': 'O. Vinyals'}, {'authorId': '35188630', 'name': 'George E. Dahl'}], 'abstract': 'Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.'}}, {'citedPaper': {'paperId': 'cd8a9914d50b0ac63315872530274d158d6aff09', 'title': 'Modeling Relational Data with Graph Convolutional Networks', 'openAccessPdf': {'url': '', 'status': 'CLOSED', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1703.06103, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '8804828', 'name': 'M. Schlichtkrull'}, {'authorId': '41016725', 'name': 'Thomas Kipf'}, {'authorId': '2789097', 'name': 'Peter Bloem'}, {'authorId': '9965217', 'name': 'Rianne van den Berg'}, {'authorId': '144889265', 'name': 'Ivan Titov'}, {'authorId': '1678311', 'name': 'M. Welling'}], 'abstract': 'Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.'}}, {'citedPaper': {'paperId': '0e779fd59353a7f1f5b559b9d65fa4bfe367890c', 'title': 'Geometric Deep Learning: Going beyond Euclidean data', 'openAccessPdf': {'url': 'http://arxiv.org/pdf/1611.08097', 'status': 'GREEN', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1611.08097, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1732570', 'name': 'M. Bronstein'}, {'authorId': '143627859', 'name': 'Joan Bruna'}, {'authorId': '1688882', 'name': 'Yann LeCun'}, {'authorId': '3149531', 'name': 'Arthur Szlam'}, {'authorId': '1697397', 'name': 'P. Vandergheynst'}], 'abstract': 'Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.'}}, {'citedPaper': {'paperId': '36eff562f65125511b5dfab68ce7f7a943c27478', 'title': 'Semi-Supervised Classification with Graph Convolutional Networks', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1609.02907, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '41016725', 'name': 'Thomas Kipf'}, {'authorId': '1678311', 'name': 'M. Welling'}], 'abstract': 'We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.'}}, {'citedPaper': {'paperId': 'c41eb895616e453dcba1a70c9b942c5063cc656c', 'title': 'Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1606.09375, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '3422350', 'name': 'Michaël Defferrard'}, {'authorId': '2549032', 'name': 'X. Bresson'}, {'authorId': '1697397', 'name': 'P. Vandergheynst'}], 'abstract': "In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs."}}, {'citedPaper': {'paperId': 'da2e04453b6f0d89ee75e6f68d619d936cd9c0b5', 'title': 'On Valid Optimal Assignment Kernels and Applications to Graph Classification', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1606.01141, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1810077', 'name': 'Nils M. Kriege'}, {'authorId': '144790942', 'name': 'P. Giscard'}, {'authorId': '2111010157', 'name': 'Richard C. Wilson'}], 'abstract': 'The success of kernel methods has initiated the design of novel positive semidefinite functions, in particular for structured data. A leading design paradigm for this is the convolution kernel, which decomposes structured objects into their parts and sums over all pairs of parts. Assignment kernels, in contrast, are obtained from an optimal bijection between parts, which can provide a more valid notion of similarity. In general however, optimal assignments yield indefinite functions, which complicates their use in kernel methods. We characterize a class of base kernels used to compare parts that guarantees positive semidefinite optimal assignment kernels. These base kernels give rise to hierarchies from which the optimal assignment kernels are computed in linear time by histogram intersection. We apply these results by developing the Weisfeiler-Lehman optimal assignment kernel for graphs. It provides high classification accuracy on widely-used benchmark data sets improving over the original Weisfeiler-Lehman kernel.'}}, {'citedPaper': {'paperId': '7c6de5a9e02a779e24504619050c6118f4eac181', 'title': 'Learning Convolutional Neural Networks for Graphs', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1605.05273, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2780262', 'name': 'Mathias Niepert'}, {'authorId': '24931083', 'name': 'Mohamed Ahmed'}, {'authorId': '1712289', 'name': 'Konstantin Kutzkov'}], 'abstract': 'Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.'}}, {'citedPaper': {'paperId': '322cf9bcde458a45eaeca989a1eec92f7c6db984', 'title': 'Discriminative Embeddings of Latent Variable Models for Structured Data', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1603.05629, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2791430', 'name': 'H. Dai'}, {'authorId': '144445933', 'name': 'Bo Dai'}, {'authorId': '1779453', 'name': 'Le Song'}], 'abstract': 'Kernel classifiers and regressors designed for structured data, such as sequences, trees and graphs, have significantly advanced a number of interdisciplinary areas such as computational biology and drug design. Typically, kernels are designed beforehand for a data type which either exploit statistics of the structures or make use of probabilistic generative models, and then a discriminative classifier is learned based on the kernels via convex optimization. However, such an elegant two-stage approach also limited kernel methods from scaling up to millions of data points, and exploiting discriminative information to learn feature representations. \n \nWe propose, structure2vec, an effective and scalable approach for structured data representation based on the idea of embedding latent variable models into feature spaces, and learning such feature spaces using discriminative information. Interestingly, structure2vec extracts features by performing a sequence of function mappings in a way similar to graphical model inference procedures, such as mean field and belief propagation. In applications involving millions of data points, we showed that structure2vec runs 2 times faster, produces models which are 10, 000 times smaller, while at the same time achieving the state-of-the-art predictive performance.'}}, {'citedPaper': {'paperId': '3ce05152dbedab572167e031b90d677c13b49767', 'title': 'A Structural Smoothing Framework For Robust Graph Comparison', 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'authors': [{'authorId': '2304445773', 'name': 'Pinar Yanardag'}, {'authorId': '145713876', 'name': 'S. Vishwanathan'}], 'abstract': None}}, {'citedPaper': {'paperId': 'd01379ebb53c66a4ccf5f4959d904dcf9e161e41', 'title': 'Order Matters: Sequence to sequence for sets', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1511.06391, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1689108', 'name': 'O. Vinyals'}, {'authorId': '1751569', 'name': 'Samy Bengio'}, {'authorId': '1942300', 'name': 'M. Kudlur'}], 'abstract': 'Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models.'}}, {'citedPaper': {'paperId': '492f57ee9ceb61fb5a47ad7aebfec1121887a175', 'title': 'Gated Graph Sequence Neural Networks', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1511.05493, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '47002813', 'name': 'Yujia Li'}, {'authorId': '1725299', 'name': 'Daniel Tarlow'}, {'authorId': '2107692', 'name': 'Marc Brockschmidt'}, {'authorId': '1804104', 'name': 'R. Zemel'}], 'abstract': 'Abstract: Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.'}}, {'citedPaper': {'paperId': '5d1bfeed240709725c78bc72ea40e55410b373dc', 'title': 'Convolutional Networks on Graphs for Learning Molecular Fingerprints', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1509.09292, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1704657', 'name': 'D. Duvenaud'}, {'authorId': '1683298', 'name': 'D. Maclaurin'}, {'authorId': '1422175619', 'name': 'J. Aguilera-Iparraguirre'}, {'authorId': '2344011563', 'name': 'Rafael Gómez-Bombarelli'}, {'authorId': '145916942', 'name': 'Timothy D. Hirzel'}, {'authorId': '1380248954', 'name': 'Alán Aspuru-Guzik'}, {'authorId': '1722180', 'name': 'Ryan P. Adams'}], 'abstract': 'We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.'}}, {'citedPaper': {'paperId': '995c5f5e62614fcb4d2796ad2faab969da51713e', 'title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1502.03167, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2054165706', 'name': 'Sergey Ioffe'}, {'authorId': '2574060', 'name': 'Christian Szegedy'}], 'abstract': "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."}}, {'citedPaper': {'paperId': '5e925a9f1e20df61d1e860a7aa71894b35a1c186', 'title': 'Spectral Networks and Locally Connected Networks on Graphs', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1312.6203, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '143627859', 'name': 'Joan Bruna'}, {'authorId': '2563432', 'name': 'Wojciech Zaremba'}, {'authorId': '3149531', 'name': 'Arthur Szlam'}, {'authorId': '1688882', 'name': 'Yann LeCun'}], 'abstract': 'Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.'}}, {'citedPaper': {'paperId': 'd87762aa2ef5a259825cf75d5865d7ddf86b7018', 'title': 'Scalable kernels for graphs with continuous attributes', 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'authors': [{'authorId': '1808965', 'name': 'Aasa Feragen'}, {'authorId': '2423123', 'name': 'Niklas Kasenburg'}, {'authorId': '152800798', 'name': 'Jens Petersen'}, {'authorId': '32895376', 'name': 'Marleen de Bruijne'}, {'authorId': '1704422', 'name': 'Karsten M. Borgwardt'}], 'abstract': None}}, {'citedPaper': {'paperId': 'b3e025049142be3d97d559cca12b027f16c6349e', 'title': 'Deep Architectures and Deep Learning in Chemoinformatics: The Prediction of Aqueous Solubility for Drug-Like Molecules', 'openAccessPdf': {'url': 'https://europepmc.org/articles/pmc3739985?pdf=render', 'status': 'GREEN', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1021/ci400187y?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1021/ci400187y, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '3318293', 'name': 'A. Lusci'}, {'authorId': '2028880', 'name': 'G. Pollastri'}, {'authorId': '144902513', 'name': 'P. Baldi'}], 'abstract': 'Shallow machine learning methods have been applied to chemoinformatics problems with some success. As more data becomes available and more complex problems are tackled, deep machine learning methods may also become useful. Here, we present a brief overview of deep learning methods and show in particular how recursive neural network approaches can be applied to the problem of predicting molecular properties. However, molecules are typically described by undirected cyclic graphs, while recursive approaches typically use directed acyclic graphs. Thus, we develop methods to address this discrepancy, essentially by considering an ensemble of recursive neural networks associated with all possible vertex-centered acyclic orientations of the molecular graph. One advantage of this approach is that it relies only minimally on the identification of suitable molecular descriptors because suitable representations are learned automatically from the data. Several variants of this approach are applied to the problem of predicting aqueous solubility and tested on four benchmark data sets. Experimental results show that the performance of the deep learning methods matches or exceeds the performance of other state-of-the-art methods according to several evaluation metrics and expose the fundamental limitations arising from training sets that are too small or too noisy. A Web-based predictor, AquaSol, is available online through the ChemDB portal ( cdb.ics.uci.edu ) together with additional material.'}}, {'citedPaper': {'paperId': 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', 'title': 'ImageNet classification with deep convolutional neural networks', 'openAccessPdf': {'url': 'http://dl.acm.org/ft_gateway.cfm?id=3065386&type=pdf', 'status': 'BRONZE', 'license': None, 'disclaimer': "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3065386?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3065386, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, 'authors': [{'authorId': '2064160', 'name': 'A. Krizhevsky'}, {'authorId': '1701686', 'name': 'I. Sutskever'}, {'authorId': '1695689', 'name': 'Geoffrey E. Hinton'}], 'abstract': None}}, {'citedPaper': {'paperId': '273dfbcb68080251f5e9ff38b4413d7bd84b10a1', 'title': 'LIBSVM: A library for support vector machines', 'openAccessPdf': {'url': '', 'status': 'CLOSED', 'license': None, 'disclaimer': "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1961189.1961199?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1961189.1961199, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, 'authors': [{'authorId': None, 'name': 'Chih-Chung Chang'}, {'authorId': '1711460', 'name': 'Chih-Jen Lin'}], 'abstract': None}}, {'citedPaper': {'paperId': '7e1874986cf6433fabf96fff93ef42b60bdc49f8', 'title': 'Weisfeiler-Lehman Graph Kernels', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.5555/1953048.2078187?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5555/1953048.2078187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1898417', 'name': 'N. Shervashidze'}, {'authorId': '31500557', 'name': 'Pascal Schweitzer'}, {'authorId': '1711983', 'name': 'E. J. V. Leeuwen'}, {'authorId': '1698752', 'name': 'K. Mehlhorn'}, {'authorId': '1704422', 'name': 'Karsten M. Borgwardt'}], 'abstract': 'In this article, we propose a family of efficient kernels for large graphs with discrete node labels. Key to our method is a rapid feature extraction scheme based on the Weisfeiler-Lehman test of isomorphism on graphs. It maps the original graph to a sequence of graphs, whose node attributes capture topological and label information. A family of kernels can be defined based on this Weisfeiler-Lehman sequence of graphs, including a highly efficient kernel comparing subtree-like patterns. Its runtime scales only linearly in the number of edges of the graphs and the length of the Weisfeiler-Lehman graph sequence. In our experimental evaluation, our kernels outperform state-of-the-art graph kernels on several graph classification benchmark data sets in terms of accuracy and runtime. Our kernels open the door to large-scale applications of graph kernels in various disciplines such as computational biology and social network analysis.'}}, {'citedPaper': {'paperId': 'a7fc751cd95bd1a409a26daaef69fc3aa8a35e0e', 'title': 'Efficient graphlet kernels for large graph comparison', 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'authors': [{'authorId': '1898417', 'name': 'N. Shervashidze'}, {'authorId': '145713876', 'name': 'S. Vishwanathan'}, {'authorId': '70590076', 'name': 'Tobias Petri'}, {'authorId': '1698752', 'name': 'K. Mehlhorn'}, {'authorId': '1704422', 'name': 'Karsten M. Borgwardt'}], 'abstract': None}}, {'citedPaper': {'paperId': '34795caa356317f2c662bd4553cd7608e0aeb657', 'title': 'Weighted Graph Cuts without Eigenvectors A Multilevel Approach', 'openAccessPdf': {'url': '', 'status': 'CLOSED', 'license': None, 'disclaimer': "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TPAMI.2007.1115?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TPAMI.2007.1115, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, 'authors': [{'authorId': '1783667', 'name': 'I. Dhillon'}, {'authorId': '1853389', 'name': 'Yuqiang Guan'}, {'authorId': '1692670', 'name': 'Brian Kulis'}], 'abstract': None}}, {'citedPaper': {'paperId': '70ff05e93f4eaeef4b8b51ba4e11d1c4cdf856ca', 'title': 'Shortest-path kernels on graphs', 'openAccessPdf': {'url': 'http://cbio.ensmp.fr/~jvert/svn/bibli/local/Borgwardt2005Shortest-Path.pdf', 'status': 'GREEN', 'license': None, 'disclaimer': "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDM.2005.132?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDM.2005.132, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, 'authors': [{'authorId': '1704422', 'name': 'Karsten M. Borgwardt'}, {'authorId': '1688561', 'name': 'H. Kriegel'}], 'abstract': None}}, {'citedPaper': {'paperId': '20036cd7fac884130f642d3df30b1fd7fb24c6d2', 'title': 'Automatic Generation of Complementary Descriptors with Molecular Graph Networks', 'openAccessPdf': {'url': '', 'status': 'CLOSED', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1021/CI049613B?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1021/CI049613B, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2114450', 'name': 'Christian Merkwirth'}, {'authorId': '49370597', 'name': 'Thomas Lengauer'}], 'abstract': 'We describe a method for the automatic generation of weakly correlated descriptors for molecular data sets. The method can be regarded as a statistical learning procedure that turns the molecular graph, representing the 2D formula of the compound, into an adaptive whole molecule composite descriptor. By translating the molecular graph structure into a dynamical system, the algorithm can compute an output value that is highly sensitive to the molecular topology. This system can be trained by gradient descent techniques, which rely on the efficient calculation of the gradient by back-propagation. We present computational experiments concerning the classification of the Developmental Therapeutics Program AIDS antiviral screen data set on which the performance of the method compares with that of approaches based on substructure comparison.'}}, {'citedPaper': {'paperId': 'c2815f76c690ec2ca6523b1faacf9b6a401bdff1', 'title': 'Distinguishing enzyme structures from non-enzymes without alignments.', 'openAccessPdf': {'url': 'http://cbio.ensmp.fr/~jvert/svn/bibli/local/Dobson2003Distinguishing.pdf', 'status': 'GREEN', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1016/S0022-2836(03)00628-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/S0022-2836(03)00628-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '145002767', 'name': 'P. Dobson'}, {'authorId': '1970705', 'name': 'A. Doig'}], 'abstract': None}}, {'citedPaper': {'paperId': '25ec680e94d84e93e3bd0409d6084bfb43c4cdb6', 'title': 'Processing directed acyclic graphs with recursive neural networks', 'openAccessPdf': {'url': '', 'status': 'CLOSED', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/72.963781?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/72.963781, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '144020416', 'name': 'M. Bianchini'}, {'authorId': '145467467', 'name': 'M. Gori'}, {'authorId': '47260481', 'name': 'F. Scarselli'}], 'abstract': 'Recursive neural networks are conceived for processing graphs and extend the well-known recurrent model for processing sequences. In Frasconi et al. (1998), recursive neural networks can deal only with directed ordered acyclic graphs (DOAGs), in which the children of any given node are ordered. While this assumption is reasonable in some applications, it introduces unnecessary constraints in others. In this paper, it is shown that the constraint on the ordering can be relaxed by using an appropriate weight sharing, that guarantees the independence of the network output with respect to the permutations of the arcs leaving from each node. The method can be used with graphs having low connectivity and, in particular, few outcoming arcs. Some theoretical properties of the proposed architecture are given. They guarantee that the approximation capabilities are maintained, despite the weight sharing.'}}, {'citedPaper': {'paperId': '89b1766a77f7a34aadd67861123d5cf3cd3ece52', 'title': 'Neural Relational Inference for Interacting Systems', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1802.04687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1643311826', 'name': 'T. Fetaya'}, {'authorId': '50844928', 'name': 'E. Wang'}, {'authorId': '1643758681', 'name': 'K.-C. Welling'}, {'authorId': '47446467', 'name': 'M. Zemel'}, {'authorId': '41016725', 'name': 'Thomas Kipf'}, {'authorId': '2645055', 'name': 'Ethan Fetaya'}, {'authorId': '122782486', 'name': 'Kuan-Chieh Jackson Wang'}, {'authorId': '1678311', 'name': 'M. Welling'}, {'authorId': '1804104', 'name': 'R. Zemel'}], 'abstract': "Interacting systems are prevalent in nature, from dynamical systems in physics to complex societal dynamics. The interplay of components can give rise to complex behavior, which can often be explained using a simple model of the system's constituent parts. In this work, we introduce the neural relational inference (NRI) model: an unsupervised model that learns to infer interactions while simultaneously learning the dynamics purely from observational data. Our model takes the form of a variational auto-encoder, in which the latent code represents the underlying interaction graph and the reconstruction is based on graph neural networks. In experiments on simulated physical systems, we show that our NRI model can accurately recover ground-truth interactions in an unsupervised manner. We further demonstrate that we can find an interpretable structure and predict complex dynamics in real motion capture and sports tracking data."}}, {'citedPaper': {'paperId': None, 'title': 'Benchmark data sets for graph kernels, 2016', 'openAccessPdf': None, 'authors': [], 'abstract': None}}, {'citedPaper': {'paperId': '3efd851140aa28e95221b55fcc5659eea97b172d', 'title': 'The Graph Neural Network Model', 'openAccessPdf': {'url': 'https://ro.uow.edu.au/articles/journal_contribution/The_graph_neural_network_model/27757629/1/files/50520873.pdf', 'status': 'GREEN', 'license': 'other-oa', 'disclaimer': "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNN.2008.2005605?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNN.2008.2005605, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, 'authors': [{'authorId': '47260481', 'name': 'F. Scarselli'}, {'authorId': '145467467', 'name': 'M. Gori'}, {'authorId': '1733691', 'name': 'A. Tsoi'}, {'authorId': '1784450', 'name': 'M. Hagenbuchner'}, {'authorId': '3073217', 'name': 'G. Monfardini'}], 'abstract': None}}, {'citedPaper': {'paperId': 'ba640c0682b242cb480bb4eb5b934ee6db949269', 'title': 'Protein function prediction via graph kernels', 'openAccessPdf': {'url': 'https://academic.oup.com/bioinformatics/article-pdf/21/suppl_1/i47/524364/bti1007.pdf', 'status': 'BRONZE', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1093/bioinformatics/bti1007?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1093/bioinformatics/bti1007, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1704422', 'name': 'Karsten M. Borgwardt'}, {'authorId': '1706780', 'name': 'Cheng Soon Ong'}, {'authorId': '2617402', 'name': 'Stefan Schönauer'}, {'authorId': '145713876', 'name': 'S. Vishwanathan'}, {'authorId': '46234526', 'name': 'Alex Smola'}, {'authorId': '1688561', 'name': 'H. Kriegel'}], 'abstract': 'MOTIVATION\nComputational approaches to protein function prediction infer protein function by finding proteins with similar sequence, structure, surface clefts, chemical properties, amino acid motifs, interaction partners or phylogenetic profiles. We present a new approach that combines sequential, structural and chemical information into one graph model of proteins. We predict functional class membership of enzymes and non-enzymes using graph kernels and support vector machine classification on these protein graphs.\n\n\nRESULTS\nOur graph model, derivable from protein sequence and structure only, is competitive with vector models that require additional protein information, such as the size of surface pockets. If we include this extra information into our graph model, our classifier yields significantly higher accuracy levels than the vector models. Hyperkernels allow us to select and to optimally combine the most relevant node attributes in our protein graphs. We have laid the foundation for a protein function prediction system that integrates protein information from various sources efficiently and effectively.\n\n\nAVAILABILITY\nMore information available via www.dbs.ifi.lmu.de/Mitarbeiter/borgwardt.html.'}}]}
[{'title': 'Graph Capsule Convolutional Neural Networks', 'abstract': 'Graph Convolutional Neural Networks (GCNNs) are the most recent exciting advancement in deep learning field and their applications are quickly spreading in multi-cross-domains including bioinformatics, chemoinformatics, social networks, natural language processing and computer vision. In this paper, we expose and tackle some of the basic weaknesses of a GCNN model with a capsule idea presented in \\cite{hinton2011transforming} and propose our Graph Capsule Network (GCAPS-CNN) model. In addition, we design our GCAPS-CNN model to solve especially graph classification problem which current GCNN models find challenging. Through extensive experiments, we show that our proposed Graph Capsule Network can significantly outperforms both the existing state-of-art deep learning methods and graph kernels on graph classification benchmark datasets.'}, {'title': 'An End-to-End Deep Learning Architecture for Graph Classification', 'abstract': '\n \n Neural networks are typically designed to deal with data in tensor forms. In this paper, we propose a novel neural network architecture accepting graphs of arbitrary structure. Given a dataset containing graphs in the form of (G,y) where G is a graph and y is its class, we aim to develop neural networks that read the graphs directly and learn a classification function. There are two main challenges: 1) how to extract useful features characterizing the rich information encoded in a graph for classification purpose, and 2) how to sequentially read a graph in a meaningful and consistent order. To address the first challenge, we design a localized graph convolution model and show its connection with two graph kernels. To address the second challenge, we design a novel SortPooling layer which sorts graph vertices in a consistent order so that traditional neural networks can be trained on the graphs. Experiments on benchmark graph classification datasets demonstrate that the proposed architecture achieves highly competitive performance with state-of-the-art graph kernels and other graph neural network methods. Moreover, the architecture allows end-to-end gradient-based training with original graphs, without the need to first transform graphs into vectors.\n \n'}, {'title': 'Graph Partition Neural Networks for Semi-Supervised Classification', 'abstract': 'We present graph partition neural networks (GPNN), an extension of graph neural networks (GNNs) able to handle extremely large graphs. GPNNs alternate between locally propagating information between nodes in small subgraphs and globally propagating information between the subgraphs. To efficiently partition graphs, we experiment with several partitioning algorithms and also propose a novel variant for fast processing of large scale graphs. We extensively test our model on a variety of semi-supervised node classification tasks. Experimental results indicate that GPNNs are either superior or comparable to state-of-the-art methods on a wide variety of datasets for graph-based semi-supervised classification. We also show that GPNNs can achieve similar performance as standard GNNs with fewer propagation steps.'}, {'title': 'SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels', 'abstract': 'We present Spline-based Convolutional Neural Networks (SplineCNNs), a variant of deep neural networks for irregular structured and geometric input, e.g., graphs or meshes. Our main contribution is a novel convolution operator based on B-splines, that makes the computation time independent from the kernel size due to the local support property of the B-spline basis functions. As a result, we obtain a generalization of the traditional CNN convolution operator by using continuous kernel functions parametrized by a fixed number of trainable weights. In contrast to related approaches that filter in the spectral domain, the proposed method aggregates features purely in the spatial domain. In addition, SplineCNN allows entire end-to-end training of deep architectures, using only the geometric structure as input, instead of handcrafted feature descriptors. For validation, we apply our method on tasks from the fields of image graph classification, shape correspondence and graph node classification, and show that it outperforms or pars state-of-the-art approaches while being significantly faster and having favorable properties like domain-independence. Our source code is available on GitHub1.'}, {'title': 'Protein Interface Prediction using Graph Convolutional Networks', 'abstract': None}, {'title': 'Graph Attention Networks', 'abstract': "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training)."}, {'title': 'Representation Learning on Graphs: Methods and Applications', 'abstract': 'Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.'}, {'title': 'Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network', 'abstract': 'The prediction of organic reaction outcomes is a fundamental problem in computational chemistry. Since a reaction may involve hundreds of atoms, fully exploring the space of possible transformations is intractable. The current solution utilizes reaction templates to limit the space, but it suffers from coverage and efficiency issues. In this paper, we propose a template-free approach to efficiently explore the space of product molecules by first pinpointing the reaction center -- the set of nodes and edges where graph edits occur. Since only a small number of atoms contribute to reaction center, we can directly enumerate candidate products. The generated candidates are scored by a Weisfeiler-Lehman Difference Network that models high-order interactions between changes occurring at nodes across the molecule. Our framework outperforms the top-performing template-based approach with a 10\\% margin, while running orders of magnitude faster. Finally, we demonstrate that the model accuracy rivals the performance of domain experts.'}, {'title': 'SchNet: A continuous-filter convolutional neural network for modeling quantum interactions', 'abstract': 'Deep learning has the potential to revolutionize quantum chemistry as it is ideally suited to learn representations for structured data and speed up the exploration of chemical space. While convolutional neural networks have proven to be the first choice for images, audio and video data, the atoms in molecules are not restricted to a grid. Instead, their precise locations contain essential physical information, that would get lost if discretized. Thus, we propose to use continuous-filter convolutional layers to be able to model local correlations without requiring the data to lie on a grid. We apply those layers in SchNet: a novel deep learning architecture modeling quantum interactions in molecules. We obtain a joint model for the total energy and interatomic forces that follows fundamental quantum-chemical principles. This includes rotationally invariant energy predictions and a smooth, differentiable potential energy surface. Our architecture achieves state-of-the-art performance for benchmarks of equilibrium molecules and molecular dynamics trajectories. Finally, we introduce a more challenging benchmark with chemical and structural variations that suggests the path for further work.'}, {'title': 'Inductive Representation Learning on Large Graphs', 'abstract': "Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions."}, {'title': 'Deriving Neural Architectures from Sequence and Graph Kernels', 'abstract': 'The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects (cf. filters in CNN) via the kernels. Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training. We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art results across these applications.'}, {'title': 'Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs', 'abstract': 'A number of problems can be formulated as prediction on graph-structured data. In this work, we generalize the convolution operator from regular grids to arbitrary graphs while avoiding the spectral domain, which allows us to handle graphs of varying size and connectivity. To move beyond a simple diffusion, filter weights are conditioned on the specific edge labels in the neighborhood of a vertex. Together with the proper choice of graph coarsening, we explore constructing deep neural networks for graph classification. In particular, we demonstrate the generality of our formulation in point cloud classification, where we set the new state of the art, and on a graph classification dataset, where we outperform other deep learning approaches.'}, {'title': 'Neural Message Passing for Quantum Chemistry', 'abstract': 'Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.'}, {'title': 'Modeling Relational Data with Graph Convolutional Networks', 'abstract': 'Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.'}, {'title': 'Geometric Deep Learning: Going beyond Euclidean data', 'abstract': 'Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.'}, {'title': 'Semi-Supervised Classification with Graph Convolutional Networks', 'abstract': 'We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.'}, {'title': 'Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering', 'abstract': "In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs."}, {'title': 'On Valid Optimal Assignment Kernels and Applications to Graph Classification', 'abstract': 'The success of kernel methods has initiated the design of novel positive semidefinite functions, in particular for structured data. A leading design paradigm for this is the convolution kernel, which decomposes structured objects into their parts and sums over all pairs of parts. Assignment kernels, in contrast, are obtained from an optimal bijection between parts, which can provide a more valid notion of similarity. In general however, optimal assignments yield indefinite functions, which complicates their use in kernel methods. We characterize a class of base kernels used to compare parts that guarantees positive semidefinite optimal assignment kernels. These base kernels give rise to hierarchies from which the optimal assignment kernels are computed in linear time by histogram intersection. We apply these results by developing the Weisfeiler-Lehman optimal assignment kernel for graphs. It provides high classification accuracy on widely-used benchmark data sets improving over the original Weisfeiler-Lehman kernel.'}, {'title': 'Learning Convolutional Neural Networks for Graphs', 'abstract': 'Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.'}, {'title': 'Discriminative Embeddings of Latent Variable Models for Structured Data', 'abstract': 'Kernel classifiers and regressors designed for structured data, such as sequences, trees and graphs, have significantly advanced a number of interdisciplinary areas such as computational biology and drug design. Typically, kernels are designed beforehand for a data type which either exploit statistics of the structures or make use of probabilistic generative models, and then a discriminative classifier is learned based on the kernels via convex optimization. However, such an elegant two-stage approach also limited kernel methods from scaling up to millions of data points, and exploiting discriminative information to learn feature representations. \n \nWe propose, structure2vec, an effective and scalable approach for structured data representation based on the idea of embedding latent variable models into feature spaces, and learning such feature spaces using discriminative information. Interestingly, structure2vec extracts features by performing a sequence of function mappings in a way similar to graphical model inference procedures, such as mean field and belief propagation. In applications involving millions of data points, we showed that structure2vec runs 2 times faster, produces models which are 10, 000 times smaller, while at the same time achieving the state-of-the-art predictive performance.'}, {'title': 'A Structural Smoothing Framework For Robust Graph Comparison', 'abstract': None}, {'title': 'Order Matters: Sequence to sequence for sets', 'abstract': 'Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models.'}, {'title': 'Gated Graph Sequence Neural Networks', 'abstract': 'Abstract: Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.'}, {'title': 'Convolutional Networks on Graphs for Learning Molecular Fingerprints', 'abstract': 'We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.'}, {'title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift', 'abstract': "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."}, {'title': 'Spectral Networks and Locally Connected Networks on Graphs', 'abstract': 'Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.'}, {'title': 'Scalable kernels for graphs with continuous attributes', 'abstract': None}, {'title': 'Deep Architectures and Deep Learning in Chemoinformatics: The Prediction of Aqueous Solubility for Drug-Like Molecules', 'abstract': 'Shallow machine learning methods have been applied to chemoinformatics problems with some success. As more data becomes available and more complex problems are tackled, deep machine learning methods may also become useful. Here, we present a brief overview of deep learning methods and show in particular how recursive neural network approaches can be applied to the problem of predicting molecular properties. However, molecules are typically described by undirected cyclic graphs, while recursive approaches typically use directed acyclic graphs. Thus, we develop methods to address this discrepancy, essentially by considering an ensemble of recursive neural networks associated with all possible vertex-centered acyclic orientations of the molecular graph. One advantage of this approach is that it relies only minimally on the identification of suitable molecular descriptors because suitable representations are learned automatically from the data. Several variants of this approach are applied to the problem of predicting aqueous solubility and tested on four benchmark data sets. Experimental results show that the performance of the deep learning methods matches or exceeds the performance of other state-of-the-art methods according to several evaluation metrics and expose the fundamental limitations arising from training sets that are too small or too noisy. A Web-based predictor, AquaSol, is available online through the ChemDB portal ( cdb.ics.uci.edu ) together with additional material.'}, {'title': 'ImageNet classification with deep convolutional neural networks', 'abstract': None}, {'title': 'LIBSVM: A library for support vector machines', 'abstract': None}, {'title': 'Weisfeiler-Lehman Graph Kernels', 'abstract': 'In this article, we propose a family of efficient kernels for large graphs with discrete node labels. Key to our method is a rapid feature extraction scheme based on the Weisfeiler-Lehman test of isomorphism on graphs. It maps the original graph to a sequence of graphs, whose node attributes capture topological and label information. A family of kernels can be defined based on this Weisfeiler-Lehman sequence of graphs, including a highly efficient kernel comparing subtree-like patterns. Its runtime scales only linearly in the number of edges of the graphs and the length of the Weisfeiler-Lehman graph sequence. In our experimental evaluation, our kernels outperform state-of-the-art graph kernels on several graph classification benchmark data sets in terms of accuracy and runtime. Our kernels open the door to large-scale applications of graph kernels in various disciplines such as computational biology and social network analysis.'}, {'title': 'Efficient graphlet kernels for large graph comparison', 'abstract': None}, {'title': 'Weighted Graph Cuts without Eigenvectors A Multilevel Approach', 'abstract': None}, {'title': 'Shortest-path kernels on graphs', 'abstract': None}, {'title': 'Automatic Generation of Complementary Descriptors with Molecular Graph Networks', 'abstract': 'We describe a method for the automatic generation of weakly correlated descriptors for molecular data sets. The method can be regarded as a statistical learning procedure that turns the molecular graph, representing the 2D formula of the compound, into an adaptive whole molecule composite descriptor. By translating the molecular graph structure into a dynamical system, the algorithm can compute an output value that is highly sensitive to the molecular topology. This system can be trained by gradient descent techniques, which rely on the efficient calculation of the gradient by back-propagation. We present computational experiments concerning the classification of the Developmental Therapeutics Program AIDS antiviral screen data set on which the performance of the method compares with that of approaches based on substructure comparison.'}, {'title': 'Distinguishing enzyme structures from non-enzymes without alignments.', 'abstract': None}, {'title': 'Processing directed acyclic graphs with recursive neural networks', 'abstract': 'Recursive neural networks are conceived for processing graphs and extend the well-known recurrent model for processing sequences. In Frasconi et al. (1998), recursive neural networks can deal only with directed ordered acyclic graphs (DOAGs), in which the children of any given node are ordered. While this assumption is reasonable in some applications, it introduces unnecessary constraints in others. In this paper, it is shown that the constraint on the ordering can be relaxed by using an appropriate weight sharing, that guarantees the independence of the network output with respect to the permutations of the arcs leaving from each node. The method can be used with graphs having low connectivity and, in particular, few outcoming arcs. Some theoretical properties of the proposed architecture are given. They guarantee that the approximation capabilities are maintained, despite the weight sharing.'}, {'title': 'Neural Relational Inference for Interacting Systems', 'abstract': "Interacting systems are prevalent in nature, from dynamical systems in physics to complex societal dynamics. The interplay of components can give rise to complex behavior, which can often be explained using a simple model of the system's constituent parts. In this work, we introduce the neural relational inference (NRI) model: an unsupervised model that learns to infer interactions while simultaneously learning the dynamics purely from observational data. Our model takes the form of a variational auto-encoder, in which the latent code represents the underlying interaction graph and the reconstruction is based on graph neural networks. In experiments on simulated physical systems, we show that our NRI model can accurately recover ground-truth interactions in an unsupervised manner. We further demonstrate that we can find an interpretable structure and predict complex dynamics in real motion capture and sports tracking data."}, {'title': 'Benchmark data sets for graph kernels, 2016', 'abstract': None}, {'title': 'The Graph Neural Network Model', 'abstract': None}, {'title': 'Protein function prediction via graph kernels', 'abstract': 'MOTIVATION\nComputational approaches to protein function prediction infer protein function by finding proteins with similar sequence, structure, surface clefts, chemical properties, amino acid motifs, interaction partners or phylogenetic profiles. We present a new approach that combines sequential, structural and chemical information into one graph model of proteins. We predict functional class membership of enzymes and non-enzymes using graph kernels and support vector machine classification on these protein graphs.\n\n\nRESULTS\nOur graph model, derivable from protein sequence and structure only, is competitive with vector models that require additional protein information, such as the size of surface pockets. If we include this extra information into our graph model, our classifier yields significantly higher accuracy levels than the vector models. Hyperkernels allow us to select and to optimally combine the most relevant node attributes in our protein graphs. We have laid the foundation for a protein function prediction system that integrates protein information from various sources efficiently and effectively.\n\n\nAVAILABILITY\nMore information available via www.dbs.ifi.lmu.de/Mitarbeiter/borgwardt.html.'}]
Cited Paper:
Attempt 0
Retrieved data: {'offset': 0, 'citingPaperInfo': {'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1806.08804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'title': 'Hierarchical Graph Representation Learning with Differentiable Pooling', 'authors': [{'authorId': '83539859', 'name': 'Rex Ying'}, {'authorId': '145829303', 'name': 'Jiaxuan You'}, {'authorId': '143622465', 'name': 'Christopher Morris'}, {'authorId': '145201124', 'name': 'Xiang Ren'}, {'authorId': '49437682', 'name': 'William L. Hamilton'}, {'authorId': '1702139', 'name': 'J. Leskovec'}]}, 'data': [{'citedPaper': {'paperId': '1a8ee9382e79b92986f9d780a929fc2d2be2f47b', 'title': 'Graph Capsule Convolutional Neural Networks', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1805.08090, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '47383867', 'name': 'Saurabh Verma'}, {'authorId': '1708494', 'name': 'Zhi-Li Zhang'}], 'abstract': 'Graph Convolutional Neural Networks (GCNNs) are the most recent exciting advancement in deep learning field and their applications are quickly spreading in multi-cross-domains including bioinformatics, chemoinformatics, social networks, natural language processing and computer vision. In this paper, we expose and tackle some of the basic weaknesses of a GCNN model with a capsule idea presented in \\cite{hinton2011transforming} and propose our Graph Capsule Network (GCAPS-CNN) model. In addition, we design our GCAPS-CNN model to solve especially graph classification problem which current GCNN models find challenging. Through extensive experiments, we show that our proposed Graph Capsule Network can significantly outperforms both the existing state-of-art deep learning methods and graph kernels on graph classification benchmark datasets.'}}, {'citedPaper': {'paperId': 'd81fc968196e06ccafd7ea4c008b13e1cad1be64', 'title': 'An End-to-End Deep Learning Architecture for Graph Classification', 'openAccessPdf': {'url': 'https://ojs.aaai.org/index.php/AAAI/article/download/11782/11641', 'status': 'BRONZE', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v32i1.11782?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v32i1.11782, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '3098251', 'name': 'Muhan Zhang'}, {'authorId': '7217944', 'name': 'Zhicheng Cui'}, {'authorId': '40059761', 'name': 'Marion Neumann'}, {'authorId': '9527255', 'name': 'Yixin Chen'}], 'abstract': '\n \n Neural networks are typically designed to deal with data in tensor forms. In this paper, we propose a novel neural network architecture accepting graphs of arbitrary structure. Given a dataset containing graphs in the form of (G,y) where G is a graph and y is its class, we aim to develop neural networks that read the graphs directly and learn a classification function. There are two main challenges: 1) how to extract useful features characterizing the rich information encoded in a graph for classification purpose, and 2) how to sequentially read a graph in a meaningful and consistent order. To address the first challenge, we design a localized graph convolution model and show its connection with two graph kernels. To address the second challenge, we design a novel SortPooling layer which sorts graph vertices in a consistent order so that traditional neural networks can be trained on the graphs. Experiments on benchmark graph classification datasets demonstrate that the proposed architecture achieves highly competitive performance with state-of-the-art graph kernels and other graph neural network methods. Moreover, the architecture allows end-to-end gradient-based training with original graphs, without the need to first transform graphs into vectors.\n \n'}}, {'citedPaper': {'paperId': '4aa8e316bc1c5959537517ed16b4bf81b4bd73ed', 'title': 'Graph Partition Neural Networks for Semi-Supervised Classification', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1803.06272, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2246396', 'name': 'Renjie Liao'}, {'authorId': '2107692', 'name': 'Marc Brockschmidt'}, {'authorId': '1725299', 'name': 'Daniel Tarlow'}, {'authorId': '35058304', 'name': 'Alexander L. Gaunt'}, {'authorId': '2422559', 'name': 'R. Urtasun'}, {'authorId': '1804104', 'name': 'R. Zemel'}], 'abstract': 'We present graph partition neural networks (GPNN), an extension of graph neural networks (GNNs) able to handle extremely large graphs. GPNNs alternate between locally propagating information between nodes in small subgraphs and globally propagating information between the subgraphs. To efficiently partition graphs, we experiment with several partitioning algorithms and also propose a novel variant for fast processing of large scale graphs. We extensively test our model on a variety of semi-supervised node classification tasks. Experimental results indicate that GPNNs are either superior or comparable to state-of-the-art methods on a wide variety of datasets for graph-based semi-supervised classification. We also show that GPNNs can achieve similar performance as standard GNNs with fewer propagation steps.'}}, {'citedPaper': {'paperId': 'a73531abe4cafbccd5b3e949e84410a50016bd33', 'title': 'SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels', 'openAccessPdf': {'url': 'https://arxiv.org/pdf/1711.08920', 'status': 'GREEN', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1711.08920, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '3410500', 'name': 'Matthias Fey'}, {'authorId': '9572099', 'name': 'J. E. Lenssen'}, {'authorId': '2595376', 'name': 'F. Weichert'}, {'authorId': '2151194196', 'name': 'H. Müller'}], 'abstract': 'We present Spline-based Convolutional Neural Networks (SplineCNNs), a variant of deep neural networks for irregular structured and geometric input, e.g., graphs or meshes. Our main contribution is a novel convolution operator based on B-splines, that makes the computation time independent from the kernel size due to the local support property of the B-spline basis functions. As a result, we obtain a generalization of the traditional CNN convolution operator by using continuous kernel functions parametrized by a fixed number of trainable weights. In contrast to related approaches that filter in the spectral domain, the proposed method aggregates features purely in the spatial domain. In addition, SplineCNN allows entire end-to-end training of deep architectures, using only the geometric structure as input, instead of handcrafted feature descriptors. For validation, we apply our method on tasks from the fields of image graph classification, shape correspondence and graph node classification, and show that it outperforms or pars state-of-the-art approaches while being significantly faster and having favorable properties like domain-independence. Our source code is available on GitHub1.'}}, {'citedPaper': {'paperId': 'c751ab01aedc2888a7fe6e8b4f77ab1afa94072f', 'title': 'Protein Interface Prediction using Graph Convolutional Networks', 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'authors': [{'authorId': '29837788', 'name': 'A. Fout'}, {'authorId': '40205705', 'name': 'Jonathon Byrd'}, {'authorId': '11545747', 'name': 'Basir Shariat'}, {'authorId': '1399356737', 'name': 'Asa Ben-Hur'}], 'abstract': None}}, {'citedPaper': {'paperId': '33998aff64ce51df8dee45989cdca4b6b1329ec4', 'title': 'Graph Attention Networks', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1710.10903, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '3444569', 'name': 'Petar Velickovic'}, {'authorId': '7153363', 'name': 'Guillem Cucurull'}, {'authorId': '8742492', 'name': 'Arantxa Casanova'}, {'authorId': '144290131', 'name': 'Adriana Romero'}, {'authorId': '144269589', 'name': 'P. Lio’'}, {'authorId': '1751762', 'name': 'Yoshua Bengio'}], 'abstract': "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training)."}}, {'citedPaper': {'paperId': 'ecf6c42d84351f34e1625a6a2e4cc6526da45c74', 'title': 'Representation Learning on Graphs: Methods and Applications', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1709.05584, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '49437682', 'name': 'William L. Hamilton'}, {'authorId': '83539859', 'name': 'Rex Ying'}, {'authorId': '1702139', 'name': 'J. Leskovec'}], 'abstract': 'Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.'}}, {'citedPaper': {'paperId': 'aaf046c4da99ee6184f3fd31961a9967272152f9', 'title': 'Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1709.04555, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2400119', 'name': 'Wengong Jin'}, {'authorId': '13027820', 'name': 'Connor W. Coley'}, {'authorId': '1741283', 'name': 'R. Barzilay'}, {'authorId': '35132120', 'name': 'T. Jaakkola'}], 'abstract': 'The prediction of organic reaction outcomes is a fundamental problem in computational chemistry. Since a reaction may involve hundreds of atoms, fully exploring the space of possible transformations is intractable. The current solution utilizes reaction templates to limit the space, but it suffers from coverage and efficiency issues. In this paper, we propose a template-free approach to efficiently explore the space of product molecules by first pinpointing the reaction center -- the set of nodes and edges where graph edits occur. Since only a small number of atoms contribute to reaction center, we can directly enumerate candidate products. The generated candidates are scored by a Weisfeiler-Lehman Difference Network that models high-order interactions between changes occurring at nodes across the molecule. Our framework outperforms the top-performing template-based approach with a 10\\% margin, while running orders of magnitude faster. Finally, we demonstrate that the model accuracy rivals the performance of domain experts.'}}, {'citedPaper': {'paperId': '5bf31dc4bd54b623008c13f8bc8954dc7c9a2d80', 'title': 'SchNet: A continuous-filter convolutional neural network for modeling quantum interactions', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1706.08566, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '31961144', 'name': 'Kristof Schütt'}, {'authorId': '2113697', 'name': 'Pieter-Jan Kindermans'}, {'authorId': '29800712', 'name': 'Huziel Enoc Sauceda Felix'}, {'authorId': '7631063', 'name': 'Stefan Chmiela'}, {'authorId': '2462983', 'name': 'A. Tkatchenko'}, {'authorId': '145034054', 'name': 'K. Müller'}], 'abstract': 'Deep learning has the potential to revolutionize quantum chemistry as it is ideally suited to learn representations for structured data and speed up the exploration of chemical space. While convolutional neural networks have proven to be the first choice for images, audio and video data, the atoms in molecules are not restricted to a grid. Instead, their precise locations contain essential physical information, that would get lost if discretized. Thus, we propose to use continuous-filter convolutional layers to be able to model local correlations without requiring the data to lie on a grid. We apply those layers in SchNet: a novel deep learning architecture modeling quantum interactions in molecules. We obtain a joint model for the total energy and interatomic forces that follows fundamental quantum-chemical principles. This includes rotationally invariant energy predictions and a smooth, differentiable potential energy surface. Our architecture achieves state-of-the-art performance for benchmarks of equilibrium molecules and molecular dynamics trajectories. Finally, we introduce a more challenging benchmark with chemical and structural variations that suggests the path for further work.'}}, {'citedPaper': {'paperId': '6b7d6e6416343b2a122f8416e69059ce919026ef', 'title': 'Inductive Representation Learning on Large Graphs', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1706.02216, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '49437682', 'name': 'William L. Hamilton'}, {'authorId': '4058003', 'name': 'Z. Ying'}, {'authorId': '1702139', 'name': 'J. Leskovec'}], 'abstract': "Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions."}}, {'citedPaper': {'paperId': 'ee9c6aeb6e29cf3c9081df2cc100b8203ebf5cff', 'title': 'Deriving Neural Architectures from Sequence and Graph Kernels', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1705.09037, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '49986267', 'name': 'Tao Lei'}, {'authorId': '2400119', 'name': 'Wengong Jin'}, {'authorId': '1741283', 'name': 'R. Barzilay'}, {'authorId': '35132120', 'name': 'T. Jaakkola'}], 'abstract': 'The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects (cf. filters in CNN) via the kernels. Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training. We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art results across these applications.'}}, {'citedPaper': {'paperId': '1a39bb2caa151d15efd6718f3a80d9f4bff95af2', 'title': 'Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs', 'openAccessPdf': {'url': 'https://arxiv.org/pdf/1704.02901', 'status': 'GREEN', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1704.02901, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '3451689', 'name': 'M. Simonovsky'}, {'authorId': '2505902', 'name': 'N. Komodakis'}], 'abstract': 'A number of problems can be formulated as prediction on graph-structured data. In this work, we generalize the convolution operator from regular grids to arbitrary graphs while avoiding the spectral domain, which allows us to handle graphs of varying size and connectivity. To move beyond a simple diffusion, filter weights are conditioned on the specific edge labels in the neighborhood of a vertex. Together with the proper choice of graph coarsening, we explore constructing deep neural networks for graph classification. In particular, we demonstrate the generality of our formulation in point cloud classification, where we set the new state of the art, and on a graph classification dataset, where we outperform other deep learning approaches.'}}, {'citedPaper': {'paperId': 'e24cdf73b3e7e590c2fe5ecac9ae8aa983801367', 'title': 'Neural Message Passing for Quantum Chemistry', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1704.01212, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2058362', 'name': 'J. Gilmer'}, {'authorId': '2601641', 'name': 'S. Schoenholz'}, {'authorId': '119508204', 'name': 'Patrick F. Riley'}, {'authorId': '1689108', 'name': 'O. Vinyals'}, {'authorId': '35188630', 'name': 'George E. Dahl'}], 'abstract': 'Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.'}}, {'citedPaper': {'paperId': 'cd8a9914d50b0ac63315872530274d158d6aff09', 'title': 'Modeling Relational Data with Graph Convolutional Networks', 'openAccessPdf': {'url': '', 'status': 'CLOSED', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1703.06103, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '8804828', 'name': 'M. Schlichtkrull'}, {'authorId': '41016725', 'name': 'Thomas Kipf'}, {'authorId': '2789097', 'name': 'Peter Bloem'}, {'authorId': '9965217', 'name': 'Rianne van den Berg'}, {'authorId': '144889265', 'name': 'Ivan Titov'}, {'authorId': '1678311', 'name': 'M. Welling'}], 'abstract': 'Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.'}}, {'citedPaper': {'paperId': '0e779fd59353a7f1f5b559b9d65fa4bfe367890c', 'title': 'Geometric Deep Learning: Going beyond Euclidean data', 'openAccessPdf': {'url': 'http://arxiv.org/pdf/1611.08097', 'status': 'GREEN', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1611.08097, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1732570', 'name': 'M. Bronstein'}, {'authorId': '143627859', 'name': 'Joan Bruna'}, {'authorId': '1688882', 'name': 'Yann LeCun'}, {'authorId': '3149531', 'name': 'Arthur Szlam'}, {'authorId': '1697397', 'name': 'P. Vandergheynst'}], 'abstract': 'Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.'}}, {'citedPaper': {'paperId': '36eff562f65125511b5dfab68ce7f7a943c27478', 'title': 'Semi-Supervised Classification with Graph Convolutional Networks', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1609.02907, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '41016725', 'name': 'Thomas Kipf'}, {'authorId': '1678311', 'name': 'M. Welling'}], 'abstract': 'We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.'}}, {'citedPaper': {'paperId': 'c41eb895616e453dcba1a70c9b942c5063cc656c', 'title': 'Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1606.09375, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '3422350', 'name': 'Michaël Defferrard'}, {'authorId': '2549032', 'name': 'X. Bresson'}, {'authorId': '1697397', 'name': 'P. Vandergheynst'}], 'abstract': "In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs."}}, {'citedPaper': {'paperId': 'da2e04453b6f0d89ee75e6f68d619d936cd9c0b5', 'title': 'On Valid Optimal Assignment Kernels and Applications to Graph Classification', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1606.01141, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1810077', 'name': 'Nils M. Kriege'}, {'authorId': '144790942', 'name': 'P. Giscard'}, {'authorId': '2111010157', 'name': 'Richard C. Wilson'}], 'abstract': 'The success of kernel methods has initiated the design of novel positive semidefinite functions, in particular for structured data. A leading design paradigm for this is the convolution kernel, which decomposes structured objects into their parts and sums over all pairs of parts. Assignment kernels, in contrast, are obtained from an optimal bijection between parts, which can provide a more valid notion of similarity. In general however, optimal assignments yield indefinite functions, which complicates their use in kernel methods. We characterize a class of base kernels used to compare parts that guarantees positive semidefinite optimal assignment kernels. These base kernels give rise to hierarchies from which the optimal assignment kernels are computed in linear time by histogram intersection. We apply these results by developing the Weisfeiler-Lehman optimal assignment kernel for graphs. It provides high classification accuracy on widely-used benchmark data sets improving over the original Weisfeiler-Lehman kernel.'}}, {'citedPaper': {'paperId': '7c6de5a9e02a779e24504619050c6118f4eac181', 'title': 'Learning Convolutional Neural Networks for Graphs', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1605.05273, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2780262', 'name': 'Mathias Niepert'}, {'authorId': '24931083', 'name': 'Mohamed Ahmed'}, {'authorId': '1712289', 'name': 'Konstantin Kutzkov'}], 'abstract': 'Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.'}}, {'citedPaper': {'paperId': '322cf9bcde458a45eaeca989a1eec92f7c6db984', 'title': 'Discriminative Embeddings of Latent Variable Models for Structured Data', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1603.05629, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2791430', 'name': 'H. Dai'}, {'authorId': '144445933', 'name': 'Bo Dai'}, {'authorId': '1779453', 'name': 'Le Song'}], 'abstract': 'Kernel classifiers and regressors designed for structured data, such as sequences, trees and graphs, have significantly advanced a number of interdisciplinary areas such as computational biology and drug design. Typically, kernels are designed beforehand for a data type which either exploit statistics of the structures or make use of probabilistic generative models, and then a discriminative classifier is learned based on the kernels via convex optimization. However, such an elegant two-stage approach also limited kernel methods from scaling up to millions of data points, and exploiting discriminative information to learn feature representations. \n \nWe propose, structure2vec, an effective and scalable approach for structured data representation based on the idea of embedding latent variable models into feature spaces, and learning such feature spaces using discriminative information. Interestingly, structure2vec extracts features by performing a sequence of function mappings in a way similar to graphical model inference procedures, such as mean field and belief propagation. In applications involving millions of data points, we showed that structure2vec runs 2 times faster, produces models which are 10, 000 times smaller, while at the same time achieving the state-of-the-art predictive performance.'}}, {'citedPaper': {'paperId': '3ce05152dbedab572167e031b90d677c13b49767', 'title': 'A Structural Smoothing Framework For Robust Graph Comparison', 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'authors': [{'authorId': '2304445773', 'name': 'Pinar Yanardag'}, {'authorId': '145713876', 'name': 'S. Vishwanathan'}], 'abstract': None}}, {'citedPaper': {'paperId': 'd01379ebb53c66a4ccf5f4959d904dcf9e161e41', 'title': 'Order Matters: Sequence to sequence for sets', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1511.06391, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1689108', 'name': 'O. Vinyals'}, {'authorId': '1751569', 'name': 'Samy Bengio'}, {'authorId': '1942300', 'name': 'M. Kudlur'}], 'abstract': 'Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models.'}}, {'citedPaper': {'paperId': '492f57ee9ceb61fb5a47ad7aebfec1121887a175', 'title': 'Gated Graph Sequence Neural Networks', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1511.05493, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '47002813', 'name': 'Yujia Li'}, {'authorId': '1725299', 'name': 'Daniel Tarlow'}, {'authorId': '2107692', 'name': 'Marc Brockschmidt'}, {'authorId': '1804104', 'name': 'R. Zemel'}], 'abstract': 'Abstract: Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.'}}, {'citedPaper': {'paperId': '5d1bfeed240709725c78bc72ea40e55410b373dc', 'title': 'Convolutional Networks on Graphs for Learning Molecular Fingerprints', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1509.09292, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1704657', 'name': 'D. Duvenaud'}, {'authorId': '1683298', 'name': 'D. Maclaurin'}, {'authorId': '1422175619', 'name': 'J. Aguilera-Iparraguirre'}, {'authorId': '2344011563', 'name': 'Rafael Gómez-Bombarelli'}, {'authorId': '145916942', 'name': 'Timothy D. Hirzel'}, {'authorId': '1380248954', 'name': 'Alán Aspuru-Guzik'}, {'authorId': '1722180', 'name': 'Ryan P. Adams'}], 'abstract': 'We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.'}}, {'citedPaper': {'paperId': '995c5f5e62614fcb4d2796ad2faab969da51713e', 'title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1502.03167, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2054165706', 'name': 'Sergey Ioffe'}, {'authorId': '2574060', 'name': 'Christian Szegedy'}], 'abstract': "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."}}, {'citedPaper': {'paperId': '5e925a9f1e20df61d1e860a7aa71894b35a1c186', 'title': 'Spectral Networks and Locally Connected Networks on Graphs', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1312.6203, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '143627859', 'name': 'Joan Bruna'}, {'authorId': '2563432', 'name': 'Wojciech Zaremba'}, {'authorId': '3149531', 'name': 'Arthur Szlam'}, {'authorId': '1688882', 'name': 'Yann LeCun'}], 'abstract': 'Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.'}}, {'citedPaper': {'paperId': 'd87762aa2ef5a259825cf75d5865d7ddf86b7018', 'title': 'Scalable kernels for graphs with continuous attributes', 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'authors': [{'authorId': '1808965', 'name': 'Aasa Feragen'}, {'authorId': '2423123', 'name': 'Niklas Kasenburg'}, {'authorId': '152800798', 'name': 'Jens Petersen'}, {'authorId': '32895376', 'name': 'Marleen de Bruijne'}, {'authorId': '1704422', 'name': 'Karsten M. Borgwardt'}], 'abstract': None}}, {'citedPaper': {'paperId': 'b3e025049142be3d97d559cca12b027f16c6349e', 'title': 'Deep Architectures and Deep Learning in Chemoinformatics: The Prediction of Aqueous Solubility for Drug-Like Molecules', 'openAccessPdf': {'url': 'https://europepmc.org/articles/pmc3739985?pdf=render', 'status': 'GREEN', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1021/ci400187y?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1021/ci400187y, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '3318293', 'name': 'A. Lusci'}, {'authorId': '2028880', 'name': 'G. Pollastri'}, {'authorId': '144902513', 'name': 'P. Baldi'}], 'abstract': 'Shallow machine learning methods have been applied to chemoinformatics problems with some success. As more data becomes available and more complex problems are tackled, deep machine learning methods may also become useful. Here, we present a brief overview of deep learning methods and show in particular how recursive neural network approaches can be applied to the problem of predicting molecular properties. However, molecules are typically described by undirected cyclic graphs, while recursive approaches typically use directed acyclic graphs. Thus, we develop methods to address this discrepancy, essentially by considering an ensemble of recursive neural networks associated with all possible vertex-centered acyclic orientations of the molecular graph. One advantage of this approach is that it relies only minimally on the identification of suitable molecular descriptors because suitable representations are learned automatically from the data. Several variants of this approach are applied to the problem of predicting aqueous solubility and tested on four benchmark data sets. Experimental results show that the performance of the deep learning methods matches or exceeds the performance of other state-of-the-art methods according to several evaluation metrics and expose the fundamental limitations arising from training sets that are too small or too noisy. A Web-based predictor, AquaSol, is available online through the ChemDB portal ( cdb.ics.uci.edu ) together with additional material.'}}, {'citedPaper': {'paperId': 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', 'title': 'ImageNet classification with deep convolutional neural networks', 'openAccessPdf': {'url': 'http://dl.acm.org/ft_gateway.cfm?id=3065386&type=pdf', 'status': 'BRONZE', 'license': None, 'disclaimer': "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3065386?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3065386, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, 'authors': [{'authorId': '2064160', 'name': 'A. Krizhevsky'}, {'authorId': '1701686', 'name': 'I. Sutskever'}, {'authorId': '1695689', 'name': 'Geoffrey E. Hinton'}], 'abstract': None}}, {'citedPaper': {'paperId': '273dfbcb68080251f5e9ff38b4413d7bd84b10a1', 'title': 'LIBSVM: A library for support vector machines', 'openAccessPdf': {'url': '', 'status': 'CLOSED', 'license': None, 'disclaimer': "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1961189.1961199?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1961189.1961199, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, 'authors': [{'authorId': None, 'name': 'Chih-Chung Chang'}, {'authorId': '1711460', 'name': 'Chih-Jen Lin'}], 'abstract': None}}, {'citedPaper': {'paperId': '7e1874986cf6433fabf96fff93ef42b60bdc49f8', 'title': 'Weisfeiler-Lehman Graph Kernels', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.5555/1953048.2078187?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5555/1953048.2078187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1898417', 'name': 'N. Shervashidze'}, {'authorId': '31500557', 'name': 'Pascal Schweitzer'}, {'authorId': '1711983', 'name': 'E. J. V. Leeuwen'}, {'authorId': '1698752', 'name': 'K. Mehlhorn'}, {'authorId': '1704422', 'name': 'Karsten M. Borgwardt'}], 'abstract': 'In this article, we propose a family of efficient kernels for large graphs with discrete node labels. Key to our method is a rapid feature extraction scheme based on the Weisfeiler-Lehman test of isomorphism on graphs. It maps the original graph to a sequence of graphs, whose node attributes capture topological and label information. A family of kernels can be defined based on this Weisfeiler-Lehman sequence of graphs, including a highly efficient kernel comparing subtree-like patterns. Its runtime scales only linearly in the number of edges of the graphs and the length of the Weisfeiler-Lehman graph sequence. In our experimental evaluation, our kernels outperform state-of-the-art graph kernels on several graph classification benchmark data sets in terms of accuracy and runtime. Our kernels open the door to large-scale applications of graph kernels in various disciplines such as computational biology and social network analysis.'}}, {'citedPaper': {'paperId': 'a7fc751cd95bd1a409a26daaef69fc3aa8a35e0e', 'title': 'Efficient graphlet kernels for large graph comparison', 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'authors': [{'authorId': '1898417', 'name': 'N. Shervashidze'}, {'authorId': '145713876', 'name': 'S. Vishwanathan'}, {'authorId': '70590076', 'name': 'Tobias Petri'}, {'authorId': '1698752', 'name': 'K. Mehlhorn'}, {'authorId': '1704422', 'name': 'Karsten M. Borgwardt'}], 'abstract': None}}, {'citedPaper': {'paperId': '34795caa356317f2c662bd4553cd7608e0aeb657', 'title': 'Weighted Graph Cuts without Eigenvectors A Multilevel Approach', 'openAccessPdf': {'url': '', 'status': 'CLOSED', 'license': None, 'disclaimer': "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TPAMI.2007.1115?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TPAMI.2007.1115, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, 'authors': [{'authorId': '1783667', 'name': 'I. Dhillon'}, {'authorId': '1853389', 'name': 'Yuqiang Guan'}, {'authorId': '1692670', 'name': 'Brian Kulis'}], 'abstract': None}}, {'citedPaper': {'paperId': '70ff05e93f4eaeef4b8b51ba4e11d1c4cdf856ca', 'title': 'Shortest-path kernels on graphs', 'openAccessPdf': {'url': 'http://cbio.ensmp.fr/~jvert/svn/bibli/local/Borgwardt2005Shortest-Path.pdf', 'status': 'GREEN', 'license': None, 'disclaimer': "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDM.2005.132?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDM.2005.132, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, 'authors': [{'authorId': '1704422', 'name': 'Karsten M. Borgwardt'}, {'authorId': '1688561', 'name': 'H. Kriegel'}], 'abstract': None}}, {'citedPaper': {'paperId': '20036cd7fac884130f642d3df30b1fd7fb24c6d2', 'title': 'Automatic Generation of Complementary Descriptors with Molecular Graph Networks', 'openAccessPdf': {'url': '', 'status': 'CLOSED', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1021/CI049613B?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1021/CI049613B, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2114450', 'name': 'Christian Merkwirth'}, {'authorId': '49370597', 'name': 'Thomas Lengauer'}], 'abstract': 'We describe a method for the automatic generation of weakly correlated descriptors for molecular data sets. The method can be regarded as a statistical learning procedure that turns the molecular graph, representing the 2D formula of the compound, into an adaptive whole molecule composite descriptor. By translating the molecular graph structure into a dynamical system, the algorithm can compute an output value that is highly sensitive to the molecular topology. This system can be trained by gradient descent techniques, which rely on the efficient calculation of the gradient by back-propagation. We present computational experiments concerning the classification of the Developmental Therapeutics Program AIDS antiviral screen data set on which the performance of the method compares with that of approaches based on substructure comparison.'}}, {'citedPaper': {'paperId': 'c2815f76c690ec2ca6523b1faacf9b6a401bdff1', 'title': 'Distinguishing enzyme structures from non-enzymes without alignments.', 'openAccessPdf': {'url': 'http://cbio.ensmp.fr/~jvert/svn/bibli/local/Dobson2003Distinguishing.pdf', 'status': 'GREEN', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1016/S0022-2836(03)00628-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/S0022-2836(03)00628-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '145002767', 'name': 'P. Dobson'}, {'authorId': '1970705', 'name': 'A. Doig'}], 'abstract': None}}, {'citedPaper': {'paperId': '25ec680e94d84e93e3bd0409d6084bfb43c4cdb6', 'title': 'Processing directed acyclic graphs with recursive neural networks', 'openAccessPdf': {'url': '', 'status': 'CLOSED', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/72.963781?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/72.963781, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '144020416', 'name': 'M. Bianchini'}, {'authorId': '145467467', 'name': 'M. Gori'}, {'authorId': '47260481', 'name': 'F. Scarselli'}], 'abstract': 'Recursive neural networks are conceived for processing graphs and extend the well-known recurrent model for processing sequences. In Frasconi et al. (1998), recursive neural networks can deal only with directed ordered acyclic graphs (DOAGs), in which the children of any given node are ordered. While this assumption is reasonable in some applications, it introduces unnecessary constraints in others. In this paper, it is shown that the constraint on the ordering can be relaxed by using an appropriate weight sharing, that guarantees the independence of the network output with respect to the permutations of the arcs leaving from each node. The method can be used with graphs having low connectivity and, in particular, few outcoming arcs. Some theoretical properties of the proposed architecture are given. They guarantee that the approximation capabilities are maintained, despite the weight sharing.'}}, {'citedPaper': {'paperId': '89b1766a77f7a34aadd67861123d5cf3cd3ece52', 'title': 'Neural Relational Inference for Interacting Systems', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1802.04687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1643311826', 'name': 'T. Fetaya'}, {'authorId': '50844928', 'name': 'E. Wang'}, {'authorId': '1643758681', 'name': 'K.-C. Welling'}, {'authorId': '47446467', 'name': 'M. Zemel'}, {'authorId': '41016725', 'name': 'Thomas Kipf'}, {'authorId': '2645055', 'name': 'Ethan Fetaya'}, {'authorId': '122782486', 'name': 'Kuan-Chieh Jackson Wang'}, {'authorId': '1678311', 'name': 'M. Welling'}, {'authorId': '1804104', 'name': 'R. Zemel'}], 'abstract': "Interacting systems are prevalent in nature, from dynamical systems in physics to complex societal dynamics. The interplay of components can give rise to complex behavior, which can often be explained using a simple model of the system's constituent parts. In this work, we introduce the neural relational inference (NRI) model: an unsupervised model that learns to infer interactions while simultaneously learning the dynamics purely from observational data. Our model takes the form of a variational auto-encoder, in which the latent code represents the underlying interaction graph and the reconstruction is based on graph neural networks. In experiments on simulated physical systems, we show that our NRI model can accurately recover ground-truth interactions in an unsupervised manner. We further demonstrate that we can find an interpretable structure and predict complex dynamics in real motion capture and sports tracking data."}}, {'citedPaper': {'paperId': None, 'title': 'Benchmark data sets for graph kernels, 2016', 'openAccessPdf': None, 'authors': [], 'abstract': None}}, {'citedPaper': {'paperId': '3efd851140aa28e95221b55fcc5659eea97b172d', 'title': 'The Graph Neural Network Model', 'openAccessPdf': {'url': 'https://ro.uow.edu.au/articles/journal_contribution/The_graph_neural_network_model/27757629/1/files/50520873.pdf', 'status': 'GREEN', 'license': 'other-oa', 'disclaimer': "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNN.2008.2005605?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNN.2008.2005605, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, 'authors': [{'authorId': '47260481', 'name': 'F. Scarselli'}, {'authorId': '145467467', 'name': 'M. Gori'}, {'authorId': '1733691', 'name': 'A. Tsoi'}, {'authorId': '1784450', 'name': 'M. Hagenbuchner'}, {'authorId': '3073217', 'name': 'G. Monfardini'}], 'abstract': None}}, {'citedPaper': {'paperId': 'ba640c0682b242cb480bb4eb5b934ee6db949269', 'title': 'Protein function prediction via graph kernels', 'openAccessPdf': {'url': 'https://academic.oup.com/bioinformatics/article-pdf/21/suppl_1/i47/524364/bti1007.pdf', 'status': 'BRONZE', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1093/bioinformatics/bti1007?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1093/bioinformatics/bti1007, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1704422', 'name': 'Karsten M. Borgwardt'}, {'authorId': '1706780', 'name': 'Cheng Soon Ong'}, {'authorId': '2617402', 'name': 'Stefan Schönauer'}, {'authorId': '145713876', 'name': 'S. Vishwanathan'}, {'authorId': '46234526', 'name': 'Alex Smola'}, {'authorId': '1688561', 'name': 'H. Kriegel'}], 'abstract': 'MOTIVATION\nComputational approaches to protein function prediction infer protein function by finding proteins with similar sequence, structure, surface clefts, chemical properties, amino acid motifs, interaction partners or phylogenetic profiles. We present a new approach that combines sequential, structural and chemical information into one graph model of proteins. We predict functional class membership of enzymes and non-enzymes using graph kernels and support vector machine classification on these protein graphs.\n\n\nRESULTS\nOur graph model, derivable from protein sequence and structure only, is competitive with vector models that require additional protein information, such as the size of surface pockets. If we include this extra information into our graph model, our classifier yields significantly higher accuracy levels than the vector models. Hyperkernels allow us to select and to optimally combine the most relevant node attributes in our protein graphs. We have laid the foundation for a protein function prediction system that integrates protein information from various sources efficiently and effectively.\n\n\nAVAILABILITY\nMore information available via www.dbs.ifi.lmu.de/Mitarbeiter/borgwardt.html.'}}]}
[{'title': 'Graph Capsule Convolutional Neural Networks', 'abstract': 'Graph Convolutional Neural Networks (GCNNs) are the most recent exciting advancement in deep learning field and their applications are quickly spreading in multi-cross-domains including bioinformatics, chemoinformatics, social networks, natural language processing and computer vision. In this paper, we expose and tackle some of the basic weaknesses of a GCNN model with a capsule idea presented in \\cite{hinton2011transforming} and propose our Graph Capsule Network (GCAPS-CNN) model. In addition, we design our GCAPS-CNN model to solve especially graph classification problem which current GCNN models find challenging. Through extensive experiments, we show that our proposed Graph Capsule Network can significantly outperforms both the existing state-of-art deep learning methods and graph kernels on graph classification benchmark datasets.'}, {'title': 'An End-to-End Deep Learning Architecture for Graph Classification', 'abstract': '\n \n Neural networks are typically designed to deal with data in tensor forms. In this paper, we propose a novel neural network architecture accepting graphs of arbitrary structure. Given a dataset containing graphs in the form of (G,y) where G is a graph and y is its class, we aim to develop neural networks that read the graphs directly and learn a classification function. There are two main challenges: 1) how to extract useful features characterizing the rich information encoded in a graph for classification purpose, and 2) how to sequentially read a graph in a meaningful and consistent order. To address the first challenge, we design a localized graph convolution model and show its connection with two graph kernels. To address the second challenge, we design a novel SortPooling layer which sorts graph vertices in a consistent order so that traditional neural networks can be trained on the graphs. Experiments on benchmark graph classification datasets demonstrate that the proposed architecture achieves highly competitive performance with state-of-the-art graph kernels and other graph neural network methods. Moreover, the architecture allows end-to-end gradient-based training with original graphs, without the need to first transform graphs into vectors.\n \n'}, {'title': 'Graph Partition Neural Networks for Semi-Supervised Classification', 'abstract': 'We present graph partition neural networks (GPNN), an extension of graph neural networks (GNNs) able to handle extremely large graphs. GPNNs alternate between locally propagating information between nodes in small subgraphs and globally propagating information between the subgraphs. To efficiently partition graphs, we experiment with several partitioning algorithms and also propose a novel variant for fast processing of large scale graphs. We extensively test our model on a variety of semi-supervised node classification tasks. Experimental results indicate that GPNNs are either superior or comparable to state-of-the-art methods on a wide variety of datasets for graph-based semi-supervised classification. We also show that GPNNs can achieve similar performance as standard GNNs with fewer propagation steps.'}, {'title': 'SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels', 'abstract': 'We present Spline-based Convolutional Neural Networks (SplineCNNs), a variant of deep neural networks for irregular structured and geometric input, e.g., graphs or meshes. Our main contribution is a novel convolution operator based on B-splines, that makes the computation time independent from the kernel size due to the local support property of the B-spline basis functions. As a result, we obtain a generalization of the traditional CNN convolution operator by using continuous kernel functions parametrized by a fixed number of trainable weights. In contrast to related approaches that filter in the spectral domain, the proposed method aggregates features purely in the spatial domain. In addition, SplineCNN allows entire end-to-end training of deep architectures, using only the geometric structure as input, instead of handcrafted feature descriptors. For validation, we apply our method on tasks from the fields of image graph classification, shape correspondence and graph node classification, and show that it outperforms or pars state-of-the-art approaches while being significantly faster and having favorable properties like domain-independence. Our source code is available on GitHub1.'}, {'title': 'Protein Interface Prediction using Graph Convolutional Networks', 'abstract': None}, {'title': 'Graph Attention Networks', 'abstract': "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training)."}, {'title': 'Representation Learning on Graphs: Methods and Applications', 'abstract': 'Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.'}, {'title': 'Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network', 'abstract': 'The prediction of organic reaction outcomes is a fundamental problem in computational chemistry. Since a reaction may involve hundreds of atoms, fully exploring the space of possible transformations is intractable. The current solution utilizes reaction templates to limit the space, but it suffers from coverage and efficiency issues. In this paper, we propose a template-free approach to efficiently explore the space of product molecules by first pinpointing the reaction center -- the set of nodes and edges where graph edits occur. Since only a small number of atoms contribute to reaction center, we can directly enumerate candidate products. The generated candidates are scored by a Weisfeiler-Lehman Difference Network that models high-order interactions between changes occurring at nodes across the molecule. Our framework outperforms the top-performing template-based approach with a 10\\% margin, while running orders of magnitude faster. Finally, we demonstrate that the model accuracy rivals the performance of domain experts.'}, {'title': 'SchNet: A continuous-filter convolutional neural network for modeling quantum interactions', 'abstract': 'Deep learning has the potential to revolutionize quantum chemistry as it is ideally suited to learn representations for structured data and speed up the exploration of chemical space. While convolutional neural networks have proven to be the first choice for images, audio and video data, the atoms in molecules are not restricted to a grid. Instead, their precise locations contain essential physical information, that would get lost if discretized. Thus, we propose to use continuous-filter convolutional layers to be able to model local correlations without requiring the data to lie on a grid. We apply those layers in SchNet: a novel deep learning architecture modeling quantum interactions in molecules. We obtain a joint model for the total energy and interatomic forces that follows fundamental quantum-chemical principles. This includes rotationally invariant energy predictions and a smooth, differentiable potential energy surface. Our architecture achieves state-of-the-art performance for benchmarks of equilibrium molecules and molecular dynamics trajectories. Finally, we introduce a more challenging benchmark with chemical and structural variations that suggests the path for further work.'}, {'title': 'Inductive Representation Learning on Large Graphs', 'abstract': "Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions."}, {'title': 'Deriving Neural Architectures from Sequence and Graph Kernels', 'abstract': 'The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects (cf. filters in CNN) via the kernels. Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training. We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art results across these applications.'}, {'title': 'Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs', 'abstract': 'A number of problems can be formulated as prediction on graph-structured data. In this work, we generalize the convolution operator from regular grids to arbitrary graphs while avoiding the spectral domain, which allows us to handle graphs of varying size and connectivity. To move beyond a simple diffusion, filter weights are conditioned on the specific edge labels in the neighborhood of a vertex. Together with the proper choice of graph coarsening, we explore constructing deep neural networks for graph classification. In particular, we demonstrate the generality of our formulation in point cloud classification, where we set the new state of the art, and on a graph classification dataset, where we outperform other deep learning approaches.'}, {'title': 'Neural Message Passing for Quantum Chemistry', 'abstract': 'Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.'}, {'title': 'Modeling Relational Data with Graph Convolutional Networks', 'abstract': 'Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.'}, {'title': 'Geometric Deep Learning: Going beyond Euclidean data', 'abstract': 'Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.'}, {'title': 'Semi-Supervised Classification with Graph Convolutional Networks', 'abstract': 'We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.'}, {'title': 'Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering', 'abstract': "In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs."}, {'title': 'On Valid Optimal Assignment Kernels and Applications to Graph Classification', 'abstract': 'The success of kernel methods has initiated the design of novel positive semidefinite functions, in particular for structured data. A leading design paradigm for this is the convolution kernel, which decomposes structured objects into their parts and sums over all pairs of parts. Assignment kernels, in contrast, are obtained from an optimal bijection between parts, which can provide a more valid notion of similarity. In general however, optimal assignments yield indefinite functions, which complicates their use in kernel methods. We characterize a class of base kernels used to compare parts that guarantees positive semidefinite optimal assignment kernels. These base kernels give rise to hierarchies from which the optimal assignment kernels are computed in linear time by histogram intersection. We apply these results by developing the Weisfeiler-Lehman optimal assignment kernel for graphs. It provides high classification accuracy on widely-used benchmark data sets improving over the original Weisfeiler-Lehman kernel.'}, {'title': 'Learning Convolutional Neural Networks for Graphs', 'abstract': 'Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.'}, {'title': 'Discriminative Embeddings of Latent Variable Models for Structured Data', 'abstract': 'Kernel classifiers and regressors designed for structured data, such as sequences, trees and graphs, have significantly advanced a number of interdisciplinary areas such as computational biology and drug design. Typically, kernels are designed beforehand for a data type which either exploit statistics of the structures or make use of probabilistic generative models, and then a discriminative classifier is learned based on the kernels via convex optimization. However, such an elegant two-stage approach also limited kernel methods from scaling up to millions of data points, and exploiting discriminative information to learn feature representations. \n \nWe propose, structure2vec, an effective and scalable approach for structured data representation based on the idea of embedding latent variable models into feature spaces, and learning such feature spaces using discriminative information. Interestingly, structure2vec extracts features by performing a sequence of function mappings in a way similar to graphical model inference procedures, such as mean field and belief propagation. In applications involving millions of data points, we showed that structure2vec runs 2 times faster, produces models which are 10, 000 times smaller, while at the same time achieving the state-of-the-art predictive performance.'}, {'title': 'A Structural Smoothing Framework For Robust Graph Comparison', 'abstract': None}, {'title': 'Order Matters: Sequence to sequence for sets', 'abstract': 'Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models.'}, {'title': 'Gated Graph Sequence Neural Networks', 'abstract': 'Abstract: Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.'}, {'title': 'Convolutional Networks on Graphs for Learning Molecular Fingerprints', 'abstract': 'We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.'}, {'title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift', 'abstract': "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."}, {'title': 'Spectral Networks and Locally Connected Networks on Graphs', 'abstract': 'Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.'}, {'title': 'Scalable kernels for graphs with continuous attributes', 'abstract': None}, {'title': 'Deep Architectures and Deep Learning in Chemoinformatics: The Prediction of Aqueous Solubility for Drug-Like Molecules', 'abstract': 'Shallow machine learning methods have been applied to chemoinformatics problems with some success. As more data becomes available and more complex problems are tackled, deep machine learning methods may also become useful. Here, we present a brief overview of deep learning methods and show in particular how recursive neural network approaches can be applied to the problem of predicting molecular properties. However, molecules are typically described by undirected cyclic graphs, while recursive approaches typically use directed acyclic graphs. Thus, we develop methods to address this discrepancy, essentially by considering an ensemble of recursive neural networks associated with all possible vertex-centered acyclic orientations of the molecular graph. One advantage of this approach is that it relies only minimally on the identification of suitable molecular descriptors because suitable representations are learned automatically from the data. Several variants of this approach are applied to the problem of predicting aqueous solubility and tested on four benchmark data sets. Experimental results show that the performance of the deep learning methods matches or exceeds the performance of other state-of-the-art methods according to several evaluation metrics and expose the fundamental limitations arising from training sets that are too small or too noisy. A Web-based predictor, AquaSol, is available online through the ChemDB portal ( cdb.ics.uci.edu ) together with additional material.'}, {'title': 'ImageNet classification with deep convolutional neural networks', 'abstract': None}, {'title': 'LIBSVM: A library for support vector machines', 'abstract': None}, {'title': 'Weisfeiler-Lehman Graph Kernels', 'abstract': 'In this article, we propose a family of efficient kernels for large graphs with discrete node labels. Key to our method is a rapid feature extraction scheme based on the Weisfeiler-Lehman test of isomorphism on graphs. It maps the original graph to a sequence of graphs, whose node attributes capture topological and label information. A family of kernels can be defined based on this Weisfeiler-Lehman sequence of graphs, including a highly efficient kernel comparing subtree-like patterns. Its runtime scales only linearly in the number of edges of the graphs and the length of the Weisfeiler-Lehman graph sequence. In our experimental evaluation, our kernels outperform state-of-the-art graph kernels on several graph classification benchmark data sets in terms of accuracy and runtime. Our kernels open the door to large-scale applications of graph kernels in various disciplines such as computational biology and social network analysis.'}, {'title': 'Efficient graphlet kernels for large graph comparison', 'abstract': None}, {'title': 'Weighted Graph Cuts without Eigenvectors A Multilevel Approach', 'abstract': None}, {'title': 'Shortest-path kernels on graphs', 'abstract': None}, {'title': 'Automatic Generation of Complementary Descriptors with Molecular Graph Networks', 'abstract': 'We describe a method for the automatic generation of weakly correlated descriptors for molecular data sets. The method can be regarded as a statistical learning procedure that turns the molecular graph, representing the 2D formula of the compound, into an adaptive whole molecule composite descriptor. By translating the molecular graph structure into a dynamical system, the algorithm can compute an output value that is highly sensitive to the molecular topology. This system can be trained by gradient descent techniques, which rely on the efficient calculation of the gradient by back-propagation. We present computational experiments concerning the classification of the Developmental Therapeutics Program AIDS antiviral screen data set on which the performance of the method compares with that of approaches based on substructure comparison.'}, {'title': 'Distinguishing enzyme structures from non-enzymes without alignments.', 'abstract': None}, {'title': 'Processing directed acyclic graphs with recursive neural networks', 'abstract': 'Recursive neural networks are conceived for processing graphs and extend the well-known recurrent model for processing sequences. In Frasconi et al. (1998), recursive neural networks can deal only with directed ordered acyclic graphs (DOAGs), in which the children of any given node are ordered. While this assumption is reasonable in some applications, it introduces unnecessary constraints in others. In this paper, it is shown that the constraint on the ordering can be relaxed by using an appropriate weight sharing, that guarantees the independence of the network output with respect to the permutations of the arcs leaving from each node. The method can be used with graphs having low connectivity and, in particular, few outcoming arcs. Some theoretical properties of the proposed architecture are given. They guarantee that the approximation capabilities are maintained, despite the weight sharing.'}, {'title': 'Neural Relational Inference for Interacting Systems', 'abstract': "Interacting systems are prevalent in nature, from dynamical systems in physics to complex societal dynamics. The interplay of components can give rise to complex behavior, which can often be explained using a simple model of the system's constituent parts. In this work, we introduce the neural relational inference (NRI) model: an unsupervised model that learns to infer interactions while simultaneously learning the dynamics purely from observational data. Our model takes the form of a variational auto-encoder, in which the latent code represents the underlying interaction graph and the reconstruction is based on graph neural networks. In experiments on simulated physical systems, we show that our NRI model can accurately recover ground-truth interactions in an unsupervised manner. We further demonstrate that we can find an interpretable structure and predict complex dynamics in real motion capture and sports tracking data."}, {'title': 'Benchmark data sets for graph kernels, 2016', 'abstract': None}, {'title': 'The Graph Neural Network Model', 'abstract': None}, {'title': 'Protein function prediction via graph kernels', 'abstract': 'MOTIVATION\nComputational approaches to protein function prediction infer protein function by finding proteins with similar sequence, structure, surface clefts, chemical properties, amino acid motifs, interaction partners or phylogenetic profiles. We present a new approach that combines sequential, structural and chemical information into one graph model of proteins. We predict functional class membership of enzymes and non-enzymes using graph kernels and support vector machine classification on these protein graphs.\n\n\nRESULTS\nOur graph model, derivable from protein sequence and structure only, is competitive with vector models that require additional protein information, such as the size of surface pockets. If we include this extra information into our graph model, our classifier yields significantly higher accuracy levels than the vector models. Hyperkernels allow us to select and to optimally combine the most relevant node attributes in our protein graphs. We have laid the foundation for a protein function prediction system that integrates protein information from various sources efficiently and effectively.\n\n\nAVAILABILITY\nMore information available via www.dbs.ifi.lmu.de/Mitarbeiter/borgwardt.html.'}]
Attempt 0
Retrieved data: {'offset': 0, 'citingPaperInfo': {'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1806.08804, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'title': 'Hierarchical Graph Representation Learning with Differentiable Pooling', 'authors': [{'authorId': '83539859', 'name': 'Rex Ying'}, {'authorId': '145829303', 'name': 'Jiaxuan You'}, {'authorId': '143622465', 'name': 'Christopher Morris'}, {'authorId': '145201124', 'name': 'Xiang Ren'}, {'authorId': '49437682', 'name': 'William L. Hamilton'}, {'authorId': '1702139', 'name': 'J. Leskovec'}]}, 'data': [{'citedPaper': {'paperId': '1a8ee9382e79b92986f9d780a929fc2d2be2f47b', 'title': 'Graph Capsule Convolutional Neural Networks', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1805.08090, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '47383867', 'name': 'Saurabh Verma'}, {'authorId': '1708494', 'name': 'Zhi-Li Zhang'}], 'abstract': 'Graph Convolutional Neural Networks (GCNNs) are the most recent exciting advancement in deep learning field and their applications are quickly spreading in multi-cross-domains including bioinformatics, chemoinformatics, social networks, natural language processing and computer vision. In this paper, we expose and tackle some of the basic weaknesses of a GCNN model with a capsule idea presented in \\cite{hinton2011transforming} and propose our Graph Capsule Network (GCAPS-CNN) model. In addition, we design our GCAPS-CNN model to solve especially graph classification problem which current GCNN models find challenging. Through extensive experiments, we show that our proposed Graph Capsule Network can significantly outperforms both the existing state-of-art deep learning methods and graph kernels on graph classification benchmark datasets.'}}, {'citedPaper': {'paperId': 'd81fc968196e06ccafd7ea4c008b13e1cad1be64', 'title': 'An End-to-End Deep Learning Architecture for Graph Classification', 'openAccessPdf': {'url': 'https://ojs.aaai.org/index.php/AAAI/article/download/11782/11641', 'status': 'BRONZE', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1609/aaai.v32i1.11782?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1609/aaai.v32i1.11782, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '3098251', 'name': 'Muhan Zhang'}, {'authorId': '7217944', 'name': 'Zhicheng Cui'}, {'authorId': '40059761', 'name': 'Marion Neumann'}, {'authorId': '9527255', 'name': 'Yixin Chen'}], 'abstract': '\n \n Neural networks are typically designed to deal with data in tensor forms. In this paper, we propose a novel neural network architecture accepting graphs of arbitrary structure. Given a dataset containing graphs in the form of (G,y) where G is a graph and y is its class, we aim to develop neural networks that read the graphs directly and learn a classification function. There are two main challenges: 1) how to extract useful features characterizing the rich information encoded in a graph for classification purpose, and 2) how to sequentially read a graph in a meaningful and consistent order. To address the first challenge, we design a localized graph convolution model and show its connection with two graph kernels. To address the second challenge, we design a novel SortPooling layer which sorts graph vertices in a consistent order so that traditional neural networks can be trained on the graphs. Experiments on benchmark graph classification datasets demonstrate that the proposed architecture achieves highly competitive performance with state-of-the-art graph kernels and other graph neural network methods. Moreover, the architecture allows end-to-end gradient-based training with original graphs, without the need to first transform graphs into vectors.\n \n'}}, {'citedPaper': {'paperId': '4aa8e316bc1c5959537517ed16b4bf81b4bd73ed', 'title': 'Graph Partition Neural Networks for Semi-Supervised Classification', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1803.06272, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2246396', 'name': 'Renjie Liao'}, {'authorId': '2107692', 'name': 'Marc Brockschmidt'}, {'authorId': '1725299', 'name': 'Daniel Tarlow'}, {'authorId': '35058304', 'name': 'Alexander L. Gaunt'}, {'authorId': '2422559', 'name': 'R. Urtasun'}, {'authorId': '1804104', 'name': 'R. Zemel'}], 'abstract': 'We present graph partition neural networks (GPNN), an extension of graph neural networks (GNNs) able to handle extremely large graphs. GPNNs alternate between locally propagating information between nodes in small subgraphs and globally propagating information between the subgraphs. To efficiently partition graphs, we experiment with several partitioning algorithms and also propose a novel variant for fast processing of large scale graphs. We extensively test our model on a variety of semi-supervised node classification tasks. Experimental results indicate that GPNNs are either superior or comparable to state-of-the-art methods on a wide variety of datasets for graph-based semi-supervised classification. We also show that GPNNs can achieve similar performance as standard GNNs with fewer propagation steps.'}}, {'citedPaper': {'paperId': 'a73531abe4cafbccd5b3e949e84410a50016bd33', 'title': 'SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels', 'openAccessPdf': {'url': 'https://arxiv.org/pdf/1711.08920', 'status': 'GREEN', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1711.08920, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '3410500', 'name': 'Matthias Fey'}, {'authorId': '9572099', 'name': 'J. E. Lenssen'}, {'authorId': '2595376', 'name': 'F. Weichert'}, {'authorId': '2151194196', 'name': 'H. Müller'}], 'abstract': 'We present Spline-based Convolutional Neural Networks (SplineCNNs), a variant of deep neural networks for irregular structured and geometric input, e.g., graphs or meshes. Our main contribution is a novel convolution operator based on B-splines, that makes the computation time independent from the kernel size due to the local support property of the B-spline basis functions. As a result, we obtain a generalization of the traditional CNN convolution operator by using continuous kernel functions parametrized by a fixed number of trainable weights. In contrast to related approaches that filter in the spectral domain, the proposed method aggregates features purely in the spatial domain. In addition, SplineCNN allows entire end-to-end training of deep architectures, using only the geometric structure as input, instead of handcrafted feature descriptors. For validation, we apply our method on tasks from the fields of image graph classification, shape correspondence and graph node classification, and show that it outperforms or pars state-of-the-art approaches while being significantly faster and having favorable properties like domain-independence. Our source code is available on GitHub1.'}}, {'citedPaper': {'paperId': 'c751ab01aedc2888a7fe6e8b4f77ab1afa94072f', 'title': 'Protein Interface Prediction using Graph Convolutional Networks', 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'authors': [{'authorId': '29837788', 'name': 'A. Fout'}, {'authorId': '40205705', 'name': 'Jonathon Byrd'}, {'authorId': '11545747', 'name': 'Basir Shariat'}, {'authorId': '1399356737', 'name': 'Asa Ben-Hur'}], 'abstract': None}}, {'citedPaper': {'paperId': '33998aff64ce51df8dee45989cdca4b6b1329ec4', 'title': 'Graph Attention Networks', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1710.10903, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '3444569', 'name': 'Petar Velickovic'}, {'authorId': '7153363', 'name': 'Guillem Cucurull'}, {'authorId': '8742492', 'name': 'Arantxa Casanova'}, {'authorId': '144290131', 'name': 'Adriana Romero'}, {'authorId': '144269589', 'name': 'P. Lio’'}, {'authorId': '1751762', 'name': 'Yoshua Bengio'}], 'abstract': "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training)."}}, {'citedPaper': {'paperId': 'ecf6c42d84351f34e1625a6a2e4cc6526da45c74', 'title': 'Representation Learning on Graphs: Methods and Applications', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1709.05584, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '49437682', 'name': 'William L. Hamilton'}, {'authorId': '83539859', 'name': 'Rex Ying'}, {'authorId': '1702139', 'name': 'J. Leskovec'}], 'abstract': 'Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.'}}, {'citedPaper': {'paperId': 'aaf046c4da99ee6184f3fd31961a9967272152f9', 'title': 'Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1709.04555, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2400119', 'name': 'Wengong Jin'}, {'authorId': '13027820', 'name': 'Connor W. Coley'}, {'authorId': '1741283', 'name': 'R. Barzilay'}, {'authorId': '35132120', 'name': 'T. Jaakkola'}], 'abstract': 'The prediction of organic reaction outcomes is a fundamental problem in computational chemistry. Since a reaction may involve hundreds of atoms, fully exploring the space of possible transformations is intractable. The current solution utilizes reaction templates to limit the space, but it suffers from coverage and efficiency issues. In this paper, we propose a template-free approach to efficiently explore the space of product molecules by first pinpointing the reaction center -- the set of nodes and edges where graph edits occur. Since only a small number of atoms contribute to reaction center, we can directly enumerate candidate products. The generated candidates are scored by a Weisfeiler-Lehman Difference Network that models high-order interactions between changes occurring at nodes across the molecule. Our framework outperforms the top-performing template-based approach with a 10\\% margin, while running orders of magnitude faster. Finally, we demonstrate that the model accuracy rivals the performance of domain experts.'}}, {'citedPaper': {'paperId': '5bf31dc4bd54b623008c13f8bc8954dc7c9a2d80', 'title': 'SchNet: A continuous-filter convolutional neural network for modeling quantum interactions', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1706.08566, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '31961144', 'name': 'Kristof Schütt'}, {'authorId': '2113697', 'name': 'Pieter-Jan Kindermans'}, {'authorId': '29800712', 'name': 'Huziel Enoc Sauceda Felix'}, {'authorId': '7631063', 'name': 'Stefan Chmiela'}, {'authorId': '2462983', 'name': 'A. Tkatchenko'}, {'authorId': '145034054', 'name': 'K. Müller'}], 'abstract': 'Deep learning has the potential to revolutionize quantum chemistry as it is ideally suited to learn representations for structured data and speed up the exploration of chemical space. While convolutional neural networks have proven to be the first choice for images, audio and video data, the atoms in molecules are not restricted to a grid. Instead, their precise locations contain essential physical information, that would get lost if discretized. Thus, we propose to use continuous-filter convolutional layers to be able to model local correlations without requiring the data to lie on a grid. We apply those layers in SchNet: a novel deep learning architecture modeling quantum interactions in molecules. We obtain a joint model for the total energy and interatomic forces that follows fundamental quantum-chemical principles. This includes rotationally invariant energy predictions and a smooth, differentiable potential energy surface. Our architecture achieves state-of-the-art performance for benchmarks of equilibrium molecules and molecular dynamics trajectories. Finally, we introduce a more challenging benchmark with chemical and structural variations that suggests the path for further work.'}}, {'citedPaper': {'paperId': '6b7d6e6416343b2a122f8416e69059ce919026ef', 'title': 'Inductive Representation Learning on Large Graphs', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1706.02216, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '49437682', 'name': 'William L. Hamilton'}, {'authorId': '4058003', 'name': 'Z. Ying'}, {'authorId': '1702139', 'name': 'J. Leskovec'}], 'abstract': "Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions."}}, {'citedPaper': {'paperId': 'ee9c6aeb6e29cf3c9081df2cc100b8203ebf5cff', 'title': 'Deriving Neural Architectures from Sequence and Graph Kernels', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1705.09037, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '49986267', 'name': 'Tao Lei'}, {'authorId': '2400119', 'name': 'Wengong Jin'}, {'authorId': '1741283', 'name': 'R. Barzilay'}, {'authorId': '35132120', 'name': 'T. Jaakkola'}], 'abstract': 'The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects (cf. filters in CNN) via the kernels. Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training. We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art results across these applications.'}}, {'citedPaper': {'paperId': '1a39bb2caa151d15efd6718f3a80d9f4bff95af2', 'title': 'Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs', 'openAccessPdf': {'url': 'https://arxiv.org/pdf/1704.02901', 'status': 'GREEN', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1704.02901, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '3451689', 'name': 'M. Simonovsky'}, {'authorId': '2505902', 'name': 'N. Komodakis'}], 'abstract': 'A number of problems can be formulated as prediction on graph-structured data. In this work, we generalize the convolution operator from regular grids to arbitrary graphs while avoiding the spectral domain, which allows us to handle graphs of varying size and connectivity. To move beyond a simple diffusion, filter weights are conditioned on the specific edge labels in the neighborhood of a vertex. Together with the proper choice of graph coarsening, we explore constructing deep neural networks for graph classification. In particular, we demonstrate the generality of our formulation in point cloud classification, where we set the new state of the art, and on a graph classification dataset, where we outperform other deep learning approaches.'}}, {'citedPaper': {'paperId': 'e24cdf73b3e7e590c2fe5ecac9ae8aa983801367', 'title': 'Neural Message Passing for Quantum Chemistry', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1704.01212, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2058362', 'name': 'J. Gilmer'}, {'authorId': '2601641', 'name': 'S. Schoenholz'}, {'authorId': '119508204', 'name': 'Patrick F. Riley'}, {'authorId': '1689108', 'name': 'O. Vinyals'}, {'authorId': '35188630', 'name': 'George E. Dahl'}], 'abstract': 'Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.'}}, {'citedPaper': {'paperId': 'cd8a9914d50b0ac63315872530274d158d6aff09', 'title': 'Modeling Relational Data with Graph Convolutional Networks', 'openAccessPdf': {'url': '', 'status': 'CLOSED', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1703.06103, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '8804828', 'name': 'M. Schlichtkrull'}, {'authorId': '41016725', 'name': 'Thomas Kipf'}, {'authorId': '2789097', 'name': 'Peter Bloem'}, {'authorId': '9965217', 'name': 'Rianne van den Berg'}, {'authorId': '144889265', 'name': 'Ivan Titov'}, {'authorId': '1678311', 'name': 'M. Welling'}], 'abstract': 'Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.'}}, {'citedPaper': {'paperId': '0e779fd59353a7f1f5b559b9d65fa4bfe367890c', 'title': 'Geometric Deep Learning: Going beyond Euclidean data', 'openAccessPdf': {'url': 'http://arxiv.org/pdf/1611.08097', 'status': 'GREEN', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1611.08097, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1732570', 'name': 'M. Bronstein'}, {'authorId': '143627859', 'name': 'Joan Bruna'}, {'authorId': '1688882', 'name': 'Yann LeCun'}, {'authorId': '3149531', 'name': 'Arthur Szlam'}, {'authorId': '1697397', 'name': 'P. Vandergheynst'}], 'abstract': 'Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.'}}, {'citedPaper': {'paperId': '36eff562f65125511b5dfab68ce7f7a943c27478', 'title': 'Semi-Supervised Classification with Graph Convolutional Networks', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1609.02907, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '41016725', 'name': 'Thomas Kipf'}, {'authorId': '1678311', 'name': 'M. Welling'}], 'abstract': 'We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.'}}, {'citedPaper': {'paperId': 'c41eb895616e453dcba1a70c9b942c5063cc656c', 'title': 'Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1606.09375, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '3422350', 'name': 'Michaël Defferrard'}, {'authorId': '2549032', 'name': 'X. Bresson'}, {'authorId': '1697397', 'name': 'P. Vandergheynst'}], 'abstract': "In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs."}}, {'citedPaper': {'paperId': 'da2e04453b6f0d89ee75e6f68d619d936cd9c0b5', 'title': 'On Valid Optimal Assignment Kernels and Applications to Graph Classification', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1606.01141, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1810077', 'name': 'Nils M. Kriege'}, {'authorId': '144790942', 'name': 'P. Giscard'}, {'authorId': '2111010157', 'name': 'Richard C. Wilson'}], 'abstract': 'The success of kernel methods has initiated the design of novel positive semidefinite functions, in particular for structured data. A leading design paradigm for this is the convolution kernel, which decomposes structured objects into their parts and sums over all pairs of parts. Assignment kernels, in contrast, are obtained from an optimal bijection between parts, which can provide a more valid notion of similarity. In general however, optimal assignments yield indefinite functions, which complicates their use in kernel methods. We characterize a class of base kernels used to compare parts that guarantees positive semidefinite optimal assignment kernels. These base kernels give rise to hierarchies from which the optimal assignment kernels are computed in linear time by histogram intersection. We apply these results by developing the Weisfeiler-Lehman optimal assignment kernel for graphs. It provides high classification accuracy on widely-used benchmark data sets improving over the original Weisfeiler-Lehman kernel.'}}, {'citedPaper': {'paperId': '7c6de5a9e02a779e24504619050c6118f4eac181', 'title': 'Learning Convolutional Neural Networks for Graphs', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1605.05273, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2780262', 'name': 'Mathias Niepert'}, {'authorId': '24931083', 'name': 'Mohamed Ahmed'}, {'authorId': '1712289', 'name': 'Konstantin Kutzkov'}], 'abstract': 'Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.'}}, {'citedPaper': {'paperId': '322cf9bcde458a45eaeca989a1eec92f7c6db984', 'title': 'Discriminative Embeddings of Latent Variable Models for Structured Data', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1603.05629, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2791430', 'name': 'H. Dai'}, {'authorId': '144445933', 'name': 'Bo Dai'}, {'authorId': '1779453', 'name': 'Le Song'}], 'abstract': 'Kernel classifiers and regressors designed for structured data, such as sequences, trees and graphs, have significantly advanced a number of interdisciplinary areas such as computational biology and drug design. Typically, kernels are designed beforehand for a data type which either exploit statistics of the structures or make use of probabilistic generative models, and then a discriminative classifier is learned based on the kernels via convex optimization. However, such an elegant two-stage approach also limited kernel methods from scaling up to millions of data points, and exploiting discriminative information to learn feature representations. \n \nWe propose, structure2vec, an effective and scalable approach for structured data representation based on the idea of embedding latent variable models into feature spaces, and learning such feature spaces using discriminative information. Interestingly, structure2vec extracts features by performing a sequence of function mappings in a way similar to graphical model inference procedures, such as mean field and belief propagation. In applications involving millions of data points, we showed that structure2vec runs 2 times faster, produces models which are 10, 000 times smaller, while at the same time achieving the state-of-the-art predictive performance.'}}, {'citedPaper': {'paperId': '3ce05152dbedab572167e031b90d677c13b49767', 'title': 'A Structural Smoothing Framework For Robust Graph Comparison', 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'authors': [{'authorId': '2304445773', 'name': 'Pinar Yanardag'}, {'authorId': '145713876', 'name': 'S. Vishwanathan'}], 'abstract': None}}, {'citedPaper': {'paperId': 'd01379ebb53c66a4ccf5f4959d904dcf9e161e41', 'title': 'Order Matters: Sequence to sequence for sets', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1511.06391, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1689108', 'name': 'O. Vinyals'}, {'authorId': '1751569', 'name': 'Samy Bengio'}, {'authorId': '1942300', 'name': 'M. Kudlur'}], 'abstract': 'Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models.'}}, {'citedPaper': {'paperId': '492f57ee9ceb61fb5a47ad7aebfec1121887a175', 'title': 'Gated Graph Sequence Neural Networks', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1511.05493, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '47002813', 'name': 'Yujia Li'}, {'authorId': '1725299', 'name': 'Daniel Tarlow'}, {'authorId': '2107692', 'name': 'Marc Brockschmidt'}, {'authorId': '1804104', 'name': 'R. Zemel'}], 'abstract': 'Abstract: Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.'}}, {'citedPaper': {'paperId': '5d1bfeed240709725c78bc72ea40e55410b373dc', 'title': 'Convolutional Networks on Graphs for Learning Molecular Fingerprints', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1509.09292, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1704657', 'name': 'D. Duvenaud'}, {'authorId': '1683298', 'name': 'D. Maclaurin'}, {'authorId': '1422175619', 'name': 'J. Aguilera-Iparraguirre'}, {'authorId': '2344011563', 'name': 'Rafael Gómez-Bombarelli'}, {'authorId': '145916942', 'name': 'Timothy D. Hirzel'}, {'authorId': '1380248954', 'name': 'Alán Aspuru-Guzik'}, {'authorId': '1722180', 'name': 'Ryan P. Adams'}], 'abstract': 'We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.'}}, {'citedPaper': {'paperId': '995c5f5e62614fcb4d2796ad2faab969da51713e', 'title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1502.03167, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2054165706', 'name': 'Sergey Ioffe'}, {'authorId': '2574060', 'name': 'Christian Szegedy'}], 'abstract': "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."}}, {'citedPaper': {'paperId': '5e925a9f1e20df61d1e860a7aa71894b35a1c186', 'title': 'Spectral Networks and Locally Connected Networks on Graphs', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1312.6203, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '143627859', 'name': 'Joan Bruna'}, {'authorId': '2563432', 'name': 'Wojciech Zaremba'}, {'authorId': '3149531', 'name': 'Arthur Szlam'}, {'authorId': '1688882', 'name': 'Yann LeCun'}], 'abstract': 'Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.'}}, {'citedPaper': {'paperId': 'd87762aa2ef5a259825cf75d5865d7ddf86b7018', 'title': 'Scalable kernels for graphs with continuous attributes', 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'authors': [{'authorId': '1808965', 'name': 'Aasa Feragen'}, {'authorId': '2423123', 'name': 'Niklas Kasenburg'}, {'authorId': '152800798', 'name': 'Jens Petersen'}, {'authorId': '32895376', 'name': 'Marleen de Bruijne'}, {'authorId': '1704422', 'name': 'Karsten M. Borgwardt'}], 'abstract': None}}, {'citedPaper': {'paperId': 'b3e025049142be3d97d559cca12b027f16c6349e', 'title': 'Deep Architectures and Deep Learning in Chemoinformatics: The Prediction of Aqueous Solubility for Drug-Like Molecules', 'openAccessPdf': {'url': 'https://europepmc.org/articles/pmc3739985?pdf=render', 'status': 'GREEN', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1021/ci400187y?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1021/ci400187y, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '3318293', 'name': 'A. Lusci'}, {'authorId': '2028880', 'name': 'G. Pollastri'}, {'authorId': '144902513', 'name': 'P. Baldi'}], 'abstract': 'Shallow machine learning methods have been applied to chemoinformatics problems with some success. As more data becomes available and more complex problems are tackled, deep machine learning methods may also become useful. Here, we present a brief overview of deep learning methods and show in particular how recursive neural network approaches can be applied to the problem of predicting molecular properties. However, molecules are typically described by undirected cyclic graphs, while recursive approaches typically use directed acyclic graphs. Thus, we develop methods to address this discrepancy, essentially by considering an ensemble of recursive neural networks associated with all possible vertex-centered acyclic orientations of the molecular graph. One advantage of this approach is that it relies only minimally on the identification of suitable molecular descriptors because suitable representations are learned automatically from the data. Several variants of this approach are applied to the problem of predicting aqueous solubility and tested on four benchmark data sets. Experimental results show that the performance of the deep learning methods matches or exceeds the performance of other state-of-the-art methods according to several evaluation metrics and expose the fundamental limitations arising from training sets that are too small or too noisy. A Web-based predictor, AquaSol, is available online through the ChemDB portal ( cdb.ics.uci.edu ) together with additional material.'}}, {'citedPaper': {'paperId': 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', 'title': 'ImageNet classification with deep convolutional neural networks', 'openAccessPdf': {'url': 'http://dl.acm.org/ft_gateway.cfm?id=3065386&type=pdf', 'status': 'BRONZE', 'license': None, 'disclaimer': "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/3065386?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/3065386, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, 'authors': [{'authorId': '2064160', 'name': 'A. Krizhevsky'}, {'authorId': '1701686', 'name': 'I. Sutskever'}, {'authorId': '1695689', 'name': 'Geoffrey E. Hinton'}], 'abstract': None}}, {'citedPaper': {'paperId': '273dfbcb68080251f5e9ff38b4413d7bd84b10a1', 'title': 'LIBSVM: A library for support vector machines', 'openAccessPdf': {'url': '', 'status': 'CLOSED', 'license': None, 'disclaimer': "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1145/1961189.1961199?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1145/1961189.1961199, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, 'authors': [{'authorId': None, 'name': 'Chih-Chung Chang'}, {'authorId': '1711460', 'name': 'Chih-Jen Lin'}], 'abstract': None}}, {'citedPaper': {'paperId': '7e1874986cf6433fabf96fff93ef42b60bdc49f8', 'title': 'Weisfeiler-Lehman Graph Kernels', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.5555/1953048.2078187?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.5555/1953048.2078187, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1898417', 'name': 'N. Shervashidze'}, {'authorId': '31500557', 'name': 'Pascal Schweitzer'}, {'authorId': '1711983', 'name': 'E. J. V. Leeuwen'}, {'authorId': '1698752', 'name': 'K. Mehlhorn'}, {'authorId': '1704422', 'name': 'Karsten M. Borgwardt'}], 'abstract': 'In this article, we propose a family of efficient kernels for large graphs with discrete node labels. Key to our method is a rapid feature extraction scheme based on the Weisfeiler-Lehman test of isomorphism on graphs. It maps the original graph to a sequence of graphs, whose node attributes capture topological and label information. A family of kernels can be defined based on this Weisfeiler-Lehman sequence of graphs, including a highly efficient kernel comparing subtree-like patterns. Its runtime scales only linearly in the number of edges of the graphs and the length of the Weisfeiler-Lehman graph sequence. In our experimental evaluation, our kernels outperform state-of-the-art graph kernels on several graph classification benchmark data sets in terms of accuracy and runtime. Our kernels open the door to large-scale applications of graph kernels in various disciplines such as computational biology and social network analysis.'}}, {'citedPaper': {'paperId': 'a7fc751cd95bd1a409a26daaef69fc3aa8a35e0e', 'title': 'Efficient graphlet kernels for large graph comparison', 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'authors': [{'authorId': '1898417', 'name': 'N. Shervashidze'}, {'authorId': '145713876', 'name': 'S. Vishwanathan'}, {'authorId': '70590076', 'name': 'Tobias Petri'}, {'authorId': '1698752', 'name': 'K. Mehlhorn'}, {'authorId': '1704422', 'name': 'Karsten M. Borgwardt'}], 'abstract': None}}, {'citedPaper': {'paperId': '34795caa356317f2c662bd4553cd7608e0aeb657', 'title': 'Weighted Graph Cuts without Eigenvectors A Multilevel Approach', 'openAccessPdf': {'url': '', 'status': 'CLOSED', 'license': None, 'disclaimer': "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TPAMI.2007.1115?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TPAMI.2007.1115, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, 'authors': [{'authorId': '1783667', 'name': 'I. Dhillon'}, {'authorId': '1853389', 'name': 'Yuqiang Guan'}, {'authorId': '1692670', 'name': 'Brian Kulis'}], 'abstract': None}}, {'citedPaper': {'paperId': '70ff05e93f4eaeef4b8b51ba4e11d1c4cdf856ca', 'title': 'Shortest-path kernels on graphs', 'openAccessPdf': {'url': 'http://cbio.ensmp.fr/~jvert/svn/bibli/local/Borgwardt2005Shortest-Path.pdf', 'status': 'GREEN', 'license': None, 'disclaimer': "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/ICDM.2005.132?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/ICDM.2005.132, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, 'authors': [{'authorId': '1704422', 'name': 'Karsten M. Borgwardt'}, {'authorId': '1688561', 'name': 'H. Kriegel'}], 'abstract': None}}, {'citedPaper': {'paperId': '20036cd7fac884130f642d3df30b1fd7fb24c6d2', 'title': 'Automatic Generation of Complementary Descriptors with Molecular Graph Networks', 'openAccessPdf': {'url': '', 'status': 'CLOSED', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1021/CI049613B?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1021/CI049613B, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '2114450', 'name': 'Christian Merkwirth'}, {'authorId': '49370597', 'name': 'Thomas Lengauer'}], 'abstract': 'We describe a method for the automatic generation of weakly correlated descriptors for molecular data sets. The method can be regarded as a statistical learning procedure that turns the molecular graph, representing the 2D formula of the compound, into an adaptive whole molecule composite descriptor. By translating the molecular graph structure into a dynamical system, the algorithm can compute an output value that is highly sensitive to the molecular topology. This system can be trained by gradient descent techniques, which rely on the efficient calculation of the gradient by back-propagation. We present computational experiments concerning the classification of the Developmental Therapeutics Program AIDS antiviral screen data set on which the performance of the method compares with that of approaches based on substructure comparison.'}}, {'citedPaper': {'paperId': 'c2815f76c690ec2ca6523b1faacf9b6a401bdff1', 'title': 'Distinguishing enzyme structures from non-enzymes without alignments.', 'openAccessPdf': {'url': 'http://cbio.ensmp.fr/~jvert/svn/bibli/local/Dobson2003Distinguishing.pdf', 'status': 'GREEN', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1016/S0022-2836(03)00628-4?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1016/S0022-2836(03)00628-4, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '145002767', 'name': 'P. Dobson'}, {'authorId': '1970705', 'name': 'A. Doig'}], 'abstract': None}}, {'citedPaper': {'paperId': '25ec680e94d84e93e3bd0409d6084bfb43c4cdb6', 'title': 'Processing directed acyclic graphs with recursive neural networks', 'openAccessPdf': {'url': '', 'status': 'CLOSED', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1109/72.963781?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/72.963781, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '144020416', 'name': 'M. Bianchini'}, {'authorId': '145467467', 'name': 'M. Gori'}, {'authorId': '47260481', 'name': 'F. Scarselli'}], 'abstract': 'Recursive neural networks are conceived for processing graphs and extend the well-known recurrent model for processing sequences. In Frasconi et al. (1998), recursive neural networks can deal only with directed ordered acyclic graphs (DOAGs), in which the children of any given node are ordered. While this assumption is reasonable in some applications, it introduces unnecessary constraints in others. In this paper, it is shown that the constraint on the ordering can be relaxed by using an appropriate weight sharing, that guarantees the independence of the network output with respect to the permutations of the arcs leaving from each node. The method can be used with graphs having low connectivity and, in particular, few outcoming arcs. Some theoretical properties of the proposed architecture are given. They guarantee that the approximation capabilities are maintained, despite the weight sharing.'}}, {'citedPaper': {'paperId': '89b1766a77f7a34aadd67861123d5cf3cd3ece52', 'title': 'Neural Relational Inference for Interacting Systems', 'openAccessPdf': {'url': '', 'status': None, 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://arxiv.org/abs/1802.04687, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1643311826', 'name': 'T. Fetaya'}, {'authorId': '50844928', 'name': 'E. Wang'}, {'authorId': '1643758681', 'name': 'K.-C. Welling'}, {'authorId': '47446467', 'name': 'M. Zemel'}, {'authorId': '41016725', 'name': 'Thomas Kipf'}, {'authorId': '2645055', 'name': 'Ethan Fetaya'}, {'authorId': '122782486', 'name': 'Kuan-Chieh Jackson Wang'}, {'authorId': '1678311', 'name': 'M. Welling'}, {'authorId': '1804104', 'name': 'R. Zemel'}], 'abstract': "Interacting systems are prevalent in nature, from dynamical systems in physics to complex societal dynamics. The interplay of components can give rise to complex behavior, which can often be explained using a simple model of the system's constituent parts. In this work, we introduce the neural relational inference (NRI) model: an unsupervised model that learns to infer interactions while simultaneously learning the dynamics purely from observational data. Our model takes the form of a variational auto-encoder, in which the latent code represents the underlying interaction graph and the reconstruction is based on graph neural networks. In experiments on simulated physical systems, we show that our NRI model can accurately recover ground-truth interactions in an unsupervised manner. We further demonstrate that we can find an interpretable structure and predict complex dynamics in real motion capture and sports tracking data."}}, {'citedPaper': {'paperId': None, 'title': 'Benchmark data sets for graph kernels, 2016', 'openAccessPdf': None, 'authors': [], 'abstract': None}}, {'citedPaper': {'paperId': '3efd851140aa28e95221b55fcc5659eea97b172d', 'title': 'The Graph Neural Network Model', 'openAccessPdf': {'url': 'https://ro.uow.edu.au/articles/journal_contribution/The_graph_neural_network_model/27757629/1/files/50520873.pdf', 'status': 'GREEN', 'license': 'other-oa', 'disclaimer': "Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1109/TNN.2008.2005605?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1109/TNN.2008.2005605, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use."}, 'authors': [{'authorId': '47260481', 'name': 'F. Scarselli'}, {'authorId': '145467467', 'name': 'M. Gori'}, {'authorId': '1733691', 'name': 'A. Tsoi'}, {'authorId': '1784450', 'name': 'M. Hagenbuchner'}, {'authorId': '3073217', 'name': 'G. Monfardini'}], 'abstract': None}}, {'citedPaper': {'paperId': 'ba640c0682b242cb480bb4eb5b934ee6db949269', 'title': 'Protein function prediction via graph kernels', 'openAccessPdf': {'url': 'https://academic.oup.com/bioinformatics/article-pdf/21/suppl_1/i47/524364/bti1007.pdf', 'status': 'BRONZE', 'license': None, 'disclaimer': 'Notice: This content is from the open access paper or abstract available at https://api.unpaywall.org/v2/10.1093/bioinformatics/bti1007?email=<INSERT_YOUR_EMAIL> or https://doi.org/10.1093/bioinformatics/bti1007, which is subject to the license by the author or copyright owner provided with this content. Please go to the source to verify the license and copyright information for your use.'}, 'authors': [{'authorId': '1704422', 'name': 'Karsten M. Borgwardt'}, {'authorId': '1706780', 'name': 'Cheng Soon Ong'}, {'authorId': '2617402', 'name': 'Stefan Schönauer'}, {'authorId': '145713876', 'name': 'S. Vishwanathan'}, {'authorId': '46234526', 'name': 'Alex Smola'}, {'authorId': '1688561', 'name': 'H. Kriegel'}], 'abstract': 'MOTIVATION\nComputational approaches to protein function prediction infer protein function by finding proteins with similar sequence, structure, surface clefts, chemical properties, amino acid motifs, interaction partners or phylogenetic profiles. We present a new approach that combines sequential, structural and chemical information into one graph model of proteins. We predict functional class membership of enzymes and non-enzymes using graph kernels and support vector machine classification on these protein graphs.\n\n\nRESULTS\nOur graph model, derivable from protein sequence and structure only, is competitive with vector models that require additional protein information, such as the size of surface pockets. If we include this extra information into our graph model, our classifier yields significantly higher accuracy levels than the vector models. Hyperkernels allow us to select and to optimally combine the most relevant node attributes in our protein graphs. We have laid the foundation for a protein function prediction system that integrates protein information from various sources efficiently and effectively.\n\n\nAVAILABILITY\nMore information available via www.dbs.ifi.lmu.de/Mitarbeiter/borgwardt.html.'}}]}
Cited Paper:
[{'title': 'Graph Capsule Convolutional Neural Networks', 'abstract': 'Graph Convolutional Neural Networks (GCNNs) are the most recent exciting advancement in deep learning field and their applications are quickly spreading in multi-cross-domains including bioinformatics, chemoinformatics, social networks, natural language processing and computer vision. In this paper, we expose and tackle some of the basic weaknesses of a GCNN model with a capsule idea presented in \\cite{hinton2011transforming} and propose our Graph Capsule Network (GCAPS-CNN) model. In addition, we design our GCAPS-CNN model to solve especially graph classification problem which current GCNN models find challenging. Through extensive experiments, we show that our proposed Graph Capsule Network can significantly outperforms both the existing state-of-art deep learning methods and graph kernels on graph classification benchmark datasets.'}, {'title': 'An End-to-End Deep Learning Architecture for Graph Classification', 'abstract': '\n \n Neural networks are typically designed to deal with data in tensor forms. In this paper, we propose a novel neural network architecture accepting graphs of arbitrary structure. Given a dataset containing graphs in the form of (G,y) where G is a graph and y is its class, we aim to develop neural networks that read the graphs directly and learn a classification function. There are two main challenges: 1) how to extract useful features characterizing the rich information encoded in a graph for classification purpose, and 2) how to sequentially read a graph in a meaningful and consistent order. To address the first challenge, we design a localized graph convolution model and show its connection with two graph kernels. To address the second challenge, we design a novel SortPooling layer which sorts graph vertices in a consistent order so that traditional neural networks can be trained on the graphs. Experiments on benchmark graph classification datasets demonstrate that the proposed architecture achieves highly competitive performance with state-of-the-art graph kernels and other graph neural network methods. Moreover, the architecture allows end-to-end gradient-based training with original graphs, without the need to first transform graphs into vectors.\n \n'}, {'title': 'Graph Partition Neural Networks for Semi-Supervised Classification', 'abstract': 'We present graph partition neural networks (GPNN), an extension of graph neural networks (GNNs) able to handle extremely large graphs. GPNNs alternate between locally propagating information between nodes in small subgraphs and globally propagating information between the subgraphs. To efficiently partition graphs, we experiment with several partitioning algorithms and also propose a novel variant for fast processing of large scale graphs. We extensively test our model on a variety of semi-supervised node classification tasks. Experimental results indicate that GPNNs are either superior or comparable to state-of-the-art methods on a wide variety of datasets for graph-based semi-supervised classification. We also show that GPNNs can achieve similar performance as standard GNNs with fewer propagation steps.'}, {'title': 'SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels', 'abstract': 'We present Spline-based Convolutional Neural Networks (SplineCNNs), a variant of deep neural networks for irregular structured and geometric input, e.g., graphs or meshes. Our main contribution is a novel convolution operator based on B-splines, that makes the computation time independent from the kernel size due to the local support property of the B-spline basis functions. As a result, we obtain a generalization of the traditional CNN convolution operator by using continuous kernel functions parametrized by a fixed number of trainable weights. In contrast to related approaches that filter in the spectral domain, the proposed method aggregates features purely in the spatial domain. In addition, SplineCNN allows entire end-to-end training of deep architectures, using only the geometric structure as input, instead of handcrafted feature descriptors. For validation, we apply our method on tasks from the fields of image graph classification, shape correspondence and graph node classification, and show that it outperforms or pars state-of-the-art approaches while being significantly faster and having favorable properties like domain-independence. Our source code is available on GitHub1.'}, {'title': 'Protein Interface Prediction using Graph Convolutional Networks', 'abstract': None}, {'title': 'Graph Attention Networks', 'abstract': "We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training)."}, {'title': 'Representation Learning on Graphs: Methods and Applications', 'abstract': 'Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.'}, {'title': 'Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network', 'abstract': 'The prediction of organic reaction outcomes is a fundamental problem in computational chemistry. Since a reaction may involve hundreds of atoms, fully exploring the space of possible transformations is intractable. The current solution utilizes reaction templates to limit the space, but it suffers from coverage and efficiency issues. In this paper, we propose a template-free approach to efficiently explore the space of product molecules by first pinpointing the reaction center -- the set of nodes and edges where graph edits occur. Since only a small number of atoms contribute to reaction center, we can directly enumerate candidate products. The generated candidates are scored by a Weisfeiler-Lehman Difference Network that models high-order interactions between changes occurring at nodes across the molecule. Our framework outperforms the top-performing template-based approach with a 10\\% margin, while running orders of magnitude faster. Finally, we demonstrate that the model accuracy rivals the performance of domain experts.'}, {'title': 'SchNet: A continuous-filter convolutional neural network for modeling quantum interactions', 'abstract': 'Deep learning has the potential to revolutionize quantum chemistry as it is ideally suited to learn representations for structured data and speed up the exploration of chemical space. While convolutional neural networks have proven to be the first choice for images, audio and video data, the atoms in molecules are not restricted to a grid. Instead, their precise locations contain essential physical information, that would get lost if discretized. Thus, we propose to use continuous-filter convolutional layers to be able to model local correlations without requiring the data to lie on a grid. We apply those layers in SchNet: a novel deep learning architecture modeling quantum interactions in molecules. We obtain a joint model for the total energy and interatomic forces that follows fundamental quantum-chemical principles. This includes rotationally invariant energy predictions and a smooth, differentiable potential energy surface. Our architecture achieves state-of-the-art performance for benchmarks of equilibrium molecules and molecular dynamics trajectories. Finally, we introduce a more challenging benchmark with chemical and structural variations that suggests the path for further work.'}, {'title': 'Inductive Representation Learning on Large Graphs', 'abstract': "Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions."}, {'title': 'Deriving Neural Architectures from Sequence and Graph Kernels', 'abstract': 'The design of neural architectures for structured objects is typically guided by experimental insights rather than a formal process. In this work, we appeal to kernels over combinatorial structures, such as sequences and graphs, to derive appropriate neural operations. We introduce a class of deep recurrent neural operations and formally characterize their associated kernel spaces. Our recurrent modules compare the input to virtual reference objects (cf. filters in CNN) via the kernels. Similar to traditional neural operations, these reference objects are parameterized and directly optimized in end-to-end training. We empirically evaluate the proposed class of neural architectures on standard applications such as language modeling and molecular graph regression, achieving state-of-the-art results across these applications.'}, {'title': 'Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs', 'abstract': 'A number of problems can be formulated as prediction on graph-structured data. In this work, we generalize the convolution operator from regular grids to arbitrary graphs while avoiding the spectral domain, which allows us to handle graphs of varying size and connectivity. To move beyond a simple diffusion, filter weights are conditioned on the specific edge labels in the neighborhood of a vertex. Together with the proper choice of graph coarsening, we explore constructing deep neural networks for graph classification. In particular, we demonstrate the generality of our formulation in point cloud classification, where we set the new state of the art, and on a graph classification dataset, where we outperform other deep learning approaches.'}, {'title': 'Neural Message Passing for Quantum Chemistry', 'abstract': 'Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.'}, {'title': 'Modeling Relational Data with Graph Convolutional Networks', 'abstract': 'Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline.'}, {'title': 'Geometric Deep Learning: Going beyond Euclidean data', 'abstract': 'Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.'}, {'title': 'Semi-Supervised Classification with Graph Convolutional Networks', 'abstract': 'We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.'}, {'title': 'Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering', 'abstract': "In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs."}, {'title': 'On Valid Optimal Assignment Kernels and Applications to Graph Classification', 'abstract': 'The success of kernel methods has initiated the design of novel positive semidefinite functions, in particular for structured data. A leading design paradigm for this is the convolution kernel, which decomposes structured objects into their parts and sums over all pairs of parts. Assignment kernels, in contrast, are obtained from an optimal bijection between parts, which can provide a more valid notion of similarity. In general however, optimal assignments yield indefinite functions, which complicates their use in kernel methods. We characterize a class of base kernels used to compare parts that guarantees positive semidefinite optimal assignment kernels. These base kernels give rise to hierarchies from which the optimal assignment kernels are computed in linear time by histogram intersection. We apply these results by developing the Weisfeiler-Lehman optimal assignment kernel for graphs. It provides high classification accuracy on widely-used benchmark data sets improving over the original Weisfeiler-Lehman kernel.'}, {'title': 'Learning Convolutional Neural Networks for Graphs', 'abstract': 'Numerous important problems can be framed as learning from graph data. We propose a framework for learning convolutional neural networks for arbitrary graphs. These graphs may be undirected, directed, and with both discrete and continuous node and edge attributes. Analogous to image-based convolutional networks that operate on locally connected regions of the input, we present a general approach to extracting locally connected regions from graphs. Using established benchmark data sets, we demonstrate that the learned feature representations are competitive with state of the art graph kernels and that their computation is highly efficient.'}, {'title': 'Discriminative Embeddings of Latent Variable Models for Structured Data', 'abstract': 'Kernel classifiers and regressors designed for structured data, such as sequences, trees and graphs, have significantly advanced a number of interdisciplinary areas such as computational biology and drug design. Typically, kernels are designed beforehand for a data type which either exploit statistics of the structures or make use of probabilistic generative models, and then a discriminative classifier is learned based on the kernels via convex optimization. However, such an elegant two-stage approach also limited kernel methods from scaling up to millions of data points, and exploiting discriminative information to learn feature representations. \n \nWe propose, structure2vec, an effective and scalable approach for structured data representation based on the idea of embedding latent variable models into feature spaces, and learning such feature spaces using discriminative information. Interestingly, structure2vec extracts features by performing a sequence of function mappings in a way similar to graphical model inference procedures, such as mean field and belief propagation. In applications involving millions of data points, we showed that structure2vec runs 2 times faster, produces models which are 10, 000 times smaller, while at the same time achieving the state-of-the-art predictive performance.'}, {'title': 'A Structural Smoothing Framework For Robust Graph Comparison', 'abstract': None}, {'title': 'Order Matters: Sequence to sequence for sets', 'abstract': 'Sequences have become first class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the sequence-to-sequence (seq2seq) framework which employs the chain rule to efficiently represent the joint probability of sequences. In many cases, however, variable sized inputs and/or outputs might not be naturally expressed as sequences. For instance, it is not clear how to input a set of numbers into a model where the task is to sort them; similarly, we do not know how to organize outputs when they correspond to random variables and the task is to model their unknown joint probability. In this paper, we first show using various examples that the order in which we organize input and/or output data matters significantly when learning an underlying model. We then discuss an extension of the seq2seq framework that goes beyond sequences and handles input sets in a principled way. In addition, we propose a loss which, by searching over possible orders during training, deals with the lack of structure of output sets. We show empirical evidence of our claims regarding ordering, and on the modifications to the seq2seq framework on benchmark language modeling and parsing tasks, as well as two artificial tasks -- sorting numbers and estimating the joint probability of unknown graphical models.'}, {'title': 'Gated Graph Sequence Neural Networks', 'abstract': 'Abstract: Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.'}, {'title': 'Convolutional Networks on Graphs for Learning Molecular Fingerprints', 'abstract': 'We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.'}, {'title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift', 'abstract': "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."}, {'title': 'Spectral Networks and Locally Connected Networks on Graphs', 'abstract': 'Convolutional Neural Networks are extremely efficient architectures in image and audio recognition tasks, thanks to their ability to exploit the local translational invariance of signal classes over their domain. In this paper we consider possible generalizations of CNNs to signals defined on more general domains without the action of a translation group. In particular, we propose two constructions, one based upon a hierarchical clustering of the domain, and another based on the spectrum of the graph Laplacian. We show through experiments that for low-dimensional graphs it is possible to learn convolutional layers with a number of parameters independent of the input size, resulting in efficient deep architectures.'}, {'title': 'Scalable kernels for graphs with continuous attributes', 'abstract': None}, {'title': 'Deep Architectures and Deep Learning in Chemoinformatics: The Prediction of Aqueous Solubility for Drug-Like Molecules', 'abstract': 'Shallow machine learning methods have been applied to chemoinformatics problems with some success. As more data becomes available and more complex problems are tackled, deep machine learning methods may also become useful. Here, we present a brief overview of deep learning methods and show in particular how recursive neural network approaches can be applied to the problem of predicting molecular properties. However, molecules are typically described by undirected cyclic graphs, while recursive approaches typically use directed acyclic graphs. Thus, we develop methods to address this discrepancy, essentially by considering an ensemble of recursive neural networks associated with all possible vertex-centered acyclic orientations of the molecular graph. One advantage of this approach is that it relies only minimally on the identification of suitable molecular descriptors because suitable representations are learned automatically from the data. Several variants of this approach are applied to the problem of predicting aqueous solubility and tested on four benchmark data sets. Experimental results show that the performance of the deep learning methods matches or exceeds the performance of other state-of-the-art methods according to several evaluation metrics and expose the fundamental limitations arising from training sets that are too small or too noisy. A Web-based predictor, AquaSol, is available online through the ChemDB portal ( cdb.ics.uci.edu ) together with additional material.'}, {'title': 'ImageNet classification with deep convolutional neural networks', 'abstract': None}, {'title': 'LIBSVM: A library for support vector machines', 'abstract': None}, {'title': 'Weisfeiler-Lehman Graph Kernels', 'abstract': 'In this article, we propose a family of efficient kernels for large graphs with discrete node labels. Key to our method is a rapid feature extraction scheme based on the Weisfeiler-Lehman test of isomorphism on graphs. It maps the original graph to a sequence of graphs, whose node attributes capture topological and label information. A family of kernels can be defined based on this Weisfeiler-Lehman sequence of graphs, including a highly efficient kernel comparing subtree-like patterns. Its runtime scales only linearly in the number of edges of the graphs and the length of the Weisfeiler-Lehman graph sequence. In our experimental evaluation, our kernels outperform state-of-the-art graph kernels on several graph classification benchmark data sets in terms of accuracy and runtime. Our kernels open the door to large-scale applications of graph kernels in various disciplines such as computational biology and social network analysis.'}, {'title': 'Efficient graphlet kernels for large graph comparison', 'abstract': None}, {'title': 'Weighted Graph Cuts without Eigenvectors A Multilevel Approach', 'abstract': None}, {'title': 'Shortest-path kernels on graphs', 'abstract': None}, {'title': 'Automatic Generation of Complementary Descriptors with Molecular Graph Networks', 'abstract': 'We describe a method for the automatic generation of weakly correlated descriptors for molecular data sets. The method can be regarded as a statistical learning procedure that turns the molecular graph, representing the 2D formula of the compound, into an adaptive whole molecule composite descriptor. By translating the molecular graph structure into a dynamical system, the algorithm can compute an output value that is highly sensitive to the molecular topology. This system can be trained by gradient descent techniques, which rely on the efficient calculation of the gradient by back-propagation. We present computational experiments concerning the classification of the Developmental Therapeutics Program AIDS antiviral screen data set on which the performance of the method compares with that of approaches based on substructure comparison.'}, {'title': 'Distinguishing enzyme structures from non-enzymes without alignments.', 'abstract': None}, {'title': 'Processing directed acyclic graphs with recursive neural networks', 'abstract': 'Recursive neural networks are conceived for processing graphs and extend the well-known recurrent model for processing sequences. In Frasconi et al. (1998), recursive neural networks can deal only with directed ordered acyclic graphs (DOAGs), in which the children of any given node are ordered. While this assumption is reasonable in some applications, it introduces unnecessary constraints in others. In this paper, it is shown that the constraint on the ordering can be relaxed by using an appropriate weight sharing, that guarantees the independence of the network output with respect to the permutations of the arcs leaving from each node. The method can be used with graphs having low connectivity and, in particular, few outcoming arcs. Some theoretical properties of the proposed architecture are given. They guarantee that the approximation capabilities are maintained, despite the weight sharing.'}, {'title': 'Neural Relational Inference for Interacting Systems', 'abstract': "Interacting systems are prevalent in nature, from dynamical systems in physics to complex societal dynamics. The interplay of components can give rise to complex behavior, which can often be explained using a simple model of the system's constituent parts. In this work, we introduce the neural relational inference (NRI) model: an unsupervised model that learns to infer interactions while simultaneously learning the dynamics purely from observational data. Our model takes the form of a variational auto-encoder, in which the latent code represents the underlying interaction graph and the reconstruction is based on graph neural networks. In experiments on simulated physical systems, we show that our NRI model can accurately recover ground-truth interactions in an unsupervised manner. We further demonstrate that we can find an interpretable structure and predict complex dynamics in real motion capture and sports tracking data."}, {'title': 'Benchmark data sets for graph kernels, 2016', 'abstract': None}, {'title': 'The Graph Neural Network Model', 'abstract': None}, {'title': 'Protein function prediction via graph kernels', 'abstract': 'MOTIVATION\nComputational approaches to protein function prediction infer protein function by finding proteins with similar sequence, structure, surface clefts, chemical properties, amino acid motifs, interaction partners or phylogenetic profiles. We present a new approach that combines sequential, structural and chemical information into one graph model of proteins. We predict functional class membership of enzymes and non-enzymes using graph kernels and support vector machine classification on these protein graphs.\n\n\nRESULTS\nOur graph model, derivable from protein sequence and structure only, is competitive with vector models that require additional protein information, such as the size of surface pockets. If we include this extra information into our graph model, our classifier yields significantly higher accuracy levels than the vector models. Hyperkernels allow us to select and to optimally combine the most relevant node attributes in our protein graphs. We have laid the foundation for a protein function prediction system that integrates protein information from various sources efficiently and effectively.\n\n\nAVAILABILITY\nMore information available via www.dbs.ifi.lmu.de/Mitarbeiter/borgwardt.html.'}]
41
