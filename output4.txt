SEMANTIC_SCHOLAR_API_KEY not set in .env
SEMANTIC_SCHOLAR_API_KEY not set in .env
[+] Task for 2507.10539v1 initialized.
arxiv ids: {'2507.10539v1'}
paper with id 2507.10539v1 downloaded
arxiv ids: ['2507.10539v1']
seed: ['2507.10539v1']
cnt: 1
BFS_que.qsize(): 1
current paper: 2507.10539v1
Thread 13016903680 Processing 2507.10539v1
Thread 13016903680 Processing file main.tex
res before parsing: {'original': '\\begin{figure*}[ht]\n\\centering\n\\includegraphics[width=\\linewidth]{figs/r1.pdf} \n\\vspace{-5mm}\n\\caption{\\textbf{Multi-modal world state transition can be modeled via graphs}. We model the current state as a graph and each node contains one or more modalities from image, table, and text. Further, the world action is modeled as an action node that queries the current state nodes. We categorize actions into two types: intended actions, which include three levels—node, edge, and graph—and unintended actions, whose implementation involves similarity computation similar to RAG. Finally, the transition function updates states at three different levels based on state and action: update nodes, update edges, and update graphs.}\n\\label{fig:graph_world}\n\\vspace{-2mm}\n\\end{figure*}', 'caption': '', 'label': None, 'subfigures': [], 'figure_paths': []}
node.macroname: includegraphics
node.latex_verbatim(): \includegraphics[width=\linewidth]{figs/r1.pdf}
parsed figure info: {'original': '\\begin{figure*}[ht]\n\\centering\n\\includegraphics[width=\\linewidth]{figs/r1.pdf} \n\\vspace{-5mm}\n\\caption{\\textbf{Multi-modal world state transition can be modeled via graphs}. We model the current state as a graph and each node contains one or more modalities from image, table, and text. Further, the world action is modeled as an action node that queries the current state nodes. We categorize actions into two types: intended actions, which include three levels—node, edge, and graph—and unintended actions, whose implementation involves similarity computation similar to RAG. Finally, the transition function updates states at three different levels based on state and action: update nodes, update edges, and update graphs.}\n\\label{fig:graph_world}\n\\vspace{-2mm}\n\\end{figure*}', 'caption': '\\caption{\\textbf{Multi-modal world state transition can be modeled via graphs}. We model the current state as a graph and each node contains one or more modalities from image, table, and text. Further, the world action is modeled as an action node that queries the current state nodes. We categorize actions into two types: intended actions, which include three levels—node, edge, and graph—and unintended actions, whose implementation involves similarity computation similar to RAG. Finally, the transition function updates states at three different levels based on state and action: update nodes, update edges, and update graphs.}', 'label': '\\label{fig:graph_world}', 'subfigures': [], 'figure_paths': ['figs/r1.pdf']}
parsed figure info with consideration of subfigures: {'original': '\\begin{figure*}[ht]\n\\centering\n\\includegraphics[width=\\linewidth]{figs/r1.pdf} \n\\vspace{-5mm}\n\\caption{\\textbf{Multi-modal world state transition can be modeled via graphs}. We model the current state as a graph and each node contains one or more modalities from image, table, and text. Further, the world action is modeled as an action node that queries the current state nodes. We categorize actions into two types: intended actions, which include three levels—node, edge, and graph—and unintended actions, whose implementation involves similarity computation similar to RAG. Finally, the transition function updates states at three different levels based on state and action: update nodes, update edges, and update graphs.}\n\\label{fig:graph_world}\n\\vspace{-2mm}\n\\end{figure*}', 'caption': '\\caption{\\textbf{Multi-modal world state transition can be modeled via graphs}. We model the current state as a graph and each node contains one or more modalities from image, table, and text. Further, the world action is modeled as an action node that queries the current state nodes. We categorize actions into two types: intended actions, which include three levels—node, edge, and graph—and unintended actions, whose implementation involves similarity computation similar to RAG. Finally, the transition function updates states at three different levels based on state and action: update nodes, update edges, and update graphs.}', 'label': '\\label{fig:graph_world}', 'subfigures': [], 'figure_paths': ['figs/r1.pdf', 'figs/r1.pdf']}
res before parsing: {'original': '\\begin{figure*}[ht]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{figs/Instantiations.pdf} \n\\vspace{-2mm}\n\\caption{\\textbf{Instantiations of GWM}.  \n(a) Multi-modal generation\nand matching contains two sub-tasks. For multi-modal generation, it models modal clusters as nodes, whereas for multi-modal matching, it models nodes for each modality. It includes edges that represent inter-modal correspondences and cross-modal similarities. For these two types of subtasks, there are node-level and edge-level action nodes, respectively. (b) The state nodes of recommendation include user nodes and item nodes. Moreover, its edges are primarily derived from user-item interactions. We model edge-level action nodes to perform interaction prediction. (c) In traditional graph prediction, we follow existing work to construct task nodes and edges, and model three levels of action nodes according to different types of tasks. (d) The state nodes of multi-agent collaboration include agent nodes and multi-modal nodes from external knowledge. Its edges primarily consist of communications between agents and interactions between agents and external knowledge. We set up a graph-level action node to generate content based on the interactions of agents. (e)  Retrieval-augmented generation treats each data chunk as a state node and builds edges through the embedding similarity between chunks. For this task, we have established unintended action nodes.  (f) For planning and optimization, we model each decision as a state node and construct edges based on their relationships. We have established graph-level action nodes to generate the next decision.}\n\\label{fig: Instantiations of GWM}\n\\vspace{-3mm}\n\\end{figure*}', 'caption': '', 'label': None, 'subfigures': [], 'figure_paths': []}
node.macroname: includegraphics
node.latex_verbatim(): \includegraphics[width=0.95\linewidth]{figs/Instantiations.pdf}
parsed figure info: {'original': '\\begin{figure*}[ht]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{figs/Instantiations.pdf} \n\\vspace{-2mm}\n\\caption{\\textbf{Instantiations of GWM}.  \n(a) Multi-modal generation\nand matching contains two sub-tasks. For multi-modal generation, it models modal clusters as nodes, whereas for multi-modal matching, it models nodes for each modality. It includes edges that represent inter-modal correspondences and cross-modal similarities. For these two types of subtasks, there are node-level and edge-level action nodes, respectively. (b) The state nodes of recommendation include user nodes and item nodes. Moreover, its edges are primarily derived from user-item interactions. We model edge-level action nodes to perform interaction prediction. (c) In traditional graph prediction, we follow existing work to construct task nodes and edges, and model three levels of action nodes according to different types of tasks. (d) The state nodes of multi-agent collaboration include agent nodes and multi-modal nodes from external knowledge. Its edges primarily consist of communications between agents and interactions between agents and external knowledge. We set up a graph-level action node to generate content based on the interactions of agents. (e)  Retrieval-augmented generation treats each data chunk as a state node and builds edges through the embedding similarity between chunks. For this task, we have established unintended action nodes.  (f) For planning and optimization, we model each decision as a state node and construct edges based on their relationships. We have established graph-level action nodes to generate the next decision.}\n\\label{fig: Instantiations of GWM}\n\\vspace{-3mm}\n\\end{figure*}', 'caption': '\\caption{\\textbf{Instantiations of GWM}.  \n(a) Multi-modal generation\nand matching contains two sub-tasks. For multi-modal generation, it models modal clusters as nodes, whereas for multi-modal matching, it models nodes for each modality. It includes edges that represent inter-modal correspondences and cross-modal similarities. For these two types of subtasks, there are node-level and edge-level action nodes, respectively. (b) The state nodes of recommendation include user nodes and item nodes. Moreover, its edges are primarily derived from user-item interactions. We model edge-level action nodes to perform interaction prediction. (c) In traditional graph prediction, we follow existing work to construct task nodes and edges, and model three levels of action nodes according to different types of tasks. (d) The state nodes of multi-agent collaboration include agent nodes and multi-modal nodes from external knowledge. Its edges primarily consist of communications between agents and interactions between agents and external knowledge. We set up a graph-level action node to generate content based on the interactions of agents. (e)  Retrieval-augmented generation treats each data chunk as a state node and builds edges through the embedding similarity between chunks. For this task, we have established unintended action nodes.  (f) For planning and optimization, we model each decision as a state node and construct edges based on their relationships. We have established graph-level action nodes to generate the next decision.}', 'label': '\\label{fig: Instantiations of GWM}', 'subfigures': [], 'figure_paths': ['figs/Instantiations.pdf']}
parsed figure info with consideration of subfigures: {'original': '\\begin{figure*}[ht]\n\\centering\n\\includegraphics[width=0.95\\linewidth]{figs/Instantiations.pdf} \n\\vspace{-2mm}\n\\caption{\\textbf{Instantiations of GWM}.  \n(a) Multi-modal generation\nand matching contains two sub-tasks. For multi-modal generation, it models modal clusters as nodes, whereas for multi-modal matching, it models nodes for each modality. It includes edges that represent inter-modal correspondences and cross-modal similarities. For these two types of subtasks, there are node-level and edge-level action nodes, respectively. (b) The state nodes of recommendation include user nodes and item nodes. Moreover, its edges are primarily derived from user-item interactions. We model edge-level action nodes to perform interaction prediction. (c) In traditional graph prediction, we follow existing work to construct task nodes and edges, and model three levels of action nodes according to different types of tasks. (d) The state nodes of multi-agent collaboration include agent nodes and multi-modal nodes from external knowledge. Its edges primarily consist of communications between agents and interactions between agents and external knowledge. We set up a graph-level action node to generate content based on the interactions of agents. (e)  Retrieval-augmented generation treats each data chunk as a state node and builds edges through the embedding similarity between chunks. For this task, we have established unintended action nodes.  (f) For planning and optimization, we model each decision as a state node and construct edges based on their relationships. We have established graph-level action nodes to generate the next decision.}\n\\label{fig: Instantiations of GWM}\n\\vspace{-3mm}\n\\end{figure*}', 'caption': '\\caption{\\textbf{Instantiations of GWM}.  \n(a) Multi-modal generation\nand matching contains two sub-tasks. For multi-modal generation, it models modal clusters as nodes, whereas for multi-modal matching, it models nodes for each modality. It includes edges that represent inter-modal correspondences and cross-modal similarities. For these two types of subtasks, there are node-level and edge-level action nodes, respectively. (b) The state nodes of recommendation include user nodes and item nodes. Moreover, its edges are primarily derived from user-item interactions. We model edge-level action nodes to perform interaction prediction. (c) In traditional graph prediction, we follow existing work to construct task nodes and edges, and model three levels of action nodes according to different types of tasks. (d) The state nodes of multi-agent collaboration include agent nodes and multi-modal nodes from external knowledge. Its edges primarily consist of communications between agents and interactions between agents and external knowledge. We set up a graph-level action node to generate content based on the interactions of agents. (e)  Retrieval-augmented generation treats each data chunk as a state node and builds edges through the embedding similarity between chunks. For this task, we have established unintended action nodes.  (f) For planning and optimization, we model each decision as a state node and construct edges based on their relationships. We have established graph-level action nodes to generate the next decision.}', 'label': '\\label{fig: Instantiations of GWM}', 'subfigures': [], 'figure_paths': ['figs/Instantiations.pdf', 'figs/Instantiations.pdf']}
res before parsing: {'original': '\\begin{figure*}[ht]\n\\centering\n\\vspace{-1mm}\n\\includegraphics[width=\\linewidth]{figs/r2.pdf}\n\\vspace{-3mm}\n\\caption{\\textbf{Framework of GWM.} For both token-based and embedding-based GFM, we initially unify the multi-modal current state into graph nodes, conduct message passing, and then combine actions to predict the next state across different modalities through respective decoders. The key distinctions are: 1) Token-based GWM integrates multi-modalities into text, whereas embedding-based GWM uses modality-specific encoders to process them into embeddings; 2) Token-based GWM utilizes text-based methods for message passing, while embedding-based GWM operates at the embedding level; 3) Token-based GWM converts information into prompt form for the decoder, while embedding-based GWM employs a multi-hop projector to manage embedding-level information.}\n\\label{fig:framework}\n\\vspace{-4mm}\n\\end{figure*}', 'caption': '', 'label': None, 'subfigures': [], 'figure_paths': []}
node.macroname: includegraphics
node.latex_verbatim(): \includegraphics[width=\linewidth]{figs/r2.pdf}
parsed figure info: {'original': '\\begin{figure*}[ht]\n\\centering\n\\vspace{-1mm}\n\\includegraphics[width=\\linewidth]{figs/r2.pdf}\n\\vspace{-3mm}\n\\caption{\\textbf{Framework of GWM.} For both token-based and embedding-based GFM, we initially unify the multi-modal current state into graph nodes, conduct message passing, and then combine actions to predict the next state across different modalities through respective decoders. The key distinctions are: 1) Token-based GWM integrates multi-modalities into text, whereas embedding-based GWM uses modality-specific encoders to process them into embeddings; 2) Token-based GWM utilizes text-based methods for message passing, while embedding-based GWM operates at the embedding level; 3) Token-based GWM converts information into prompt form for the decoder, while embedding-based GWM employs a multi-hop projector to manage embedding-level information.}\n\\label{fig:framework}\n\\vspace{-4mm}\n\\end{figure*}', 'caption': '\\caption{\\textbf{Framework of GWM.} For both token-based and embedding-based GFM, we initially unify the multi-modal current state into graph nodes, conduct message passing, and then combine actions to predict the next state across different modalities through respective decoders. The key distinctions are: 1) Token-based GWM integrates multi-modalities into text, whereas embedding-based GWM uses modality-specific encoders to process them into embeddings; 2) Token-based GWM utilizes text-based methods for message passing, while embedding-based GWM operates at the embedding level; 3) Token-based GWM converts information into prompt form for the decoder, while embedding-based GWM employs a multi-hop projector to manage embedding-level information.}', 'label': '\\label{fig:framework}', 'subfigures': [], 'figure_paths': ['figs/r2.pdf']}
parsed figure info with consideration of subfigures: {'original': '\\begin{figure*}[ht]\n\\centering\n\\vspace{-1mm}\n\\includegraphics[width=\\linewidth]{figs/r2.pdf}\n\\vspace{-3mm}\n\\caption{\\textbf{Framework of GWM.} For both token-based and embedding-based GFM, we initially unify the multi-modal current state into graph nodes, conduct message passing, and then combine actions to predict the next state across different modalities through respective decoders. The key distinctions are: 1) Token-based GWM integrates multi-modalities into text, whereas embedding-based GWM uses modality-specific encoders to process them into embeddings; 2) Token-based GWM utilizes text-based methods for message passing, while embedding-based GWM operates at the embedding level; 3) Token-based GWM converts information into prompt form for the decoder, while embedding-based GWM employs a multi-hop projector to manage embedding-level information.}\n\\label{fig:framework}\n\\vspace{-4mm}\n\\end{figure*}', 'caption': '\\caption{\\textbf{Framework of GWM.} For both token-based and embedding-based GFM, we initially unify the multi-modal current state into graph nodes, conduct message passing, and then combine actions to predict the next state across different modalities through respective decoders. The key distinctions are: 1) Token-based GWM integrates multi-modalities into text, whereas embedding-based GWM uses modality-specific encoders to process them into embeddings; 2) Token-based GWM utilizes text-based methods for message passing, while embedding-based GWM operates at the embedding level; 3) Token-based GWM converts information into prompt form for the decoder, while embedding-based GWM employs a multi-hop projector to manage embedding-level information.}', 'label': '\\label{fig:framework}', 'subfigures': [], 'figure_paths': ['figs/r2.pdf', 'figs/r2.pdf']}
res before parsing: {'original': "\\begin{figure}[ht]\n\\centering\n\\includegraphics[width=\\linewidth]{figs/hops_radar.pdf} \n\\vspace{-4mm}\n\\caption{\\textbf{Multi-hop graphs enhance GWM's performance across representative tasks in six domains}. We can observe that the introduction of graphs has benefited GWM-E across all tasks compared to no graph. Moreover, excessive hops can lead to over-smoothing, thereby decreasing performance.} %We can observe that the introduction of graphs has benefited GWM-E across all tasks compared to no graph. Moreover, excessive hops can lead to over-smoothing, thereby decreasing performance.\n\\label{fig:performance_hops}\n\\vspace{-3mm}\n\\end{figure}", 'caption': '', 'label': None, 'subfigures': [], 'figure_paths': []}
node.macroname: includegraphics
node.latex_verbatim(): \includegraphics[width=\linewidth]{figs/hops_radar.pdf}
parsed figure info: {'original': "\\begin{figure}[ht]\n\\centering\n\\includegraphics[width=\\linewidth]{figs/hops_radar.pdf} \n\\vspace{-4mm}\n\\caption{\\textbf{Multi-hop graphs enhance GWM's performance across representative tasks in six domains}. We can observe that the introduction of graphs has benefited GWM-E across all tasks compared to no graph. Moreover, excessive hops can lead to over-smoothing, thereby decreasing performance.} %We can observe that the introduction of graphs has benefited GWM-E across all tasks compared to no graph. Moreover, excessive hops can lead to over-smoothing, thereby decreasing performance.\n\\label{fig:performance_hops}\n\\vspace{-3mm}\n\\end{figure}", 'caption': "\\caption{\\textbf{Multi-hop graphs enhance GWM's performance across representative tasks in six domains}. We can observe that the introduction of graphs has benefited GWM-E across all tasks compared to no graph. Moreover, excessive hops can lead to over-smoothing, thereby decreasing performance.}", 'label': '\\label{fig:performance_hops}', 'subfigures': [], 'figure_paths': ['figs/hops_radar.pdf']}
parsed figure info with consideration of subfigures: {'original': "\\begin{figure}[ht]\n\\centering\n\\includegraphics[width=\\linewidth]{figs/hops_radar.pdf} \n\\vspace{-4mm}\n\\caption{\\textbf{Multi-hop graphs enhance GWM's performance across representative tasks in six domains}. We can observe that the introduction of graphs has benefited GWM-E across all tasks compared to no graph. Moreover, excessive hops can lead to over-smoothing, thereby decreasing performance.} %We can observe that the introduction of graphs has benefited GWM-E across all tasks compared to no graph. Moreover, excessive hops can lead to over-smoothing, thereby decreasing performance.\n\\label{fig:performance_hops}\n\\vspace{-3mm}\n\\end{figure}", 'caption': "\\caption{\\textbf{Multi-hop graphs enhance GWM's performance across representative tasks in six domains}. We can observe that the introduction of graphs has benefited GWM-E across all tasks compared to no graph. Moreover, excessive hops can lead to over-smoothing, thereby decreasing performance.}", 'label': '\\label{fig:performance_hops}', 'subfigures': [], 'figure_paths': ['figs/hops_radar.pdf', 'figs/hops_radar.pdf']}
res before parsing: {'original': "\\begin{figure}[t]\n\\centering\n\\includegraphics[width=\\linewidth]{figs/zero_few_shot.pdf} \n\\caption{\\textbf{GWM boosts zero-shot/few-shot performance on multi-agent collaboration (Agent) and retrieval-augmented generation (RAG) tasks}. Note that ``-T'' and ``-E'' respectively represent the experimental results of GWM-T and GWM-E. It can be observed that  GWM can quickly adapt to new tasks with a small amount of domain-specific training data. Moreover, GWM's strong generalization ability can boost the performance of Agent and RAG.} %It can be observed that  GWM can quickly adapt to new tasks with a small amount of domain-specific training data. Moreover, GWM's strong generalization ability can boost the performance of Agent and RAG.\n\\label{fig:zero_few_shot}\n\\end{figure}", 'caption': '', 'label': None, 'subfigures': [], 'figure_paths': []}
node.macroname: includegraphics
node.latex_verbatim(): \includegraphics[width=\linewidth]{figs/zero_few_shot.pdf}
parsed figure info: {'original': "\\begin{figure}[t]\n\\centering\n\\includegraphics[width=\\linewidth]{figs/zero_few_shot.pdf} \n\\caption{\\textbf{GWM boosts zero-shot/few-shot performance on multi-agent collaboration (Agent) and retrieval-augmented generation (RAG) tasks}. Note that ``-T'' and ``-E'' respectively represent the experimental results of GWM-T and GWM-E. It can be observed that  GWM can quickly adapt to new tasks with a small amount of domain-specific training data. Moreover, GWM's strong generalization ability can boost the performance of Agent and RAG.} %It can be observed that  GWM can quickly adapt to new tasks with a small amount of domain-specific training data. Moreover, GWM's strong generalization ability can boost the performance of Agent and RAG.\n\\label{fig:zero_few_shot}\n\\end{figure}", 'caption': "\\caption{\\textbf{GWM boosts zero-shot/few-shot performance on multi-agent collaboration (Agent) and retrieval-augmented generation (RAG) tasks}. Note that ``-T'' and ``-E'' respectively represent the experimental results of GWM-T and GWM-E. It can be observed that  GWM can quickly adapt to new tasks with a small amount of domain-specific training data. Moreover, GWM's strong generalization ability can boost the performance of Agent and RAG.}", 'label': '\\label{fig:zero_few_shot}', 'subfigures': [], 'figure_paths': ['figs/zero_few_shot.pdf']}
parsed figure info with consideration of subfigures: {'original': "\\begin{figure}[t]\n\\centering\n\\includegraphics[width=\\linewidth]{figs/zero_few_shot.pdf} \n\\caption{\\textbf{GWM boosts zero-shot/few-shot performance on multi-agent collaboration (Agent) and retrieval-augmented generation (RAG) tasks}. Note that ``-T'' and ``-E'' respectively represent the experimental results of GWM-T and GWM-E. It can be observed that  GWM can quickly adapt to new tasks with a small amount of domain-specific training data. Moreover, GWM's strong generalization ability can boost the performance of Agent and RAG.} %It can be observed that  GWM can quickly adapt to new tasks with a small amount of domain-specific training data. Moreover, GWM's strong generalization ability can boost the performance of Agent and RAG.\n\\label{fig:zero_few_shot}\n\\end{figure}", 'caption': "\\caption{\\textbf{GWM boosts zero-shot/few-shot performance on multi-agent collaboration (Agent) and retrieval-augmented generation (RAG) tasks}. Note that ``-T'' and ``-E'' respectively represent the experimental results of GWM-T and GWM-E. It can be observed that  GWM can quickly adapt to new tasks with a small amount of domain-specific training data. Moreover, GWM's strong generalization ability can boost the performance of Agent and RAG.}", 'label': '\\label{fig:zero_few_shot}', 'subfigures': [], 'figure_paths': ['figs/zero_few_shot.pdf', 'figs/zero_few_shot.pdf']}
numbmer of citations in node info method: 0
bib_path: download/working_folder/13016903680/paper.bib
key2id: {'su2024tablegpt2': '2411.02059', 'jin2024instructg2i': '2410.07157', 'ni2023content': '2309.15379', 'zhuge2024language': '2402.16823', 'liu2023dynamic': '2310.02170', 'wu2024agentkit': '2404.11483', 'gao2023retrieval': '2312.10997', 'zhao2024retrieval': '2402.19473', 'edge2024local': '2404.16130', 'peng2024graph': '2408.08921', 'siebenborn2022crucial': '2211.14655', 'jiang2018graph': '1810.09202', 'zhang2023instruction': '2308.10792', 'peng2023instruction': '2304.03277', 'li2021prefix': '2101.00190', 'oquab2023dinov2': '2304.07193', 'schmidgall2024agentclinic': '2405.07960', 'madotto2021few': '2110.08118', 'bai2024longbench': '2412.15204', 'lin2023train': '2302.07452', 'zhang2019bertscore': '1904.09675', 'shridhar2020alfworld': '2010.03768', 'ha2018world': '1803.10122', 'wu2024ivideogpt': '2405.15223', 'zhu2022value': '2206.04384', 'liu2023one': '2310.00149', 'chen2024llaga': '2402.08170', 'kipf2016semi': '1609.02907', 'velivckovic2017graph': '1710.10903', 'ding2024hybrid': '2404.14618', 'chen2023frugalgpt': '2305.05176', 'dai2024cost': '2405.16587', 'he2020deberta': '2006.03654', 'liu2019roberta': '1907.11692', 'zhu2023optimal': '2306.02003', 'stripelis2024polyrouter': '2408.12320', 'madaan2023automix': '2310.12963', 'cobbe2021training': '2110.14168', 'rajpurkar2016squad': '1606.05250', 'fabbri2019multi': '1906.01749', 'liu2023tcra': '2310.15556', 'snell2024scaling': '2408.03314', 'chen2024role': '2409.06857', 'ahmed2024studying': '2402.15100', 'zhang2024pybench': '2407.16732', 'wang2022self': '2212.10560', 'devlin2018bert': '1810.04805', 'cao2023relational': '2303.07666', 'fey2023relational': '2312.04615', 'dwivedi2020benchmarking': '2003.00982', 'kirillov2023segany': '2304.02643', 'valmeekam2022llmreason': '2206.10498', 'wang2023any': '2305.12081', 'velivckovic2017gat': '1710.10903', 'gin': '1810.00826', 'hgcn': '1906.04580', 'motl2015ctu': '1511.03086', 'fey2019fast': '1903.02428', 'kingma2014adam': '1412.6980', 'touvron2023llama': '2302.13971', 'jiang2024mixtral': '2401.04088', 'singhal2022large': '2212.13138', 'ahn2024large': '2402.00157', 'imani2023mathprompter': '2303.05398', 'schlichtkrull2017modeling': '1703.06103', 'chen2021evaluating': '2107.03374', 'yang2018hotpotqa': '1809.09600', 'hu2021lora': '2106.09685', 'hafner2023mastering': '2301.04104', 'beltagy2020longformer': '2004.05150'}
Thread 13016903680 Finished processing 2507.10539v1 (2/1001) Time elapsed: 2.17s
fromisoformat: argument must be str
Thread 13016903680 Failed to process 2507.10539v1
Thread 8642191936 Finished processing 2 papers
File path: download/output/2507.10539v1.json
Time of loading metadata: 0.0007403750000065656
Time of constructing paper json file: 0.0058197500000005675
Time of finding authors and adding authors to database: 8.329999943157418e-07
Time of adding figures, tables, and sections to database: 0.05806562499999757
Now processing citations
Time of searching arxiv id of cited paper (if not provided) and adding citation information to database: 0.031069833000003655
