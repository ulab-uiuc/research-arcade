arXiv:2506.20807v1  [cs.LG]  25 Jun 2025GPU Kernel Scientist: An LLM-Driven Framework for Iterative Kernel
Optimization
Martin Andrews1Sam Witteveen1
Abstract
Optimizing GPU kernels for high performance
is a complex task, often demanding deep archi-
tectural knowledge, extensive profiling, and itera-
tive experimentation. This challenge is amplified
when targeting newer or less-documented GPU ar-
chitectures where traditional development aids are
scarce. This paper introduces an LLM-powered
“GPU Kernel Scientist,” an automated methodol-
ogy for iteratively refining accelerator kernels.
Our methodology employs LLMs in a multi-stage,
evolutionary process: (a) strategically selecting
promising prior code versions as a basis for new
iterations; (b) generating hypotheses for optimiza-
tion experiments, based on existing code and as-
similated knowledge from general GPU literature;
and (c) autonomously implementing these experi-
ments through code modification and subsequent
submission to an external evaluation system, us-
ing only observed timing data as performance
feedback. We detail how this approach navigates
the challenges of the AMD MI300 target archi-
tecture and leverages LLMs to compensate for
limited domain-specific human expertise.
Since quantitative results from an ongoing per-
formance competition were embargoed on pa-
per submission date, we present the architectural
design, operational workflow, and qualitative in-
sights, highlighting the potential of LLM-driven
agents to democratise and accelerate GPU kernel
optimization, especially in resource-constrained
or rapidly evolving hardware environments.
1Singapore. Correspondence to: Martin Andrews <kernelsci-
entist@mdda.net >.
ES-FoMo : Third Workshop on Efficient Systems for Foundation
Models @ ICML 2025 , Vancouver, Canada. Copyright 2025 by
the author(s).
FailuresKernel Writer
Documentation:
* Original Problem Statement
* PyT orch model code
* MVP HIP code (inefficient)
Additional Data:
* 'findings' - about HIP
* Pseudocode for efficient kernelBenchmarks and Ancestry Evolutionary Selector Experiment Designer
Benchmarking Platform Most
InnovativeHighest 
PotentialMost
Certain3 experiments to run:
Code, with 1-step history:
* Reference Code
* Base Code
* Performance uplift in last step
Instructions
* Use the 'Experiment' provided
* Create 'diff' of base code
Outputs HIP codeFor all the kernels:
* Show the kernel ID
* With their parent's ID
* And an array of benchmarksDocumentation:
* Mission statement
* Listing of IDs and Benchmarks
Return the following:
* Suitable Base code ID
* Relevant Ref code ID
* Explanation of approachDocumentation:
* Mission statement
* ? Related Blog Post 
* Base HIP code
Create the following:
* 10 'avenues' of research
* 5 experiment designs
* Rate each design
Compilation:
* Does the code compile?
* If not, return to Kernel Writer
T esting:
* Correct results from code?
* If not, return to Kernel Writer
Benchmarking:
* Accurate timing (18 seeds)
* No profiling data available
2 code samples 
: Base & RefPopulation 
DetailsGemini 
FlashGemini 
Flash
Gemini 
ProAdd results 
to populationFigure 1. GPU Kernel Scientist Process
1. Introduction
GPU kernel optimization is a significant challenge and tra-
ditionally requires specialist expertise. This challenge be-
comes magnified when tackling new/niche hardware with
limited documentation, doubly so with weak profiling tools.
Our proposed solution to this problem is a “GPU Kernel
Scientist” – an automated, iterative framework that can opti-
mise kernels for non-CUDA hardware, with access only to
end-to-end timing results.
The core idea is to use frontier LLMs to create a cycle
of code selection, experiment ideation, and code genera-
tion/modification based only on limited feedback from an
online testing/benchmark platform.
The key contributions of this brief paper are:
• Presenting of the novel framework itself;
•Demonstrating its application to HIP kernel optimization
under severe information/tooling constraints; and
•Highlighting how LLMs can bridge knowledge gaps and
drive optimization in such scenarios.
We will first discuss related work, then detail our methodol-
ogy, and early experimental findings. Examples and code
are available in the Appendix and Supplementary materials.
1
<pikepdf.Dictionary({
  "/ColorSpace": {
    "/pgfprgb": [ "/Pattern", "/DeviceRGB" ]
  },
  "/ExtGState": {

  },
  "/Font": {
    "/F113": {
      "/BaseFont": "/OOEQLW+NimbusRomNo9L-Medi",
      "/Encoding": {
        "/Differences": [ 2, "/fi", "/fl", 31, "/quotesingle", 34, "/quotedbl", "/numbersign", 38, "/ampersand", "/quoteright", "/parenleft", "/parenright", "/asterisk", "/plus", "/comma", "/hyphen", "/period", "/slash", "/zero", "/one", "/two", "/three", "/four", "/five", "/six", "/seven", "/eight", "/nine", <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...>, <...> ],
        "/Type": <...>
      },
      "/FirstChar": <...>,
      "/FontDescriptor": <...>,
      "/LastChar": <...>,
      "/Subtype": <...>,
      "/ToUnicode": <...>,
      "/Type": <...>,
      "/Widths": <...>
    },
    "/F119": <...>,
    "/F148": <...>,
    "/F153": <...>,
    "/F64": <...>,
    "/Times-Roman": <...>
  },
  "/Pattern": {

  },
  "/ProcSet": [ <...>, <...> ],
  "/XObject": {
    "/Im1": <...>
  }
})>
Wrapped PDF written to figure_im1_wrapped.pdf
