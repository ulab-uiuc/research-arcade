SEMANTIC_SCHOLAR_API_KEY not set in .env
SEMANTIC_SCHOLAR_API_KEY not set in .env
arxiv ids: {'2412.17767v2'}
seed: ['2412.17767v2']
cnt: 1
BFS_que.qsize(): 1
current paper: 2412.17767v2
Thread 12933263360 Processing 2412.17767v2
Thread 8600085056 Finished processing 1 papers
File path: download/output/2412.17767v2.json
Time of loading metadata: 9.962500000071373e-05
Time of constructing paper json file: 0.005630208999999553
Time of finding authors and adding authors to database: 2.919999992911926e-07
Time of adding figures, tables, and sections to database: 0.05032399999999981
Time of searching arxiv id of cited paper (if not provided) and adding citation information to database: 0.007171334000000584
['download/output/2412.17767v2.json']
2412.17767v2
text_node
{'id': 'text_39', 'cites': ['AI4Science2023TheIO'], 'refs': [], 'content': 'LLMs have proved to be powerful copilots in scientific research~\\citep{AI4Science2023TheIO}, demonstrating their great potential for accelerating scientific discovery.\nDespite the promising finding, a more ambitious question remains: \\textit{Can we simulate the human research community with LLMs}? Answering such a question has multiple benefits: (1) simulating the human research community helps understand the underlying process behind the discovery of existing research ideas; (2) it can further help democratize and accelerate the discovery process of new research ideas.', 'type': 'textNode', 'isolation': False}
No paper found for  AI4Science2023TheIO
text_node
{'id': 'text_40', 'cites': ['zhou2023sotopia', 'Gao2023S3SS', 'hua2023war', 'xu2023language', 'girotra2023ideas', 'baek2024researchagent', 'huang2024mlagentbench', 'lu2024ai'], 'refs': [], 'content': 'However, simulating the human research community is challenging, as it involves leveraging multiple LLM agents to interact with complex research data. While existing multi-agent LLM frameworks have been successfully applied to areas like social simulation~\\citep{zhou2023sotopia,Gao2023S3SS} and game simulation~\\citep{hua2023war,xu2023language}, they are not well-suited for simulating research communities due to the complexity of collaborative research activities like paper writing and review writing. Although recent efforts have explored research automation using LLMs, these frameworks are typically limited to specific research tasks, such as idea generation~\\citep{girotra2023ideas, baek2024researchagent} or code experimentation~\\citep{huang2024mlagentbench}, or focus on simulating single-agent workflows~\\citep{lu2024ai}. These frameworks cannot simulate collaborative research activities where researchers with diverse backgrounds work together to brainstorm ideas, review papers, etc—processes that are fundamental to modern human research.', 'type': 'textNode', 'isolation': False}
No paper found for  zhou2023sotopia
No paper found for  Gao2023S3SS
No paper found for  hua2023war
No paper found for  xu2023language
No paper found for  girotra2023ideas
No paper found for  baek2024researchagent
No paper found for  huang2024mlagentbench
No paper found for  lu2024ai
text_node
{'id': 'text_41', 'cites': ['newman2001structure', 'Tang2008ArnetMinerEA', 'holm2020longitudinal', 'West2016ARS', 'Yang2012DefiningAE'], 'refs': [], 'content': '\\xhdr{Research community as graph}Research community as graph\nOur key observation is that the deeply interconnected research community can be naturally represented as graphs. Indeed, similar graph structures like citation networks~\\citep{newman2001structure} and academic social networks~\\citep{Tang2008ArnetMinerEA} have been extensively studied within data mining research, with proven values in applications such as citation prediction~\\citep{holm2020longitudinal}, recommendation~\\citep{West2016ARS}, and community detection~\\citep{Yang2012DefiningAE}.\nHowever, introducing LLMs to a graph-structured research community can extend these previous works from prediction and analysis with existing data to dynamic simulation and real-time forecasting.', 'type': 'textNode', 'isolation': False}
No paper found for  newman2001structure
No paper found for  Tang2008ArnetMinerEA
No paper found for  holm2020longitudinal
No paper found for  West2016ARS
No paper found for  Yang2012DefiningAE
text_node
{'id': 'text_42', 'cites': ['wei2023larger', 'lee2024reasoning'], 'refs': ['figures_2'], 'content': '\\xhdr{Novel framework for research simulation}Novel framework for research simulation\nIn this work, we propose \\envname, a simulator of the human research community. To bridge the gap between existing multi-agent simulation frameworks and the complexity of research activities, we propose a graph-based framework, inspired by the message-passing mechanism in Graph Neural Networks (GNNs), for multi-agent simulation.\nConcretely, as shown in Figure \\ref{fig:community-graph}, we propose a new concept of \\textit{agent-data graph} with 2 generic types of nodes: (1) \\textit{agent} nodes, suitable for entities like agents; (2) \\textit{data} nodes, suitable for entities such as papers, reviews, and blogs. \nAgent-data graphs are unique from standard heterogeneous graphs; here, the key conceptual difference between agent and data nodes is that an agent node can be considered a function over data nodes.\nTo inference on agent-data graphs, we propose a \\textit{TextGNN} framework where message-passing processes are defined based on text-form information processing with LLMs, thanks to their strong in-context learning~\\citep{wei2023larger} and reasoning~\\citep{lee2024reasoning} ability. \nWe apply the proposed agent-data graph and TextGNN to the research simulation. Here, a research community can be regarded as a special form of agent-data graph, called \\textit{community graph}, with research agents and research papers as two types of nodes, and we consider three types of edges (review, author, and cite) in the graph. Different community activities, such as paper writing and review writing, can be modeled as special message-passing processes on the community graph.', 'type': 'textNode', 'isolation': False}
No paper found for  wei2023larger
No paper found for  lee2024reasoning
text_node
{'id': 'text_43', 'cites': ['si2024can', 'hu2024nova'], 'refs': [], 'content': '\\xhdr{Novel evaluation for research simulation}Novel evaluation for research simulation \nWith \\envname for research simulation, a further research question is to evaluate the quality of that. Prior works primarily use human evaluation with breakdown metrics such as novelty, excitement, feasibility, and expected effectiveness~\\citep{si2024can,hu2024nova}. These approaches inevitably suffer from subjectiveness and high costs. In our work, since \\envname functions as a simulator, our primary focus is on measuring how closely its outputs align with those of the real-world research community. Community graphs naturally provide a similarity-based evaluation method by masking a given paper node in the community graph and evaluating whether a simulator can reconstruct the masked nodes. This definition focuses on simulation similarity, making it scalable and objective. Based on such a node masking prediction task, we build a benchmark called \\benchname with 1,000 paper writing tasks and 200 review writing tasks requiring multi-agent collaboration.', 'type': 'textNode', 'isolation': False}
No paper found for  si2024can
No paper found for  hu2024nova
text_node
{'id': 'text_44', 'cites': [], 'refs': [], 'content': '\\xhdr{Main discoveries}Main discoveries Based on the evaluation results from \\benchname, we highlight three key findings: (1) \\envname effectively simulates collaborative research activities, achieving an average similarity score of 0.68 for paper writing and 0.49 for review writing, as measured by the state-of-the-art text embedding model; (2) \\envname demonstrates robustness and effectiveness in research simulation, showing improvement when more agents are added and maintaining performance when including unrelated papers; (3) \\envname inspires interdisciplinary research, generating innovative ideas that combine insights from NLP, criminology, and astronomy and does not exist in the real-world research.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_45', 'cites': [], 'refs': [], 'content': '\\xhdr{Stressing ethical concerns}Stressing ethical concerns As our work targets simulating the human research community, multiple ethical concerns, including facilitating research plagiarism and producing low-quality or misleading claims, appear. These ethical concerns are addressed in detail in Appendix~\\S\\ref{appendix:ethical}.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_46', 'cites': [], 'refs': [], 'content': '\\vspace{-3mm}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_47', 'cites': ['yang2021graphformers', 'he2023explanations', 'Zhao2022LearningOL', 'Chen2023LabelfreeNC', 'yan2023comprehensive'], 'refs': [], 'content': '\\xhdr{Graphs with text attributes}Graphs with text attributes In real-world graph tasks, nodes often have textual attributes to carry richer information, forming text-attributed graphs (TAGs)~\\citep{yang2021graphformers, he2023explanations}. \nPrevious work on TAGs mainly treats LLMs as tools for understanding text attributes and aims at achieving co-training LLMs and GNNs~\\citep{Zhao2022LearningOL,Chen2023LabelfreeNC}. In contrast, our approach incorporates agent nodes into the graph, enabling text-based message passing between agent nodes and data nodes. Furthermore, while previous TAG research mainly focuses on node prediction and link prediction tasks~\\citep{yan2023comprehensive}, \\envname extends it to both the reconstruction of existing nodes and the prediction of new, non-existent nodes.', 'type': 'textNode', 'isolation': False}
No paper found for  yang2021graphformers
No paper found for  he2023explanations
No paper found for  Zhao2022LearningOL
No paper found for  Chen2023LabelfreeNC
No paper found for  yan2023comprehensive
text_node
{'id': 'text_48', 'cites': ['zhuge2024language', 'martinkus2022agent', 'hu2024learning'], 'refs': [], 'content': '\\xhdr{Graphs for multi-agent modeling}Graphs for multi-agent modeling Recent works model multi-agent communication using graphs and develop learnable methods to optimize the communication process~\\citep{zhuge2024language, martinkus2022agent, hu2024learning}. However, these works often neglect the interactive nature of data, where agents can read, write, and update shared data iteratively. Currently, few works include a well-defined framework to represent graphs that integrate both agents and their associated data.', 'type': 'textNode', 'isolation': False}
No paper found for  zhuge2024language
No paper found for  martinkus2022agent
No paper found for  hu2024learning
text_node
{'id': 'text_49', 'cites': [], 'refs': [], 'content': '\\vspace{-2mm}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_50', 'cites': [], 'refs': [], 'content': '\\label{sec:community-graph-design}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_51', 'cites': [], 'refs': [], 'content': '\\xhdr{Definition of agent-data graphs}Definition of agent-data graphs\nTo initiate our discussion, we formally define the proposed agent-data graph. An agent-data graph is a special type of heterogeneous graph $ \\mathcal{G} = (\\mathcal{V}, \\mathcal{E}) $ \\mathcal{G} = (\\mathcal{V}, \\mathcal{E}) , where $ \\mathcal{V} = \\mathcal{V}_a \\cup \\mathcal{V}_d $ \\mathcal{V} = \\mathcal{V}_a \\cup \\mathcal{V}_d  is the node set consisting of two types of nodes, agent nodes and data nodes, and $\\mathcal{E} = \\mathcal{E}_{aa} \\cup \\mathcal{E}_{ad} \\cup \\mathcal{E}_{dd}$\\mathcal{E} = \\mathcal{E}_{aa}aa \\cup \\mathcal{E}_{ad}ad \\cup \\mathcal{E}_{dd}dd is the edge set consisting of three types of relations, agent-agent, data-data, and agent-data interactions.\nHere, each data node $v \\in \\mathcal{V}_d$v \\in \\mathcal{V}_d comes with attributes, \\eg, a piece of text, $\\mathbf{x}_v$\\mathbf{x}_v; each agent node $u$u is accompanied with an \\textit{agent function}, \\eg, an LLM $f_u(\\cdot)$f_u(\\cdot) with its prompt template and the profile. Each agent function is responsible for two types of tasks: message generation and message aggregation. More details about agent functions are in Appendix~\\S\\ref{agent-function-implementation}. Without loss of generality, we assume that the data nodes have text attributes, and leave the multi-modal extension of our work, \\eg, images, audio, and videos, to future works.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_52', 'cites': [], 'refs': [], 'content': '\\xhdr{Uniqueness of agent-data graphs}Uniqueness of agent-data graphs\nUnlike standard heterogeneous graphs, the uniqueness of an agent-data graph is that the agent nodes take functions as their attributes, rather than embeddings. Concretely, each agent node could take a piece of text, \\eg, $\\mathbf{x}_v$\\mathbf{x}_v from one data node, as the input and output new data based on its profile prompt $\\mathbf{x}_u$\\mathbf{x}_u, \\eg, $\\mathbf{x}_{uv} = f_u([\\mathbf{x}_u, \\mathbf{x}_v])$\\mathbf{x}_{uv}uv = f_u([\\mathbf{x}_u, \\mathbf{x}_v]) where $[\\cdot]$[\\cdot] indicates filling the prompt template with $\\mathbf{x}_u$\\mathbf{x}_u and $\\mathbf{x}_v$\\mathbf{x}_v. Such definition greatly facilitates the multi-agent scenarios where agents could communicate among themselves, with edge type $\\mathcal{E}_{aa}$\\mathcal{E}_{aa}aa; interacting with the environment, with edge type $\\mathcal{E}_{ad}$\\mathcal{E}_{ad}ad; representing the inherent data relationships within an environment $\\mathcal{E}_{dd}$\\mathcal{E}_{dd}dd.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_53', 'cites': [], 'refs': ['figures_2'], 'content': '\\xhdr{Example of agent-data graphs}Example of agent-data graphs Figure~\\ref{fig:community-graph} shows an example of the agent-data graph. Its definition could be extended to more node types (\\eg, codebase, blogs) and edge types (\\eg, attend, post, commit). Typically, one blog post can be directly connected to multiple researchers, papers, and other blog posts if they are related to each other.', 'type': 'textNode', 'isolation': False}
text_node
{'id': 'text_54', 'cites': [], 'refs': [], 'content': '\\label{sec:text-gnn}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_55', 'cites': [], 'refs': [], 'content': '\\xhdr{TextGNN motivations}TextGNN motivations\nThe agent-data graph $\\mathcal{G}$\\mathcal{G} provides a platform for expressing a complex multi-agent scenario, \\eg, a human research community.\nTo further simulate based on a given real-world agent-data graph, we need agentic models, \\eg, LLMs, to generate new data and interactions on the agent-data graph.\nTo this end, motivated by the message-passing algorithm in GNNs, we proposed a text-based message-passing mechanism on an agent-data graph, called \\textit{TextGNN}, where all hidden states are defined in the text space instead of the embedding space.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_56', 'cites': [], 'refs': [], 'content': '\\xhdr{Recap: message passing in standard GNN}Recap: message passing in standard GNN \nIn standard GNNs, input features $\\mb{x}_v$\\mb{x}x_v are used to initialize the initial states $\\mb{x}_v = \\mb{h}_v^{(0)}$\\mb{x}x_v = \\mb{h}h_v^{(0)}(0). Afterward, the goal is to learn useful node embeddings \\( \\mb{h}_v \\) \\mb{h}h_v  by iteratively aggregating information from local neighborhoods. Hidden states, message functions, and aggregation functions are the three main components in one GNN layer. The \\( k \\) k -th iteration of message passing (or the \\( k \\) k -th GNN layer) is typically defined as:', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_57', 'cites': [], 'refs': [], 'content': '\\begingroup\n\\small\n\\begin{equation}\n    \\label{eq:gnn}\n    \\mb{m}_u^{(k)} = \\textsc{MSG}^{(k)}(\\mb{h}_u^{(k-1)})\n\\end{equation}\\begin{equation}\n    \\label{eq:gnn}\n    \\mb{m}_u^{(k)} = \\textsc{MSG}^{(k)}(\\mb{h}_u^{(k-1)})\n\\end{equation}\n    \\label{eq:gnn}\n    \\mb{m}m_u^{(k)}(k) = \\textsc{MSG}^{(k)}(k)(\\mb{h}h_u^{(k-1)}(k-1))', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_58', 'cites': [], 'refs': [], 'content': '\\endgroup\n\\begingroup\n\\small\n\\begin{equation}\n    \\label{eq:gnn2}\n    \\mb{h}_v^{(k)} = \\textsc{AGG}^{(k)}\\big(\\mb{h}_v^{(k-1)}, \\{\\mb{m}_u^{(k)} \\mid u \\in \\mathcal{N}(v)\\}\\big)\n\\end{equation}\\begin{equation}\n    \\label{eq:gnn2}\n    \\mb{h}_v^{(k)} = \\textsc{AGG}^{(k)}\\big(\\mb{h}_v^{(k-1)}, \\{\\mb{m}_u^{(k)} \\mid u \\in \\mathcal{N}(v)\\}\\big)\n\\end{equation}\n    \\label{eq:gnn2}\n    \\mb{h}h_v^{(k)}(k) = \\textsc{AGG}^{(k)}(k)\\big(\\mb{h}h_v^{(k-1)}(k-1), \\{\\mb{m}m_u^{(k)}(k) \\mid u \\in \\mathcal{N}(v)\\}\\big)', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_59', 'cites': [], 'refs': [], 'content': '\\endgroup\nwhere \\( \\mb{h}_v^{(k)} \\) \\mb{h}h_v^{(k)}(k)  is the node embedding at the \\( k \\) k -th layer, \\( \\mb{h}_v^{(0)} = \\mb{x}_v \\) \\mb{h}h_v^{(0)}(0) = \\mb{x}x_v  is the initial node feature, and \\( \\mathcal{N}(v) \\) \\mathcal{N}(v)  is the set of neighbors of node \\( v \\) v . \\(\\textsc{MSG}^{(k)}(\\cdot)\\)\\textsc{MSG}^{(k)}(k)(\\cdot) is a transformative function to convert the hidden states of one node into a message for aggregation. \\(\\textsc{AGG}^{(k)}(\\cdot)\\)\\textsc{AGG}^{(k)}(k)(\\cdot) is defined to update the hidden states of a node based on neighborhood messages. More generally, we can broadly consider the $k$k-th layer of GNN to be an aggregation function that implicitly includes message functions inside:', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_60', 'cites': [], 'refs': [], 'content': '\\begingroup\n\\small\n\\begin{equation}\n\\mb{h}_v^{(k)} = \\textsc{AGG}^{(k)}\\big(\\mb{h}_v^{(k-1)}, \\{\\mb{h}_u^{(k-1)} \\mid u \\in \\mathcal{N}(v)\\}\\big)\n\\end{equation}\\begin{equation}\n\\mb{h}_v^{(k)} = \\textsc{AGG}^{(k)}\\big(\\mb{h}_v^{(k-1)}, \\{\\mb{h}_u^{(k-1)} \\mid u \\in \\mathcal{N}(v)\\}\\big)\n\\end{equation}\n\\mb{h}h_v^{(k)}(k) = \\textsc{AGG}^{(k)}(k)\\big(\\mb{h}h_v^{(k-1)}(k-1), \\{\\mb{h}h_u^{(k-1)}(k-1) \\mid u \\in \\mathcal{N}(v)\\}\\big)', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_61', 'cites': [], 'refs': [], 'content': '\\endgroup', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_62', 'cites': [], 'refs': [], 'content': '\\xhdr{Message passing in TextGNN}Message passing in TextGNN Following the message-passing process in the standard GNN, we now define a general form of the aggregation function to describe the text-based message-passing process on an agent-data graph $\\mathcal{G}$\\mathcal{G}. The key difference between a standard GNN and a TextGNN is that all hidden states in the standard GNN are defined in the embedding space ($\\mb{h}_v \\in \\mathbb{R}^d$\\mb{h}h_v \\in \\mathbb{R}^d) while those in TextGNN are defined in the text space ($\\mb{h}_v \\in \\Sigma^{*}$\\mb{h}h_v \\in \\Sigma^{*}*). In a TextGNN, we first set the initial hidden states for data nodes $\\mb{h}_v^{(0)} = \\mb{x}_v$\\mb{h}h_v^{(0)}(0) = \\mb{x}x_v, where $\\mb{x}_v$\\mb{x}x_v are text attributes, and the initial hidden states for agent nodes is empty $\\mb{h}_u^{(0)} = \\emptyset$\\mb{h}h_u^{(0)}(0) = \\emptyset. Next, we design a general form of message passing function that handles three distinctive types of interaction, including agent-agent $\\mathcal{E}_{aa}$\\mathcal{E}_{aa}aa, agent-data $\\mathcal{E}_{ad}$\\mathcal{E}_{ad}ad, and data-data $\\mathcal{E}_{dd}$\\mathcal{E}_{dd}dd.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_63', 'cites': [], 'refs': [], 'content': 'Specifically, the $k$k-th TextGNN layer for an agent node $u\\in \\mathcal{V}_a$u\\in \\mathcal{V}_a can be written as:\n\\begingroup\n\\small\n\\begin{equation}\n\\begin{aligned}\n    \\mb{h}_{u}^{(k)} &= \\textsc{AGG}^{(k)}\\big(f_u(\\cdot), \\mb{h}_u^{(k-1)}, \\{\\mb{h}_d^{(k-1)} \\mid (u,d) \\in \\mathcal{E}_{ad}\\}, \\\\\n    &\\quad \\{f_a(\\cdot), \\mb{h}_a^{(k-1)} \\mid (u,a) \\in \\mathcal{E}_{aa}\\}\\big) \\\\\n    &= f_u\\Big(\\Big[\\mb{h}_u^{(k-1)}, \\big\\{f_a\\big(\\big[\\mb{h}_a^{(k-1)}, \\mb{h}_u^{(k-1)}, \\mb{h}_d^{(k-1)}\\big]\\big) \\mid \\\\\n    &\\quad (u,a) \\in \\mathcal{E}_{aa}, (u,d) \\in \\mathcal{E}_{ad}\\big\\}\\Big]\\Big)\n\\end{aligned}\n\\label{agg_agent}\n\\end{equation}\\begin{equation}\n\\begin{aligned}\n    \\mb{h}_{u}^{(k)} &= \\textsc{AGG}^{(k)}\\big(f_u(\\cdot), \\mb{h}_u^{(k-1)}, \\{\\mb{h}_d^{(k-1)} \\mid (u,d) \\in \\mathcal{E}_{ad}\\}, \\\\\n    &\\quad \\{f_a(\\cdot), \\mb{h}_a^{(k-1)} \\mid (u,a) \\in \\mathcal{E}_{aa}\\}\\big) \\\\\n    &= f_u\\Big(\\Big[\\mb{h}_u^{(k-1)}, \\big\\{f_a\\big(\\big[\\mb{h}_a^{(k-1)}, \\mb{h}_u^{(k-1)}, \\mb{h}_d^{(k-1)}\\big]\\big) \\mid \\\\\n    &\\quad (u,a) \\in \\mathcal{E}_{aa}, (u,d) \\in \\mathcal{E}_{ad}\\big\\}\\Big]\\Big)\n\\end{aligned}\n\\label{agg_agent}\n\\end{equation}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_64', 'cites': [], 'refs': [], 'content': '\\mb{h}h_{u}u^{(k)}(k) &= \\textsc{AGG}^{(k)}(k)\\big(f_u(\\cdot), \\mb{h}h_u^{(k-1)}(k-1), \\{\\mb{h}h_d^{(k-1)}(k-1) \\mid (u,d) \\in \\mathcal{E}_{ad}ad\\}, \\\\\n    &\\quad \\{f_a(\\cdot), \\mb{h}h_a^{(k-1)}(k-1) \\mid (u,a) \\in \\mathcal{E}_{aa}aa\\}\\big) \\\\\n    &= f_u\\Big(\\Big[\\mb{h}h_u^{(k-1)}(k-1), \\big\\{f_a\\big(\\big[\\mb{h}h_a^{(k-1)}(k-1), \\mb{h}h_u^{(k-1)}(k-1), \\mb{h}h_d^{(k-1)}(k-1)\\big]\\big) \\mid \\\\\n    &\\quad (u,a) \\in \\mathcal{E}_{aa}aa, (u,d) \\in \\mathcal{E}_{ad}ad\\big\\}\\Big]\\Big)', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_65', 'cites': [], 'refs': [], 'content': '\\label{agg_agent}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_66', 'cites': [], 'refs': [], 'content': '\\endgroup\nwhere $[\\cdot]$[\\cdot] is the concatenation function between texts to fill in the prompt template, $\\mb{h}_v^{(k)}$\\mb{h}h_v^{(k)}(k) represents the hidden states of the $k$k-th layer of $v\\in \\mathcal{V}$v\\in \\mathcal{V}, $f_a(\\cdot)$f_a(\\cdot) represents the agent function paired with the agent node in the neighborhood and $f_u(\\cdot)$f_u(\\cdot) represents the agent function paired with the agent node.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_67', 'cites': [], 'refs': [], 'content': 'Similarly, the forwarding process of the $k$k-th TextGNN layer for a data node $v\\in \\mathcal{V}_d$v\\in \\mathcal{V}_d can be written as:\n\\begingroup\n\\small\n\\begin{equation}\n\\begin{aligned}\n    \\mb{h}_{v}^{(k)} & = \\textsc{AGG}^{(k)}\\Big( \\mb{h}_v^{(k-1)}, \\{\\mb{h}_d^{(k-1)} \\mid (v,d) \\in \\mathcal{E}_{dd}\\}, \\\\\n    &\\quad \\{f_a(\\cdot), \\mb{h}_a^{(k-1)} \\mid (v,a) \\in \\mathcal{E}_{ad}\\}\\Big) \\\\\n    &= f_g\\Big(\\Big[\\mb{h}_v^{(k-1)}, \\big\\{f_a\\big(\\big[\\mb{h}_a^{(k-1)}, \\mb{h}_v^{(k-1)}, \\mb{h}_d^{(k-1)}\\big]\\big) \\mid \\\\\n    &\\quad (v,a) \\in \\mathcal{E}_{ad}, (v,d) \\in \\mathcal{E}_{dd}\\big\\}\\Big]\\Big)\n\\end{aligned}\n\\label{agg_data}\n\\end{equation}\\begin{equation}\n\\begin{aligned}\n    \\mb{h}_{v}^{(k)} & = \\textsc{AGG}^{(k)}\\Big( \\mb{h}_v^{(k-1)}, \\{\\mb{h}_d^{(k-1)} \\mid (v,d) \\in \\mathcal{E}_{dd}\\}, \\\\\n    &\\quad \\{f_a(\\cdot), \\mb{h}_a^{(k-1)} \\mid (v,a) \\in \\mathcal{E}_{ad}\\}\\Big) \\\\\n    &= f_g\\Big(\\Big[\\mb{h}_v^{(k-1)}, \\big\\{f_a\\big(\\big[\\mb{h}_a^{(k-1)}, \\mb{h}_v^{(k-1)}, \\mb{h}_d^{(k-1)}\\big]\\big) \\mid \\\\\n    &\\quad (v,a) \\in \\mathcal{E}_{ad}, (v,d) \\in \\mathcal{E}_{dd}\\big\\}\\Big]\\Big)\n\\end{aligned}\n\\label{agg_data}\n\\end{equation}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_68', 'cites': [], 'refs': [], 'content': '\\mb{h}h_{v}v^{(k)}(k) & = \\textsc{AGG}^{(k)}(k)\\Big( \\mb{h}h_v^{(k-1)}(k-1), \\{\\mb{h}h_d^{(k-1)}(k-1) \\mid (v,d) \\in \\mathcal{E}_{dd}dd\\}, \\\\\n    &\\quad \\{f_a(\\cdot), \\mb{h}h_a^{(k-1)}(k-1) \\mid (v,a) \\in \\mathcal{E}_{ad}ad\\}\\Big) \\\\\n    &= f_g\\Big(\\Big[\\mb{h}h_v^{(k-1)}(k-1), \\big\\{f_a\\big(\\big[\\mb{h}h_a^{(k-1)}(k-1), \\mb{h}h_v^{(k-1)}(k-1), \\mb{h}h_d^{(k-1)}(k-1)\\big]\\big) \\mid \\\\\n    &\\quad (v,a) \\in \\mathcal{E}_{ad}ad, (v,d) \\in \\mathcal{E}_{dd}dd\\big\\}\\Big]\\Big)', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_69', 'cites': [], 'refs': [], 'content': '\\label{agg_data}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_70', 'cites': [], 'refs': [], 'content': '\\endgroup\nwhere $f_g(\\cdot)$f_g(\\cdot) is defined as a global agent function without a specialized profile, and $f_a(\\cdot)$f_a(\\cdot) is the agent function paired with the agent node in the neighborhood.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_71', 'cites': [], 'refs': [], 'content': '\\label{sec:method}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_72', 'cites': [], 'refs': [], 'content': '\\xhdr{Inputs and outputs of \\envname}Inputs and outputs of \\envname Building on the definitions of TextGNN and the agent-data graph in Section~\\S\\ref{sec:community-graph-design} and Section~\\S\\ref{sec:text-gnn}, we simulate different research activities by modeling each as a specific instantiation of a TextGNN layer. \\envname processes diverse research materials and produces structured outputs. The input varies by task: only paper abstracts are used for paper reading and writing, while full papers are provided for review writing. The output format is also task-specific: paper reading generates profile descriptions, paper writing generates bullet-point summaries, and review writing produces bullet-point critiques along with a numerical review score. These standardized output formats—described in more detail in Appendix~\\S\\ref{evaluation-details}—facilitate evaluation over long-context inputs and enable fine-grained, sub-component similarity scoring.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_73', 'cites': [], 'refs': [], 'content': '\\xhdr{Hidden states of \\envname}Hidden states of \\envname In \\envname, the hidden state of each node represents a condensed version of research materials, such as papers or reviews. Initially, paper nodes are initialized with the full text of papers. Through iterative message passing, these nodes gradually evolve into a standardized bullet-point format, distilling key information for easier downstream evaluation. Similarly, review attributes associated with paper nodes are also represented using bullet points to make it in a compact form. Bullet-point compact form with limited length allows TextGNN to conduct message passing multiple times efficiently.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_74', 'cites': [], 'refs': ['figures_3'], 'content': '\\xhdr{Agent-data graph for research community modeling - community graph}Agent-data graph for research community modeling - community graph \nWe adopt the agent-data graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$\\mathcal{G} = (\\mathcal{V}, \\mathcal{E}) to research community simulation, which we named as \\textit{community graph}. As is shown in Figure \\ref{fig:community-activity}, each agent node $\\mathcal{V}_a$\\mathcal{V}_a represents one researcher, and each data node $\\mathcal{V}_d$\\mathcal{V}_d represents a paper. The edge set $ \\mathcal{E}_{dd}$ \\mathcal{E}_{dd}dd captures paper citations, the edge set $\\mathcal{E}_{ad}$\\mathcal{E}_{ad}ad captures authorship (a researcher writes a paper) and reviewing expertise (a researcher is qualified to review a paper). We omit the edge set $ \\mathcal{E}_{aa}$ \\mathcal{E}_{aa}aa to simplify the framework, as a collaboration between authors can typically be inferred through 2-hop paths via $\\mathcal{E}_{ad}$\\mathcal{E}_{ad}ad edges.', 'type': 'textNode', 'isolation': False}
text_node
{'id': 'text_75', 'cites': [], 'refs': ['figures_3'], 'content': '\\xhdr{TextGNN for research activity simulation}TextGNN for research activity simulation\nBased on the constructed community graph, we further identify the key types of research activities where TextGNN can be used for simulation.\nSpecifically, as shown in Figure~\\ref{fig:community-activity}, we split the research simulation process into three critical stages: (1) paper reading, (2) paper writing, and (3) review writing. We believe these stages are crucial in the research community, and each stage relies on the output of the previous stage as input. \nWe provide a detailed description for each stage and the corresponding TextGNN layer definition below.', 'type': 'textNode', 'isolation': False}
text_node
{'id': 'text_76', 'cites': [], 'refs': [], 'content': '\\hangindent=0em\n\\hangafter=0\n\\xhdr{$\\triangleright$ Stage 1: Paper reading}$\\triangleright$\\triangleright Stage 1: Paper reading Reading papers to collect insights is a necessary process for initializing a research project. In the community graph, the paper reading process can be described as \\textit{inserting a new agent node} to the community graph and aggregating its neighborhood information based on Equation \\ref{agg_agent}. Here, the new agent profile is non-existent before reading a collection of papers, and the profile is created after the paper reading process, making the TextGNN layer unique. Concretely, by adapting Equation \\ref{agg_agent}, the TextGNN layer for paper reading can be written as:', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_77', 'cites': [], 'refs': [], 'content': '\\vspace{-6mm}\n\\begingroup\n\\small\n\\begin{equation}\n\\begin{split}\n    \\mb{h}_{u} & = \\textsc{AGG}\\Big(f_u(\\cdot), \\{\\mb{h}_d \\mid (u,d) \\in \\mathcal{E}_{ad}\\}\\Big) \\\\\n    & = f_u\\Big(\\Big[\\left\\{\\mb{h}_d \\mid (u,d) \\in \\mathcal{E}_{ad}\\right\\}\\Big]\\Big)\n\\end{split}\n\\label{paper_reading}\n\\end{equation}\\begin{equation}\n\\begin{split}\n    \\mb{h}_{u} & = \\textsc{AGG}\\Big(f_u(\\cdot), \\{\\mb{h}_d \\mid (u,d) \\in \\mathcal{E}_{ad}\\}\\Big) \\\\\n    & = f_u\\Big(\\Big[\\left\\{\\mb{h}_d \\mid (u,d) \\in \\mathcal{E}_{ad}\\right\\}\\Big]\\Big)\n\\end{split}\n\\label{paper_reading}\n\\end{equation}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_78', 'cites': [], 'refs': [], 'content': '\\mb{h}h_{u}u & = \\textsc{AGG}\\Big(f_u(\\cdot), \\{\\mb{h}h_d \\mid (u,d) \\in \\mathcal{E}_{ad}ad\\}\\Big) \\\\\n    & = f_u\\Big(\\Big[\\left\\{\\mb{h}h_d \\mid (u,d) \\in \\mathcal{E}_{ad}ad\\right\\}\\Big]\\Big)', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_79', 'cites': [], 'refs': [], 'content': '\\label{paper_reading}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_80', 'cites': [], 'refs': ['figures_3'], 'content': "\\endgroup\nwhere $\\mb{h}_u, \\{f_a(\\cdot), \\mb{h}_a \\mid (u, a) \\in \\mathcal{E}_{aa}\\}$\\mb{h}h_u, \\{f_a(\\cdot), \\mb{h}h_a \\mid (u, a) \\in \\mathcal{E}_{aa}aa\\} in Equation \\ref{agg_agent} are empty since the agent node is initialized as empty and is not directly connected with any agents, and $\\mathcal{E}_{ad}$\\mathcal{E}_{ad}ad specifically refers to the authorship relation between agent and data nodes. Equation \\ref{agg_agent} degrades to an aggregation of papers based on the researcher agent without the profile, illustrated in Figure \\ref{fig:community-activity} ``Stage 1''.", 'type': 'textNode', 'isolation': False}
text_node
{'id': 'text_81', 'cites': [], 'refs': [], 'content': '\\hangindent=0em\n\\hangafter=0\n\\xhdr{$\\triangleright$ Stage 2: Paper writing}$\\triangleright$\\triangleright Stage 2: Paper writing After paper reading, the next important research stage is paper writing. Different from paper reading, the paper writing process can be understood as inserting \\textit{a new data node} into the community graph. Here, the new data node is non-existent before writing the paper, and the data node is created after the paper writing process. Concretely, by adapting Equation \\ref{agg_data}, the TextGNN layer for paper writing can be written as:', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_82', 'cites': [], 'refs': [], 'content': '\\vspace{-5mm}\n\\begingroup\n\\small\n\\begin{equation}\n\\begin{aligned}\n    \\mb{h}_{v} &= \\textsc{AGG}\\Big( \n        \\big\\{f_a(\\cdot), \\big\\{\\mb{h}_d \\mid (v,d) \\in \\mathcal{E}_{dd}\\big\\}, \\mb{h}_a \\mid (v,a) \\in \\mathcal{E}_{ad}\\big\\}\n    \\Big) \\\\\n    &= f_g\\Big(\n        \\Big[\n            \\big\\{f_a\\big([\\mb{h}_a, \\mb{h}_d]\\big) \\mid (v,a) \\in \\mathcal{E}_{ad}, \n            (v,d) \\in \\mathcal{E}_{dd}\\big\\}\n        \\Big]\n    \\Big)\n\\end{aligned}\n\\label{paper_writing}\n\\end{equation}\\begin{equation}\n\\begin{aligned}\n    \\mb{h}_{v} &= \\textsc{AGG}\\Big( \n        \\big\\{f_a(\\cdot), \\big\\{\\mb{h}_d \\mid (v,d) \\in \\mathcal{E}_{dd}\\big\\}, \\mb{h}_a \\mid (v,a) \\in \\mathcal{E}_{ad}\\big\\}\n    \\Big) \\\\\n    &= f_g\\Big(\n        \\Big[\n            \\big\\{f_a\\big([\\mb{h}_a, \\mb{h}_d]\\big) \\mid (v,a) \\in \\mathcal{E}_{ad}, \n            (v,d) \\in \\mathcal{E}_{dd}\\big\\}\n        \\Big]\n    \\Big)\n\\end{aligned}\n\\label{paper_writing}\n\\end{equation}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_83', 'cites': [], 'refs': [], 'content': '\\mb{h}h_{v}v &= \\textsc{AGG}\\Big( \n        \\big\\{f_a(\\cdot), \\big\\{\\mb{h}h_d \\mid (v,d) \\in \\mathcal{E}_{dd}dd\\big\\}, \\mb{h}h_a \\mid (v,a) \\in \\mathcal{E}_{ad}ad\\big\\}\n    \\Big) \\\\\n    &= f_g\\Big(\n        \\Big[\n            \\big\\{f_a\\big([\\mb{h}h_a, \\mb{h}h_d]\\big) \\mid (v,a) \\in \\mathcal{E}_{ad}ad, \n            (v,d) \\in \\mathcal{E}_{dd}dd\\big\\}\n        \\Big]\n    \\Big)', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_84', 'cites': [], 'refs': [], 'content': '\\label{paper_writing}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_85', 'cites': [], 'refs': ['figures_3'], 'content': "\\endgroup\nwhere $\\mb{h}_v$\\mb{h}h_v in Equation \\ref{agg_data} is empty since paper node contents are non-existent before paper writing; $\\mathcal{E}_{ad}$\\mathcal{E}_{ad}ad specifically refers to authorship relations between agent and data nodes, and $\\mathcal{E}_{dd}$\\mathcal{E}_{dd}dd refers to citation relations within data nodes. A visualization of Equation \\ref{paper_writing} is shown in Figure \\ref{fig:community-activity} ``Stage 2''.", 'type': 'textNode', 'isolation': False}
text_node
{'id': 'text_86', 'cites': [], 'refs': [], 'content': '\\hangindent=0em\n\\hangafter=0\n\\xhdr{$\\triangleright$ Stage 3: Review writing}$\\triangleright$\\triangleright Stage 3: Review writing The review writing task is the final stage of the automatic research simulation, serving as a reflection stage in the multi-agent research simulator. The difference between the previous 2 stages is that, first, the researchers involved during review writing are not the authors but the reviewers of the paper. Additionally, review writing is based on a written paper where $\\mb{h}_v$\\mb{h}h_v is no longer empty. Concretely, by adapting Equation \\ref{agg_data}, the TextGNN layer for review writing can be written as:', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_87', 'cites': [], 'refs': [], 'content': '\\vspace{-5mm}\n\\begingroup\n\\small\n\\begin{equation}\n\\begin{aligned}\n    \\mb{r}_{v} &= \\textsc{AGG}\\Big(\\mb{h}_v, \n    , \\big\\{\\mb{h}_d \\mid (v,d) \\in \\mathcal{E}_{dd}\\big\\}\\big\\{f_a(\\cdot), \\mb{h}_a \\mid (v,a) \\in \\mathcal{E}_{ad}\\big\\}\\Big) \\\\\n    &= f_g\\Big(\\Big[\\mb{h}_v, \n    \\big\\{f_a\\big([\\mb{h}_a, \\mb{h}_v, \\mb{h}_d]\\big) \\mid  (v,a) \\in \\mathcal{E}_{ad}, (v,d) \\in \\mathcal{E}_{dd}\\big\\}\\Big]\\Big)\n\\end{aligned}\n\\label{review_writing}\n\\end{equation}\\begin{equation}\n\\begin{aligned}\n    \\mb{r}_{v} &= \\textsc{AGG}\\Big(\\mb{h}_v, \n    , \\big\\{\\mb{h}_d \\mid (v,d) \\in \\mathcal{E}_{dd}\\big\\}\\big\\{f_a(\\cdot), \\mb{h}_a \\mid (v,a) \\in \\mathcal{E}_{ad}\\big\\}\\Big) \\\\\n    &= f_g\\Big(\\Big[\\mb{h}_v, \n    \\big\\{f_a\\big([\\mb{h}_a, \\mb{h}_v, \\mb{h}_d]\\big) \\mid  (v,a) \\in \\mathcal{E}_{ad}, (v,d) \\in \\mathcal{E}_{dd}\\big\\}\\Big]\\Big)\n\\end{aligned}\n\\label{review_writing}\n\\end{equation}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_88', 'cites': [], 'refs': [], 'content': '\\mb{r}r_{v}v &= \\textsc{AGG}\\Big(\\mb{h}h_v, \n    , \\big\\{\\mb{h}h_d \\mid (v,d) \\in \\mathcal{E}_{dd}dd\\big\\}\\big\\{f_a(\\cdot), \\mb{h}h_a \\mid (v,a) \\in \\mathcal{E}_{ad}ad\\big\\}\\Big) \\\\\n    &= f_g\\Big(\\Big[\\mb{h}h_v, \n    \\big\\{f_a\\big([\\mb{h}h_a, \\mb{h}h_v, \\mb{h}h_d]\\big) \\mid  (v,a) \\in \\mathcal{E}_{ad}ad, (v,d) \\in \\mathcal{E}_{dd}dd\\big\\}\\Big]\\Big)', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_89', 'cites': [], 'refs': [], 'content': '\\label{review_writing}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_90', 'cites': [], 'refs': [], 'content': '\\endgroup', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_91', 'cites': [], 'refs': [], 'content': '\\hangindent=0em\n\\hangafter=0\n\\xhdr{$\\triangleright$ Summary: \\envname simulation algorithm}$\\triangleright$\\triangleright Summary: \\envname simulation algorithm Utilizing the community graph $\\mathcal{G}$\\mathcal{G}, we propose a simulation algorithm named as \\envname. Overall, the simulation algorithm can be considered as a 2-layer GNN where the paper reading is the first layer of information aggregation. Both paper writing and review writing are the second layer of the GNN to generate the final simulation outputs. We formally summarize the research community simulation in Algorithm \\ref{alg:paper_brainstorming}. To achieve better efficiency, the modified version for implementation is in Appendix~\\S\\ref{simulation-algorithm-implementation}.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_92', 'cites': [], 'refs': [], 'content': '\\begin{algorithm}\n\\small\n\\caption{\\small \\envname simulation algorithm}\n\\label{alg:paper_brainstorming}\n\\begin{algorithmic}[1]\n\\REQUIRE community graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$,\\\\ \n         \\hspace{2.6em}paper contents $\\mb{x}_v$ for all paper nodes,\\\\ \n         \\hspace{2.6em}target paper node $v$\n\\ENSURE paper content $\\mb{h}_v$ and review content $\\mb{r}_v$ for paper node $v$\n\\FOR{each $u \\in \\mathcal{N}(v)$}\n    \\IF{$u \\in \\mathcal{V}_d$}\n        \\STATE $\\mb{h}_u \\gets \\mb{x}_u$\n    \\ELSE\n        \\STATE $\\mb{h}_{u} \\gets f_u\\left(\\left[\\left\\{\\mb{x}_d \\mid (u,d) \\in \\mathcal{E}_{ad}\\right\\}\\right]\\right)$ \\hfill $\\triangleright$ Eq.~\\eqref{paper_reading}\n    \\ENDIF\n\\ENDFOR\n\\STATE $\\mb{h}_{v} \\gets f_g\\Big(\\Big[\\big\\{f_a([\\mb{h}_a, \\mb{h}_d]) \\mid$ \n    \\STATE \\quad\\quad $(v,a) \\in \\mathcal{E}_{ad}, (v,d) \\in \\mathcal{E}_{dd}\\big\\}\\Big]\\Big)$ \\hfill $\\triangleright$ Eq.~\\eqref{paper_writing}\n\\STATE $\\mb{r}_{v} \\gets f_g\\Big(\\Big[\\mb{h}_v, \\{f_a([\\mb{h}_a, \\mb{h}_v, \\mb{h}_d]) \\mid$ \n    \\STATE \\quad\\quad $(v,a) \\in \\mathcal{E}_{ad}, (v,d) \\in \\mathcal{E}_{dd}\\}\\Big]\\Big)$ \\hfill $\\triangleright$ Eq.~\\eqref{review_writing}\n\\STATE \\textbf{return} $\\mb{h}_v$, $\\mb{r}_v$ \n\\end{algorithmic}\n\\end{algorithm}\\begin{algorithm}\n\\small\n\\caption{\\small \\envname simulation algorithm}\n\\label{alg:paper_brainstorming}\n\\begin{algorithmic}[1]\n\\REQUIRE community graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$,\\\\ \n         \\hspace{2.6em}paper contents $\\mb{x}_v$ for all paper nodes,\\\\ \n         \\hspace{2.6em}target paper node $v$\n\\ENSURE paper content $\\mb{h}_v$ and review content $\\mb{r}_v$ for paper node $v$\n\\FOR{each $u \\in \\mathcal{N}(v)$}\n    \\IF{$u \\in \\mathcal{V}_d$}\n        \\STATE $\\mb{h}_u \\gets \\mb{x}_u$\n    \\ELSE\n        \\STATE $\\mb{h}_{u} \\gets f_u\\left(\\left[\\left\\{\\mb{x}_d \\mid (u,d) \\in \\mathcal{E}_{ad}\\right\\}\\right]\\right)$ \\hfill $\\triangleright$ Eq.~\\eqref{paper_reading}\n    \\ENDIF\n\\ENDFOR\n\\STATE $\\mb{h}_{v} \\gets f_g\\Big(\\Big[\\big\\{f_a([\\mb{h}_a, \\mb{h}_d]) \\mid$ \n    \\STATE \\quad\\quad $(v,a) \\in \\mathcal{E}_{ad}, (v,d) \\in \\mathcal{E}_{dd}\\big\\}\\Big]\\Big)$ \\hfill $\\triangleright$ Eq.~\\eqref{paper_writing}\n\\STATE $\\mb{r}_{v} \\gets f_g\\Big(\\Big[\\mb{h}_v, \\{f_a([\\mb{h}_a, \\mb{h}_v, \\mb{h}_d]) \\mid$ \n    \\STATE \\quad\\quad $(v,a) \\in \\mathcal{E}_{ad}, (v,d) \\in \\mathcal{E}_{dd}\\}\\Big]\\Big)$ \\hfill $\\triangleright$ Eq.~\\eqref{review_writing}\n\\STATE \\textbf{return} $\\mb{h}_v$, $\\mb{r}_v$ \n\\end{algorithmic}\n\\end{algorithm}\n\\small\n\\caption{\\small \\envname simulation algorithm}\n\\label{alg:paper_brainstorming}\n[1]\n\\REQUIRE community graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$\\mathcal{G}(\\mathcal{V}, \\mathcal{E}),\\\\ \n         \\hspace{2.6em}paper contents $\\mb{x}_v$\\mb{x}x_v for all paper nodes,\\\\ \n         \\hspace{2.6em}target paper node $v$v\n\\ENSURE paper content $\\mb{h}_v$\\mb{h}h_v and review content $\\mb{r}_v$\\mb{r}r_v for paper node $v$v\n\\FOR{each $u \\in \\mathcal{N}(v)$}each $u \\in \\mathcal{N}(v)$u \\in \\mathcal{N}(v)\n    \\IF{$u \\in \\mathcal{V}_d$}$u \\in \\mathcal{V}_d$u \\in \\mathcal{V}_d\n        \\STATE $\\mb{h}_u \\gets \\mb{x}_u$\\mb{h}h_u \\gets \\mb{x}x_u\n    \\ELSE\n        \\STATE $\\mb{h}_{u} \\gets f_u\\left(\\left[\\left\\{\\mb{x}_d \\mid (u,d) \\in \\mathcal{E}_{ad}\\right\\}\\right]\\right)$\\mb{h}h_{u}u \\gets f_u\\left(\\left[\\left\\{\\mb{x}x_d \\mid (u,d) \\in \\mathcal{E}_{ad}ad\\right\\}\\right]\\right) \\hfill $\\triangleright$\\triangleright Eq.~\\eqref{paper_reading}\n    \\ENDIF\n\\ENDFOR\n\\STATE $\\mb{h}_{v} \\gets f_g\\Big(\\Big[\\big\\{f_a([\\mb{h}_a, \\mb{h}_d]) \\mid$\\mb{h}h_{v}v \\gets f_g\\Big(\\Big[\\big\\{f_a([\\mb{h}h_a, \\mb{h}h_d]) \\mid \n    \\STATE \\quad\\quad $(v,a) \\in \\mathcal{E}_{ad}, (v,d) \\in \\mathcal{E}_{dd}\\big\\}\\Big]\\Big)$(v,a) \\in \\mathcal{E}_{ad}ad, (v,d) \\in \\mathcal{E}_{dd}dd\\big\\}\\Big]\\Big) \\hfill $\\triangleright$\\triangleright Eq.~\\eqref{paper_writing}\n\\STATE $\\mb{r}_{v} \\gets f_g\\Big(\\Big[\\mb{h}_v, \\{f_a([\\mb{h}_a, \\mb{h}_v, \\mb{h}_d]) \\mid$\\mb{r}r_{v}v \\gets f_g\\Big(\\Big[\\mb{h}h_v, \\{f_a([\\mb{h}h_a, \\mb{h}h_v, \\mb{h}h_d]) \\mid \n    \\STATE \\quad\\quad $(v,a) \\in \\mathcal{E}_{ad}, (v,d) \\in \\mathcal{E}_{dd}\\}\\Big]\\Big)$(v,a) \\in \\mathcal{E}_{ad}ad, (v,d) \\in \\mathcal{E}_{dd}dd\\}\\Big]\\Big) \\hfill $\\triangleright$\\triangleright Eq.~\\eqref{review_writing}\n\\STATE \\textbf{return} $\\mb{h}_v$\\mb{h}h_v, $\\mb{r}_v$\\mb{r}r_v', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_93', 'cites': [], 'refs': [], 'content': '\\vspace{-3mm}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_94', 'cites': [], 'refs': [], 'content': '\\label{evaluation}\nUtilizing graph structures not only enables the design of the research simulation algorithm but also provides a natural way to evaluate it. As we show next, we propose to view research evaluation as a masked node prediction task, including evaluation for both paper writing and review writing.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_95', 'cites': [], 'refs': [], 'content': '\\xhdr{Evaluation by masked node prediction}Evaluation by masked node prediction A masked node prediction task in the community graph $\\mathcal{G}$\\mathcal{G} can be defined as first masking a specific node $v \\in \\mathcal{V}$v \\in \\mathcal{V} in the community graph by setting its hidden states $\\mb{h}_v = \\emptyset$\\mb{h}h_v = \\emptyset, where the original hidden state is saved as $\\mb{h}_v^*$\\mb{h}h_v^*; then an ideal model should be able to predict the hidden states $\\mb{h}_v^*$\\mb{h}h_v^* of the masked node from its neighborhood $\\mathcal{N}(v)$\\mathcal{N}(v). Concretely, in Equation \\ref{paper_writing}, the output $\\mb{h}_v$\\mb{h}h_v can be regarded as the masked node prediction for evaluation of paper writing, suppose that the node $v$v is a masked version of a ground truth data node. Similarly, in Equation \\ref{review_writing}, the output $\\mb{r}_v$\\mb{r}r_v can be regarded as the predicted node attributes for review writing, where the original review is represented as $\\mb{r}_v^*$\\mb{r}r_v^*.\nIn general, we have:\\\\', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_96', 'cites': [], 'refs': [], 'content': '\\vspace{-8mm}\n\\begingroup\n\\small\n\\begin{equation}\n\\begin{split}\n\\mb{h}_v, \\mb{r}_v &= \\textsc{ResearchTown}\\Big(\n    \\mathcal{G}(\\mathcal{V}, \\mathcal{E}); \\{\\mb{x}_u \\mid u \\in \\mathcal{N}(v)\\}; v\n\\Big)\n\\end{split}\n\\end{equation}\\begin{equation}\n\\begin{split}\n\\mb{h}_v, \\mb{r}_v &= \\textsc{ResearchTown}\\Big(\n    \\mathcal{G}(\\mathcal{V}, \\mathcal{E}); \\{\\mb{x}_u \\mid u \\in \\mathcal{N}(v)\\}; v\n\\Big)\n\\end{split}\n\\end{equation}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_97', 'cites': [], 'refs': [], 'content': '\\mb{h}h_v, \\mb{r}r_v &= \\textsc{ResearchTown}\\Big(\n    \\mathcal{G}(\\mathcal{V}, \\mathcal{E}); \\{\\mb{x}x_u \\mid u \\in \\mathcal{N}(v)\\}; v\n\\Big)', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_98', 'cites': [], 'refs': [], 'content': '\\endgroup\nwhere $\\mb{h}_v$\\mb{h}h_v is the text-form hidden states of a masked node $v$v and  $\\mb{r}_v$\\mb{r}r_v is the text-form prediction output of a masked node $v$v. Since we have real-world results for both paper writing and review, we treat them as ground truth even though they are not perfect because the goal of \\envname is to simulate the human research community rather than to find optimal solutions for papers and reviews ($\\mb{h}_v^*$\\mb{h}h_v^* for paper ground-truth and $\\mb{r}_v^*$\\mb{r}r_v^* for review ground-truth) and we can systematically evaluate both processes to check the effectiveness of our simulation algorithm. More specifically, since we have access to ground-truth papers $\\mb{h}_v^*$\\mb{h}h_v^* when evaluating the review writing simulation, to avoid accumulated errors, we update Equation \\ref{review_writing} during evaluation so that reviews $\\mb{r}_v$\\mb{r}r_v are generated based on $\\mb{h}_v^*$\\mb{h}h_v^*, instead of $\\mb{h}_v$\\mb{h}h_v:', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_99', 'cites': [], 'refs': [], 'content': '\\vspace{-6mm}\n\\begingroup\n\\small\n\\begin{equation}\n\\begin{split}\n\\mb{r}_{v} &= \\textsc{AGG}\\Big(\n    \\mb{h}_v^*, \\big\\{\\mb{h}_d \\mid (v,d) \\in \\mathcal{E}_{dd}\\big\\}\n    , \\big\\{f_a(\\cdot), \\mb{h}_a \\mid (v,a) \\in \\mathcal{E}_{ad}\\big\\}\n\\Big)\n\\end{split}\n\\end{equation}\\begin{equation}\n\\begin{split}\n\\mb{r}_{v} &= \\textsc{AGG}\\Big(\n    \\mb{h}_v^*, \\big\\{\\mb{h}_d \\mid (v,d) \\in \\mathcal{E}_{dd}\\big\\}\n    , \\big\\{f_a(\\cdot), \\mb{h}_a \\mid (v,a) \\in \\mathcal{E}_{ad}\\big\\}\n\\Big)\n\\end{split}\n\\end{equation}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_100', 'cites': [], 'refs': [], 'content': '\\mb{r}r_{v}v &= \\textsc{AGG}\\Big(\n    \\mb{h}h_v^*, \\big\\{\\mb{h}h_d \\mid (v,d) \\in \\mathcal{E}_{dd}dd\\big\\}\n    , \\big\\{f_a(\\cdot), \\mb{h}h_a \\mid (v,a) \\in \\mathcal{E}_{ad}ad\\big\\}\n\\Big)', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_101', 'cites': [], 'refs': [], 'content': '\\endgroup\n\\xhdr{Evaluation metric}Evaluation metric We utilize state-of-the-art embedding models like text-embedding-large-3~\\footnote{\\url{https://openai.com/index/new-embedding-models-and-api-updates/}} to build distance function for $d_p(\\mb{h}_v, \\mb{h}_v^*)$d_p(\\mb{h}h_v, \\mb{h}h_v^*) and $d_r(\\mb{r}_v, \\mb{r}_v^*)$d_r(\\mb{r}r_v, \\mb{r}r_v^*). More details related to formal embedding-based metric definitions for paper writing and review writing tasks are available in Appendix~\\S\\ref{evaluation-details}.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_102', 'cites': [], 'refs': [], 'content': '\\xhdr{\\envname setting}\\envname setting\nWe utilize GPT-4o-mini~\\footnote{We point to \\texttt{GPT-4o-mini-2024-07-18} for use.} as the LLM backbone for implementing the agent functions, with the decoding temperature set to \\( 0 \\) 0  to ensure reproducibility. To evaluate different aggregation strategies, we conduct experiments using specific types of nodes connected to the target node: (1) \\textit{AGG-self}, where the aggregation relies solely on the target node; (2) \\textit{AGG-agent}, which includes the target node and its neighboring agent nodes; (3) \\textit{AGG-data}, which involves the target node and its neighboring data nodes; and (4) \\textit{AGG-global}, which incorporates the target node and all its neighboring nodes, including agent and data nodes. We specifically refer to \\textit{AGG-global} as our proposed \\envname method for simulation, while the others serve as baselines. This experimental design enables a systematic comparison of the effects of different neighborhood information on the aggregation process. More details about different settings are available in Appendix~\\S\\ref{agg-setting-implementation}.\n\\label{researchtown-setting}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_103', 'cites': [], 'refs': [], 'content': '\\xhdr{\\benchname setting}\\benchname setting To evaluate \\envname for research simulation, we introduce \\benchname, which consists of 1,000 paper writing tasks and 200 review writing tasks. All tasks are sourced from recent top-tier machine learning conferences such as NeurIPS 2024~\\footnote{\\url{https://neurips.cc/Conferences/2024}} and ICLR 2024~\\footnote{\\url{https://openreview.net/group?id=ICLR.cc/2024/Conference}}. Since most papers are released after the cutoff date of GPT-4o-mini, information leakage is not considered an issue. For paper writing tasks, we categorize them into three difficulty levels—\\textit{hard} (333 tasks), \\textit{medium} (334 tasks), and \\textit{easy} (333 tasks)—based on the similarity results of data-only aggregation. Specifically, for review writing tasks, the reviewers prepared for each paper are selected from the top 5 researchers most related to the paper, as reviewer information is not publicly available in the real world. More details about the data collection and prevention of information leakage during simulation are in Appendix~\\S\\ref{research-bench-tech-details}.\n\\label{researchbench-setting}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_104', 'cites': [], 'refs': [], 'content': '\\label{tab:paper-writing-result}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_105', 'cites': [], 'refs': [], 'content': '\\label{tab:review-writing-result}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_106', 'cites': [], 'refs': [], 'content': '\\vspace{-6mm}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_107', 'cites': [], 'refs': [], 'content': '\\label{sec:core-results}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_108', 'cites': [], 'refs': [], 'content': 'In this section, we present the main results of our research simulation on \\benchname, including 1,000 paper writing tasks and 200 review writing tasks. We evaluate existing paper nodes that have fully known their content and their neighborhoods within the community graph. We refer to these scenarios as \\textit{in-distribution} cases.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_109', 'cites': [], 'refs': ['table_7', 'table_8'], 'content': '\\xhdr{Overall: \\envname can provide a realistic simulation of research activity}Overall: \\envname can provide a realistic simulation of research activity To evaluate research simulation, we utilize state-of-the-art embedding models (text-embedding-3-large) to compare the semantic similarity between simulated results and real-world results. For paper writing, as shown in Table~\\ref{tab:paper-writing-result}, the overall similarity score obtained using text-embedding-3-large across 1,000 papers is 67.51. Notably, the score increases to 73.79 for an easy subset of the benchmark. These results demonstrate that paper writing with \\envname can produce realistic outputs compared to real-world ones. Moreover, it indicates that some ideas in top-tier conference papers are not hard to think of and can be imagined by LLMs. For review writing, as shown in Table~\\ref{tab:review-writing-result}, the similarity scores are generally lower compared with paper writing, with strength-related scores averaging around 51 and weakness-related scores averaging around 47. This suggests that review writing is more challenging to generate with \\envname, particularly for weakness identification. A possible explanation is that real-world review data is often noisier and more diverse, making it harder to simulate accurately.', 'type': 'textNode', 'isolation': False}
tab:paper-writing-result
tab:review-writing-result
text_node
{'id': 'text_110', 'cites': [], 'refs': ['table_7'], 'content': '\\xhdr{Paper writing: participation of multi-researchers improves paper quality}Paper writing: participation of multi-researchers improves paper quality As shown in Table \\ref{tab:paper-writing-result}, cited papers contribute more effectively than authors in the paper writing simulation, with data-aggregation achieving a score of 65.30 compared to 55.24 for agent-aggregation. The best results are obtained by combining both, surpassing data aggregation by 2.21 points. Researchers are particularly beneficial under difficult scenarios, improving the text-embedding-large-3 score from 56.02 to 60.89, likely due to the inclusion of multi-hop paper information from researchers.', 'type': 'textNode', 'isolation': False}
tab:paper-writing-result
text_node
{'id': 'text_111', 'cites': [], 'refs': ['table_8', 'table_9'], 'content': '\\xhdr{Review writing: participation of multi-reviewers improves review quality}Review writing: participation of multi-reviewers improves review quality Unlike paper writing, review writing mainly relies on the paper that needs to be reviewed, making reviewers and cited papers less impactful, with differences limited to within 1 point. However, as shown in Table~\\ref{tab:review-writing-result}, adding additional information consistently improves performance over the self-aggregation baseline. Agent aggregation performs best for writing strengths and assigning scores, while data aggregation achieves the best results for writing weaknesses. This pattern likely reflects the role of related work comparisons in highlighting weaknesses, while multiple reviewers help provide a more balanced assessment of strengths. Interestingly, global aggregation leads to larger differences in scores. We consider it an exception since GPT-4o-mini tends to apply stricter novelty judgments under global aggregation—its average assigned score drops from 5.3 to 5.0. As shown in Table~\\ref{tab:model-ablation}, this effect is not observed for Qwen-2.5-7B-Instruct or Deepseek-v3, which gain better results with global aggregation.', 'type': 'textNode', 'isolation': False}
tab:review-writing-result
tab:model-ablation
text_node
{'id': 'text_112', 'cites': [], 'refs': [], 'content': '\\label{sec:underexplored-idea}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_113', 'cites': [], 'refs': [], 'content': '\\vspace{-3mm}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_114', 'cites': [], 'refs': [], 'content': 'We conduct ablation studies on both hyperparameters and model selection. The results show that \\envname consistently produces high-quality simulations across a range of settings, demonstrating strong robustness. Detailed experimental configurations are provided in Appendix~\\S\\ref{ablation-study-details}.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_115', 'cites': [], 'refs': [], 'content': '\\xhdr{Ablation on paper number}Ablation on paper number\nIn paper writing tasks, users can freely assign papers to simulate non-existent work, making robustness to the number of papers essential. As shown in Figure~\\ref{paper_writing_paper_number}, papers cited in the related work section have the greatest positive impact, increasing the similarity score from 66.4 to 66.7 compared to using all papers. In contrast, using only papers cited in the introduction lowers the score to 65.2, while including papers from other sections reduces it further to 58.4. These results highlight the importance of selecting informative references when generating papers. In review writing, the number of papers is fixed, so no ablation study on the paper number is applicable.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_116', 'cites': [], 'refs': ['figures_4'], 'content': '\\xhdr{Ablation on agent number}Ablation on agent number\nFor \\envname simulation, users can assign different numbers of agents, making robustness to agent number critical for \\envname. In Figure~\\ref{paper_writing_researcher_num}, in the paper writing task, increasing the agent number improves simulation quality under the agent-aggregation setting. The most notable gain occurs when increasing from 1 to 2, boosting the similarity score from 49.0 to 52.7.\nSimilar trends hold in review writing (Figure~\\ref{review_writing_researcher_num}), where increasing the agent number consistently enhances output quality. The strength score improves from 50.8 to 51.5 when increasing the reviewer from 1 to 5.', 'type': 'textNode', 'isolation': False}
text_node
{'id': 'text_117', 'cites': [], 'refs': ['table_9'], 'content': '\\xhdr{Ablation on generation models}Ablation on generation models The choice of LLMs significantly impacts simulation quality. In addition to GPT-4o-mini, we evaluate two models from different families: Qwen-2.5-7B-Instruct\\footnote{\\url{https://huggingface.co/Qwen/Qwen2.5-7B-Instruct}} and Deepseek-v3\\footnote{We point \\texttt{DeepSeek-V3-0324} for use.}. In Table~\\ref{tab:model-ablation}, for both paper writing tasks, global aggregation (\\envname) consistently yields the highest similarity scores across all models. It also achieves the best review difference scores for Qwen-2.5-7B-Instruct and Deepseek-v3. The only exception is GPT-4o-mini, which shows an unexpected increase in review difference under AGG-global. Overall, Deepseek-v3 outperforms GPT-4o-mini, which in turn outperforms Qwen-2.5-7B-Instruct—consistent with their relative performance on other tasks.', 'type': 'textNode', 'isolation': False}
tab:model-ablation
text_node
{'id': 'text_118', 'cites': [], 'refs': ['figures_4'], 'content': '\\xhdr{Ablation on embedding models}Ablation on embedding models Similarity scores can be computed using different models, and voyage-3\\footnote{\\url{https://blog.voyageai.com/2024/09/18/voyage-3/}} serves as an alternative to the text-embedding-3-large used in our main experiments. As shown in Figures~\\ref{paper_writing_paper_number}, \\ref{paper_writing_researcher_num}, and \\ref{review_writing_researcher_num}, voyage-3 produces consistent trends in ablation studies involving the number of papers and agents. This consistency suggests that \\envname is robust to the choice of embedding model, and different models lead to the same conclusions.', 'type': 'textNode', 'isolation': False}
text_node
{'id': 'text_119', 'cites': [], 'refs': [], 'content': '\\vspace{-1mm}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_120', 'cites': [], 'refs': [], 'content': 'Besides computing embedding-based similarities, we provide more types of evaluations here. First, we prompt LLMs to calculate fine-grained similarity scores that assess consistency between real-world data and simulated ones across various dimensions. Next, we evaluate the intrinsic quality of the simulated outputs themselves and compare them with real-world data. Finally, we report results from human evaluations to validate the alignment between LLM-based evaluation and human judgments. More details about LLM-based evaluation are available in Appendix~\\S\\ref{sec:llm-based-eval}, and details about human evaluation are available in Appendix~\\S\\ref{sec:human-eval}.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_121', 'cites': [], 'refs': ['figures_5'], 'content': '\\xhdr{Automatic evaluation on fine-grained similarity}Automatic evaluation on fine-grained similarity A high cosine similarity score alone can mask important issues in simulated results.\nTo capture a more complete picture of similarity, we move beyond a single score and instead evaluate across five fine-grained dimensions: \\textit{topic consistency}, \\textit{method consistency}, \\textit{factual consistency}, \\textit{claim consistency}, and \\textit{application context consistency}. These dimensions collectively reflect subcomponents of overall semantic similarity. For evaluation, we use GPT-4o to assign scores from 0 to 10 for each dimension for each paper. As shown in Figure~\\ref{fig:fine-grained-similarity}, our proposed global aggregation method (\\envname) consistently outperforms all other aggregation baselines across these dimensions. This demonstrates that \\envname provides a more effective simulation of research activities compared to baselines.', 'type': 'textNode', 'isolation': False}
text_node
{'id': 'text_122', 'cites': [], 'refs': [], 'content': '\\vspace{-4mm}\n\\label{tab:model-ablation}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_123', 'cites': [], 'refs': [], 'content': '\\vspace{-5mm}\n\\label{tab:novelty_feasibility}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_124', 'cites': [], 'refs': ['table_10'], 'content': '\\xhdr{Automatic evaluation on intrinsic quality}Automatic evaluation on intrinsic quality In addition to evaluating semantic similarity between simulated and real-world data, we also assess the intrinsic quality of the generated content. Specifically, we focus on two key dimensions: \\textit{novelty} and \\textit{feasibility}, which we consider the two most critical aspects of a research proposal. As shown in Table~\\ref{tab:novelty_feasibility}, the simulated outputs still do not match the novelty and feasibility levels of real-world articles but are close to those. This gap indicates that \\envname would benefit from a more coordinated agentic workflow to enhance the quality of the generated research outputs.', 'type': 'textNode', 'isolation': False}
tab:novelty_feasibility
text_node
{'id': 'text_125', 'cites': [], 'refs': [], 'content': '\\xhdr{Human evaluation}Human evaluation Evaluation based on LLMs may introduce bias into the results. To validate the reliability of LLM-based evaluations, we conduct additional human evaluations. For similarity-based assessments, human judgments correlate well with LLM scores, achieving a Pearson correlation of 0.61, indicating reasonable agreement. However, for intrinsic quality evaluations, the correlation between human and LLM scores is low. This is likely due to the inherent ambiguity of such tasks and the need for domain-specific expertise. Despite this, both human and LLM evaluations consistently indicate that simulated papers are slightly less novel than real-world ones—though the gap is relatively small (5.50 vs 5.90 for humans and 7.39 vs 7.85 for LLMs).', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_126', 'cites': [], 'refs': [], 'content': '\\vspace{-2mm}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_127', 'cites': [], 'refs': [], 'content': '\\label{case-study-section}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_128', 'cites': [], 'refs': [], 'content': 'As discussed in Section~\\S\\ref{sec:core-results}, the node masking evaluation in \\envname targets \\textit{in-distribution} settings with predefined neighborhoods. In real-world use, however, \\envname must generate non-existing papers and reviews without such neighborhoods, requiring automatic construction via paper–researcher matching. This leads to \\textit{out-of-distribution} cases, such as interdisciplinary research, where unrelated papers and researchers form unconventional neighborhoods without prior related works.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_129', 'cites': [], 'refs': ['figures_6'], 'content': '\\xhdr{\\envname can inspire interdisciplinary research}\\envname can inspire interdisciplinary research Interdisciplinary research is often challenging due to limited collaboration across fields. \\envname addresses this by enabling agents with diverse expertise to read, interact, and co-create novel ideas. For example, as shown in Figure~\\ref{fig:case-study}, combining NLP and astronomy papers leads to using kinematic models to analyze language evolution, while linking NLP and criminology inspires the use of LLMs to support communities affected by mass incarceration. These domain pairings are rarely explored in existing literature, demonstrating \\envname’s ability to generate innovative, cross-disciplinary research directions.', 'type': 'textNode', 'isolation': False}
text_node
{'id': 'text_130', 'cites': [], 'refs': [], 'content': "\\xhdr{\\envname-written contents might have limited use in the real world}\\envname-written contents might have limited use in the real world \n\\envname exhibits failure modes when combining too many disparate domains, often producing incoherent or superficial outputs. For example, combining researchers and papers from LLM, biology, criminology, and astronomy, \\envname generates a research question of ``\\textit{How does coded language in political discourse influence societal biases, and how can a Bayesian hierarchical model be employed to analyze this effect while simultaneously addressing observational biases in white dwarf population studies?}'' It simply strings together terminology from different domains without presenting a clear research direction. Such vagueness might hinder the real use of the papers simulated from \\envname.", 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_131', 'cites': [], 'refs': [], 'content': '\\vspace{-3mm}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_132', 'cites': [], 'refs': [], 'content': 'We introduce \\envname, a graph-based multi-agent framework that simulates research communities by modeling them as heterogeneous graphs. \\envname integrates key research activities—paper reading, writing, and reviewing—into a unified TextGNN-driven inference process. It enables realistic and robust simulations through agent collaboration and facilitates rare interdisciplinary interactions. \\envname offers a valuable platform for studying research dynamics and developing algorithms to support automated scientific discovery.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_133', 'cites': [], 'refs': [], 'content': '\\envname presents an LLM-based simulation framework that models human research communities as graph-based multi-agent systems, enabling the study of collaboration, knowledge diffusion, and institutional dynamics. By formalizing how agents create, refine, and evaluate academic papers, the simulator can inform the design of autonomous research systems that assist, rather than replace, human researchers. Potential applications include optimizing collaboration structures, identifying systemic bottlenecks in peer review or discovery, and stress-testing scientific workflows under various incentive and communication settings. While the framework is primarily a research tool, we acknowledge that future extensions involving autonomous agents could raise ethical considerations around authorship, influence, and epistemic trust. Our work highlights the imperative for adaptive ethical frameworks that keep pace with technological capabilities while protecting scholarly values.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_134', 'cites': [], 'refs': [], 'content': 'We sincerely appreciate the support from Amazon grant funding project \\#120359, "GRAG: Enhance RAG Applications with Graph-structured Knowledge", and Meta gift funding project "PERM: Toward Parameter Efficient Foundation Models for Recommenders".', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_135', 'cites': [], 'refs': [], 'content': '\\nocite{langley00}langley00', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_136', 'cites': [], 'refs': [], 'content': '\\bibliography{example_paper}\n\\bibliographystyle{icml2025}icml2025', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_137', 'cites': [], 'refs': [], 'content': '\\newpage\n\\onecolumn', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_138', 'cites': [], 'refs': [], 'content': '\\label{appendix:ethical}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_139', 'cites': [], 'refs': [], 'content': 'The development and deployment of \\envname raises several important ethical considerations that we have carefully addressed in our work. We first discuss how \\envname prevents dangerous use, including facilitating plagiarism, producing misleading or low-quality claims, and role-playing human researchers. Furthermore, we discuss the attribution and authorship issues for generated content and discuss the model and data license in our work.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_140', 'cites': [], 'refs': [], 'content': '\\subsection{Potential to facilitate plagiarism}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_141', 'cites': ['elali2023ai'], 'refs': [], 'content': "Generative AI's capabilities for image and text generation can potentially lead to plagiarism in research~\\citep{elali2023ai}. To address this, we have implemented safeguards to ensure responsible usage. \\envname is designed as an assistive tool to help researchers gather inspiration for papers and review writing, rather than generating complete, ready-to-use content. By design, \\envname ensures that its outputs serve as a starting point for further intellectual effort, rather than a replacement for human researchers.", 'type': 'textNode', 'isolation': False}
No paper found for  elali2023ai
text_node
{'id': 'text_142', 'cites': [], 'refs': [], 'content': 'For generated papers, \\envname provides only preliminary answers to five key research questions. These outputs are intentionally incomplete and generic, requiring significant refinement and further development by the user. Critical sections such as the introduction, background, methodology, discussion, and conclusion are not included, placing the responsibility for completing and validating the content on the researcher.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_143', 'cites': [], 'refs': [], 'content': 'For generated reviews, \\envname provides general guidance on potential strengths and weaknesses, accompanied by an indicative score for reference. However, these reviews are intentionally non-definitive and generic, only as a supplementary aid to help reviewers organize their thoughts. Generated reviews do not replace human judgment in determining the acceptance or rejection of a paper. The final evaluation, including critical reasoning, detailed feedback, and the ultimate decision, remains the sole responsibility of the reviewer. Reviewers must ensure fairness, accuracy, and rigor, using AI outputs only as a starting point to enhance their assessment process.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_144', 'cites': [], 'refs': [], 'content': '\\subsection{Potential to produce misleading or low-quality claims}\nThe motivation of our paper is to simulate research activities and generate preliminary research progress (\\eg, papers and reviews that are in their condensed bullet-point summarized format) that can be scrutinized and validated by human researchers, ultimately contributing to the acceleration of the research process. We acknowledge that AI-generated ideas may vary in quality, and therefore, these outputs are not intended for direct dissemination. Instead, they serve as initial, unofficial suggestions that require further experimental validation by human researchers. This approach ensures that only rigorously tested and verified research is presented as final, high-quality work.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_145', 'cites': [], 'refs': [], 'content': '\\subsection{Potential to role-play human researchers}\nThe primary objective of our work is to leverage existing academic literature to simulate research activities. In this paper, our research agents are designed to act as research domain experts, generating informative and relevant content based on a given and limited research domain. Importantly, we do not aim to simulate human-like interactive dialogues between research agents, nor do we attempt to mimic the specific research styles of individual human researchers. Instead, we focus on using related academic papers as conditions for generating more related research content.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_146', 'cites': [], 'refs': [], 'content': "The research agents are built using publicly available, properly cited academic papers, which eliminates the need for additional consent. We utilize the LLM-based research agents, each with one or more specific research domains, modeling the typical academic process, where researchers read, synthesize, and build upon available public academic data. By focusing on publicly available research papers, we align with the papers' intended purpose: contributing to the collective advancement of knowledge and fostering academic growth.", 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_147', 'cites': [], 'refs': [], 'content': '\\subsection{Attribution and authorship}\nThe AI-generated content, such as papers, reviews, or other research outputs, is meant for internal discussion and as a reference to assist human researchers. These outputs are not intended for direct publication. Our proposed methods serve as tools to accelerate the research process by offering starting points that require further elaboration, critical analysis, and human refinement to reach a publishable standard. The final authority to complete and submit research lies solely with human authors, ensuring that full responsibility and ownership remain with them. Since the AI-generated content is not considered complete or officially authored, it does not raise issues of authorship or attribution.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_148', 'cites': [], 'refs': [], 'content': "We list all licenses for the data and models used in our paper in this section.\n\\subsection{Data license}\nAll papers in \\benchname come from top-tier machine learning conferences (ICLR 2024 and NeurIPS 2024). These papers are publicly available and under the license of CC-BY 4.0, allowing for redistribution and sharing. For the evaluation results of \\benchname, all inputs and outputs are logged and open for access. Additionally, we keep an accessible record of all supplementary papers referenced during \\envname's inference process. All outputs from \\envname are released under the licenses of the papers used for generation.", 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_149', 'cites': [], 'refs': [], 'content': '\\subsection{Model license}\nOur work relies on multiple foundation models, including GPT-4o-mini, Qwen-2.5-7B-Instruct, Deepseek-v3, text-embedding-3-large, and voyage-3. Specifically, we use \\texttt{gpt-4o-mini-2024-07-18} accessed via the OpenAI API. We use \\texttt{Qwen-2.5-7B-Instruct-Turbo} and \\texttt{Deepseek-v3-0324} via the together.ai~\\footnote{\\url{https://www.together.ai/inference}} inference API. We utilize the official inference API provided by OpenAI and VoyageAI to use text-embedding-3-large and voyage-3 separately.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_150', 'cites': [], 'refs': [], 'content': 'The GPT-4o-mini, text-embedding-3-large, and voyage-3 models are closed-source and operate under proprietary licenses. We use them only for academic, and non-commercial purposes and ensure all inputs come from publicly available data, complying with their usage restrictions. By contrast, Qwen-2.5-7B-Instruct is released under the permissive Apache 2.0 license, and Deepseek-v3-0324 is available under the MIT License, allowing for broad academic and research use. We make no modifications to these models and use them as-is via their public APIs.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_151', 'cites': [], 'refs': [], 'content': '\\label{research-bench-tech-details}\nIn this section, we provide the technical details included in the construction process of \\benchname. We describe the methodologies used for data collection across its three main components, and we name them as: (1) \\textsc{PaperBench}, (2) \\textsc{HighImpactPaperBench}, and (3) \\textsc{ReviewBench}. Statistically, \\textsc{PaperBench} and \\textsc{HighImpactPaperBench} focus on a paper writing simulation, which contains 1,000 and 100 tasks, respectively. \\textsc{ReviewBench} focuses on review writing simulation and includes 200 tasks.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_152', 'cites': [], 'refs': [], 'content': '\\subsection{Data collection details}\n\\label{data-collection}\nWe first include technical details related to how we collect paper, author, and review data from publicly available platforms as a source to build \\benchname.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_153', 'cites': [], 'refs': [], 'content': '\\xhdr{Paper data collection}Paper data collection We begin by recording the titles of all papers that we plan to crawl. Then, using the \\texttt{arxiv} Python package\\footnote{\\url{https://pypi.org/project/arxiv/}}, we query the arXiv API to check for any papers with identical titles. If a match is found, we note the corresponding arXiv ID and use the API to retrieve the paper’s metadata, including its title, arXiv ID, author list, abstract, and citation information.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_154', 'cites': [], 'refs': [], 'content': '\\xhdr{Author data collection}Author data collection A primary challenge in collecting author data is that there might be multiple human researchers with the same name, and some human researchers may not have any publicly available publication records on public platforms, including arXiv, Google Scholar, or Semantic Scholar. As a result, at the paper collection stage, we only have each author’s name. We use the \\texttt{semanticscholar} Python package\\footnote{\\url{https://github.com/danielnsilva/semanticscholar}} to search for the author by name, verify that they have contributed to the specific target paper, and obtain a unique author ID from Semantic Scholar. This ID then allows us to retrieve their available publication information. To prevent information leakage when simulating paper writing and review scenarios, we exclude any of the author’s publications released after the target paper’s publication year. For example, if we aim to simulate a paper published in 2022, we ignore all of the author’s publications appearing after 2022. We also exclude the target paper itself to avoid leaking information. Generally, we limit the maximum number of collected publications to around 20, focusing on those most relevant to the target time frame. Additionally, we gather each author’s co-author network and their top publications to enrich the dataset with useful relational information.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_155', 'cites': [], 'refs': [], 'content': '\\xhdr{Review data collection}Review data collection In addition to paper and author data, we also leverage OpenReview to extract public review information. Since fully public review data is predominantly available for ICLR, we focus on collecting reviews from ICLR2024. Using the \\texttt{openreview} Python package\\footnote{\\url{https://openreview-py.readthedocs.io/en/latest/}}, we first verify the arXiv ID to ensure that we are retrieving the correct paper and its corresponding reviews. The collected review data aligns with ICLR’s criteria, including detailed feedback on soundness, presentation, contributions, reviewer scores, and commentary on strengths and weaknesses. We adopt this review structure when generating our reviews, incorporating strengths, weaknesses, and ratings for the paper.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_156', 'cites': [], 'refs': [], 'content': "\\subsection{\\textsc{PaperBench} details}\n\\label{paper-bench-detail}\n\\textsc{PaperBench} is designed to evaluate the effectiveness of paper-writing simulations by gathering high-quality paper metadata from top-tier ML conferences, such as NeurIPS 2024 and ICLR 2024. Both NeurIPS 2024 and ICLR 2024 post-date beyond GPT-4o-mini's October 2023 knowledge cutoff. Thus, data leakage is not a concern. We also mask the full text during the simulation to avoid accidental exposure. Based on the collected author and paper data, we perform the following two post-processing steps:", 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_157', 'cites': [], 'refs': [], 'content': 'First, we address cases where authors have no accessible publications beyond the current paper or where citation data extraction fails due to API issues. In such cases, we exclude these papers. We only retain those with full author publication information, as well as complete metadata including introduction, abstract, title, and citations. After this filtering step, we end up with approximately 1,200 papers, and then randomly sample 1,000 from them.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_158', 'cites': [], 'refs': [], 'content': 'Second, to allow more fine-grained analysis, we split these 1,000 paper-writing tasks into three subgroups based on their difficulty level. We use the \\textit{data-agg} settings described in Section~\\S\\ref{researchtown-setting} to obtain results and compute similarity scores for our simulations. We then divide the dataset into three equal subsets: the worst 333 data points (hard), the middle 334 data points (medium), and the top 333 data points (easy). This results in a more granular categorization of the dataset’s difficulty.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_159', 'cites': ['domingo2023stochastic', 'tu2024mixed', 'han2024multistable', 'yu20244real', 'tasseskill', 'cao2024worst'], 'refs': [], 'content': "Intuitively, papers in the hard sub-part tend to be more theoretical and math-focused, while those in the easy sub-part are more application-oriented. Examples for hard sub-parts of the dataset include ``\\textit{Stochastic Optimal Control Matching}''~\\citep{domingo2023stochastic}, ``\\textit{Mixed Dynamics In Linear Networks: Unifying the Lazy and Active Regimes}''~\\citep{tu2024mixed}, and ``\\textit{Multistable Shape from Shading Emerges from Patch Diffusion}''~\\citep{han2024multistable}. Examples for easy sub-parts of the dataset include ``\\textit{4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models}''~\\citep{yu20244real}, ``\\textit{Skill Machines: Temporal Logic Skill Composition in Reinforcement Learning}''~\\citep{tasseskill}, and ``\\textit{On the Worst Prompt Performance of Large Language Models}''~\\citep{cao2024worst}.", 'type': 'textNode', 'isolation': False}
No paper found for  domingo2023stochastic
No paper found for  tu2024mixed
No paper found for  han2024multistable
No paper found for  yu20244real
No paper found for  tasseskill
No paper found for  cao2024worst
text_node
{'id': 'text_160', 'cites': [], 'refs': [], 'content': '\\subsection{\\textsc{Reviewbench} details}\nSince public review data is only fully accessible from ICLR, we focus on collecting review data for the ICLR 2024 papers included in \\textsc{PaperBench}. All reviews are anonymous, so no direct reviewer information is available. To address this, we identify suitable reviewers by first summarizing each researcher’s publications. We then use the abstract of the target paper as a query and the researcher profiles as documents for a ranking task with the voyage-3 model. All authors included in \\benchname serve as the corpus for retrieval. The top 20 most relevant authors, excluding the paper’s authors, become the suitable reviewer candidates.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_161', 'cites': [], 'refs': [], 'content': 'After obtaining the reviewer, paper, and author data, we filter out any papers lacking valid reviews during crawling. From the remaining set, we randomly select 200 reviews, each corresponding to one paper as \\textsc{ReviewBench}.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_162', 'cites': [], 'refs': [], 'content': '\\subsection{\\textsc{HighImpactPaperBench} details}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_163', 'cites': ['kingma2013auto', 'goodfellow2014generative', 'kingma2014adam'], 'refs': [], 'content': '\\textsc{HighImpactPaperBench} serves as an extreme benchmark for \\envname, focused on simulating impactful research. We begin by collecting the 20 most-cited papers from each of 10 leading AI-related conferences—CVPR, ECCV, NeurIPS, ICLR, ICML, AAAI, IJCAI, ACL, EMNLP, and NAACL—based on Google Scholar citation rankings.\\footnote{\\url{https://scholar.google.es/citations?view_op=top_venues&hl=en&vq=eng}} Additionally, we include classic machine learning algorithm papers such as those introducing VAE~\\citep{kingma2013auto}, GAN~\\citep{goodfellow2014generative}, and Adam~\\citep{kingma2014adam}, each with over 1,000 citations, even if they are no longer listed in the current Google Scholar citation rankings.', 'type': 'textNode', 'isolation': False}
No paper found for  kingma2013auto
No paper found for  goodfellow2014generative
No paper found for  kingma2014adam
text_node
{'id': 'text_164', 'cites': [], 'refs': [], 'content': "For these impactful papers, it is crucial to prevent the inclusion of publications released after their publication year when gathering authors' publication data. Later works such as these could significantly alter the trajectory of the researcher, misrepresent the historical context of these influential contributions, and leak information for simulation. After collecting paper and author data, we remove any papers with incomplete information due to crawling errors. From the remaining set, we randomly sample 100 papers to form the final benchmark. These selected papers have averaged over 100 citations in the past five years, ensuring that \\textsc{HighImpactPaperBench} represents a collection of influential and well-established research.", 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_165', 'cites': [], 'refs': [], 'content': 'The motivation for using impactful papers as evaluation is to use them as an extreme-case test for idea simulation. While some may exist in the LLM’s training data, this benchmark is separate from our main results and serves to explore how LLMs handle well-known concepts. Our similarity analysis shows that 55\\% of generated papers score between 0.65–0.75, and 18\\% exceed 0.75, indicating moderate to high alignment. Only 1\\% scored below 0.45. These scores are comparable to \\textsc{PaperBench}, suggesting no abnormal inflation. Even famous papers like VAE, GAN, and LayerNorm do not receive notably high scores, implying that semantic similarity, not memorization based on citation relationships, drives the results, especially for tool/benchmark papers, which naturally resemble their references more.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_166', 'cites': [], 'refs': [], 'content': '\\label{evaluation-details}\nIn this section, we first explain the motivation for our designed multi-component embedding-based evaluation, then we provide a more formal definition and implementation details related to our evaluation process.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_167', 'cites': ['chen2025learning', 'jin2025search'], 'refs': [], 'content': '\\xhdr{Decompositionality}Decompositionality A single idea or a review can manifest through diverse descriptions or implementation strategies. Therefore, directly applying a cosine similarity-based metric is inadequate for capturing conceptual equivalence. To solve this, we design point-wise evaluation metrics to paraphrase the paper and review it into aligned key points with the same LLM-based prompting. This structure enables alignment between papers that differ methodologically but share similar motivations and problem framings. For instance, in \\citet{chen2025learning} and \\citet{jin2025search}, despite distinct methods and settings, experts would find strong alignment on the motivations and core concepts in these papers.', 'type': 'textNode', 'isolation': False}
No paper found for  chen2025learning
No paper found for  jin2025search
text_node
{'id': 'text_168', 'cites': ['si2024can'], 'refs': [], 'content': '\\xhdr{Scalability}Scalability To address the challenge that a single idea can take many concrete forms, we complement decomposition with scalability. LLMs can generate hundreds of semantically distinct research questions from a single prompt, but evaluating these outputs traditionally requires domain experts—a process that is costly, slow, unscalable, and hard to reproduce. For example, \\citet{si2024can} spent thousands hiring top-tier researchers solely for annotation and review, which is infeasible for evaluating large-scale, automated research generation. Our approach replaces this bottleneck with semantic similarity over 5Q-decomposed representations. We can select the best among the samples and make the score the final result.', 'type': 'textNode', 'isolation': False}
No paper found for  si2024can
text_node
{'id': 'text_169', 'cites': [], 'refs': [], 'content': '\\xhdr{Extensibility}Extensibility While we acknowledge the importance of elements like mathematical formulations or algorithmic workflows, our framework is inherently extensible—the original format can be expanded with more key points by adding domain-specific dimensions such as algorithmic structure or key theoretical results. This is especially valuable in systems and theory papers, enabling more fine-grained and domain-aware similarity analysis. As demonstrated in [Fine-Grained Evaluation with LLM and Human], our approach also supports the integration of non-semantic metrics like logical consistency and factual accuracy, making it extensible from an evaluation metric perspective.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_170', 'cites': [], 'refs': [], 'content': '\\xhdr{Reliability}Reliability Our embedding-based / LLM-based similarity metric builds on state-of-the-art models optimized for knowledge-intensive tasks. Voyage AI embeddings, widely adopted in real-world RAG systems, are designed to reduce hallucination and excel in high-precision semantic retrieval, making them ideal for evaluating research content. Additionally, state-of-the-art LLMs are highly effective at semantic comparison.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_171', 'cites': ['chen2025learning', 'jin2025search'], 'refs': [], 'content': '\\xhdr{Baselines for evaluation}Baselines for evaluation To check whether \\envname provides a realistic simulation, we benchmark similarity in real-world research activity. For paper writing, we reference two concurrent papers~\\citep{chen2025learning,jin2025search} recognized for presenting nearly identical ideas, yet with different writing styles and experiments, which yield a VoyageAI similarity of 0.8244. This suggests that scores above 0.82 can potentially indicate strong idea overlap. For review writing, we analyze the data of reviewers evaluating the same paper. The average inter-reviewer similarity is 0.5900 (strengths) and 0.5904 (weaknesses), reflecting natural variance in human judgment. These inter-similarity scores in the real world confirm that similarity scores represent realistic simulation.', 'type': 'textNode', 'isolation': False}
No paper found for  chen2025learning
No paper found for  jin2025search
text_node
{'id': 'text_172', 'cites': [], 'refs': [], 'content': "\\xhdr{More details on paper evaluation}More details on paper evaluation To evaluate the paper writing stage, we define a distance function $ d_p(\\cdot, \\cdot) $ d_p(\\cdot, \\cdot)  to measure the similarity between the generated paper $ \\mathbf{h}_v $ \\mathbf{h}_v  and the ground-truth paper $ \\mathbf{h}_v^* $ \\mathbf{h}_v^* . Since directly comparing full papers in different formats can be challenging and inaccurate, we align $ \\mathbf{h}_v $ \\mathbf{h}_v  and $ \\mathbf{h}_v^* $ \\mathbf{h}_v^*  into a unified format using a well-recognized framework~\\footnote{\\url{https://cs.stanford.edu/people/widom/paper-writing.html}} that summarizes the core components of a paper through five questions: (1) What is the problem? (2) Why is it interesting and important? (3) Why is it hard? (4) Why hasn't it been solved before? (5) What are the key components of my approach and results? We mark these questions as Q1-Q5 for short. By using an LLM-based summarization function $ f_\\text{sum}(\\cdot) $ f_\\text{sum}(\\cdot) , we convert the input papers into an aligned text-based list $ \\mathbf{a}_v = f_\\text{sum}(\\mathbf{h}_v) $ \\mathbf{a}_v = f_\\text{sum}(\\mathbf{h}_v)  and $ \\mathbf{a}^*_v = f_\\text{sum}(\\mathbf{h}^*_v) $ \\mathbf{a}^*_v = f_\\text{sum}(\\mathbf{h}^*_v) , where each element in $ \\mathbf{a}_v $ \\mathbf{a}_v  and $ \\mathbf{a}_v^* $ \\mathbf{a}_v^*  corresponds to the answer of one question mentioned above. The distance function for paper writing is formally defined as:\n\\begin{align}\n\\vspace{-1mm}\nd_p(\\mathbf{h}_v, \\mathbf{h}^*_v) = \\frac{1}{5} \\sum_{i=1}^{5} \\textsc{sim}(\\mathbf{a}_{v,i}, \\mathbf{a}^*_{v,i})\n\\label{paper-score}\n\\vspace{-1mm}\n\\end{align}\\begin{align}\n\\vspace{-1mm}\nd_p(\\mathbf{h}_v, \\mathbf{h}^*_v) = \\frac{1}{5} \\sum_{i=1}^{5} \\textsc{sim}(\\mathbf{a}_{v,i}, \\mathbf{a}^*_{v,i})\n\\label{paper-score}\n\\vspace{-1mm}\n\\end{align}\n\\vspace{-1mm}\nd_p(\\mathbf{h}_v, \\mathbf{h}^*_v) = \\frac{1}{5} \\sum_{i=1}i=1^{5}5 \\textsc{sim}(\\mathbf{a}_{v,i}v,i, \\mathbf{a}^*_{v,i}v,i)\n\\label{paper-score}\n\\vspace{-1mm}", 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_173', 'cites': [], 'refs': [], 'content': 'where $ \\textsc{sim}(\\cdot, \\cdot) $ \\textsc{sim}(\\cdot, \\cdot)  represents an embedding-based similarity metric, such as voyage-3~\\footnote{\\url{https://blog.voyageai.com/2024/09/18/voyage-3/}} and text-embedding-large-3~\\footnote{\\url{https://openai.com/index/new-embedding-models-and-api-updates/}}. By leveraging the LLM to generate structured embeddings for each question, this approach ensures a meaningful and consistent comparison of the generated and ground-truth papers.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_174', 'cites': [], 'refs': [], 'content': '\\xhdr{More details on review evaluation}More details on review evaluation Another research activity we aim to evaluate is review writing. Similar to paper writing evaluation, we project both real-world and generated reviews into a unified format for comparison. For this purpose, we adopt a bullet point-based format to represent weaknesses and advantages in the review, as it effectively captures the key aspects of a review. Using an LLM-based summarization function $ f_\\text{sum}(\\cdot) $ f_\\text{sum}(\\cdot) , we convert the input reviews $ \\mathbf{r}_v $ \\mathbf{r}_v  and $ \\mathbf{r}^*_v $ \\mathbf{r}^*_v  into a bullet point list $ \\mathbf{b}_v = f_\\text{sum}(\\mathbf{r}_v) $ \\mathbf{b}_v = f_\\text{sum}(\\mathbf{r}_v)  and $ \\mathbf{b}^*_v = f_\\text{sum}(\\mathbf{r}^*_v) $ \\mathbf{b}^*_v = f_\\text{sum}(\\mathbf{r}^*_v) , where each element of $ \\mathbf{b}_v $ \\mathbf{b}_v  and $ \\mathbf{b}_v^* $ \\mathbf{b}_v^*  corresponds to a bullet point of the review. Formally, the distance function for review writing is computed as:\n\\begin{align}\n\\vspace{-1mm}\nd_r(\\mathbf{r}_v, \\mathbf{r}^*_v) = \\frac{1}{n} \\sum_{j=1}^{n} \\max_{i} \\textsc{sim}(\\mathbf{b}_{v,i}, \\mathbf{b}^*_{v,j})\n\\label{review-score}\n\\vspace{-1mm}\n\\end{align}\\begin{align}\n\\vspace{-1mm}\nd_r(\\mathbf{r}_v, \\mathbf{r}^*_v) = \\frac{1}{n} \\sum_{j=1}^{n} \\max_{i} \\textsc{sim}(\\mathbf{b}_{v,i}, \\mathbf{b}^*_{v,j})\n\\label{review-score}\n\\vspace{-1mm}\n\\end{align}\n\\vspace{-1mm}\nd_r(\\mathbf{r}_v, \\mathbf{r}^*_v) = \\frac{1}{n} \\sum_{j=1}j=1^{n}n \\max_{i}i \\textsc{sim}(\\mathbf{b}_{v,i}v,i, \\mathbf{b}^*_{v,j}v,j)\n\\label{review-score}\n\\vspace{-1mm}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_175', 'cites': [], 'refs': [], 'content': 'where $ \\textsc{sim}(\\cdot, \\cdot) $ \\textsc{sim}(\\cdot, \\cdot)  refers to the same similarity metric in paper writing evaluation. This metric emphasizes the \\textit{recall} rate of the generated review by measuring whether each point in the real-world review is potentially included in the generated review. Since each review consists of both strengths and weaknesses, we compute separate similarity scores for strengths and weaknesses. Additionally, since both $ \\mathbf{r}_v $ \\mathbf{r}_v  and $ \\mathbf{r}_v^* $ \\mathbf{r}_v^*  include a final score $\\mb{S}_v$\\mb{S}S_v and $\\mb{S}^*_v$\\mb{S}S^*_v as attributes, we calculate $ \\Delta \\mb{S}_v = |\\mb{S}_v - \\mb{S}_v^*| $ \\Delta \\mb{S}S_v = |\\mb{S}S_v - \\mb{S}S_v^*|  to quantify the difference between the generated and real-world review scores.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_176', 'cites': [], 'refs': ['table_21', 'table_22'], 'content': '\\xhdr{Prompt}Prompt Table~\\ref{tab:Paper_Ground_Truth_Transform} presents the prompt used to convert any existing paper into responses to the five critical research questions. Similarly, Table~\\ref{tab:Review_Ground_Truth_Transform} shows the prompt used to transform any existing review into a bullet-point format. Both prompts ensure that the transformed papers and reviews are aligned with the generated ones, facilitating consistent evaluation in the same format. The transformed format for the paper is considered as $\\mb{a}_v^{*}$\\mb{a}a_v^{*}* and the concatenation of all ground-truth reviews is considered as $\\mb{b}_v^{*}$\\mb{b}b_v^{*}*, as mentioned in Section \\S\\ref{evaluation}.', 'type': 'textNode', 'isolation': False}
tab:Paper_Ground_Truth_Transform
tab:Review_Ground_Truth_Transform
text_node
{'id': 'text_177', 'cites': [], 'refs': [], 'content': '\\xhdr{Metric}Metric For our embedding-based similarity calculations, we use the \\textit{text-embedding-large-3} model via the \\texttt{litellm} Python package by calling \\texttt{litellm.embedding()}. For the \\textit{voyage-3} model, we rely on the \\texttt{voyageai} Python package by calling \\texttt{voyageai.Client().embed()}. We then compute the cosine similarity between the resulting embeddings to measure their similarity.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_178', 'cites': [], 'refs': [], 'content': '\\label{sec:llm-based-eval}\nIn this section, we provide more technical details about using LLM prompting for evaluation.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_179', 'cites': [], 'refs': [], 'content': '\\xhdr{Prompting for similarity}Prompting for similarity For prompting-based evaluation, we decompose overall similarity into six fine-grained dimensions: (1) topic consistency, (2) method consistency, (3) factual consistency, (4) claim consistency, (5) application context consistency, and (6) overall semantic similarity. These dimensions are designed to capture distinct yet complementary aspects of alignment between the generated and reference proposals, ranging from high-level research focus (such as topic and application context) to specific technical content (such as methods, facts, and claims). Importantly, they are intended to capture nuances that may not be easily detected by embedding-based models, enabling a more comprehensive and interpretable assessment than relying on a single similarity score. Each dimension is rated on a scale from 0 to 10.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_180', 'cites': [], 'refs': [], 'content': "\\xhdr{Prompting for novelty and feasibility}Prompting for novelty and feasibility In addition to measuring similarity, we prompt LLMs to assess two intrinsic quality dimensions: (1) novelty and (2) feasibility, which we consider essential characteristics of a strong research proposal. While similarity captures how well the generated content aligns with a reference, it does not fully reflect the proposal's originality or practicality. These intrinsic dimensions address that gap by evaluating the creativity of the proposed idea and its potential for real-world implementation. Each dimension is scored on a scale from 0 to 10, complementing similarity-based metrics for a more holistic evaluation.", 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_181', 'cites': [], 'refs': ['table_23'], 'content': '\\xhdr{Prompt}Prompt To enable efficient evaluation, we adopt parallel prompting, where both the reference and generated proposals are input to the LLM in a single prompt, along with all evaluation criteria. This allows the model to produce scores for all dimensions simultaneously in one forward pass. The detailed descriptions of these evaluation criteria and the full prompts are provided in Table~\\ref{tab:Eval_Prompt_Finegrained}.', 'type': 'textNode', 'isolation': False}
tab:Eval_Prompt_Finegrained
text_node
{'id': 'text_182', 'cites': [], 'refs': [], 'content': '\\label{sec:human-eval}\n\\xhdr{Annotator Information}Annotator Information\nWe recruit two graduate-level students with backgrounds in computer science and artificial intelligence. Both annotators have prior experience publishing in top-tier machine-learning conferences.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_183', 'cites': [], 'refs': [], 'content': '\\xhdr{Annotated Data}Annotated Data\nWe randomly sample 40 reference proposals and their corresponding generated proposals from \\textsc{PaperBench} for human evaluation. We ask annotators to annotate on overall similarity, novelty, and feasibility.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_184', 'cites': [], 'refs': [], 'content': '\\xhdr{Annotation Process}Annotation Process\nThe annotation process consists of three stages: (1) preliminary annotation, (2) discussion, and (3) final annotation. In the preliminary stage, each annotator independently labels 10 examples. They then meet to discuss discrepancies, align their understanding, and refine the annotation criteria. Based on this discussion, they proceed to annotate the official 40 examples using the agreed-upon guidelines as the final results.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_185', 'cites': [], 'refs': [], 'content': '\\xhdr{Annotation Instructions}Annotation Instructions\nAt the start, annotators receive the same input information as used in the LLM-based prompting setup. During the discussion phase, they collaboratively develop more detailed and consistent annotation guidelines to ensure alignment in their final evaluations.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_186', 'cites': [], 'refs': [], 'content': '\\label{ablation-study-details}\nDue to the experimental setting, the ablation study on paper writing simulation tasks does not include all the 1,000 tasks that existed in \\benchname. Therefore, we provide detailed explanations and technical details for this.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_187', 'cites': [], 'refs': [], 'content': '\\xhdr{Data for paper-writing researcher number ablation.}Data for paper-writing researcher number ablation.  \nNot all papers in \\benchname have more than five authors. To ablate the effect of the number of researchers (1 to 5), we select a subset from the hard part of \\textsc{PaperBench} within \\benchname, including 333 paper writing tasks, ensuring each paper has more than five authors. This filtering results in a subset of 172 paper-writing tasks. We focus on the hard subset because we believe that involving multiple research agents in more challenging scenarios yields a more significant difference in performance.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_188', 'cites': [], 'refs': [], 'content': '\\xhdr{Data for paper-writing paper number ablation.}Data for paper-writing paper number ablation.  \nIn this ablation, we vary the number of cited papers included in different sections of the target paper. Specifically, we examine citations in the related work, introduction, and other sections. To do this, we retrieve the raw \\LaTeX{} source from arXiv and extract references at the section level. Due to varying data availability, we finalize a subset of \\benchname that includes 296 paper-writing tasks for this study.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_189', 'cites': [], 'refs': [], 'content': '\\xhdr{Data for review-writing researcher number ablation.}Data for review-writing researcher number ablation.  \nSince the reviewer construction does not depend on any complex data preprocessing, we do not encounter data issues for the review-writing ablation. Consequently, the ablation results are based on all 200 review-writing tasks in \\benchname.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_190', 'cites': [], 'refs': [], 'content': '\\label{additional-exp-results}\nWe provide more comprehensive experimental results on each sub-part of \\textsc{ResearchBench} (\\textsc{PaperBench}, \\textsc{ReviewBench}, \\textsc{HighImpactPaperBench}) in this section.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_191', 'cites': [], 'refs': ['table_11'], 'content': '\\xhdr{Additional Results on \\textsc{PaperBench}}Additional Results on \\textsc{PaperBench}\nTable~\\ref{tab:paperbench-all-in-one} shows that all models—Qwen-2.5-7B-Instruct, GPT-4o-mini, and Deepseek-v3—consistently achieve better performance with richer reference contexts (AGG-data and AGG-global) compared to narrower ones (AGG-self and AGG-agent), highlighting the importance of contextual information in similarity evaluation.', 'type': 'textNode', 'isolation': False}
tab:paperbench-all-in-one
text_node
{'id': 'text_192', 'cites': [], 'refs': ['table_12'], 'content': '\\xhdr{Additional Results on \\textsc{ReviewBench}}Additional Results on \\textsc{ReviewBench}\nAs shown in Table~\\ref{tab:review-writing-result-appendix}, voyage-3 embeddings yield higher strength scores and larger $\\Delta \\mb{S}$\\Delta \\mb{S}S values than text-embedding-3, indicating greater discriminative power. While Qwen-2.5-7B-Instruct maintains strong similarity scores across all aggregation types, it exhibits larger deviations from human scores, suggesting potential scoring bias or overconfidence in its own outputs.', 'type': 'textNode', 'isolation': False}
tab:review-writing-result-appendix
text_node
{'id': 'text_193', 'cites': ['ba2016layer', 'strubell2019energy', 'jin2020bert', 'qi2020stanza'], 'refs': [], 'content': "\\xhdr{Additional results on \\textsc{HighImpactPaperBench}}Additional results on \\textsc{HighImpactPaperBench}\nBesides the full results on \\textsc{PaperBench}, we also evaluate \\envname under extreme conditions by attempting to simulate 100 of the most-cited machine learning papers from the past decade. \\envname achieves low similarity scores for papers introducing groundbreaking methods, such as ``\\textit{Layer Normalization}''~\\citep{ba2016layer}, or novel topics, such as ``\\textit{Energy and Policy Considerations for Deep Learning in NLP}''~\\citep{strubell2019energy}. However, the framework performs notably better on impactful papers focused on analysis or tool development. For instance, it achieves a similarity score exceeding 0.8 for papers like ``\\textit{Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment}''~\\citep{jin2020bert}, which provides adversarial analysis, and ``\\textit{Stanza: A Python Natural Language Processing Toolkit for Many Human Languages}''~\\citep{qi2020stanza}, which offers a practical toolkit. These results suggest that high-impact research ideas may be more feasible than commonly perceived, and \\envname could potentially serve as a tool to inspire future impactful research.", 'type': 'textNode', 'isolation': False}
No paper found for  ba2016layer
No paper found for  strubell2019energy
No paper found for  jin2020bert
No paper found for  qi2020stanza
text_node
{'id': 'text_194', 'cites': [], 'refs': [], 'content': '\\label{tab:paperbench-all-in-one}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_195', 'cites': [], 'refs': [], 'content': '\\label{tab:review-writing-result-appendix}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_196', 'cites': [], 'refs': [], 'content': '\\label{additional-case-study}\nBeyond the examples included in Case Study Section \\S\\ref{case-study-section}, we provide additional examples to show the generation results of our work and provide further insights about the strengths and weaknesses of \\envname.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_197', 'cites': [], 'refs': ['table_24', 'table_25', 'table_26', 'table_27', 'table_28'], 'content': '\\xhdr{Additional case study for in-distribution evaluation}Additional case study for in-distribution evaluation\nTables \\ref{tab:paperbench easy}, \\ref{tab:paperbench mid}, \\ref{tab:paperbench hard}, \\ref{tab:oodbench}, and \\ref{tab:reviewbench} present examples of tasks and their corresponding outputs for the in-distribution evaluation of \\envname. These examples illustrate the evaluation process defined in this work.', 'type': 'textNode', 'isolation': False}
tab:paperbench easy
tab:paperbench mid
tab:paperbench hard
tab:oodbench
tab:reviewbench
text_node
{'id': 'text_198', 'cites': [], 'refs': ['table_29', 'table_30', 'table_31', 'table_32', 'table_33', 'table_34', 'table_35', 'table_36', 'table_37', 'table_38'], 'content': '\\xhdr{Additional case study for out-of-distribution application}Additional case study for out-of-distribution application\n\\label{more-example}\nIn Table \\ref{tab:LLM+Astronomy}, \\ref{tab:LLM+Criminology}, \\ref{tab:LLM+Biology}, \\ref{tab:System+Biology}, \\ref{tab:Math+Criminology}, \\ref{tab:LLM+Math+Criminology}, \\ref{tab:System+Biology+Criminology}, \\ref{tab:LLM+Biology+Criminology}, \\ref{tab:Astronomy+Biology+Criminology }, and \\ref{tab:LLM+Astronomy+Biology+Criminology }, we show examples of the inputs and outputs of the out-of-distribution application of \\envname. Additionally, each table caption includes a brief comment on the quality of the generated papers for reference.', 'type': 'textNode', 'isolation': False}
tab:LLM+Astronomy
tab:LLM+Criminology
tab:LLM+Biology
tab:System+Biology
tab:Math+Criminology
tab:LLM+Math+Criminology
tab:System+Biology+Criminology
tab:LLM+Biology+Criminology
tab:Astronomy+Biology+Criminology 
tab:LLM+Astronomy+Biology+Criminology 
text_node
{'id': 'text_199', 'cites': [], 'refs': [], 'content': '\\newpage', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_200', 'cites': [], 'refs': [], 'content': '\\caption{Paper reading message prompt template for $f_u(\\cdot)$.}\n\\label{tab:paper-reading-prompt}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_201', 'cites': [], 'refs': [], 'content': '\\caption{Paper writing message prompt template for $f_a(\\cdot)$.}\n\\label{tab:Paper_Writing_Prompt}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_202', 'cites': [], 'refs': [], 'content': '\\caption{Paper writing aggregation prompt template for $f_g(\\cdot)$.}\n\\label{tab:Paper_Writing_Summary_Prompt}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_203', 'cites': [], 'refs': [], 'content': '\\caption{Review writing (strength) message prompt template for $f_u(\\cdot)$.}\n\\label{tab:Agent_Review_Strength_Writing_Prompt}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_204', 'cites': [], 'refs': [], 'content': '\\caption{Review writing (weakness) message prompt template for $f_u(\\cdot)$.}\n\\label{tab:Agent_Review_Weakness_Writing_Prompt}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_205', 'cites': [], 'refs': [], 'content': '\\caption{Review writing (score) message prompt template for $f_u(\\cdot)$.}\n\\label{tab:Agent_Review_Scoring_Prompt}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_206', 'cites': [], 'refs': [], 'content': '\\caption{Review writing (strength) aggregation prompt template for $f_g(\\cdot)$.}\n\\label{tab:Agent_Metareview_Strength_Writing_Prompt}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_207', 'cites': [], 'refs': [], 'content': '\\caption{Review writing (weakness) aggregation prompt template for $f_g(\\cdot)$.}\n\\label{tab:Agent_Metareview_Weakness_Writing_Prompt}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_208', 'cites': [], 'refs': [], 'content': '\\caption{Format transformative prompt for real-world papers.}\n\\label{tab:Paper_Ground_Truth_Transform}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_209', 'cites': [], 'refs': [], 'content': '\\caption{Format transformative prompt for real-world reviews.}\n\\label{tab:Review_Ground_Truth_Transform}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_210', 'cites': [], 'refs': [], 'content': '- Topic Consistency: Do both proposals address the same research topic or problem area?\\newline\n- Method Consistency: Are the research methods and approaches used in both proposals similar?\\newline\n- Factual Consistency: Are the datasets, metrics, and models mentioned consistent between the two proposals?\\newline\n- Claim Consistency: Do both proposals present similar conclusions or findings?\\newline\n- Application Context Consistency: Are the application domains or use-cases targeted by both proposals the same?\\newline\n- Overall Semantic Similarity: Do the two proposals convey the same overall idea or message?\\newline', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_211', 'cites': [], 'refs': [], 'content': 'Additionally, assess the following for each proposal individually:\\newline\n- Novelty of Reference Proposal: Does the reference proposal introduce new ideas or perspectives?\\newline\n- Feasibility of Reference Proposal: How realistic and implementable is the reference proposal based on its described methods and objectives?\\newline\n- Novelty of Generated Proposal: Does the generated proposal introduce new ideas or perspectives?\\newline\n- Feasibility of Generated Proposal: How realistic and implementable is the generated proposal based on its described methods and objectives?\\newline', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_212', 'cites': [], 'refs': [], 'content': '- Topic Consistency: Do both proposals address the same research topic or problem area?\\newline\n- Method Consistency: Are the research methods and approaches used in both proposals similar?\\newline\n- Factual Consistency: Are the datasets, metrics, and models mentioned consistent between the two proposals?\\newline\n- Claim Consistency: Do both proposals present similar conclusions or findings?\\newline\n- Application Context Consistency: Are the application domains or use-cases targeted by both proposals the same?\\newline\n- Overall Semantic Similarity: Do the two proposals convey the same overall idea or message?\\newline', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_213', 'cites': [], 'refs': [], 'content': 'Additionally, assess the following for each proposal individually:\\newline\n- Novelty of Reference Proposal: Does the reference proposal introduce new ideas or perspectives?\\newline\n- Feasibility of Reference Proposal: How realistic and implementable is the reference proposal based on its described methods and objectives?\\newline\n- Novelty of Generated Proposal: Does the generated proposal introduce new ideas or perspectives?\\newline\n- Feasibility of Generated Proposal: How realistic and implementable is the generated proposal based on its described methods and objectives?\\newline', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_214', 'cites': [], 'refs': [], 'content': '\\caption{Prompt for LLM-based evaluation of fine-grained similarity and quality assessment of research proposals.}\n\\label{tab:Eval_Prompt_Finegrained}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_215', 'cites': [], 'refs': [], 'content': '\\caption{Case study on paper writing results of \\textsc{PaperBench}-easy.}\n\\label{tab:paperbench easy}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_216', 'cites': [], 'refs': [], 'content': '\\caption{Case study on paper writing results of \\textsc{PaperBench}-medium.}\n\\label{tab:paperbench mid}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_217', 'cites': [], 'refs': [], 'content': '\\caption{Case study on paper writing results of \\textsc{PaperBench}-hard.}\n\\label{tab:paperbench hard}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_218', 'cites': [], 'refs': [], 'content': '\\caption{Case study on paper writing results of \\textsc{HighImpactPaperBench}.}\n\\label{tab:oodbench}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_219', 'cites': [], 'refs': [], 'content': '\\caption{Case study on review writing results of \\textsc{ReviewBench}.}\n\\label{tab:reviewbench}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_220', 'cites': [], 'refs': [], 'content': '\\caption{Case study on using \\envname to write interdisciplinary research papers combining LLM and Astronomy. The idea creatively applies modeling techniques from astrophysics to explore how language styles evolve over time.}\n\\label{tab:LLM+Astronomy}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_221', 'cites': [], 'refs': [], 'content': '\\caption{Case study on using \\envname to write interdisciplinary research papers combining LLM and Criminology. The idea creatively utilizes a multimodal LLM to integrate qualitative narrative analysis with real-time speech translation, aiming to enhance communication for communities impacted by mass incarceration.}\n\\label{tab:LLM+Criminology}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_222', 'cites': [], 'refs': [], 'content': '\\caption{Case study on using \\envname to write interdisciplinary research papers combining LLM and Biology. The idea integrates patterns of inherited traits with generated retrieval methods to study and improve how language models grow and perform over time.}\n\\label{tab:LLM+Biology}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_223', 'cites': [], 'refs': [], 'content': '\\caption{Case study on using \\envname to write interdisciplinary research papers combining System and Biology. The idea attempts to build a hybrid system combining genetic variation models and IoT protocols for resilient crop breeding, but it risks being overshadowed by excessive terminologies.}\n\\label{tab:System+Biology}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_224', 'cites': [], 'refs': [], 'content': '\\caption{Case study on using \\envname to write interdisciplinary research papers combining Math and Criminology. Due to the two fields being too far apart conceptually, the generated idea primarily focuses on mathematical methods, with minimal incorporation of criminology insights.}\n\\label{tab:Math+Criminology}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_225', 'cites': [], 'refs': [], 'content': '\\caption{Case study on using \\envname to write interdisciplinary research papers combining LLM, Math, and Criminology. The idea focuses on modeling social network dynamics in child welfare interventions by integrating a series of mathematical concepts. The practicability of the method remains questioned due to its heavy reliance on complex mathematical frameworks.}\n\\label{tab:LLM+Math+Criminology}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_226', 'cites': [], 'refs': [], 'content': '\\caption{Case study on using \\envname to write interdisciplinary research papers combining System, Biology, and Criminology. The idea investigates "ghost networks" in marginalized communities, exploring how systemic disruptions and mass incarceration affect perceptions of safety, justice, and social cohesion while incorporating the role of policing technologies in this dynamic.}\n\\label{tab:System+Biology+Criminology}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_227', 'cites': [], 'refs': [], 'content': '\\caption{Case study on using \\envname to write interdisciplinary research papers combining LLM, Biology, and Criminology. The idea offers a novel interdisciplinary approach that developing an online platform that detects online toxicity in real-time while addressing its societal impacts on marginalized communities.}\n\\label{tab:LLM+Biology+Criminology}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_228', 'cites': [], 'refs': [], 'content': '\\caption{Case study on using \\envname to write interdisciplinary research papers combining Astronomy, Biology, and Criminology. Due to the significant conceptual gap between the three fields, the generated idea heavily leans on terminology accumulation.}\n\\label{tab:Astronomy+Biology+Criminology }', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_229', 'cites': [], 'refs': [], 'content': '\\caption{Case study on using \\envname to write interdisciplinary research papers combining LLM, Astronomy, Biology, and Criminology. Due to combining researchers and papers from too many diverse domains, the generated idea becomes an incoherent mix of terms without a clear focus or practical direction.}\n\\label{tab:LLM+Astronomy+Biology+Criminology }', 'type': 'textNode', 'isolation': True}
Paper count:  1
Total nodes:  216
Total edges:  363
Paper nodes:  1
Figure nodes:  0
Table nodes:  24
Text nodes:  191
2
SEMANTIC_SCHOLAR_API_KEY not set in .env
SEMANTIC_SCHOLAR_API_KEY not set in .env
arxiv ids: {'2412.17767v2'}
seed: ['2412.17767v2']
cnt: 1
BFS_que.qsize(): 1
current paper: 2412.17767v2
Thread 12953268224 Processing 2412.17767v2
Thread 8609423936 Finished processing 1 papers
File path: download/output/2412.17767v2.json
Time of loading metadata: 0.0006819579999994829
Time of constructing paper json file: 0.0030647079999983617
Time of finding authors and adding authors to database: 7.090000000431473e-07
Time of adding figures, tables, and sections to database: 0.0686110830000004
Time of searching arxiv id of cited paper (if not provided) and adding citation information to database: 0.009762124999999955
['download/output/2412.17767v2.json']
2412.17767v2
text_node
{'id': 'text_39', 'cites': ['AI4Science2023TheIO'], 'refs': [], 'content': 'LLMs have proved to be powerful copilots in scientific research~\\citep{AI4Science2023TheIO}, demonstrating their great potential for accelerating scientific discovery.\nDespite the promising finding, a more ambitious question remains: \\textit{Can we simulate the human research community with LLMs}? Answering such a question has multiple benefits: (1) simulating the human research community helps understand the underlying process behind the discovery of existing research ideas; (2) it can further help democratize and accelerate the discovery process of new research ideas.', 'type': 'textNode', 'isolation': False}
No paper found for  AI4Science2023TheIO
text_node
{'id': 'text_40', 'cites': ['zhou2023sotopia', 'Gao2023S3SS', 'hua2023war', 'xu2023language', 'girotra2023ideas', 'baek2024researchagent', 'huang2024mlagentbench', 'lu2024ai'], 'refs': [], 'content': 'However, simulating the human research community is challenging, as it involves leveraging multiple LLM agents to interact with complex research data. While existing multi-agent LLM frameworks have been successfully applied to areas like social simulation~\\citep{zhou2023sotopia,Gao2023S3SS} and game simulation~\\citep{hua2023war,xu2023language}, they are not well-suited for simulating research communities due to the complexity of collaborative research activities like paper writing and review writing. Although recent efforts have explored research automation using LLMs, these frameworks are typically limited to specific research tasks, such as idea generation~\\citep{girotra2023ideas, baek2024researchagent} or code experimentation~\\citep{huang2024mlagentbench}, or focus on simulating single-agent workflows~\\citep{lu2024ai}. These frameworks cannot simulate collaborative research activities where researchers with diverse backgrounds work together to brainstorm ideas, review papers, etc—processes that are fundamental to modern human research.', 'type': 'textNode', 'isolation': False}
No paper found for  zhou2023sotopia
No paper found for  Gao2023S3SS
No paper found for  hua2023war
No paper found for  xu2023language
No paper found for  girotra2023ideas
No paper found for  baek2024researchagent
No paper found for  huang2024mlagentbench
No paper found for  lu2024ai
text_node
{'id': 'text_41', 'cites': ['newman2001structure', 'Tang2008ArnetMinerEA', 'holm2020longitudinal', 'West2016ARS', 'Yang2012DefiningAE'], 'refs': [], 'content': '\\xhdr{Research community as graph}Research community as graph\nOur key observation is that the deeply interconnected research community can be naturally represented as graphs. Indeed, similar graph structures like citation networks~\\citep{newman2001structure} and academic social networks~\\citep{Tang2008ArnetMinerEA} have been extensively studied within data mining research, with proven values in applications such as citation prediction~\\citep{holm2020longitudinal}, recommendation~\\citep{West2016ARS}, and community detection~\\citep{Yang2012DefiningAE}.\nHowever, introducing LLMs to a graph-structured research community can extend these previous works from prediction and analysis with existing data to dynamic simulation and real-time forecasting.', 'type': 'textNode', 'isolation': False}
No paper found for  newman2001structure
No paper found for  Tang2008ArnetMinerEA
No paper found for  holm2020longitudinal
No paper found for  West2016ARS
No paper found for  Yang2012DefiningAE
text_node
{'id': 'text_42', 'cites': ['wei2023larger', 'lee2024reasoning'], 'refs': ['figures_2'], 'content': '\\xhdr{Novel framework for research simulation}Novel framework for research simulation\nIn this work, we propose \\envname, a simulator of the human research community. To bridge the gap between existing multi-agent simulation frameworks and the complexity of research activities, we propose a graph-based framework, inspired by the message-passing mechanism in Graph Neural Networks (GNNs), for multi-agent simulation.\nConcretely, as shown in Figure \\ref{fig:community-graph}, we propose a new concept of \\textit{agent-data graph} with 2 generic types of nodes: (1) \\textit{agent} nodes, suitable for entities like agents; (2) \\textit{data} nodes, suitable for entities such as papers, reviews, and blogs. \nAgent-data graphs are unique from standard heterogeneous graphs; here, the key conceptual difference between agent and data nodes is that an agent node can be considered a function over data nodes.\nTo inference on agent-data graphs, we propose a \\textit{TextGNN} framework where message-passing processes are defined based on text-form information processing with LLMs, thanks to their strong in-context learning~\\citep{wei2023larger} and reasoning~\\citep{lee2024reasoning} ability. \nWe apply the proposed agent-data graph and TextGNN to the research simulation. Here, a research community can be regarded as a special form of agent-data graph, called \\textit{community graph}, with research agents and research papers as two types of nodes, and we consider three types of edges (review, author, and cite) in the graph. Different community activities, such as paper writing and review writing, can be modeled as special message-passing processes on the community graph.', 'type': 'textNode', 'isolation': False}
No paper found for  wei2023larger
No paper found for  lee2024reasoning
text_node
{'id': 'text_43', 'cites': ['si2024can', 'hu2024nova'], 'refs': [], 'content': '\\xhdr{Novel evaluation for research simulation}Novel evaluation for research simulation \nWith \\envname for research simulation, a further research question is to evaluate the quality of that. Prior works primarily use human evaluation with breakdown metrics such as novelty, excitement, feasibility, and expected effectiveness~\\citep{si2024can,hu2024nova}. These approaches inevitably suffer from subjectiveness and high costs. In our work, since \\envname functions as a simulator, our primary focus is on measuring how closely its outputs align with those of the real-world research community. Community graphs naturally provide a similarity-based evaluation method by masking a given paper node in the community graph and evaluating whether a simulator can reconstruct the masked nodes. This definition focuses on simulation similarity, making it scalable and objective. Based on such a node masking prediction task, we build a benchmark called \\benchname with 1,000 paper writing tasks and 200 review writing tasks requiring multi-agent collaboration.', 'type': 'textNode', 'isolation': False}
No paper found for  si2024can
No paper found for  hu2024nova
text_node
{'id': 'text_44', 'cites': [], 'refs': [], 'content': '\\xhdr{Main discoveries}Main discoveries Based on the evaluation results from \\benchname, we highlight three key findings: (1) \\envname effectively simulates collaborative research activities, achieving an average similarity score of 0.68 for paper writing and 0.49 for review writing, as measured by the state-of-the-art text embedding model; (2) \\envname demonstrates robustness and effectiveness in research simulation, showing improvement when more agents are added and maintaining performance when including unrelated papers; (3) \\envname inspires interdisciplinary research, generating innovative ideas that combine insights from NLP, criminology, and astronomy and does not exist in the real-world research.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_45', 'cites': [], 'refs': [], 'content': '\\xhdr{Stressing ethical concerns}Stressing ethical concerns As our work targets simulating the human research community, multiple ethical concerns, including facilitating research plagiarism and producing low-quality or misleading claims, appear. These ethical concerns are addressed in detail in Appendix~\\S\\ref{appendix:ethical}.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_46', 'cites': [], 'refs': [], 'content': '\\vspace{-3mm}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_47', 'cites': ['yang2021graphformers', 'he2023explanations', 'Zhao2022LearningOL', 'Chen2023LabelfreeNC', 'yan2023comprehensive'], 'refs': [], 'content': '\\xhdr{Graphs with text attributes}Graphs with text attributes In real-world graph tasks, nodes often have textual attributes to carry richer information, forming text-attributed graphs (TAGs)~\\citep{yang2021graphformers, he2023explanations}. \nPrevious work on TAGs mainly treats LLMs as tools for understanding text attributes and aims at achieving co-training LLMs and GNNs~\\citep{Zhao2022LearningOL,Chen2023LabelfreeNC}. In contrast, our approach incorporates agent nodes into the graph, enabling text-based message passing between agent nodes and data nodes. Furthermore, while previous TAG research mainly focuses on node prediction and link prediction tasks~\\citep{yan2023comprehensive}, \\envname extends it to both the reconstruction of existing nodes and the prediction of new, non-existent nodes.', 'type': 'textNode', 'isolation': False}
No paper found for  yang2021graphformers
No paper found for  he2023explanations
No paper found for  Zhao2022LearningOL
No paper found for  Chen2023LabelfreeNC
No paper found for  yan2023comprehensive
text_node
{'id': 'text_48', 'cites': ['zhuge2024language', 'martinkus2022agent', 'hu2024learning'], 'refs': [], 'content': '\\xhdr{Graphs for multi-agent modeling}Graphs for multi-agent modeling Recent works model multi-agent communication using graphs and develop learnable methods to optimize the communication process~\\citep{zhuge2024language, martinkus2022agent, hu2024learning}. However, these works often neglect the interactive nature of data, where agents can read, write, and update shared data iteratively. Currently, few works include a well-defined framework to represent graphs that integrate both agents and their associated data.', 'type': 'textNode', 'isolation': False}
No paper found for  zhuge2024language
No paper found for  martinkus2022agent
No paper found for  hu2024learning
text_node
{'id': 'text_49', 'cites': [], 'refs': [], 'content': '\\vspace{-2mm}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_50', 'cites': [], 'refs': [], 'content': '\\label{sec:community-graph-design}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_51', 'cites': [], 'refs': [], 'content': '\\xhdr{Definition of agent-data graphs}Definition of agent-data graphs\nTo initiate our discussion, we formally define the proposed agent-data graph. An agent-data graph is a special type of heterogeneous graph $ \\mathcal{G} = (\\mathcal{V}, \\mathcal{E}) $ \\mathcal{G} = (\\mathcal{V}, \\mathcal{E}) , where $ \\mathcal{V} = \\mathcal{V}_a \\cup \\mathcal{V}_d $ \\mathcal{V} = \\mathcal{V}_a \\cup \\mathcal{V}_d  is the node set consisting of two types of nodes, agent nodes and data nodes, and $\\mathcal{E} = \\mathcal{E}_{aa} \\cup \\mathcal{E}_{ad} \\cup \\mathcal{E}_{dd}$\\mathcal{E} = \\mathcal{E}_{aa}aa \\cup \\mathcal{E}_{ad}ad \\cup \\mathcal{E}_{dd}dd is the edge set consisting of three types of relations, agent-agent, data-data, and agent-data interactions.\nHere, each data node $v \\in \\mathcal{V}_d$v \\in \\mathcal{V}_d comes with attributes, \\eg, a piece of text, $\\mathbf{x}_v$\\mathbf{x}_v; each agent node $u$u is accompanied with an \\textit{agent function}, \\eg, an LLM $f_u(\\cdot)$f_u(\\cdot) with its prompt template and the profile. Each agent function is responsible for two types of tasks: message generation and message aggregation. More details about agent functions are in Appendix~\\S\\ref{agent-function-implementation}. Without loss of generality, we assume that the data nodes have text attributes, and leave the multi-modal extension of our work, \\eg, images, audio, and videos, to future works.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_52', 'cites': [], 'refs': [], 'content': '\\xhdr{Uniqueness of agent-data graphs}Uniqueness of agent-data graphs\nUnlike standard heterogeneous graphs, the uniqueness of an agent-data graph is that the agent nodes take functions as their attributes, rather than embeddings. Concretely, each agent node could take a piece of text, \\eg, $\\mathbf{x}_v$\\mathbf{x}_v from one data node, as the input and output new data based on its profile prompt $\\mathbf{x}_u$\\mathbf{x}_u, \\eg, $\\mathbf{x}_{uv} = f_u([\\mathbf{x}_u, \\mathbf{x}_v])$\\mathbf{x}_{uv}uv = f_u([\\mathbf{x}_u, \\mathbf{x}_v]) where $[\\cdot]$[\\cdot] indicates filling the prompt template with $\\mathbf{x}_u$\\mathbf{x}_u and $\\mathbf{x}_v$\\mathbf{x}_v. Such definition greatly facilitates the multi-agent scenarios where agents could communicate among themselves, with edge type $\\mathcal{E}_{aa}$\\mathcal{E}_{aa}aa; interacting with the environment, with edge type $\\mathcal{E}_{ad}$\\mathcal{E}_{ad}ad; representing the inherent data relationships within an environment $\\mathcal{E}_{dd}$\\mathcal{E}_{dd}dd.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_53', 'cites': [], 'refs': ['figures_2'], 'content': '\\xhdr{Example of agent-data graphs}Example of agent-data graphs Figure~\\ref{fig:community-graph} shows an example of the agent-data graph. Its definition could be extended to more node types (\\eg, codebase, blogs) and edge types (\\eg, attend, post, commit). Typically, one blog post can be directly connected to multiple researchers, papers, and other blog posts if they are related to each other.', 'type': 'textNode', 'isolation': False}
text_node
{'id': 'text_54', 'cites': [], 'refs': [], 'content': '\\label{sec:text-gnn}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_55', 'cites': [], 'refs': [], 'content': '\\xhdr{TextGNN motivations}TextGNN motivations\nThe agent-data graph $\\mathcal{G}$\\mathcal{G} provides a platform for expressing a complex multi-agent scenario, \\eg, a human research community.\nTo further simulate based on a given real-world agent-data graph, we need agentic models, \\eg, LLMs, to generate new data and interactions on the agent-data graph.\nTo this end, motivated by the message-passing algorithm in GNNs, we proposed a text-based message-passing mechanism on an agent-data graph, called \\textit{TextGNN}, where all hidden states are defined in the text space instead of the embedding space.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_56', 'cites': [], 'refs': [], 'content': '\\xhdr{Recap: message passing in standard GNN}Recap: message passing in standard GNN \nIn standard GNNs, input features $\\mb{x}_v$\\mb{x}x_v are used to initialize the initial states $\\mb{x}_v = \\mb{h}_v^{(0)}$\\mb{x}x_v = \\mb{h}h_v^{(0)}(0). Afterward, the goal is to learn useful node embeddings \\( \\mb{h}_v \\) \\mb{h}h_v  by iteratively aggregating information from local neighborhoods. Hidden states, message functions, and aggregation functions are the three main components in one GNN layer. The \\( k \\) k -th iteration of message passing (or the \\( k \\) k -th GNN layer) is typically defined as:', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_57', 'cites': [], 'refs': [], 'content': '\\begingroup\n\\small\n\\begin{equation}\n    \\label{eq:gnn}\n    \\mb{m}_u^{(k)} = \\textsc{MSG}^{(k)}(\\mb{h}_u^{(k-1)})\n\\end{equation}\\begin{equation}\n    \\label{eq:gnn}\n    \\mb{m}_u^{(k)} = \\textsc{MSG}^{(k)}(\\mb{h}_u^{(k-1)})\n\\end{equation}\n    \\label{eq:gnn}\n    \\mb{m}m_u^{(k)}(k) = \\textsc{MSG}^{(k)}(k)(\\mb{h}h_u^{(k-1)}(k-1))', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_58', 'cites': [], 'refs': [], 'content': '\\endgroup\n\\begingroup\n\\small\n\\begin{equation}\n    \\label{eq:gnn2}\n    \\mb{h}_v^{(k)} = \\textsc{AGG}^{(k)}\\big(\\mb{h}_v^{(k-1)}, \\{\\mb{m}_u^{(k)} \\mid u \\in \\mathcal{N}(v)\\}\\big)\n\\end{equation}\\begin{equation}\n    \\label{eq:gnn2}\n    \\mb{h}_v^{(k)} = \\textsc{AGG}^{(k)}\\big(\\mb{h}_v^{(k-1)}, \\{\\mb{m}_u^{(k)} \\mid u \\in \\mathcal{N}(v)\\}\\big)\n\\end{equation}\n    \\label{eq:gnn2}\n    \\mb{h}h_v^{(k)}(k) = \\textsc{AGG}^{(k)}(k)\\big(\\mb{h}h_v^{(k-1)}(k-1), \\{\\mb{m}m_u^{(k)}(k) \\mid u \\in \\mathcal{N}(v)\\}\\big)', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_59', 'cites': [], 'refs': [], 'content': '\\endgroup\nwhere \\( \\mb{h}_v^{(k)} \\) \\mb{h}h_v^{(k)}(k)  is the node embedding at the \\( k \\) k -th layer, \\( \\mb{h}_v^{(0)} = \\mb{x}_v \\) \\mb{h}h_v^{(0)}(0) = \\mb{x}x_v  is the initial node feature, and \\( \\mathcal{N}(v) \\) \\mathcal{N}(v)  is the set of neighbors of node \\( v \\) v . \\(\\textsc{MSG}^{(k)}(\\cdot)\\)\\textsc{MSG}^{(k)}(k)(\\cdot) is a transformative function to convert the hidden states of one node into a message for aggregation. \\(\\textsc{AGG}^{(k)}(\\cdot)\\)\\textsc{AGG}^{(k)}(k)(\\cdot) is defined to update the hidden states of a node based on neighborhood messages. More generally, we can broadly consider the $k$k-th layer of GNN to be an aggregation function that implicitly includes message functions inside:', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_60', 'cites': [], 'refs': [], 'content': '\\begingroup\n\\small\n\\begin{equation}\n\\mb{h}_v^{(k)} = \\textsc{AGG}^{(k)}\\big(\\mb{h}_v^{(k-1)}, \\{\\mb{h}_u^{(k-1)} \\mid u \\in \\mathcal{N}(v)\\}\\big)\n\\end{equation}\\begin{equation}\n\\mb{h}_v^{(k)} = \\textsc{AGG}^{(k)}\\big(\\mb{h}_v^{(k-1)}, \\{\\mb{h}_u^{(k-1)} \\mid u \\in \\mathcal{N}(v)\\}\\big)\n\\end{equation}\n\\mb{h}h_v^{(k)}(k) = \\textsc{AGG}^{(k)}(k)\\big(\\mb{h}h_v^{(k-1)}(k-1), \\{\\mb{h}h_u^{(k-1)}(k-1) \\mid u \\in \\mathcal{N}(v)\\}\\big)', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_61', 'cites': [], 'refs': [], 'content': '\\endgroup', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_62', 'cites': [], 'refs': [], 'content': '\\xhdr{Message passing in TextGNN}Message passing in TextGNN Following the message-passing process in the standard GNN, we now define a general form of the aggregation function to describe the text-based message-passing process on an agent-data graph $\\mathcal{G}$\\mathcal{G}. The key difference between a standard GNN and a TextGNN is that all hidden states in the standard GNN are defined in the embedding space ($\\mb{h}_v \\in \\mathbb{R}^d$\\mb{h}h_v \\in \\mathbb{R}^d) while those in TextGNN are defined in the text space ($\\mb{h}_v \\in \\Sigma^{*}$\\mb{h}h_v \\in \\Sigma^{*}*). In a TextGNN, we first set the initial hidden states for data nodes $\\mb{h}_v^{(0)} = \\mb{x}_v$\\mb{h}h_v^{(0)}(0) = \\mb{x}x_v, where $\\mb{x}_v$\\mb{x}x_v are text attributes, and the initial hidden states for agent nodes is empty $\\mb{h}_u^{(0)} = \\emptyset$\\mb{h}h_u^{(0)}(0) = \\emptyset. Next, we design a general form of message passing function that handles three distinctive types of interaction, including agent-agent $\\mathcal{E}_{aa}$\\mathcal{E}_{aa}aa, agent-data $\\mathcal{E}_{ad}$\\mathcal{E}_{ad}ad, and data-data $\\mathcal{E}_{dd}$\\mathcal{E}_{dd}dd.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_63', 'cites': [], 'refs': [], 'content': 'Specifically, the $k$k-th TextGNN layer for an agent node $u\\in \\mathcal{V}_a$u\\in \\mathcal{V}_a can be written as:\n\\begingroup\n\\small\n\\begin{equation}\n\\begin{aligned}\n    \\mb{h}_{u}^{(k)} &= \\textsc{AGG}^{(k)}\\big(f_u(\\cdot), \\mb{h}_u^{(k-1)}, \\{\\mb{h}_d^{(k-1)} \\mid (u,d) \\in \\mathcal{E}_{ad}\\}, \\\\\n    &\\quad \\{f_a(\\cdot), \\mb{h}_a^{(k-1)} \\mid (u,a) \\in \\mathcal{E}_{aa}\\}\\big) \\\\\n    &= f_u\\Big(\\Big[\\mb{h}_u^{(k-1)}, \\big\\{f_a\\big(\\big[\\mb{h}_a^{(k-1)}, \\mb{h}_u^{(k-1)}, \\mb{h}_d^{(k-1)}\\big]\\big) \\mid \\\\\n    &\\quad (u,a) \\in \\mathcal{E}_{aa}, (u,d) \\in \\mathcal{E}_{ad}\\big\\}\\Big]\\Big)\n\\end{aligned}\n\\label{agg_agent}\n\\end{equation}\\begin{equation}\n\\begin{aligned}\n    \\mb{h}_{u}^{(k)} &= \\textsc{AGG}^{(k)}\\big(f_u(\\cdot), \\mb{h}_u^{(k-1)}, \\{\\mb{h}_d^{(k-1)} \\mid (u,d) \\in \\mathcal{E}_{ad}\\}, \\\\\n    &\\quad \\{f_a(\\cdot), \\mb{h}_a^{(k-1)} \\mid (u,a) \\in \\mathcal{E}_{aa}\\}\\big) \\\\\n    &= f_u\\Big(\\Big[\\mb{h}_u^{(k-1)}, \\big\\{f_a\\big(\\big[\\mb{h}_a^{(k-1)}, \\mb{h}_u^{(k-1)}, \\mb{h}_d^{(k-1)}\\big]\\big) \\mid \\\\\n    &\\quad (u,a) \\in \\mathcal{E}_{aa}, (u,d) \\in \\mathcal{E}_{ad}\\big\\}\\Big]\\Big)\n\\end{aligned}\n\\label{agg_agent}\n\\end{equation}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_64', 'cites': [], 'refs': [], 'content': '\\mb{h}h_{u}u^{(k)}(k) &= \\textsc{AGG}^{(k)}(k)\\big(f_u(\\cdot), \\mb{h}h_u^{(k-1)}(k-1), \\{\\mb{h}h_d^{(k-1)}(k-1) \\mid (u,d) \\in \\mathcal{E}_{ad}ad\\}, \\\\\n    &\\quad \\{f_a(\\cdot), \\mb{h}h_a^{(k-1)}(k-1) \\mid (u,a) \\in \\mathcal{E}_{aa}aa\\}\\big) \\\\\n    &= f_u\\Big(\\Big[\\mb{h}h_u^{(k-1)}(k-1), \\big\\{f_a\\big(\\big[\\mb{h}h_a^{(k-1)}(k-1), \\mb{h}h_u^{(k-1)}(k-1), \\mb{h}h_d^{(k-1)}(k-1)\\big]\\big) \\mid \\\\\n    &\\quad (u,a) \\in \\mathcal{E}_{aa}aa, (u,d) \\in \\mathcal{E}_{ad}ad\\big\\}\\Big]\\Big)', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_65', 'cites': [], 'refs': [], 'content': '\\label{agg_agent}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_66', 'cites': [], 'refs': [], 'content': '\\endgroup\nwhere $[\\cdot]$[\\cdot] is the concatenation function between texts to fill in the prompt template, $\\mb{h}_v^{(k)}$\\mb{h}h_v^{(k)}(k) represents the hidden states of the $k$k-th layer of $v\\in \\mathcal{V}$v\\in \\mathcal{V}, $f_a(\\cdot)$f_a(\\cdot) represents the agent function paired with the agent node in the neighborhood and $f_u(\\cdot)$f_u(\\cdot) represents the agent function paired with the agent node.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_67', 'cites': [], 'refs': [], 'content': 'Similarly, the forwarding process of the $k$k-th TextGNN layer for a data node $v\\in \\mathcal{V}_d$v\\in \\mathcal{V}_d can be written as:\n\\begingroup\n\\small\n\\begin{equation}\n\\begin{aligned}\n    \\mb{h}_{v}^{(k)} & = \\textsc{AGG}^{(k)}\\Big( \\mb{h}_v^{(k-1)}, \\{\\mb{h}_d^{(k-1)} \\mid (v,d) \\in \\mathcal{E}_{dd}\\}, \\\\\n    &\\quad \\{f_a(\\cdot), \\mb{h}_a^{(k-1)} \\mid (v,a) \\in \\mathcal{E}_{ad}\\}\\Big) \\\\\n    &= f_g\\Big(\\Big[\\mb{h}_v^{(k-1)}, \\big\\{f_a\\big(\\big[\\mb{h}_a^{(k-1)}, \\mb{h}_v^{(k-1)}, \\mb{h}_d^{(k-1)}\\big]\\big) \\mid \\\\\n    &\\quad (v,a) \\in \\mathcal{E}_{ad}, (v,d) \\in \\mathcal{E}_{dd}\\big\\}\\Big]\\Big)\n\\end{aligned}\n\\label{agg_data}\n\\end{equation}\\begin{equation}\n\\begin{aligned}\n    \\mb{h}_{v}^{(k)} & = \\textsc{AGG}^{(k)}\\Big( \\mb{h}_v^{(k-1)}, \\{\\mb{h}_d^{(k-1)} \\mid (v,d) \\in \\mathcal{E}_{dd}\\}, \\\\\n    &\\quad \\{f_a(\\cdot), \\mb{h}_a^{(k-1)} \\mid (v,a) \\in \\mathcal{E}_{ad}\\}\\Big) \\\\\n    &= f_g\\Big(\\Big[\\mb{h}_v^{(k-1)}, \\big\\{f_a\\big(\\big[\\mb{h}_a^{(k-1)}, \\mb{h}_v^{(k-1)}, \\mb{h}_d^{(k-1)}\\big]\\big) \\mid \\\\\n    &\\quad (v,a) \\in \\mathcal{E}_{ad}, (v,d) \\in \\mathcal{E}_{dd}\\big\\}\\Big]\\Big)\n\\end{aligned}\n\\label{agg_data}\n\\end{equation}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_68', 'cites': [], 'refs': [], 'content': '\\mb{h}h_{v}v^{(k)}(k) & = \\textsc{AGG}^{(k)}(k)\\Big( \\mb{h}h_v^{(k-1)}(k-1), \\{\\mb{h}h_d^{(k-1)}(k-1) \\mid (v,d) \\in \\mathcal{E}_{dd}dd\\}, \\\\\n    &\\quad \\{f_a(\\cdot), \\mb{h}h_a^{(k-1)}(k-1) \\mid (v,a) \\in \\mathcal{E}_{ad}ad\\}\\Big) \\\\\n    &= f_g\\Big(\\Big[\\mb{h}h_v^{(k-1)}(k-1), \\big\\{f_a\\big(\\big[\\mb{h}h_a^{(k-1)}(k-1), \\mb{h}h_v^{(k-1)}(k-1), \\mb{h}h_d^{(k-1)}(k-1)\\big]\\big) \\mid \\\\\n    &\\quad (v,a) \\in \\mathcal{E}_{ad}ad, (v,d) \\in \\mathcal{E}_{dd}dd\\big\\}\\Big]\\Big)', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_69', 'cites': [], 'refs': [], 'content': '\\label{agg_data}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_70', 'cites': [], 'refs': [], 'content': '\\endgroup\nwhere $f_g(\\cdot)$f_g(\\cdot) is defined as a global agent function without a specialized profile, and $f_a(\\cdot)$f_a(\\cdot) is the agent function paired with the agent node in the neighborhood.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_71', 'cites': [], 'refs': [], 'content': '\\label{sec:method}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_72', 'cites': [], 'refs': [], 'content': '\\xhdr{Inputs and outputs of \\envname}Inputs and outputs of \\envname Building on the definitions of TextGNN and the agent-data graph in Section~\\S\\ref{sec:community-graph-design} and Section~\\S\\ref{sec:text-gnn}, we simulate different research activities by modeling each as a specific instantiation of a TextGNN layer. \\envname processes diverse research materials and produces structured outputs. The input varies by task: only paper abstracts are used for paper reading and writing, while full papers are provided for review writing. The output format is also task-specific: paper reading generates profile descriptions, paper writing generates bullet-point summaries, and review writing produces bullet-point critiques along with a numerical review score. These standardized output formats—described in more detail in Appendix~\\S\\ref{evaluation-details}—facilitate evaluation over long-context inputs and enable fine-grained, sub-component similarity scoring.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_73', 'cites': [], 'refs': [], 'content': '\\xhdr{Hidden states of \\envname}Hidden states of \\envname In \\envname, the hidden state of each node represents a condensed version of research materials, such as papers or reviews. Initially, paper nodes are initialized with the full text of papers. Through iterative message passing, these nodes gradually evolve into a standardized bullet-point format, distilling key information for easier downstream evaluation. Similarly, review attributes associated with paper nodes are also represented using bullet points to make it in a compact form. Bullet-point compact form with limited length allows TextGNN to conduct message passing multiple times efficiently.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_74', 'cites': [], 'refs': ['figures_3'], 'content': '\\xhdr{Agent-data graph for research community modeling - community graph}Agent-data graph for research community modeling - community graph \nWe adopt the agent-data graph $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$\\mathcal{G} = (\\mathcal{V}, \\mathcal{E}) to research community simulation, which we named as \\textit{community graph}. As is shown in Figure \\ref{fig:community-activity}, each agent node $\\mathcal{V}_a$\\mathcal{V}_a represents one researcher, and each data node $\\mathcal{V}_d$\\mathcal{V}_d represents a paper. The edge set $ \\mathcal{E}_{dd}$ \\mathcal{E}_{dd}dd captures paper citations, the edge set $\\mathcal{E}_{ad}$\\mathcal{E}_{ad}ad captures authorship (a researcher writes a paper) and reviewing expertise (a researcher is qualified to review a paper). We omit the edge set $ \\mathcal{E}_{aa}$ \\mathcal{E}_{aa}aa to simplify the framework, as a collaboration between authors can typically be inferred through 2-hop paths via $\\mathcal{E}_{ad}$\\mathcal{E}_{ad}ad edges.', 'type': 'textNode', 'isolation': False}
text_node
{'id': 'text_75', 'cites': [], 'refs': ['figures_3'], 'content': '\\xhdr{TextGNN for research activity simulation}TextGNN for research activity simulation\nBased on the constructed community graph, we further identify the key types of research activities where TextGNN can be used for simulation.\nSpecifically, as shown in Figure~\\ref{fig:community-activity}, we split the research simulation process into three critical stages: (1) paper reading, (2) paper writing, and (3) review writing. We believe these stages are crucial in the research community, and each stage relies on the output of the previous stage as input. \nWe provide a detailed description for each stage and the corresponding TextGNN layer definition below.', 'type': 'textNode', 'isolation': False}
text_node
{'id': 'text_76', 'cites': [], 'refs': [], 'content': '\\hangindent=0em\n\\hangafter=0\n\\xhdr{$\\triangleright$ Stage 1: Paper reading}$\\triangleright$\\triangleright Stage 1: Paper reading Reading papers to collect insights is a necessary process for initializing a research project. In the community graph, the paper reading process can be described as \\textit{inserting a new agent node} to the community graph and aggregating its neighborhood information based on Equation \\ref{agg_agent}. Here, the new agent profile is non-existent before reading a collection of papers, and the profile is created after the paper reading process, making the TextGNN layer unique. Concretely, by adapting Equation \\ref{agg_agent}, the TextGNN layer for paper reading can be written as:', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_77', 'cites': [], 'refs': [], 'content': '\\vspace{-6mm}\n\\begingroup\n\\small\n\\begin{equation}\n\\begin{split}\n    \\mb{h}_{u} & = \\textsc{AGG}\\Big(f_u(\\cdot), \\{\\mb{h}_d \\mid (u,d) \\in \\mathcal{E}_{ad}\\}\\Big) \\\\\n    & = f_u\\Big(\\Big[\\left\\{\\mb{h}_d \\mid (u,d) \\in \\mathcal{E}_{ad}\\right\\}\\Big]\\Big)\n\\end{split}\n\\label{paper_reading}\n\\end{equation}\\begin{equation}\n\\begin{split}\n    \\mb{h}_{u} & = \\textsc{AGG}\\Big(f_u(\\cdot), \\{\\mb{h}_d \\mid (u,d) \\in \\mathcal{E}_{ad}\\}\\Big) \\\\\n    & = f_u\\Big(\\Big[\\left\\{\\mb{h}_d \\mid (u,d) \\in \\mathcal{E}_{ad}\\right\\}\\Big]\\Big)\n\\end{split}\n\\label{paper_reading}\n\\end{equation}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_78', 'cites': [], 'refs': [], 'content': '\\mb{h}h_{u}u & = \\textsc{AGG}\\Big(f_u(\\cdot), \\{\\mb{h}h_d \\mid (u,d) \\in \\mathcal{E}_{ad}ad\\}\\Big) \\\\\n    & = f_u\\Big(\\Big[\\left\\{\\mb{h}h_d \\mid (u,d) \\in \\mathcal{E}_{ad}ad\\right\\}\\Big]\\Big)', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_79', 'cites': [], 'refs': [], 'content': '\\label{paper_reading}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_80', 'cites': [], 'refs': ['figures_3'], 'content': "\\endgroup\nwhere $\\mb{h}_u, \\{f_a(\\cdot), \\mb{h}_a \\mid (u, a) \\in \\mathcal{E}_{aa}\\}$\\mb{h}h_u, \\{f_a(\\cdot), \\mb{h}h_a \\mid (u, a) \\in \\mathcal{E}_{aa}aa\\} in Equation \\ref{agg_agent} are empty since the agent node is initialized as empty and is not directly connected with any agents, and $\\mathcal{E}_{ad}$\\mathcal{E}_{ad}ad specifically refers to the authorship relation between agent and data nodes. Equation \\ref{agg_agent} degrades to an aggregation of papers based on the researcher agent without the profile, illustrated in Figure \\ref{fig:community-activity} ``Stage 1''.", 'type': 'textNode', 'isolation': False}
text_node
{'id': 'text_81', 'cites': [], 'refs': [], 'content': '\\hangindent=0em\n\\hangafter=0\n\\xhdr{$\\triangleright$ Stage 2: Paper writing}$\\triangleright$\\triangleright Stage 2: Paper writing After paper reading, the next important research stage is paper writing. Different from paper reading, the paper writing process can be understood as inserting \\textit{a new data node} into the community graph. Here, the new data node is non-existent before writing the paper, and the data node is created after the paper writing process. Concretely, by adapting Equation \\ref{agg_data}, the TextGNN layer for paper writing can be written as:', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_82', 'cites': [], 'refs': [], 'content': '\\vspace{-5mm}\n\\begingroup\n\\small\n\\begin{equation}\n\\begin{aligned}\n    \\mb{h}_{v} &= \\textsc{AGG}\\Big( \n        \\big\\{f_a(\\cdot), \\big\\{\\mb{h}_d \\mid (v,d) \\in \\mathcal{E}_{dd}\\big\\}, \\mb{h}_a \\mid (v,a) \\in \\mathcal{E}_{ad}\\big\\}\n    \\Big) \\\\\n    &= f_g\\Big(\n        \\Big[\n            \\big\\{f_a\\big([\\mb{h}_a, \\mb{h}_d]\\big) \\mid (v,a) \\in \\mathcal{E}_{ad}, \n            (v,d) \\in \\mathcal{E}_{dd}\\big\\}\n        \\Big]\n    \\Big)\n\\end{aligned}\n\\label{paper_writing}\n\\end{equation}\\begin{equation}\n\\begin{aligned}\n    \\mb{h}_{v} &= \\textsc{AGG}\\Big( \n        \\big\\{f_a(\\cdot), \\big\\{\\mb{h}_d \\mid (v,d) \\in \\mathcal{E}_{dd}\\big\\}, \\mb{h}_a \\mid (v,a) \\in \\mathcal{E}_{ad}\\big\\}\n    \\Big) \\\\\n    &= f_g\\Big(\n        \\Big[\n            \\big\\{f_a\\big([\\mb{h}_a, \\mb{h}_d]\\big) \\mid (v,a) \\in \\mathcal{E}_{ad}, \n            (v,d) \\in \\mathcal{E}_{dd}\\big\\}\n        \\Big]\n    \\Big)\n\\end{aligned}\n\\label{paper_writing}\n\\end{equation}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_83', 'cites': [], 'refs': [], 'content': '\\mb{h}h_{v}v &= \\textsc{AGG}\\Big( \n        \\big\\{f_a(\\cdot), \\big\\{\\mb{h}h_d \\mid (v,d) \\in \\mathcal{E}_{dd}dd\\big\\}, \\mb{h}h_a \\mid (v,a) \\in \\mathcal{E}_{ad}ad\\big\\}\n    \\Big) \\\\\n    &= f_g\\Big(\n        \\Big[\n            \\big\\{f_a\\big([\\mb{h}h_a, \\mb{h}h_d]\\big) \\mid (v,a) \\in \\mathcal{E}_{ad}ad, \n            (v,d) \\in \\mathcal{E}_{dd}dd\\big\\}\n        \\Big]\n    \\Big)', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_84', 'cites': [], 'refs': [], 'content': '\\label{paper_writing}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_85', 'cites': [], 'refs': ['figures_3'], 'content': "\\endgroup\nwhere $\\mb{h}_v$\\mb{h}h_v in Equation \\ref{agg_data} is empty since paper node contents are non-existent before paper writing; $\\mathcal{E}_{ad}$\\mathcal{E}_{ad}ad specifically refers to authorship relations between agent and data nodes, and $\\mathcal{E}_{dd}$\\mathcal{E}_{dd}dd refers to citation relations within data nodes. A visualization of Equation \\ref{paper_writing} is shown in Figure \\ref{fig:community-activity} ``Stage 2''.", 'type': 'textNode', 'isolation': False}
text_node
{'id': 'text_86', 'cites': [], 'refs': [], 'content': '\\hangindent=0em\n\\hangafter=0\n\\xhdr{$\\triangleright$ Stage 3: Review writing}$\\triangleright$\\triangleright Stage 3: Review writing The review writing task is the final stage of the automatic research simulation, serving as a reflection stage in the multi-agent research simulator. The difference between the previous 2 stages is that, first, the researchers involved during review writing are not the authors but the reviewers of the paper. Additionally, review writing is based on a written paper where $\\mb{h}_v$\\mb{h}h_v is no longer empty. Concretely, by adapting Equation \\ref{agg_data}, the TextGNN layer for review writing can be written as:', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_87', 'cites': [], 'refs': [], 'content': '\\vspace{-5mm}\n\\begingroup\n\\small\n\\begin{equation}\n\\begin{aligned}\n    \\mb{r}_{v} &= \\textsc{AGG}\\Big(\\mb{h}_v, \n    , \\big\\{\\mb{h}_d \\mid (v,d) \\in \\mathcal{E}_{dd}\\big\\}\\big\\{f_a(\\cdot), \\mb{h}_a \\mid (v,a) \\in \\mathcal{E}_{ad}\\big\\}\\Big) \\\\\n    &= f_g\\Big(\\Big[\\mb{h}_v, \n    \\big\\{f_a\\big([\\mb{h}_a, \\mb{h}_v, \\mb{h}_d]\\big) \\mid  (v,a) \\in \\mathcal{E}_{ad}, (v,d) \\in \\mathcal{E}_{dd}\\big\\}\\Big]\\Big)\n\\end{aligned}\n\\label{review_writing}\n\\end{equation}\\begin{equation}\n\\begin{aligned}\n    \\mb{r}_{v} &= \\textsc{AGG}\\Big(\\mb{h}_v, \n    , \\big\\{\\mb{h}_d \\mid (v,d) \\in \\mathcal{E}_{dd}\\big\\}\\big\\{f_a(\\cdot), \\mb{h}_a \\mid (v,a) \\in \\mathcal{E}_{ad}\\big\\}\\Big) \\\\\n    &= f_g\\Big(\\Big[\\mb{h}_v, \n    \\big\\{f_a\\big([\\mb{h}_a, \\mb{h}_v, \\mb{h}_d]\\big) \\mid  (v,a) \\in \\mathcal{E}_{ad}, (v,d) \\in \\mathcal{E}_{dd}\\big\\}\\Big]\\Big)\n\\end{aligned}\n\\label{review_writing}\n\\end{equation}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_88', 'cites': [], 'refs': [], 'content': '\\mb{r}r_{v}v &= \\textsc{AGG}\\Big(\\mb{h}h_v, \n    , \\big\\{\\mb{h}h_d \\mid (v,d) \\in \\mathcal{E}_{dd}dd\\big\\}\\big\\{f_a(\\cdot), \\mb{h}h_a \\mid (v,a) \\in \\mathcal{E}_{ad}ad\\big\\}\\Big) \\\\\n    &= f_g\\Big(\\Big[\\mb{h}h_v, \n    \\big\\{f_a\\big([\\mb{h}h_a, \\mb{h}h_v, \\mb{h}h_d]\\big) \\mid  (v,a) \\in \\mathcal{E}_{ad}ad, (v,d) \\in \\mathcal{E}_{dd}dd\\big\\}\\Big]\\Big)', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_89', 'cites': [], 'refs': [], 'content': '\\label{review_writing}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_90', 'cites': [], 'refs': [], 'content': '\\endgroup', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_91', 'cites': [], 'refs': [], 'content': '\\hangindent=0em\n\\hangafter=0\n\\xhdr{$\\triangleright$ Summary: \\envname simulation algorithm}$\\triangleright$\\triangleright Summary: \\envname simulation algorithm Utilizing the community graph $\\mathcal{G}$\\mathcal{G}, we propose a simulation algorithm named as \\envname. Overall, the simulation algorithm can be considered as a 2-layer GNN where the paper reading is the first layer of information aggregation. Both paper writing and review writing are the second layer of the GNN to generate the final simulation outputs. We formally summarize the research community simulation in Algorithm \\ref{alg:paper_brainstorming}. To achieve better efficiency, the modified version for implementation is in Appendix~\\S\\ref{simulation-algorithm-implementation}.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_92', 'cites': [], 'refs': [], 'content': '\\begin{algorithm}\n\\small\n\\caption{\\small \\envname simulation algorithm}\n\\label{alg:paper_brainstorming}\n\\begin{algorithmic}[1]\n\\REQUIRE community graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$,\\\\ \n         \\hspace{2.6em}paper contents $\\mb{x}_v$ for all paper nodes,\\\\ \n         \\hspace{2.6em}target paper node $v$\n\\ENSURE paper content $\\mb{h}_v$ and review content $\\mb{r}_v$ for paper node $v$\n\\FOR{each $u \\in \\mathcal{N}(v)$}\n    \\IF{$u \\in \\mathcal{V}_d$}\n        \\STATE $\\mb{h}_u \\gets \\mb{x}_u$\n    \\ELSE\n        \\STATE $\\mb{h}_{u} \\gets f_u\\left(\\left[\\left\\{\\mb{x}_d \\mid (u,d) \\in \\mathcal{E}_{ad}\\right\\}\\right]\\right)$ \\hfill $\\triangleright$ Eq.~\\eqref{paper_reading}\n    \\ENDIF\n\\ENDFOR\n\\STATE $\\mb{h}_{v} \\gets f_g\\Big(\\Big[\\big\\{f_a([\\mb{h}_a, \\mb{h}_d]) \\mid$ \n    \\STATE \\quad\\quad $(v,a) \\in \\mathcal{E}_{ad}, (v,d) \\in \\mathcal{E}_{dd}\\big\\}\\Big]\\Big)$ \\hfill $\\triangleright$ Eq.~\\eqref{paper_writing}\n\\STATE $\\mb{r}_{v} \\gets f_g\\Big(\\Big[\\mb{h}_v, \\{f_a([\\mb{h}_a, \\mb{h}_v, \\mb{h}_d]) \\mid$ \n    \\STATE \\quad\\quad $(v,a) \\in \\mathcal{E}_{ad}, (v,d) \\in \\mathcal{E}_{dd}\\}\\Big]\\Big)$ \\hfill $\\triangleright$ Eq.~\\eqref{review_writing}\n\\STATE \\textbf{return} $\\mb{h}_v$, $\\mb{r}_v$ \n\\end{algorithmic}\n\\end{algorithm}\\begin{algorithm}\n\\small\n\\caption{\\small \\envname simulation algorithm}\n\\label{alg:paper_brainstorming}\n\\begin{algorithmic}[1]\n\\REQUIRE community graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$,\\\\ \n         \\hspace{2.6em}paper contents $\\mb{x}_v$ for all paper nodes,\\\\ \n         \\hspace{2.6em}target paper node $v$\n\\ENSURE paper content $\\mb{h}_v$ and review content $\\mb{r}_v$ for paper node $v$\n\\FOR{each $u \\in \\mathcal{N}(v)$}\n    \\IF{$u \\in \\mathcal{V}_d$}\n        \\STATE $\\mb{h}_u \\gets \\mb{x}_u$\n    \\ELSE\n        \\STATE $\\mb{h}_{u} \\gets f_u\\left(\\left[\\left\\{\\mb{x}_d \\mid (u,d) \\in \\mathcal{E}_{ad}\\right\\}\\right]\\right)$ \\hfill $\\triangleright$ Eq.~\\eqref{paper_reading}\n    \\ENDIF\n\\ENDFOR\n\\STATE $\\mb{h}_{v} \\gets f_g\\Big(\\Big[\\big\\{f_a([\\mb{h}_a, \\mb{h}_d]) \\mid$ \n    \\STATE \\quad\\quad $(v,a) \\in \\mathcal{E}_{ad}, (v,d) \\in \\mathcal{E}_{dd}\\big\\}\\Big]\\Big)$ \\hfill $\\triangleright$ Eq.~\\eqref{paper_writing}\n\\STATE $\\mb{r}_{v} \\gets f_g\\Big(\\Big[\\mb{h}_v, \\{f_a([\\mb{h}_a, \\mb{h}_v, \\mb{h}_d]) \\mid$ \n    \\STATE \\quad\\quad $(v,a) \\in \\mathcal{E}_{ad}, (v,d) \\in \\mathcal{E}_{dd}\\}\\Big]\\Big)$ \\hfill $\\triangleright$ Eq.~\\eqref{review_writing}\n\\STATE \\textbf{return} $\\mb{h}_v$, $\\mb{r}_v$ \n\\end{algorithmic}\n\\end{algorithm}\n\\small\n\\caption{\\small \\envname simulation algorithm}\n\\label{alg:paper_brainstorming}\n[1]\n\\REQUIRE community graph $\\mathcal{G}(\\mathcal{V}, \\mathcal{E})$\\mathcal{G}(\\mathcal{V}, \\mathcal{E}),\\\\ \n         \\hspace{2.6em}paper contents $\\mb{x}_v$\\mb{x}x_v for all paper nodes,\\\\ \n         \\hspace{2.6em}target paper node $v$v\n\\ENSURE paper content $\\mb{h}_v$\\mb{h}h_v and review content $\\mb{r}_v$\\mb{r}r_v for paper node $v$v\n\\FOR{each $u \\in \\mathcal{N}(v)$}each $u \\in \\mathcal{N}(v)$u \\in \\mathcal{N}(v)\n    \\IF{$u \\in \\mathcal{V}_d$}$u \\in \\mathcal{V}_d$u \\in \\mathcal{V}_d\n        \\STATE $\\mb{h}_u \\gets \\mb{x}_u$\\mb{h}h_u \\gets \\mb{x}x_u\n    \\ELSE\n        \\STATE $\\mb{h}_{u} \\gets f_u\\left(\\left[\\left\\{\\mb{x}_d \\mid (u,d) \\in \\mathcal{E}_{ad}\\right\\}\\right]\\right)$\\mb{h}h_{u}u \\gets f_u\\left(\\left[\\left\\{\\mb{x}x_d \\mid (u,d) \\in \\mathcal{E}_{ad}ad\\right\\}\\right]\\right) \\hfill $\\triangleright$\\triangleright Eq.~\\eqref{paper_reading}\n    \\ENDIF\n\\ENDFOR\n\\STATE $\\mb{h}_{v} \\gets f_g\\Big(\\Big[\\big\\{f_a([\\mb{h}_a, \\mb{h}_d]) \\mid$\\mb{h}h_{v}v \\gets f_g\\Big(\\Big[\\big\\{f_a([\\mb{h}h_a, \\mb{h}h_d]) \\mid \n    \\STATE \\quad\\quad $(v,a) \\in \\mathcal{E}_{ad}, (v,d) \\in \\mathcal{E}_{dd}\\big\\}\\Big]\\Big)$(v,a) \\in \\mathcal{E}_{ad}ad, (v,d) \\in \\mathcal{E}_{dd}dd\\big\\}\\Big]\\Big) \\hfill $\\triangleright$\\triangleright Eq.~\\eqref{paper_writing}\n\\STATE $\\mb{r}_{v} \\gets f_g\\Big(\\Big[\\mb{h}_v, \\{f_a([\\mb{h}_a, \\mb{h}_v, \\mb{h}_d]) \\mid$\\mb{r}r_{v}v \\gets f_g\\Big(\\Big[\\mb{h}h_v, \\{f_a([\\mb{h}h_a, \\mb{h}h_v, \\mb{h}h_d]) \\mid \n    \\STATE \\quad\\quad $(v,a) \\in \\mathcal{E}_{ad}, (v,d) \\in \\mathcal{E}_{dd}\\}\\Big]\\Big)$(v,a) \\in \\mathcal{E}_{ad}ad, (v,d) \\in \\mathcal{E}_{dd}dd\\}\\Big]\\Big) \\hfill $\\triangleright$\\triangleright Eq.~\\eqref{review_writing}\n\\STATE \\textbf{return} $\\mb{h}_v$\\mb{h}h_v, $\\mb{r}_v$\\mb{r}r_v', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_93', 'cites': [], 'refs': [], 'content': '\\vspace{-3mm}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_94', 'cites': [], 'refs': [], 'content': '\\label{evaluation}\nUtilizing graph structures not only enables the design of the research simulation algorithm but also provides a natural way to evaluate it. As we show next, we propose to view research evaluation as a masked node prediction task, including evaluation for both paper writing and review writing.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_95', 'cites': [], 'refs': [], 'content': '\\xhdr{Evaluation by masked node prediction}Evaluation by masked node prediction A masked node prediction task in the community graph $\\mathcal{G}$\\mathcal{G} can be defined as first masking a specific node $v \\in \\mathcal{V}$v \\in \\mathcal{V} in the community graph by setting its hidden states $\\mb{h}_v = \\emptyset$\\mb{h}h_v = \\emptyset, where the original hidden state is saved as $\\mb{h}_v^*$\\mb{h}h_v^*; then an ideal model should be able to predict the hidden states $\\mb{h}_v^*$\\mb{h}h_v^* of the masked node from its neighborhood $\\mathcal{N}(v)$\\mathcal{N}(v). Concretely, in Equation \\ref{paper_writing}, the output $\\mb{h}_v$\\mb{h}h_v can be regarded as the masked node prediction for evaluation of paper writing, suppose that the node $v$v is a masked version of a ground truth data node. Similarly, in Equation \\ref{review_writing}, the output $\\mb{r}_v$\\mb{r}r_v can be regarded as the predicted node attributes for review writing, where the original review is represented as $\\mb{r}_v^*$\\mb{r}r_v^*.\nIn general, we have:\\\\', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_96', 'cites': [], 'refs': [], 'content': '\\vspace{-8mm}\n\\begingroup\n\\small\n\\begin{equation}\n\\begin{split}\n\\mb{h}_v, \\mb{r}_v &= \\textsc{ResearchTown}\\Big(\n    \\mathcal{G}(\\mathcal{V}, \\mathcal{E}); \\{\\mb{x}_u \\mid u \\in \\mathcal{N}(v)\\}; v\n\\Big)\n\\end{split}\n\\end{equation}\\begin{equation}\n\\begin{split}\n\\mb{h}_v, \\mb{r}_v &= \\textsc{ResearchTown}\\Big(\n    \\mathcal{G}(\\mathcal{V}, \\mathcal{E}); \\{\\mb{x}_u \\mid u \\in \\mathcal{N}(v)\\}; v\n\\Big)\n\\end{split}\n\\end{equation}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_97', 'cites': [], 'refs': [], 'content': '\\mb{h}h_v, \\mb{r}r_v &= \\textsc{ResearchTown}\\Big(\n    \\mathcal{G}(\\mathcal{V}, \\mathcal{E}); \\{\\mb{x}x_u \\mid u \\in \\mathcal{N}(v)\\}; v\n\\Big)', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_98', 'cites': [], 'refs': [], 'content': '\\endgroup\nwhere $\\mb{h}_v$\\mb{h}h_v is the text-form hidden states of a masked node $v$v and  $\\mb{r}_v$\\mb{r}r_v is the text-form prediction output of a masked node $v$v. Since we have real-world results for both paper writing and review, we treat them as ground truth even though they are not perfect because the goal of \\envname is to simulate the human research community rather than to find optimal solutions for papers and reviews ($\\mb{h}_v^*$\\mb{h}h_v^* for paper ground-truth and $\\mb{r}_v^*$\\mb{r}r_v^* for review ground-truth) and we can systematically evaluate both processes to check the effectiveness of our simulation algorithm. More specifically, since we have access to ground-truth papers $\\mb{h}_v^*$\\mb{h}h_v^* when evaluating the review writing simulation, to avoid accumulated errors, we update Equation \\ref{review_writing} during evaluation so that reviews $\\mb{r}_v$\\mb{r}r_v are generated based on $\\mb{h}_v^*$\\mb{h}h_v^*, instead of $\\mb{h}_v$\\mb{h}h_v:', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_99', 'cites': [], 'refs': [], 'content': '\\vspace{-6mm}\n\\begingroup\n\\small\n\\begin{equation}\n\\begin{split}\n\\mb{r}_{v} &= \\textsc{AGG}\\Big(\n    \\mb{h}_v^*, \\big\\{\\mb{h}_d \\mid (v,d) \\in \\mathcal{E}_{dd}\\big\\}\n    , \\big\\{f_a(\\cdot), \\mb{h}_a \\mid (v,a) \\in \\mathcal{E}_{ad}\\big\\}\n\\Big)\n\\end{split}\n\\end{equation}\\begin{equation}\n\\begin{split}\n\\mb{r}_{v} &= \\textsc{AGG}\\Big(\n    \\mb{h}_v^*, \\big\\{\\mb{h}_d \\mid (v,d) \\in \\mathcal{E}_{dd}\\big\\}\n    , \\big\\{f_a(\\cdot), \\mb{h}_a \\mid (v,a) \\in \\mathcal{E}_{ad}\\big\\}\n\\Big)\n\\end{split}\n\\end{equation}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_100', 'cites': [], 'refs': [], 'content': '\\mb{r}r_{v}v &= \\textsc{AGG}\\Big(\n    \\mb{h}h_v^*, \\big\\{\\mb{h}h_d \\mid (v,d) \\in \\mathcal{E}_{dd}dd\\big\\}\n    , \\big\\{f_a(\\cdot), \\mb{h}h_a \\mid (v,a) \\in \\mathcal{E}_{ad}ad\\big\\}\n\\Big)', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_101', 'cites': [], 'refs': [], 'content': '\\endgroup\n\\xhdr{Evaluation metric}Evaluation metric We utilize state-of-the-art embedding models like text-embedding-large-3~\\footnote{\\url{https://openai.com/index/new-embedding-models-and-api-updates/}} to build distance function for $d_p(\\mb{h}_v, \\mb{h}_v^*)$d_p(\\mb{h}h_v, \\mb{h}h_v^*) and $d_r(\\mb{r}_v, \\mb{r}_v^*)$d_r(\\mb{r}r_v, \\mb{r}r_v^*). More details related to formal embedding-based metric definitions for paper writing and review writing tasks are available in Appendix~\\S\\ref{evaluation-details}.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_102', 'cites': [], 'refs': [], 'content': '\\xhdr{\\envname setting}\\envname setting\nWe utilize GPT-4o-mini~\\footnote{We point to \\texttt{GPT-4o-mini-2024-07-18} for use.} as the LLM backbone for implementing the agent functions, with the decoding temperature set to \\( 0 \\) 0  to ensure reproducibility. To evaluate different aggregation strategies, we conduct experiments using specific types of nodes connected to the target node: (1) \\textit{AGG-self}, where the aggregation relies solely on the target node; (2) \\textit{AGG-agent}, which includes the target node and its neighboring agent nodes; (3) \\textit{AGG-data}, which involves the target node and its neighboring data nodes; and (4) \\textit{AGG-global}, which incorporates the target node and all its neighboring nodes, including agent and data nodes. We specifically refer to \\textit{AGG-global} as our proposed \\envname method for simulation, while the others serve as baselines. This experimental design enables a systematic comparison of the effects of different neighborhood information on the aggregation process. More details about different settings are available in Appendix~\\S\\ref{agg-setting-implementation}.\n\\label{researchtown-setting}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_103', 'cites': [], 'refs': [], 'content': '\\xhdr{\\benchname setting}\\benchname setting To evaluate \\envname for research simulation, we introduce \\benchname, which consists of 1,000 paper writing tasks and 200 review writing tasks. All tasks are sourced from recent top-tier machine learning conferences such as NeurIPS 2024~\\footnote{\\url{https://neurips.cc/Conferences/2024}} and ICLR 2024~\\footnote{\\url{https://openreview.net/group?id=ICLR.cc/2024/Conference}}. Since most papers are released after the cutoff date of GPT-4o-mini, information leakage is not considered an issue. For paper writing tasks, we categorize them into three difficulty levels—\\textit{hard} (333 tasks), \\textit{medium} (334 tasks), and \\textit{easy} (333 tasks)—based on the similarity results of data-only aggregation. Specifically, for review writing tasks, the reviewers prepared for each paper are selected from the top 5 researchers most related to the paper, as reviewer information is not publicly available in the real world. More details about the data collection and prevention of information leakage during simulation are in Appendix~\\S\\ref{research-bench-tech-details}.\n\\label{researchbench-setting}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_104', 'cites': [], 'refs': [], 'content': '\\label{tab:paper-writing-result}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_105', 'cites': [], 'refs': [], 'content': '\\label{tab:review-writing-result}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_106', 'cites': [], 'refs': [], 'content': '\\vspace{-6mm}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_107', 'cites': [], 'refs': [], 'content': '\\label{sec:core-results}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_108', 'cites': [], 'refs': [], 'content': 'In this section, we present the main results of our research simulation on \\benchname, including 1,000 paper writing tasks and 200 review writing tasks. We evaluate existing paper nodes that have fully known their content and their neighborhoods within the community graph. We refer to these scenarios as \\textit{in-distribution} cases.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_109', 'cites': [], 'refs': ['table_7', 'table_8'], 'content': '\\xhdr{Overall: \\envname can provide a realistic simulation of research activity}Overall: \\envname can provide a realistic simulation of research activity To evaluate research simulation, we utilize state-of-the-art embedding models (text-embedding-3-large) to compare the semantic similarity between simulated results and real-world results. For paper writing, as shown in Table~\\ref{tab:paper-writing-result}, the overall similarity score obtained using text-embedding-3-large across 1,000 papers is 67.51. Notably, the score increases to 73.79 for an easy subset of the benchmark. These results demonstrate that paper writing with \\envname can produce realistic outputs compared to real-world ones. Moreover, it indicates that some ideas in top-tier conference papers are not hard to think of and can be imagined by LLMs. For review writing, as shown in Table~\\ref{tab:review-writing-result}, the similarity scores are generally lower compared with paper writing, with strength-related scores averaging around 51 and weakness-related scores averaging around 47. This suggests that review writing is more challenging to generate with \\envname, particularly for weakness identification. A possible explanation is that real-world review data is often noisier and more diverse, making it harder to simulate accurately.', 'type': 'textNode', 'isolation': False}
tab:paper-writing-result
tab:review-writing-result
text_node
{'id': 'text_110', 'cites': [], 'refs': ['table_7'], 'content': '\\xhdr{Paper writing: participation of multi-researchers improves paper quality}Paper writing: participation of multi-researchers improves paper quality As shown in Table \\ref{tab:paper-writing-result}, cited papers contribute more effectively than authors in the paper writing simulation, with data-aggregation achieving a score of 65.30 compared to 55.24 for agent-aggregation. The best results are obtained by combining both, surpassing data aggregation by 2.21 points. Researchers are particularly beneficial under difficult scenarios, improving the text-embedding-large-3 score from 56.02 to 60.89, likely due to the inclusion of multi-hop paper information from researchers.', 'type': 'textNode', 'isolation': False}
tab:paper-writing-result
text_node
{'id': 'text_111', 'cites': [], 'refs': ['table_8', 'table_9'], 'content': '\\xhdr{Review writing: participation of multi-reviewers improves review quality}Review writing: participation of multi-reviewers improves review quality Unlike paper writing, review writing mainly relies on the paper that needs to be reviewed, making reviewers and cited papers less impactful, with differences limited to within 1 point. However, as shown in Table~\\ref{tab:review-writing-result}, adding additional information consistently improves performance over the self-aggregation baseline. Agent aggregation performs best for writing strengths and assigning scores, while data aggregation achieves the best results for writing weaknesses. This pattern likely reflects the role of related work comparisons in highlighting weaknesses, while multiple reviewers help provide a more balanced assessment of strengths. Interestingly, global aggregation leads to larger differences in scores. We consider it an exception since GPT-4o-mini tends to apply stricter novelty judgments under global aggregation—its average assigned score drops from 5.3 to 5.0. As shown in Table~\\ref{tab:model-ablation}, this effect is not observed for Qwen-2.5-7B-Instruct or Deepseek-v3, which gain better results with global aggregation.', 'type': 'textNode', 'isolation': False}
tab:review-writing-result
tab:model-ablation
text_node
{'id': 'text_112', 'cites': [], 'refs': [], 'content': '\\label{sec:underexplored-idea}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_113', 'cites': [], 'refs': [], 'content': '\\vspace{-3mm}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_114', 'cites': [], 'refs': [], 'content': 'We conduct ablation studies on both hyperparameters and model selection. The results show that \\envname consistently produces high-quality simulations across a range of settings, demonstrating strong robustness. Detailed experimental configurations are provided in Appendix~\\S\\ref{ablation-study-details}.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_115', 'cites': [], 'refs': [], 'content': '\\xhdr{Ablation on paper number}Ablation on paper number\nIn paper writing tasks, users can freely assign papers to simulate non-existent work, making robustness to the number of papers essential. As shown in Figure~\\ref{paper_writing_paper_number}, papers cited in the related work section have the greatest positive impact, increasing the similarity score from 66.4 to 66.7 compared to using all papers. In contrast, using only papers cited in the introduction lowers the score to 65.2, while including papers from other sections reduces it further to 58.4. These results highlight the importance of selecting informative references when generating papers. In review writing, the number of papers is fixed, so no ablation study on the paper number is applicable.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_116', 'cites': [], 'refs': ['figures_4'], 'content': '\\xhdr{Ablation on agent number}Ablation on agent number\nFor \\envname simulation, users can assign different numbers of agents, making robustness to agent number critical for \\envname. In Figure~\\ref{paper_writing_researcher_num}, in the paper writing task, increasing the agent number improves simulation quality under the agent-aggregation setting. The most notable gain occurs when increasing from 1 to 2, boosting the similarity score from 49.0 to 52.7.\nSimilar trends hold in review writing (Figure~\\ref{review_writing_researcher_num}), where increasing the agent number consistently enhances output quality. The strength score improves from 50.8 to 51.5 when increasing the reviewer from 1 to 5.', 'type': 'textNode', 'isolation': False}
text_node
{'id': 'text_117', 'cites': [], 'refs': ['table_9'], 'content': '\\xhdr{Ablation on generation models}Ablation on generation models The choice of LLMs significantly impacts simulation quality. In addition to GPT-4o-mini, we evaluate two models from different families: Qwen-2.5-7B-Instruct\\footnote{\\url{https://huggingface.co/Qwen/Qwen2.5-7B-Instruct}} and Deepseek-v3\\footnote{We point \\texttt{DeepSeek-V3-0324} for use.}. In Table~\\ref{tab:model-ablation}, for both paper writing tasks, global aggregation (\\envname) consistently yields the highest similarity scores across all models. It also achieves the best review difference scores for Qwen-2.5-7B-Instruct and Deepseek-v3. The only exception is GPT-4o-mini, which shows an unexpected increase in review difference under AGG-global. Overall, Deepseek-v3 outperforms GPT-4o-mini, which in turn outperforms Qwen-2.5-7B-Instruct—consistent with their relative performance on other tasks.', 'type': 'textNode', 'isolation': False}
tab:model-ablation
text_node
{'id': 'text_118', 'cites': [], 'refs': ['figures_4'], 'content': '\\xhdr{Ablation on embedding models}Ablation on embedding models Similarity scores can be computed using different models, and voyage-3\\footnote{\\url{https://blog.voyageai.com/2024/09/18/voyage-3/}} serves as an alternative to the text-embedding-3-large used in our main experiments. As shown in Figures~\\ref{paper_writing_paper_number}, \\ref{paper_writing_researcher_num}, and \\ref{review_writing_researcher_num}, voyage-3 produces consistent trends in ablation studies involving the number of papers and agents. This consistency suggests that \\envname is robust to the choice of embedding model, and different models lead to the same conclusions.', 'type': 'textNode', 'isolation': False}
text_node
{'id': 'text_119', 'cites': [], 'refs': [], 'content': '\\vspace{-1mm}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_120', 'cites': [], 'refs': [], 'content': 'Besides computing embedding-based similarities, we provide more types of evaluations here. First, we prompt LLMs to calculate fine-grained similarity scores that assess consistency between real-world data and simulated ones across various dimensions. Next, we evaluate the intrinsic quality of the simulated outputs themselves and compare them with real-world data. Finally, we report results from human evaluations to validate the alignment between LLM-based evaluation and human judgments. More details about LLM-based evaluation are available in Appendix~\\S\\ref{sec:llm-based-eval}, and details about human evaluation are available in Appendix~\\S\\ref{sec:human-eval}.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_121', 'cites': [], 'refs': ['figures_5'], 'content': '\\xhdr{Automatic evaluation on fine-grained similarity}Automatic evaluation on fine-grained similarity A high cosine similarity score alone can mask important issues in simulated results.\nTo capture a more complete picture of similarity, we move beyond a single score and instead evaluate across five fine-grained dimensions: \\textit{topic consistency}, \\textit{method consistency}, \\textit{factual consistency}, \\textit{claim consistency}, and \\textit{application context consistency}. These dimensions collectively reflect subcomponents of overall semantic similarity. For evaluation, we use GPT-4o to assign scores from 0 to 10 for each dimension for each paper. As shown in Figure~\\ref{fig:fine-grained-similarity}, our proposed global aggregation method (\\envname) consistently outperforms all other aggregation baselines across these dimensions. This demonstrates that \\envname provides a more effective simulation of research activities compared to baselines.', 'type': 'textNode', 'isolation': False}
text_node
{'id': 'text_122', 'cites': [], 'refs': [], 'content': '\\vspace{-4mm}\n\\label{tab:model-ablation}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_123', 'cites': [], 'refs': [], 'content': '\\vspace{-5mm}\n\\label{tab:novelty_feasibility}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_124', 'cites': [], 'refs': ['table_10'], 'content': '\\xhdr{Automatic evaluation on intrinsic quality}Automatic evaluation on intrinsic quality In addition to evaluating semantic similarity between simulated and real-world data, we also assess the intrinsic quality of the generated content. Specifically, we focus on two key dimensions: \\textit{novelty} and \\textit{feasibility}, which we consider the two most critical aspects of a research proposal. As shown in Table~\\ref{tab:novelty_feasibility}, the simulated outputs still do not match the novelty and feasibility levels of real-world articles but are close to those. This gap indicates that \\envname would benefit from a more coordinated agentic workflow to enhance the quality of the generated research outputs.', 'type': 'textNode', 'isolation': False}
tab:novelty_feasibility
text_node
{'id': 'text_125', 'cites': [], 'refs': [], 'content': '\\xhdr{Human evaluation}Human evaluation Evaluation based on LLMs may introduce bias into the results. To validate the reliability of LLM-based evaluations, we conduct additional human evaluations. For similarity-based assessments, human judgments correlate well with LLM scores, achieving a Pearson correlation of 0.61, indicating reasonable agreement. However, for intrinsic quality evaluations, the correlation between human and LLM scores is low. This is likely due to the inherent ambiguity of such tasks and the need for domain-specific expertise. Despite this, both human and LLM evaluations consistently indicate that simulated papers are slightly less novel than real-world ones—though the gap is relatively small (5.50 vs 5.90 for humans and 7.39 vs 7.85 for LLMs).', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_126', 'cites': [], 'refs': [], 'content': '\\vspace{-2mm}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_127', 'cites': [], 'refs': [], 'content': '\\label{case-study-section}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_128', 'cites': [], 'refs': [], 'content': 'As discussed in Section~\\S\\ref{sec:core-results}, the node masking evaluation in \\envname targets \\textit{in-distribution} settings with predefined neighborhoods. In real-world use, however, \\envname must generate non-existing papers and reviews without such neighborhoods, requiring automatic construction via paper–researcher matching. This leads to \\textit{out-of-distribution} cases, such as interdisciplinary research, where unrelated papers and researchers form unconventional neighborhoods without prior related works.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_129', 'cites': [], 'refs': ['figures_6'], 'content': '\\xhdr{\\envname can inspire interdisciplinary research}\\envname can inspire interdisciplinary research Interdisciplinary research is often challenging due to limited collaboration across fields. \\envname addresses this by enabling agents with diverse expertise to read, interact, and co-create novel ideas. For example, as shown in Figure~\\ref{fig:case-study}, combining NLP and astronomy papers leads to using kinematic models to analyze language evolution, while linking NLP and criminology inspires the use of LLMs to support communities affected by mass incarceration. These domain pairings are rarely explored in existing literature, demonstrating \\envname’s ability to generate innovative, cross-disciplinary research directions.', 'type': 'textNode', 'isolation': False}
text_node
{'id': 'text_130', 'cites': [], 'refs': [], 'content': "\\xhdr{\\envname-written contents might have limited use in the real world}\\envname-written contents might have limited use in the real world \n\\envname exhibits failure modes when combining too many disparate domains, often producing incoherent or superficial outputs. For example, combining researchers and papers from LLM, biology, criminology, and astronomy, \\envname generates a research question of ``\\textit{How does coded language in political discourse influence societal biases, and how can a Bayesian hierarchical model be employed to analyze this effect while simultaneously addressing observational biases in white dwarf population studies?}'' It simply strings together terminology from different domains without presenting a clear research direction. Such vagueness might hinder the real use of the papers simulated from \\envname.", 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_131', 'cites': [], 'refs': [], 'content': '\\vspace{-3mm}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_132', 'cites': [], 'refs': [], 'content': 'We introduce \\envname, a graph-based multi-agent framework that simulates research communities by modeling them as heterogeneous graphs. \\envname integrates key research activities—paper reading, writing, and reviewing—into a unified TextGNN-driven inference process. It enables realistic and robust simulations through agent collaboration and facilitates rare interdisciplinary interactions. \\envname offers a valuable platform for studying research dynamics and developing algorithms to support automated scientific discovery.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_133', 'cites': [], 'refs': [], 'content': '\\envname presents an LLM-based simulation framework that models human research communities as graph-based multi-agent systems, enabling the study of collaboration, knowledge diffusion, and institutional dynamics. By formalizing how agents create, refine, and evaluate academic papers, the simulator can inform the design of autonomous research systems that assist, rather than replace, human researchers. Potential applications include optimizing collaboration structures, identifying systemic bottlenecks in peer review or discovery, and stress-testing scientific workflows under various incentive and communication settings. While the framework is primarily a research tool, we acknowledge that future extensions involving autonomous agents could raise ethical considerations around authorship, influence, and epistemic trust. Our work highlights the imperative for adaptive ethical frameworks that keep pace with technological capabilities while protecting scholarly values.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_134', 'cites': [], 'refs': [], 'content': 'We sincerely appreciate the support from Amazon grant funding project \\#120359, "GRAG: Enhance RAG Applications with Graph-structured Knowledge", and Meta gift funding project "PERM: Toward Parameter Efficient Foundation Models for Recommenders".', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_135', 'cites': [], 'refs': [], 'content': '\\nocite{langley00}langley00', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_136', 'cites': [], 'refs': [], 'content': '\\bibliography{example_paper}\n\\bibliographystyle{icml2025}icml2025', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_137', 'cites': [], 'refs': [], 'content': '\\newpage\n\\onecolumn', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_138', 'cites': [], 'refs': [], 'content': '\\label{appendix:ethical}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_139', 'cites': [], 'refs': [], 'content': 'The development and deployment of \\envname raises several important ethical considerations that we have carefully addressed in our work. We first discuss how \\envname prevents dangerous use, including facilitating plagiarism, producing misleading or low-quality claims, and role-playing human researchers. Furthermore, we discuss the attribution and authorship issues for generated content and discuss the model and data license in our work.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_140', 'cites': [], 'refs': [], 'content': '\\subsection{Potential to facilitate plagiarism}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_141', 'cites': ['elali2023ai'], 'refs': [], 'content': "Generative AI's capabilities for image and text generation can potentially lead to plagiarism in research~\\citep{elali2023ai}. To address this, we have implemented safeguards to ensure responsible usage. \\envname is designed as an assistive tool to help researchers gather inspiration for papers and review writing, rather than generating complete, ready-to-use content. By design, \\envname ensures that its outputs serve as a starting point for further intellectual effort, rather than a replacement for human researchers.", 'type': 'textNode', 'isolation': False}
No paper found for  elali2023ai
text_node
{'id': 'text_142', 'cites': [], 'refs': [], 'content': 'For generated papers, \\envname provides only preliminary answers to five key research questions. These outputs are intentionally incomplete and generic, requiring significant refinement and further development by the user. Critical sections such as the introduction, background, methodology, discussion, and conclusion are not included, placing the responsibility for completing and validating the content on the researcher.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_143', 'cites': [], 'refs': [], 'content': 'For generated reviews, \\envname provides general guidance on potential strengths and weaknesses, accompanied by an indicative score for reference. However, these reviews are intentionally non-definitive and generic, only as a supplementary aid to help reviewers organize their thoughts. Generated reviews do not replace human judgment in determining the acceptance or rejection of a paper. The final evaluation, including critical reasoning, detailed feedback, and the ultimate decision, remains the sole responsibility of the reviewer. Reviewers must ensure fairness, accuracy, and rigor, using AI outputs only as a starting point to enhance their assessment process.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_144', 'cites': [], 'refs': [], 'content': '\\subsection{Potential to produce misleading or low-quality claims}\nThe motivation of our paper is to simulate research activities and generate preliminary research progress (\\eg, papers and reviews that are in their condensed bullet-point summarized format) that can be scrutinized and validated by human researchers, ultimately contributing to the acceleration of the research process. We acknowledge that AI-generated ideas may vary in quality, and therefore, these outputs are not intended for direct dissemination. Instead, they serve as initial, unofficial suggestions that require further experimental validation by human researchers. This approach ensures that only rigorously tested and verified research is presented as final, high-quality work.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_145', 'cites': [], 'refs': [], 'content': '\\subsection{Potential to role-play human researchers}\nThe primary objective of our work is to leverage existing academic literature to simulate research activities. In this paper, our research agents are designed to act as research domain experts, generating informative and relevant content based on a given and limited research domain. Importantly, we do not aim to simulate human-like interactive dialogues between research agents, nor do we attempt to mimic the specific research styles of individual human researchers. Instead, we focus on using related academic papers as conditions for generating more related research content.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_146', 'cites': [], 'refs': [], 'content': "The research agents are built using publicly available, properly cited academic papers, which eliminates the need for additional consent. We utilize the LLM-based research agents, each with one or more specific research domains, modeling the typical academic process, where researchers read, synthesize, and build upon available public academic data. By focusing on publicly available research papers, we align with the papers' intended purpose: contributing to the collective advancement of knowledge and fostering academic growth.", 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_147', 'cites': [], 'refs': [], 'content': '\\subsection{Attribution and authorship}\nThe AI-generated content, such as papers, reviews, or other research outputs, is meant for internal discussion and as a reference to assist human researchers. These outputs are not intended for direct publication. Our proposed methods serve as tools to accelerate the research process by offering starting points that require further elaboration, critical analysis, and human refinement to reach a publishable standard. The final authority to complete and submit research lies solely with human authors, ensuring that full responsibility and ownership remain with them. Since the AI-generated content is not considered complete or officially authored, it does not raise issues of authorship or attribution.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_148', 'cites': [], 'refs': [], 'content': "We list all licenses for the data and models used in our paper in this section.\n\\subsection{Data license}\nAll papers in \\benchname come from top-tier machine learning conferences (ICLR 2024 and NeurIPS 2024). These papers are publicly available and under the license of CC-BY 4.0, allowing for redistribution and sharing. For the evaluation results of \\benchname, all inputs and outputs are logged and open for access. Additionally, we keep an accessible record of all supplementary papers referenced during \\envname's inference process. All outputs from \\envname are released under the licenses of the papers used for generation.", 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_149', 'cites': [], 'refs': [], 'content': '\\subsection{Model license}\nOur work relies on multiple foundation models, including GPT-4o-mini, Qwen-2.5-7B-Instruct, Deepseek-v3, text-embedding-3-large, and voyage-3. Specifically, we use \\texttt{gpt-4o-mini-2024-07-18} accessed via the OpenAI API. We use \\texttt{Qwen-2.5-7B-Instruct-Turbo} and \\texttt{Deepseek-v3-0324} via the together.ai~\\footnote{\\url{https://www.together.ai/inference}} inference API. We utilize the official inference API provided by OpenAI and VoyageAI to use text-embedding-3-large and voyage-3 separately.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_150', 'cites': [], 'refs': [], 'content': 'The GPT-4o-mini, text-embedding-3-large, and voyage-3 models are closed-source and operate under proprietary licenses. We use them only for academic, and non-commercial purposes and ensure all inputs come from publicly available data, complying with their usage restrictions. By contrast, Qwen-2.5-7B-Instruct is released under the permissive Apache 2.0 license, and Deepseek-v3-0324 is available under the MIT License, allowing for broad academic and research use. We make no modifications to these models and use them as-is via their public APIs.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_151', 'cites': [], 'refs': [], 'content': '\\label{research-bench-tech-details}\nIn this section, we provide the technical details included in the construction process of \\benchname. We describe the methodologies used for data collection across its three main components, and we name them as: (1) \\textsc{PaperBench}, (2) \\textsc{HighImpactPaperBench}, and (3) \\textsc{ReviewBench}. Statistically, \\textsc{PaperBench} and \\textsc{HighImpactPaperBench} focus on a paper writing simulation, which contains 1,000 and 100 tasks, respectively. \\textsc{ReviewBench} focuses on review writing simulation and includes 200 tasks.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_152', 'cites': [], 'refs': [], 'content': '\\subsection{Data collection details}\n\\label{data-collection}\nWe first include technical details related to how we collect paper, author, and review data from publicly available platforms as a source to build \\benchname.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_153', 'cites': [], 'refs': [], 'content': '\\xhdr{Paper data collection}Paper data collection We begin by recording the titles of all papers that we plan to crawl. Then, using the \\texttt{arxiv} Python package\\footnote{\\url{https://pypi.org/project/arxiv/}}, we query the arXiv API to check for any papers with identical titles. If a match is found, we note the corresponding arXiv ID and use the API to retrieve the paper’s metadata, including its title, arXiv ID, author list, abstract, and citation information.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_154', 'cites': [], 'refs': [], 'content': '\\xhdr{Author data collection}Author data collection A primary challenge in collecting author data is that there might be multiple human researchers with the same name, and some human researchers may not have any publicly available publication records on public platforms, including arXiv, Google Scholar, or Semantic Scholar. As a result, at the paper collection stage, we only have each author’s name. We use the \\texttt{semanticscholar} Python package\\footnote{\\url{https://github.com/danielnsilva/semanticscholar}} to search for the author by name, verify that they have contributed to the specific target paper, and obtain a unique author ID from Semantic Scholar. This ID then allows us to retrieve their available publication information. To prevent information leakage when simulating paper writing and review scenarios, we exclude any of the author’s publications released after the target paper’s publication year. For example, if we aim to simulate a paper published in 2022, we ignore all of the author’s publications appearing after 2022. We also exclude the target paper itself to avoid leaking information. Generally, we limit the maximum number of collected publications to around 20, focusing on those most relevant to the target time frame. Additionally, we gather each author’s co-author network and their top publications to enrich the dataset with useful relational information.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_155', 'cites': [], 'refs': [], 'content': '\\xhdr{Review data collection}Review data collection In addition to paper and author data, we also leverage OpenReview to extract public review information. Since fully public review data is predominantly available for ICLR, we focus on collecting reviews from ICLR2024. Using the \\texttt{openreview} Python package\\footnote{\\url{https://openreview-py.readthedocs.io/en/latest/}}, we first verify the arXiv ID to ensure that we are retrieving the correct paper and its corresponding reviews. The collected review data aligns with ICLR’s criteria, including detailed feedback on soundness, presentation, contributions, reviewer scores, and commentary on strengths and weaknesses. We adopt this review structure when generating our reviews, incorporating strengths, weaknesses, and ratings for the paper.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_156', 'cites': [], 'refs': [], 'content': "\\subsection{\\textsc{PaperBench} details}\n\\label{paper-bench-detail}\n\\textsc{PaperBench} is designed to evaluate the effectiveness of paper-writing simulations by gathering high-quality paper metadata from top-tier ML conferences, such as NeurIPS 2024 and ICLR 2024. Both NeurIPS 2024 and ICLR 2024 post-date beyond GPT-4o-mini's October 2023 knowledge cutoff. Thus, data leakage is not a concern. We also mask the full text during the simulation to avoid accidental exposure. Based on the collected author and paper data, we perform the following two post-processing steps:", 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_157', 'cites': [], 'refs': [], 'content': 'First, we address cases where authors have no accessible publications beyond the current paper or where citation data extraction fails due to API issues. In such cases, we exclude these papers. We only retain those with full author publication information, as well as complete metadata including introduction, abstract, title, and citations. After this filtering step, we end up with approximately 1,200 papers, and then randomly sample 1,000 from them.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_158', 'cites': [], 'refs': [], 'content': 'Second, to allow more fine-grained analysis, we split these 1,000 paper-writing tasks into three subgroups based on their difficulty level. We use the \\textit{data-agg} settings described in Section~\\S\\ref{researchtown-setting} to obtain results and compute similarity scores for our simulations. We then divide the dataset into three equal subsets: the worst 333 data points (hard), the middle 334 data points (medium), and the top 333 data points (easy). This results in a more granular categorization of the dataset’s difficulty.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_159', 'cites': ['domingo2023stochastic', 'tu2024mixed', 'han2024multistable', 'yu20244real', 'tasseskill', 'cao2024worst'], 'refs': [], 'content': "Intuitively, papers in the hard sub-part tend to be more theoretical and math-focused, while those in the easy sub-part are more application-oriented. Examples for hard sub-parts of the dataset include ``\\textit{Stochastic Optimal Control Matching}''~\\citep{domingo2023stochastic}, ``\\textit{Mixed Dynamics In Linear Networks: Unifying the Lazy and Active Regimes}''~\\citep{tu2024mixed}, and ``\\textit{Multistable Shape from Shading Emerges from Patch Diffusion}''~\\citep{han2024multistable}. Examples for easy sub-parts of the dataset include ``\\textit{4Real: Towards Photorealistic 4D Scene Generation via Video Diffusion Models}''~\\citep{yu20244real}, ``\\textit{Skill Machines: Temporal Logic Skill Composition in Reinforcement Learning}''~\\citep{tasseskill}, and ``\\textit{On the Worst Prompt Performance of Large Language Models}''~\\citep{cao2024worst}.", 'type': 'textNode', 'isolation': False}
No paper found for  domingo2023stochastic
No paper found for  tu2024mixed
No paper found for  han2024multistable
No paper found for  yu20244real
No paper found for  tasseskill
No paper found for  cao2024worst
text_node
{'id': 'text_160', 'cites': [], 'refs': [], 'content': '\\subsection{\\textsc{Reviewbench} details}\nSince public review data is only fully accessible from ICLR, we focus on collecting review data for the ICLR 2024 papers included in \\textsc{PaperBench}. All reviews are anonymous, so no direct reviewer information is available. To address this, we identify suitable reviewers by first summarizing each researcher’s publications. We then use the abstract of the target paper as a query and the researcher profiles as documents for a ranking task with the voyage-3 model. All authors included in \\benchname serve as the corpus for retrieval. The top 20 most relevant authors, excluding the paper’s authors, become the suitable reviewer candidates.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_161', 'cites': [], 'refs': [], 'content': 'After obtaining the reviewer, paper, and author data, we filter out any papers lacking valid reviews during crawling. From the remaining set, we randomly select 200 reviews, each corresponding to one paper as \\textsc{ReviewBench}.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_162', 'cites': [], 'refs': [], 'content': '\\subsection{\\textsc{HighImpactPaperBench} details}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_163', 'cites': ['kingma2013auto', 'goodfellow2014generative', 'kingma2014adam'], 'refs': [], 'content': '\\textsc{HighImpactPaperBench} serves as an extreme benchmark for \\envname, focused on simulating impactful research. We begin by collecting the 20 most-cited papers from each of 10 leading AI-related conferences—CVPR, ECCV, NeurIPS, ICLR, ICML, AAAI, IJCAI, ACL, EMNLP, and NAACL—based on Google Scholar citation rankings.\\footnote{\\url{https://scholar.google.es/citations?view_op=top_venues&hl=en&vq=eng}} Additionally, we include classic machine learning algorithm papers such as those introducing VAE~\\citep{kingma2013auto}, GAN~\\citep{goodfellow2014generative}, and Adam~\\citep{kingma2014adam}, each with over 1,000 citations, even if they are no longer listed in the current Google Scholar citation rankings.', 'type': 'textNode', 'isolation': False}
No paper found for  kingma2013auto
No paper found for  goodfellow2014generative
No paper found for  kingma2014adam
text_node
{'id': 'text_164', 'cites': [], 'refs': [], 'content': "For these impactful papers, it is crucial to prevent the inclusion of publications released after their publication year when gathering authors' publication data. Later works such as these could significantly alter the trajectory of the researcher, misrepresent the historical context of these influential contributions, and leak information for simulation. After collecting paper and author data, we remove any papers with incomplete information due to crawling errors. From the remaining set, we randomly sample 100 papers to form the final benchmark. These selected papers have averaged over 100 citations in the past five years, ensuring that \\textsc{HighImpactPaperBench} represents a collection of influential and well-established research.", 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_165', 'cites': [], 'refs': [], 'content': 'The motivation for using impactful papers as evaluation is to use them as an extreme-case test for idea simulation. While some may exist in the LLM’s training data, this benchmark is separate from our main results and serves to explore how LLMs handle well-known concepts. Our similarity analysis shows that 55\\% of generated papers score between 0.65–0.75, and 18\\% exceed 0.75, indicating moderate to high alignment. Only 1\\% scored below 0.45. These scores are comparable to \\textsc{PaperBench}, suggesting no abnormal inflation. Even famous papers like VAE, GAN, and LayerNorm do not receive notably high scores, implying that semantic similarity, not memorization based on citation relationships, drives the results, especially for tool/benchmark papers, which naturally resemble their references more.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_166', 'cites': [], 'refs': [], 'content': '\\label{evaluation-details}\nIn this section, we first explain the motivation for our designed multi-component embedding-based evaluation, then we provide a more formal definition and implementation details related to our evaluation process.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_167', 'cites': ['chen2025learning', 'jin2025search'], 'refs': [], 'content': '\\xhdr{Decompositionality}Decompositionality A single idea or a review can manifest through diverse descriptions or implementation strategies. Therefore, directly applying a cosine similarity-based metric is inadequate for capturing conceptual equivalence. To solve this, we design point-wise evaluation metrics to paraphrase the paper and review it into aligned key points with the same LLM-based prompting. This structure enables alignment between papers that differ methodologically but share similar motivations and problem framings. For instance, in \\citet{chen2025learning} and \\citet{jin2025search}, despite distinct methods and settings, experts would find strong alignment on the motivations and core concepts in these papers.', 'type': 'textNode', 'isolation': False}
No paper found for  chen2025learning
No paper found for  jin2025search
text_node
{'id': 'text_168', 'cites': ['si2024can'], 'refs': [], 'content': '\\xhdr{Scalability}Scalability To address the challenge that a single idea can take many concrete forms, we complement decomposition with scalability. LLMs can generate hundreds of semantically distinct research questions from a single prompt, but evaluating these outputs traditionally requires domain experts—a process that is costly, slow, unscalable, and hard to reproduce. For example, \\citet{si2024can} spent thousands hiring top-tier researchers solely for annotation and review, which is infeasible for evaluating large-scale, automated research generation. Our approach replaces this bottleneck with semantic similarity over 5Q-decomposed representations. We can select the best among the samples and make the score the final result.', 'type': 'textNode', 'isolation': False}
No paper found for  si2024can
text_node
{'id': 'text_169', 'cites': [], 'refs': [], 'content': '\\xhdr{Extensibility}Extensibility While we acknowledge the importance of elements like mathematical formulations or algorithmic workflows, our framework is inherently extensible—the original format can be expanded with more key points by adding domain-specific dimensions such as algorithmic structure or key theoretical results. This is especially valuable in systems and theory papers, enabling more fine-grained and domain-aware similarity analysis. As demonstrated in [Fine-Grained Evaluation with LLM and Human], our approach also supports the integration of non-semantic metrics like logical consistency and factual accuracy, making it extensible from an evaluation metric perspective.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_170', 'cites': [], 'refs': [], 'content': '\\xhdr{Reliability}Reliability Our embedding-based / LLM-based similarity metric builds on state-of-the-art models optimized for knowledge-intensive tasks. Voyage AI embeddings, widely adopted in real-world RAG systems, are designed to reduce hallucination and excel in high-precision semantic retrieval, making them ideal for evaluating research content. Additionally, state-of-the-art LLMs are highly effective at semantic comparison.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_171', 'cites': ['chen2025learning', 'jin2025search'], 'refs': [], 'content': '\\xhdr{Baselines for evaluation}Baselines for evaluation To check whether \\envname provides a realistic simulation, we benchmark similarity in real-world research activity. For paper writing, we reference two concurrent papers~\\citep{chen2025learning,jin2025search} recognized for presenting nearly identical ideas, yet with different writing styles and experiments, which yield a VoyageAI similarity of 0.8244. This suggests that scores above 0.82 can potentially indicate strong idea overlap. For review writing, we analyze the data of reviewers evaluating the same paper. The average inter-reviewer similarity is 0.5900 (strengths) and 0.5904 (weaknesses), reflecting natural variance in human judgment. These inter-similarity scores in the real world confirm that similarity scores represent realistic simulation.', 'type': 'textNode', 'isolation': False}
No paper found for  chen2025learning
No paper found for  jin2025search
text_node
{'id': 'text_172', 'cites': [], 'refs': [], 'content': "\\xhdr{More details on paper evaluation}More details on paper evaluation To evaluate the paper writing stage, we define a distance function $ d_p(\\cdot, \\cdot) $ d_p(\\cdot, \\cdot)  to measure the similarity between the generated paper $ \\mathbf{h}_v $ \\mathbf{h}_v  and the ground-truth paper $ \\mathbf{h}_v^* $ \\mathbf{h}_v^* . Since directly comparing full papers in different formats can be challenging and inaccurate, we align $ \\mathbf{h}_v $ \\mathbf{h}_v  and $ \\mathbf{h}_v^* $ \\mathbf{h}_v^*  into a unified format using a well-recognized framework~\\footnote{\\url{https://cs.stanford.edu/people/widom/paper-writing.html}} that summarizes the core components of a paper through five questions: (1) What is the problem? (2) Why is it interesting and important? (3) Why is it hard? (4) Why hasn't it been solved before? (5) What are the key components of my approach and results? We mark these questions as Q1-Q5 for short. By using an LLM-based summarization function $ f_\\text{sum}(\\cdot) $ f_\\text{sum}(\\cdot) , we convert the input papers into an aligned text-based list $ \\mathbf{a}_v = f_\\text{sum}(\\mathbf{h}_v) $ \\mathbf{a}_v = f_\\text{sum}(\\mathbf{h}_v)  and $ \\mathbf{a}^*_v = f_\\text{sum}(\\mathbf{h}^*_v) $ \\mathbf{a}^*_v = f_\\text{sum}(\\mathbf{h}^*_v) , where each element in $ \\mathbf{a}_v $ \\mathbf{a}_v  and $ \\mathbf{a}_v^* $ \\mathbf{a}_v^*  corresponds to the answer of one question mentioned above. The distance function for paper writing is formally defined as:\n\\begin{align}\n\\vspace{-1mm}\nd_p(\\mathbf{h}_v, \\mathbf{h}^*_v) = \\frac{1}{5} \\sum_{i=1}^{5} \\textsc{sim}(\\mathbf{a}_{v,i}, \\mathbf{a}^*_{v,i})\n\\label{paper-score}\n\\vspace{-1mm}\n\\end{align}\\begin{align}\n\\vspace{-1mm}\nd_p(\\mathbf{h}_v, \\mathbf{h}^*_v) = \\frac{1}{5} \\sum_{i=1}^{5} \\textsc{sim}(\\mathbf{a}_{v,i}, \\mathbf{a}^*_{v,i})\n\\label{paper-score}\n\\vspace{-1mm}\n\\end{align}\n\\vspace{-1mm}\nd_p(\\mathbf{h}_v, \\mathbf{h}^*_v) = \\frac{1}{5} \\sum_{i=1}i=1^{5}5 \\textsc{sim}(\\mathbf{a}_{v,i}v,i, \\mathbf{a}^*_{v,i}v,i)\n\\label{paper-score}\n\\vspace{-1mm}", 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_173', 'cites': [], 'refs': [], 'content': 'where $ \\textsc{sim}(\\cdot, \\cdot) $ \\textsc{sim}(\\cdot, \\cdot)  represents an embedding-based similarity metric, such as voyage-3~\\footnote{\\url{https://blog.voyageai.com/2024/09/18/voyage-3/}} and text-embedding-large-3~\\footnote{\\url{https://openai.com/index/new-embedding-models-and-api-updates/}}. By leveraging the LLM to generate structured embeddings for each question, this approach ensures a meaningful and consistent comparison of the generated and ground-truth papers.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_174', 'cites': [], 'refs': [], 'content': '\\xhdr{More details on review evaluation}More details on review evaluation Another research activity we aim to evaluate is review writing. Similar to paper writing evaluation, we project both real-world and generated reviews into a unified format for comparison. For this purpose, we adopt a bullet point-based format to represent weaknesses and advantages in the review, as it effectively captures the key aspects of a review. Using an LLM-based summarization function $ f_\\text{sum}(\\cdot) $ f_\\text{sum}(\\cdot) , we convert the input reviews $ \\mathbf{r}_v $ \\mathbf{r}_v  and $ \\mathbf{r}^*_v $ \\mathbf{r}^*_v  into a bullet point list $ \\mathbf{b}_v = f_\\text{sum}(\\mathbf{r}_v) $ \\mathbf{b}_v = f_\\text{sum}(\\mathbf{r}_v)  and $ \\mathbf{b}^*_v = f_\\text{sum}(\\mathbf{r}^*_v) $ \\mathbf{b}^*_v = f_\\text{sum}(\\mathbf{r}^*_v) , where each element of $ \\mathbf{b}_v $ \\mathbf{b}_v  and $ \\mathbf{b}_v^* $ \\mathbf{b}_v^*  corresponds to a bullet point of the review. Formally, the distance function for review writing is computed as:\n\\begin{align}\n\\vspace{-1mm}\nd_r(\\mathbf{r}_v, \\mathbf{r}^*_v) = \\frac{1}{n} \\sum_{j=1}^{n} \\max_{i} \\textsc{sim}(\\mathbf{b}_{v,i}, \\mathbf{b}^*_{v,j})\n\\label{review-score}\n\\vspace{-1mm}\n\\end{align}\\begin{align}\n\\vspace{-1mm}\nd_r(\\mathbf{r}_v, \\mathbf{r}^*_v) = \\frac{1}{n} \\sum_{j=1}^{n} \\max_{i} \\textsc{sim}(\\mathbf{b}_{v,i}, \\mathbf{b}^*_{v,j})\n\\label{review-score}\n\\vspace{-1mm}\n\\end{align}\n\\vspace{-1mm}\nd_r(\\mathbf{r}_v, \\mathbf{r}^*_v) = \\frac{1}{n} \\sum_{j=1}j=1^{n}n \\max_{i}i \\textsc{sim}(\\mathbf{b}_{v,i}v,i, \\mathbf{b}^*_{v,j}v,j)\n\\label{review-score}\n\\vspace{-1mm}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_175', 'cites': [], 'refs': [], 'content': 'where $ \\textsc{sim}(\\cdot, \\cdot) $ \\textsc{sim}(\\cdot, \\cdot)  refers to the same similarity metric in paper writing evaluation. This metric emphasizes the \\textit{recall} rate of the generated review by measuring whether each point in the real-world review is potentially included in the generated review. Since each review consists of both strengths and weaknesses, we compute separate similarity scores for strengths and weaknesses. Additionally, since both $ \\mathbf{r}_v $ \\mathbf{r}_v  and $ \\mathbf{r}_v^* $ \\mathbf{r}_v^*  include a final score $\\mb{S}_v$\\mb{S}S_v and $\\mb{S}^*_v$\\mb{S}S^*_v as attributes, we calculate $ \\Delta \\mb{S}_v = |\\mb{S}_v - \\mb{S}_v^*| $ \\Delta \\mb{S}S_v = |\\mb{S}S_v - \\mb{S}S_v^*|  to quantify the difference between the generated and real-world review scores.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_176', 'cites': [], 'refs': ['table_21', 'table_22'], 'content': '\\xhdr{Prompt}Prompt Table~\\ref{tab:Paper_Ground_Truth_Transform} presents the prompt used to convert any existing paper into responses to the five critical research questions. Similarly, Table~\\ref{tab:Review_Ground_Truth_Transform} shows the prompt used to transform any existing review into a bullet-point format. Both prompts ensure that the transformed papers and reviews are aligned with the generated ones, facilitating consistent evaluation in the same format. The transformed format for the paper is considered as $\\mb{a}_v^{*}$\\mb{a}a_v^{*}* and the concatenation of all ground-truth reviews is considered as $\\mb{b}_v^{*}$\\mb{b}b_v^{*}*, as mentioned in Section \\S\\ref{evaluation}.', 'type': 'textNode', 'isolation': False}
tab:Paper_Ground_Truth_Transform
tab:Review_Ground_Truth_Transform
text_node
{'id': 'text_177', 'cites': [], 'refs': [], 'content': '\\xhdr{Metric}Metric For our embedding-based similarity calculations, we use the \\textit{text-embedding-large-3} model via the \\texttt{litellm} Python package by calling \\texttt{litellm.embedding()}. For the \\textit{voyage-3} model, we rely on the \\texttt{voyageai} Python package by calling \\texttt{voyageai.Client().embed()}. We then compute the cosine similarity between the resulting embeddings to measure their similarity.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_178', 'cites': [], 'refs': [], 'content': '\\label{sec:llm-based-eval}\nIn this section, we provide more technical details about using LLM prompting for evaluation.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_179', 'cites': [], 'refs': [], 'content': '\\xhdr{Prompting for similarity}Prompting for similarity For prompting-based evaluation, we decompose overall similarity into six fine-grained dimensions: (1) topic consistency, (2) method consistency, (3) factual consistency, (4) claim consistency, (5) application context consistency, and (6) overall semantic similarity. These dimensions are designed to capture distinct yet complementary aspects of alignment between the generated and reference proposals, ranging from high-level research focus (such as topic and application context) to specific technical content (such as methods, facts, and claims). Importantly, they are intended to capture nuances that may not be easily detected by embedding-based models, enabling a more comprehensive and interpretable assessment than relying on a single similarity score. Each dimension is rated on a scale from 0 to 10.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_180', 'cites': [], 'refs': [], 'content': "\\xhdr{Prompting for novelty and feasibility}Prompting for novelty and feasibility In addition to measuring similarity, we prompt LLMs to assess two intrinsic quality dimensions: (1) novelty and (2) feasibility, which we consider essential characteristics of a strong research proposal. While similarity captures how well the generated content aligns with a reference, it does not fully reflect the proposal's originality or practicality. These intrinsic dimensions address that gap by evaluating the creativity of the proposed idea and its potential for real-world implementation. Each dimension is scored on a scale from 0 to 10, complementing similarity-based metrics for a more holistic evaluation.", 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_181', 'cites': [], 'refs': ['table_23'], 'content': '\\xhdr{Prompt}Prompt To enable efficient evaluation, we adopt parallel prompting, where both the reference and generated proposals are input to the LLM in a single prompt, along with all evaluation criteria. This allows the model to produce scores for all dimensions simultaneously in one forward pass. The detailed descriptions of these evaluation criteria and the full prompts are provided in Table~\\ref{tab:Eval_Prompt_Finegrained}.', 'type': 'textNode', 'isolation': False}
tab:Eval_Prompt_Finegrained
text_node
{'id': 'text_182', 'cites': [], 'refs': [], 'content': '\\label{sec:human-eval}\n\\xhdr{Annotator Information}Annotator Information\nWe recruit two graduate-level students with backgrounds in computer science and artificial intelligence. Both annotators have prior experience publishing in top-tier machine-learning conferences.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_183', 'cites': [], 'refs': [], 'content': '\\xhdr{Annotated Data}Annotated Data\nWe randomly sample 40 reference proposals and their corresponding generated proposals from \\textsc{PaperBench} for human evaluation. We ask annotators to annotate on overall similarity, novelty, and feasibility.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_184', 'cites': [], 'refs': [], 'content': '\\xhdr{Annotation Process}Annotation Process\nThe annotation process consists of three stages: (1) preliminary annotation, (2) discussion, and (3) final annotation. In the preliminary stage, each annotator independently labels 10 examples. They then meet to discuss discrepancies, align their understanding, and refine the annotation criteria. Based on this discussion, they proceed to annotate the official 40 examples using the agreed-upon guidelines as the final results.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_185', 'cites': [], 'refs': [], 'content': '\\xhdr{Annotation Instructions}Annotation Instructions\nAt the start, annotators receive the same input information as used in the LLM-based prompting setup. During the discussion phase, they collaboratively develop more detailed and consistent annotation guidelines to ensure alignment in their final evaluations.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_186', 'cites': [], 'refs': [], 'content': '\\label{ablation-study-details}\nDue to the experimental setting, the ablation study on paper writing simulation tasks does not include all the 1,000 tasks that existed in \\benchname. Therefore, we provide detailed explanations and technical details for this.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_187', 'cites': [], 'refs': [], 'content': '\\xhdr{Data for paper-writing researcher number ablation.}Data for paper-writing researcher number ablation.  \nNot all papers in \\benchname have more than five authors. To ablate the effect of the number of researchers (1 to 5), we select a subset from the hard part of \\textsc{PaperBench} within \\benchname, including 333 paper writing tasks, ensuring each paper has more than five authors. This filtering results in a subset of 172 paper-writing tasks. We focus on the hard subset because we believe that involving multiple research agents in more challenging scenarios yields a more significant difference in performance.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_188', 'cites': [], 'refs': [], 'content': '\\xhdr{Data for paper-writing paper number ablation.}Data for paper-writing paper number ablation.  \nIn this ablation, we vary the number of cited papers included in different sections of the target paper. Specifically, we examine citations in the related work, introduction, and other sections. To do this, we retrieve the raw \\LaTeX{} source from arXiv and extract references at the section level. Due to varying data availability, we finalize a subset of \\benchname that includes 296 paper-writing tasks for this study.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_189', 'cites': [], 'refs': [], 'content': '\\xhdr{Data for review-writing researcher number ablation.}Data for review-writing researcher number ablation.  \nSince the reviewer construction does not depend on any complex data preprocessing, we do not encounter data issues for the review-writing ablation. Consequently, the ablation results are based on all 200 review-writing tasks in \\benchname.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_190', 'cites': [], 'refs': [], 'content': '\\label{additional-exp-results}\nWe provide more comprehensive experimental results on each sub-part of \\textsc{ResearchBench} (\\textsc{PaperBench}, \\textsc{ReviewBench}, \\textsc{HighImpactPaperBench}) in this section.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_191', 'cites': [], 'refs': ['table_11'], 'content': '\\xhdr{Additional Results on \\textsc{PaperBench}}Additional Results on \\textsc{PaperBench}\nTable~\\ref{tab:paperbench-all-in-one} shows that all models—Qwen-2.5-7B-Instruct, GPT-4o-mini, and Deepseek-v3—consistently achieve better performance with richer reference contexts (AGG-data and AGG-global) compared to narrower ones (AGG-self and AGG-agent), highlighting the importance of contextual information in similarity evaluation.', 'type': 'textNode', 'isolation': False}
tab:paperbench-all-in-one
text_node
{'id': 'text_192', 'cites': [], 'refs': ['table_12'], 'content': '\\xhdr{Additional Results on \\textsc{ReviewBench}}Additional Results on \\textsc{ReviewBench}\nAs shown in Table~\\ref{tab:review-writing-result-appendix}, voyage-3 embeddings yield higher strength scores and larger $\\Delta \\mb{S}$\\Delta \\mb{S}S values than text-embedding-3, indicating greater discriminative power. While Qwen-2.5-7B-Instruct maintains strong similarity scores across all aggregation types, it exhibits larger deviations from human scores, suggesting potential scoring bias or overconfidence in its own outputs.', 'type': 'textNode', 'isolation': False}
tab:review-writing-result-appendix
text_node
{'id': 'text_193', 'cites': ['ba2016layer', 'strubell2019energy', 'jin2020bert', 'qi2020stanza'], 'refs': [], 'content': "\\xhdr{Additional results on \\textsc{HighImpactPaperBench}}Additional results on \\textsc{HighImpactPaperBench}\nBesides the full results on \\textsc{PaperBench}, we also evaluate \\envname under extreme conditions by attempting to simulate 100 of the most-cited machine learning papers from the past decade. \\envname achieves low similarity scores for papers introducing groundbreaking methods, such as ``\\textit{Layer Normalization}''~\\citep{ba2016layer}, or novel topics, such as ``\\textit{Energy and Policy Considerations for Deep Learning in NLP}''~\\citep{strubell2019energy}. However, the framework performs notably better on impactful papers focused on analysis or tool development. For instance, it achieves a similarity score exceeding 0.8 for papers like ``\\textit{Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment}''~\\citep{jin2020bert}, which provides adversarial analysis, and ``\\textit{Stanza: A Python Natural Language Processing Toolkit for Many Human Languages}''~\\citep{qi2020stanza}, which offers a practical toolkit. These results suggest that high-impact research ideas may be more feasible than commonly perceived, and \\envname could potentially serve as a tool to inspire future impactful research.", 'type': 'textNode', 'isolation': False}
No paper found for  ba2016layer
No paper found for  strubell2019energy
No paper found for  jin2020bert
No paper found for  qi2020stanza
text_node
{'id': 'text_194', 'cites': [], 'refs': [], 'content': '\\label{tab:paperbench-all-in-one}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_195', 'cites': [], 'refs': [], 'content': '\\label{tab:review-writing-result-appendix}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_196', 'cites': [], 'refs': [], 'content': '\\label{additional-case-study}\nBeyond the examples included in Case Study Section \\S\\ref{case-study-section}, we provide additional examples to show the generation results of our work and provide further insights about the strengths and weaknesses of \\envname.', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_197', 'cites': [], 'refs': ['table_24', 'table_25', 'table_26', 'table_27', 'table_28'], 'content': '\\xhdr{Additional case study for in-distribution evaluation}Additional case study for in-distribution evaluation\nTables \\ref{tab:paperbench easy}, \\ref{tab:paperbench mid}, \\ref{tab:paperbench hard}, \\ref{tab:oodbench}, and \\ref{tab:reviewbench} present examples of tasks and their corresponding outputs for the in-distribution evaluation of \\envname. These examples illustrate the evaluation process defined in this work.', 'type': 'textNode', 'isolation': False}
tab:paperbench easy
tab:paperbench mid
tab:paperbench hard
tab:oodbench
tab:reviewbench
text_node
{'id': 'text_198', 'cites': [], 'refs': ['table_29', 'table_30', 'table_31', 'table_32', 'table_33', 'table_34', 'table_35', 'table_36', 'table_37', 'table_38'], 'content': '\\xhdr{Additional case study for out-of-distribution application}Additional case study for out-of-distribution application\n\\label{more-example}\nIn Table \\ref{tab:LLM+Astronomy}, \\ref{tab:LLM+Criminology}, \\ref{tab:LLM+Biology}, \\ref{tab:System+Biology}, \\ref{tab:Math+Criminology}, \\ref{tab:LLM+Math+Criminology}, \\ref{tab:System+Biology+Criminology}, \\ref{tab:LLM+Biology+Criminology}, \\ref{tab:Astronomy+Biology+Criminology }, and \\ref{tab:LLM+Astronomy+Biology+Criminology }, we show examples of the inputs and outputs of the out-of-distribution application of \\envname. Additionally, each table caption includes a brief comment on the quality of the generated papers for reference.', 'type': 'textNode', 'isolation': False}
tab:LLM+Astronomy
tab:LLM+Criminology
tab:LLM+Biology
tab:System+Biology
tab:Math+Criminology
tab:LLM+Math+Criminology
tab:System+Biology+Criminology
tab:LLM+Biology+Criminology
tab:Astronomy+Biology+Criminology 
tab:LLM+Astronomy+Biology+Criminology 
text_node
{'id': 'text_199', 'cites': [], 'refs': [], 'content': '\\newpage', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_200', 'cites': [], 'refs': [], 'content': '\\caption{Paper reading message prompt template for $f_u(\\cdot)$.}\n\\label{tab:paper-reading-prompt}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_201', 'cites': [], 'refs': [], 'content': '\\caption{Paper writing message prompt template for $f_a(\\cdot)$.}\n\\label{tab:Paper_Writing_Prompt}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_202', 'cites': [], 'refs': [], 'content': '\\caption{Paper writing aggregation prompt template for $f_g(\\cdot)$.}\n\\label{tab:Paper_Writing_Summary_Prompt}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_203', 'cites': [], 'refs': [], 'content': '\\caption{Review writing (strength) message prompt template for $f_u(\\cdot)$.}\n\\label{tab:Agent_Review_Strength_Writing_Prompt}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_204', 'cites': [], 'refs': [], 'content': '\\caption{Review writing (weakness) message prompt template for $f_u(\\cdot)$.}\n\\label{tab:Agent_Review_Weakness_Writing_Prompt}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_205', 'cites': [], 'refs': [], 'content': '\\caption{Review writing (score) message prompt template for $f_u(\\cdot)$.}\n\\label{tab:Agent_Review_Scoring_Prompt}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_206', 'cites': [], 'refs': [], 'content': '\\caption{Review writing (strength) aggregation prompt template for $f_g(\\cdot)$.}\n\\label{tab:Agent_Metareview_Strength_Writing_Prompt}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_207', 'cites': [], 'refs': [], 'content': '\\caption{Review writing (weakness) aggregation prompt template for $f_g(\\cdot)$.}\n\\label{tab:Agent_Metareview_Weakness_Writing_Prompt}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_208', 'cites': [], 'refs': [], 'content': '\\caption{Format transformative prompt for real-world papers.}\n\\label{tab:Paper_Ground_Truth_Transform}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_209', 'cites': [], 'refs': [], 'content': '\\caption{Format transformative prompt for real-world reviews.}\n\\label{tab:Review_Ground_Truth_Transform}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_210', 'cites': [], 'refs': [], 'content': '- Topic Consistency: Do both proposals address the same research topic or problem area?\\newline\n- Method Consistency: Are the research methods and approaches used in both proposals similar?\\newline\n- Factual Consistency: Are the datasets, metrics, and models mentioned consistent between the two proposals?\\newline\n- Claim Consistency: Do both proposals present similar conclusions or findings?\\newline\n- Application Context Consistency: Are the application domains or use-cases targeted by both proposals the same?\\newline\n- Overall Semantic Similarity: Do the two proposals convey the same overall idea or message?\\newline', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_211', 'cites': [], 'refs': [], 'content': 'Additionally, assess the following for each proposal individually:\\newline\n- Novelty of Reference Proposal: Does the reference proposal introduce new ideas or perspectives?\\newline\n- Feasibility of Reference Proposal: How realistic and implementable is the reference proposal based on its described methods and objectives?\\newline\n- Novelty of Generated Proposal: Does the generated proposal introduce new ideas or perspectives?\\newline\n- Feasibility of Generated Proposal: How realistic and implementable is the generated proposal based on its described methods and objectives?\\newline', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_212', 'cites': [], 'refs': [], 'content': '- Topic Consistency: Do both proposals address the same research topic or problem area?\\newline\n- Method Consistency: Are the research methods and approaches used in both proposals similar?\\newline\n- Factual Consistency: Are the datasets, metrics, and models mentioned consistent between the two proposals?\\newline\n- Claim Consistency: Do both proposals present similar conclusions or findings?\\newline\n- Application Context Consistency: Are the application domains or use-cases targeted by both proposals the same?\\newline\n- Overall Semantic Similarity: Do the two proposals convey the same overall idea or message?\\newline', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_213', 'cites': [], 'refs': [], 'content': 'Additionally, assess the following for each proposal individually:\\newline\n- Novelty of Reference Proposal: Does the reference proposal introduce new ideas or perspectives?\\newline\n- Feasibility of Reference Proposal: How realistic and implementable is the reference proposal based on its described methods and objectives?\\newline\n- Novelty of Generated Proposal: Does the generated proposal introduce new ideas or perspectives?\\newline\n- Feasibility of Generated Proposal: How realistic and implementable is the generated proposal based on its described methods and objectives?\\newline', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_214', 'cites': [], 'refs': [], 'content': '\\caption{Prompt for LLM-based evaluation of fine-grained similarity and quality assessment of research proposals.}\n\\label{tab:Eval_Prompt_Finegrained}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_215', 'cites': [], 'refs': [], 'content': '\\caption{Case study on paper writing results of \\textsc{PaperBench}-easy.}\n\\label{tab:paperbench easy}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_216', 'cites': [], 'refs': [], 'content': '\\caption{Case study on paper writing results of \\textsc{PaperBench}-medium.}\n\\label{tab:paperbench mid}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_217', 'cites': [], 'refs': [], 'content': '\\caption{Case study on paper writing results of \\textsc{PaperBench}-hard.}\n\\label{tab:paperbench hard}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_218', 'cites': [], 'refs': [], 'content': '\\caption{Case study on paper writing results of \\textsc{HighImpactPaperBench}.}\n\\label{tab:oodbench}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_219', 'cites': [], 'refs': [], 'content': '\\caption{Case study on review writing results of \\textsc{ReviewBench}.}\n\\label{tab:reviewbench}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_220', 'cites': [], 'refs': [], 'content': '\\caption{Case study on using \\envname to write interdisciplinary research papers combining LLM and Astronomy. The idea creatively applies modeling techniques from astrophysics to explore how language styles evolve over time.}\n\\label{tab:LLM+Astronomy}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_221', 'cites': [], 'refs': [], 'content': '\\caption{Case study on using \\envname to write interdisciplinary research papers combining LLM and Criminology. The idea creatively utilizes a multimodal LLM to integrate qualitative narrative analysis with real-time speech translation, aiming to enhance communication for communities impacted by mass incarceration.}\n\\label{tab:LLM+Criminology}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_222', 'cites': [], 'refs': [], 'content': '\\caption{Case study on using \\envname to write interdisciplinary research papers combining LLM and Biology. The idea integrates patterns of inherited traits with generated retrieval methods to study and improve how language models grow and perform over time.}\n\\label{tab:LLM+Biology}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_223', 'cites': [], 'refs': [], 'content': '\\caption{Case study on using \\envname to write interdisciplinary research papers combining System and Biology. The idea attempts to build a hybrid system combining genetic variation models and IoT protocols for resilient crop breeding, but it risks being overshadowed by excessive terminologies.}\n\\label{tab:System+Biology}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_224', 'cites': [], 'refs': [], 'content': '\\caption{Case study on using \\envname to write interdisciplinary research papers combining Math and Criminology. Due to the two fields being too far apart conceptually, the generated idea primarily focuses on mathematical methods, with minimal incorporation of criminology insights.}\n\\label{tab:Math+Criminology}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_225', 'cites': [], 'refs': [], 'content': '\\caption{Case study on using \\envname to write interdisciplinary research papers combining LLM, Math, and Criminology. The idea focuses on modeling social network dynamics in child welfare interventions by integrating a series of mathematical concepts. The practicability of the method remains questioned due to its heavy reliance on complex mathematical frameworks.}\n\\label{tab:LLM+Math+Criminology}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_226', 'cites': [], 'refs': [], 'content': '\\caption{Case study on using \\envname to write interdisciplinary research papers combining System, Biology, and Criminology. The idea investigates "ghost networks" in marginalized communities, exploring how systemic disruptions and mass incarceration affect perceptions of safety, justice, and social cohesion while incorporating the role of policing technologies in this dynamic.}\n\\label{tab:System+Biology+Criminology}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_227', 'cites': [], 'refs': [], 'content': '\\caption{Case study on using \\envname to write interdisciplinary research papers combining LLM, Biology, and Criminology. The idea offers a novel interdisciplinary approach that developing an online platform that detects online toxicity in real-time while addressing its societal impacts on marginalized communities.}\n\\label{tab:LLM+Biology+Criminology}', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_228', 'cites': [], 'refs': [], 'content': '\\caption{Case study on using \\envname to write interdisciplinary research papers combining Astronomy, Biology, and Criminology. Due to the significant conceptual gap between the three fields, the generated idea heavily leans on terminology accumulation.}\n\\label{tab:Astronomy+Biology+Criminology }', 'type': 'textNode', 'isolation': True}
text_node
{'id': 'text_229', 'cites': [], 'refs': [], 'content': '\\caption{Case study on using \\envname to write interdisciplinary research papers combining LLM, Astronomy, Biology, and Criminology. Due to combining researchers and papers from too many diverse domains, the generated idea becomes an incoherent mix of terms without a clear focus or practical direction.}\n\\label{tab:LLM+Astronomy+Biology+Criminology }', 'type': 'textNode', 'isolation': True}
Paper count:  1
Total nodes:  216
Total edges:  363
Paper nodes:  1
Figure nodes:  0
Table nodes:  24
Text nodes:  191
2
