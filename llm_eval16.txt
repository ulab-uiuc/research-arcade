INFO 09-01 00:51:48 [__init__.py:241] Automatically detected platform cuda.
Loaded LLaVA model: llava-hf/llava-1.5-7b-hf
LLaVA description: The image is a collage of various photos featuring a woman's face. There are multiple instances of t...
Image tag list: [["The image is a collage of various photos featuring a woman's face. There are multiple instances of the woman's face, each with different expressions and poses. The photos are arranged in a grid-like pattern, with each image occupying a square on the grid. The grid is composed of multiple rows and columns, with the images placed in a visually appealing manner. The collage showcases the woman's face in various forms and styles, creating a diverse and engaging visual experience."]]
[WARN] Path not found: download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Path not found: download/output/figures/2310.07894v1/tex_figures_5d.png
Image tag list: [None, None]
[WARN] Path not found: download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
Image tag list: [None]
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_demog
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_time
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_model
[WARN] Path not found: download/output/figures/2508.00717/results_ai_pays_dem
Image tag list: [None, None, None, None]
Loaded LLaVA model: llava-hf/llava-1.5-7b-hf
LLaVA description: The image features a black background with a pattern of dots, which are scattered across the entire ...
LLaVA description: The image is a black and white graphic featuring a pattern of dots, with each dot having a different...
Image tag list: [['The image features a black background with a pattern of dots, which are scattered across the entire frame. The dots are of various sizes and colors, creating a visually interesting and dynamic scene. The dots are arranged in a way that they appear to be connected, forming a network-like pattern.\n\nThe dots are distributed in different areas of the image, with some appearing closer to the foreground and others further in the background. The overall composition of the image is intriguing and captivating, with the dots creating a sense of depth and complexity.'], ['The image is a black and white graphic featuring a pattern of dots, with each dot having a different color. The dots are arranged in a grid-like pattern, creating a visually striking and intricate design. The colors of the dots include blue, orange, and gray, which are distributed throughout the pattern. The dots are of varying sizes, adding depth and complexity to the overall composition. The image appears to be a close-up of the pattern, emphasizing the details and intricacies of the design.']]
Loaded LLaVA model: llava-hf/llava-1.5-7b-hf
LLaVA description: The image displays a graph with multiple lines and columns, likely representing data or information....
LLaVA description: The image displays a graph with multiple lines and columns, likely representing data or information....
LLaVA description: The image displays a graph with multiple lines and columns, likely representing data or information....
LLaVA description: The image displays a graph with multiple lines, each representing a different data set. The lines ar...
LLaVA description: The image displays a graph with multiple lines, each representing a different data set. The lines ar...
LLaVA description: The image displays a graph with multiple lines, each representing a different data set. The lines ar...
LLaVA description: The image is a collection of graphs and charts, each displaying different types of data. There are a...
LLaVA description: The image displays a graph with two different types of data, one on the left side and the other on t...
Image tag list: [['The image displays a graph with multiple lines and columns, likely representing data or information. The graph is divided into three sections, each with its own set of lines and columns. The first section is on the left side of the graph, the second section is in the middle, and the third section is on the right side.\n\nThere are several lines in each section, with some of them appearing to be vertical and others horizontal. The lines are labeled with numbers, indicating that they represent specific data points or categories. The graph seems to be a visual representation of data, possibly related to scientific or technical fields.'], ['The image displays a graph with multiple lines and columns, likely representing data or information. The graph is divided into three sections, each with its own set of lines and columns. The first section is located on the left side of the graph, the second section is in the middle, and the third section is on the right side.\n\nThere are several lines in each section, with some of them appearing to be vertical and others horizontal. The lines vary in length and height, indicating different data points or categories. The columns within each section are labeled with numbers, possibly representing specific data points or categories.\n\nOverall, the graph appears to be a visual representation of data or information, with each section and line providing different insights into the subject matter.'], ['The image displays a graph with multiple lines and columns, likely representing data or information. The graph is divided into sections, with each section containing a different color. The lines within the graph are labeled with numbers, indicating a specific order or hierarchy.\n\nThere are several columns in the graph, each containing a different color, which may represent different categories or aspects of the data being analyzed. The graph appears to be a visual representation of data, possibly related to a scientific or technical field.'], ['The image displays a graph with multiple lines, each representing a different data set. The lines are color-coded, with some being blue, green, and orange. The graph is organized in a way that allows for easy comparison and analysis of the data.\n\nThere are two main sections in the graph, with one section being larger and occupying the majority of the image, while the other section is smaller and located towards the bottom right corner. The smaller section appears to be a subplot of the larger section, providing additional information or a different perspective on the data.\n\nThe graph is likely used for scientific or technical purposes, as it presents data in a clear and organized manner, allowing for easy interpretation and comparison.'], ['The image displays a graph with multiple lines, each representing a different data set. The lines are color-coded, with some being blue, green, and orange. The graph is organized in a way that allows for easy comparison and analysis of the data.\n\nThere are several columns in the graph, each containing a different set of data. The columns are labeled with numbers, indicating the specific data set they represent. The graph is divided into sections, with each section containing a different color-coded line.\n\nThe graph is likely used for scientific or technical purposes, as it provides a visual representation of the data sets and allows for easy comparison and analysis. The use of color-coded lines helps to differentiate between the various data sets and makes it easier to identify trends or patterns in the data.'], ['The image displays a graph with multiple lines, each representing a different data set. The lines are color-coded, with some being green, blue, and orange. The graph is divided into sections, with each section containing a different number of lines. The lines are arranged in a way that allows for easy comparison and analysis of the data.\n\nThe graph is accompanied by a text description, which provides additional information about the data being displayed. The text is located at the top of the graph, making it easy to read and understand. The combination of the graph and the text description allows for a clear visual representation of the data and its significance.'], ['The image is a collection of graphs and charts, each displaying different types of data. There are a total of 12 graphs, each with varying colors and shapes, arranged in a grid-like pattern. The graphs are organized in a way that allows for easy comparison and analysis of the data.\n\nThe graphs showcase a range of information, including average accuracy, order number, and test number. The graphs are labeled with numbers, indicating the specific data being displayed. The graphs are spread across the entire image, with some positioned closer to the top, middle, and bottom sections. Overall, the image presents a comprehensive visual representation of various data sets.'], ['The image displays a graph with two different types of data, one on the left side and the other on the right side. The left side of the graph shows a progressive increase in performance, while the right side shows a progressive decrease in performance. The graph is divided into several sections, each representing a different data point.\n\nThere are multiple columns and rows in the graph, with each row representing a different data point, and each column representing a different data category. The data points are labeled with numbers, indicating their specific positions within the graph. The graph provides a clear visual representation of the relationship between the two types of data, allowing for easy comparison and analysis.']]
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
Prompt list: ['You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Manifold Preserving Guided Diffusion\nAbstract: Despite the recent advancements, conditional image generation still faces challenges of cost, generalizability, and the need for task-specific training. In this paper, we propose Manifold Preserving Guided Diffusion (MPGD), a training-free conditional generation framework that leverages pretrained diffusion models and off-the-shelf neural networks with minimal additional inference cost for a broad range of tasks. Specifically, we leverage the manifold hypothesis to refine the guided diffusion steps and introduce a shortcut algorithm in the process. We then propose two methods for on-manifold training-free guidance using pre-trained autoencoders and demonstrate that our shortcut inherently preserves the manifolds when applied to latent diffusion models. Our experiments show that MPGD is efficient and effective for solving a variety of conditional generation applications in low-compute settings, and can consistently offer up to 3.8x speed-ups with the same number of diffusion steps while maintaining high sample quality compared to the baselines.\nSection name: Experiments\n\nFigure block (optional):\n- label: \\label{fig:qualitative_faceid}; caption: \\caption{Examples of the FaceID guidance generation with pre-trained CelebA-HQ models. }\nTable block (optional):\n- label: \\label{tab:faceid}; text: \\begin{tabular}{@{}l|ccc@{}} \\toprule \\textbf{Method} & \\textbf{KID}$\\downarrow$        & \\textbf{FaceID}$\\downarrow$ & \\textbf{Time}$\\downarrow$ \\\\ \\midrule DDIM            & 0.0442          & 1.3914               & 3.41s             \\\\ \\m\nCited paper titles/abstracts (optional):\n- [lgd] Loss-guided diffusion models for plug-and-play controllable generation.: \n- [freedom] FreeDoM: Training-free energy-guided conditional diffusion model.: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. \\vspace{-5pt}\n\\subsection{Pixel space diffusion models}\nIn this section, we evaluate the performance of our proposed pixel domain methods (i.e. \\textbf{MPGD w/o Proj.}, \\textbf{MPGD-AE} and \\textbf{MPGD-Z}) with two different sets of conditional image generation tasks: solving liner inverse problems and human face generation guided by face recognition loss, which we refer to as FaceID guidance generation. For MPGD-AE and MPGD-Z, we use the pre-trained VQGAN models provided by~\\citet{rombach2021highresolution}.\n\\subsubsection{Noisy Linear Inverse Problem}\n2. For linear tasks, we use noisy super-resolution and noisy Gaussian deblurring as the test bed. We choose DPS~\\citep{dps}, LGD-MC~\\citep{lgd}, and MCG~\\citep{mcg} as the basleines. We test each method with two pre-trained diffusion models provided by~\\citet{dps}: one trained on FFHQ dataset~\\citep{karras2019style} and another on ImageNet~\\citep{deng2009imagenet}, both with $256 \\times 256$256 \\times 256 resolution. \nFor super-resolution, we down-sample ground truth images to $64 \\times 64$64 \\times 64. In the Gaussian deblurring task, we apply a $61\\times 61$61\\times 61 sized Gaussian blur with kernel intensity $3.0$3.0. to original images, Measurements in both tasks have a random noise with a variance of $\\sigma^{2}=0.05^{2}$\\sigma^{2}2=0.05^{2}2. We evaluate each task on a set of 1000~samples. We use the Kernel Inception distance (KID)~\\citep{binkowski2018demystifying} to assess the fidelity, Learned Perceptual Image Patch Similarity (LPIPS)~\\citep{zhang2018unreasonable} to evaluate the guidance quality, and the inference time to test the efficiency of the methods. All experiments are conducted on a single NVIDIA GeForce RTX 2080 Ti GPU . Figure~\\ref{fig:linear_main} shows the generated examples for qualitative comparison, and Figure~\\ref{fig:ffhq_sr} presents the quantitative results for the super-resolution task on FFHQ. All three of our methods significantly outperform the baselines with all metrics tested across a variety of different numbers of DDIM steps, and we can observe manifold projection improves the sample fidelity by a large margin.\nNext:\n1. \\hfill\n    [t]{0.53\\textwidth}0.53\\textwidth\n        \\centering\n        \\caption{Quantitative results for style guidance with Stable Diffusion experiment. Our method finds the sweet spot between following the prompt and following the style guidance.}\n        \\label{tab:style}\n2. \\vspace{-7pt}\n\\subsection{Latent diffusion models}\nTo evaluate \\textbf{MPGD-LDM}, we test our methods against the same baselines as the pixel-space FaceID experiments with text-to-image style guided generation task. The goal of this task is to generate images that fit both the text input prompts and the style of the reference images.\nWe use Stable Diffusion~\\citep{rombach2021highresolution} as the pre-trained text-to-image model and deploy the guided sampling methods to incorporate a style loss, which is calculated by the Frobenius norm between the Gram matrices of the reference images and the generated images.\nFor reference style images and text prompts, we randomly created 1000 conditioning pairs, using images from WikiArt~\\citep{saleh2015large} and prompts from PartiPrompts~\\citep{yu2022scaling} dataset. \nFigure~\\ref{fig:style_main} and Table~\\ref{tab:style} show qualitative and quantitative results for this experiment respectively. All the samples are generated on a single NVIDIA A100 GPU with 100 DDIM steps.\nOur method finds the sweet spot between following the text prompts, which usually instruct the generation to generate photo realistic images that do not suit the given style, and following the style guidance, which deviate from the prompts. Notably, because MPGD does not require propagation through the diffusion model, our method can provide significant speedup and can be fitted into a 16GB GPU while all the other methods cannot.\n3. \\vspace{-7pt}\n\nTarget length (characters): ~1212 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:qualitative_faceid\']\ntable_labels = [\'tab:faceid\']\nbib_keys = [\'freedom\', \'lgd\', \'freedom\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Efficient Integrators for Diffusion Generative Models\nAbstract: Diffusion models suffer from slow sample generation at inference time. Therefore, developing a principled framework for fast deterministic/stochastic sampling for a broader class of diffusion models is a promising direction. We propose two complementary frameworks for accelerating sample generation in pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate integrators generalize DDIM, mapping the reverse diffusion dynamics to a more amenable space for sampling. In contrast, splitting-based integrators, commonly used in molecular dynamics, reduce the numerical simulation error by cleverly alternating between numerical updates involving the data and auxiliary variables. After extensively studying these methods empirically and theoretically, we present a hybrid method that leads to the best-reported performance for diffusion models in augmented spaces. Applied to Phase Space Langevin Diffusion [Pandey & Mandt, 2023] on CIFAR-10, our deterministic and stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing baselines, respectively. Our code and model checkpoints wil\nSection name: Additional Experimental Results\n\nFigure block (optional):\n- label: \\label{fig:precond_a}; caption: \\caption{}\n- label: \\label{fig:precond_b}; caption: \\caption{}\nTable block (optional):\n- label: \\label{table:main}; text: \\begin{tabular}{@{}clcccc@{}} \\toprule                                         &               &                                                             &           & \\multicolumn{2}{c}{NFE (FID@50k $\\downarrow$)} \\\\ \\midrule           \nCited paper titles/abstracts (optional):\n- [songdenoising] Denoising diffusion implicit models.: \n- [lu2022dpm] Dpm-solver: A fast ode solver for diffusion probabilistic model   sampling in around 10 steps.: \n- [zhang2023fast] Fast sampling of diffusion models with exponential integrator.: \n- [karraselucidating] Elucidating the design space of diffusion-based generative models.: \n- [liupseudo] Pseudo numerical methods for diffusion models on manifolds.: \n- [bao2022analyticdpm] Analytic-DPM: an analytic estimate of the optimal reverse variance   in diffusion probabilistic models.: \n- [xue2023sasolver] Sa-solver: Stochastic adams solver for fast sampling of diffusion   models, 2023: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. \\textbf{Notation.} For simplicity, we denote our best-performing Reduced Splitting and Conjugate Splitting integrators as \\textit{\\gls{SPS}} and \\textit{\\gls{CSPS}}, respectively. Consequently, we refer to the \\underline{D}eterministic \\gls{RVV}RVV and \\underline{S}tochastic \\gls{ROBA}ROBA samplers as \\gls{SPS}SPS-D and \\gls{SPS}SPS-S, and their conjugate variants as \\gls{CSPS}CSPS-D and \\gls{CSPS}CSPS-S, respectively.\n2. \\textbf{Datasets and Evaluation Metrics.} We use the CIFAR-10, CelebA-64 \\citep{liu2015faceattributes} and the AFHQ-v2 \\citep{choi2020stargan} datasets for comparisons. Unless specified otherwise, we report FID for 50k generated samples for all datasets and quantify sampling efficiency using \\gls{NFE}NFE. We include full experimental details in Appendix \\ref{app:exp_setup}.\nNext:\n1. \\textbf{Empirical Observations:} For CIFAR-10, our ODE sampler performs comparably or outperforms all other baselines for \\gls{NFE}NFE $\\geq$\\geq 50 (Fig. \\ref{fig:sota_all}, Top Left). Similarly, our SDE sampler outperforms all other baselines for \\gls{NFE}NFE $\\geq$\\geq 40 (Fig. \\ref{fig:sota_all}, Top Right). We make similar observations for the CelebA-64 and AFHQv2-64 datasets, where our proposed samplers can obtain significant gains over prior methods for \\gls{NFE}NFE $\\geq$\\geq 70 (See Fig. \\ref{fig:sota_all}, Bottom Left). Therefore, our proposed samplers for \\gls{PSLD}PSLD are competitive with recent work. Moreover, for all datasets, our stochastic sampler achieves better sample quality for low sampling budgets (\\gls{NFE}NFE $<$< 50) as compared to our deterministic sampler. \nLastly, in contrast to CIFAR-10, we find that the \\gls{CSPS}CSPS-S sampler works better than the \\gls{SPS}SPS-S sampler for the CelebA-64 and AFHQv2-64 datasets, indicating its effectiveness for higher-resolution sampling.\n\nTarget length (characters): ~1323 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:precond_a\', \'fig:precond_b\']\ntable_labels = [\'table:main\']\nbib_keys = [\'songdenoising\', \'zhang2023fast\', \'lu2022dpm\', \'liupseudo\', \'karraselucidating\', \'xue2023sasolver\', \'bao2022analyticdpm\', \'karraselucidating\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average\nAbstract: Presently the most successful approaches to semi-supervised learning are based on consistency regularization, whereby a model is trained to be robust to small perturbations of its inputs and parameters. To understand consistency regularization, we conceptually explore how loss geometry interacts with training procedures. The consistency loss dramatically improves generalization performance over supervised-only training; however, we show that SGD struggles to converge on the consistency loss and continues to make large steps that lead to changes in predictions on the test data. Motivated by these observations, we propose to train consistency-based methods with Stochastic Weight Averaging (SWA), a recent approach which averages weights along the trajectory of SGD with a modified learning rate schedule. We also propose fast-SWA, which further accelerates convergence by averaging multiple points within each cycle of a cyclical learning rate schedule. With weight averaging, we achieve the best known semi-supervised results on CIFAR-10 and CIFAR-100, over many different quantities of labeled training data. For example, we achieve 5.0% error on CIFAR-10 with only 4000 labels, compared to \nSection name: Experiments\n\nFigure block (optional):\n- label: \\label{fig:cnn_c10_c100_trend}; caption: \\caption{Prediction errors of base models and their weight averages (\\fastswa and \\swa) for CNN    on \\textbf{(left)} CIFAR-10 with $4k$ labels,   \\textbf{(middle)} CIFAR-100 with $10k$ labels,   and \\textbf{(right)} CIFAR-100 $50k$ labels \nTable block (optional):\n- label: \\label{table:cifar100_shakeshake}; text: \\begin{tabular}{l c c c c c l l l l l } Number of labels\t\t\t& \t10k \t\t& \t50k \t\t& \t50k + 500k \t\t& 50k +   237k$^*$ \t\\\\  \\midrule TE (CNN) \\citep{te}  & \t38.65 \\tpm 0.51\t\t&\t26.30 \\tpm 0.15\t\t&\t23.62 \\tpm 0.17  \t&\t 23.79 \\tpm 0.17  \t\\\\  \\midrule \nCited paper titles/abstracts (optional):\n- [te] Temporal Ensembling for Semi-Supervised Learning.: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. In Figure \\ref{fig:cnn_c10_c100_trend} (left), we visualize the test\nerror as a function of iteration using the CNN.\nWe observe that when the cyclical learning rate starts after epoch $\\ell=180$\\ell=180, \nthe base models drop in performance due to the sudden increase in learning rate (see Figure \\ref{fig:swa_model} left).\nHowever, \\fastswa continues to improve while collecting the weights corresponding to high learning rates for averaging.\nIn general, we also find that the cyclical learning rate improves the base models beyond the usual cosine annealing \nschedule and increases the performance of \\fastswa as training progresses. \nCompared to \\swa, we also observe that \\fastswa converges substantially faster, \nfor instance, reducing the error to $10.5\\%$10.5\\%  at epoch $200$200 while \\swa attains \nsimilar error at epoch $350$350 for CIFAR-10 $4k$4k labels (Figure \n\\ref{fig:cnn_c10_c100_trend} left). We provide additional plots in Section \\ref{sec:add_results} \nshowing the convergence of the \\pi and MT models in all label \nsettings, where we observe similar trends that \\fastswa results in faster error reduction.\n2. We also find that the performance gains of \\fastswa over base models are higher for the \\pi model compared to the MT model, which is consistent with the convexity observation in Section \\ref{sec:empirical_convexity} \nand Figure \\ref{fig:convexity}.\nIn the previous evaluations \\citep[see e.g.][]{ssl-eval, mt}, the \\pi model was shown to be inferior to the MT model.\nHowever, with weight averaging, \\fastswa reduces the gap between \\pi and MT performance. \nSurprisingly, we find that the \\pi model can outperform MT after applying \\fastswa  \nwith moderate to large numbers of labeled points. \nIn particular, the $\\Pi$\\Pi+fast-SWA model outperforms MT+\\fastswa on CIFAR-10 with $4k$4k, $10k$10k, and $50k$50k labeled\ndata points for the Shake-Shake  architecture.\n3. \\vspace{-0.8\\baselineskip}\n4. \\subsection{CIFAR-100 and Extra Unlabeled Data} \\label{sec:cifar100_cnn} \\label{sec:cifar100}\n5. We evaluate the \\pi and MT models with \\fastswa on \nCIFAR-100.\nWe train our models using $50000$50000 images with $10k$10k and $50k$50k labels using the $13$13-layer CNN. \nWe also analyze the effect of using the Tiny Images dataset \\citep{tiny_images} as an additional source of unlabeled data.\nNext:\n1. \\subsection{Advancing State-of-the-Art} \\label{sec:soa}\n2. We have shown that \\fastswa can significantly improve the performance of both the \\pi and \nMT models. We provide a summary comparing our results with the previous best results in the literature in Table  \\ref{table:soa}, using the $13$13-layer CNN and\nthe Shake-Shake architecture that had been applied previously. We also provide detailed results the Appendix \\ref{sec:add_results}.\n3. \\subsection{Preliminary Results on Domain Adaptation}\n\\label{sec:domain_adaptation}\nDomain adaptation problems involve learning using a source domain $X_s$X_s \nequipped with labels $Y_s$Y_s and performing classification on the target \ndomain $X_t$X_t while having no access to the target labels at training time. \nA recent model by \\citet{da_mt} applies the consistency enforcing principle for \ndomain adaptation and achieves state-of-the-art results on many datasets. \nApplying \\fastswa to this model on domain adaptation from CIFAR-10 to STL we \nwere able to improve the best results reported in the literature from\n$19.9\\%$19.9\\% to $16.8\\%$16.8\\%. \nSee Section \\ref{sec:exp_da} for more details on the domain adaptation experiments.\n\nTarget length (characters): ~1438 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:cnn_c10_c100_trend\']\ntable_labels = [\'table:cifar100_shakeshake\']\nbib_keys = [\'te\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Generative AI in Higher Education: Evidence from an Elite College\nAbstract: Generative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPT\'s release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AI\'s potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AI\'s educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.\nSection name: Generative AI Usage Patterns Among Students\n\nFigure block (optional):\n- label: \\label{fig:ai_use}; caption: \\caption{The Adoption of Generative AI among Middlebury College Students}\n- label: \\label{fig:ai_use_time}; caption: \\caption{The Evolution of Generative AI Adoption among Middlebury College Students}\n- label: \\label{fig:ai_models}; caption: \\caption{Adoption of Generative AI Models Among College Students}\n- label: \\label{fig:ai_pays_dem}; caption: \\caption{Percent of Students Who Pay for Generative AI Tools}\nTable block (optional):\n- label: \\label{tab:ai_usage_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{8}{c}{Outcome: Uses AI during the semester with frequency of at least...}  \\\\\\cmidrule{3-10}  \t\t\t\t\t%\t\t\n- label: \\label{tab:ai_adopt_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{10}{c}{Outcome: Started using generative AI...}  \\\\\\cmidrule{3-12}  \t\t\t\t\t&& Before\n- label: \\label{tab:ai_freq_het}; text: \\begin{tabular}{lccccc} \t\t\t\t\t\t\\toprule \t\t\t\t\t\t&& \\multicolumn{4}{c}{By Usage Frequency} \\\\ \\cmidrule{3-6} \t\t\t\t\t\t& Any use & Rarely & Occasionally & Frequently & Very Frequently \\\\ \t\t\t\t\t\t& (1) & (2) & (3) & (4) & (5) \\\\ \\midrule\t\t\t\t\t\t \t\t\t\t\t\t\\\n- label: \\label{tab:ai_model_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{10}{c}{Outcome: =1 if student uses}  \\\\\\cmidrule{3-12}  \t\t\t\t\t&& OpenAI\'s && Google\nCited paper titles/abstracts (optional):\n- [bick.etal2025] The rapid adoption of generative AI.: \n- [mcclain2024] Americans\' use of ChatGPT is ticking up, but few trust its   election information: \n- [nam2023ai] 56\\% of college students have used ai on assignments or exams.: \n- [stohr.etal2024] \\"o: \n- [ravselj.etal2025] s: \n- [otis2024global] Global Evidence on Gender Gaps and Generative AI.: \n- [hartley.etal2025] The Labor Market Effects of Generative Artificial   Intelligence: \n- [humlum.vestergaard2025] Large Language Models, Small Labor Market Effects: \n- [carvajal2024] Will artificial intelligence get in the way of achieving gender   equality?: \n- [david1990dynamo] The dynamo and the computer: an historical perspective on the modern   productivity paradox.: \n- [gallup2024] AI in the Workplace: Answering 3 Big Questions: \n- [hall2003adoption] Adoption of new technology: \n- [stokey2021technology] Technology diffusion.: \n- [world2016world] World Bank: \n- [bick.etal2025] The rapid adoption of generative AI.: \n- [mcclain2024] Americans\' use of ChatGPT is ticking up, but few trust its   election information: \n- [nam2023ai] 56\\% of college students have used ai on assignments or exams.: \n- [stohr.etal2024] \\"o: \n- [ravselj.etal2025] s: \n- [otis2024global] Global Evidence on Gender Gaps and Generative AI.: \n- [hartley.etal2025] The Labor Market Effects of Generative Artificial   Intelligence: \n- [humlum.vestergaard2025] Large Language Models, Small Labor Market Effects: \n- [carvajal2024] Will artificial intelligence get in the way of achieving gender   equality?: \n- [david1990dynamo] The dynamo and the computer: an historical perspective on the modern   productivity paradox.: \n- [gallup2024] AI in the Workplace: Answering 3 Big Questions: \n- [hall2003adoption] Adoption of new technology: \n- [stokey2021technology] Technology diffusion.: \n- [world2016world] World Bank: \n\nk-most adjacent paragraphs (context):\n(none)\n\nTarget length (characters): ~13653 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:ai_use\', \'fig:ai_use_time\', \'fig:ai_models\', \'fig:ai_pays_dem\']\ntable_labels = [\'tab:ai_usage_correlates\', \'tab:ai_adopt_correlates\', \'tab:ai_freq_het\', \'tab:ai_model_correlates\']\nbib_keys = [\'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'nam2023ai\', \'stohr.etal2024\', \'carvajal2024\', \'ravselj.etal2025\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'david1990dynamo\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'hall2003adoption\', \'otis2024global\', \'nam2023ai\', \'stokey2021technology\', \'carvajal2024\', \'stohr.etal2024\', \'world2016world\', \'ravselj.etal2025\', \'carvajal2024\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Neural-Symbolic Message Passing with Dynamic Pruning\nAbstract: Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs) is a challenging task. Recently, a line of message-passing-based research has been proposed to solve CQA. However, they perform unsatisfactorily on negative queries and fail to address the noisy messages between variable nodes in the query graph. Moreover, they offer little interpretability and require complex query data and resource-intensive training. In this paper, we propose a Neural-Symbolic Message Passing (NSMP) framework based on pre-trained neural link predictors. By introducing symbolic reasoning and fuzzy logic, NSMP can generalize to arbitrary existential first order logic queries without requiring training while providing interpretable answers. Furthermore, we introduce a dynamic pruning strategy to filter out noisy messages between variable nodes. Experimental results show that NSMP achieves a strong performance. Additionally, through complexity analysis and empirical verification, we demonstrate the superiority of NSMP in inference time over the current state-of-the-art neural-symbolic method. Compared to this approach, NSMP demonstrates faster inference times across all query types on benchmark dat\nSection name: More Details about the Datasets\n\nFigure block (optional):\n- label: \\label{figure4}; caption: \\caption{Graphical representation of the query types of the BetaE dataset considered in our experiment, where $p$, $i$, $u$, and $n$ represent projection, intersection, union, and negation, respectively. }\n- label: \\label{figure5}; caption: \\caption{Graphical representation of the query types of the FIT dataset considered in our experiment, where $l$, $m$, and $c$ represent existential leaf, multi graph, and circle, respectively. }\nTable block (optional):\n- label: \\label{KG stat}; text: \\begin{tabular}{ccccccc}     \\toprule     \\textbf{Knowledge Graph} & \\textbf{Entities} & \\textbf{Relations} & \\textbf{Training Edges} & \\textbf{Val Edges} & \\textbf{Test Edges} & \\textbf{Total Edges} \\\\     \\midrule     FB15k-237 & 14,505 &\n- label: \\label{BetaE stat}; text: \\begin{tabular}{ccccccc}     \\toprule     \\multirow{2}{*}{\\textbf{Knowledge Graph}} & \\multicolumn{2}{c}{\\textbf{Training Queries}} & \\multicolumn{2}{c}{\\textbf{Validation Queries}} & \\multicolumn{2}{c}{\\textbf{Test Queries}} \\\\     \\cmidru\n- label: \\label{FIT stat}; text: \\begin{tabular}{ccccccccccc}     \\toprule     \\textbf{Knowledge Graph} & \\textbf{pni} & \\textbf{2il} & \\textbf{3il} & \\textbf{2m} & \\textbf{2nm} & \\textbf{3mp} & \\textbf{3pm} & \\textbf{im} & \\textbf{3c} & \\textbf{3cm} \\\\     \\midrule     FB\nCited paper titles/abstracts (optional):\n- [ren2020beta] Beta embeddings for multi-hop logical reasoning in knowledge graphs: \n- [yin2024rethinking] Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors: \n\nk-most adjacent paragraphs (context):\n(none)\n\nTarget length (characters): ~973 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'figure4\', \'figure5\']\ntable_labels = [\'KG stat\', \'BetaE stat\', \'FIT stat\']\nbib_keys = [\'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Progressive Prompts: Continual Learning for Language Models\nAbstract: We introduce Progressive Prompts - a simple and efficient approach for continual learning in language models. Our method allows forward transfer and resists catastrophic forgetting, without relying on data replay or a large number of task-specific parameters. Progressive Prompts learns a new soft prompt for each task and sequentially concatenates it with the previously learned prompts, while keeping the base model frozen. Experiments on standard continual learning benchmarks show that our approach outperforms state-of-the-art methods, with an improvement >20% in average test accuracy over the previous best-preforming method on T5 model. We also explore a more challenging continual learning setup with longer sequences of tasks and show that Progressive Prompts significantly outperforms prior methods.\nSection name: Further experimental results\n\nFigure block (optional):\n- label: \\label{fig:fwt_order8}; caption: \\caption{Forward transfer score of different approaches on order 8. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:fwt_order9}; caption: \\caption{Forward transfer score of different approaches on order 9. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:fwt_order10}; caption: \\caption{Forward transfer score of different approaches on order 10. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order8}; caption: \\caption{Backward transfer score of different approaches on order 8. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order9}; caption: \\caption{Backward transfer score of different approaches on order 9. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order10}; caption: \\caption{Backward transfer score of different approaches on order 10. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:evolution}; caption: \\caption{Evolution of average accuracy after learning new tasks.}\n- label: \\label{fig:fig_long_bars}; caption: \\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improve\nTable block (optional):\n- label: \\label{wrap-tab:init}; text: \\begin{tabular}{lcc}\\\\\\toprule   Method & Few-shot & Full-shot \\\\\\midrule PT + Prev. Init. & 48.2 & 50.0 \\\\  ProgPrompt & \\textbf{53.5} & \\textbf{69.3} \\\\ \\bottomrule \\end{tabular}\n- label: \\label{tab:table_appendix_long}; text: \\begin{tabular}{lccc|ccc|ccc|ccc} % <-- Alignments: 1st column left & 2nd middle and 3rd right & with vertical lines in between     \\toprule       \\textbf{Method} $\\downarrow$ & \\multicolumn{3}{c}{\\textbf{Order 8}}  & \\multicolumn{3}{c}{\\te\nCited paper titles/abstracts (optional):\n- [lopez2017gradient] Gradient episodic memory for continual learning: \n\nk-most adjacent paragraphs (context):\nNext:\n1. \\includegraphics[width=0.9\\textwidth]{images/long_exp_bars.png}\n\\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improvement of Progressive Prompts test score compared to per-task prompt on the corresponding task.}\n\\label{fig:fig_long_bars}\n\\end{figure}\n2. \\includegraphics[width=0.9\\textwidth]{images/long_exp_bars.png}\n\\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improvement of Progressive Prompts test score compared to per-task prompt on the corresponding task.}\n\\label{fig:fig_long_bars}\n3. \\includegraphics[width=0.7\\textwidth]{images/superglue_illustration.png}\n4. \\caption{Original prompt tuning versus Progressive Prompts on SuperGLUE datasets. For illustration, we show how SuperGLUE task WiC is leaned (we have similar scheme for other tasks). Prompt tuning trains a single prompt of 100 tokens for WiC task. Progressive Prompts method learns a prompt of 40 tokens, which is progressively appended to the six frozen prompts of 10 tokens learned on GLUE benchmark (with random task order). Total prompt length is equal in both approaches. }\n\\label{fig:fig_super_illustration}\n\\end{figure}\n5. \\includegraphics[width=0.7\\textwidth]{images/superglue_illustration.png}\n\nTarget length (characters): ~1627 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:fwt_order8\', \'fig:fwt_order9\', \'fig:fwt_order10\', \'fig:bwt_order8\', \'fig:bwt_order9\', \'fig:bwt_order10\', \'fig:evolution\', \'fig:fig_long_bars\']\ntable_labels = [\'wrap-tab:init\', \'tab:table_appendix_long\']\nbib_keys = [\'lopez2017gradient\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).']
Number of prompts: 6
Image tag lists: [[["The image is a collage of various photos featuring a woman's face. There are multiple instances of the woman's face, each with different expressions and poses. The photos are arranged in a grid-like pattern, with each image occupying a square on the grid. The grid is composed of multiple rows and columns, with the images placed in a visually appealing manner. The collage showcases the woman's face in various forms and styles, creating a diverse and engaging visual experience."]], [None, None], [None], [None, None, None, None], [['The image features a black background with a pattern of dots, which are scattered across the entire frame. The dots are of various sizes and colors, creating a visually interesting and dynamic scene. The dots are arranged in a way that they appear to be connected, forming a network-like pattern.\n\nThe dots are distributed in different areas of the image, with some appearing closer to the foreground and others further in the background. The overall composition of the image is intriguing and captivating, with the dots creating a sense of depth and complexity.'], ['The image is a black and white graphic featuring a pattern of dots, with each dot having a different color. The dots are arranged in a grid-like pattern, creating a visually striking and intricate design. The colors of the dots include blue, orange, and gray, which are distributed throughout the pattern. The dots are of varying sizes, adding depth and complexity to the overall composition. The image appears to be a close-up of the pattern, emphasizing the details and intricacies of the design.']], [['The image displays a graph with multiple lines and columns, likely representing data or information. The graph is divided into three sections, each with its own set of lines and columns. The first section is on the left side of the graph, the second section is in the middle, and the third section is on the right side.\n\nThere are several lines in each section, with some of them appearing to be vertical and others horizontal. The lines are labeled with numbers, indicating that they represent specific data points or categories. The graph seems to be a visual representation of data, possibly related to scientific or technical fields.'], ['The image displays a graph with multiple lines and columns, likely representing data or information. The graph is divided into three sections, each with its own set of lines and columns. The first section is located on the left side of the graph, the second section is in the middle, and the third section is on the right side.\n\nThere are several lines in each section, with some of them appearing to be vertical and others horizontal. The lines vary in length and height, indicating different data points or categories. The columns within each section are labeled with numbers, possibly representing specific data points or categories.\n\nOverall, the graph appears to be a visual representation of data or information, with each section and line providing different insights into the subject matter.'], ['The image displays a graph with multiple lines and columns, likely representing data or information. The graph is divided into sections, with each section containing a different color. The lines within the graph are labeled with numbers, indicating a specific order or hierarchy.\n\nThere are several columns in the graph, each containing a different color, which may represent different categories or aspects of the data being analyzed. The graph appears to be a visual representation of data, possibly related to a scientific or technical field.'], ['The image displays a graph with multiple lines, each representing a different data set. The lines are color-coded, with some being blue, green, and orange. The graph is organized in a way that allows for easy comparison and analysis of the data.\n\nThere are two main sections in the graph, with one section being larger and occupying the majority of the image, while the other section is smaller and located towards the bottom right corner. The smaller section appears to be a subplot of the larger section, providing additional information or a different perspective on the data.\n\nThe graph is likely used for scientific or technical purposes, as it presents data in a clear and organized manner, allowing for easy interpretation and comparison.'], ['The image displays a graph with multiple lines, each representing a different data set. The lines are color-coded, with some being blue, green, and orange. The graph is organized in a way that allows for easy comparison and analysis of the data.\n\nThere are several columns in the graph, each containing a different set of data. The columns are labeled with numbers, indicating the specific data set they represent. The graph is divided into sections, with each section containing a different color-coded line.\n\nThe graph is likely used for scientific or technical purposes, as it provides a visual representation of the data sets and allows for easy comparison and analysis. The use of color-coded lines helps to differentiate between the various data sets and makes it easier to identify trends or patterns in the data.'], ['The image displays a graph with multiple lines, each representing a different data set. The lines are color-coded, with some being green, blue, and orange. The graph is divided into sections, with each section containing a different number of lines. The lines are arranged in a way that allows for easy comparison and analysis of the data.\n\nThe graph is accompanied by a text description, which provides additional information about the data being displayed. The text is located at the top of the graph, making it easy to read and understand. The combination of the graph and the text description allows for a clear visual representation of the data and its significance.'], ['The image is a collection of graphs and charts, each displaying different types of data. There are a total of 12 graphs, each with varying colors and shapes, arranged in a grid-like pattern. The graphs are organized in a way that allows for easy comparison and analysis of the data.\n\nThe graphs showcase a range of information, including average accuracy, order number, and test number. The graphs are labeled with numbers, indicating the specific data being displayed. The graphs are spread across the entire image, with some positioned closer to the top, middle, and bottom sections. Overall, the image presents a comprehensive visual representation of various data sets.'], ['The image displays a graph with two different types of data, one on the left side and the other on the right side. The left side of the graph shows a progressive increase in performance, while the right side shows a progressive decrease in performance. The graph is divided into several sections, each representing a different data point.\n\nThere are multiple columns and rows in the graph, with each row representing a different data point, and each column representing a different data category. The data points are labeled with numbers, indicating their specific positions within the graph. The graph provides a clear visual representation of the relationship between the two types of data, allowing for easy comparison and analysis.']]]
Number of Image tag lists: 6
Number of Image projection lists: 6
INFO 09-01 00:53:16 [utils.py:326] non-default args: {'model': 'Qwen/Qwen3-8B', 'enable_prompt_embeds': True, 'trust_remote_code': True, 'dtype': 'float16', 'gpu_memory_utilization': 0.6, 'disable_log_stats': True, 'enforce_eager': True}
INFO 09-01 00:53:23 [__init__.py:711] Resolved architecture: Qwen3ForCausalLM
WARNING 09-01 00:53:23 [__init__.py:2819] Casting torch.bfloat16 to torch.float16.
INFO 09-01 00:53:23 [__init__.py:1750] Using max model len 40960
WARNING 09-01 00:53:23 [arg_utils.py:1770] --enable-prompt-embeds is not supported by the V1 Engine. Falling back to V0. 
WARNING 09-01 00:53:23 [arg_utils.py:1544] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.
INFO 09-01 00:53:24 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 09-01 00:53:24 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 09-01 00:53:24 [__init__.py:3565] Cudagraph is disabled under eager mode
INFO 09-01 00:53:24 [llm_engine.py:222] Initializing a V0 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen3-8B', speculative_config=None, tokenizer='Qwen/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{"enable_fusion":false,"enable_noop":false},"max_capture_size":0,"local_cache_dir":null}, use_cached_outputs=False, 
INFO 09-01 00:53:26 [cuda.py:436] Using Flash Attention backend.
INFO 09-01 00:53:26 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 09-01 00:53:26 [model_runner.py:1080] Starting to load model Qwen/Qwen3-8B...
INFO 09-01 00:53:26 [weight_utils.py:296] Using model weights format ['*.safetensors']
INFO 09-01 00:53:31 [default_loader.py:262] Loading weights took 4.18 seconds
INFO 09-01 00:53:31 [model_runner.py:1112] Model loading took 15.2673 GiB and 4.429126 seconds
INFO 09-01 00:53:32 [worker.py:295] Memory profiling takes 0.55 seconds
INFO 09-01 00:53:32 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.53GiB) x gpu_memory_utilization (0.60) = 28.52GiB
INFO 09-01 00:53:32 [worker.py:295] model weights take 15.27GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 11.86GiB.
INFO 09-01 00:53:32 [executor_base.py:114] # cuda blocks: 5396, # CPU blocks: 1820
INFO 09-01 00:53:32 [executor_base.py:119] Maximum concurrency for 40960 tokens per request: 2.11x
INFO 09-01 00:53:34 [llm_engine.py:417] init engine (profile, create kv cache, warmup model) took 2.49 seconds
INFO 09-01 00:53:34 [llm.py:298] Supported_tasks: ['generate']
INFO 09-01 01:01:41 [__init__.py:241] Automatically detected platform cuda.
Loading data with VLM=False, MODEL=Qwen/Qwen2.5-7B-Instruct
Loaded LLaVA model: llava-hf/llava-1.5-7b-hf
LLaVA description: The image is a collage of various photos featuring a woman's face. There are multiple instances of t...
Image tag list: [["The image is a collage of various photos featuring a woman's face. There are multiple instances of the woman's face, each with different expressions and poses. The photos are arranged in a grid-like pattern, with each image occupying a square on the grid. The grid is composed of multiple rows and columns, with the images placed in a visually appealing manner. The collage showcases the woman's face in various forms and styles, creating a diverse and engaging visual experience."]]
[WARN] Path not found: download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Path not found: download/output/figures/2310.07894v1/tex_figures_5d.png
Image tag list: [None, None]
[WARN] Path not found: download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
Image tag list: [None]
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_demog
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_time
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_model
[WARN] Path not found: download/output/figures/2508.00717/results_ai_pays_dem
Image tag list: [None, None, None, None]
Loaded LLaVA model: llava-hf/llava-1.5-7b-hf
LLaVA description: The image features a black background with a pattern of dots, which are scattered across the entire ...
LLaVA description: The image is a black and white graphic featuring a pattern of dots, with each dot having a different...
Image tag list: [['The image features a black background with a pattern of dots, which are scattered across the entire frame. The dots are of various sizes and colors, creating a visually interesting and dynamic scene. The dots are arranged in a way that they appear to be connected, forming a network-like pattern.\n\nThe dots are distributed in different areas of the image, with some appearing closer to the foreground and others further in the background. The overall composition of the image is intriguing and captivating, with the dots creating a sense of depth and complexity.'], ['The image is a black and white graphic featuring a pattern of dots, with each dot having a different color. The dots are arranged in a grid-like pattern, creating a visually striking and intricate design. The colors of the dots include blue, orange, and gray, which are distributed throughout the pattern. The dots are of varying sizes, adding depth and complexity to the overall composition. The image appears to be a close-up of the pattern, emphasizing the details and intricacies of the design.']]
Loaded LLaVA model: llava-hf/llava-1.5-7b-hf
LLaVA description: The image displays a graph with multiple lines and columns, likely representing data or information....
LLaVA description: The image displays a graph with multiple lines and columns, likely representing data or information....
LLaVA description: The image displays a graph with multiple lines and columns, likely representing data or information....
LLaVA description: The image displays a graph with multiple lines, each representing a different data set. The lines ar...
LLaVA description: The image displays a graph with multiple lines, each representing a different data set. The lines ar...
LLaVA description: The image displays a graph with multiple lines, each representing a different data set. The lines ar...
LLaVA description: The image is a collection of graphs and charts, each displaying different types of data. There are a...
LLaVA description: The image displays a graph with two different types of data, one on the left side and the other on t...
Image tag list: [['The image displays a graph with multiple lines and columns, likely representing data or information. The graph is divided into three sections, each with its own set of lines and columns. The first section is on the left side of the graph, the second section is in the middle, and the third section is on the right side.\n\nThere are several lines in each section, with some of them appearing to be vertical and others horizontal. The lines are labeled with numbers, indicating that they represent specific data points or categories. The graph seems to be a visual representation of data, possibly related to scientific or technical fields.'], ['The image displays a graph with multiple lines and columns, likely representing data or information. The graph is divided into three sections, each with its own set of lines and columns. The first section is located on the left side of the graph, the second section is in the middle, and the third section is on the right side.\n\nThere are several lines in each section, with some of them appearing to be vertical and others horizontal. The lines vary in length and height, indicating different data points or categories. The columns within each section are labeled with numbers, possibly representing specific data points or categories.\n\nOverall, the graph appears to be a visual representation of data or information, with each section and line providing different insights into the subject matter.'], ['The image displays a graph with multiple lines and columns, likely representing data or information. The graph is divided into sections, with each section containing a different color. The lines within the graph are labeled with numbers, indicating a specific order or hierarchy.\n\nThere are several columns in the graph, each containing a different color, which may represent different categories or aspects of the data being analyzed. The graph appears to be a visual representation of data, possibly related to a scientific or technical field.'], ['The image displays a graph with multiple lines, each representing a different data set. The lines are color-coded, with some being blue, green, and orange. The graph is organized in a way that allows for easy comparison and analysis of the data.\n\nThere are two main sections in the graph, with one section being larger and occupying the majority of the image, while the other section is smaller and located towards the bottom right corner. The smaller section appears to be a subplot of the larger section, providing additional information or a different perspective on the data.\n\nThe graph is likely used for scientific or technical purposes, as it presents data in a clear and organized manner, allowing for easy interpretation and comparison.'], ['The image displays a graph with multiple lines, each representing a different data set. The lines are color-coded, with some being blue, green, and orange. The graph is organized in a way that allows for easy comparison and analysis of the data.\n\nThere are several columns in the graph, each containing a different set of data. The columns are labeled with numbers, indicating the specific data set they represent. The graph is divided into sections, with each section containing a different color-coded line.\n\nThe graph is likely used for scientific or technical purposes, as it provides a visual representation of the data sets and allows for easy comparison and analysis. The use of color-coded lines helps to differentiate between the various data sets and makes it easier to identify trends or patterns in the data.'], ['The image displays a graph with multiple lines, each representing a different data set. The lines are color-coded, with some being green, blue, and orange. The graph is divided into sections, with each section containing a different number of lines. The lines are arranged in a way that allows for easy comparison and analysis of the data.\n\nThe graph is accompanied by a text description, which provides additional information about the data being displayed. The text is located at the top of the graph, making it easy to read and understand. The combination of the graph and the text description allows for a clear visual representation of the data and its significance.'], ['The image is a collection of graphs and charts, each displaying different types of data. There are a total of 12 graphs, each with varying colors and shapes, arranged in a grid-like pattern. The graphs are organized in a way that allows for easy comparison and analysis of the data.\n\nThe graphs showcase a range of information, including average accuracy, order number, and test number. The graphs are labeled with numbers, indicating the specific data being displayed. The graphs are spread across the entire image, with some positioned closer to the top, middle, and bottom sections. Overall, the image presents a comprehensive visual representation of various data sets.'], ['The image displays a graph with two different types of data, one on the left side and the other on the right side. The left side of the graph shows a progressive increase in performance, while the right side shows a progressive decrease in performance. The graph is divided into several sections, each representing a different data point.\n\nThere are multiple columns and rows in the graph, with each row representing a different data point, and each column representing a different data category. The data points are labeled with numbers, indicating their specific positions within the graph. The graph provides a clear visual representation of the relationship between the two types of data, allowing for easy comparison and analysis.']]
Loaded 6 paragraph data entries
Prepared 6 prompts
Image tag lists: 6
Image projection lists: 6
INFO 09-01 01:02:53 [utils.py:326] non-default args: {'model': 'Qwen/Qwen3-8B', 'enable_prompt_embeds': True, 'trust_remote_code': True, 'dtype': 'float16', 'gpu_memory_utilization': 0.6, 'disable_log_stats': True, 'enforce_eager': True}
INFO 09-01 01:03:01 [__init__.py:711] Resolved architecture: Qwen3ForCausalLM
WARNING 09-01 01:03:01 [__init__.py:2819] Casting torch.bfloat16 to torch.float16.
INFO 09-01 01:03:01 [__init__.py:1750] Using max model len 40960
WARNING 09-01 01:03:01 [arg_utils.py:1770] --enable-prompt-embeds is not supported by the V1 Engine. Falling back to V0. 
WARNING 09-01 01:03:01 [arg_utils.py:1544] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.
INFO 09-01 01:03:02 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 09-01 01:03:02 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 09-01 01:03:02 [__init__.py:3565] Cudagraph is disabled under eager mode
INFO 09-01 01:03:02 [llm_engine.py:222] Initializing a V0 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen3-8B', speculative_config=None, tokenizer='Qwen/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{"enable_fusion":false,"enable_noop":false},"max_capture_size":0,"local_cache_dir":null}, use_cached_outputs=False, 
INFO 09-01 01:03:04 [cuda.py:436] Using Flash Attention backend.
INFO 09-01 01:03:04 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 09-01 01:03:04 [model_runner.py:1080] Starting to load model Qwen/Qwen3-8B...
INFO 09-01 01:03:05 [weight_utils.py:296] Using model weights format ['*.safetensors']
INFO 09-01 01:03:09 [default_loader.py:262] Loading weights took 4.19 seconds
INFO 09-01 01:03:10 [model_runner.py:1112] Model loading took 15.2683 GiB and 4.531815 seconds
INFO 09-01 01:03:11 [worker.py:295] Memory profiling takes 0.65 seconds
INFO 09-01 01:03:11 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.53GiB) x gpu_memory_utilization (0.60) = 28.52GiB
INFO 09-01 01:03:11 [worker.py:295] model weights take 15.27GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 11.86GiB.
INFO 09-01 01:03:11 [executor_base.py:114] # cuda blocks: 5395, # CPU blocks: 1820
INFO 09-01 01:03:11 [executor_base.py:119] Maximum concurrency for 40960 tokens per request: 2.11x
INFO 09-01 01:03:13 [llm_engine.py:417] init engine (profile, create kv cache, warmup model) took 2.91 seconds
INFO 09-01 01:03:13 [llm.py:298] Supported_tasks: ['generate']
