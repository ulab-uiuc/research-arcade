{'papers': [{'id': 1, 'arxiv_id': '2506.24106v1', 'base_arxiv_id': '2506.24106', 'version': '1', 'title': 'On the Predictive Power of Representation Dispersion in Language Models', 'abstract': "We show that a language model's ability to predict text is tightly linked to\nthe breadth of its embedding space: models that spread their contextual\nrepresentations more widely tend to achieve lower perplexity. Concretely, we\nfind that representation dispersion - the average pairwise cosine distance\namong hidden vectors - strongly and negatively correlates with perplexity\nacross diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia,\nnews, scientific abstracts). Beyond illustrating this link, we show how\ndispersion can be leveraged for a range of practical tasks without requiring\nlabeled data. First, measuring dispersion on unlabeled text allows us to\npredict downstream accuracy in new domains, offering a data-efficient tool for\nmodel selection. Next, we find that identifying layers with higher dispersion\npinpoints the best representations for retrieval-based methods such as kNN-LM,\nbypassing exhaustive layer-by-layer searches. Finally, we integrate a simple\npush-away objective into training, which increases dispersion in both\nsingle-domain and cross-domain scenarios and directly improves perplexity in\neach.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.24106v1\', \'title\': \'On the Predictive Power of Representation Dispersion in Language Models\', \'abstract\': "We show that a language model\'s ability to predict text is tightly linked to\\nthe breadth of its embedding space: models that spread their contextual\\nrepresentations more widely tend to achieve lower perplexity. Concretely, we\\nfind that representation dispersion - the average pairwise cosine distance\\namong hidden vectors - strongly and negatively correlates with perplexity\\nacross diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia,\\nnews, scientific abstracts). Beyond illustrating this link, we show how\\ndispersion can be leveraged for a range of practical tasks without requiring\\nlabeled data. First, measuring dispersion on unlabeled text allows us to\\npredict downstream accuracy in new domains, offering a data-efficient tool for\\nmodel selection. Next, we find that identifying layers with higher dispersion\\npinpoints the best representations for retrieval-based methods such as kNN-LM,\\nbypassing exhaustive layer-by-layer searches. Finally, we integrate a simple\\npush-away objective into training, which increases dispersion in both\\nsingle-domain and cross-domain scenarios and directly improves perplexity in\\neach.", \'authors\': [\'Yanhong Li\', \'Ming Li\', \'Karen Livescu\', \'Jiawei Zhou\'], \'published\': \'2025-06-30T17:53:50+00:00\', \'categories\': [\'cs.CL\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.24106v1\'}'}, {'id': 2, 'arxiv_id': '2506.24093v1', 'base_arxiv_id': '2506.24093', 'version': '1', 'title': 'Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies', 'abstract': 'Synthetic data has emerged as a cost-effective alternative to real data for\ntraining artificial neural networks (ANN). However, the disparity between\nsynthetic and real data results in a domain gap. That gap leads to poor\nperformance and generalization of the trained ANN when applied to real-world\nscenarios. Several strategies have been developed to bridge this gap, which\ncombine synthetic and real data, known as mixed training using hybrid datasets.\nWhile these strategies have been shown to mitigate the domain gap, a systematic\nevaluation of their generalizability and robustness across various tasks and\narchitectures remains underexplored. To address this challenge, our study\ncomprehensively analyzes two widely used mixing strategies on three prevalent\narchitectures and three distinct hybrid datasets. From these datasets, we\nsample subsets with varying proportions of synthetic to real data to\ninvestigate the impact of synthetic and real components. The findings of this\npaper provide valuable insights into optimizing the use of synthetic data in\nthe training process of any ANN, contributing to enhancing robustness and\nefficacy.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.24093v1', 'title': 'Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies', 'abstract': 'Synthetic data has emerged as a cost-effective alternative to real data for\\ntraining artificial neural networks (ANN). However, the disparity between\\nsynthetic and real data results in a domain gap. That gap leads to poor\\nperformance and generalization of the trained ANN when applied to real-world\\nscenarios. Several strategies have been developed to bridge this gap, which\\ncombine synthetic and real data, known as mixed training using hybrid datasets.\\nWhile these strategies have been shown to mitigate the domain gap, a systematic\\nevaluation of their generalizability and robustness across various tasks and\\narchitectures remains underexplored. To address this challenge, our study\\ncomprehensively analyzes two widely used mixing strategies on three prevalent\\narchitectures and three distinct hybrid datasets. From these datasets, we\\nsample subsets with varying proportions of synthetic to real data to\\ninvestigate the impact of synthetic and real components. The findings of this\\npaper provide valuable insights into optimizing the use of synthetic data in\\nthe training process of any ANN, contributing to enhancing robustness and\\nefficacy.', 'authors': ['Paul Wachter', 'Lukas Niehaus', 'Julius Sch√∂ning'], 'published': '2025-06-30T17:48:14+00:00', 'categories': ['cs.LG', 'cs.AI', 'I.2.1; I.2.0; F.2.3'], 'url': 'http://arxiv.org/abs/2506.24093v1'}"}, {'id': 3, 'arxiv_id': '2506.24085v1', 'base_arxiv_id': '2506.24085', 'version': '1', 'title': 'Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention', 'abstract': 'Blending visual and textual concepts into a new visual concept is a unique\nand powerful trait of human beings that can fuel creativity. However, in\npractice, cross-modal conceptual blending for humans is prone to cognitive\nbiases, like design fixation, which leads to local minima in the design space.\nIn this paper, we propose a T2I diffusion adapter "IT-Blender" that can\nautomate the blending process to enhance human creativity. Prior works related\nto cross-modal conceptual blending are limited in encoding a real image without\nloss of details or in disentangling the image and text inputs. To address these\ngaps, IT-Blender leverages pretrained diffusion models (SD and FLUX) to blend\nthe latent representations of a clean reference image with those of the noisy\ngenerated image. Combined with our novel blended attention, IT-Blender encodes\nthe real reference image without loss of details and blends the visual concept\nwith the object specified by the text in a disentangled way. Our experiment\nresults show that IT-Blender outperforms the baselines by a large margin in\nblending visual and textual concepts, shedding light on the new application of\nimage generative models to augment human creativity.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.24085v1\', \'title\': \'Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention\', \'abstract\': \'Blending visual and textual concepts into a new visual concept is a unique\\nand powerful trait of human beings that can fuel creativity. However, in\\npractice, cross-modal conceptual blending for humans is prone to cognitive\\nbiases, like design fixation, which leads to local minima in the design space.\\nIn this paper, we propose a T2I diffusion adapter "IT-Blender" that can\\nautomate the blending process to enhance human creativity. Prior works related\\nto cross-modal conceptual blending are limited in encoding a real image without\\nloss of details or in disentangling the image and text inputs. To address these\\ngaps, IT-Blender leverages pretrained diffusion models (SD and FLUX) to blend\\nthe latent representations of a clean reference image with those of the noisy\\ngenerated image. Combined with our novel blended attention, IT-Blender encodes\\nthe real reference image without loss of details and blends the visual concept\\nwith the object specified by the text in a disentangled way. Our experiment\\nresults show that IT-Blender outperforms the baselines by a large margin in\\nblending visual and textual concepts, shedding light on the new application of\\nimage generative models to augment human creativity.\', \'authors\': [\'Wonwoong Cho\', \'Yanxia Zhang\', \'Yan-Ying Chen\', \'David I. Inouye\'], \'published\': \'2025-06-30T17:41:25+00:00\', \'categories\': [\'cs.CV\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.24085v1\'}'}, {'id': 4, 'arxiv_id': '2506.24081v1', 'base_arxiv_id': '2506.24081', 'version': '1', 'title': 'SQUASH: A SWAP-Based Quantum Attack to Sabotage Hybrid Quantum Neural Networks', 'abstract': 'We propose a circuit-level attack, SQUASH, a SWAP-Based Quantum Attack to\nsabotage Hybrid Quantum Neural Networks (HQNNs) for classification tasks.\nSQUASH is executed by inserting SWAP gate(s) into the variational quantum\ncircuit of the victim HQNN. Unlike conventional noise-based or adversarial\ninput attacks, SQUASH directly manipulates the circuit structure, leading to\nqubit misalignment and disrupting quantum state evolution. This attack is\nhighly stealthy, as it does not require access to training data or introduce\ndetectable perturbations in input states. Our results demonstrate that SQUASH\nsignificantly degrades classification performance, with untargeted SWAP attacks\nreducing accuracy by up to 74.08\\% and targeted SWAP attacks reducing target\nclass accuracy by up to 79.78\\%. These findings reveal a critical vulnerability\nin HQNN implementations, underscoring the need for more resilient architectures\nagainst circuit-level adversarial interventions.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.24081v1', 'title': 'SQUASH: A SWAP-Based Quantum Attack to Sabotage Hybrid Quantum Neural Networks', 'abstract': 'We propose a circuit-level attack, SQUASH, a SWAP-Based Quantum Attack to\\nsabotage Hybrid Quantum Neural Networks (HQNNs) for classification tasks.\\nSQUASH is executed by inserting SWAP gate(s) into the variational quantum\\ncircuit of the victim HQNN. Unlike conventional noise-based or adversarial\\ninput attacks, SQUASH directly manipulates the circuit structure, leading to\\nqubit misalignment and disrupting quantum state evolution. This attack is\\nhighly stealthy, as it does not require access to training data or introduce\\ndetectable perturbations in input states. Our results demonstrate that SQUASH\\nsignificantly degrades classification performance, with untargeted SWAP attacks\\nreducing accuracy by up to 74.08\\\\% and targeted SWAP attacks reducing target\\nclass accuracy by up to 79.78\\\\%. These findings reveal a critical vulnerability\\nin HQNN implementations, underscoring the need for more resilient architectures\\nagainst circuit-level adversarial interventions.', 'authors': ['Rahul Kumar', 'Wenqi Wei', 'Ying Mao', 'Junaid Farooq', 'Ying Wang', 'Juntao Chen'], 'published': '2025-06-30T17:36:31+00:00', 'categories': ['quant-ph', 'cs.AI', 'cs.LG'], 'url': 'http://arxiv.org/abs/2506.24081v1'}"}, {'id': 5, 'arxiv_id': '2506.24068v1', 'base_arxiv_id': '2506.24068', 'version': '1', 'title': 'STACK: Adversarial Attacks on LLM Safeguard Pipelines', 'abstract': 'Frontier AI developers are relying on layers of safeguards to protect against\ncatastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus\nmodel using one such defense pipeline, and other frontier developers including\nGoogle DeepMind and OpenAI pledge to soon deploy similar defenses. However, the\nsecurity of such pipelines is unclear, with limited prior work evaluating or\nattacking these pipelines. We address this gap by developing and red-teaming an\nopen-source defense pipeline. First, we find that a novel few-shot-prompted\ninput and output classifier outperforms state-of-the-art open-weight safeguard\nmodel ShieldGemma across three attacks and two datasets, reducing the attack\nsuccess rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,\nwe introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on\nClearHarm in a black-box attack against the few-shot-prompted classifier\npipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%\nASR, providing initial evidence that it is feasible to design attacks with no\naccess to the target pipeline. We conclude by suggesting specific mitigations\nthat developers could use to thwart staged attacks.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.24068v1', 'title': 'STACK: Adversarial Attacks on LLM Safeguard Pipelines', 'abstract': 'Frontier AI developers are relying on layers of safeguards to protect against\\ncatastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus\\nmodel using one such defense pipeline, and other frontier developers including\\nGoogle DeepMind and OpenAI pledge to soon deploy similar defenses. However, the\\nsecurity of such pipelines is unclear, with limited prior work evaluating or\\nattacking these pipelines. We address this gap by developing and red-teaming an\\nopen-source defense pipeline. First, we find that a novel few-shot-prompted\\ninput and output classifier outperforms state-of-the-art open-weight safeguard\\nmodel ShieldGemma across three attacks and two datasets, reducing the attack\\nsuccess rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,\\nwe introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on\\nClearHarm in a black-box attack against the few-shot-prompted classifier\\npipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%\\nASR, providing initial evidence that it is feasible to design attacks with no\\naccess to the target pipeline. We conclude by suggesting specific mitigations\\nthat developers could use to thwart staged attacks.', 'authors': ['Ian R. McKenzie', 'Oskar J. Hollinsworth', 'Tom Tseng', 'Xander Davies', 'Stephen Casper', 'Aaron D. Tucker', 'Robert Kirk', 'Adam Gleave'], 'published': '2025-06-30T17:21:08+00:00', 'categories': ['cs.CL', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.24068v1'}"}, {'id': 6, 'arxiv_id': '2506.24044v1', 'base_arxiv_id': '2506.24044', 'version': '1', 'title': 'A Survey on Vision-Language-Action Models for Autonomous Driving', 'abstract': "The rapid progress of multimodal large language models (MLLM) has paved the\nway for Vision-Language-Action (VLA) paradigms, which integrate visual\nperception, natural language understanding, and control within a single policy.\nResearchers in autonomous driving are actively adapting these methods to the\nvehicle domain. Such models promise autonomous vehicles that can interpret\nhigh-level instructions, reason about complex traffic scenes, and make their\nown decisions. However, the literature remains fragmented and is rapidly\nexpanding. This survey offers the first comprehensive overview of VLA for\nAutonomous Driving (VLA4AD). We (i) formalize the architectural building blocks\nshared across recent work, (ii) trace the evolution from early explainer to\nreasoning-centric VLA models, and (iii) compare over 20 representative models\naccording to VLA's progress in the autonomous driving domain. We also\nconsolidate existing datasets and benchmarks, highlighting protocols that\njointly measure driving safety, accuracy, and explanation quality. Finally, we\ndetail open challenges - robustness, real-time efficiency, and formal\nverification - and outline future directions of VLA4AD. This survey provides a\nconcise yet complete reference for advancing interpretable socially aligned\nautonomous vehicles. Github repo is available at\nhttps://github.com/JohnsonJiang1996/Awesome-VLA4ADSicongJiang/Awesome-VLA4AD.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.24044v1\', \'title\': \'A Survey on Vision-Language-Action Models for Autonomous Driving\', \'abstract\': "The rapid progress of multimodal large language models (MLLM) has paved the\\nway for Vision-Language-Action (VLA) paradigms, which integrate visual\\nperception, natural language understanding, and control within a single policy.\\nResearchers in autonomous driving are actively adapting these methods to the\\nvehicle domain. Such models promise autonomous vehicles that can interpret\\nhigh-level instructions, reason about complex traffic scenes, and make their\\nown decisions. However, the literature remains fragmented and is rapidly\\nexpanding. This survey offers the first comprehensive overview of VLA for\\nAutonomous Driving (VLA4AD). We (i) formalize the architectural building blocks\\nshared across recent work, (ii) trace the evolution from early explainer to\\nreasoning-centric VLA models, and (iii) compare over 20 representative models\\naccording to VLA\'s progress in the autonomous driving domain. We also\\nconsolidate existing datasets and benchmarks, highlighting protocols that\\njointly measure driving safety, accuracy, and explanation quality. Finally, we\\ndetail open challenges - robustness, real-time efficiency, and formal\\nverification - and outline future directions of VLA4AD. This survey provides a\\nconcise yet complete reference for advancing interpretable socially aligned\\nautonomous vehicles. Github repo is available at\\n\\\\href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.", \'authors\': [\'Sicong Jiang\', \'Zilin Huang\', \'Kangan Qian\', \'Ziang Luo\', \'Tianze Zhu\', \'Yang Zhong\', \'Yihong Tang\', \'Menglin Kong\', \'Yunlong Wang\', \'Siwen Jiao\', \'Hao Ye\', \'Zihao Sheng\', \'Xin Zhao\', \'Tuopu Wen\', \'Zheng Fu\', \'Sikai Chen\', \'Kun Jiang\', \'Diange Yang\', \'Seongjin Choi\', \'Lijun Sun\'], \'published\': \'2025-06-30T16:50:02+00:00\', \'categories\': [\'cs.CV\', \'cs.AI\', \'cs.RO\'], \'url\': \'http://arxiv.org/abs/2506.24044v1\'}'}, {'id': 7, 'arxiv_id': '2506.24120v1', 'base_arxiv_id': '2506.24120', 'version': '1', 'title': 'Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime', 'abstract': 'Data selection plays a crucial role in data-driven decision-making, including\nin large language models (LLMs), and is typically task-dependent. Properties\nsuch as data quality and diversity have been extensively studied and are known\nto enhance model performance. However, it remains unclear whether there exist\nother quantitative and general principles of data selection that can\nconsistently improve performance, especially for complex tasks with limited\nprior knowledge. In this paper, we demonstrate that selecting more uniformly\ndistributed data can improve training efficiency while enhancing performance.\nSpecifically, we establish that more uniform (less biased) distribution leads\nto a larger minimum pairwise distance between data points, denoted by\n$h_$, and prove that a smaller $h_$ can slow down the training\ndynamics of gradient descent (GD). Moreover, we theoretically show that the\napproximation error of neural networks decreases as $h_$ increases. Our\nanalysis introduces a convergence framework for GD beyond the Neural Tangent\nKernel (NTK) regime, applicable to a broad class of architectures, including\ntransformers, without requiring Lipschitz smoothness. This framework further\nprovides theoretical justification for the use of residual connections and\nfunction compositions in deep neural architectures. In the end, we conduct\ncomprehensive experiments for supervised fine-tuning across various settings,\nincluding different optimization strategies, model sizes, and training\ndatasets. The results consistently demonstrate that selecting data by\nmaximizing pairwise distance significantly accelerates training and achieves\ncomparable or better performance in LLMs across diverse datasets. Code and\nDatasets are available at the link:\nhttps://github.com/SafeRL-Lab/data-uniformity.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.24120v1', 'title': 'Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime', 'abstract': 'Data selection plays a crucial role in data-driven decision-making, including\\nin large language models (LLMs), and is typically task-dependent. Properties\\nsuch as data quality and diversity have been extensively studied and are known\\nto enhance model performance. However, it remains unclear whether there exist\\nother quantitative and general principles of data selection that can\\nconsistently improve performance, especially for complex tasks with limited\\nprior knowledge. In this paper, we demonstrate that selecting more uniformly\\ndistributed data can improve training efficiency while enhancing performance.\\nSpecifically, we establish that more uniform (less biased) distribution leads\\nto a larger minimum pairwise distance between data points, denoted by\\n$h_{\\\\min}$, and prove that a smaller $h_{\\\\min}$ can slow down the training\\ndynamics of gradient descent (GD). Moreover, we theoretically show that the\\napproximation error of neural networks decreases as $h_{\\\\min}$ increases. Our\\nanalysis introduces a convergence framework for GD beyond the Neural Tangent\\nKernel (NTK) regime, applicable to a broad class of architectures, including\\ntransformers, without requiring Lipschitz smoothness. This framework further\\nprovides theoretical justification for the use of residual connections and\\nfunction compositions in deep neural architectures. In the end, we conduct\\ncomprehensive experiments for supervised fine-tuning across various settings,\\nincluding different optimization strategies, model sizes, and training\\ndatasets. The results consistently demonstrate that selecting data by\\nmaximizing pairwise distance significantly accelerates training and achieves\\ncomparable or better performance in LLMs across diverse datasets. Code and\\nDatasets are available at the link:\\nhttps://github.com/SafeRL-Lab/data-uniformity.', 'authors': ['Yuqing Wang', 'Shangding Gu'], 'published': '2025-06-30T17:58:30+00:00', 'categories': ['cs.LG', 'cs.AI', 'math.OC', 'stat.ML'], 'url': 'http://arxiv.org/abs/2506.24120v1'}"}, {'id': 8, 'arxiv_id': '2506.24119v1', 'base_arxiv_id': '2506.24119', 'version': '1', 'title': 'SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning', 'abstract': 'Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.24119v1', 'title': 'SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning', 'abstract': 'Recent advances in reinforcement learning have shown that language models can\\ndevelop sophisticated reasoning through training on tasks with verifiable\\nrewards, but these approaches depend on human-curated problem-answer pairs and\\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\\nwhere models learn by playing multi-turn, zero-sum games against continuously\\nimproving versions of themselves, eliminating the need for human supervision.\\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\\nchallenging problems as models must constantly adapt to stronger opponents. To\\nenable this self-play training at scale, We implement a fully online,\\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\\nexpert game trajectories. Analysis reveals that this transfer occurs through\\nthree cognitive patterns: systematic decomposition, expected value calculation,\\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\\nNegotiation) further enhances performance as each game develops distinct\\nreasoning strengths. Applying SPIRAL to a strong reasoning model\\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\\nresults demonstrate that zero-sum games naturally develop transferable\\nreasoning capabilities, highlighting a promising direction for autonomous\\nreasoning development.', 'authors': ['Bo Liu', 'Leon Guertler', 'Simon Yu', 'Zichen Liu', 'Penghui Qi', 'Daniel Balcells', 'Mickel Liu', 'Cheston Tan', 'Weiyan Shi', 'Min Lin', 'Wee Sun Lee', 'Natasha Jaques'], 'published': '2025-06-30T17:58:13+00:00', 'categories': ['cs.AI', 'cs.CL', 'cs.LG'], 'url': 'http://arxiv.org/abs/2506.24119v1'}"}, {'id': 9, 'arxiv_id': '2506.24108v1', 'base_arxiv_id': '2506.24108', 'version': '1', 'title': 'Navigating with Annealing Guidance Scale in Diffusion Space', 'abstract': 'Denoising diffusion models excel at generating high-quality images\nconditioned on text prompts, yet their effectiveness heavily relies on careful\nguidance during the sampling process. Classifier-Free Guidance (CFG) provides a\nwidely used mechanism for steering generation by setting the guidance scale,\nwhich balances image quality and prompt alignment. However, the choice of the\nguidance scale has a critical impact on the convergence toward a visually\nappealing and prompt-adherent image. In this work, we propose an annealing\nguidance scheduler which dynamically adjusts the guidance scale over time based\non the conditional noisy signal. By learning a scheduling policy, our method\naddresses the temperamental behavior of CFG. Empirical results demonstrate that\nour guidance scheduler significantly enhances image quality and alignment with\nthe text prompt, advancing the performance of text-to-image generation.\nNotably, our novel scheduler requires no additional activations or memory\nconsumption, and can seamlessly replace the common classifier-free guidance,\noffering an improved trade-off between prompt alignment and quality.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.24108v1', 'title': 'Navigating with Annealing Guidance Scale in Diffusion Space', 'abstract': 'Denoising diffusion models excel at generating high-quality images\\nconditioned on text prompts, yet their effectiveness heavily relies on careful\\nguidance during the sampling process. Classifier-Free Guidance (CFG) provides a\\nwidely used mechanism for steering generation by setting the guidance scale,\\nwhich balances image quality and prompt alignment. However, the choice of the\\nguidance scale has a critical impact on the convergence toward a visually\\nappealing and prompt-adherent image. In this work, we propose an annealing\\nguidance scheduler which dynamically adjusts the guidance scale over time based\\non the conditional noisy signal. By learning a scheduling policy, our method\\naddresses the temperamental behavior of CFG. Empirical results demonstrate that\\nour guidance scheduler significantly enhances image quality and alignment with\\nthe text prompt, advancing the performance of text-to-image generation.\\nNotably, our novel scheduler requires no additional activations or memory\\nconsumption, and can seamlessly replace the common classifier-free guidance,\\noffering an improved trade-off between prompt alignment and quality.', 'authors': ['Shai Yehezkel', 'Omer Dahary', 'Andrey Voynov', 'Daniel Cohen-Or'], 'published': '2025-06-30T17:55:00+00:00', 'categories': ['cs.GR', 'cs.AI', 'cs.CV', 'cs.LG'], 'url': 'http://arxiv.org/abs/2506.24108v1'}"}, {'id': 10, 'arxiv_id': '2506.24026v1', 'base_arxiv_id': '2506.24026', 'version': '1', 'title': 'Constructing Non-Markovian Decision Process via History Aggregator', 'abstract': 'In the domain of algorithmic decision-making, non-Markovian dynamics manifest\nas a significant impediment, especially for paradigms such as Reinforcement\nLearning (RL), thereby exerting far-reaching consequences on the advancement\nand effectiveness of the associated systems. Nevertheless, the existing\nbenchmarks are deficient in comprehensively assessing the capacity of decision\nalgorithms to handle non-Markovian dynamics. To address this deficiency, we\nhave devised a generalized methodology grounded in category theory. Notably, we\nestablished the category of Markov Decision Processes (MDP) and the category of\nnon-Markovian Decision Processes (NMDP), and proved the equivalence\nrelationship between them. This theoretical foundation provides a novel\nperspective for understanding and addressing non-Markovian dynamics. We further\nintroduced non-Markovianity into decision-making problem settings via the\nHistory Aggregator for State (HAS). With HAS, we can precisely control the\nstate dependency structure of decision-making problems in the time series. Our\nanalysis demonstrates the effectiveness of our method in representing a broad\nrange of non-Markovian dynamics. This approach facilitates a more rigorous and\nflexible evaluation of decision algorithms by testing them in problem settings\nwhere non-Markovian dynamics are explicitly constructed.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.24026v1', 'title': 'Constructing Non-Markovian Decision Process via History Aggregator', 'abstract': 'In the domain of algorithmic decision-making, non-Markovian dynamics manifest\\nas a significant impediment, especially for paradigms such as Reinforcement\\nLearning (RL), thereby exerting far-reaching consequences on the advancement\\nand effectiveness of the associated systems. Nevertheless, the existing\\nbenchmarks are deficient in comprehensively assessing the capacity of decision\\nalgorithms to handle non-Markovian dynamics. To address this deficiency, we\\nhave devised a generalized methodology grounded in category theory. Notably, we\\nestablished the category of Markov Decision Processes (MDP) and the category of\\nnon-Markovian Decision Processes (NMDP), and proved the equivalence\\nrelationship between them. This theoretical foundation provides a novel\\nperspective for understanding and addressing non-Markovian dynamics. We further\\nintroduced non-Markovianity into decision-making problem settings via the\\nHistory Aggregator for State (HAS). With HAS, we can precisely control the\\nstate dependency structure of decision-making problems in the time series. Our\\nanalysis demonstrates the effectiveness of our method in representing a broad\\nrange of non-Markovian dynamics. This approach facilitates a more rigorous and\\nflexible evaluation of decision algorithms by testing them in problem settings\\nwhere non-Markovian dynamics are explicitly constructed.', 'authors': ['Yongyi Wang', 'Wenxin Li'], 'published': '2025-06-30T16:32:31+00:00', 'categories': ['cs.AI'], 'url': 'http://arxiv.org/abs/2506.24026v1'}"}, {'id': 11, 'arxiv_id': '2506.24018v1', 'base_arxiv_id': '2506.24018', 'version': '1', 'title': 'Bridging Theory and Practice in Link Representation with Graph Neural Networks', 'abstract': 'Graph Neural Networks (GNNs) are widely used to compute representations of\nnode pairs for downstream tasks such as link prediction. Yet, theoretical\nunderstanding of their expressive power has focused almost entirely on\ngraph-level representations. In this work, we shift the focus to links and\nprovide the first comprehensive study of GNN expressiveness in link\nrepresentation. We introduce a unifying framework, the $k_$-$k_$-$m$\nframework, that subsumes existing message-passing link models and enables\nformal expressiveness comparisons. Using this framework, we derive a hierarchy\nof state-of-the-art methods and offer theoretical tools to analyze future\narchitectures. To complement our analysis, we propose a synthetic evaluation\nprotocol comprising the first benchmark specifically designed to assess\nlink-level expressiveness. Finally, we ask: does expressiveness matter in\npractice? We use a graph symmetry metric that quantifies the difficulty of\ndistinguishing links and show that while expressive models may underperform on\nstandard benchmarks, they significantly outperform simpler ones as symmetry\nincreases, highlighting the need for dataset-aware model selection.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.24018v1', 'title': 'Bridging Theory and Practice in Link Representation with Graph Neural Networks', 'abstract': 'Graph Neural Networks (GNNs) are widely used to compute representations of\\nnode pairs for downstream tasks such as link prediction. Yet, theoretical\\nunderstanding of their expressive power has focused almost entirely on\\ngraph-level representations. In this work, we shift the focus to links and\\nprovide the first comprehensive study of GNN expressiveness in link\\nrepresentation. We introduce a unifying framework, the $k_\\\\phi$-$k_\\\\rho$-$m$\\nframework, that subsumes existing message-passing link models and enables\\nformal expressiveness comparisons. Using this framework, we derive a hierarchy\\nof state-of-the-art methods and offer theoretical tools to analyze future\\narchitectures. To complement our analysis, we propose a synthetic evaluation\\nprotocol comprising the first benchmark specifically designed to assess\\nlink-level expressiveness. Finally, we ask: does expressiveness matter in\\npractice? We use a graph symmetry metric that quantifies the difficulty of\\ndistinguishing links and show that while expressive models may underperform on\\nstandard benchmarks, they significantly outperform simpler ones as symmetry\\nincreases, highlighting the need for dataset-aware model selection.', 'authors': ['Veronica Lachi', 'Francesco Ferrini', 'Antonio Longa', 'Bruno Lepri', 'Andrea Passerini', 'Manfred Jaeger'], 'published': '2025-06-30T16:22:15+00:00', 'categories': ['cs.LG', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.24018v1'}"}, {'id': 12, 'arxiv_id': '2506.24016v1', 'base_arxiv_id': '2506.24016', 'version': '1', 'title': 'EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations', 'abstract': 'Recent advances in large language models and vision-language models have led\nto growing interest in explainable evaluation metrics for image captioning.\nHowever, these metrics generate explanations without standardized criteria, and\nthe overall quality of the generated explanations remains unverified. In this\npaper, we propose EXPERT, a reference-free evaluation metric that provides\nstructured explanations based on three fundamental criteria: fluency,\nrelevance, and descriptiveness. By constructing large-scale datasets of\nhigh-quality structured explanations, we develop a two-stage evaluation\ntemplate to effectively supervise a vision-language model for both scoring and\nexplanation generation. EXPERT achieves state-of-the-art results on benchmark\ndatasets while providing significantly higher-quality explanations than\nexisting metrics, as validated through comprehensive human evaluation. Our code\nand datasets are available at https://github.com/hjkim811/EXPERT.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.24016v1', 'title': 'EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations', 'abstract': 'Recent advances in large language models and vision-language models have led\\nto growing interest in explainable evaluation metrics for image captioning.\\nHowever, these metrics generate explanations without standardized criteria, and\\nthe overall quality of the generated explanations remains unverified. In this\\npaper, we propose EXPERT, a reference-free evaluation metric that provides\\nstructured explanations based on three fundamental criteria: fluency,\\nrelevance, and descriptiveness. By constructing large-scale datasets of\\nhigh-quality structured explanations, we develop a two-stage evaluation\\ntemplate to effectively supervise a vision-language model for both scoring and\\nexplanation generation. EXPERT achieves state-of-the-art results on benchmark\\ndatasets while providing significantly higher-quality explanations than\\nexisting metrics, as validated through comprehensive human evaluation. Our code\\nand datasets are available at https://github.com/hjkim811/EXPERT.', 'authors': ['Hyunjong Kim', 'Sangyeop Kim', 'Jongheon Jeong', 'Yeongjae Cho', 'Sungzoon Cho'], 'published': '2025-06-30T16:20:51+00:00', 'categories': ['cs.CL', 'cs.AI', 'cs.CV'], 'url': 'http://arxiv.org/abs/2506.24016v1'}"}, {'id': 13, 'arxiv_id': '2506.24009v1', 'base_arxiv_id': '2506.24009', 'version': '1', 'title': 'Bridging Physical and Digital Worlds: Embodied Large AI for Future Wireless Systems', 'abstract': 'Large artificial intelligence (AI) models offer revolutionary potential for\nfuture wireless systems, promising unprecedented capabilities in network\noptimization and performance. However, current paradigms largely overlook\ncrucial physical interactions. This oversight means they primarily rely on\noffline datasets, leading to difficulties in handling real-time wireless\ndynamics and non-stationary environments. Furthermore, these models often lack\nthe capability for active environmental probing. This paper proposes a\nfundamental paradigm shift towards wireless embodied large AI (WELAI), moving\nfrom passive observation to active embodiment. We first identify key challenges\nfaced by existing models, then we explore the design principles and system\nstructure of WELAI. Besides, we outline prospective applications in\nnext-generation wireless. Finally, through an illustrative case study, we\ndemonstrate the effectiveness of WELAI and point out promising research\ndirections for realizing adaptive, robust, and autonomous wireless systems.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.24009v1', 'title': 'Bridging Physical and Digital Worlds: Embodied Large AI for Future Wireless Systems', 'abstract': 'Large artificial intelligence (AI) models offer revolutionary potential for\\nfuture wireless systems, promising unprecedented capabilities in network\\noptimization and performance. However, current paradigms largely overlook\\ncrucial physical interactions. This oversight means they primarily rely on\\noffline datasets, leading to difficulties in handling real-time wireless\\ndynamics and non-stationary environments. Furthermore, these models often lack\\nthe capability for active environmental probing. This paper proposes a\\nfundamental paradigm shift towards wireless embodied large AI (WELAI), moving\\nfrom passive observation to active embodiment. We first identify key challenges\\nfaced by existing models, then we explore the design principles and system\\nstructure of WELAI. Besides, we outline prospective applications in\\nnext-generation wireless. Finally, through an illustrative case study, we\\ndemonstrate the effectiveness of WELAI and point out promising research\\ndirections for realizing adaptive, robust, and autonomous wireless systems.', 'authors': ['Xinquan Wang', 'Fenghao Zhu', 'Zhaohui Yang', 'Chongwen Huang', 'Xiaoming Chen', 'Zhaoyang Zhang', 'Sami Muhaidat', 'M√©rouane Debbah'], 'published': '2025-06-30T16:13:55+00:00', 'categories': ['cs.IT', 'cs.AI', 'math.IT'], 'url': 'http://arxiv.org/abs/2506.24009v1'}"}, {'id': 14, 'arxiv_id': '2506.23995v1', 'base_arxiv_id': '2506.23995', 'version': '1', 'title': 'STCLocker: Deadlock Avoidance Testing for Autonomous Driving Systems', 'abstract': 'Autonomous Driving System (ADS) testing is essential to ensure the safety and\nreliability of autonomous vehicles (AVs) before deployment. However, existing\ntechniques primarily focus on evaluating ADS functionalities in single-AV\nsettings. As ADSs are increasingly deployed in multi-AV traffic, it becomes\ncrucial to assess their cooperative performance, particularly regarding\ndeadlocks, a fundamental coordination failure in which multiple AVs enter a\ncircular waiting state indefinitely, resulting in motion planning failures.\nDespite its importance, the cooperative capability of ADSs to prevent deadlocks\nremains insufficiently underexplored. To address this gap, we propose the first\ndedicated Spatio-Temporal Conflict-Guided Deadlock Avoidance Testing technique,\nSTCLocker, for generating DeadLock Scenarios (DLSs), where a group of AVs\ncontrolled by the ADS under test are in a circular wait state. STCLocker\nconsists of three key components: Deadlock Oracle, Conflict Feedback, and\nConflict-aware Scenario Generation. Deadlock Oracle provides a reliable\nblack-box mechanism for detecting deadlock cycles among multiple AVs within a\ngiven scenario. Conflict Feedback and Conflict-aware Scenario Generation\ncollaborate to actively guide AVs into simultaneous competition over spatial\nconflict resources (i.e., shared passing regions) and temporal competitive\nbehaviors (i.e., reaching the conflict region at the same time), thereby\nincreasing the effectiveness of generating conflict-prone deadlocks. We\nevaluate STCLocker on two types of ADSs: Roach, an end-to-end ADS, and OpenCDA,\na module-based ADS supporting cooperative communication. Experimental results\nshow that, on average, STCLocker generates more DLS than the best-performing\nbaseline.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23995v1', 'title': 'STCLocker: Deadlock Avoidance Testing for Autonomous Driving Systems', 'abstract': 'Autonomous Driving System (ADS) testing is essential to ensure the safety and\\nreliability of autonomous vehicles (AVs) before deployment. However, existing\\ntechniques primarily focus on evaluating ADS functionalities in single-AV\\nsettings. As ADSs are increasingly deployed in multi-AV traffic, it becomes\\ncrucial to assess their cooperative performance, particularly regarding\\ndeadlocks, a fundamental coordination failure in which multiple AVs enter a\\ncircular waiting state indefinitely, resulting in motion planning failures.\\nDespite its importance, the cooperative capability of ADSs to prevent deadlocks\\nremains insufficiently underexplored. To address this gap, we propose the first\\ndedicated Spatio-Temporal Conflict-Guided Deadlock Avoidance Testing technique,\\nSTCLocker, for generating DeadLock Scenarios (DLSs), where a group of AVs\\ncontrolled by the ADS under test are in a circular wait state. STCLocker\\nconsists of three key components: Deadlock Oracle, Conflict Feedback, and\\nConflict-aware Scenario Generation. Deadlock Oracle provides a reliable\\nblack-box mechanism for detecting deadlock cycles among multiple AVs within a\\ngiven scenario. Conflict Feedback and Conflict-aware Scenario Generation\\ncollaborate to actively guide AVs into simultaneous competition over spatial\\nconflict resources (i.e., shared passing regions) and temporal competitive\\nbehaviors (i.e., reaching the conflict region at the same time), thereby\\nincreasing the effectiveness of generating conflict-prone deadlocks. We\\nevaluate STCLocker on two types of ADSs: Roach, an end-to-end ADS, and OpenCDA,\\na module-based ADS supporting cooperative communication. Experimental results\\nshow that, on average, STCLocker generates more DLS than the best-performing\\nbaseline.', 'authors': ['Mingfei Cheng', 'Renzhi Wang', 'Xiaofei Xie', 'Yuan Zhou', 'Lei Ma'], 'published': '2025-06-30T15:58:10+00:00', 'categories': ['cs.SE', 'cs.AI', 'cs.RO'], 'url': 'http://arxiv.org/abs/2506.23995v1'}"}, {'id': 15, 'arxiv_id': '2506.23960v1', 'base_arxiv_id': '2506.23960', 'version': '1', 'title': 'ADReFT: Adaptive Decision Repair for Safe Autonomous Driving via Reinforcement Fine-Tuning', 'abstract': "Autonomous Driving Systems (ADSs) continue to face safety-critical risks due\nto the inherent limitations in their design and performance capabilities.\nOnline repair plays a crucial role in mitigating such limitations, ensuring the\nruntime safety and reliability of ADSs. Existing online repair solutions\nenforce ADS compliance by transforming unacceptable trajectories into\nacceptable ones based on predefined specifications, such as rule-based\nconstraints or training datasets. However, these approaches often lack\ngeneralizability, adaptability and tend to be overly conservative, resulting in\nineffective repairs that not only fail to mitigate safety risks sufficiently\nbut also degrade the overall driving experience. To address this issue, we\npropose Adaptive Decision Repair (ADReFT), a novel and effective repair method\nthat identifies safety-critical states through offline learning from failed\ntests and generates appropriate mitigation actions to improve ADS safety.\nSpecifically, ADReFT incorporates a transformer-based model with two joint\nheads, State Monitor and Decision Adapter, designed to capture complex driving\nenvironment interactions to evaluate state safety severity and generate\nadaptive repair actions. Given the absence of oracles for state safety\nidentification, we first pretrain ADReFT using supervised learning with coarse\nannotations, i.e., labeling states preceding violations as positive samples and\nothers as negative samples. It establishes ADReFT's foundational capability to\nmitigate safety-critical violations, though it may result in somewhat\nconservative mitigation strategies. Therefore, we subsequently finetune ADReFT\nusing reinforcement learning to improve its initial capability and generate\nmore precise and contextually appropriate repair decisions. Our evaluation\nresults illustrate that ADReFT achieves better repair performance.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23960v1\', \'title\': \'ADReFT: Adaptive Decision Repair for Safe Autonomous Driving via Reinforcement Fine-Tuning\', \'abstract\': "Autonomous Driving Systems (ADSs) continue to face safety-critical risks due\\nto the inherent limitations in their design and performance capabilities.\\nOnline repair plays a crucial role in mitigating such limitations, ensuring the\\nruntime safety and reliability of ADSs. Existing online repair solutions\\nenforce ADS compliance by transforming unacceptable trajectories into\\nacceptable ones based on predefined specifications, such as rule-based\\nconstraints or training datasets. However, these approaches often lack\\ngeneralizability, adaptability and tend to be overly conservative, resulting in\\nineffective repairs that not only fail to mitigate safety risks sufficiently\\nbut also degrade the overall driving experience. To address this issue, we\\npropose Adaptive Decision Repair (ADReFT), a novel and effective repair method\\nthat identifies safety-critical states through offline learning from failed\\ntests and generates appropriate mitigation actions to improve ADS safety.\\nSpecifically, ADReFT incorporates a transformer-based model with two joint\\nheads, State Monitor and Decision Adapter, designed to capture complex driving\\nenvironment interactions to evaluate state safety severity and generate\\nadaptive repair actions. Given the absence of oracles for state safety\\nidentification, we first pretrain ADReFT using supervised learning with coarse\\nannotations, i.e., labeling states preceding violations as positive samples and\\nothers as negative samples. It establishes ADReFT\'s foundational capability to\\nmitigate safety-critical violations, though it may result in somewhat\\nconservative mitigation strategies. Therefore, we subsequently finetune ADReFT\\nusing reinforcement learning to improve its initial capability and generate\\nmore precise and contextually appropriate repair decisions. Our evaluation\\nresults illustrate that ADReFT achieves better repair performance.", \'authors\': [\'Mingfei Cheng\', \'Xiaofei Xie\', \'Renzhi Wang\', \'Yuan Zhou\', \'Ming Hu\'], \'published\': \'2025-06-30T15:29:36+00:00\', \'categories\': [\'cs.LG\', \'cs.AI\', \'cs.SE\'], \'url\': \'http://arxiv.org/abs/2506.23960v1\'}'}, {'id': 16, 'arxiv_id': '2506.23944v1', 'base_arxiv_id': '2506.23944', 'version': '1', 'title': 'Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning', 'abstract': 'Imitation learning models for robotic tasks typically rely on multi-modal\ninputs, such as RGB images, language, and proprioceptive states. While\nproprioception is intuitively important for decision-making and obstacle\navoidance, simply incorporating all proprioceptive states leads to a surprising\ndegradation in imitation learning performance. In this work, we identify the\nunderlying issue as the proprioception shift problem, where the distributions\nof proprioceptive states diverge significantly between training and deployment.\nTo address this challenge, we propose a domain adaptation framework that\nbridges the gap by utilizing rollout data collected during deployment. Using\nWasserstein distance, we quantify the discrepancy between expert and rollout\nproprioceptive states and minimize this gap by adding noise to both sets of\nstates, proportional to the Wasserstein distance. This strategy enhances\nrobustness against proprioception shifts by aligning the training and\ndeployment distributions. Experiments on robotic manipulation tasks demonstrate\nthe efficacy of our method, enabling the imitation policy to leverage\nproprioception while mitigating its adverse effects. Our approach outperforms\nthe naive solution which discards proprioception, and other baselines designed\nto address distributional shifts.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23944v1', 'title': 'Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning', 'abstract': 'Imitation learning models for robotic tasks typically rely on multi-modal\\ninputs, such as RGB images, language, and proprioceptive states. While\\nproprioception is intuitively important for decision-making and obstacle\\navoidance, simply incorporating all proprioceptive states leads to a surprising\\ndegradation in imitation learning performance. In this work, we identify the\\nunderlying issue as the proprioception shift problem, where the distributions\\nof proprioceptive states diverge significantly between training and deployment.\\nTo address this challenge, we propose a domain adaptation framework that\\nbridges the gap by utilizing rollout data collected during deployment. Using\\nWasserstein distance, we quantify the discrepancy between expert and rollout\\nproprioceptive states and minimize this gap by adding noise to both sets of\\nstates, proportional to the Wasserstein distance. This strategy enhances\\nrobustness against proprioception shifts by aligning the training and\\ndeployment distributions. Experiments on robotic manipulation tasks demonstrate\\nthe efficacy of our method, enabling the imitation policy to leverage\\nproprioception while mitigating its adverse effects. Our approach outperforms\\nthe naive solution which discards proprioception, and other baselines designed\\nto address distributional shifts.', 'authors': ['Fuhang Kuang', 'Jiacheng You', 'Yingdong Hu', 'Tong Zhang', 'Chuan Wen', 'Yang Gao'], 'published': '2025-06-30T15:09:14+00:00', 'categories': ['cs.RO', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23944v1'}"}, {'id': 43, 'arxiv_id': '2506.23719v1', 'base_arxiv_id': '2506.23719', 'version': '1', 'title': 'DABstep: Data Agent Benchmark for Multi-step Reasoning', 'abstract': "We introduce DABstep, a novel benchmark for evaluating AI agents on realistic\nmulti-step data analysis tasks. DABstep comprises over 450 real-world\nchallenges derived from a financial analytics platform, requiring models to\ncombine code-based data processing with contextual reasoning over heterogeneous\ndocumentation. Each task demands an iterative, multi-step problem-solving\napproach, testing capabilities in data manipulation, cross-referencing multiple\nsources, and precise result reporting. The benchmark provides a factoid-style\nanswer format with automatic correctness checks for objective scoring at scale.\nWe evaluate leading LLM-based agents, revealing a substantial performance gap:\neven the best agent achieves only 14.55% accuracy on the hardest tasks. We\ndetail our benchmark's design, dataset composition, task formulation,\nevaluation protocol, report baseline results and analyze failure modes. DABstep\nis released with a public leaderboard and toolkit to accelerate research in\nautonomous data analysis.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23719v1\', \'title\': \'DABstep: Data Agent Benchmark for Multi-step Reasoning\', \'abstract\': "We introduce DABstep, a novel benchmark for evaluating AI agents on realistic\\nmulti-step data analysis tasks. DABstep comprises over 450 real-world\\nchallenges derived from a financial analytics platform, requiring models to\\ncombine code-based data processing with contextual reasoning over heterogeneous\\ndocumentation. Each task demands an iterative, multi-step problem-solving\\napproach, testing capabilities in data manipulation, cross-referencing multiple\\nsources, and precise result reporting. The benchmark provides a factoid-style\\nanswer format with automatic correctness checks for objective scoring at scale.\\nWe evaluate leading LLM-based agents, revealing a substantial performance gap:\\neven the best agent achieves only 14.55% accuracy on the hardest tasks. We\\ndetail our benchmark\'s design, dataset composition, task formulation,\\nevaluation protocol, report baseline results and analyze failure modes. DABstep\\nis released with a public leaderboard and toolkit to accelerate research in\\nautonomous data analysis.", \'authors\': [\'Alex Egg\', \'Martin Iglesias Goyanes\', \'Friso Kingma\', \'Andreu Mora\', \'Leandro von Werra\', \'Thomas Wolf\'], \'published\': \'2025-06-30T10:49:21+00:00\', \'categories\': [\'cs.LG\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23719v1\'}'}, {'id': 17, 'arxiv_id': '2506.23934v1', 'base_arxiv_id': '2506.23934', 'version': '1', 'title': 'QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference', 'abstract': "As machine learning inferences increasingly move to edge devices, adapting to\ndiverse computational capabilities, hardware, and memory constraints becomes\nmore critical. Instead of relying on a pre-trained model fixed for all future\ninference queries across diverse edge devices, we argue that planning an\ninference pattern with a request-specific model tailored to the device's\ncomputational capacity, accuracy requirements, and time constraints is more\ncost-efficient and robust to diverse scenarios. To this end, we propose an\naccuracy-aware and workload-balanced inference system that integrates joint\nmodel quantization and inference partitioning. In this approach, the server\ndynamically responds to inference queries by sending a quantized model and\nadaptively sharing the inference workload with the device. Meanwhile, the\ndevice's computational power, channel capacity, and accuracy requirements are\nconsidered when deciding.\n  Furthermore, we introduce a new optimization framework for the inference\nsystem, incorporating joint model quantization and partitioning. Our approach\noptimizes layer-wise quantization bit width and partition points to minimize\ntime consumption and cost while accounting for varying accuracy requirements of\ntasks through an accuracy degradation metric in our optimization model. To our\nknowledge, this work represents the first exploration of optimizing\nquantization layer-wise bit-width in the inference serving system, by\nintroducing theoretical measurement of accuracy degradation. Simulation results\ndemonstrate a substantial reduction in overall time and power consumption, with\ncomputation payloads decreasing by over 80% and accuracy degradation kept below\n1%.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23934v1\', \'title\': \'QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference\', \'abstract\': "As machine learning inferences increasingly move to edge devices, adapting to\\ndiverse computational capabilities, hardware, and memory constraints becomes\\nmore critical. Instead of relying on a pre-trained model fixed for all future\\ninference queries across diverse edge devices, we argue that planning an\\ninference pattern with a request-specific model tailored to the device\'s\\ncomputational capacity, accuracy requirements, and time constraints is more\\ncost-efficient and robust to diverse scenarios. To this end, we propose an\\naccuracy-aware and workload-balanced inference system that integrates joint\\nmodel quantization and inference partitioning. In this approach, the server\\ndynamically responds to inference queries by sending a quantized model and\\nadaptively sharing the inference workload with the device. Meanwhile, the\\ndevice\'s computational power, channel capacity, and accuracy requirements are\\nconsidered when deciding.\\n  Furthermore, we introduce a new optimization framework for the inference\\nsystem, incorporating joint model quantization and partitioning. Our approach\\noptimizes layer-wise quantization bit width and partition points to minimize\\ntime consumption and cost while accounting for varying accuracy requirements of\\ntasks through an accuracy degradation metric in our optimization model. To our\\nknowledge, this work represents the first exploration of optimizing\\nquantization layer-wise bit-width in the inference serving system, by\\nintroducing theoretical measurement of accuracy degradation. Simulation results\\ndemonstrate a substantial reduction in overall time and power consumption, with\\ncomputation payloads decreasing by over 80% and accuracy degradation kept below\\n1%.", \'authors\': [\'Xiangchen Li\', \'Saeid Ghafouri\', \'Bo Ji\', \'Hans Vandierendonck\', \'Deepu John\', \'Dimitrios S. Nikolopoulos\'], \'published\': \'2025-06-30T15:03:35+00:00\', \'categories\': [\'cs.DC\', \'cs.AI\', \'cs.LG\', \'cs.PF\'], \'url\': \'http://arxiv.org/abs/2506.23934v1\'}'}, {'id': 18, 'arxiv_id': '2506.23930v1', 'base_arxiv_id': '2506.23930', 'version': '1', 'title': 'Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages', 'abstract': 'The rapid expansion of social media leads to a marked increase in hate\nspeech, which threatens personal lives and results in numerous hate crimes.\nDetecting hate speech presents several challenges: diverse dialects, frequent\ncode-mixing, and the prevalence of misspelled words in user-generated content\non social media platforms. Recent progress in hate speech detection is\ntypically concentrated on high-resource languages. However, low-resource\nlanguages still face significant challenges due to the lack of large-scale,\nhigh-quality datasets. This paper investigates how we can overcome this\nlimitation via prompt engineering on large language models (LLMs) focusing on\nlow-resource Bengali language. We investigate six prompting strategies -\nzero-shot prompting, refusal suppression, flattering the classifier, multi-shot\nprompting, role prompting, and finally our innovative metaphor prompting to\ndetect hate speech effectively in low-resource languages. We pioneer the\nmetaphor prompting to circumvent the built-in safety mechanisms of LLMs that\nmarks a significant departure from existing jailbreaking methods. We\ninvestigate all six different prompting strategies on the Llama2-7B model and\ncompare the results extensively with three pre-trained word embeddings - GloVe,\nWord2Vec, and FastText for three different deep learning models - multilayer\nperceptron (MLP), convolutional neural network (CNN), and bidirectional gated\nrecurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in\nthe low-resource Bengali language, we also evaluate it in another low-resource\nlanguage - Hindi, and two high-resource languages - English and German. The\nperformance of all prompting techniques is evaluated using the F1 score, and\nenvironmental impact factor (IF), which measures CO$_2$ emissions, electricity\nusage, and computational time.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23930v1', 'title': 'Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages', 'abstract': 'The rapid expansion of social media leads to a marked increase in hate\\nspeech, which threatens personal lives and results in numerous hate crimes.\\nDetecting hate speech presents several challenges: diverse dialects, frequent\\ncode-mixing, and the prevalence of misspelled words in user-generated content\\non social media platforms. Recent progress in hate speech detection is\\ntypically concentrated on high-resource languages. However, low-resource\\nlanguages still face significant challenges due to the lack of large-scale,\\nhigh-quality datasets. This paper investigates how we can overcome this\\nlimitation via prompt engineering on large language models (LLMs) focusing on\\nlow-resource Bengali language. We investigate six prompting strategies -\\nzero-shot prompting, refusal suppression, flattering the classifier, multi-shot\\nprompting, role prompting, and finally our innovative metaphor prompting to\\ndetect hate speech effectively in low-resource languages. We pioneer the\\nmetaphor prompting to circumvent the built-in safety mechanisms of LLMs that\\nmarks a significant departure from existing jailbreaking methods. We\\ninvestigate all six different prompting strategies on the Llama2-7B model and\\ncompare the results extensively with three pre-trained word embeddings - GloVe,\\nWord2Vec, and FastText for three different deep learning models - multilayer\\nperceptron (MLP), convolutional neural network (CNN), and bidirectional gated\\nrecurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in\\nthe low-resource Bengali language, we also evaluate it in another low-resource\\nlanguage - Hindi, and two high-resource languages - English and German. The\\nperformance of all prompting techniques is evaluated using the F1 score, and\\nenvironmental impact factor (IF), which measures CO$_2$ emissions, electricity\\nusage, and computational time.', 'authors': ['Ruhina Tabasshum Prome', 'Tarikul Islam Tamiti', 'Anomadarshi Barua'], 'published': '2025-06-30T14:59:25+00:00', 'categories': ['cs.CL', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23930v1'}"}, {'id': 19, 'arxiv_id': '2506.23926v1', 'base_arxiv_id': '2506.23926', 'version': '1', 'title': 'Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system', 'abstract': 'Resilience non-equilibrium measurement, the ability to maintain fundamental\nfunctionality amidst failures and errors, is crucial for scientific management\nand engineering applications of industrial chain. The problem is particularly\nchallenging when the number or types of multiple co-evolution of resilience\n(for example, randomly placed) are extremely chaos. Existing end-to-end deep\nlearning ordinarily do not generalize well to unseen full-feld reconstruction\nof spatiotemporal co-evolution structure, and predict resilience of network\ntopology, especially in multiple chaos data regimes typically seen in\nreal-world applications. To address this challenge, here we propose industrial\nbrain, a human-like autonomous cognitive decision-making and planning framework\nintegrating higher-order activity-driven neuro network and CT-OODA symbolic\nreasoning to autonomous plan resilience directly from observational data of\nglobal variable. The industrial brain not only understands and model structure\nof node activity dynamics and network co-evolution topology without simplifying\nassumptions, and reveal the underlying laws hidden behind complex networks, but\nalso enabling accurate resilience prediction, inference, and planning.\nExperimental results show that industrial brain significantly outperforms\nresilience prediction and planning methods, with an accurate improvement of up\nto 10.8\\% over GoT and OlaGPT framework and 11.03\\% over spectral dimension\nreduction. It also generalizes to unseen topologies and dynamics and maintains\nrobust performance despite observational disturbances. Our findings suggest\nthat industrial brain addresses an important gap in resilience prediction and\nplanning for industrial chain.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23926v1', 'title': 'Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system', 'abstract': 'Resilience non-equilibrium measurement, the ability to maintain fundamental\\nfunctionality amidst failures and errors, is crucial for scientific management\\nand engineering applications of industrial chain. The problem is particularly\\nchallenging when the number or types of multiple co-evolution of resilience\\n(for example, randomly placed) are extremely chaos. Existing end-to-end deep\\nlearning ordinarily do not generalize well to unseen full-feld reconstruction\\nof spatiotemporal co-evolution structure, and predict resilience of network\\ntopology, especially in multiple chaos data regimes typically seen in\\nreal-world applications. To address this challenge, here we propose industrial\\nbrain, a human-like autonomous cognitive decision-making and planning framework\\nintegrating higher-order activity-driven neuro network and CT-OODA symbolic\\nreasoning to autonomous plan resilience directly from observational data of\\nglobal variable. The industrial brain not only understands and model structure\\nof node activity dynamics and network co-evolution topology without simplifying\\nassumptions, and reveal the underlying laws hidden behind complex networks, but\\nalso enabling accurate resilience prediction, inference, and planning.\\nExperimental results show that industrial brain significantly outperforms\\nresilience prediction and planning methods, with an accurate improvement of up\\nto 10.8\\\\% over GoT and OlaGPT framework and 11.03\\\\% over spectral dimension\\nreduction. It also generalizes to unseen topologies and dynamics and maintains\\nrobust performance despite observational disturbances. Our findings suggest\\nthat industrial brain addresses an important gap in resilience prediction and\\nplanning for industrial chain.', 'authors': ['Junping Wang', 'Bicheng Wang', 'Yibo Xuea', 'Yuan Xie'], 'published': '2025-06-30T14:54:52+00:00', 'categories': ['cs.AI', 'cs.LG'], 'url': 'http://arxiv.org/abs/2506.23926v1'}"}, {'id': 20, 'arxiv_id': '2506.23924v1', 'base_arxiv_id': '2506.23924', 'version': '1', 'title': 'Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice', 'abstract': "Large language models (LLMs) have exhibited expert-level capabilities across\nvarious domains. However, their abilities to solve problems in Operations\nResearch (OR) -- the analysis and optimization of mathematical models derived\nfrom real-world problems or their verbal descriptions -- remain underexplored.\nIn this work, we take a first step toward evaluating LLMs' abilities to solve\nstochastic modeling problems, a core class of OR problems characterized by\nuncertainty and typically involving tools from probability, statistics, and\nstochastic processes. We manually procure a representative set of\ngraduate-level homework and doctoral qualification-exam problems and test LLMs'\nabilities to solve them. We further leverage SimOpt, an open-source library of\nsimulation-optimization problems and solvers, to investigate LLMs' abilities to\nmake real-world decisions under uncertainty. Our results show that, though a\nnontrivial amount of work is still needed to reliably automate the stochastic\nmodeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on\npar with human experts in both classroom and practical settings. These findings\nhighlight the potential of building AI agents that assist OR researchers and\namplify the real-world impact of OR through automation.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23924v1\', \'title\': \'Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice\', \'abstract\': "Large language models (LLMs) have exhibited expert-level capabilities across\\nvarious domains. However, their abilities to solve problems in Operations\\nResearch (OR) -- the analysis and optimization of mathematical models derived\\nfrom real-world problems or their verbal descriptions -- remain underexplored.\\nIn this work, we take a first step toward evaluating LLMs\' abilities to solve\\nstochastic modeling problems, a core class of OR problems characterized by\\nuncertainty and typically involving tools from probability, statistics, and\\nstochastic processes. We manually procure a representative set of\\ngraduate-level homework and doctoral qualification-exam problems and test LLMs\'\\nabilities to solve them. We further leverage SimOpt, an open-source library of\\nsimulation-optimization problems and solvers, to investigate LLMs\' abilities to\\nmake real-world decisions under uncertainty. Our results show that, though a\\nnontrivial amount of work is still needed to reliably automate the stochastic\\nmodeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on\\npar with human experts in both classroom and practical settings. These findings\\nhighlight the potential of building AI agents that assist OR researchers and\\namplify the real-world impact of OR through automation.", \'authors\': [\'Akshit Kumar\', \'Tianyi Peng\', \'Yuhang Wu\', \'Assaf Zeevi\'], \'published\': \'2025-06-30T14:54:15+00:00\', \'categories\': [\'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23924v1\'}'}, {'id': 21, 'arxiv_id': '2506.23923v1', 'base_arxiv_id': '2506.23923', 'version': '1', 'title': 'Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System', 'abstract': 'Resin infusion (RI) and resin transfer moulding (RTM) are critical processes\nfor the manufacturing of high-performance fibre-reinforced polymer composites,\nparticularly for large-scale applications such as wind turbine blades.\nControlling the resin flow dynamics in these processes is critical to ensure\nthe uniform impregnation of the fibre reinforcements, thereby preventing\nresidual porosities and dry spots that impact the consequent structural\nintegrity of the final component. This paper presents a reinforcement learning\n(RL) based strategy, established using process simulations, for synchronising\nthe different resin flow fronts in an infusion scenario involving two resin\ninlets and a single outlet. Using Proximal Policy Optimisation (PPO), our\napproach addresses the challenge of managing the fluid dynamics in a partially\nobservable environment. The results demonstrate the effectiveness of the RL\napproach in achieving an accurate flow convergence, highlighting its potential\ntowards improving process control and product quality in composites\nmanufacturing.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23923v1', 'title': 'Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System', 'abstract': 'Resin infusion (RI) and resin transfer moulding (RTM) are critical processes\\nfor the manufacturing of high-performance fibre-reinforced polymer composites,\\nparticularly for large-scale applications such as wind turbine blades.\\nControlling the resin flow dynamics in these processes is critical to ensure\\nthe uniform impregnation of the fibre reinforcements, thereby preventing\\nresidual porosities and dry spots that impact the consequent structural\\nintegrity of the final component. This paper presents a reinforcement learning\\n(RL) based strategy, established using process simulations, for synchronising\\nthe different resin flow fronts in an infusion scenario involving two resin\\ninlets and a single outlet. Using Proximal Policy Optimisation (PPO), our\\napproach addresses the challenge of managing the fluid dynamics in a partially\\nobservable environment. The results demonstrate the effectiveness of the RL\\napproach in achieving an accurate flow convergence, highlighting its potential\\ntowards improving process control and product quality in composites\\nmanufacturing.', 'authors': ['Miguel Camacho-S√°nchez', 'Fernando Garc√≠a-Torres', 'Jesper John Lisegaard', 'Roc√≠o del Amor', 'Sankhya Mohanty', 'Valery Naranjo'], 'published': '2025-06-30T14:50:18+00:00', 'categories': ['cs.LG', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23923v1'}"}, {'id': 22, 'arxiv_id': '2506.23908v1', 'base_arxiv_id': '2506.23908', 'version': '1', 'title': 'Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence', 'abstract': 'Sound deductive reasoning -- the ability to derive new knowledge from\nexisting facts and rules -- is an indisputably desirable aspect of general\nintelligence. Despite the major advances of AI systems in areas such as math\nand science, especially since the introduction of transformer architectures, it\nis well-documented that even the most advanced frontier systems regularly and\nconsistently falter on easily-solvable deductive reasoning tasks. Hence, these\nsystems are unfit to fulfill the dream of achieving artificial general\nintelligence capable of sound deductive reasoning. We argue that their unsound\nbehavior is a consequence of the statistical learning approach powering their\ndevelopment. To overcome this, we contend that to achieve reliable deductive\nreasoning in learning-based AI systems, researchers must fundamentally shift\nfrom optimizing for statistical performance against distributions on reasoning\nproblems and algorithmic tasks to embracing the more ambitious exact learning\nparadigm, which demands correctness on all inputs. We argue that exact learning\nis both essential and possible, and that this ambitious objective should guide\nalgorithm design.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23908v1', 'title': 'Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence', 'abstract': 'Sound deductive reasoning -- the ability to derive new knowledge from\\nexisting facts and rules -- is an indisputably desirable aspect of general\\nintelligence. Despite the major advances of AI systems in areas such as math\\nand science, especially since the introduction of transformer architectures, it\\nis well-documented that even the most advanced frontier systems regularly and\\nconsistently falter on easily-solvable deductive reasoning tasks. Hence, these\\nsystems are unfit to fulfill the dream of achieving artificial general\\nintelligence capable of sound deductive reasoning. We argue that their unsound\\nbehavior is a consequence of the statistical learning approach powering their\\ndevelopment. To overcome this, we contend that to achieve reliable deductive\\nreasoning in learning-based AI systems, researchers must fundamentally shift\\nfrom optimizing for statistical performance against distributions on reasoning\\nproblems and algorithmic tasks to embracing the more ambitious exact learning\\nparadigm, which demands correctness on all inputs. We argue that exact learning\\nis both essential and possible, and that this ambitious objective should guide\\nalgorithm design.', 'authors': ['Andr√°s Gy√∂rgy', 'Tor Lattimore', 'Nevena Laziƒá', 'Csaba Szepesv√°ri'], 'published': '2025-06-30T14:37:50+00:00', 'categories': ['cs.AI', 'cs.LG'], 'url': 'http://arxiv.org/abs/2506.23908v1'}"}, {'id': 23, 'arxiv_id': '2506.23903v1', 'base_arxiv_id': '2506.23903', 'version': '1', 'title': 'GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models', 'abstract': 'Accurate and generalizable object segmentation in ultrasound imaging remains\na significant challenge due to anatomical variability, diverse imaging\nprotocols, and limited annotated data. In this study, we propose a\nprompt-driven vision-language model (VLM) that integrates Grounding DINO with\nSAM2 to enable object segmentation across multiple ultrasound organs. A total\nof 18 public ultrasound datasets, encompassing the breast, thyroid, liver,\nprostate, kidney, and paraspinal muscle, were utilized. These datasets were\ndivided into 15 for fine-tuning and validation of Grounding DINO using Low Rank\nAdaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for\ntesting to evaluate performance in unseen distributions. Comprehensive\nexperiments demonstrate that our approach outperforms state-of-the-art\nsegmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse,\nand SAMUS on most seen datasets while maintaining strong performance on unseen\ndatasets without additional fine-tuning. These results underscore the promise\nof VLMs in scalable and robust ultrasound image analysis, reducing dependence\non large, organ-specific annotated datasets. We will publish our code on\ncode.sonography.ai after acceptance.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23903v1', 'title': 'GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models', 'abstract': 'Accurate and generalizable object segmentation in ultrasound imaging remains\\na significant challenge due to anatomical variability, diverse imaging\\nprotocols, and limited annotated data. In this study, we propose a\\nprompt-driven vision-language model (VLM) that integrates Grounding DINO with\\nSAM2 to enable object segmentation across multiple ultrasound organs. A total\\nof 18 public ultrasound datasets, encompassing the breast, thyroid, liver,\\nprostate, kidney, and paraspinal muscle, were utilized. These datasets were\\ndivided into 15 for fine-tuning and validation of Grounding DINO using Low Rank\\nAdaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for\\ntesting to evaluate performance in unseen distributions. Comprehensive\\nexperiments demonstrate that our approach outperforms state-of-the-art\\nsegmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse,\\nand SAMUS on most seen datasets while maintaining strong performance on unseen\\ndatasets without additional fine-tuning. These results underscore the promise\\nof VLMs in scalable and robust ultrasound image analysis, reducing dependence\\non large, organ-specific annotated datasets. We will publish our code on\\ncode.sonography.ai after acceptance.', 'authors': ['Hamza Rasaee', 'Taha Koleilat', 'Hassan Rivaz'], 'published': '2025-06-30T14:33:44+00:00', 'categories': ['cs.CV', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23903v1'}"}, {'id': 24, 'arxiv_id': '2506.23875v1', 'base_arxiv_id': '2506.23875', 'version': '1', 'title': 'Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic', 'abstract': 'The chain of thought is fundamental in Transformers, which is to perform\nstep-by-step reasoning. Besides what intermediate steps work, the order of\nthese steps critically affects the difficulty of the reasoning. This study\naddresses a novel task of unraveling chain of thought - reordering decoder\ninput tokens to a learning-friendly sequence for Transformers to learn\narithmetic tasks. The proposed pipeline first trains a Transformer on a mixture\nof target sequences arranged in different orders and then identifies benign\norders as those with fast loss drops in the early stage. As the search space\ngrows factorially with sequence length, we propose a two-stage hierarchical\napproach for inter- and intra-block reordering. Experiments on four\norder-sensitive arithmetic tasks show that our method identifies a\nlearning-friendly order out of a few billion candidates. Notably, on the\nmultiplication task, it recovered the reverse-digit order reported in prior\nstudies.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23875v1', 'title': 'Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic', 'abstract': 'The chain of thought is fundamental in Transformers, which is to perform\\nstep-by-step reasoning. Besides what intermediate steps work, the order of\\nthese steps critically affects the difficulty of the reasoning. This study\\naddresses a novel task of unraveling chain of thought - reordering decoder\\ninput tokens to a learning-friendly sequence for Transformers to learn\\narithmetic tasks. The proposed pipeline first trains a Transformer on a mixture\\nof target sequences arranged in different orders and then identifies benign\\norders as those with fast loss drops in the early stage. As the search space\\ngrows factorially with sequence length, we propose a two-stage hierarchical\\napproach for inter- and intra-block reordering. Experiments on four\\norder-sensitive arithmetic tasks show that our method identifies a\\nlearning-friendly order out of a few billion candidates. Notably, on the\\nmultiplication task, it recovered the reverse-digit order reported in prior\\nstudies.', 'authors': ['Yuta Sato', 'Kazuhiko Kawamoto', 'Hiroshi Kera'], 'published': '2025-06-30T14:05:53+00:00', 'categories': ['cs.LG', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23875v1'}"}, {'id': 25, 'arxiv_id': '2506.23869v1', 'base_arxiv_id': '2506.23869', 'version': '1', 'title': 'Scaling Self-Supervised Representation Learning for Symbolic Piano Performance', 'abstract': 'We study the capabilities of generative autoregressive transformer models\ntrained on large amounts of symbolic solo-piano transcriptions. After first\npretraining on approximately 60,000 hours of music, we use a comparatively\nsmaller, high-quality subset, to finetune models to produce musical\ncontinuations, perform symbolic classification tasks, and produce\ngeneral-purpose contrastive MIDI embeddings by adapting the SimCLR framework to\nsymbolic music. When evaluating piano continuation coherence, our generative\nmodel outperforms leading symbolic generation techniques and remains\ncompetitive with proprietary audio generation models. On MIR classification\nbenchmarks, frozen representations from our contrastive model achieve\nstate-of-the-art results in linear probe experiments, while direct finetuning\ndemonstrates the generalizability of pretrained representations, often\nrequiring only a few hundred labeled examples to specialize to downstream\ntasks.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23869v1', 'title': 'Scaling Self-Supervised Representation Learning for Symbolic Piano Performance', 'abstract': 'We study the capabilities of generative autoregressive transformer models\\ntrained on large amounts of symbolic solo-piano transcriptions. After first\\npretraining on approximately 60,000 hours of music, we use a comparatively\\nsmaller, high-quality subset, to finetune models to produce musical\\ncontinuations, perform symbolic classification tasks, and produce\\ngeneral-purpose contrastive MIDI embeddings by adapting the SimCLR framework to\\nsymbolic music. When evaluating piano continuation coherence, our generative\\nmodel outperforms leading symbolic generation techniques and remains\\ncompetitive with proprietary audio generation models. On MIR classification\\nbenchmarks, frozen representations from our contrastive model achieve\\nstate-of-the-art results in linear probe experiments, while direct finetuning\\ndemonstrates the generalizability of pretrained representations, often\\nrequiring only a few hundred labeled examples to specialize to downstream\\ntasks.', 'authors': ['Louis Bradshaw', 'Honglu Fan', 'Alexander Spangher', 'Stella Biderman', 'Simon Colton'], 'published': '2025-06-30T14:00:14+00:00', 'categories': ['cs.SD', 'cs.AI', 'cs.LG', 'eess.AS'], 'url': 'http://arxiv.org/abs/2506.23869v1'}"}, {'id': 26, 'arxiv_id': '2506.23855v1', 'base_arxiv_id': '2506.23855', 'version': '1', 'title': 'Differentially Private Synthetic Data Release for Topics API Outputs', 'abstract': "The analysis of the privacy properties of Privacy-Preserving Ads APIs is an\narea of research that has received strong interest from academics, industry,\nand regulators. Despite this interest, the empirical study of these methods is\nhindered by the lack of publicly available data. Reliable empirical analysis of\nthe privacy properties of an API, in fact, requires access to a dataset\nconsisting of realistic API outputs; however, privacy concerns prevent the\ngeneral release of such data to the public.\n  In this work, we develop a novel methodology to construct synthetic API\noutputs that are simultaneously realistic enough to enable accurate study and\nprovide strong privacy protections. We focus on one Privacy-Preserving Ads\nAPIs: the Topics API, part of Google Chrome's Privacy Sandbox. We developed a\nmethodology to generate a differentially-private dataset that closely matches\nthe re-identification risk properties of the real Topics API data. The use of\ndifferential privacy provides strong theoretical bounds on the leakage of\nprivate user information from this release.\n  Our methodology is based on first computing a large number of\ndifferentially-private statistics describing how output API traces evolve over\ntime. Then, we design a parameterized distribution over sequences of API traces\nand optimize its parameters so that they closely match the statistics obtained.\nFinally, we create the synthetic data by drawing from this distribution.\n  Our work is complemented by an open-source release of the anonymized dataset\nobtained by this methodology. We hope this will enable external researchers to\nanalyze the API in-depth and replicate prior and future work on a realistic\nlarge-scale dataset. We believe that this work will contribute to fostering\ntransparency regarding the privacy properties of Privacy-Preserving Ads APIs.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23855v1\', \'title\': \'Differentially Private Synthetic Data Release for Topics API Outputs\', \'abstract\': "The analysis of the privacy properties of Privacy-Preserving Ads APIs is an\\narea of research that has received strong interest from academics, industry,\\nand regulators. Despite this interest, the empirical study of these methods is\\nhindered by the lack of publicly available data. Reliable empirical analysis of\\nthe privacy properties of an API, in fact, requires access to a dataset\\nconsisting of realistic API outputs; however, privacy concerns prevent the\\ngeneral release of such data to the public.\\n  In this work, we develop a novel methodology to construct synthetic API\\noutputs that are simultaneously realistic enough to enable accurate study and\\nprovide strong privacy protections. We focus on one Privacy-Preserving Ads\\nAPIs: the Topics API, part of Google Chrome\'s Privacy Sandbox. We developed a\\nmethodology to generate a differentially-private dataset that closely matches\\nthe re-identification risk properties of the real Topics API data. The use of\\ndifferential privacy provides strong theoretical bounds on the leakage of\\nprivate user information from this release.\\n  Our methodology is based on first computing a large number of\\ndifferentially-private statistics describing how output API traces evolve over\\ntime. Then, we design a parameterized distribution over sequences of API traces\\nand optimize its parameters so that they closely match the statistics obtained.\\nFinally, we create the synthetic data by drawing from this distribution.\\n  Our work is complemented by an open-source release of the anonymized dataset\\nobtained by this methodology. We hope this will enable external researchers to\\nanalyze the API in-depth and replicate prior and future work on a realistic\\nlarge-scale dataset. We believe that this work will contribute to fostering\\ntransparency regarding the privacy properties of Privacy-Preserving Ads APIs.", \'authors\': [\'Travis Dick\', \'Alessandro Epasto\', \'Adel Javanmard\', \'Josh Karlin\', \'Andres Munoz Medina\', \'Vahab Mirrokni\', \'Sergei Vassilvitskii\', \'Peilin Zhong\'], \'published\': \'2025-06-30T13:46:57+00:00\', \'categories\': [\'cs.CR\', \'cs.AI\', \'cs.LG\'], \'url\': \'http://arxiv.org/abs/2506.23855v1\'}'}, {'id': 27, 'arxiv_id': '2506.23845v1', 'base_arxiv_id': '2506.23845', 'version': '1', 'title': 'Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts', 'abstract': 'While sparse autoencoders (SAEs) have generated significant excitement, a\nseries of negative results have added to skepticism about their usefulness.\nHere, we establish a conceptual distinction that reconciles competing\nnarratives surrounding SAEs. We argue that while SAEs may be less effective for\nacting on known concepts, SAEs are powerful tools for discovering unknown\nconcepts. This distinction cleanly separates existing negative and positive\nresults, and suggests several classes of SAE applications. Specifically, we\noutline use cases for SAEs in (i) ML interpretability, explainability,\nfairness, auditing, and safety, and (ii) social and health sciences.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23845v1', 'title': 'Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts', 'abstract': 'While sparse autoencoders (SAEs) have generated significant excitement, a\\nseries of negative results have added to skepticism about their usefulness.\\nHere, we establish a conceptual distinction that reconciles competing\\nnarratives surrounding SAEs. We argue that while SAEs may be less effective for\\nacting on known concepts, SAEs are powerful tools for discovering unknown\\nconcepts. This distinction cleanly separates existing negative and positive\\nresults, and suggests several classes of SAE applications. Specifically, we\\noutline use cases for SAEs in (i) ML interpretability, explainability,\\nfairness, auditing, and safety, and (ii) social and health sciences.', 'authors': ['Kenny Peng', 'Rajiv Movva', 'Jon Kleinberg', 'Emma Pierson', 'Nikhil Garg'], 'published': '2025-06-30T13:35:56+00:00', 'categories': ['cs.LG', 'cs.AI', 'cs.CL', 'cs.CY'], 'url': 'http://arxiv.org/abs/2506.23845v1'}"}, {'id': 58, 'arxiv_id': '2506.23628v1', 'base_arxiv_id': '2506.23628', 'version': '1', 'title': 'The Kubernetes Network Driver Model: A Composable Architecture for High-Performance Networking', 'abstract': 'Traditional Kubernetes networking struggles to meet the escalating demands of\nAI/ML and evolving Telco infrastructure. This paper introduces Kubernetes\nNetwork Drivers (KNDs), a transformative, modular, and declarative architecture\ndesigned to overcome current imperative provisioning and API limitations. KNDs\nintegrate network resource management into Kubernetes\' core by utilizing\nDynamic Resource Allocation (DRA), Node Resource Interface (NRI) improvements,\nand upcoming OCI Runtime Specification changes. Our DraNet implementation\ndemonstrates declarative attachment of network interfaces, including Remote\nDirect Memory Access (RDMA) devices, significantly boosting high-performance\nAI/ML workloads. This capability enables sophisticated cloud-native\napplications and lays crucial groundwork for future Telco solutions, fostering\na "galaxy" of specialized KNDs for enhanced application delivery and reduced\noperational complexity.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23628v1\', \'title\': \'The Kubernetes Network Driver Model: A Composable Architecture for High-Performance Networking\', \'abstract\': \'Traditional Kubernetes networking struggles to meet the escalating demands of\\nAI/ML and evolving Telco infrastructure. This paper introduces Kubernetes\\nNetwork Drivers (KNDs), a transformative, modular, and declarative architecture\\ndesigned to overcome current imperative provisioning and API limitations. KNDs\\nintegrate network resource management into Kubernetes\\\' core by utilizing\\nDynamic Resource Allocation (DRA), Node Resource Interface (NRI) improvements,\\nand upcoming OCI Runtime Specification changes. Our DraNet implementation\\ndemonstrates declarative attachment of network interfaces, including Remote\\nDirect Memory Access (RDMA) devices, significantly boosting high-performance\\nAI/ML workloads. This capability enables sophisticated cloud-native\\napplications and lays crucial groundwork for future Telco solutions, fostering\\na "galaxy" of specialized KNDs for enhanced application delivery and reduced\\noperational complexity.\', \'authors\': [\'Antonio Ojea\'], \'published\': \'2025-06-30T08:45:54+00:00\', \'categories\': [\'cs.NI\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23628v1\'}'}, {'id': 28, 'arxiv_id': '2506.23844v1', 'base_arxiv_id': '2506.23844', 'version': '1', 'title': 'A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents', 'abstract': "Recent advances in large language models (LLMs) have catalyzed the rise of\nautonomous AI agents capable of perceiving, reasoning, and acting in dynamic,\nopen-ended environments. These large-model agents mark a paradigm shift from\nstatic inference systems to interactive, memory-augmented entities. While these\ncapabilities significantly expand the functional scope of AI, they also\nintroduce qualitatively novel security risks - such as memory poisoning, tool\nmisuse, reward hacking, and emergent misalignment - that extend beyond the\nthreat models of conventional systems or standalone LLMs. In this survey, we\nfirst examine the structural foundations and key capabilities that underpin\nincreasing levels of agent autonomy, including long-term memory retention,\nmodular tool use, recursive planning, and reflective reasoning. We then analyze\nthe corresponding security vulnerabilities across the agent stack, identifying\nfailure modes such as deferred decision hazards, irreversible tool chains, and\ndeceptive behaviors arising from internal state drift or value misalignment.\nThese risks are traced to architectural fragilities that emerge across\nperception, cognition, memory, and action modules. To address these challenges,\nwe systematically review recent defense strategies deployed at different\nautonomy layers, including input sanitization, memory lifecycle control,\nconstrained decision-making, structured tool invocation, and introspective\nreflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a\nunified cognitive framework grounded in Constrained Markov Decision Processes\n(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,\nand joint reward-risk optimization to enable principled, proactive safety\nacross the agent's decision-making loop.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23844v1\', \'title\': \'A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents\', \'abstract\': "Recent advances in large language models (LLMs) have catalyzed the rise of\\nautonomous AI agents capable of perceiving, reasoning, and acting in dynamic,\\nopen-ended environments. These large-model agents mark a paradigm shift from\\nstatic inference systems to interactive, memory-augmented entities. While these\\ncapabilities significantly expand the functional scope of AI, they also\\nintroduce qualitatively novel security risks - such as memory poisoning, tool\\nmisuse, reward hacking, and emergent misalignment - that extend beyond the\\nthreat models of conventional systems or standalone LLMs. In this survey, we\\nfirst examine the structural foundations and key capabilities that underpin\\nincreasing levels of agent autonomy, including long-term memory retention,\\nmodular tool use, recursive planning, and reflective reasoning. We then analyze\\nthe corresponding security vulnerabilities across the agent stack, identifying\\nfailure modes such as deferred decision hazards, irreversible tool chains, and\\ndeceptive behaviors arising from internal state drift or value misalignment.\\nThese risks are traced to architectural fragilities that emerge across\\nperception, cognition, memory, and action modules. To address these challenges,\\nwe systematically review recent defense strategies deployed at different\\nautonomy layers, including input sanitization, memory lifecycle control,\\nconstrained decision-making, structured tool invocation, and introspective\\nreflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a\\nunified cognitive framework grounded in Constrained Markov Decision Processes\\n(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,\\nand joint reward-risk optimization to enable principled, proactive safety\\nacross the agent\'s decision-making loop.", \'authors\': [\'Hang Su\', \'Jun Luo\', \'Chang Liu\', \'Xiao Yang\', \'Yichi Zhang\', \'Yinpeng Dong\', \'Jun Zhu\'], \'published\': \'2025-06-30T13:34:34+00:00\', \'categories\': [\'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23844v1\'}'}, {'id': 29, 'arxiv_id': '2506.23840v1', 'base_arxiv_id': '2506.23840', 'version': '1', 'title': 'Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model', 'abstract': 'Large Reasoning Models (LRMs) excel at solving complex problems but face an\noverthinking dilemma. When handling simple tasks, they often produce verbose\nresponses overloaded with thinking tokens (e.g., wait, however). These tokens\ntrigger unnecessary high-level reasoning behaviors like reflection and\nbacktracking, reducing efficiency. In this work, our pilot study reveals that\nthese thinking-token-induced behaviors are not essential for effective\nproblem-solving and may even hinder correct reasoning within constrained token\nbudgets. We identify this phenomenon as the thinking trap. To mitigate this\nissue, we propose Dual Policy Preference Optimization (DuP-PO), a novel\nalgorithm featuring: (1) A rollout sampling strategy that guarantees balanced\nexposure to responses with and without thinking tokens; (2) A fine-grained\nadvantage control technique to dynamically regulate the prediction of target\ntokens; (3) A policy shaping method ensuring stable gradient contributions from\nthinking tokens. Experimental results on five popular math reasoning benchmarks\nshow that DuP-PO performs well on the popular LRM, which significantly improves\ntheir token efficiency during reasoning, while achieving superior performance\nof the base model.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23840v1', 'title': 'Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model', 'abstract': 'Large Reasoning Models (LRMs) excel at solving complex problems but face an\\noverthinking dilemma. When handling simple tasks, they often produce verbose\\nresponses overloaded with thinking tokens (e.g., wait, however). These tokens\\ntrigger unnecessary high-level reasoning behaviors like reflection and\\nbacktracking, reducing efficiency. In this work, our pilot study reveals that\\nthese thinking-token-induced behaviors are not essential for effective\\nproblem-solving and may even hinder correct reasoning within constrained token\\nbudgets. We identify this phenomenon as the thinking trap. To mitigate this\\nissue, we propose Dual Policy Preference Optimization (DuP-PO), a novel\\nalgorithm featuring: (1) A rollout sampling strategy that guarantees balanced\\nexposure to responses with and without thinking tokens; (2) A fine-grained\\nadvantage control technique to dynamically regulate the prediction of target\\ntokens; (3) A policy shaping method ensuring stable gradient contributions from\\nthinking tokens. Experimental results on five popular math reasoning benchmarks\\nshow that DuP-PO performs well on the popular LRM, which significantly improves\\ntheir token efficiency during reasoning, while achieving superior performance\\nof the base model.', 'authors': ['Bowen Ding', 'Yuhan Chen', 'Futing Wang', 'Lingfeng Ming', 'Tao Lin'], 'published': '2025-06-30T13:30:33+00:00', 'categories': ['cs.CL', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23840v1'}"}, {'id': 30, 'arxiv_id': '2506.23826v1', 'base_arxiv_id': '2506.23826', 'version': '1', 'title': 'Towards the "Digital Me": A vision of authentic Conversational Agents powered by personal Human Digital Twins', 'abstract': "Human Digital Twins (HDTs) have traditionally been conceptualized as\ndata-driven models designed to support decision-making across various domains.\nHowever, recent advancements in conversational AI open new possibilities for\nHDTs to function as authentic, interactive digital counterparts of individuals.\nThis paper introduces a novel HDT system architecture that integrates large\nlanguage models with dynamically updated personal data, enabling it to mirror\nan individual's conversational style, memories, and behaviors. To achieve this,\nour approach implements context-aware memory retrieval, neural\nplasticity-inspired consolidation, and adaptive learning mechanisms, creating a\nmore natural and evolving digital persona. The resulting system does not only\nreplicate an individual's unique conversational style depending on who they are\nspeaking with, but also enriches responses with dynamically captured personal\nexperiences, opinions, and memories. While this marks a significant step toward\ndeveloping authentic virtual counterparts, it also raises critical ethical\nconcerns regarding privacy, accountability, and the long-term implications of\npersistent digital identities. This study contributes to the field of HDTs by\ndescribing our novel system architecture, demonstrating its capabilities, and\ndiscussing future directions and emerging challenges to ensure the responsible\nand ethical development of HDTs.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23826v1\', \'title\': \'Towards the "Digital Me": A vision of authentic Conversational Agents powered by personal Human Digital Twins\', \'abstract\': "Human Digital Twins (HDTs) have traditionally been conceptualized as\\ndata-driven models designed to support decision-making across various domains.\\nHowever, recent advancements in conversational AI open new possibilities for\\nHDTs to function as authentic, interactive digital counterparts of individuals.\\nThis paper introduces a novel HDT system architecture that integrates large\\nlanguage models with dynamically updated personal data, enabling it to mirror\\nan individual\'s conversational style, memories, and behaviors. To achieve this,\\nour approach implements context-aware memory retrieval, neural\\nplasticity-inspired consolidation, and adaptive learning mechanisms, creating a\\nmore natural and evolving digital persona. The resulting system does not only\\nreplicate an individual\'s unique conversational style depending on who they are\\nspeaking with, but also enriches responses with dynamically captured personal\\nexperiences, opinions, and memories. While this marks a significant step toward\\ndeveloping authentic virtual counterparts, it also raises critical ethical\\nconcerns regarding privacy, accountability, and the long-term implications of\\npersistent digital identities. This study contributes to the field of HDTs by\\ndescribing our novel system architecture, demonstrating its capabilities, and\\ndiscussing future directions and emerging challenges to ensure the responsible\\nand ethical development of HDTs.", \'authors\': [\'Llu√≠s C. Coll\', \'Martin W. Lauer-Schmaltz\', \'Philip Cash\', \'John P. Hansen\', \'Anja Maier\'], \'published\': \'2025-06-30T13:18:31+00:00\', \'categories\': [\'cs.ET\', \'cs.AI\', \'cs.CY\', \'cs.HC\', \'cs.IR\'], \'url\': \'http://arxiv.org/abs/2506.23826v1\'}'}, {'id': 31, 'arxiv_id': '2506.23793v1', 'base_arxiv_id': '2506.23793', 'version': '1', 'title': 'Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning', 'abstract': 'Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot\ntrajectory planning problems, where multiple homogeneous robots simultaneously\nmove in the shared environment. While solving MAPF optimally has been proven to\nbe NP-hard, scalable, and efficient, solvers are vital for real-world\napplications like logistics, search-and-rescue, etc. To this end, decentralized\nsuboptimal MAPF solvers that leverage machine learning have come on stage.\nBuilding on the success of the recently introduced MAPF-GPT, a pure imitation\nlearning solver, we introduce MAPF-GPT-DDG. This novel approach effectively\nfine-tunes the pre-trained MAPF model using centralized expert data. Leveraging\na novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training\nwhile significantly improving performance at test time. Our experiments\ndemonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF\nsolvers, including the original MAPF-GPT, regarding solution quality across\nmany testing scenarios. Remarkably, it can work with MAPF instances involving\nup to 1 million agents in a single environment, setting a new milestone for\nscalability in MAPF domains.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23793v1', 'title': 'Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning', 'abstract': 'Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot\\ntrajectory planning problems, where multiple homogeneous robots simultaneously\\nmove in the shared environment. While solving MAPF optimally has been proven to\\nbe NP-hard, scalable, and efficient, solvers are vital for real-world\\napplications like logistics, search-and-rescue, etc. To this end, decentralized\\nsuboptimal MAPF solvers that leverage machine learning have come on stage.\\nBuilding on the success of the recently introduced MAPF-GPT, a pure imitation\\nlearning solver, we introduce MAPF-GPT-DDG. This novel approach effectively\\nfine-tunes the pre-trained MAPF model using centralized expert data. Leveraging\\na novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training\\nwhile significantly improving performance at test time. Our experiments\\ndemonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF\\nsolvers, including the original MAPF-GPT, regarding solution quality across\\nmany testing scenarios. Remarkably, it can work with MAPF instances involving\\nup to 1 million agents in a single environment, setting a new milestone for\\nscalability in MAPF domains.', 'authors': ['Anton Andreychuk', 'Konstantin Yakovlev', 'Aleksandr Panov', 'Alexey Skrynnik'], 'published': '2025-06-30T12:34:31+00:00', 'categories': ['cs.AI', 'cs.LG', 'cs.MA'], 'url': 'http://arxiv.org/abs/2506.23793v1'}"}, {'id': 32, 'arxiv_id': '2506.23784v1', 'base_arxiv_id': '2506.23784', 'version': '1', 'title': 'When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)', 'abstract': 'Nielsen transformation is a standard approach for solving word equations: by\nrepeatedly splitting equations and applying simplification steps, equations are\nrewritten until a solution is reached. When solving a conjunction of word\nequations in this way, the performance of the solver will depend considerably\non the order in which equations are processed. In this work, the use of Graph\nNeural Networks (GNNs) for ranking word equations before and during the solving\nprocess is explored. For this, a novel graph-based representation for word\nequations is presented, preserving global information across conjuncts,\nenabling the GNN to have a holistic view during ranking. To handle the variable\nnumber of conjuncts, three approaches to adapt a multi-classification task to\nthe problem of ranking equations are proposed. The training of the GNN is done\nwith the help of minimum unsatisfiable subsets (MUSes) of word equations. The\nexperimental results show that, compared to state-of-the-art string solvers,\nthe new framework solves more problems in benchmarks where each variable\nappears at most once in each equation.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23784v1', 'title': 'When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)', 'abstract': 'Nielsen transformation is a standard approach for solving word equations: by\\nrepeatedly splitting equations and applying simplification steps, equations are\\nrewritten until a solution is reached. When solving a conjunction of word\\nequations in this way, the performance of the solver will depend considerably\\non the order in which equations are processed. In this work, the use of Graph\\nNeural Networks (GNNs) for ranking word equations before and during the solving\\nprocess is explored. For this, a novel graph-based representation for word\\nequations is presented, preserving global information across conjuncts,\\nenabling the GNN to have a holistic view during ranking. To handle the variable\\nnumber of conjuncts, three approaches to adapt a multi-classification task to\\nthe problem of ranking equations are proposed. The training of the GNN is done\\nwith the help of minimum unsatisfiable subsets (MUSes) of word equations. The\\nexperimental results show that, compared to state-of-the-art string solvers,\\nthe new framework solves more problems in benchmarks where each variable\\nappears at most once in each equation.', 'authors': ['Parosh Aziz Abdulla', 'Mohamed Faouzi Atig', 'Julie Cailler', 'Chencheng Liang', 'Philipp R√ºmmer'], 'published': '2025-06-30T12:24:24+00:00', 'categories': ['cs.AI', 'cs.LG'], 'url': 'http://arxiv.org/abs/2506.23784v1'}"}, {'id': 33, 'arxiv_id': '2506.23783v1', 'base_arxiv_id': '2506.23783', 'version': '1', 'title': 'Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking', 'abstract': 'Combining traditional RGB cameras with bio-inspired event cameras for robust\nobject tracking has garnered increasing attention in recent years. However,\nmost existing multimodal tracking algorithms depend heavily on high-complexity\nVision Transformer architectures for feature extraction and fusion across\nmodalities. This not only leads to substantial computational overhead but also\nlimits the effectiveness of cross-modal interactions. In this paper, we propose\nan efficient RGB-Event object tracking framework based on the linear-complexity\nVision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a\nlightweight Prompt Generator that utilizes embedded features from each\nmodality, together with a shared prompt pool, to dynamically generate\nmodality-specific learnable prompt vectors. These prompts, along with the\nmodality-specific embedded features, are then fed into a Vision Mamba-based\nFEMamba backbone, which facilitates prompt-guided feature extraction,\ncross-modal interaction, and fusion in a unified manner. Finally, the fused\nrepresentations are passed to the tracking head for accurate target\nlocalization. Extensive experimental evaluations on multiple RGB-Event tracking\nbenchmarks, including short-term COESOT dataset and long-term datasets, i.e.,\nFE108 and FELT V2, demonstrate the superior performance and efficiency of the\nproposed tracking framework. The source code and pre-trained models will be\nreleased on https://github.com/Event-AHU/Mamba_FETrack', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23783v1', 'title': 'Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking', 'abstract': 'Combining traditional RGB cameras with bio-inspired event cameras for robust\\nobject tracking has garnered increasing attention in recent years. However,\\nmost existing multimodal tracking algorithms depend heavily on high-complexity\\nVision Transformer architectures for feature extraction and fusion across\\nmodalities. This not only leads to substantial computational overhead but also\\nlimits the effectiveness of cross-modal interactions. In this paper, we propose\\nan efficient RGB-Event object tracking framework based on the linear-complexity\\nVision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a\\nlightweight Prompt Generator that utilizes embedded features from each\\nmodality, together with a shared prompt pool, to dynamically generate\\nmodality-specific learnable prompt vectors. These prompts, along with the\\nmodality-specific embedded features, are then fed into a Vision Mamba-based\\nFEMamba backbone, which facilitates prompt-guided feature extraction,\\ncross-modal interaction, and fusion in a unified manner. Finally, the fused\\nrepresentations are passed to the tracking head for accurate target\\nlocalization. Extensive experimental evaluations on multiple RGB-Event tracking\\nbenchmarks, including short-term COESOT dataset and long-term datasets, i.e.,\\nFE108 and FELT V2, demonstrate the superior performance and efficiency of the\\nproposed tracking framework. The source code and pre-trained models will be\\nreleased on https://github.com/Event-AHU/Mamba_FETrack', 'authors': ['Shiao Wang', 'Ju Huang', 'Qingchuan Ma', 'Jinfeng Gao', 'Chunyi Xu', 'Xiao Wang', 'Lan Chen', 'Bo Jiang'], 'published': '2025-06-30T12:24:01+00:00', 'categories': ['cs.CV', 'cs.AI', 'cs.LG'], 'url': 'http://arxiv.org/abs/2506.23783v1'}"}, {'id': 34, 'arxiv_id': '2506.23782v1', 'base_arxiv_id': '2506.23782', 'version': '1', 'title': 'Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling', 'abstract': 'Graph Neural Networks (GNNs) have demonstrated strong predictive performance\non relational data; however, their confidence estimates often misalign with\nactual predictive correctness, posing significant limitations for deployment in\nsafety-critical settings. While existing graph-aware calibration methods seek\nto mitigate this limitation, they primarily depend on coarse one-hop\nstatistics, such as neighbor-predicted confidence, or latent node embeddings,\nthereby neglecting the fine-grained structural heterogeneity inherent in graph\ntopology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a\npost-hoc calibration framework that assigns node-specific temperatures based on\ntunable heat-kernel graph wavelet features. Specifically, WATS harnesses the\nscalability and topology sensitivity of graph wavelets to refine confidence\nestimates, all without necessitating model retraining or access to neighboring\nlogits or predictions. Extensive evaluations across seven benchmark datasets\nwith varying graph structures and two GNN backbones demonstrate that WATS\nachieves the lowest Expected Calibration Error (ECE) among all compared\nmethods, outperforming both classical and graph-specific baselines by up to\n42.3\\% in ECE and reducing calibration variance by 17.24\\% on average compared\nwith graph-specific methods. Moreover, WATS remains computationally efficient,\nscaling well across graphs of diverse sizes and densities. Code will be\nreleased based on publication.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23782v1', 'title': 'Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling', 'abstract': 'Graph Neural Networks (GNNs) have demonstrated strong predictive performance\\non relational data; however, their confidence estimates often misalign with\\nactual predictive correctness, posing significant limitations for deployment in\\nsafety-critical settings. While existing graph-aware calibration methods seek\\nto mitigate this limitation, they primarily depend on coarse one-hop\\nstatistics, such as neighbor-predicted confidence, or latent node embeddings,\\nthereby neglecting the fine-grained structural heterogeneity inherent in graph\\ntopology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a\\npost-hoc calibration framework that assigns node-specific temperatures based on\\ntunable heat-kernel graph wavelet features. Specifically, WATS harnesses the\\nscalability and topology sensitivity of graph wavelets to refine confidence\\nestimates, all without necessitating model retraining or access to neighboring\\nlogits or predictions. Extensive evaluations across seven benchmark datasets\\nwith varying graph structures and two GNN backbones demonstrate that WATS\\nachieves the lowest Expected Calibration Error (ECE) among all compared\\nmethods, outperforming both classical and graph-specific baselines by up to\\n42.3\\\\% in ECE and reducing calibration variance by 17.24\\\\% on average compared\\nwith graph-specific methods. Moreover, WATS remains computationally efficient,\\nscaling well across graphs of diverse sizes and densities. Code will be\\nreleased based on publication.', 'authors': ['Xiaoyang Li', 'Linwei Tao', 'Haohui Lu', 'Minjing Dong', 'Junbin Gao', 'Chang Xu'], 'published': '2025-06-30T12:23:57+00:00', 'categories': ['cs.LG', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23782v1'}"}, {'id': 35, 'arxiv_id': '2506.23771v1', 'base_arxiv_id': '2506.23771', 'version': '1', 'title': 'Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving', 'abstract': 'Reinforcement Learning (RL) is increasingly used in autonomous driving (AD)\nand shows clear advantages. However, most RL-based AD methods overlook policy\nstructure design. An RL policy that only outputs short-timescale vehicle\ncontrol commands results in fluctuating driving behavior due to fluctuations in\nnetwork outputs, while one that only outputs long-timescale driving goals\ncannot achieve unified optimality of driving behavior and control. Therefore,\nwe propose a multi-timescale hierarchical reinforcement learning approach. Our\napproach adopts a hierarchical policy structure, where high- and low-level RL\npolicies are unified-trained to produce long-timescale motion guidance and\nshort-timescale control commands, respectively. Therein, motion guidance is\nexplicitly represented by hybrid actions to capture multimodal driving\nbehaviors on structured road and support incremental low-level extend-state\nupdates. Additionally, a hierarchical safety mechanism is designed to ensure\nmulti-timescale safety. Evaluation in simulator-based and HighD dataset-based\nhighway multi-lane scenarios demonstrates that our approach significantly\nimproves AD performance, effectively increasing driving efficiency, action\nconsistency and safety.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23771v1', 'title': 'Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving', 'abstract': 'Reinforcement Learning (RL) is increasingly used in autonomous driving (AD)\\nand shows clear advantages. However, most RL-based AD methods overlook policy\\nstructure design. An RL policy that only outputs short-timescale vehicle\\ncontrol commands results in fluctuating driving behavior due to fluctuations in\\nnetwork outputs, while one that only outputs long-timescale driving goals\\ncannot achieve unified optimality of driving behavior and control. Therefore,\\nwe propose a multi-timescale hierarchical reinforcement learning approach. Our\\napproach adopts a hierarchical policy structure, where high- and low-level RL\\npolicies are unified-trained to produce long-timescale motion guidance and\\nshort-timescale control commands, respectively. Therein, motion guidance is\\nexplicitly represented by hybrid actions to capture multimodal driving\\nbehaviors on structured road and support incremental low-level extend-state\\nupdates. Additionally, a hierarchical safety mechanism is designed to ensure\\nmulti-timescale safety. Evaluation in simulator-based and HighD dataset-based\\nhighway multi-lane scenarios demonstrates that our approach significantly\\nimproves AD performance, effectively increasing driving efficiency, action\\nconsistency and safety.', 'authors': ['Guizhe Jin', 'Zhuoren Li', 'Bo Leng', 'Ran Yu', 'Lu Xiong'], 'published': '2025-06-30T12:17:42+00:00', 'categories': ['cs.RO', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23771v1'}"}, {'id': 36, 'arxiv_id': '2506.23762v1', 'base_arxiv_id': '2506.23762', 'version': '1', 'title': 'Software Engineering for Large Language Models: Research Status, Challenges and the Road Ahead', 'abstract': 'The rapid advancement of large language models (LLMs) has redefined\nartificial intelligence (AI), pushing the boundaries of AI research and\nenabling unbounded possibilities for both academia and the industry. However,\nLLM development faces increasingly complex challenges throughout its lifecycle,\nyet no existing research systematically explores these challenges and solutions\nfrom the perspective of software engineering (SE) approaches. To fill the gap,\nwe systematically analyze research status throughout the LLM development\nlifecycle, divided into six phases: requirements engineering, dataset\nconstruction, model development and enhancement, testing and evaluation,\ndeployment and operations, and maintenance and evolution. We then conclude by\nidentifying the key challenges for each phase and presenting potential research\ndirections to address these challenges. In general, we provide valuable\ninsights from an SE perspective to facilitate future advances in LLM\ndevelopment.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23762v1', 'title': 'Software Engineering for Large Language Models: Research Status, Challenges and the Road Ahead', 'abstract': 'The rapid advancement of large language models (LLMs) has redefined\\nartificial intelligence (AI), pushing the boundaries of AI research and\\nenabling unbounded possibilities for both academia and the industry. However,\\nLLM development faces increasingly complex challenges throughout its lifecycle,\\nyet no existing research systematically explores these challenges and solutions\\nfrom the perspective of software engineering (SE) approaches. To fill the gap,\\nwe systematically analyze research status throughout the LLM development\\nlifecycle, divided into six phases: requirements engineering, dataset\\nconstruction, model development and enhancement, testing and evaluation,\\ndeployment and operations, and maintenance and evolution. We then conclude by\\nidentifying the key challenges for each phase and presenting potential research\\ndirections to address these challenges. In general, we provide valuable\\ninsights from an SE perspective to facilitate future advances in LLM\\ndevelopment.', 'authors': ['Hongzhou Rao', 'Yanjie Zhao', 'Xinyi Hou', 'Shenao Wang', 'Haoyu Wang'], 'published': '2025-06-30T12:09:29+00:00', 'categories': ['cs.SE', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23762v1'}"}, {'id': 37, 'arxiv_id': '2506.23735v1', 'base_arxiv_id': '2506.23735', 'version': '1', 'title': 'AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data', 'abstract': 'Large language models (LLMs) have shown remarkable performance on various\ntasks, but existing evaluation benchmarks are often static and insufficient to\nfully assess their robustness and generalization in realistic scenarios. Prior\nwork using evolutionary or adversarial data augmentation has improved\nevaluation diversity but lacks systematic control over perturbation types and\nmulti-step complexity, limiting comprehensive robustness analysis. To address\nthese gaps, we propose AutoEvoEval, an evolution-based evaluation framework for\nclose-ended tasks such as multi-choice question answering. AutoEvoEval\nintroduces 22 interpretable atomic evolution operations and supports\nmulti-round compositions, enabling controlled generation of diverse,\nchallenging, and realistic test samples. We conduct extensive experiments\naddressing four research questions on a broad set of open- and closed-source\nLLMs. Our results show that atomic operations cause an average accuracy drop of\n7.283\\%, with structure-disrupting or misleading semantic edits causing the\nlargest declines. Model sensitivities vary significantly for the same\nperturbation, and combining multiple evolution steps amplifies adversarial\neffects by up to 52.932\\%. These findings suggest current benchmarks may\noverestimate true model generalization and emphasize the need for\nevolution-aware robustness evaluation. Code and resources are available at:\nhttps://github.com/SYSUSELab/AutoEvoEval.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23735v1', 'title': 'AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data', 'abstract': 'Large language models (LLMs) have shown remarkable performance on various\\ntasks, but existing evaluation benchmarks are often static and insufficient to\\nfully assess their robustness and generalization in realistic scenarios. Prior\\nwork using evolutionary or adversarial data augmentation has improved\\nevaluation diversity but lacks systematic control over perturbation types and\\nmulti-step complexity, limiting comprehensive robustness analysis. To address\\nthese gaps, we propose AutoEvoEval, an evolution-based evaluation framework for\\nclose-ended tasks such as multi-choice question answering. AutoEvoEval\\nintroduces 22 interpretable atomic evolution operations and supports\\nmulti-round compositions, enabling controlled generation of diverse,\\nchallenging, and realistic test samples. We conduct extensive experiments\\naddressing four research questions on a broad set of open- and closed-source\\nLLMs. Our results show that atomic operations cause an average accuracy drop of\\n7.283\\\\%, with structure-disrupting or misleading semantic edits causing the\\nlargest declines. Model sensitivities vary significantly for the same\\nperturbation, and combining multiple evolution steps amplifies adversarial\\neffects by up to 52.932\\\\%. These findings suggest current benchmarks may\\noverestimate true model generalization and emphasize the need for\\nevolution-aware robustness evaluation. Code and resources are available at:\\nhttps://github.com/SYSUSELab/AutoEvoEval.', 'authors': ['JiaRu Wu', 'Mingwei Liu'], 'published': '2025-06-30T11:18:56+00:00', 'categories': ['cs.CL', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23735v1'}"}, {'id': 38, 'arxiv_id': '2506.23734v1', 'base_arxiv_id': '2506.23734', 'version': '1', 'title': 'Marker Gene Method : Identifying Stable Solutions in a Dynamic Environment', 'abstract': "Competitive Co-evolutionary Algorithms (CCEAs) are often hampered by complex\ndynamics like intransitivity and the Red Queen effect, leading to unstable\nconvergence. To counter these challenges, this paper introduces the Marker Gene\nMethod (MGM), a framework that establishes stability by using a 'marker gene'\nas a dynamic benchmark and an adaptive weighting mechanism to balance\nexploration and exploitation. We provide rigorous mathematical proofs\ndemonstrating that MGM creates strong attractors near Nash Equilibria within\nthe Strictly Competitive Game framework. Empirically, MGM demonstrates its\nefficacy across a spectrum of challenges: it stabilizes the canonical\nRock-Paper-Scissors game, significantly improves the performance of C-RMOEA/D\non ZDT benchmarks, and, when augmented with a Memory Pool (MP) extension, it\nsuccessfully tames the notoriously pathological Shapley Biased Game. This work\npresents a theoretically sound and empirically validated framework that\nsubstantially enhances the stability and robustness of CCEAs in complex\ncompetitive environments.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23734v1\', \'title\': \'Marker Gene Method : Identifying Stable Solutions in a Dynamic Environment\', \'abstract\': "Competitive Co-evolutionary Algorithms (CCEAs) are often hampered by complex\\ndynamics like intransitivity and the Red Queen effect, leading to unstable\\nconvergence. To counter these challenges, this paper introduces the Marker Gene\\nMethod (MGM), a framework that establishes stability by using a \'marker gene\'\\nas a dynamic benchmark and an adaptive weighting mechanism to balance\\nexploration and exploitation. We provide rigorous mathematical proofs\\ndemonstrating that MGM creates strong attractors near Nash Equilibria within\\nthe Strictly Competitive Game framework. Empirically, MGM demonstrates its\\nefficacy across a spectrum of challenges: it stabilizes the canonical\\nRock-Paper-Scissors game, significantly improves the performance of C-RMOEA/D\\non ZDT benchmarks, and, when augmented with a Memory Pool (MP) extension, it\\nsuccessfully tames the notoriously pathological Shapley Biased Game. This work\\npresents a theoretically sound and empirically validated framework that\\nsubstantially enhances the stability and robustness of CCEAs in complex\\ncompetitive environments.", \'authors\': [\'Hao Shi\', \'Xi Li\', \'Fangfang Xie\'], \'published\': \'2025-06-30T11:13:36+00:00\', \'categories\': [\'cs.NE\', \'cs.AI\', \'cs.GT\'], \'url\': \'http://arxiv.org/abs/2506.23734v1\'}'}, {'id': 39, 'arxiv_id': '2506.23726v1', 'base_arxiv_id': '2506.23726', 'version': '1', 'title': 'System-Embedded Diffusion Bridge Models', 'abstract': 'Solving inverse problems -- recovering signals from incomplete or noisy\nmeasurements -- is fundamental in science and engineering. Score-based\ngenerative models (SGMs) have recently emerged as a powerful framework for this\ntask. Two main paradigms have formed: unsupervised approaches that adapt\npretrained generative models to inverse problems, and supervised bridge methods\nthat train stochastic processes conditioned on paired clean and corrupted data.\nWhile the former typically assume knowledge of the measurement model, the\nlatter have largely overlooked this structural information. We introduce System\nembedded Diffusion Bridge Models (SDBs), a new class of supervised bridge\nmethods that explicitly embed the known linear measurement system into the\ncoefficients of a matrix-valued SDE. This principled integration yields\nconsistent improvements across diverse linear inverse problems and demonstrates\nrobust generalization under system misspecification between training and\ndeployment, offering a promising solution to real-world applications.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23726v1', 'title': 'System-Embedded Diffusion Bridge Models', 'abstract': 'Solving inverse problems -- recovering signals from incomplete or noisy\\nmeasurements -- is fundamental in science and engineering. Score-based\\ngenerative models (SGMs) have recently emerged as a powerful framework for this\\ntask. Two main paradigms have formed: unsupervised approaches that adapt\\npretrained generative models to inverse problems, and supervised bridge methods\\nthat train stochastic processes conditioned on paired clean and corrupted data.\\nWhile the former typically assume knowledge of the measurement model, the\\nlatter have largely overlooked this structural information. We introduce System\\nembedded Diffusion Bridge Models (SDBs), a new class of supervised bridge\\nmethods that explicitly embed the known linear measurement system into the\\ncoefficients of a matrix-valued SDE. This principled integration yields\\nconsistent improvements across diverse linear inverse problems and demonstrates\\nrobust generalization under system misspecification between training and\\ndeployment, offering a promising solution to real-world applications.', 'authors': ['Bartlomiej Sobieski', 'Matthew Tivnan', 'Yuang Wang', 'Siyeop Yoon', 'Pengfei Jin', 'Dufan Wu', 'Quanzheng Li', 'Przemyslaw Biecek'], 'published': '2025-06-30T10:58:49+00:00', 'categories': ['cs.LG', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23726v1'}"}, {'id': 40, 'arxiv_id': '2506.23725v1', 'base_arxiv_id': '2506.23725', 'version': '1', 'title': 'PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?', 'abstract': "Vision-Language Models (VLMs) are increasingly pivotal for generalist robot\nmanipulation, enabling tasks such as physical reasoning, policy generation, and\nfailure detection. However, their proficiency in these high-level applications\noften assumes a deep understanding of low-level physical prerequisites, a\ncapability that remains largely unverified. For robots to perform actions\nreliably, they must comprehend intrinsic object properties (e.g., material,\nweight), action affordances (e.g., graspable, stackable), and physical\nconstraints (e.g., stability, reachability, or an object's state, such as being\nclosed). Despite the widespread use of VLMs in manipulation tasks, we argue\nthat off-the-shelf models may lack this granular, physically grounded\nunderstanding, as such prerequisites are often overlooked during training.\n  To address this critical gap, we introduce PAC Bench, a comprehensive\nbenchmark designed to systematically evaluate VLMs on their understanding of\ncore Properties, Affordances, and Constraints (PAC) from a task executability\nperspective. PAC Bench features a diverse dataset with over 30,000 annotations,\ncomprising 673 real-world images (115 object classes, 15 property types, and 1\nto 3 affordances defined per class), 100 real-world humanoid-view scenarios,\nand 120 unique simulated constraint scenarios across four tasks.\n  Our evaluations reveal significant gaps in the ability of current VLMs to\ngrasp fundamental physical concepts, highlighting limitations in their\nsuitability for reliable robot manipulation and pointing to key areas for\ntargeted research. PAC Bench also serves as a standardized benchmark for\nrigorously evaluating physical reasoning in VLMs and guiding the development of\nmore robust, physically grounded models for robotic applications.\n  Project Page: https://pacbench.github.io/", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23725v1\', \'title\': \'PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?\', \'abstract\': "Vision-Language Models (VLMs) are increasingly pivotal for generalist robot\\nmanipulation, enabling tasks such as physical reasoning, policy generation, and\\nfailure detection. However, their proficiency in these high-level applications\\noften assumes a deep understanding of low-level physical prerequisites, a\\ncapability that remains largely unverified. For robots to perform actions\\nreliably, they must comprehend intrinsic object properties (e.g., material,\\nweight), action affordances (e.g., graspable, stackable), and physical\\nconstraints (e.g., stability, reachability, or an object\'s state, such as being\\nclosed). Despite the widespread use of VLMs in manipulation tasks, we argue\\nthat off-the-shelf models may lack this granular, physically grounded\\nunderstanding, as such prerequisites are often overlooked during training.\\n  To address this critical gap, we introduce PAC Bench, a comprehensive\\nbenchmark designed to systematically evaluate VLMs on their understanding of\\ncore Properties, Affordances, and Constraints (PAC) from a task executability\\nperspective. PAC Bench features a diverse dataset with over 30,000 annotations,\\ncomprising 673 real-world images (115 object classes, 15 property types, and 1\\nto 3 affordances defined per class), 100 real-world humanoid-view scenarios,\\nand 120 unique simulated constraint scenarios across four tasks.\\n  Our evaluations reveal significant gaps in the ability of current VLMs to\\ngrasp fundamental physical concepts, highlighting limitations in their\\nsuitability for reliable robot manipulation and pointing to key areas for\\ntargeted research. PAC Bench also serves as a standardized benchmark for\\nrigorously evaluating physical reasoning in VLMs and guiding the development of\\nmore robust, physically grounded models for robotic applications.\\n  Project Page: https://pacbench.github.io/", \'authors\': [\'Atharva Gundawar\', \'Som Sagar\', \'Ransalu Senanayake\'], \'published\': \'2025-06-30T10:58:36+00:00\', \'categories\': [\'cs.RO\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23725v1\'}'}, {'id': 41, 'arxiv_id': '2506.23724v1', 'base_arxiv_id': '2506.23724', 'version': '1', 'title': 'When Small Guides Large: Cross-Model Co-Learning for Test-Time Adaptation', 'abstract': "Test-time Adaptation (TTA) adapts a given model to testing domain data with\npotential domain shifts through online unsupervised learning, yielding\nimpressive performance. However, to date, existing TTA methods primarily focus\non single-model adaptation. In this work, we investigate an intriguing\nquestion: how does cross-model knowledge influence the TTA process? Our\nfindings reveal that, in TTA's unsupervised online setting, each model can\nprovide complementary, confident knowledge to the others, even when there are\nsubstantial differences in model size. For instance, a smaller model like\nMobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base\n(86.6M parameters). In light of this, we propose COCA, a Cross-Model\nCo-Learning framework for TTA, which mainly consists of two main strategies. 1)\nCo-adaptation adaptively integrates complementary knowledge from other models\nthroughout the TTA process, reducing individual model biases. 2)\nSelf-adaptation enhances each model's unique strengths via unsupervised\nlearning, enabling diverse adaptation to the target domain. Extensive\nexperiments show that COCA, which can also serve as a plug-and-play module,\nsignificantly boosts existing SOTAs, on models with various sizes--including\nResNets, ViTs, and Mobile-ViTs--via cross-model co-learned TTA. For example,\nwith Mobile-ViT's guidance, COCA raises ViT-Base's average adaptation accuracy\non ImageNet-C from 51.7% to 64.5%. The code is publicly available at\nhttps://github.com/ycarobot/COCA.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23724v1\', \'title\': \'When Small Guides Large: Cross-Model Co-Learning for Test-Time Adaptation\', \'abstract\': "Test-time Adaptation (TTA) adapts a given model to testing domain data with\\npotential domain shifts through online unsupervised learning, yielding\\nimpressive performance. However, to date, existing TTA methods primarily focus\\non single-model adaptation. In this work, we investigate an intriguing\\nquestion: how does cross-model knowledge influence the TTA process? Our\\nfindings reveal that, in TTA\'s unsupervised online setting, each model can\\nprovide complementary, confident knowledge to the others, even when there are\\nsubstantial differences in model size. For instance, a smaller model like\\nMobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base\\n(86.6M parameters). In light of this, we propose COCA, a Cross-Model\\nCo-Learning framework for TTA, which mainly consists of two main strategies. 1)\\nCo-adaptation adaptively integrates complementary knowledge from other models\\nthroughout the TTA process, reducing individual model biases. 2)\\nSelf-adaptation enhances each model\'s unique strengths via unsupervised\\nlearning, enabling diverse adaptation to the target domain. Extensive\\nexperiments show that COCA, which can also serve as a plug-and-play module,\\nsignificantly boosts existing SOTAs, on models with various sizes--including\\nResNets, ViTs, and Mobile-ViTs--via cross-model co-learned TTA. For example,\\nwith Mobile-ViT\'s guidance, COCA raises ViT-Base\'s average adaptation accuracy\\non ImageNet-C from 51.7% to 64.5%. The code is publicly available at\\nhttps://github.com/ycarobot/COCA.", \'authors\': ["Chang\'an Yi", \'Xiaohui Deng\', \'Guohao Chen\', \'Yan Zhou\', \'Qinghua Lu\', \'Shuaicheng Niu\'], \'published\': \'2025-06-30T10:54:50+00:00\', \'categories\': [\'cs.CV\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23724v1\'}'}, {'id': 42, 'arxiv_id': '2506.23721v1', 'base_arxiv_id': '2506.23721', 'version': '1', 'title': 'Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound', 'abstract': "Ultrasound (US) is widely accessible and radiation-free but has a steep\nlearning curve due to its dynamic nature and non-standard imaging planes.\nAdditionally, the constant need to shift focus between the US screen and the\npatient poses a challenge. To address these issues, we integrate deep learning\n(DL)-based semantic segmentation for real-time (RT) automated kidney volumetric\nmeasurements, which are essential for clinical assessment but are traditionally\ntime-consuming and prone to fatigue. This automation allows clinicians to\nconcentrate on image interpretation rather than manual measurements.\nComplementing DL, augmented reality (AR) enhances the usability of US by\nprojecting the display directly into the clinician's field of view, improving\nergonomics and reducing the cognitive load associated with screen-to-patient\ntransitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one\nstreams directly via the application programming interface for a wireless\nsetup, while the other supports any US device with video output for broader\naccessibility. We evaluate RT feasibility and accuracy using the Open Kidney\nDataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with\nMedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model\nimplementations, measurement algorithms, and a Wi-Fi-based streaming solution,\nenhancing US training and diagnostics, especially in point-of-care settings.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23721v1\', \'title\': \'Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound\', \'abstract\': "Ultrasound (US) is widely accessible and radiation-free but has a steep\\nlearning curve due to its dynamic nature and non-standard imaging planes.\\nAdditionally, the constant need to shift focus between the US screen and the\\npatient poses a challenge. To address these issues, we integrate deep learning\\n(DL)-based semantic segmentation for real-time (RT) automated kidney volumetric\\nmeasurements, which are essential for clinical assessment but are traditionally\\ntime-consuming and prone to fatigue. This automation allows clinicians to\\nconcentrate on image interpretation rather than manual measurements.\\nComplementing DL, augmented reality (AR) enhances the usability of US by\\nprojecting the display directly into the clinician\'s field of view, improving\\nergonomics and reducing the cognitive load associated with screen-to-patient\\ntransitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one\\nstreams directly via the application programming interface for a wireless\\nsetup, while the other supports any US device with video output for broader\\naccessibility. We evaluate RT feasibility and accuracy using the Open Kidney\\nDataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with\\nMedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model\\nimplementations, measurement algorithms, and a Wi-Fi-based streaming solution,\\nenhancing US training and diagnostics, especially in point-of-care settings.", \'authors\': [\'Gijs Luijten\', \'Roberto Maria Scardigno\', \'Lisle Faray de Paiva\', \'Peter Hoyer\', \'Jens Kleesiek\', \'Domenico Buongiorno\', \'Vitoantonio Bevilacqua\', \'Jan Egger\'], \'published\': \'2025-06-30T10:49:54+00:00\', \'categories\': [\'eess.IV\', \'cs.AI\', \'cs.CV\', \'cs.HC\', \'cs.LG\'], \'url\': \'http://arxiv.org/abs/2506.23721v1\'}'}, {'id': 44, 'arxiv_id': '2506.23717v1', 'base_arxiv_id': '2506.23717', 'version': '1', 'title': 'Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation', 'abstract': "Multi-bit spiking neural networks (SNNs) have recently become a heated\nresearch spot, pursuing energy-efficient and high-accurate AI. However, with\nmore bits involved, the associated memory and computation demands escalate to\nthe point where the performance improvements become disproportionate. Based on\nthe insight that different layers demonstrate different importance and extra\nbits could be wasted and interfering, this paper presents an adaptive bit\nallocation strategy for direct-trained SNNs, achieving fine-grained layer-wise\nallocation of memory and computation resources. Thus, SNN's efficiency and\naccuracy can be improved. Specifically, we parametrize the temporal lengths and\nthe bit widths of weights and spikes, and make them learnable and controllable\nthrough gradients. To address the challenges caused by changeable bit widths\nand temporal lengths, we propose the refined spiking neuron, which can handle\ndifferent temporal lengths, enable the derivation of gradients for temporal\nlengths, and suit spike quantization better. In addition, we theoretically\nformulate the step-size mismatch problem of learnable bit widths, which may\nincur severe quantization errors to SNN, and accordingly propose the step-size\nrenewal mechanism to alleviate this issue. Experiments on various datasets,\nincluding the static CIFAR and ImageNet and the dynamic CIFAR-DVS and\nDVS-GESTURE, demonstrate that our methods can reduce the overall memory and\ncomputation cost while achieving higher accuracy. Particularly, our\nSEWResNet-34 can achieve a 2.69\\% accuracy gain and 4.16$$ lower bit\nbudgets over the advanced baseline work on ImageNet. This work will be fully\nopen-sourced.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23717v1\', \'title\': \'Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation\', \'abstract\': "Multi-bit spiking neural networks (SNNs) have recently become a heated\\nresearch spot, pursuing energy-efficient and high-accurate AI. However, with\\nmore bits involved, the associated memory and computation demands escalate to\\nthe point where the performance improvements become disproportionate. Based on\\nthe insight that different layers demonstrate different importance and extra\\nbits could be wasted and interfering, this paper presents an adaptive bit\\nallocation strategy for direct-trained SNNs, achieving fine-grained layer-wise\\nallocation of memory and computation resources. Thus, SNN\'s efficiency and\\naccuracy can be improved. Specifically, we parametrize the temporal lengths and\\nthe bit widths of weights and spikes, and make them learnable and controllable\\nthrough gradients. To address the challenges caused by changeable bit widths\\nand temporal lengths, we propose the refined spiking neuron, which can handle\\ndifferent temporal lengths, enable the derivation of gradients for temporal\\nlengths, and suit spike quantization better. In addition, we theoretically\\nformulate the step-size mismatch problem of learnable bit widths, which may\\nincur severe quantization errors to SNN, and accordingly propose the step-size\\nrenewal mechanism to alleviate this issue. Experiments on various datasets,\\nincluding the static CIFAR and ImageNet and the dynamic CIFAR-DVS and\\nDVS-GESTURE, demonstrate that our methods can reduce the overall memory and\\ncomputation cost while achieving higher accuracy. Particularly, our\\nSEWResNet-34 can achieve a 2.69\\\\% accuracy gain and 4.16$\\\\times$ lower bit\\nbudgets over the advanced baseline work on ImageNet. This work will be fully\\nopen-sourced.", \'authors\': [\'Xingting Yao\', \'Qinghao Hu\', \'Fei Zhou\', \'Tielong Liu\', \'Gang Li\', \'Peisong Wang\', \'Jian Cheng\'], \'published\': \'2025-06-30T10:45:16+00:00\', \'categories\': [\'cs.NE\', \'cs.AI\', \'cs.CV\', \'cs.LG\'], \'url\': \'http://arxiv.org/abs/2506.23717v1\'}'}, {'id': 45, 'arxiv_id': '2506.23706v1', 'base_arxiv_id': '2506.23706', 'version': '1', 'title': 'Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments', 'abstract': 'Benchmarks are important measures to evaluate safety and compliance of AI\nmodels at scale. However, they typically do not offer verifiable results and\nlack confidentiality for model IP and benchmark datasets. We propose Attestable\nAudits, which run inside Trusted Execution Environments and enable users to\nverify interaction with a compliant AI model. Our work protects sensitive data\neven when model provider and auditor do not trust each other. This addresses\nverification challenges raised in recent AI governance frameworks. We build a\nprototype demonstrating feasibility on typical audit benchmarks against\nLlama-3.1.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23706v1', 'title': 'Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments', 'abstract': 'Benchmarks are important measures to evaluate safety and compliance of AI\\nmodels at scale. However, they typically do not offer verifiable results and\\nlack confidentiality for model IP and benchmark datasets. We propose Attestable\\nAudits, which run inside Trusted Execution Environments and enable users to\\nverify interaction with a compliant AI model. Our work protects sensitive data\\neven when model provider and auditor do not trust each other. This addresses\\nverification challenges raised in recent AI governance frameworks. We build a\\nprototype demonstrating feasibility on typical audit benchmarks against\\nLlama-3.1.', 'authors': ['Christoph Schnabl', 'Daniel Hugenroth', 'Bill Marino', 'Alastair R. Beresford'], 'published': '2025-06-30T10:29:42+00:00', 'categories': ['cs.AI', 'cs.CL', 'cs.CR'], 'url': 'http://arxiv.org/abs/2506.23706v1'}"}, {'id': 46, 'arxiv_id': '2506.23703v1', 'base_arxiv_id': '2506.23703', 'version': '1', 'title': 'A New Perspective On AI Safety Through Control Theory Methodologies', 'abstract': 'While artificial intelligence (AI) is advancing rapidly and mastering\nincreasingly complex problems with astonishing performance, the safety\nassurance of such systems is a major concern. Particularly in the context of\nsafety-critical, real-world cyber-physical systems, AI promises to achieve a\nnew level of autonomy but is hampered by a lack of safety assurance. While\ndata-driven control takes up recent developments in AI to improve control\nsystems, control theory in general could be leveraged to improve AI safety.\nTherefore, this article outlines a new perspective on AI safety based on an\ninterdisciplinary interpretation of the underlying data-generation process and\nthe respective abstraction by AI systems in a system theory-inspired and system\nanalysis-driven manner. In this context, the new perspective, also referred to\nas data control, aims to stimulate AI engineering to take advantage of existing\nsafety analysis and assurance in an interdisciplinary way to drive the paradigm\nof data control. Following a top-down approach, a generic foundation for safety\nanalysis and assurance is outlined at an abstract level that can be refined for\nspecific AI systems and applications and is prepared for future innovation.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23703v1', 'title': 'A New Perspective On AI Safety Through Control Theory Methodologies', 'abstract': 'While artificial intelligence (AI) is advancing rapidly and mastering\\nincreasingly complex problems with astonishing performance, the safety\\nassurance of such systems is a major concern. Particularly in the context of\\nsafety-critical, real-world cyber-physical systems, AI promises to achieve a\\nnew level of autonomy but is hampered by a lack of safety assurance. While\\ndata-driven control takes up recent developments in AI to improve control\\nsystems, control theory in general could be leveraged to improve AI safety.\\nTherefore, this article outlines a new perspective on AI safety based on an\\ninterdisciplinary interpretation of the underlying data-generation process and\\nthe respective abstraction by AI systems in a system theory-inspired and system\\nanalysis-driven manner. In this context, the new perspective, also referred to\\nas data control, aims to stimulate AI engineering to take advantage of existing\\nsafety analysis and assurance in an interdisciplinary way to drive the paradigm\\nof data control. Following a top-down approach, a generic foundation for safety\\nanalysis and assurance is outlined at an abstract level that can be refined for\\nspecific AI systems and applications and is prepared for future innovation.', 'authors': ['Lars Ullrich', 'Walter Zimmer', 'Ross Greer', 'Knut Graichen', 'Alois C. Knoll', 'Mohan Trivedi'], 'published': '2025-06-30T10:26:59+00:00', 'categories': ['cs.AI'], 'url': 'http://arxiv.org/abs/2506.23703v1'}"}, {'id': 47, 'arxiv_id': '2506.23692v1', 'base_arxiv_id': '2506.23692', 'version': '1', 'title': 'Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models', 'abstract': 'While AI for Science (AI4S) serves as an analytical tool in the current\nresearch paradigm, it doesn\'t solve its core inefficiency. We propose "Agent\nfor Science" (Agent4S)-the use of LLM-driven agents to automate the entire\nresearch workflow-as the true Fifth Scientific Paradigm. This paper introduces\na five-level classification for Agent4S, outlining a clear roadmap from simple\ntask automation to fully autonomous, collaborative "AI Scientists." This\nframework defines the next revolutionary step in scientific discovery.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23692v1\', \'title\': \'Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models\', \'abstract\': \'While AI for Science (AI4S) serves as an analytical tool in the current\\nresearch paradigm, it doesn\\\'t solve its core inefficiency. We propose "Agent\\nfor Science" (Agent4S)-the use of LLM-driven agents to automate the entire\\nresearch workflow-as the true Fifth Scientific Paradigm. This paper introduces\\na five-level classification for Agent4S, outlining a clear roadmap from simple\\ntask automation to fully autonomous, collaborative "AI Scientists." This\\nframework defines the next revolutionary step in scientific discovery.\', \'authors\': [\'Boyuan Zheng\', \'Zerui Fang\', \'Zhe Xu\', \'Rui Wang\', \'Yiwen Chen\', \'Cunshi Wang\', \'Mengwei Qu\', \'Lei Lei\', \'Zhen Feng\', \'Yan Liu\', \'Yuyang Li\', \'Mingzhou Tan\', \'Jiaji Wu\', \'Jianwei Shuai\', \'Jia Li\', \'Fangfu Ye\'], \'published\': \'2025-06-30T10:11:39+00:00\', \'categories\': [\'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23692v1\'}'}, {'id': 74, 'arxiv_id': '2506.23508v1', 'base_arxiv_id': '2506.23508', 'version': '1', 'title': 'Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably', 'abstract': "Post-training algorithms such as Supervised Fine-Tuning (SFT) and\nReinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large\nlanguage models to downstream tasks. While effective at task adaptation, their\nimpact on prior knowledge remains unclear. In this paper, we introduce jigsaw\npuzzles as a novel task absent from existing pretraining corpora and\nsystematically study the behavior of SFT and RFT on an open-source multimodal\nmodel, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid\ntask acquisition but leads to catastrophic forgetting, whereas RFT learns more\nslowly on novel tasks but maintains prior knowledge. We analyze this phenomenon\nthrough the lens of learning dynamics, showing that RFT reinforces correct\nsamples that are naturally aligned with the base model's probability landscape,\nmitigating interference with prior knowledge. Moreover, supervised training on\ncorrect RFT-simulated rollouts allows SFT to preserve knowledge while rapidly\nlearning new tasks. These findings suggest that data distribution, rather than\nalgorithmic differences, plays a central role in forgetting, and highlight\nRFT's potential for stable continual learning in multimodal large language\nmodels.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23508v1\', \'title\': \'Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably\', \'abstract\': "Post-training algorithms such as Supervised Fine-Tuning (SFT) and\\nReinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large\\nlanguage models to downstream tasks. While effective at task adaptation, their\\nimpact on prior knowledge remains unclear. In this paper, we introduce jigsaw\\npuzzles as a novel task absent from existing pretraining corpora and\\nsystematically study the behavior of SFT and RFT on an open-source multimodal\\nmodel, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid\\ntask acquisition but leads to catastrophic forgetting, whereas RFT learns more\\nslowly on novel tasks but maintains prior knowledge. We analyze this phenomenon\\nthrough the lens of learning dynamics, showing that RFT reinforces correct\\nsamples that are naturally aligned with the base model\'s probability landscape,\\nmitigating interference with prior knowledge. Moreover, supervised training on\\ncorrect RFT-simulated rollouts allows SFT to preserve knowledge while rapidly\\nlearning new tasks. These findings suggest that data distribution, rather than\\nalgorithmic differences, plays a central role in forgetting, and highlight\\nRFT\'s potential for stable continual learning in multimodal large language\\nmodels.", \'authors\': [\'Zhihao Zhang\', \'Qiaole Dong\', \'Qi Zhang\', \'Jun Zhao\', \'Enyu Zhou\', \'Zhiheng Xi\', \'Senjie Jin\', \'Xiaoran Fan\', \'Yuhao Zhou\', \'Yanwei Fu\', \'Tao Ji\', \'Tao Gui\', \'Xuanjing Huang\'], \'published\': \'2025-06-30T04:15:01+00:00\', \'categories\': [\'cs.CL\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23508v1\'}'}, {'id': 48, 'arxiv_id': '2506.23689v1', 'base_arxiv_id': '2506.23689', 'version': '1', 'title': 'Pok√©AI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red', 'abstract': "We introduce Pok\\'eAI, the first text-based, multi-agent large language model\n(LLM) framework designed to autonomously play and progress through Pok\\'emon\nRed. Our system consists of three specialized agents-Planning, Execution, and\nCritique-each with its own memory bank, role, and skill set. The Planning Agent\nfunctions as the central brain, generating tasks to progress through the game.\nThese tasks are then delegated to the Execution Agent, which carries them out\nwithin the game environment. Upon task completion, the Critique Agent evaluates\nthe outcome to determine whether the objective was successfully achieved. Once\nverification is complete, control returns to the Planning Agent, forming a\nclosed-loop decision-making system.\n  As a preliminary step, we developed a battle module within the Execution\nAgent. Our results show that the battle AI achieves an average win rate of\n80.8% across 50 wild encounters, only 6% lower than the performance of an\nexperienced human player. Furthermore, we find that a model's battle\nperformance correlates strongly with its LLM Arena score on language-related\ntasks, indicating a meaningful link between linguistic ability and strategic\nreasoning. Finally, our analysis of gameplay logs reveals that each LLM\nexhibits a unique playstyle, suggesting that individual models develop distinct\nstrategic behaviors.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23689v1\', \'title\': \'Pok√©AI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red\', \'abstract\': "We introduce Pok\\\\\'eAI, the first text-based, multi-agent large language model\\n(LLM) framework designed to autonomously play and progress through Pok\\\\\'emon\\nRed. Our system consists of three specialized agents-Planning, Execution, and\\nCritique-each with its own memory bank, role, and skill set. The Planning Agent\\nfunctions as the central brain, generating tasks to progress through the game.\\nThese tasks are then delegated to the Execution Agent, which carries them out\\nwithin the game environment. Upon task completion, the Critique Agent evaluates\\nthe outcome to determine whether the objective was successfully achieved. Once\\nverification is complete, control returns to the Planning Agent, forming a\\nclosed-loop decision-making system.\\n  As a preliminary step, we developed a battle module within the Execution\\nAgent. Our results show that the battle AI achieves an average win rate of\\n80.8% across 50 wild encounters, only 6% lower than the performance of an\\nexperienced human player. Furthermore, we find that a model\'s battle\\nperformance correlates strongly with its LLM Arena score on language-related\\ntasks, indicating a meaningful link between linguistic ability and strategic\\nreasoning. Finally, our analysis of gameplay logs reveals that each LLM\\nexhibits a unique playstyle, suggesting that individual models develop distinct\\nstrategic behaviors.", \'authors\': [\'Zihao Liu\', \'Xinhang Sui\', \'Yueran Song\', \'Siwen Wang\'], \'published\': \'2025-06-30T10:09:13+00:00\', \'categories\': [\'cs.AI\', \'cs.MA\'], \'url\': \'http://arxiv.org/abs/2506.23689v1\'}'}, {'id': 49, 'arxiv_id': '2506.23679v1', 'base_arxiv_id': '2506.23679', 'version': '1', 'title': 'Learning Modular Exponentiation with Transformers', 'abstract': 'Modular exponentiation is crucial to number theory and cryptography, yet\nremains largely unexplored from a mechanistic interpretability standpoint. We\ntrain a 4-layer encoder-decoder Transformer model to perform this operation and\ninvestigate the emergence of numerical reasoning during training. Utilizing\nprincipled sampling strategies, PCA-based embedding analysis, and activation\npatching, we examine how number-theoretic properties are encoded within the\nmodel. We find that reciprocal operand training leads to strong performance\ngains, with sudden generalization across related moduli. These synchronized\naccuracy surges reflect grokking-like dynamics, suggesting the model\ninternalizes shared arithmetic structure. We also find a subgraph consisting\nentirely of attention heads in the final layer sufficient to achieve full\nperformance on the task of regular exponentiation. These results suggest that\ntransformer models learn modular arithmetic through specialized computational\ncircuits, paving the way for more interpretable and efficient neural approaches\nto modular exponentiation.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23679v1', 'title': 'Learning Modular Exponentiation with Transformers', 'abstract': 'Modular exponentiation is crucial to number theory and cryptography, yet\\nremains largely unexplored from a mechanistic interpretability standpoint. We\\ntrain a 4-layer encoder-decoder Transformer model to perform this operation and\\ninvestigate the emergence of numerical reasoning during training. Utilizing\\nprincipled sampling strategies, PCA-based embedding analysis, and activation\\npatching, we examine how number-theoretic properties are encoded within the\\nmodel. We find that reciprocal operand training leads to strong performance\\ngains, with sudden generalization across related moduli. These synchronized\\naccuracy surges reflect grokking-like dynamics, suggesting the model\\ninternalizes shared arithmetic structure. We also find a subgraph consisting\\nentirely of attention heads in the final layer sufficient to achieve full\\nperformance on the task of regular exponentiation. These results suggest that\\ntransformer models learn modular arithmetic through specialized computational\\ncircuits, paving the way for more interpretable and efficient neural approaches\\nto modular exponentiation.', 'authors': ['David Demitri Africa', 'Sara M. Kapoor', 'Theo Simon Sorg'], 'published': '2025-06-30T10:00:44+00:00', 'categories': ['cs.LG', 'cs.AI', 'cs.CR'], 'url': 'http://arxiv.org/abs/2506.23679v1'}"}, {'id': 50, 'arxiv_id': '2506.23678v1', 'base_arxiv_id': '2506.23678', 'version': '1', 'title': 'Interactive Reasoning: Visualizing and Controlling Chain-of-Thought Reasoning in Large Language Models', 'abstract': 'The output quality of large language models (LLMs) can be improved via\n"reasoning": generating segments of chain-of-thought (CoT) content to further\ncondition the model prior to producing user-facing output. While these chains\ncontain valuable information, they are verbose and lack explicit organization,\nmaking them tedious to review. Moreover, they lack opportunities for user\nfeedback, such as to remove unwanted considerations, add desired ones, or\nclarify unclear assumptions. We introduce Interactive Reasoning, an interaction\ndesign that visualizes chain-of-thought outputs as a hierarchy of topics and\nenables user review and modification. We implement interactive reasoning in\nHippo, a prototype for AI-assisted decision making in the face of uncertain\ntrade-offs. In a user study with 16 participants, we find that interactive\nreasoning in Hippo allows users to quickly identify and interrupt erroneous\ngenerations, efficiently steer the model towards customized responses, and\nbetter understand both model reasoning and model outputs. Our work contributes\nto a new paradigm that incorporates user oversight into LLM reasoning\nprocesses.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23678v1\', \'title\': \'Interactive Reasoning: Visualizing and Controlling Chain-of-Thought Reasoning in Large Language Models\', \'abstract\': \'The output quality of large language models (LLMs) can be improved via\\n"reasoning": generating segments of chain-of-thought (CoT) content to further\\ncondition the model prior to producing user-facing output. While these chains\\ncontain valuable information, they are verbose and lack explicit organization,\\nmaking them tedious to review. Moreover, they lack opportunities for user\\nfeedback, such as to remove unwanted considerations, add desired ones, or\\nclarify unclear assumptions. We introduce Interactive Reasoning, an interaction\\ndesign that visualizes chain-of-thought outputs as a hierarchy of topics and\\nenables user review and modification. We implement interactive reasoning in\\nHippo, a prototype for AI-assisted decision making in the face of uncertain\\ntrade-offs. In a user study with 16 participants, we find that interactive\\nreasoning in Hippo allows users to quickly identify and interrupt erroneous\\ngenerations, efficiently steer the model towards customized responses, and\\nbetter understand both model reasoning and model outputs. Our work contributes\\nto a new paradigm that incorporates user oversight into LLM reasoning\\nprocesses.\', \'authors\': [\'Rock Yuren Pang\', \'K. J. Kevin Feng\', \'Shangbin Feng\', \'Chu Li\', \'Weijia Shi\', \'Yulia Tsvetkov\', \'Jeffrey Heer\', \'Katharina Reinecke\'], \'published\': \'2025-06-30T10:00:43+00:00\', \'categories\': [\'cs.HC\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23678v1\'}'}, {'id': 51, 'arxiv_id': '2506.23673v1', 'base_arxiv_id': '2506.23673', 'version': '1', 'title': 'HASD: Hierarchical Adaption for pathology Slide-level Domain-shift', 'abstract': 'Domain shift is a critical problem for pathology AI as pathology data is\nheavily influenced by center-specific conditions. Current pathology domain\nadaptation methods focus on image patches rather than WSI, thus failing to\ncapture global WSI features required in typical clinical scenarios. In this\nwork, we address the challenges of slide-level domain shift by proposing a\nHierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD\nachieves multi-scale feature consistency and computationally efficient\nslide-level domain adaptation through two key components: (1) a hierarchical\nadaptation framework that integrates a Domain-level Alignment Solver for\nfeature alignment, a Slide-level Geometric Invariance Regularization to\npreserve the morphological structure, and a Patch-level Attention Consistency\nRegularization to maintain local critical diagnostic cues; and (2) a prototype\nselection mechanism that reduces computational overhead. We validate our method\non two slide-level tasks across five datasets, achieving a 4.1\\% AUROC\nimprovement in a Breast Cancer HER2 Grading cohort and a 3.9\\% C-index gain in\na UCEC survival prediction cohort. Our method provides a practical and reliable\nslide-level domain adaption solution for pathology institutions, minimizing\nboth computational and annotation costs.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23673v1', 'title': 'HASD: Hierarchical Adaption for pathology Slide-level Domain-shift', 'abstract': 'Domain shift is a critical problem for pathology AI as pathology data is\\nheavily influenced by center-specific conditions. Current pathology domain\\nadaptation methods focus on image patches rather than WSI, thus failing to\\ncapture global WSI features required in typical clinical scenarios. In this\\nwork, we address the challenges of slide-level domain shift by proposing a\\nHierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD\\nachieves multi-scale feature consistency and computationally efficient\\nslide-level domain adaptation through two key components: (1) a hierarchical\\nadaptation framework that integrates a Domain-level Alignment Solver for\\nfeature alignment, a Slide-level Geometric Invariance Regularization to\\npreserve the morphological structure, and a Patch-level Attention Consistency\\nRegularization to maintain local critical diagnostic cues; and (2) a prototype\\nselection mechanism that reduces computational overhead. We validate our method\\non two slide-level tasks across five datasets, achieving a 4.1\\\\% AUROC\\nimprovement in a Breast Cancer HER2 Grading cohort and a 3.9\\\\% C-index gain in\\na UCEC survival prediction cohort. Our method provides a practical and reliable\\nslide-level domain adaption solution for pathology institutions, minimizing\\nboth computational and annotation costs.', 'authors': ['Jingsong Liu', 'Han Li', 'Chen Yang', 'Michael Deutges', 'Ario Sadafi', 'Xin You', 'Katharina Breininger', 'Nassir Navab', 'Peter J. Sch√ºffler'], 'published': '2025-06-30T09:52:01+00:00', 'categories': ['cs.AI'], 'url': 'http://arxiv.org/abs/2506.23673v1'}"}, {'id': 52, 'arxiv_id': '2506.23644v1', 'base_arxiv_id': '2506.23644', 'version': '1', 'title': 'QLPro: Automated Code Vulnerability Discovery via LLM and Static Code Analysis Integration', 'abstract': 'We introduce QLPro, a vulnerability detection framework that systematically\nintegrates LLMs and static analysis tools to enable comprehensive vulnerability\ndetection across entire open-source projects.We constructed a new dataset,\nJavaTest, comprising 10 open-source projects from GitHub with 62 confirmed\nvulnerabilities. CodeQL, a state-of-the-art static analysis tool, detected only\n24 of these vulnerabilities while QLPro detected 41. Furthermore, QLPro\ndiscovered 6 previously unknown vulnerabilities, 2 of which have been confirmed\nas 0-days.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23644v1', 'title': 'QLPro: Automated Code Vulnerability Discovery via LLM and Static Code Analysis Integration', 'abstract': 'We introduce QLPro, a vulnerability detection framework that systematically\\nintegrates LLMs and static analysis tools to enable comprehensive vulnerability\\ndetection across entire open-source projects.We constructed a new dataset,\\nJavaTest, comprising 10 open-source projects from GitHub with 62 confirmed\\nvulnerabilities. CodeQL, a state-of-the-art static analysis tool, detected only\\n24 of these vulnerabilities while QLPro detected 41. Furthermore, QLPro\\ndiscovered 6 previously unknown vulnerabilities, 2 of which have been confirmed\\nas 0-days.', 'authors': ['Junze Hu', 'Xiangyu Jin', 'Yizhe Zeng', 'Yuling Liu', 'Yunpeng Li', 'Dan Du', 'Kaiyu Xie', 'Hongsong Zhu'], 'published': '2025-06-30T09:14:49+00:00', 'categories': ['cs.SE', 'cs.AI', 'cs.CR'], 'url': 'http://arxiv.org/abs/2506.23644v1'}"}, {'id': 53, 'arxiv_id': '2506.23641v1', 'base_arxiv_id': '2506.23641', 'version': '1', 'title': 'VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation', 'abstract': 'As the appearance of medical images is influenced by multiple underlying\nfactors, generative models require rich attribute information beyond labels to\nproduce realistic and diverse images. For instance, generating an image of skin\nlesion with specific patterns demands descriptions that go beyond diagnosis,\nsuch as shape, size, texture, and color. However, such detailed descriptions\nare not always accessible. To address this, we explore a framework, termed\nVisual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from\npre-trained Multi-modal Large Language Models (MLLMs) to improve the quality\nand diversity of medical image generation. First, to derive descriptions from\nMLLMs without hallucination, we design a series of prompts following\nChain-of-Thoughts for common medical imaging tasks, including dermatologic,\ncolorectal, and chest X-ray images. Generated descriptions are utilized during\ntraining and stored across different categories. During testing, descriptions\nare randomly retrieved from the corresponding category for inference. Moreover,\nto make the generator robust to unseen combination of descriptions at the test\ntime, we propose a Prototype Condition Mechanism that restricts test embeddings\nto be similar to those from training. Experiments on three common types of\nmedical imaging across four datasets verify the effectiveness of VAP-Diffusion.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23641v1', 'title': 'VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation', 'abstract': 'As the appearance of medical images is influenced by multiple underlying\\nfactors, generative models require rich attribute information beyond labels to\\nproduce realistic and diverse images. For instance, generating an image of skin\\nlesion with specific patterns demands descriptions that go beyond diagnosis,\\nsuch as shape, size, texture, and color. However, such detailed descriptions\\nare not always accessible. To address this, we explore a framework, termed\\nVisual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from\\npre-trained Multi-modal Large Language Models (MLLMs) to improve the quality\\nand diversity of medical image generation. First, to derive descriptions from\\nMLLMs without hallucination, we design a series of prompts following\\nChain-of-Thoughts for common medical imaging tasks, including dermatologic,\\ncolorectal, and chest X-ray images. Generated descriptions are utilized during\\ntraining and stored across different categories. During testing, descriptions\\nare randomly retrieved from the corresponding category for inference. Moreover,\\nto make the generator robust to unseen combination of descriptions at the test\\ntime, we propose a Prototype Condition Mechanism that restricts test embeddings\\nto be similar to those from training. Experiments on three common types of\\nmedical imaging across four datasets verify the effectiveness of VAP-Diffusion.', 'authors': ['Peng Huang', 'Junhu Fu', 'Bowen Guo', 'Zeju Li', 'Yuanyuan Wang', 'Yi Guo'], 'published': '2025-06-30T09:11:19+00:00', 'categories': ['cs.CV', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23641v1'}"}, {'id': 54, 'arxiv_id': '2506.23639v1', 'base_arxiv_id': '2506.23639', 'version': '1', 'title': 'Unified Multimodal Understanding via Byte-Pair Visual Encoding', 'abstract': 'Multimodal large language models (MLLMs) have made significant progress in\nvision-language understanding, yet effectively aligning different modalities\nremains a fundamental challenge. We present a framework that unifies multimodal\nunderstanding by applying byte-pair encoding to visual tokens. Unlike\nconventional approaches that rely on modality-specific encoders, our method\ndirectly incorporates structural information into visual tokens, mirroring\nsuccessful tokenization strategies in text-only language models. We introduce a\npriority-guided encoding scheme that considers both frequency and spatial\nconsistency, coupled with a multi-stage training procedure based on\ncurriculum-driven data composition. These enhancements enable the transformer\nmodel to better capture cross-modal relationships and reason with visual\ninformation. Comprehensive experiments demonstrate improved performance across\ndiverse vision-language tasks. By bridging the gap between visual and textual\nrepresentations, our approach contributes to the advancement of more capable\nand efficient multimodal foundation models.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23639v1', 'title': 'Unified Multimodal Understanding via Byte-Pair Visual Encoding', 'abstract': 'Multimodal large language models (MLLMs) have made significant progress in\\nvision-language understanding, yet effectively aligning different modalities\\nremains a fundamental challenge. We present a framework that unifies multimodal\\nunderstanding by applying byte-pair encoding to visual tokens. Unlike\\nconventional approaches that rely on modality-specific encoders, our method\\ndirectly incorporates structural information into visual tokens, mirroring\\nsuccessful tokenization strategies in text-only language models. We introduce a\\npriority-guided encoding scheme that considers both frequency and spatial\\nconsistency, coupled with a multi-stage training procedure based on\\ncurriculum-driven data composition. These enhancements enable the transformer\\nmodel to better capture cross-modal relationships and reason with visual\\ninformation. Comprehensive experiments demonstrate improved performance across\\ndiverse vision-language tasks. By bridging the gap between visual and textual\\nrepresentations, our approach contributes to the advancement of more capable\\nand efficient multimodal foundation models.', 'authors': ['Wanpeng Zhang', 'Yicheng Feng', 'Hao Luo', 'Yijiang Li', 'Zihao Yue', 'Sipeng Zheng', 'Zongqing Lu'], 'published': '2025-06-30T09:08:08+00:00', 'categories': ['cs.CV', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23639v1'}"}, {'id': 55, 'arxiv_id': '2506.23635v1', 'base_arxiv_id': '2506.23635', 'version': '1', 'title': 'Towards Building Private LLMs: Exploring Multi-Node Expert Parallelism on Apple Silicon for Mixture-of-Experts Large Language Model', 'abstract': "Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI)\nwith significant advancements such as OpenAI's ChatGPT, Meta's Llama, and\nDatabricks' DBRX. This paper addresses the cost and scalability challenges\nencountered when constructing private LLM systems for personal or small group\nservices, as aimed by Apple Intelligence. A Mac Studio cluster with Apple's M2\nUltra chips is established as a cost-efficient solution to host and accelerate\nthe pretrained DBRX model with the Mixture-of-Experts (MoE) architecture. Our\nperformance analysis reveal that parallel execution of the model's experts\nacross two to four machine nodes significantly reduces inference time. We find\nthat computation time for the experts is comparable to the communication time\nfor exchanging their outputs, emphasizing the importance of network latency\nover bandwidth. We also observe significant management overhead due to Apple\nsoftware stack's memory management logic. Based on these findings, we develop\noptimization schemes to eliminate the memory management overhead. As a result,\nthe Mac Studio cluster is 1.15 times more cost-efficient than the\nstate-of-the-art AI supercomputer with NVIDIA H100 GPUs. In addition, we\nconstruct a performance model to estimate system performance under varying\nconfigurations, and the model provides valuable insights for designing private\nLLM systems.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23635v1\', \'title\': \'Towards Building Private LLMs: Exploring Multi-Node Expert Parallelism on Apple Silicon for Mixture-of-Experts Large Language Model\', \'abstract\': "Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI)\\nwith significant advancements such as OpenAI\'s ChatGPT, Meta\'s Llama, and\\nDatabricks\' DBRX. This paper addresses the cost and scalability challenges\\nencountered when constructing private LLM systems for personal or small group\\nservices, as aimed by Apple Intelligence. A Mac Studio cluster with Apple\'s M2\\nUltra chips is established as a cost-efficient solution to host and accelerate\\nthe pretrained DBRX model with the Mixture-of-Experts (MoE) architecture. Our\\nperformance analysis reveal that parallel execution of the model\'s experts\\nacross two to four machine nodes significantly reduces inference time. We find\\nthat computation time for the experts is comparable to the communication time\\nfor exchanging their outputs, emphasizing the importance of network latency\\nover bandwidth. We also observe significant management overhead due to Apple\\nsoftware stack\'s memory management logic. Based on these findings, we develop\\noptimization schemes to eliminate the memory management overhead. As a result,\\nthe Mac Studio cluster is 1.15 times more cost-efficient than the\\nstate-of-the-art AI supercomputer with NVIDIA H100 GPUs. In addition, we\\nconstruct a performance model to estimate system performance under varying\\nconfigurations, and the model provides valuable insights for designing private\\nLLM systems.", \'authors\': [\'Mu-Chi Chen\', \'Po-Hsuan Huang\', \'Xiangrui Ke\', \'Chia-Heng Tu\', \'Chun Jason Xue\', \'Shih-Hao Hung\'], \'published\': \'2025-06-30T09:04:25+00:00\', \'categories\': [\'cs.DC\', \'cs.AI\', \'cs.PF\', \'I.6.4; I.2.7; I.2.11\'], \'url\': \'http://arxiv.org/abs/2506.23635v1\'}'}, {'id': 56, 'arxiv_id': '2506.23634v1', 'base_arxiv_id': '2506.23634', 'version': '1', 'title': 'gMBA: Expression Semantic Guided Mixed Boolean-Arithmetic Deobfuscation Using Transformer Architectures', 'abstract': "Mixed Boolean-Arithmetic (MBA) obfuscation protects intellectual property by\nconverting programs into forms that are more complex to analyze. However, MBA\nhas been increasingly exploited by malware developers to evade detection and\ncause significant real-world problems. Traditional MBA deobfuscation methods\noften consider these expressions as part of a black box and overlook their\ninternal semantic information. To bridge this gap, we propose a truth table,\nwhich is an automatically constructed semantic representation of an\nexpression's behavior that does not rely on external resources. The truth table\nis a mathematical form that represents the output of expression for all\npossible combinations of input. We also propose a general and extensible guided\nMBA deobfuscation framework (gMBA) that modifies a Transformer-based neural\nencoder-decoder Seq2Seq architecture to incorporate this semantic guidance.\nExperimental results and in-depth analysis show that integrating expression\nsemantics significantly improves performance and highlights the importance of\ninternal semantic expressions in recovering obfuscated code to its original\nform.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23634v1\', \'title\': \'gMBA: Expression Semantic Guided Mixed Boolean-Arithmetic Deobfuscation Using Transformer Architectures\', \'abstract\': "Mixed Boolean-Arithmetic (MBA) obfuscation protects intellectual property by\\nconverting programs into forms that are more complex to analyze. However, MBA\\nhas been increasingly exploited by malware developers to evade detection and\\ncause significant real-world problems. Traditional MBA deobfuscation methods\\noften consider these expressions as part of a black box and overlook their\\ninternal semantic information. To bridge this gap, we propose a truth table,\\nwhich is an automatically constructed semantic representation of an\\nexpression\'s behavior that does not rely on external resources. The truth table\\nis a mathematical form that represents the output of expression for all\\npossible combinations of input. We also propose a general and extensible guided\\nMBA deobfuscation framework (gMBA) that modifies a Transformer-based neural\\nencoder-decoder Seq2Seq architecture to incorporate this semantic guidance.\\nExperimental results and in-depth analysis show that integrating expression\\nsemantics significantly improves performance and highlights the importance of\\ninternal semantic expressions in recovering obfuscated code to its original\\nform.", \'authors\': [\'Youjeong Noh\', \'Joon-Young Paik\', \'Jingun Kwon\', \'Eun-Sun Cho\'], \'published\': \'2025-06-30T09:03:13+00:00\', \'categories\': [\'cs.CR\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23634v1\'}'}, {'id': 57, 'arxiv_id': '2506.23629v1', 'base_arxiv_id': '2506.23629', 'version': '1', 'title': 'A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data', 'abstract': 'The integrity of Water Quality Data (WQD) is critical in environmental\nmonitoring for scientific decision-making and ecological protection. However,\nwater quality monitoring systems are often challenged by large amounts of\nmissing data due to unavoidable problems such as sensor failures and\ncommunication delays, which further lead to water quality data becoming\nHigh-Dimensional and Sparse (HDS). Traditional data imputation methods are\ndifficult to depict the potential dynamics and fail to capture the deep data\nfeatures, resulting in unsatisfactory imputation performance. To effectively\naddress the above issues, this paper proposes a Nonlinear Low-rank\nRepresentation model (NLR) with Convolutional Neural Networks (CNN) for\nimputing missing WQD, which utilizes CNNs to implement two ideas: a) fusing\ntemporal features to model the temporal dependence of data between time slots,\nand b) Extracting nonlinear interactions and local patterns to mine\nhigher-order relationships features and achieve deep fusion of multidimensional\ninformation. Experimental studies on three real water quality datasets\ndemonstrate that the proposed model significantly outperforms existing\nstate-of-the-art data imputation models in terms of estimation accuracy. It\nprovides an effective approach for handling water quality monitoring data in\ncomplex dynamic environments.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23629v1', 'title': 'A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data', 'abstract': 'The integrity of Water Quality Data (WQD) is critical in environmental\\nmonitoring for scientific decision-making and ecological protection. However,\\nwater quality monitoring systems are often challenged by large amounts of\\nmissing data due to unavoidable problems such as sensor failures and\\ncommunication delays, which further lead to water quality data becoming\\nHigh-Dimensional and Sparse (HDS). Traditional data imputation methods are\\ndifficult to depict the potential dynamics and fail to capture the deep data\\nfeatures, resulting in unsatisfactory imputation performance. To effectively\\naddress the above issues, this paper proposes a Nonlinear Low-rank\\nRepresentation model (NLR) with Convolutional Neural Networks (CNN) for\\nimputing missing WQD, which utilizes CNNs to implement two ideas: a) fusing\\ntemporal features to model the temporal dependence of data between time slots,\\nand b) Extracting nonlinear interactions and local patterns to mine\\nhigher-order relationships features and achieve deep fusion of multidimensional\\ninformation. Experimental studies on three real water quality datasets\\ndemonstrate that the proposed model significantly outperforms existing\\nstate-of-the-art data imputation models in terms of estimation accuracy. It\\nprovides an effective approach for handling water quality monitoring data in\\ncomplex dynamic environments.', 'authors': ['Xin Liao', 'Bing Yang', 'Cai Yu'], 'published': '2025-06-30T08:48:19+00:00', 'categories': ['cs.LG', 'cs.AI', '68T07(Primary) 62M10, 65C60 (Secondary)', 'I.2.7'], 'url': 'http://arxiv.org/abs/2506.23629v1'}"}, {'id': 59, 'arxiv_id': '2506.23626v1', 'base_arxiv_id': '2506.23626', 'version': '1', 'title': 'Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games', 'abstract': "Reinforcement Learning (RL) in games has gained significant momentum in\nrecent years, enabling the creation of different agent behaviors that can\ntransform a player's gaming experience. However, deploying RL agents in\nproduction environments presents two key challenges: (1) designing an effective\nreward function typically requires an RL expert, and (2) when a game's content\nor mechanics are modified, previously tuned reward weights may no longer be\noptimal. Towards the latter challenge, we propose an automated approach for\niteratively fine-tuning an RL agent's reward function weights, based on a\nuser-defined language based behavioral goal. A Language Model (LM) proposes\nupdated weights at each iteration based on this target behavior and a summary\nof performance statistics from prior training rounds. This closed-loop process\nallows the LM to self-correct and refine its output over time, producing\nincreasingly aligned behavior without the need for manual reward engineering.\nWe evaluate our approach in a racing task and show that it consistently\nimproves agent performance across iterations. The LM-guided agents show a\nsignificant increase in performance from $9\\%$ to $74\\%$ success rate in just\none iteration. We compare our LM-guided tuning against a human expert's manual\nweight design in the racing task: by the final iteration, the LM-tuned agent\nachieved an $80\\%$ success rate, and completed laps in an average of $855$ time\nsteps, a competitive performance against the expert-tuned agent's peak $94\\%$\nsuccess, and $850$ time steps.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23626v1\', \'title\': \'Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games\', \'abstract\': "Reinforcement Learning (RL) in games has gained significant momentum in\\nrecent years, enabling the creation of different agent behaviors that can\\ntransform a player\'s gaming experience. However, deploying RL agents in\\nproduction environments presents two key challenges: (1) designing an effective\\nreward function typically requires an RL expert, and (2) when a game\'s content\\nor mechanics are modified, previously tuned reward weights may no longer be\\noptimal. Towards the latter challenge, we propose an automated approach for\\niteratively fine-tuning an RL agent\'s reward function weights, based on a\\nuser-defined language based behavioral goal. A Language Model (LM) proposes\\nupdated weights at each iteration based on this target behavior and a summary\\nof performance statistics from prior training rounds. This closed-loop process\\nallows the LM to self-correct and refine its output over time, producing\\nincreasingly aligned behavior without the need for manual reward engineering.\\nWe evaluate our approach in a racing task and show that it consistently\\nimproves agent performance across iterations. The LM-guided agents show a\\nsignificant increase in performance from $9\\\\%$ to $74\\\\%$ success rate in just\\none iteration. We compare our LM-guided tuning against a human expert\'s manual\\nweight design in the racing task: by the final iteration, the LM-tuned agent\\nachieved an $80\\\\%$ success rate, and completed laps in an average of $855$ time\\nsteps, a competitive performance against the expert-tuned agent\'s peak $94\\\\%$\\nsuccess, and $850$ time steps.", \'authors\': [\'Ant√≥nio Afonso\', \'Iolanda Leite\', \'Alessandro Sestini\', \'Florian Fuchs\', \'Konrad Tollmar\', \'Linus Gissl√©n\'], \'published\': \'2025-06-30T08:45:04+00:00\', \'categories\': [\'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23626v1\'}'}, {'id': 60, 'arxiv_id': '2506.23605v1', 'base_arxiv_id': '2506.23605', 'version': '1', 'title': 'AI-Generated Lecture Slides for Improving Slide Element Detection and Retrieval', 'abstract': 'Lecture slide element detection and retrieval are key problems in slide\nunderstanding. Training effective models for these tasks often depends on\nextensive manual annotation. However, annotating large volumes of lecture\nslides for supervised training is labor intensive and requires domain\nexpertise. To address this, we propose a large language model (LLM)-guided\nsynthetic lecture slide generation pipeline, SynLecSlideGen, which produces\nhigh-quality, coherent and realistic slides. We also create an evaluation\nbenchmark, namely RealSlide by manually annotating 1,050 real lecture slides.\nTo assess the utility of our synthetic slides, we perform few-shot transfer\nlearning on real data using models pre-trained on them. Experimental results\nshow that few-shot transfer learning with pretraining on synthetic slides\nsignificantly improves performance compared to training only on real data. This\ndemonstrates that synthetic data can effectively compensate for limited labeled\nlecture slides. The code and resources of our work are publicly available on\nour project website: https://synslidegen.github.io/.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23605v1', 'title': 'AI-Generated Lecture Slides for Improving Slide Element Detection and Retrieval', 'abstract': 'Lecture slide element detection and retrieval are key problems in slide\\nunderstanding. Training effective models for these tasks often depends on\\nextensive manual annotation. However, annotating large volumes of lecture\\nslides for supervised training is labor intensive and requires domain\\nexpertise. To address this, we propose a large language model (LLM)-guided\\nsynthetic lecture slide generation pipeline, SynLecSlideGen, which produces\\nhigh-quality, coherent and realistic slides. We also create an evaluation\\nbenchmark, namely RealSlide by manually annotating 1,050 real lecture slides.\\nTo assess the utility of our synthetic slides, we perform few-shot transfer\\nlearning on real data using models pre-trained on them. Experimental results\\nshow that few-shot transfer learning with pretraining on synthetic slides\\nsignificantly improves performance compared to training only on real data. This\\ndemonstrates that synthetic data can effectively compensate for limited labeled\\nlecture slides. The code and resources of our work are publicly available on\\nour project website: https://synslidegen.github.io/.', 'authors': ['Suyash Maniyar', 'Vishvesh Trivedi', 'Ajoy Mondal', 'Anand Mishra', 'C. V. Jawahar'], 'published': '2025-06-30T08:11:31+00:00', 'categories': ['cs.CV', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23605v1'}"}, {'id': 61, 'arxiv_id': '2506.23601v1', 'base_arxiv_id': '2506.23601', 'version': '1', 'title': 'Semantic-guided Diverse Decoding for Large Language Model', 'abstract': 'Diverse decoding of large language models is crucial for applications\nrequiring multiple semantically distinct responses, yet existing methods\nprimarily achieve lexical rather than semantic diversity. This limitation\nsignificantly constrains Best-of-N strategies, group-based reinforcement\nlearning, and data synthesis. While temperature sampling and diverse beam\nsearch modify token distributions or apply n-gram penalties, they fail to\nensure meaningful semantic differentiation. We introduce Semantic-guided\nDiverse Decoding (SemDiD), operating directly in embedding space that balances\nquality with diversity through three complementary mechanisms: orthogonal\ndirectional guidance, dynamic inter-group repulsion, and position-debiased\nprobability assessment. SemDiD harmonizes these competing objectives using\nadaptive gain functions and constraint optimization, ensuring both quality\nthresholds and maximal semantic differentiation. Experiments show SemDiD\nconsistently outperforms existing methods, improving Best-of-N coverage by\n1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15%\nwhile increasing accuracy by up to 2.1%.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23601v1', 'title': 'Semantic-guided Diverse Decoding for Large Language Model', 'abstract': 'Diverse decoding of large language models is crucial for applications\\nrequiring multiple semantically distinct responses, yet existing methods\\nprimarily achieve lexical rather than semantic diversity. This limitation\\nsignificantly constrains Best-of-N strategies, group-based reinforcement\\nlearning, and data synthesis. While temperature sampling and diverse beam\\nsearch modify token distributions or apply n-gram penalties, they fail to\\nensure meaningful semantic differentiation. We introduce Semantic-guided\\nDiverse Decoding (SemDiD), operating directly in embedding space that balances\\nquality with diversity through three complementary mechanisms: orthogonal\\ndirectional guidance, dynamic inter-group repulsion, and position-debiased\\nprobability assessment. SemDiD harmonizes these competing objectives using\\nadaptive gain functions and constraint optimization, ensuring both quality\\nthresholds and maximal semantic differentiation. Experiments show SemDiD\\nconsistently outperforms existing methods, improving Best-of-N coverage by\\n1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15%\\nwhile increasing accuracy by up to 2.1%.', 'authors': ['Weijie Shi', 'Yue Cui', 'Yaguang Wu', 'Jingzhi Fang', 'Shibo Zhang', 'Mengze Li', 'Sirui Han', 'Jia Zhu', 'Jiajie Xu', 'Xiaofang Zhou'], 'published': '2025-06-30T08:06:49+00:00', 'categories': ['cs.CL', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23601v1'}"}, {'id': 62, 'arxiv_id': '2506.23596v1', 'base_arxiv_id': '2506.23596', 'version': '1', 'title': 'When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series', 'abstract': 'Recently, forecasting future abnormal events has emerged as an important\nscenario to tackle real-world necessities. However, the solution of predicting\nspecific future time points when anomalies will occur, known as Anomaly\nPrediction (AP), remains under-explored. Existing methods dealing with time\nseries data fail in AP, focusing only on immediate anomalies or failing to\nprovide precise predictions for future anomalies. To address the AP task, we\npropose a novel framework called Anomaly to Prompt (A2P), comprised of\nAnomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To\nenable the forecasting model to forecast abnormal time points, we adopt a\nstrategy to learn the relationships of anomalies. For the robust detection of\nanomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP)\nthat simulates diverse anomaly patterns using signal adaptive prompt.\nComprehensive experiments on multiple real-world datasets demonstrate the\nsuperiority of A2P over state-of-the-art methods, showcasing its ability to\npredict future anomalies. Our implementation code is available at\nhttps://github.com/KU-VGI/AP.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23596v1', 'title': 'When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series', 'abstract': 'Recently, forecasting future abnormal events has emerged as an important\\nscenario to tackle real-world necessities. However, the solution of predicting\\nspecific future time points when anomalies will occur, known as Anomaly\\nPrediction (AP), remains under-explored. Existing methods dealing with time\\nseries data fail in AP, focusing only on immediate anomalies or failing to\\nprovide precise predictions for future anomalies. To address the AP task, we\\npropose a novel framework called Anomaly to Prompt (A2P), comprised of\\nAnomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To\\nenable the forecasting model to forecast abnormal time points, we adopt a\\nstrategy to learn the relationships of anomalies. For the robust detection of\\nanomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP)\\nthat simulates diverse anomaly patterns using signal adaptive prompt.\\nComprehensive experiments on multiple real-world datasets demonstrate the\\nsuperiority of A2P over state-of-the-art methods, showcasing its ability to\\npredict future anomalies. Our implementation code is available at\\nhttps://github.com/KU-VGI/AP.', 'authors': ['Min-Yeong Park', 'Won-Jeong Lee', 'Seong Tae Kim', 'Gyeong-Moon Park'], 'published': '2025-06-30T08:00:16+00:00', 'categories': ['cs.LG', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23596v1'}"}, {'id': 63, 'arxiv_id': '2506.23589v1', 'base_arxiv_id': '2506.23589', 'version': '1', 'title': 'Transition Matching: Scalable and Flexible Generative Modeling', 'abstract': 'Diffusion and flow matching models have significantly advanced media\ngeneration, yet their design space is well-explored, somewhat limiting further\nimprovements. Concurrently, autoregressive (AR) models, particularly those\ngenerating continuous tokens, have emerged as a promising direction for\nunifying text and media generation. This paper introduces Transition Matching\n(TM), a novel discrete-time, continuous-state generative paradigm that unifies\nand advances both diffusion/flow models and continuous AR generation. TM\ndecomposes complex generation tasks into simpler Markov transitions, allowing\nfor expressive non-deterministic probability transition kernels and arbitrary\nnon-continuous supervision processes, thereby unlocking new flexible design\navenues. We explore these choices through three TM variants: (i) Difference\nTransition Matching (DTM), which generalizes flow matching to discrete-time by\ndirectly learning transition probabilities, yielding state-of-the-art image\nquality and text adherence as well as improved sampling efficiency. (ii)\nAutoregressive Transition Matching (ARTM) and (iii) Full History Transition\nMatching (FHTM) are partially and fully causal models, respectively, that\ngeneralize continuous AR methods. They achieve continuous causal AR generation\nquality comparable to non-causal approaches and potentially enable seamless\nintegration with existing AR text generation techniques. Notably, FHTM is the\nfirst fully causal model to match or surpass the performance of flow-based\nmethods on text-to-image task in continuous domains. We demonstrate these\ncontributions through a rigorous large-scale comparison of TM variants and\nrelevant baselines, maintaining a fixed architecture, training data, and\nhyperparameters.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23589v1', 'title': 'Transition Matching: Scalable and Flexible Generative Modeling', 'abstract': 'Diffusion and flow matching models have significantly advanced media\\ngeneration, yet their design space is well-explored, somewhat limiting further\\nimprovements. Concurrently, autoregressive (AR) models, particularly those\\ngenerating continuous tokens, have emerged as a promising direction for\\nunifying text and media generation. This paper introduces Transition Matching\\n(TM), a novel discrete-time, continuous-state generative paradigm that unifies\\nand advances both diffusion/flow models and continuous AR generation. TM\\ndecomposes complex generation tasks into simpler Markov transitions, allowing\\nfor expressive non-deterministic probability transition kernels and arbitrary\\nnon-continuous supervision processes, thereby unlocking new flexible design\\navenues. We explore these choices through three TM variants: (i) Difference\\nTransition Matching (DTM), which generalizes flow matching to discrete-time by\\ndirectly learning transition probabilities, yielding state-of-the-art image\\nquality and text adherence as well as improved sampling efficiency. (ii)\\nAutoregressive Transition Matching (ARTM) and (iii) Full History Transition\\nMatching (FHTM) are partially and fully causal models, respectively, that\\ngeneralize continuous AR methods. They achieve continuous causal AR generation\\nquality comparable to non-causal approaches and potentially enable seamless\\nintegration with existing AR text generation techniques. Notably, FHTM is the\\nfirst fully causal model to match or surpass the performance of flow-based\\nmethods on text-to-image task in continuous domains. We demonstrate these\\ncontributions through a rigorous large-scale comparison of TM variants and\\nrelevant baselines, maintaining a fixed architecture, training data, and\\nhyperparameters.', 'authors': ['Neta Shaul', 'Uriel Singer', 'Itai Gat', 'Yaron Lipman'], 'published': '2025-06-30T07:51:58+00:00', 'categories': ['cs.LG', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23589v1'}"}, {'id': 64, 'arxiv_id': '2506.23584v1', 'base_arxiv_id': '2506.23584', 'version': '1', 'title': 'A Clinically-Grounded Two-Stage Framework for Renal CT Report Generation', 'abstract': 'Generating radiology reports from CT scans remains a complex task due to the\nnuanced nature of medical imaging and the variability in clinical\ndocumentation. In this study, we propose a two-stage framework for generating\nrenal radiology reports from 2D CT slices. First, we extract structured\nabnormality features using a multi-task learning model trained to identify\nlesion attributes such as location, size, enhancement, and attenuation. These\nextracted features are subsequently combined with the corresponding CT image\nand fed into a fine-tuned vision-language model to generate natural language\nreport sentences aligned with clinical findings. We conduct experiments on a\ncurated dataset of renal CT studies with manually annotated\nsentence-slice-feature triplets and evaluate performance using both\nclassification metrics and natural language generation metrics. Our results\ndemonstrate that the proposed model outperforms random baselines across all\nabnormality types, and the generated reports capture key clinical content with\nreasonable textual accuracy. This exploratory work highlights the feasibility\nof modular, feature-informed report generation for renal imaging. Future\nefforts will focus on extending this pipeline to 3D CT volumes and further\nimproving clinical fidelity in multimodal medical AI systems.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23584v1', 'title': 'A Clinically-Grounded Two-Stage Framework for Renal CT Report Generation', 'abstract': 'Generating radiology reports from CT scans remains a complex task due to the\\nnuanced nature of medical imaging and the variability in clinical\\ndocumentation. In this study, we propose a two-stage framework for generating\\nrenal radiology reports from 2D CT slices. First, we extract structured\\nabnormality features using a multi-task learning model trained to identify\\nlesion attributes such as location, size, enhancement, and attenuation. These\\nextracted features are subsequently combined with the corresponding CT image\\nand fed into a fine-tuned vision-language model to generate natural language\\nreport sentences aligned with clinical findings. We conduct experiments on a\\ncurated dataset of renal CT studies with manually annotated\\nsentence-slice-feature triplets and evaluate performance using both\\nclassification metrics and natural language generation metrics. Our results\\ndemonstrate that the proposed model outperforms random baselines across all\\nabnormality types, and the generated reports capture key clinical content with\\nreasonable textual accuracy. This exploratory work highlights the feasibility\\nof modular, feature-informed report generation for renal imaging. Future\\nefforts will focus on extending this pipeline to 3D CT volumes and further\\nimproving clinical fidelity in multimodal medical AI systems.', 'authors': ['Renjie Liang', 'Zhengkang Fan', 'Jinqian Pan', 'Chenkun Sun', 'Russell Terry', 'Jie Xu'], 'published': '2025-06-30T07:45:02+00:00', 'categories': ['eess.IV', 'cs.AI', 'cs.CV'], 'url': 'http://arxiv.org/abs/2506.23584v1'}"}, {'id': 65, 'arxiv_id': '2506.23581v1', 'base_arxiv_id': '2506.23581', 'version': '1', 'title': 'PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection', 'abstract': 'Object detection plays a crucial role in many security-sensitive\napplications. However, several recent studies have shown that object detectors\ncan be easily fooled by physically realizable attacks, , adversarial patches\nand recent adversarial textures, which pose realistic and urgent threats.\nAdversarial Training (AT) has been recognized as the most effective defense\nagainst adversarial attacks. While AT has been extensively studied in the\n$l_$ attack settings on classification models, AT against physically\nrealizable attacks on object detectors has received limited exploration. Early\nattempts are only performed to defend against adversarial patches, leaving AT\nagainst a wider range of physically realizable attacks under-explored. In this\nwork, we consider defending against various physically realizable attacks with\na unified AT method. We propose PBCAT, a novel Patch-Based Composite\nAdversarial Training strategy. PBCAT optimizes the model by incorporating the\ncombination of small-area gradient-guided adversarial patches and imperceptible\nglobal adversarial perturbations covering the entire image. With these designs,\nPBCAT has the potential to defend against not only adversarial patches but also\nunseen physically realizable attacks such as adversarial textures. Extensive\nexperiments in multiple settings demonstrated that PBCAT significantly improved\nrobustness against various physically realizable attacks over state-of-the-art\ndefense methods. Notably, it improved the detection accuracy by 29.7\\% over\nprevious defense methods under one recent adversarial texture attack.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23581v1', 'title': 'PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection', 'abstract': 'Object detection plays a crucial role in many security-sensitive\\napplications. However, several recent studies have shown that object detectors\\ncan be easily fooled by physically realizable attacks, \\\\eg, adversarial patches\\nand recent adversarial textures, which pose realistic and urgent threats.\\nAdversarial Training (AT) has been recognized as the most effective defense\\nagainst adversarial attacks. While AT has been extensively studied in the\\n$l_\\\\infty$ attack settings on classification models, AT against physically\\nrealizable attacks on object detectors has received limited exploration. Early\\nattempts are only performed to defend against adversarial patches, leaving AT\\nagainst a wider range of physically realizable attacks under-explored. In this\\nwork, we consider defending against various physically realizable attacks with\\na unified AT method. We propose PBCAT, a novel Patch-Based Composite\\nAdversarial Training strategy. PBCAT optimizes the model by incorporating the\\ncombination of small-area gradient-guided adversarial patches and imperceptible\\nglobal adversarial perturbations covering the entire image. With these designs,\\nPBCAT has the potential to defend against not only adversarial patches but also\\nunseen physically realizable attacks such as adversarial textures. Extensive\\nexperiments in multiple settings demonstrated that PBCAT significantly improved\\nrobustness against various physically realizable attacks over state-of-the-art\\ndefense methods. Notably, it improved the detection accuracy by 29.7\\\\% over\\nprevious defense methods under one recent adversarial texture attack.', 'authors': ['Xiao Li', 'Yiming Zhu', 'Yifan Huang', 'Wei Zhang', 'Yingzhe He', 'Jie Shi', 'Xiaolin Hu'], 'published': '2025-06-30T07:36:21+00:00', 'categories': ['cs.CV', 'cs.AI', 'cs.LG'], 'url': 'http://arxiv.org/abs/2506.23581v1'}"}, {'id': 66, 'arxiv_id': '2506.23576v1', 'base_arxiv_id': '2506.23576', 'version': '1', 'title': 'Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models', 'abstract': 'Recent advances in large language models (LLMs) have raised concerns about\njailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper\ninvestigates the use of multi-agent LLM systems as a defence against such\nattacks. We evaluate three jailbreaking strategies, including the original\nAutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the\nAutoDefense framework, we compare single-agent setups with two- and three-agent\nconfigurations. Our results show that multi-agent systems enhance resistance to\njailbreaks, especially by reducing false negatives. However, its effectiveness\nvaries by attack type, and it introduces trade-offs such as increased false\npositives and computational overhead. These findings point to the limitations\nof current automated defences and suggest directions for improving alignment\nrobustness in future LLM systems.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23576v1', 'title': 'Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models', 'abstract': 'Recent advances in large language models (LLMs) have raised concerns about\\njailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper\\ninvestigates the use of multi-agent LLM systems as a defence against such\\nattacks. We evaluate three jailbreaking strategies, including the original\\nAutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the\\nAutoDefense framework, we compare single-agent setups with two- and three-agent\\nconfigurations. Our results show that multi-agent systems enhance resistance to\\njailbreaks, especially by reducing false negatives. However, its effectiveness\\nvaries by attack type, and it introduces trade-offs such as increased false\\npositives and computational overhead. These findings point to the limitations\\nof current automated defences and suggest directions for improving alignment\\nrobustness in future LLM systems.', 'authors': ['Maria Carolina Cornelia Wit', 'Jun Pang'], 'published': '2025-06-30T07:29:07+00:00', 'categories': ['cs.AI'], 'url': 'http://arxiv.org/abs/2506.23576v1'}"}, {'id': 67, 'arxiv_id': '2506.23573v1', 'base_arxiv_id': '2506.23573', 'version': '1', 'title': 'Online Human Action Detection during Escorting', 'abstract': "The deployment of robot assistants in large indoor spaces has seen\nsignificant growth, with escorting tasks becoming a key application. However,\nmost current escorting robots primarily rely on navigation-focused strategies,\nassuming that the person being escorted will follow without issue. In crowded\nenvironments, this assumption often falls short, as individuals may struggle to\nkeep pace, become obstructed, get distracted, or need to stop unexpectedly. As\na result, conventional robotic systems are often unable to provide effective\nescorting services due to their limited understanding of human movement\ndynamics. To address these challenges, an effective escorting robot must\ncontinuously detect and interpret human actions during the escorting process\nand adjust its movement accordingly. However, there is currently no existing\ndataset designed specifically for human action detection in the context of\nescorting. Given that escorting often occurs in crowded environments, where\nother individuals may enter the robot's camera view, the robot also needs to\nidentify the specific human it is escorting (the subject) before predicting\ntheir actions. Since no existing model performs both person re-identification\nand action prediction in real-time, we propose a novel neural network\narchitecture that can accomplish both tasks. This enables the robot to adjust\nits speed dynamically based on the escortee's movements and seamlessly resume\nescorting after any disruption. In comparative evaluations against strong\nbaselines, our system demonstrates superior efficiency and effectiveness,\nshowcasing its potential to significantly improve robotic escorting services in\ncomplex, real-world scenarios.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23573v1\', \'title\': \'Online Human Action Detection during Escorting\', \'abstract\': "The deployment of robot assistants in large indoor spaces has seen\\nsignificant growth, with escorting tasks becoming a key application. However,\\nmost current escorting robots primarily rely on navigation-focused strategies,\\nassuming that the person being escorted will follow without issue. In crowded\\nenvironments, this assumption often falls short, as individuals may struggle to\\nkeep pace, become obstructed, get distracted, or need to stop unexpectedly. As\\na result, conventional robotic systems are often unable to provide effective\\nescorting services due to their limited understanding of human movement\\ndynamics. To address these challenges, an effective escorting robot must\\ncontinuously detect and interpret human actions during the escorting process\\nand adjust its movement accordingly. However, there is currently no existing\\ndataset designed specifically for human action detection in the context of\\nescorting. Given that escorting often occurs in crowded environments, where\\nother individuals may enter the robot\'s camera view, the robot also needs to\\nidentify the specific human it is escorting (the subject) before predicting\\ntheir actions. Since no existing model performs both person re-identification\\nand action prediction in real-time, we propose a novel neural network\\narchitecture that can accomplish both tasks. This enables the robot to adjust\\nits speed dynamically based on the escortee\'s movements and seamlessly resume\\nescorting after any disruption. In comparative evaluations against strong\\nbaselines, our system demonstrates superior efficiency and effectiveness,\\nshowcasing its potential to significantly improve robotic escorting services in\\ncomplex, real-world scenarios.", \'authors\': [\'Siddhartha Mondal\', \'Avik Mitra\', \'Chayan Sarkar\'], \'published\': \'2025-06-30T07:25:31+00:00\', \'categories\': [\'cs.RO\', \'cs.AI\', \'cs.LG\'], \'url\': \'http://arxiv.org/abs/2506.23573v1\'}'}, {'id': 68, 'arxiv_id': '2506.23563v1', 'base_arxiv_id': '2506.23563', 'version': '1', 'title': 'MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI', 'abstract': 'Reasoning plays a crucial role in advancing Multimodal Large Language Models\n(MLLMs) toward Artificial General Intelligence. However, existing MLLM\nbenchmarks often fall short in precisely and comprehensively evaluating\nlong-chain reasoning abilities from three key aspects: (1) lack of difficulty\nand diversity, (2) susceptibility to guessability and memorization, (3)\ninadequate assessment of intermediate reasoning steps. To fill this gap, we\nintroduce MMReason, a new benchmark designed to precisely and comprehensively\nevaluate MLLM long-chain reasoning capability with diverse, open-ended,\nchallenging questions. First, we curate challenging questions requiring\nmulti-step reasoning from various fields (i.e., 6 disciplines) and multiple\ndifficulty levels (i.e., from pre-university to university, and from\nfoundational to competition tiers). Second, these questions are reformulated\ninto an open-ended format and filtered using a multi-model voting technique to\neliminate shortcut cases related to guessing and memorization, ensuring robust\nreasoning evaluations. Third, we annotate the questions with detailed\nstep-by-step solutions, and design a reference-based ternary scoring mechanism\nto reliably assess intermediate reasoning steps. With MMReason, we benchmark\npopular leading MLLMs and provide an in-depth analysis of their reasoning\ncapabilities. We hope MMReason will serve as a valuable resource for advancing\nMLLM reasoning research. Code will be available at\nhttps://github.com/HJYao00/MMReason.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23563v1', 'title': 'MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI', 'abstract': 'Reasoning plays a crucial role in advancing Multimodal Large Language Models\\n(MLLMs) toward Artificial General Intelligence. However, existing MLLM\\nbenchmarks often fall short in precisely and comprehensively evaluating\\nlong-chain reasoning abilities from three key aspects: (1) lack of difficulty\\nand diversity, (2) susceptibility to guessability and memorization, (3)\\ninadequate assessment of intermediate reasoning steps. To fill this gap, we\\nintroduce MMReason, a new benchmark designed to precisely and comprehensively\\nevaluate MLLM long-chain reasoning capability with diverse, open-ended,\\nchallenging questions. First, we curate challenging questions requiring\\nmulti-step reasoning from various fields (i.e., 6 disciplines) and multiple\\ndifficulty levels (i.e., from pre-university to university, and from\\nfoundational to competition tiers). Second, these questions are reformulated\\ninto an open-ended format and filtered using a multi-model voting technique to\\neliminate shortcut cases related to guessing and memorization, ensuring robust\\nreasoning evaluations. Third, we annotate the questions with detailed\\nstep-by-step solutions, and design a reference-based ternary scoring mechanism\\nto reliably assess intermediate reasoning steps. With MMReason, we benchmark\\npopular leading MLLMs and provide an in-depth analysis of their reasoning\\ncapabilities. We hope MMReason will serve as a valuable resource for advancing\\nMLLM reasoning research. Code will be available at\\nhttps://github.com/HJYao00/MMReason.', 'authors': ['Huanjin Yao', 'Jiaxing Huang', 'Yawen Qiu', 'Michael K. Chen', 'Wenzheng Liu', 'Wei Zhang', 'Wenjie Zeng', 'Xikun Zhang', 'Jingyi Zhang', 'Yuxin Song', 'Wenhao Wu', 'Dacheng Tao'], 'published': '2025-06-30T07:14:38+00:00', 'categories': ['cs.AI', 'cs.CL', 'cs.CV'], 'url': 'http://arxiv.org/abs/2506.23563v1'}"}, {'id': 140, 'arxiv_id': '2506.23023v1', 'base_arxiv_id': '2506.23023', 'version': '1', 'title': 'Scenario-Based Hierarchical Reinforcement Learning for Automated Driving Decision Making', 'abstract': 'Developing decision-making algorithms for highly automated driving systems\nremains challenging, since these systems have to operate safely in an open and\ncomplex environments. Reinforcement Learning (RL) approaches can learn\ncomprehensive decision policies directly from experience and already show\npromising results in simple driving tasks. However, current approaches fail to\nachieve generalizability for more complex driving tasks and lack learning\nefficiency. Therefore, we present Scenario-based Automated Driving\nReinforcement Learning (SAD-RL), the first framework that integrates\nReinforcement Learning (RL) of hierarchical policy in a scenario-based\nenvironment. A high-level policy selects maneuver templates that are evaluated\nand executed by a low-level control logic. The scenario-based environment\nallows to control the training experience for the agent and to explicitly\nintroduce challenging, but rate situations into the training process. Our\nexperiments show that an agent trained using the SAD-RL framework can achieve\nsafe behaviour in easy as well as challenging situations efficiently. Our\nablation studies confirmed that both HRL and scenario diversity are essential\nfor achieving these results.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.23023v1', 'title': 'Scenario-Based Hierarchical Reinforcement Learning for Automated Driving Decision Making', 'abstract': 'Developing decision-making algorithms for highly automated driving systems\\nremains challenging, since these systems have to operate safely in an open and\\ncomplex environments. Reinforcement Learning (RL) approaches can learn\\ncomprehensive decision policies directly from experience and already show\\npromising results in simple driving tasks. However, current approaches fail to\\nachieve generalizability for more complex driving tasks and lack learning\\nefficiency. Therefore, we present Scenario-based Automated Driving\\nReinforcement Learning (SAD-RL), the first framework that integrates\\nReinforcement Learning (RL) of hierarchical policy in a scenario-based\\nenvironment. A high-level policy selects maneuver templates that are evaluated\\nand executed by a low-level control logic. The scenario-based environment\\nallows to control the training experience for the agent and to explicitly\\nintroduce challenging, but rate situations into the training process. Our\\nexperiments show that an agent trained using the SAD-RL framework can achieve\\nsafe behaviour in easy as well as challenging situations efficiently. Our\\nablation studies confirmed that both HRL and scenario diversity are essential\\nfor achieving these results.', 'authors': ['M. Youssef Abdelhamid', 'Lennart Vater', 'Zlatan Ajanovic'], 'published': '2025-06-28T21:55:59+00:00', 'categories': ['cs.RO', 'cs.AI', 'cs.LG'], 'url': 'http://arxiv.org/abs/2506.23023v1'}"}, {'id': 69, 'arxiv_id': '2506.23560v1', 'base_arxiv_id': '2506.23560', 'version': '1', 'title': 'Tensor Train Quantum State Tomography using Compressed Sensing', 'abstract': 'Quantum state tomography (QST) is a fundamental technique for estimating the\nstate of a quantum system from measured data and plays a crucial role in\nevaluating the performance of quantum devices. However, standard estimation\nmethods become impractical due to the exponential growth of parameters in the\nstate representation. In this work, we address this challenge by parameterizing\nthe state using a low-rank block tensor train decomposition and demonstrate\nthat our approach is both memory- and computationally efficient. This framework\napplies to a broad class of quantum states that can be well approximated by\nlow-rank decompositions, including pure states, nearly pure states, and ground\nstates of Hamiltonians.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23560v1', 'title': 'Tensor Train Quantum State Tomography using Compressed Sensing', 'abstract': 'Quantum state tomography (QST) is a fundamental technique for estimating the\\nstate of a quantum system from measured data and plays a crucial role in\\nevaluating the performance of quantum devices. However, standard estimation\\nmethods become impractical due to the exponential growth of parameters in the\\nstate representation. In this work, we address this challenge by parameterizing\\nthe state using a low-rank block tensor train decomposition and demonstrate\\nthat our approach is both memory- and computationally efficient. This framework\\napplies to a broad class of quantum states that can be well approximated by\\nlow-rank decompositions, including pure states, nearly pure states, and ground\\nstates of Hamiltonians.', 'authors': ['Shakir Showkat Sofi', 'Charlotte Vermeylen', 'Lieven De Lathauwer'], 'published': '2025-06-30T07:06:50+00:00', 'categories': ['quant-ph', 'cs.AI', 'eess.SP', 'math.OC'], 'url': 'http://arxiv.org/abs/2506.23560v1'}"}, {'id': 70, 'arxiv_id': '2506.23549v1', 'base_arxiv_id': '2506.23549', 'version': '1', 'title': 'CooT: Learning to Coordinate In-Context with Coordination Transformers', 'abstract': 'Effective coordination among artificial agents in dynamic and uncertain\nenvironments remains a significant challenge in multi-agent systems. Existing\napproaches, such as self-play and population-based methods, either generalize\npoorly to unseen partners or require extensive training. To overcome these\nlimitations, we propose Coordination Transformers (CooT), a novel in-context\ncoordination framework that uses recent interaction histories to adapt to\nunseen partners rapidly. Unlike previous approaches that primarily aim to\nincrease the diversity of training partners, CooT explicitly focuses on\nadapting to new partner behaviors by predicting actions aligned with observed\npartner interactions. Trained on interaction trajectories collected from\ndiverse pairs of agents with complementary behaviors, CooT quickly learns\neffective coordination strategies without explicit supervision or fine-tuning.\nEvaluations on the Overcooked benchmark demonstrate that CooT significantly\noutperforms baseline methods in coordination tasks involving previously unseen\npartners. Human evaluations further confirm CooT as the most effective\ncollaborative partner, while extensive ablations highlight its robustness,\nflexibility, and sensitivity to context in multi-agent scenarios.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23549v1', 'title': 'CooT: Learning to Coordinate In-Context with Coordination Transformers', 'abstract': 'Effective coordination among artificial agents in dynamic and uncertain\\nenvironments remains a significant challenge in multi-agent systems. Existing\\napproaches, such as self-play and population-based methods, either generalize\\npoorly to unseen partners or require extensive training. To overcome these\\nlimitations, we propose Coordination Transformers (CooT), a novel in-context\\ncoordination framework that uses recent interaction histories to adapt to\\nunseen partners rapidly. Unlike previous approaches that primarily aim to\\nincrease the diversity of training partners, CooT explicitly focuses on\\nadapting to new partner behaviors by predicting actions aligned with observed\\npartner interactions. Trained on interaction trajectories collected from\\ndiverse pairs of agents with complementary behaviors, CooT quickly learns\\neffective coordination strategies without explicit supervision or fine-tuning.\\nEvaluations on the Overcooked benchmark demonstrate that CooT significantly\\noutperforms baseline methods in coordination tasks involving previously unseen\\npartners. Human evaluations further confirm CooT as the most effective\\ncollaborative partner, while extensive ablations highlight its robustness,\\nflexibility, and sensitivity to context in multi-agent scenarios.', 'authors': ['Huai-Chih Wang', 'Hsiang-Chun Chuang', 'Hsi-Chun Cheng', 'Dai-Jie Wu', 'Shao-Hua Sun'], 'published': '2025-06-30T06:45:39+00:00', 'categories': ['cs.AI', 'cs.HC', 'cs.LG'], 'url': 'http://arxiv.org/abs/2506.23549v1'}"}, {'id': 71, 'arxiv_id': '2506.23538v1', 'base_arxiv_id': '2506.23538', 'version': '1', 'title': 'Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound', 'abstract': 'Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage,\npreterm birth, and an increased risk of pregnancy complications. Compared to\ntraditional 2D ultrasound (US), 3D US can reconstruct the coronal plane,\nproviding a clear visualization of the uterine morphology for assessing CUAs\naccurately. In this paper, we propose an intelligent system for simultaneous\nautomated plane localization and CUA diagnosis. Our highlights are: 1) we\ndevelop a denoising diffusion model with local (plane) and global (volume/text)\nguidance, using an adaptive weighting strategy to optimize attention allocation\nto different conditions; 2) we introduce a reinforcement learning-based\nframework with unsupervised rewards to extract the key slice summary from\nredundant sequences, fully integrating information across multiple planes to\nreduce learning difficulty; 3) we provide text-driven uncertainty modeling for\ncoarse prediction, and leverage it to adjust the classification probability for\noverall performance improvement. Extensive experiments on a large 3D uterine US\ndataset show the efficacy of our method, in terms of plane localization and CUA\ndiagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23538v1', 'title': 'Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound', 'abstract': 'Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage,\\npreterm birth, and an increased risk of pregnancy complications. Compared to\\ntraditional 2D ultrasound (US), 3D US can reconstruct the coronal plane,\\nproviding a clear visualization of the uterine morphology for assessing CUAs\\naccurately. In this paper, we propose an intelligent system for simultaneous\\nautomated plane localization and CUA diagnosis. Our highlights are: 1) we\\ndevelop a denoising diffusion model with local (plane) and global (volume/text)\\nguidance, using an adaptive weighting strategy to optimize attention allocation\\nto different conditions; 2) we introduce a reinforcement learning-based\\nframework with unsupervised rewards to extract the key slice summary from\\nredundant sequences, fully integrating information across multiple planes to\\nreduce learning difficulty; 3) we provide text-driven uncertainty modeling for\\ncoarse prediction, and leverage it to adjust the classification probability for\\noverall performance improvement. Extensive experiments on a large 3D uterine US\\ndataset show the efficacy of our method, in terms of plane localization and CUA\\ndiagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.', 'authors': ['Yuhao Huang', 'Yueyue Xu', 'Haoran Dou', 'Jiaxiao Deng', 'Xin Yang', 'Hongyu Zheng', 'Dong Ni'], 'published': '2025-06-30T06:07:41+00:00', 'categories': ['cs.CV', 'cs.AI', 'cs.LG'], 'url': 'http://arxiv.org/abs/2506.23538v1'}"}, {'id': 72, 'arxiv_id': '2506.23516v1', 'base_arxiv_id': '2506.23516', 'version': '1', 'title': 'FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization', 'abstract': 'Federated learning (FL) often suffers from performance degradation due to key\nchallenges such as data heterogeneity and communication constraints. To address\nthese limitations, we present a novel FL framework called FedWSQ, which\nintegrates weight standardization (WS) and the proposed distribution-aware\nnon-uniform quantization (DANUQ). WS enhances FL performance by filtering out\nbiased components in local updates during training, thereby improving the\nrobustness of the model against data heterogeneity and unstable client\nparticipation. In addition, DANUQ minimizes quantization errors by leveraging\nthe statistical properties of local model updates. As a result, FedWSQ\nsignificantly reduces communication overhead while maintaining superior model\naccuracy. Extensive experiments on FL benchmark datasets demonstrate that\nFedWSQ consistently outperforms existing FL methods across various challenging\nFL settings, including extreme data heterogeneity and ultra-low-bit\ncommunication scenarios.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23516v1', 'title': 'FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization', 'abstract': 'Federated learning (FL) often suffers from performance degradation due to key\\nchallenges such as data heterogeneity and communication constraints. To address\\nthese limitations, we present a novel FL framework called FedWSQ, which\\nintegrates weight standardization (WS) and the proposed distribution-aware\\nnon-uniform quantization (DANUQ). WS enhances FL performance by filtering out\\nbiased components in local updates during training, thereby improving the\\nrobustness of the model against data heterogeneity and unstable client\\nparticipation. In addition, DANUQ minimizes quantization errors by leveraging\\nthe statistical properties of local model updates. As a result, FedWSQ\\nsignificantly reduces communication overhead while maintaining superior model\\naccuracy. Extensive experiments on FL benchmark datasets demonstrate that\\nFedWSQ consistently outperforms existing FL methods across various challenging\\nFL settings, including extreme data heterogeneity and ultra-low-bit\\ncommunication scenarios.', 'authors': ['Seung-Wook Kim', 'Seongyeol Kim', 'Jiah Kim', 'Seowon Ji', 'Se-Ho Lee'], 'published': '2025-06-30T04:46:25+00:00', 'categories': ['cs.LG', 'cs.AI', 'cs.CV'], 'url': 'http://arxiv.org/abs/2506.23516v1'}"}, {'id': 73, 'arxiv_id': '2506.23514v1', 'base_arxiv_id': '2506.23514', 'version': '1', 'title': 'MGPRL: Distributed Multi-Gaussian Processes for Wi-Fi-based Multi-Robot Relative Localization in Large Indoor Environments', 'abstract': 'Relative localization is a crucial capability for multi-robot systems\noperating in GPS-denied environments. Existing approaches for multi-robot\nrelative localization often depend on costly or short-range sensors like\ncameras and LiDARs. Consequently, these approaches face challenges such as high\ncomputational overhead (e.g., map merging) and difficulties in disjoint\nenvironments. To address this limitation, this paper introduces MGPRL, a novel\ndistributed framework for multi-robot relative localization using convex-hull\nof multiple Wi-Fi access points (AP). To accomplish this, we employ\nco-regionalized multi-output Gaussian Processes for efficient Radio Signal\nStrength Indicator (RSSI) field prediction and perform uncertainty-aware\nmulti-AP localization, which is further coupled with weighted convex hull-based\nalignment for robust relative pose estimation. Each robot predicts the RSSI\nfield of the environment by an online scan of APs in its environment, which are\nutilized for position estimation of multiple APs. To perform relative\nlocalization, each robot aligns the convex hull of its predicted AP locations\nwith that of the neighbor robots. This approach is well-suited for devices with\nlimited computational resources and operates solely on widely available Wi-Fi\nRSSI measurements without necessitating any dedicated pre-calibration or\noffline fingerprinting. We rigorously evaluate the performance of the proposed\nMGPRL in ROS simulations and demonstrate it with real-world experiments,\ncomparing it against multiple state-of-the-art approaches. The results showcase\nthat MGPRL outperforms existing methods in terms of localization accuracy and\ncomputational efficiency. Finally, we open source MGPRL as a ROS package\nhttps://github.com/herolab-uga/MGPRL.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23514v1', 'title': 'MGPRL: Distributed Multi-Gaussian Processes for Wi-Fi-based Multi-Robot Relative Localization in Large Indoor Environments', 'abstract': 'Relative localization is a crucial capability for multi-robot systems\\noperating in GPS-denied environments. Existing approaches for multi-robot\\nrelative localization often depend on costly or short-range sensors like\\ncameras and LiDARs. Consequently, these approaches face challenges such as high\\ncomputational overhead (e.g., map merging) and difficulties in disjoint\\nenvironments. To address this limitation, this paper introduces MGPRL, a novel\\ndistributed framework for multi-robot relative localization using convex-hull\\nof multiple Wi-Fi access points (AP). To accomplish this, we employ\\nco-regionalized multi-output Gaussian Processes for efficient Radio Signal\\nStrength Indicator (RSSI) field prediction and perform uncertainty-aware\\nmulti-AP localization, which is further coupled with weighted convex hull-based\\nalignment for robust relative pose estimation. Each robot predicts the RSSI\\nfield of the environment by an online scan of APs in its environment, which are\\nutilized for position estimation of multiple APs. To perform relative\\nlocalization, each robot aligns the convex hull of its predicted AP locations\\nwith that of the neighbor robots. This approach is well-suited for devices with\\nlimited computational resources and operates solely on widely available Wi-Fi\\nRSSI measurements without necessitating any dedicated pre-calibration or\\noffline fingerprinting. We rigorously evaluate the performance of the proposed\\nMGPRL in ROS simulations and demonstrate it with real-world experiments,\\ncomparing it against multiple state-of-the-art approaches. The results showcase\\nthat MGPRL outperforms existing methods in terms of localization accuracy and\\ncomputational efficiency. Finally, we open source MGPRL as a ROS package\\nhttps://github.com/herolab-uga/MGPRL.', 'authors': ['Sai Krishna Ghanta', 'Ramviyas Parasuraman'], 'published': '2025-06-30T04:35:00+00:00', 'categories': ['cs.RO', 'cs.AI', 'cs.MA'], 'url': 'http://arxiv.org/abs/2506.23514v1'}"}, {'id': 75, 'arxiv_id': '2506.23492v1', 'base_arxiv_id': '2506.23492', 'version': '1', 'title': 'Sample Margin-Aware Recalibration of Temperature Scaling', 'abstract': 'Recent advances in deep learning have significantly improved predictive\naccuracy. However, modern neural networks remain systematically overconfident,\nposing risks for deployment in safety-critical scenarios. Current post-hoc\ncalibration methods face a fundamental dilemma: global approaches like\nTemperature Scaling apply uniform adjustments across all samples, introducing\nhigh bias despite computational efficiency, while more expressive methods that\noperate on full logit distributions suffer from high variance due to noisy\nhigh-dimensional inputs and insufficient validation data. To address these\nchallenges, we propose Sample Margin-Aware Recalibration of Temperature\n(SMART), a lightweight, data-efficient recalibration method that precisely\nscales logits based on the margin between the top two logits -- termed the\nlogit gap. Specifically, the logit gap serves as a denoised, scalar signal\ndirectly tied to decision boundary uncertainty, providing a robust indicator\nthat avoids the noise inherent in high-dimensional logit spaces while\npreserving model prediction invariance. Meanwhile, SMART employs a novel\nsoft-binned Expected Calibration Error (SoftECE) objective that balances model\nbias and variance through adaptive binning, enabling stable parameter updates\neven with extremely limited calibration data. Extensive evaluations across\ndiverse datasets and architectures demonstrate that SMART achieves\nstate-of-the-art calibration performance even with substantially fewer\nparameters compared to existing parametric methods, offering a principled,\nrobust, and highly efficient solution for practical uncertainty quantification\nin neural network predictions. The source code is available at:\nhttps://anonymous.4open.science/r/SMART-8B11.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23492v1', 'title': 'Sample Margin-Aware Recalibration of Temperature Scaling', 'abstract': 'Recent advances in deep learning have significantly improved predictive\\naccuracy. However, modern neural networks remain systematically overconfident,\\nposing risks for deployment in safety-critical scenarios. Current post-hoc\\ncalibration methods face a fundamental dilemma: global approaches like\\nTemperature Scaling apply uniform adjustments across all samples, introducing\\nhigh bias despite computational efficiency, while more expressive methods that\\noperate on full logit distributions suffer from high variance due to noisy\\nhigh-dimensional inputs and insufficient validation data. To address these\\nchallenges, we propose Sample Margin-Aware Recalibration of Temperature\\n(SMART), a lightweight, data-efficient recalibration method that precisely\\nscales logits based on the margin between the top two logits -- termed the\\nlogit gap. Specifically, the logit gap serves as a denoised, scalar signal\\ndirectly tied to decision boundary uncertainty, providing a robust indicator\\nthat avoids the noise inherent in high-dimensional logit spaces while\\npreserving model prediction invariance. Meanwhile, SMART employs a novel\\nsoft-binned Expected Calibration Error (SoftECE) objective that balances model\\nbias and variance through adaptive binning, enabling stable parameter updates\\neven with extremely limited calibration data. Extensive evaluations across\\ndiverse datasets and architectures demonstrate that SMART achieves\\nstate-of-the-art calibration performance even with substantially fewer\\nparameters compared to existing parametric methods, offering a principled,\\nrobust, and highly efficient solution for practical uncertainty quantification\\nin neural network predictions. The source code is available at:\\nhttps://anonymous.4open.science/r/SMART-8B11.', 'authors': ['Haolan Guo', 'Linwei Tao', 'Haoyang Luo', 'Minjing Dong', 'Chang Xu'], 'published': '2025-06-30T03:35:05+00:00', 'categories': ['cs.LG', 'cs.AI', 'cs.CV'], 'url': 'http://arxiv.org/abs/2506.23492v1'}"}, {'id': 76, 'arxiv_id': '2506.23491v1', 'base_arxiv_id': '2506.23491', 'version': '1', 'title': 'Qwen-GUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding', 'abstract': "This paper introduces Qwen-GUI-3B, a lightweight Vision-Language Model (VLM)\nspecifically designed for Graphical User Interface grounding tasks, achieving\nperformance competitive with significantly larger models. Unlike large-scale\nVLMs (>7B parameters) that are computationally intensive and impractical for\nconsumer-grade hardware, Qwen-GUI-3B delivers strong grounding accuracy while\nbeing fully trainable on a single GPU (RTX 4090). The model incorporates\nseveral key innovations: (i) combine cross-platform, multi-resolution dataset\nof 24K examples from diverse sources including mobile, desktop, and web GUI\nscreenshots to effectively address data scarcity in high-resolution desktop\nenvironments; (ii) a two-stage fine-tuning strategy, where initial\ncross-platform training establishes robust GUI understanding, followed by\nspecialized fine-tuning on high-resolution data to significantly enhance model\nadaptability; and (iii) data curation and redundancy reduction strategies,\ndemonstrating that randomly sampling a smaller subset with reduced redundancy\nachieves performance comparable to larger datasets, emphasizing data diversity\nover sheer volume. Empirical evaluation on standard GUI grounding\nbenchmarks-including ScreenSpot, ScreenSpot-v2, and the challenging\nScreenSpot-Pro, highlights Qwen-GUI-3B's exceptional accuracy, achieving 84.9%\non ScreenSpot and 86.4% on ScreenSpot-v2, surpassing prior models under 4B\nparameters. Ablation studies validate the critical role of balanced sampling\nand two-stage fine-tuning in enhancing robustness, particularly in\nhigh-resolution desktop scenarios. The Qwen-GUI-3B is available at:\nhttps://github.com/Han1018/Qwen-GUI-3B", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23491v1\', \'title\': \'Qwen-GUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding\', \'abstract\': "This paper introduces Qwen-GUI-3B, a lightweight Vision-Language Model (VLM)\\nspecifically designed for Graphical User Interface grounding tasks, achieving\\nperformance competitive with significantly larger models. Unlike large-scale\\nVLMs (>7B parameters) that are computationally intensive and impractical for\\nconsumer-grade hardware, Qwen-GUI-3B delivers strong grounding accuracy while\\nbeing fully trainable on a single GPU (RTX 4090). The model incorporates\\nseveral key innovations: (i) combine cross-platform, multi-resolution dataset\\nof 24K examples from diverse sources including mobile, desktop, and web GUI\\nscreenshots to effectively address data scarcity in high-resolution desktop\\nenvironments; (ii) a two-stage fine-tuning strategy, where initial\\ncross-platform training establishes robust GUI understanding, followed by\\nspecialized fine-tuning on high-resolution data to significantly enhance model\\nadaptability; and (iii) data curation and redundancy reduction strategies,\\ndemonstrating that randomly sampling a smaller subset with reduced redundancy\\nachieves performance comparable to larger datasets, emphasizing data diversity\\nover sheer volume. Empirical evaluation on standard GUI grounding\\nbenchmarks-including ScreenSpot, ScreenSpot-v2, and the challenging\\nScreenSpot-Pro, highlights Qwen-GUI-3B\'s exceptional accuracy, achieving 84.9%\\non ScreenSpot and 86.4% on ScreenSpot-v2, surpassing prior models under 4B\\nparameters. Ablation studies validate the critical role of balanced sampling\\nand two-stage fine-tuning in enhancing robustness, particularly in\\nhigh-resolution desktop scenarios. The Qwen-GUI-3B is available at:\\nhttps://github.com/Han1018/Qwen-GUI-3B", \'authors\': [\'ZongHan Hsieh\', \'Tzer-Jen Wei\'], \'published\': \'2025-06-30T03:33:02+00:00\', \'categories\': [\'cs.CV\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23491v1\'}'}, {'id': 77, 'arxiv_id': '2506.23490v1', 'base_arxiv_id': '2506.23490', 'version': '1', 'title': 'UltraTwin: Towards Cardiac Anatomical Twin Generation from Multi-view 2D Ultrasound', 'abstract': 'Echocardiography is routine for cardiac examination. However, 2D ultrasound\n(US) struggles with accurate metric calculation and direct observation of 3D\ncardiac structures. Moreover, 3D US is limited by low resolution, small field\nof view and scarce availability in practice. Constructing the cardiac\nanatomical twin from 2D images is promising to provide precise treatment\nplanning and clinical quantification. However, it remains challenging due to\nthe rare paired data, complex structures, and US noises. In this study, we\nintroduce a novel generative framework UltraTwin, to obtain cardiac anatomical\ntwin from sparse multi-view 2D US. Our contribution is three-fold. First,\npioneered the construction of a real-world and high-quality dataset containing\nstrictly paired multi-view 2D US and CT, and pseudo-paired data. Second, we\npropose a coarse-to-fine scheme to achieve hierarchical reconstruction\noptimization. Last, we introduce an implicit autoencoder for topology-aware\nconstraints. Extensive experiments show that UltraTwin reconstructs\nhigh-quality anatomical twins versus strong competitors. We believe it advances\nanatomical twin modeling for potential applications in personalized cardiac\ncare.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23490v1', 'title': 'UltraTwin: Towards Cardiac Anatomical Twin Generation from Multi-view 2D Ultrasound', 'abstract': 'Echocardiography is routine for cardiac examination. However, 2D ultrasound\\n(US) struggles with accurate metric calculation and direct observation of 3D\\ncardiac structures. Moreover, 3D US is limited by low resolution, small field\\nof view and scarce availability in practice. Constructing the cardiac\\nanatomical twin from 2D images is promising to provide precise treatment\\nplanning and clinical quantification. However, it remains challenging due to\\nthe rare paired data, complex structures, and US noises. In this study, we\\nintroduce a novel generative framework UltraTwin, to obtain cardiac anatomical\\ntwin from sparse multi-view 2D US. Our contribution is three-fold. First,\\npioneered the construction of a real-world and high-quality dataset containing\\nstrictly paired multi-view 2D US and CT, and pseudo-paired data. Second, we\\npropose a coarse-to-fine scheme to achieve hierarchical reconstruction\\noptimization. Last, we introduce an implicit autoencoder for topology-aware\\nconstraints. Extensive experiments show that UltraTwin reconstructs\\nhigh-quality anatomical twins versus strong competitors. We believe it advances\\nanatomical twin modeling for potential applications in personalized cardiac\\ncare.', 'authors': ['Junxuan Yu', 'Yaofei Duan', 'Yuhao Huang', 'Yu Wang', 'Rongbo Ling', 'Weihao Luo', 'Ang Zhang', 'Jingxian Xu', 'Qiongying Ni', 'Yongsong Zhou', 'Binghan Li', 'Haoran Dou', 'Liping Liu', 'Yanfen Chu', 'Feng Geng', 'Zhe Sheng', 'Zhifeng Ding', 'Dingxin Zhang', 'Rui Huang', 'Yuhang Zhang', 'Xiaowei Xu', 'Tao Tan', 'Dong Ni', 'Zhongshan Gou', 'Xin Yang'], 'published': '2025-06-30T03:27:42+00:00', 'categories': ['eess.IV', 'cs.AI', 'cs.CV'], 'url': 'http://arxiv.org/abs/2506.23490v1'}"}, {'id': 78, 'arxiv_id': '2506.23485v1', 'base_arxiv_id': '2506.23485', 'version': '1', 'title': 'Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent', 'abstract': "Interactive recommendation is a typical information-seeking task that allows\nusers to interactively express their needs through natural language and obtain\npersonalized recommendations. Large language model-powered (LLM-powered) agents\nhave become a new paradigm in interactive recommendations, effectively\ncapturing users' real-time needs and enhancing personalized experiences.\nHowever, due to limited planning and generalization capabilities, existing\nformulations of LLM-powered interactive recommender agents struggle to\neffectively address diverse and complex user intents, such as intuitive,\nunrefined, or occasionally ambiguous requests. To tackle this challenge, we\npropose a novel thought-augmented interactive recommender agent system (TAIRA)\nthat addresses complex user intents through distilled thought patterns.\nSpecifically, TAIRA is designed as an LLM-powered multi-agent system featuring\na manager agent that orchestrates recommendation tasks by decomposing user\nneeds and planning subtasks, with its planning capacity strengthened through\nThought Pattern Distillation (TPD), a thought-augmentation method that extracts\nhigh-level thoughts from the agent's and human experts' experiences. Moreover,\nwe designed a set of user simulation schemes to generate personalized queries\nof different difficulties and evaluate the recommendations based on specific\ndatasets. Through comprehensive experiments conducted across multiple datasets,\nTAIRA exhibits significantly enhanced performance compared to existing methods.\nNotably, TAIRA shows a greater advantage on more challenging tasks while\ngeneralizing effectively on novel tasks, further validating its superiority in\nmanaging complex user intents within interactive recommendation systems. The\ncode is publicly available at:https://github.com/Alcein/TAIRA.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23485v1\', \'title\': \'Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent\', \'abstract\': "Interactive recommendation is a typical information-seeking task that allows\\nusers to interactively express their needs through natural language and obtain\\npersonalized recommendations. Large language model-powered (LLM-powered) agents\\nhave become a new paradigm in interactive recommendations, effectively\\ncapturing users\' real-time needs and enhancing personalized experiences.\\nHowever, due to limited planning and generalization capabilities, existing\\nformulations of LLM-powered interactive recommender agents struggle to\\neffectively address diverse and complex user intents, such as intuitive,\\nunrefined, or occasionally ambiguous requests. To tackle this challenge, we\\npropose a novel thought-augmented interactive recommender agent system (TAIRA)\\nthat addresses complex user intents through distilled thought patterns.\\nSpecifically, TAIRA is designed as an LLM-powered multi-agent system featuring\\na manager agent that orchestrates recommendation tasks by decomposing user\\nneeds and planning subtasks, with its planning capacity strengthened through\\nThought Pattern Distillation (TPD), a thought-augmentation method that extracts\\nhigh-level thoughts from the agent\'s and human experts\' experiences. Moreover,\\nwe designed a set of user simulation schemes to generate personalized queries\\nof different difficulties and evaluate the recommendations based on specific\\ndatasets. Through comprehensive experiments conducted across multiple datasets,\\nTAIRA exhibits significantly enhanced performance compared to existing methods.\\nNotably, TAIRA shows a greater advantage on more challenging tasks while\\ngeneralizing effectively on novel tasks, further validating its superiority in\\nmanaging complex user intents within interactive recommendation systems. The\\ncode is publicly available at:https://github.com/Alcein/TAIRA.", \'authors\': [\'Haocheng Yu\', \'Yaxiong Wu\', \'Hao Wang\', \'Wei Guo\', \'Yong Liu\', \'Yawen Li\', \'Yuyang Ye\', \'Junping Du\', \'Enhong Chen\'], \'published\': \'2025-06-30T03:15:50+00:00\', \'categories\': [\'cs.CL\', \'cs.AI\', \'cs.IR\'], \'url\': \'http://arxiv.org/abs/2506.23485v1\'}'}, {'id': 79, 'arxiv_id': '2506.23465v1', 'base_arxiv_id': '2506.23465', 'version': '1', 'title': 'Sanitizing Manufacturing Dataset Labels Using Vision-Language Models', 'abstract': "The success of machine learning models in industrial applications is heavily\ndependent on the quality of the datasets used to train the models. However,\nlarge-scale datasets, specially those constructed from crowd-sourcing and\nweb-scraping, often suffer from label noise, inconsistencies, and errors. This\nproblem is particularly pronounced in manufacturing domains, where obtaining\nhigh-quality labels is costly and time-consuming. This paper introduces\nVision-Language Sanitization and Refinement (VLSR), which is a\nvision-language-based framework for label sanitization and refinement in\nmulti-label manufacturing image datasets. This method embeds both images and\ntheir associated textual labels into a shared semantic space leveraging the\nCLIP vision-language model. Then two key tasks are addressed in this process by\ncomputing the cosine similarity between embeddings. First, label sanitization\nis performed to identify irrelevant, misspelled, or semantically weak labels,\nand surface the most semantically aligned label for each image by comparing\nimage-label pairs using cosine similarity between image and label embeddings.\nSecond, the method applies density-based clustering on text embeddings,\nfollowed by iterative cluster merging, to group semantically similar labels\ninto unified label groups. The Factorynet dataset, which includes noisy labels\nfrom both human annotations and web-scraped sources, is employed to evaluate\nthe effectiveness of the proposed framework. Experimental results demonstrate\nthat the VLSR framework successfully identifies problematic labels and improves\nlabel consistency. This method enables a significant reduction in label\nvocabulary through clustering, which ultimately enhances the dataset's quality\nfor training robust machine learning models in industrial applications with\nminimal human intervention.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23465v1\', \'title\': \'Sanitizing Manufacturing Dataset Labels Using Vision-Language Models\', \'abstract\': "The success of machine learning models in industrial applications is heavily\\ndependent on the quality of the datasets used to train the models. However,\\nlarge-scale datasets, specially those constructed from crowd-sourcing and\\nweb-scraping, often suffer from label noise, inconsistencies, and errors. This\\nproblem is particularly pronounced in manufacturing domains, where obtaining\\nhigh-quality labels is costly and time-consuming. This paper introduces\\nVision-Language Sanitization and Refinement (VLSR), which is a\\nvision-language-based framework for label sanitization and refinement in\\nmulti-label manufacturing image datasets. This method embeds both images and\\ntheir associated textual labels into a shared semantic space leveraging the\\nCLIP vision-language model. Then two key tasks are addressed in this process by\\ncomputing the cosine similarity between embeddings. First, label sanitization\\nis performed to identify irrelevant, misspelled, or semantically weak labels,\\nand surface the most semantically aligned label for each image by comparing\\nimage-label pairs using cosine similarity between image and label embeddings.\\nSecond, the method applies density-based clustering on text embeddings,\\nfollowed by iterative cluster merging, to group semantically similar labels\\ninto unified label groups. The Factorynet dataset, which includes noisy labels\\nfrom both human annotations and web-scraped sources, is employed to evaluate\\nthe effectiveness of the proposed framework. Experimental results demonstrate\\nthat the VLSR framework successfully identifies problematic labels and improves\\nlabel consistency. This method enables a significant reduction in label\\nvocabulary through clustering, which ultimately enhances the dataset\'s quality\\nfor training robust machine learning models in industrial applications with\\nminimal human intervention.", \'authors\': [\'Nazanin Mahjourian\', \'Vinh Nguyen\'], \'published\': \'2025-06-30T02:13:09+00:00\', \'categories\': [\'cs.CV\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23465v1\'}'}, {'id': 80, 'arxiv_id': '2506.23464v1', 'base_arxiv_id': '2506.23464', 'version': '1', 'title': "The Confidence Paradox: Can LLM Know When It's Wrong", 'abstract': 'Document Visual Question Answering (DocVQA) systems are increasingly deployed\nin real world applications, yet they remain ethically opaque-often producing\noverconfident answers to ambiguous questions or failing to communicate\nuncertainty in a trustworthy manner. This misalignment between model confidence\nand actual knowledge poses significant risks, particularly in domains requiring\nethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT\nhave advanced SOTA performance by focusing on architectural sophistication and\naccuracy; however, they fall short in ethical responsiveness.\n  To address these limitations, we introduce HonestVQA, a self-supervised\nhonesty calibration framework for ethically aligned DocVQA. Our model-agnostic\nmethod quantifies uncertainty to identify knowledge gaps, aligns model\nconfidence with actual correctness using weighted loss functions, and enforces\nethical response behavior via contrastive learning. We further introduce two\nprincipled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence\nIndex (ECI)--to benchmark alignment between confidence, accuracy, and ethical\ncommunication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3%\nand F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces\noverconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In\ncross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score,\ndemonstrating strong generalization. Ablation shows a 3.8% drop in accuracy\nwithout alignment or contrastive loss.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23464v1\', \'title\': "The Confidence Paradox: Can LLM Know When It\'s Wrong", \'abstract\': \'Document Visual Question Answering (DocVQA) systems are increasingly deployed\\nin real world applications, yet they remain ethically opaque-often producing\\noverconfident answers to ambiguous questions or failing to communicate\\nuncertainty in a trustworthy manner. This misalignment between model confidence\\nand actual knowledge poses significant risks, particularly in domains requiring\\nethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT\\nhave advanced SOTA performance by focusing on architectural sophistication and\\naccuracy; however, they fall short in ethical responsiveness.\\n  To address these limitations, we introduce HonestVQA, a self-supervised\\nhonesty calibration framework for ethically aligned DocVQA. Our model-agnostic\\nmethod quantifies uncertainty to identify knowledge gaps, aligns model\\nconfidence with actual correctness using weighted loss functions, and enforces\\nethical response behavior via contrastive learning. We further introduce two\\nprincipled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence\\nIndex (ECI)--to benchmark alignment between confidence, accuracy, and ethical\\ncommunication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3%\\nand F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces\\noverconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In\\ncross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score,\\ndemonstrating strong generalization. Ablation shows a 3.8% drop in accuracy\\nwithout alignment or contrastive loss.\', \'authors\': [\'Sahil Tripathi\', \'Md Tabrez Nafis\', \'Imran Hussain\', \'Jiechao Gao\'], \'published\': \'2025-06-30T02:06:54+00:00\', \'categories\': [\'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23464v1\'}'}, {'id': 81, 'arxiv_id': '2506.23462v1', 'base_arxiv_id': '2506.23462', 'version': '1', 'title': 'Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for Multimodal Disaster Classification', 'abstract': 'Effective disaster management requires timely and accurate insights, yet\ntraditional methods struggle to integrate multimodal data such as images,\nweather records, and textual reports. To address this, we propose\nDisasterNet-LLM, a specialized Large Language Model (LLM) designed for\ncomprehensive disaster analysis. By leveraging advanced pretraining,\ncross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM\nexcels in disaster classification. Experimental results demonstrate its\nsuperiority over state-of-the-art models, achieving higher accuracy of 89.5%,\nan F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal\ndisaster classification tasks.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23462v1', 'title': 'Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for Multimodal Disaster Classification', 'abstract': 'Effective disaster management requires timely and accurate insights, yet\\ntraditional methods struggle to integrate multimodal data such as images,\\nweather records, and textual reports. To address this, we propose\\nDisasterNet-LLM, a specialized Large Language Model (LLM) designed for\\ncomprehensive disaster analysis. By leveraging advanced pretraining,\\ncross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM\\nexcels in disaster classification. Experimental results demonstrate its\\nsuperiority over state-of-the-art models, achieving higher accuracy of 89.5%,\\nan F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal\\ndisaster classification tasks.', 'authors': ['Manaswi Kulahara', 'Gautam Siddharth Kashyap', 'Nipun Joshi', 'Arpita Soni'], 'published': '2025-06-30T01:56:05+00:00', 'categories': ['cs.LG', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23462v1'}"}, {'id': 82, 'arxiv_id': '2506.23461v1', 'base_arxiv_id': '2506.23461', 'version': '1', 'title': 'Time-variant Image Inpainting via Interactive Distribution Transition Estimation', 'abstract': "In this work, we focus on a novel and practical task, i.e., Time-vAriant\niMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image\nby leveraging the complementary information from a reference image, where both\nimages captured the same scene but with a significant time gap in between,\ni.e., time-variant images. Different from conventional reference-guided image\ninpainting, the reference image under TAMP setup presents significant content\ndistinction to the target image and potentially also suffers from damages. Such\nan application frequently happens in our daily lives to restore a damaged image\nby referring to another reference image, where there is no guarantee of the\nreference image's source and quality. In particular, our study finds that even\nstate-of-the-art (SOTA) reference-guided image inpainting methods fail to\nachieve plausible results due to the chaotic image complementation. To address\nsuch an ill-posed problem, we propose a novel Interactive Distribution\nTransition Estimation (InDiTE) module which interactively complements the\ntime-variant images with adaptive semantics thus facilitate the restoration of\ndamaged regions. To further boost the performance, we propose our TAMP\nsolution, namely Interactive Distribution Transition Estimation-driven\nDiffusion (InDiTE-Diff), which integrates InDiTE with SOTA diffusion model and\nconducts latent cross-reference during sampling. Moreover, considering the lack\nof benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street,\nbased on existing image and mask datasets. We conduct experiments on the\nTAMP-Street datasets under two different time-variant image inpainting\nsettings, which show our method consistently outperform SOTA reference-guided\nimage inpainting methods for solving TAMP.", 'submit_date': datetime.date(2025, 6, 30), 'metadata': '{\'id\': \'2506.23461v1\', \'title\': \'Time-variant Image Inpainting via Interactive Distribution Transition Estimation\', \'abstract\': "In this work, we focus on a novel and practical task, i.e., Time-vAriant\\niMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image\\nby leveraging the complementary information from a reference image, where both\\nimages captured the same scene but with a significant time gap in between,\\ni.e., time-variant images. Different from conventional reference-guided image\\ninpainting, the reference image under TAMP setup presents significant content\\ndistinction to the target image and potentially also suffers from damages. Such\\nan application frequently happens in our daily lives to restore a damaged image\\nby referring to another reference image, where there is no guarantee of the\\nreference image\'s source and quality. In particular, our study finds that even\\nstate-of-the-art (SOTA) reference-guided image inpainting methods fail to\\nachieve plausible results due to the chaotic image complementation. To address\\nsuch an ill-posed problem, we propose a novel Interactive Distribution\\nTransition Estimation (InDiTE) module which interactively complements the\\ntime-variant images with adaptive semantics thus facilitate the restoration of\\ndamaged regions. To further boost the performance, we propose our TAMP\\nsolution, namely Interactive Distribution Transition Estimation-driven\\nDiffusion (InDiTE-Diff), which integrates InDiTE with SOTA diffusion model and\\nconducts latent cross-reference during sampling. Moreover, considering the lack\\nof benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street,\\nbased on existing image and mask datasets. We conduct experiments on the\\nTAMP-Street datasets under two different time-variant image inpainting\\nsettings, which show our method consistently outperform SOTA reference-guided\\nimage inpainting methods for solving TAMP.", \'authors\': [\'Yun Xing\', \'Qing Guo\', \'Xiaoguang Li\', \'Yihao Huang\', \'Xiaofeng Cao\', \'Di Lin\', \'Ivor Tsang\', \'Lei Ma\'], \'published\': \'2025-06-30T01:45:33+00:00\', \'categories\': [\'cs.CV\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23461v1\'}'}, {'id': 83, 'arxiv_id': '2506.23437v1', 'base_arxiv_id': '2506.23437', 'version': '1', 'title': 'From Large-scale Audio Tagging to Real-Time Explainable Emergency Vehicle Sirens Detection', 'abstract': 'Accurate recognition of Emergency Vehicle (EV) sirens is critical for the\nintegration of intelligent transportation systems, smart city monitoring\nsystems, and autonomous driving technologies. Modern automatic solutions are\nlimited by the lack of large scale, curated datasets and by the computational\ndemands of state of the art sound event detection models. This work introduces\nE2PANNs (Efficient Emergency Pre trained Audio Neural Networks), a lightweight\nConvolutional Neural Network architecture derived from the PANNs framework,\nspecifically optimized for binary EV siren detection. Leveraging our dedicated\nsubset of AudioSet (AudioSet EV) we fine-tune and evaluate E2PANNs across\nmultiple reference datasets and test its viability on embedded hardware. The\nexperimental campaign includes ablation studies, cross-domain benchmarking, and\nreal-time inference deployment on edge device. Interpretability analyses\nexploiting Guided Backpropagation and ScoreCAM algorithms provide insights into\nthe model internal representations and validate its ability to capture distinct\nspectrotemporal patterns associated with different types of EV sirens. Real\ntime performance is assessed through frame wise and event based detection\nmetrics, as well as a detailed analysis of false positive activations. Results\ndemonstrate that E2PANNs establish a new state of the art in this research\ndomain, with high computational efficiency, and suitability for edge-based\naudio monitoring and safety-critical applications.', 'submit_date': datetime.date(2025, 6, 30), 'metadata': "{'id': '2506.23437v1', 'title': 'From Large-scale Audio Tagging to Real-Time Explainable Emergency Vehicle Sirens Detection', 'abstract': 'Accurate recognition of Emergency Vehicle (EV) sirens is critical for the\\nintegration of intelligent transportation systems, smart city monitoring\\nsystems, and autonomous driving technologies. Modern automatic solutions are\\nlimited by the lack of large scale, curated datasets and by the computational\\ndemands of state of the art sound event detection models. This work introduces\\nE2PANNs (Efficient Emergency Pre trained Audio Neural Networks), a lightweight\\nConvolutional Neural Network architecture derived from the PANNs framework,\\nspecifically optimized for binary EV siren detection. Leveraging our dedicated\\nsubset of AudioSet (AudioSet EV) we fine-tune and evaluate E2PANNs across\\nmultiple reference datasets and test its viability on embedded hardware. The\\nexperimental campaign includes ablation studies, cross-domain benchmarking, and\\nreal-time inference deployment on edge device. Interpretability analyses\\nexploiting Guided Backpropagation and ScoreCAM algorithms provide insights into\\nthe model internal representations and validate its ability to capture distinct\\nspectrotemporal patterns associated with different types of EV sirens. Real\\ntime performance is assessed through frame wise and event based detection\\nmetrics, as well as a detailed analysis of false positive activations. Results\\ndemonstrate that E2PANNs establish a new state of the art in this research\\ndomain, with high computational efficiency, and suitability for edge-based\\naudio monitoring and safety-critical applications.', 'authors': ['Stefano Giacomelli', 'Marco Giordano', 'Claudia Rinaldi', 'Fabio Graziosi'], 'published': '2025-06-30T00:21:07+00:00', 'categories': ['cs.SD', 'cs.AI', 'eess.AS', '68T07', 'E.1; H.1; I.2; I.5; J.2; K.4; C.4'], 'url': 'http://arxiv.org/abs/2506.23437v1'}"}, {'id': 103, 'arxiv_id': '2506.23275v1', 'base_arxiv_id': '2506.23275', 'version': '1', 'title': 'Why Settle for One? Text-to-ImageSet Generation and Evaluation', 'abstract': "Despite remarkable progress in Text-to-Image models, many real-world\napplications require generating coherent image sets with diverse consistency\nrequirements. Existing consistent methods often focus on a specific domain with\nspecific aspects of consistency, which significantly constrains their\ngeneralizability to broader applications. In this paper, we propose a more\nchallenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate\nsets of images that meet various consistency requirements based on user\ninstructions. To systematically study this problem, we first introduce\n$T2IS-Bench$ with 596 diverse instructions across 26 subcategories,\nproviding comprehensive coverage for T2IS generation. Building on this, we\npropose $T2IS-Eval$, an evaluation framework that transforms user\ninstructions into multifaceted assessment criteria and employs effective\nevaluators to adaptively assess consistency fulfillment between criteria and\ngenerated sets. Subsequently, we propose $AutoT2IS$, a training-free\nframework that maximally leverages pretrained Diffusion Transformers'\nin-context capabilities to harmonize visual elements to satisfy both\nimage-level prompt alignment and set-level visual consistency. Extensive\nexperiments on T2IS-Bench reveal that diverse consistency challenges all\nexisting methods, while our AutoT2IS significantly outperforms current\ngeneralized and even specialized approaches. Our method also demonstrates the\nability to enable numerous underexplored real-world applications, confirming\nits substantial practical value. Visit our project in\nhttps://chengyou-jia.github.io/T2IS-Home.", 'submit_date': datetime.date(2025, 6, 29), 'metadata': '{\'id\': \'2506.23275v1\', \'title\': \'Why Settle for One? Text-to-ImageSet Generation and Evaluation\', \'abstract\': "Despite remarkable progress in Text-to-Image models, many real-world\\napplications require generating coherent image sets with diverse consistency\\nrequirements. Existing consistent methods often focus on a specific domain with\\nspecific aspects of consistency, which significantly constrains their\\ngeneralizability to broader applications. In this paper, we propose a more\\nchallenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate\\nsets of images that meet various consistency requirements based on user\\ninstructions. To systematically study this problem, we first introduce\\n$\\\\textbf{T2IS-Bench}$ with 596 diverse instructions across 26 subcategories,\\nproviding comprehensive coverage for T2IS generation. Building on this, we\\npropose $\\\\textbf{T2IS-Eval}$, an evaluation framework that transforms user\\ninstructions into multifaceted assessment criteria and employs effective\\nevaluators to adaptively assess consistency fulfillment between criteria and\\ngenerated sets. Subsequently, we propose $\\\\textbf{AutoT2IS}$, a training-free\\nframework that maximally leverages pretrained Diffusion Transformers\'\\nin-context capabilities to harmonize visual elements to satisfy both\\nimage-level prompt alignment and set-level visual consistency. Extensive\\nexperiments on T2IS-Bench reveal that diverse consistency challenges all\\nexisting methods, while our AutoT2IS significantly outperforms current\\ngeneralized and even specialized approaches. Our method also demonstrates the\\nability to enable numerous underexplored real-world applications, confirming\\nits substantial practical value. Visit our project in\\nhttps://chengyou-jia.github.io/T2IS-Home.", \'authors\': [\'Chengyou Jia\', \'Xin Shen\', \'Zhuohang Dang\', \'Zhuohang Dang\', \'Changliang Xia\', \'Weijia Wu\', \'Xinyu Zhang\', \'Hangwei Qian\', \'Ivor W. Tsang\', \'Minnan Luo\'], \'published\': \'2025-06-29T15:01:16+00:00\', \'categories\': [\'cs.CV\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23275v1\'}'}, {'id': 84, 'arxiv_id': '2506.23431v1', 'base_arxiv_id': '2506.23431', 'version': '1', 'title': 'Pipelined Decoder for Efficient Context-Aware Text Generation', 'abstract': 'As the basis of generative AI, an autoregressive model requires the\ngeneration of a new token depending on all the previously generated tokens,\nwhich brings high quality but also restricts the model to generate tokens one\nby one, forming a bottleneck limiting the generation speed. In this paper, we\npropose a new decoder architecture that efficiently generates text in parallel\nfor context-aware generation tasks. Our proposed pipelined decoder initiates\nthe generation of multiple subsequences simultaneously, and, at each time-step,\nit generates a new token for each subsequence to realize parallelism.\nExperiments on multiple text generation tasks, including question answering,\ntext summarization, and keyphrase generation, show that our pipelined decoder\nsignificantly improves the generation speed without a significant loss of\ngeneration quality or additional memory consumption.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23431v1', 'title': 'Pipelined Decoder for Efficient Context-Aware Text Generation', 'abstract': 'As the basis of generative AI, an autoregressive model requires the\\ngeneration of a new token depending on all the previously generated tokens,\\nwhich brings high quality but also restricts the model to generate tokens one\\nby one, forming a bottleneck limiting the generation speed. In this paper, we\\npropose a new decoder architecture that efficiently generates text in parallel\\nfor context-aware generation tasks. Our proposed pipelined decoder initiates\\nthe generation of multiple subsequences simultaneously, and, at each time-step,\\nit generates a new token for each subsequence to realize parallelism.\\nExperiments on multiple text generation tasks, including question answering,\\ntext summarization, and keyphrase generation, show that our pipelined decoder\\nsignificantly improves the generation speed without a significant loss of\\ngeneration quality or additional memory consumption.', 'authors': ['Zixian Huang', 'Chenxu Niu', 'Yu Gu', 'Gengyang Xiao', 'Xinwei Huang', 'Gong Cheng'], 'published': '2025-06-29T23:37:24+00:00', 'categories': ['cs.CL', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23431v1'}"}, {'id': 85, 'arxiv_id': '2506.23424v1', 'base_arxiv_id': '2506.23424', 'version': '1', 'title': 'Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting', 'abstract': 'Real-world time series often exhibit a non-stationary nature, degrading the\nperformance of pre-trained forecasting models. Test-Time Adaptation (TTA)\naddresses this by adjusting models during inference, but existing methods\ntypically update the full model, increasing memory and compute costs. We\npropose PETSA, a parameter-efficient method that adapts forecasters at test\ntime by only updating small calibration modules on the input and output. PETSA\nuses low-rank adapters and dynamic gating to adjust representations without\nretraining. To maintain accuracy despite limited adaptation capacity, we\nintroduce a specialized loss combining three components: (1) a robust term, (2)\na frequency-domain term to preserve periodicity, and (3) a patch-wise\nstructural term for structural alignment. PETSA improves the adaptability of\nvarious forecasting backbones while requiring fewer parameters than baselines.\nExperimental results on benchmark datasets show that PETSA achieves competitive\nor better performance across all horizons. Our code is available at:\nhttps://github.com/BorealisAI/PETSA', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23424v1', 'title': 'Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting', 'abstract': 'Real-world time series often exhibit a non-stationary nature, degrading the\\nperformance of pre-trained forecasting models. Test-Time Adaptation (TTA)\\naddresses this by adjusting models during inference, but existing methods\\ntypically update the full model, increasing memory and compute costs. We\\npropose PETSA, a parameter-efficient method that adapts forecasters at test\\ntime by only updating small calibration modules on the input and output. PETSA\\nuses low-rank adapters and dynamic gating to adjust representations without\\nretraining. To maintain accuracy despite limited adaptation capacity, we\\nintroduce a specialized loss combining three components: (1) a robust term, (2)\\na frequency-domain term to preserve periodicity, and (3) a patch-wise\\nstructural term for structural alignment. PETSA improves the adaptability of\\nvarious forecasting backbones while requiring fewer parameters than baselines.\\nExperimental results on benchmark datasets show that PETSA achieves competitive\\nor better performance across all horizons. Our code is available at:\\nhttps://github.com/BorealisAI/PETSA', 'authors': ['Heitor R. Medeiros', 'Hossein Sharifi-Noghabi', 'Gabriel L. Oliveira', 'Saghar Irandoust'], 'published': '2025-06-29T23:09:35+00:00', 'categories': ['cs.LG', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23424v1'}"}, {'id': 86, 'arxiv_id': '2506.23423v1', 'base_arxiv_id': '2506.23423', 'version': '1', 'title': 'TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs', 'abstract': "Past work has studied the effects of fine-tuning on large language models'\n(LLMs) overall performance on certain tasks. However, a quantitative and\nsystematic method for analyzing its effect on individual outputs is still\nlacking. Here, we propose a new method for measuring the contribution that\nfine-tuning makes to individual LLM responses, assuming access to the original\npre-trained model. Our method tracks the model's intermediate hidden states,\nproviding a more fine-grained insight into the effects of fine-tuning than a\nsimple comparison of final outputs from pre-trained and fine-tuned models. We\nintroduce and theoretically analyze an exact decomposition of any fine-tuned\nLLM into a pre-training component and a fine-tuning component. Empirically, we\nfind that model behavior and performance can be steered by up- or down-scaling\nthe fine-tuning component during the forward pass. Motivated by this finding\nand our theoretical analysis, we define the Tuning Contribution (TuCo) as the\nratio of the magnitudes of the fine-tuning component to the pre-training\ncomponent. We observe that three prominent adversarial attacks on LLMs\ncircumvent safety measures in a way that reduces TuCo, and that TuCo is\nconsistently lower on prompts where these attacks succeed compared to those\nwhere they do not. This suggests that attenuating the effect of fine-tuning on\nmodel outputs plays a role in the success of such attacks. In summary, TuCo\nenables the quantitative study of how fine-tuning influences model behavior and\nsafety, and vice versa.", 'submit_date': datetime.date(2025, 6, 29), 'metadata': '{\'id\': \'2506.23423v1\', \'title\': \'TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs\', \'abstract\': "Past work has studied the effects of fine-tuning on large language models\'\\n(LLMs) overall performance on certain tasks. However, a quantitative and\\nsystematic method for analyzing its effect on individual outputs is still\\nlacking. Here, we propose a new method for measuring the contribution that\\nfine-tuning makes to individual LLM responses, assuming access to the original\\npre-trained model. Our method tracks the model\'s intermediate hidden states,\\nproviding a more fine-grained insight into the effects of fine-tuning than a\\nsimple comparison of final outputs from pre-trained and fine-tuned models. We\\nintroduce and theoretically analyze an exact decomposition of any fine-tuned\\nLLM into a pre-training component and a fine-tuning component. Empirically, we\\nfind that model behavior and performance can be steered by up- or down-scaling\\nthe fine-tuning component during the forward pass. Motivated by this finding\\nand our theoretical analysis, we define the Tuning Contribution (TuCo) as the\\nratio of the magnitudes of the fine-tuning component to the pre-training\\ncomponent. We observe that three prominent adversarial attacks on LLMs\\ncircumvent safety measures in a way that reduces TuCo, and that TuCo is\\nconsistently lower on prompts where these attacks succeed compared to those\\nwhere they do not. This suggests that attenuating the effect of fine-tuning on\\nmodel outputs plays a role in the success of such attacks. In summary, TuCo\\nenables the quantitative study of how fine-tuning influences model behavior and\\nsafety, and vice versa.", \'authors\': [\'Felipe Nuti\', \'Tim Franzmeyer\', \'Jo√£o Henriques\'], \'published\': \'2025-06-29T23:08:36+00:00\', \'categories\': [\'cs.CL\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23423v1\'}'}, {'id': 87, 'arxiv_id': '2506.23419v1', 'base_arxiv_id': '2506.23419', 'version': '1', 'title': 'BenchMake: Turn any scientific data set into a reproducible benchmark', 'abstract': 'Benchmark data sets are a cornerstone of machine learning development and\napplications, ensuring new methods are robust, reliable and competitive. The\nrelative rarity of benchmark sets in computational science, due to the\nuniqueness of the problems and the pace of change in the associated domains,\nmakes evaluating new innovations difficult for computational scientists. In\nthis paper a new tool is developed and tested to potentially turn any of the\nincreasing numbers of scientific data sets made openly available into a\nbenchmark accessible to the community. BenchMake uses non-negative matrix\nfactorisation to deterministically identify and isolate challenging edge cases\non the convex hull (the smallest convex set that contains all existing data\ninstances) and partitions a required fraction of matched data instances into a\ntesting set that maximises divergence and statistical significance, across\ntabular, graph, image, signal and textual modalities. BenchMake splits are\ncompared to establish splits and random splits using ten publicly available\nbenchmark sets from different areas of science, with different sizes, shapes,\ndistributions.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23419v1', 'title': 'BenchMake: Turn any scientific data set into a reproducible benchmark', 'abstract': 'Benchmark data sets are a cornerstone of machine learning development and\\napplications, ensuring new methods are robust, reliable and competitive. The\\nrelative rarity of benchmark sets in computational science, due to the\\nuniqueness of the problems and the pace of change in the associated domains,\\nmakes evaluating new innovations difficult for computational scientists. In\\nthis paper a new tool is developed and tested to potentially turn any of the\\nincreasing numbers of scientific data sets made openly available into a\\nbenchmark accessible to the community. BenchMake uses non-negative matrix\\nfactorisation to deterministically identify and isolate challenging edge cases\\non the convex hull (the smallest convex set that contains all existing data\\ninstances) and partitions a required fraction of matched data instances into a\\ntesting set that maximises divergence and statistical significance, across\\ntabular, graph, image, signal and textual modalities. BenchMake splits are\\ncompared to establish splits and random splits using ten publicly available\\nbenchmark sets from different areas of science, with different sizes, shapes,\\ndistributions.', 'authors': ['Amanda S Barnard'], 'published': '2025-06-29T22:56:48+00:00', 'categories': ['cs.LG', 'cs.AI', 'cs.DL', '62G09', 'J.1'], 'url': 'http://arxiv.org/abs/2506.23419v1'}"}, {'id': 88, 'arxiv_id': '2506.23393v1', 'base_arxiv_id': '2506.23393', 'version': '1', 'title': 'Hierarchical Memory Organization for Wikipedia Generation', 'abstract': 'Generating Wikipedia articles autonomously is a challenging task requiring\nthe integration of accurate, comprehensive, and well-structured information\nfrom diverse sources. This paper introduces the Memory Organization-based\nGeneration (MOG) framework, a novel approach to address these challenges by\nleveraging a hierarchical memory architecture. MOG extracts fine-grained memory\nunits from web documents, recursively organizes them into a Wikipedia-style\nhierarchical structure, and uses this structure to guide the generation\nprocess. This ensures alignment between memory and the article outline,\nimproving both informativeness and verifiability while minimizing\nhallucinations. Additionally, a citation module is implemented to enhance\ntraceability by linking every generated sentence to specific memory units.\nEvaluations on our newly created WikiStart dataset demonstrate that MOG\noutperforms baseline methods in producing informative and reliable articles,\nmaking it particularly robust in real-world scenarios.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23393v1', 'title': 'Hierarchical Memory Organization for Wikipedia Generation', 'abstract': 'Generating Wikipedia articles autonomously is a challenging task requiring\\nthe integration of accurate, comprehensive, and well-structured information\\nfrom diverse sources. This paper introduces the Memory Organization-based\\nGeneration (MOG) framework, a novel approach to address these challenges by\\nleveraging a hierarchical memory architecture. MOG extracts fine-grained memory\\nunits from web documents, recursively organizes them into a Wikipedia-style\\nhierarchical structure, and uses this structure to guide the generation\\nprocess. This ensures alignment between memory and the article outline,\\nimproving both informativeness and verifiability while minimizing\\nhallucinations. Additionally, a citation module is implemented to enhance\\ntraceability by linking every generated sentence to specific memory units.\\nEvaluations on our newly created WikiStart dataset demonstrate that MOG\\noutperforms baseline methods in producing informative and reliable articles,\\nmaking it particularly robust in real-world scenarios.', 'authors': ['Eugene J. Yu', 'Dawei Zhu', 'Yifan Song', 'Xiangyu Wong', 'Jiebin Zhang', 'Wenxuan Shi', 'Xiaoguang Li', 'Qun Liu', 'Sujian Li'], 'published': '2025-06-29T20:22:49+00:00', 'categories': ['cs.CL', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23393v1'}"}, {'id': 104, 'arxiv_id': '2506.23274v1', 'base_arxiv_id': '2506.23274', 'version': '1', 'title': 'Predicting thinking time in Reasoning models', 'abstract': 'Reasoning models that produce long, hidden chains of thought have emerged as\npowerful tools for complex, reasoning-intensive\ntasksdeepseekai2025deepseekr1incentivizingreasoningcapability,\nopenai2024openaio1card. However, this paradigm introduces a new user\nexperience challenge: users have little insight into how much time the model\nwill spend reasoning before returning an answer. This unpredictability, can\nlead to user frustration and is likely to compound as LLMs can produce\nincreasingly long tasks asynchronously\nkwa2025measuringaiabilitycomplete. In this paper, we introduce and\nevaluate methods for both online and offline prediction of model "thinking\ntime," aiming to develop a practical "progress bar for reasoning." We discuss\nthe implications for user interaction and future research directions.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': '{\'id\': \'2506.23274v1\', \'title\': \'Predicting thinking time in Reasoning models\', \'abstract\': \'Reasoning models that produce long, hidden chains of thought have emerged as\\npowerful tools for complex, reasoning-intensive\\ntasks\\\\citep{deepseekai2025deepseekr1incentivizingreasoningcapability,\\nopenai2024openaio1card}. However, this paradigm introduces a new user\\nexperience challenge: users have little insight into how much time the model\\nwill spend reasoning before returning an answer. This unpredictability, can\\nlead to user frustration and is likely to compound as LLMs can produce\\nincreasingly long tasks asynchronously\\n\\\\citep{kwa2025measuringaiabilitycomplete}. In this paper, we introduce and\\nevaluate methods for both online and offline prediction of model "thinking\\ntime," aiming to develop a practical "progress bar for reasoning." We discuss\\nthe implications for user interaction and future research directions.\', \'authors\': [\'Hans Peter Lynsg√∏e Raaschou-jensen\', \'Constanza Fierro\', \'Anders S√∏gaard\'], \'published\': \'2025-06-29T15:01:01+00:00\', \'categories\': [\'cs.LG\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23274v1\'}'}, {'id': 89, 'arxiv_id': '2506.23382v1', 'base_arxiv_id': '2506.23382', 'version': '1', 'title': 'SIEDD: Shared-Implicit Encoder with Discrete Decoders', 'abstract': 'Implicit Neural Representations (INRs) offer exceptional fidelity for video\ncompression by learning per-video optimized functions, but their adoption is\ncrippled by impractically slow encoding times. Existing attempts to accelerate\nINR encoding often sacrifice reconstruction quality or crucial coordinate-level\ncontrol essential for adaptive streaming and transcoding. We introduce SIEDD\n(Shared-Implicit Encoder with Discrete Decoders), a novel architecture that\nfundamentally accelerates INR encoding without these compromises. SIEDD first\nrapidly trains a shared, coordinate-based encoder on sparse anchor frames to\nefficiently capture global, low-frequency video features. This encoder is then\nfrozen, enabling massively parallel training of lightweight, discrete decoders\nfor individual frame groups, further expedited by aggressive coordinate-space\nsampling. This synergistic design delivers a remarkable 20-30X encoding\nspeed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while\nmaintaining competitive reconstruction quality and compression ratios.\nCritically, SIEDD retains full coordinate-based control, enabling continuous\nresolution decoding and eliminating costly transcoding. Our approach\nsignificantly advances the practicality of high-fidelity neural video\ncompression, demonstrating a scalable and efficient path towards real-world\ndeployment. Our codebase is available at\nhttps://github.com/VikramRangarajan/SIEDD .', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23382v1', 'title': 'SIEDD: Shared-Implicit Encoder with Discrete Decoders', 'abstract': 'Implicit Neural Representations (INRs) offer exceptional fidelity for video\\ncompression by learning per-video optimized functions, but their adoption is\\ncrippled by impractically slow encoding times. Existing attempts to accelerate\\nINR encoding often sacrifice reconstruction quality or crucial coordinate-level\\ncontrol essential for adaptive streaming and transcoding. We introduce SIEDD\\n(Shared-Implicit Encoder with Discrete Decoders), a novel architecture that\\nfundamentally accelerates INR encoding without these compromises. SIEDD first\\nrapidly trains a shared, coordinate-based encoder on sparse anchor frames to\\nefficiently capture global, low-frequency video features. This encoder is then\\nfrozen, enabling massively parallel training of lightweight, discrete decoders\\nfor individual frame groups, further expedited by aggressive coordinate-space\\nsampling. This synergistic design delivers a remarkable 20-30X encoding\\nspeed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while\\nmaintaining competitive reconstruction quality and compression ratios.\\nCritically, SIEDD retains full coordinate-based control, enabling continuous\\nresolution decoding and eliminating costly transcoding. Our approach\\nsignificantly advances the practicality of high-fidelity neural video\\ncompression, demonstrating a scalable and efficient path towards real-world\\ndeployment. Our codebase is available at\\nhttps://github.com/VikramRangarajan/SIEDD .', 'authors': ['Vikram Rangarajan', 'Shishira Maiya', 'Max Ehrlich', 'Abhinav Shrivastava'], 'published': '2025-06-29T19:39:43+00:00', 'categories': ['cs.CV', 'cs.AI', 'cs.LG'], 'url': 'http://arxiv.org/abs/2506.23382v1'}"}, {'id': 90, 'arxiv_id': '2506.23377v1', 'base_arxiv_id': '2506.23377', 'version': '1', 'title': 'Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs', 'abstract': 'Large language models (LLMs) are used in a variety of mission-critical roles.\nDue to the rapidly developing nature of LLMs, there is a lack of quantifiable\nunderstanding of the bias and perspective associated with LLM output. Inspired\nby this need, this paper considers the broader issue of perspective or\nviewpoint of general text and perspective control of large-language model (LLM)\noutput. Perspective-Dial consists of two main components: a (1) metric space,\ndubbed Perspective Space, that enables quantitative measurements of different\nperspectives regarding a topic, and the use of (2) Systematic Prompt\nEngineering that utilizes greedy-coordinate descent to control LLM output\nperspective based on measurement feedback from the Perspective Space. The\nempirical nature of the approach allows progress to side step a principled\nunderstanding of perspective or bias -- effectively quantifying and adjusting\noutputs for a variety of topics. Potential applications include detection,\ntracking and mitigation of LLM bias, narrative detection, sense making and\ntracking in public discourse, and debate bot advocating given perspective.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23377v1', 'title': 'Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs', 'abstract': 'Large language models (LLMs) are used in a variety of mission-critical roles.\\nDue to the rapidly developing nature of LLMs, there is a lack of quantifiable\\nunderstanding of the bias and perspective associated with LLM output. Inspired\\nby this need, this paper considers the broader issue of perspective or\\nviewpoint of general text and perspective control of large-language model (LLM)\\noutput. Perspective-Dial consists of two main components: a (1) metric space,\\ndubbed Perspective Space, that enables quantitative measurements of different\\nperspectives regarding a topic, and the use of (2) Systematic Prompt\\nEngineering that utilizes greedy-coordinate descent to control LLM output\\nperspective based on measurement feedback from the Perspective Space. The\\nempirical nature of the approach allows progress to side step a principled\\nunderstanding of perspective or bias -- effectively quantifying and adjusting\\noutputs for a variety of topics. Potential applications include detection,\\ntracking and mitigation of LLM bias, narrative detection, sense making and\\ntracking in public discourse, and debate bot advocating given perspective.', 'authors': ['Taejin Kim', 'Siun-Chuon Mau', 'Konrad Vesey'], 'published': '2025-06-29T19:26:37+00:00', 'categories': ['cs.CL', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23377v1'}"}, {'id': 91, 'arxiv_id': '2506.23358v1', 'base_arxiv_id': '2506.23358', 'version': '1', 'title': 'Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment', 'abstract': 'We present Federated Timeline Synthesis (FTS), a novel framework for training\ngenerative foundation models across distributed timeseries data applied to\nelectronic health records (EHR). At its core, FTS represents patient history as\ntokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding\ntemporal, categorical, and continuous clinical information. Each institution\ntrains an autoregressive transformer on its local PHTs and transmits only model\nweights to a central server. The server uses the generators to synthesize a\nlarge corpus of trajectories and train a Global Generator (GG), enabling\nzero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS\non five clinically meaningful prediction tasks using MIMIC-IV data, showing\nthat models trained on synthetic data generated by GG perform comparably to\nthose trained on real data. FTS offers strong privacy guarantees, scalability\nacross institutions, and extensibility to diverse prediction and simulation\ntasks especially in healthcare, including counterfactual inference, early\nwarning detection, and synthetic trial design.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23358v1', 'title': 'Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment', 'abstract': 'We present Federated Timeline Synthesis (FTS), a novel framework for training\\ngenerative foundation models across distributed timeseries data applied to\\nelectronic health records (EHR). At its core, FTS represents patient history as\\ntokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding\\ntemporal, categorical, and continuous clinical information. Each institution\\ntrains an autoregressive transformer on its local PHTs and transmits only model\\nweights to a central server. The server uses the generators to synthesize a\\nlarge corpus of trajectories and train a Global Generator (GG), enabling\\nzero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS\\non five clinically meaningful prediction tasks using MIMIC-IV data, showing\\nthat models trained on synthetic data generated by GG perform comparably to\\nthose trained on real data. FTS offers strong privacy guarantees, scalability\\nacross institutions, and extensibility to diverse prediction and simulation\\ntasks especially in healthcare, including counterfactual inference, early\\nwarning detection, and synthetic trial design.', 'authors': ['Pawel Renc', 'Michal K. Grzeszczyk', 'Linglong Qian', 'Nassim Oufattole', 'Jeff Rasley', 'Arkadiusz Sitek'], 'published': '2025-06-29T18:29:59+00:00', 'categories': ['cs.LG', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23358v1'}"}, {'id': 92, 'arxiv_id': '2506.23351v1', 'base_arxiv_id': '2506.23351', 'version': '1', 'title': 'Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop', 'abstract': 'Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in\nrobotics, driven by the need for autonomous systems that can perceive, reason,\nand act in complex physical environments. While single-arm systems have shown\nstrong task performance, collaborative dual-arm systems are essential for\nhandling more intricate tasks involving rigid, deformable, and\ntactile-sensitive objects. To advance this goal, we launched the RoboTwin\nDual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on\nthe RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot\nplatform, the competition consisted of three stages: Simulation Round 1,\nSimulation Round 2, and a final Real-World Round. Participants totally tackled\n17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based\nscenarios. The challenge attracted 64 global teams and over 400 participants,\nproducing top-performing solutions like SEM and AnchorDP3 and generating\nvaluable insights into generalizable bimanual policy learning. This report\noutlines the competition setup, task design, evaluation methodology, key\nfindings and future direction, aiming to support future research on robust and\ngeneralizable bimanual manipulation policies. The Challenge Webpage is\navailable at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23351v1', 'title': 'Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop', 'abstract': 'Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in\\nrobotics, driven by the need for autonomous systems that can perceive, reason,\\nand act in complex physical environments. While single-arm systems have shown\\nstrong task performance, collaborative dual-arm systems are essential for\\nhandling more intricate tasks involving rigid, deformable, and\\ntactile-sensitive objects. To advance this goal, we launched the RoboTwin\\nDual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on\\nthe RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot\\nplatform, the competition consisted of three stages: Simulation Round 1,\\nSimulation Round 2, and a final Real-World Round. Participants totally tackled\\n17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based\\nscenarios. The challenge attracted 64 global teams and over 400 participants,\\nproducing top-performing solutions like SEM and AnchorDP3 and generating\\nvaluable insights into generalizable bimanual policy learning. This report\\noutlines the competition setup, task design, evaluation methodology, key\\nfindings and future direction, aiming to support future research on robust and\\ngeneralizable bimanual manipulation policies. The Challenge Webpage is\\navailable at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.', 'authors': ['Tianxing Chen', 'Kaixuan Wang', 'Zhaohui Yang', 'Yuhao Zhang', 'Zanxin Chen', 'Baijun Chen', 'Wanxi Dong', 'Ziyuan Liu', 'Dong Chen', 'Tianshuo Yang', 'Haibao Yu', 'Xiaokang Yang', 'Yusen Qin', 'Zhiqiang Xie', 'Yao Mu', 'Ping Luo', 'Tian Nian', 'Weiliang Deng', 'Yiheng Ge', 'Yibin Liu', 'Zixuan Li', 'Dehui Wang', 'Zhixuan Liang', 'Haohui Xie', 'Rijie Zeng', 'Yunfei Ge', 'Peiqing Cong', 'Guannan He', 'Zhaoming Han', 'Ruocheng Yin', 'Jingxiang Guo', 'Lunkai Lin', 'Tianling Xu', 'Hongzhe Bi', 'Xuewu Lin', 'Tianwei Lin', 'Shujie Luo', 'Keyu Li', 'Ziyan Zhao', 'Ke Fan', 'Heyang Xu', 'Bo Peng', 'Wenlong Gao', 'Dongjiang Li', 'Feng Jin', 'Hui Shen', 'Jinming Li', 'Chaowei Cui', 'Yuchen', 'Yaxin Peng', 'Lingdong Zeng', 'Wenlong Dong', 'Tengfei Li', 'Weijie Ke', 'Jun Chen', 'Erdemt Bao', 'Tian Lan', 'Tenglong Liu', 'Jin Yang', 'Huiping Zhuang', 'Baozhi Jia', 'Shuai Zhang', 'Zhengfeng Zou', 'Fangheng Guan', 'Tianyi Jia', 'Ke Zhou', 'Hongjiu Zhang', 'Yating Han', 'Cheng Fang', 'Yixian Zou', 'Chongyang Xu', 'Qinglun Zhang', 'Shen Cheng', 'Xiaohe Wang', 'Ping Tan', 'Haoqiang Fan', 'Shuaicheng Liu', 'Jiaheng Chen', 'Chuxuan Huang', 'Chengliang Lin', 'Kaijun Luo', 'Boyu Yue', 'Yi Liu', 'Jinyu Chen', 'Zichang Tan', 'Liming Deng', 'Shuo Xu', 'Zijian Cai', 'Shilong Yin', 'Hao Wang', 'Hongshan Liu', 'Tianyang Li', 'Long Shi', 'Ran Xu', 'Huilin Xu', 'Zhengquan Zhang', 'Congsheng Xu', 'Jinchang Yang', 'Feng Xu'], 'published': '2025-06-29T17:56:41+00:00', 'categories': ['cs.RO', 'cs.AI', 'cs.LG', 'cs.MA'], 'url': 'http://arxiv.org/abs/2506.23351v1'}"}, {'id': 93, 'arxiv_id': '2506.23342v1', 'base_arxiv_id': '2506.23342', 'version': '1', 'title': 'ATGen: A Framework for Active Text Generation', 'abstract': 'Active learning (AL) has demonstrated remarkable potential in reducing the\nannotation effort required for training machine learning models. However,\ndespite the surging popularity of natural language generation (NLG) tasks in\nrecent years, the application of AL to NLG has been limited. In this paper, we\nintroduce Active Text Generation (ATGen) - a comprehensive framework that\nbridges AL with text generation tasks, enabling the application of\nstate-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered\nannotation in NLG tasks using both human annotators and automatic annotation\nagents based on large language models (LLMs). The framework supports LLMs\ndeployed as services, such as ChatGPT and Claude, or operated on-premises.\nFurthermore, ATGen provides a unified platform for smooth implementation and\nbenchmarking of novel AL strategies tailored to NLG tasks. Finally, we present\nevaluation results for state-of-the-art AL strategies across diverse settings\nand multiple text generation tasks. We show that ATGen reduces both the effort\nof human annotators and costs associated with API calls to LLM-based annotation\nagents. The code of the framework is available on GitHub under the MIT license.\nThe video presentation is available at http://atgen-video.nlpresearch.group', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23342v1', 'title': 'ATGen: A Framework for Active Text Generation', 'abstract': 'Active learning (AL) has demonstrated remarkable potential in reducing the\\nannotation effort required for training machine learning models. However,\\ndespite the surging popularity of natural language generation (NLG) tasks in\\nrecent years, the application of AL to NLG has been limited. In this paper, we\\nintroduce Active Text Generation (ATGen) - a comprehensive framework that\\nbridges AL with text generation tasks, enabling the application of\\nstate-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered\\nannotation in NLG tasks using both human annotators and automatic annotation\\nagents based on large language models (LLMs). The framework supports LLMs\\ndeployed as services, such as ChatGPT and Claude, or operated on-premises.\\nFurthermore, ATGen provides a unified platform for smooth implementation and\\nbenchmarking of novel AL strategies tailored to NLG tasks. Finally, we present\\nevaluation results for state-of-the-art AL strategies across diverse settings\\nand multiple text generation tasks. We show that ATGen reduces both the effort\\nof human annotators and costs associated with API calls to LLM-based annotation\\nagents. The code of the framework is available on GitHub under the MIT license.\\nThe video presentation is available at http://atgen-video.nlpresearch.group', 'authors': ['Akim Tsvigun', 'Daniil Vasilev', 'Ivan Tsvigun', 'Ivan Lysenko', 'Talgat Bektleuov', 'Aleksandr Medvedev', 'Uliana Vinogradova', 'Nikita Severin', 'Mikhail Mozikov', 'Andrey Savchenko', 'Rostislav Grigorev', 'Ramil Kuleev', 'Fedor Zhdanov', 'Artem Shelmanov', 'Ilya Makarov'], 'published': '2025-06-29T17:27:48+00:00', 'categories': ['cs.CL', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23342v1'}"}, {'id': 94, 'arxiv_id': '2506.23334v1', 'base_arxiv_id': '2506.23334', 'version': '1', 'title': 'Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation', 'abstract': 'Federated learning (FL) has emerged as a promising paradigm for\ncollaboratively training deep learning models across institutions without\nexchanging sensitive medical data. However, its effectiveness is often hindered\nby limited data availability and non-independent, identically distributed data\nacross participating clients, which can degrade model performance and\ngeneralization. To address these challenges, we propose a generative AI based\ndata augmentation framework that integrates synthetic image sharing into the\nfederated training process for breast cancer diagnosis via ultrasound images.\nSpecifically, we train two simple class-specific Deep Convolutional Generative\nAdversarial Networks: one for benign and one for malignant lesions. We then\nsimulate a realistic FL setting using three publicly available breast\nultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are\nadopted as baseline FL algorithms. Experimental results show that incorporating\na suitable number of synthetic images improved the average AUC from 0.9206 to\n0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that\nexcessive use of synthetic data reduced performance, underscoring the\nimportance of maintaining a balanced ratio of real and synthetic samples. Our\nfindings highlight the potential of generative AI based data augmentation to\nenhance FL results in the breast ultrasound image classification task.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23334v1', 'title': 'Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation', 'abstract': 'Federated learning (FL) has emerged as a promising paradigm for\\ncollaboratively training deep learning models across institutions without\\nexchanging sensitive medical data. However, its effectiveness is often hindered\\nby limited data availability and non-independent, identically distributed data\\nacross participating clients, which can degrade model performance and\\ngeneralization. To address these challenges, we propose a generative AI based\\ndata augmentation framework that integrates synthetic image sharing into the\\nfederated training process for breast cancer diagnosis via ultrasound images.\\nSpecifically, we train two simple class-specific Deep Convolutional Generative\\nAdversarial Networks: one for benign and one for malignant lesions. We then\\nsimulate a realistic FL setting using three publicly available breast\\nultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are\\nadopted as baseline FL algorithms. Experimental results show that incorporating\\na suitable number of synthetic images improved the average AUC from 0.9206 to\\n0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that\\nexcessive use of synthetic data reduced performance, underscoring the\\nimportance of maintaining a balanced ratio of real and synthetic samples. Our\\nfindings highlight the potential of generative AI based data augmentation to\\nenhance FL results in the breast ultrasound image classification task.', 'authors': ['Hongyi Pan', 'Ziliang Hong', 'Gorkem Durak', 'Ziyue Xu', 'Ulas Bagci'], 'published': '2025-06-29T17:05:50+00:00', 'categories': ['eess.IV', 'cs.AI', 'cs.CV'], 'url': 'http://arxiv.org/abs/2506.23334v1'}"}, {'id': 95, 'arxiv_id': '2506.23325v1', 'base_arxiv_id': '2506.23325', 'version': '1', 'title': 'XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs', 'abstract': 'Speech codecs serve as bridges between speech signals and large language\nmodels. An ideal codec for speech language models should not only preserve\nacoustic information but also capture rich semantic information. However,\nexisting speech codecs struggle to balance high-quality audio reconstruction\nwith ease of modeling by language models. In this study, we analyze the\nlimitations of previous codecs in balancing semantic richness and acoustic\nfidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict\nbetween semantic and acoustic capabilities through multi-stage, multi-task\nlearning. Experimental results demonstrate that XY-Tokenizer achieves\nperformance in both semantic and acoustic tasks comparable to that of\nstate-of-the-art codecs operating at similar bitrates, even though those\nexisting codecs typically excel in only one aspect. Specifically, XY-Tokenizer\nachieves strong text alignment, surpassing distillation-based semantic modeling\nmethods such as SpeechTokenizer and Mimi, while maintaining a speaker\nsimilarity score of 0.83 between reconstructed and original audio. The\nreconstruction performance of XY-Tokenizer is comparable to that of BigCodec,\nthe current state-of-the-art among acoustic-only codecs, which achieves a\nspeaker similarity score of 0.84 at a similar bitrate. Code and models are\navailable at https://github.com/gyt1145028706/XY-Tokenizer.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23325v1', 'title': 'XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs', 'abstract': 'Speech codecs serve as bridges between speech signals and large language\\nmodels. An ideal codec for speech language models should not only preserve\\nacoustic information but also capture rich semantic information. However,\\nexisting speech codecs struggle to balance high-quality audio reconstruction\\nwith ease of modeling by language models. In this study, we analyze the\\nlimitations of previous codecs in balancing semantic richness and acoustic\\nfidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict\\nbetween semantic and acoustic capabilities through multi-stage, multi-task\\nlearning. Experimental results demonstrate that XY-Tokenizer achieves\\nperformance in both semantic and acoustic tasks comparable to that of\\nstate-of-the-art codecs operating at similar bitrates, even though those\\nexisting codecs typically excel in only one aspect. Specifically, XY-Tokenizer\\nachieves strong text alignment, surpassing distillation-based semantic modeling\\nmethods such as SpeechTokenizer and Mimi, while maintaining a speaker\\nsimilarity score of 0.83 between reconstructed and original audio. The\\nreconstruction performance of XY-Tokenizer is comparable to that of BigCodec,\\nthe current state-of-the-art among acoustic-only codecs, which achieves a\\nspeaker similarity score of 0.84 at a similar bitrate. Code and models are\\navailable at https://github.com/gyt1145028706/XY-Tokenizer.', 'authors': ['Yitian Gong', 'Luozhijie Jin', 'Ruifan Deng', 'Dong Zhang', 'Xin Zhang', 'Qinyuan Cheng', 'Zhaoye Fei', 'Shimin Li', 'Xipeng Qiu'], 'published': '2025-06-29T16:51:50+00:00', 'categories': ['cs.SD', 'cs.AI', 'eess.AS'], 'url': 'http://arxiv.org/abs/2506.23325v1'}"}, {'id': 96, 'arxiv_id': '2506.23322v1', 'base_arxiv_id': '2506.23322', 'version': '1', 'title': 'GaussMaster: An LLM-based Database Copilot System', 'abstract': 'In the financial industry, data is the lifeblood of operations, and DBAs\nshoulder significant responsibilities for SQL tuning, database deployment,\ndiagnosis, and service repair. In recent years, both database vendors and\ncustomers have increasingly turned to autonomous database platforms in an\neffort to alleviate the heavy workload of DBAs. However, existing autonomous\ndatabase platforms are limited in their capabilities, primarily addressing\nsingle-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual\nintervention remains a necessity for comprehensive database maintenance.\nGaussMaster aims to revolutionize this landscape by introducing an LLM-based\ndatabase copilot system. This innovative solution is designed not only to\nassist developers in writing efficient SQL queries but also to provide\ncomprehensive care for database services. When database instances exhibit\nabnormal behavior, GaussMaster is capable of orchestrating the entire\nmaintenance process automatically. It achieves this by analyzing hundreds of\nmetrics and logs, employing a Tree-of-thought approach to identify root causes,\nand invoking appropriate tools to resolve issues. We have successfully\nimplemented GaussMaster in real-world scenarios, such as the banking industry,\nwhere it has achieved zero human intervention for over 34 database maintenance\nscenarios. In this paper, we present significant improvements in these tasks\nwith code at https://gitcode.com/opengauss/openGauss-GaussMaster.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23322v1', 'title': 'GaussMaster: An LLM-based Database Copilot System', 'abstract': 'In the financial industry, data is the lifeblood of operations, and DBAs\\nshoulder significant responsibilities for SQL tuning, database deployment,\\ndiagnosis, and service repair. In recent years, both database vendors and\\ncustomers have increasingly turned to autonomous database platforms in an\\neffort to alleviate the heavy workload of DBAs. However, existing autonomous\\ndatabase platforms are limited in their capabilities, primarily addressing\\nsingle-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual\\nintervention remains a necessity for comprehensive database maintenance.\\nGaussMaster aims to revolutionize this landscape by introducing an LLM-based\\ndatabase copilot system. This innovative solution is designed not only to\\nassist developers in writing efficient SQL queries but also to provide\\ncomprehensive care for database services. When database instances exhibit\\nabnormal behavior, GaussMaster is capable of orchestrating the entire\\nmaintenance process automatically. It achieves this by analyzing hundreds of\\nmetrics and logs, employing a Tree-of-thought approach to identify root causes,\\nand invoking appropriate tools to resolve issues. We have successfully\\nimplemented GaussMaster in real-world scenarios, such as the banking industry,\\nwhere it has achieved zero human intervention for over 34 database maintenance\\nscenarios. In this paper, we present significant improvements in these tasks\\nwith code at https://gitcode.com/opengauss/openGauss-GaussMaster.', 'authors': ['Wei Zhou', 'Ji Sun', 'Xuanhe Zhou', 'Guoliang Li', 'Luyang Liu', 'Hao Wu', 'Tianyuan Wang'], 'published': '2025-06-29T16:39:31+00:00', 'categories': ['cs.DB', 'cs.AI', 'cs.CL', 'cs.IR'], 'url': 'http://arxiv.org/abs/2506.23322v1'}"}, {'id': 97, 'arxiv_id': '2506.23314v1', 'base_arxiv_id': '2506.23314', 'version': '1', 'title': 'Interpretable by Design: MH-AutoML for Transparent and Efficient Android Malware Detection without Compromising Performance', 'abstract': 'Malware detection in Android systems requires both cybersecurity expertise\nand machine learning (ML) techniques. Automated Machine Learning (AutoML) has\nemerged as an approach to simplify ML development by reducing the need for\nspecialized knowledge. However, current AutoML solutions typically operate as\nblack-box systems with limited transparency, interpretability, and experiment\ntraceability. To address these limitations, we present MH-AutoML, a\ndomain-specific framework for Android malware detection. MH-AutoML automates\nthe entire ML pipeline, including data preprocessing, feature engineering,\nalgorithm selection, and hyperparameter tuning. The framework incorporates\ncapabilities for interpretability, debugging, and experiment tracking that are\noften missing in general-purpose solutions. In this study, we compare MH-AutoML\nagainst seven established AutoML frameworks: Auto-Sklearn, AutoGluon, TPOT,\nHyperGBM, Auto-PyTorch, LightAutoML, and MLJAR. Results show that MH-AutoML\nachieves better recall rates while providing more transparency and control. The\nframework maintains computational efficiency comparable to other solutions,\nmaking it suitable for cybersecurity applications where both performance and\nexplainability matter.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23314v1', 'title': 'Interpretable by Design: MH-AutoML for Transparent and Efficient Android Malware Detection without Compromising Performance', 'abstract': 'Malware detection in Android systems requires both cybersecurity expertise\\nand machine learning (ML) techniques. Automated Machine Learning (AutoML) has\\nemerged as an approach to simplify ML development by reducing the need for\\nspecialized knowledge. However, current AutoML solutions typically operate as\\nblack-box systems with limited transparency, interpretability, and experiment\\ntraceability. To address these limitations, we present MH-AutoML, a\\ndomain-specific framework for Android malware detection. MH-AutoML automates\\nthe entire ML pipeline, including data preprocessing, feature engineering,\\nalgorithm selection, and hyperparameter tuning. The framework incorporates\\ncapabilities for interpretability, debugging, and experiment tracking that are\\noften missing in general-purpose solutions. In this study, we compare MH-AutoML\\nagainst seven established AutoML frameworks: Auto-Sklearn, AutoGluon, TPOT,\\nHyperGBM, Auto-PyTorch, LightAutoML, and MLJAR. Results show that MH-AutoML\\nachieves better recall rates while providing more transparency and control. The\\nframework maintains computational efficiency comparable to other solutions,\\nmaking it suitable for cybersecurity applications where both performance and\\nexplainability matter.', 'authors': ['Joner Assolin', 'Gabriel Canto', 'Diego Kreutz', 'Eduardo Feitosa', 'Hendrio Bragan√ßa', 'Angelo Nogueira', 'Vanderson Rocha'], 'published': '2025-06-29T16:12:41+00:00', 'categories': ['cs.CR', 'cs.AI', '68T99', 'I.2'], 'url': 'http://arxiv.org/abs/2506.23314v1'}"}, {'id': 98, 'arxiv_id': '2506.23306v1', 'base_arxiv_id': '2506.23306', 'version': '1', 'title': 'GATSim: Urban Mobility Simulation with Generative Agents', 'abstract': 'Traditional agent-based urban mobility simulations rely on rigid rule-based\nsystems that fail to capture the complexity, adaptability, and behavioral\ndiversity characteristic of human travel decision-making. Recent advances in\nlarge language models and AI agent technology offer opportunities to create\nagents with reasoning capabilities, persistent memory, and adaptive learning\nmechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel\nframework that leverages these advances to create generative agents with rich\nbehavioral characteristics for urban mobility simulation. Unlike conventional\napproaches, GATSim agents possess diverse socioeconomic attributes, individual\nlifestyles, and evolving preferences that shape their mobility decisions\nthrough psychologically-informed memory systems, tool usage capabilities, and\nlifelong learning mechanisms. The main contributions of this study include: (1)\na comprehensive architecture combining an urban mobility foundation model with\nagent cognitive systems and transport simulation environment, (2) a fully\nfunctional prototype implementation, and (3) systematic validation\ndemonstrating that generative agents produce believable travel behaviors.\nThrough designed reflection processes, generative agents in this study can\ntransform specific travel experiences into generalized insights, enabling\nrealistic behavioral adaptation over time with specialized mechanisms for\nactivity planning and real-time reactive behaviors tailored to urban mobility\ncontexts. Experiments show that generative agents perform competitively with\nhuman annotators in mobility scenarios while naturally producing macroscopic\ntraffic evolution patterns. The code for the prototype system is shared at\nhttps://github.com/qiliuchn/gatsim.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23306v1', 'title': 'GATSim: Urban Mobility Simulation with Generative Agents', 'abstract': 'Traditional agent-based urban mobility simulations rely on rigid rule-based\\nsystems that fail to capture the complexity, adaptability, and behavioral\\ndiversity characteristic of human travel decision-making. Recent advances in\\nlarge language models and AI agent technology offer opportunities to create\\nagents with reasoning capabilities, persistent memory, and adaptive learning\\nmechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel\\nframework that leverages these advances to create generative agents with rich\\nbehavioral characteristics for urban mobility simulation. Unlike conventional\\napproaches, GATSim agents possess diverse socioeconomic attributes, individual\\nlifestyles, and evolving preferences that shape their mobility decisions\\nthrough psychologically-informed memory systems, tool usage capabilities, and\\nlifelong learning mechanisms. The main contributions of this study include: (1)\\na comprehensive architecture combining an urban mobility foundation model with\\nagent cognitive systems and transport simulation environment, (2) a fully\\nfunctional prototype implementation, and (3) systematic validation\\ndemonstrating that generative agents produce believable travel behaviors.\\nThrough designed reflection processes, generative agents in this study can\\ntransform specific travel experiences into generalized insights, enabling\\nrealistic behavioral adaptation over time with specialized mechanisms for\\nactivity planning and real-time reactive behaviors tailored to urban mobility\\ncontexts. Experiments show that generative agents perform competitively with\\nhuman annotators in mobility scenarios while naturally producing macroscopic\\ntraffic evolution patterns. The code for the prototype system is shared at\\nhttps://github.com/qiliuchn/gatsim.', 'authors': ['Qi Liu', 'Can Li', 'Wanjing Ma'], 'published': '2025-06-29T15:52:16+00:00', 'categories': ['cs.AI'], 'url': 'http://arxiv.org/abs/2506.23306v1'}"}, {'id': 99, 'arxiv_id': '2506.23296v1', 'base_arxiv_id': '2506.23296', 'version': '1', 'title': 'Securing AI Systems: A Guide to Known Attacks and Impacts', 'abstract': 'Embedded into information systems, artificial intelligence (AI) faces\nsecurity threats that exploit AI-specific vulnerabilities. This paper provides\nan accessible overview of adversarial attacks unique to predictive and\ngenerative AI systems. We identify eleven major attack types and explicitly\nlink attack techniques to their impacts -- including information leakage,\nsystem compromise, and resource exhaustion -- mapped to the confidentiality,\nintegrity, and availability (CIA) security triad. We aim to equip researchers,\ndevelopers, security practitioners, and policymakers, even those without\nspecialized AI security expertise, with foundational knowledge to recognize\nAI-specific risks and implement effective defenses, thereby enhancing the\noverall security posture of AI systems.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23296v1', 'title': 'Securing AI Systems: A Guide to Known Attacks and Impacts', 'abstract': 'Embedded into information systems, artificial intelligence (AI) faces\\nsecurity threats that exploit AI-specific vulnerabilities. This paper provides\\nan accessible overview of adversarial attacks unique to predictive and\\ngenerative AI systems. We identify eleven major attack types and explicitly\\nlink attack techniques to their impacts -- including information leakage,\\nsystem compromise, and resource exhaustion -- mapped to the confidentiality,\\nintegrity, and availability (CIA) security triad. We aim to equip researchers,\\ndevelopers, security practitioners, and policymakers, even those without\\nspecialized AI security expertise, with foundational knowledge to recognize\\nAI-specific risks and implement effective defenses, thereby enhancing the\\noverall security posture of AI systems.', 'authors': ['Naoto Kiribuchi', 'Kengo Zenitani', 'Takayuki Semitsu'], 'published': '2025-06-29T15:32:03+00:00', 'categories': ['cs.CR', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23296v1'}"}, {'id': 100, 'arxiv_id': '2506.23293v1', 'base_arxiv_id': '2506.23293', 'version': '1', 'title': 'Objective-Free Local Learning and Emergent Language Structure in Thinking Machines', 'abstract': 'We present a neuro-symbolic framework for generative language modeling based\non local, event-driven emergent learning. At its core is a hierarchical\nHopfield memory chain acting as a compositional short-term memory and dynamic\ntokenizer (retokenizer). Rather than relying on predefined tokens or\nsupervision, the model builds structure from scratch, learning symbol sequences\nas multi-scale representations. It constructs projection tensors that bind\nco-occurring features into hierarchical tokens, introducing redundancy (i.e an\nemergent gauge structure) and enabling compression of local activations into\nlong-range dependencies. Curiously, we find that the retokenizer can filter\nnatural language patterns from noise, generating synthetic languages with\ncoherent internal morphology -- quantifiably the same as human language.\nLanguage is learned in a local (Hebbian) fashion, where model constraints\ndictate allowed emergent structure, and new information is retained in\nalignment with this structure. The absence of a global objective enables a form\nof plasticity not found in conventional language models, allowing the system to\ngeneralize beyond its initial inference class -- even without explicit data. We\ndemonstrate that briefly activating a new neuron during inference binds\ndistributed multi-scale token features into a symbolic embedding. These\nemergent embedding neurons act as long-term memory and support a key-value\nmechanism for compositional inference and generalization. This architecture\nprovides a methodological foundation for studying how symbolic structure can\nemerge from local neural learning. It offers a new pathway for building\nscalable, interpretable neuro-symbolic systems -- where tokens, grammar, and\nreasoning arise as compressed memory traces within a Hopfield hierarchy. This\napproach advances the development of neuromorphic architectures for generative\nlanguage models.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23293v1', 'title': 'Objective-Free Local Learning and Emergent Language Structure in Thinking Machines', 'abstract': 'We present a neuro-symbolic framework for generative language modeling based\\non local, event-driven emergent learning. At its core is a hierarchical\\nHopfield memory chain acting as a compositional short-term memory and dynamic\\ntokenizer (retokenizer). Rather than relying on predefined tokens or\\nsupervision, the model builds structure from scratch, learning symbol sequences\\nas multi-scale representations. It constructs projection tensors that bind\\nco-occurring features into hierarchical tokens, introducing redundancy (i.e an\\nemergent gauge structure) and enabling compression of local activations into\\nlong-range dependencies. Curiously, we find that the retokenizer can filter\\nnatural language patterns from noise, generating synthetic languages with\\ncoherent internal morphology -- quantifiably the same as human language.\\nLanguage is learned in a local (Hebbian) fashion, where model constraints\\ndictate allowed emergent structure, and new information is retained in\\nalignment with this structure. The absence of a global objective enables a form\\nof plasticity not found in conventional language models, allowing the system to\\ngeneralize beyond its initial inference class -- even without explicit data. We\\ndemonstrate that briefly activating a new neuron during inference binds\\ndistributed multi-scale token features into a symbolic embedding. These\\nemergent embedding neurons act as long-term memory and support a key-value\\nmechanism for compositional inference and generalization. This architecture\\nprovides a methodological foundation for studying how symbolic structure can\\nemerge from local neural learning. It offers a new pathway for building\\nscalable, interpretable neuro-symbolic systems -- where tokens, grammar, and\\nreasoning arise as compressed memory traces within a Hopfield hierarchy. This\\napproach advances the development of neuromorphic architectures for generative\\nlanguage models.', 'authors': ['P. Myles Eugenio'], 'published': '2025-06-29T15:29:13+00:00', 'categories': ['cs.CL', 'cs.AI', 'cs.LG', 'q-bio.NC'], 'url': 'http://arxiv.org/abs/2506.23293v1'}"}, {'id': 101, 'arxiv_id': '2506.23286v1', 'base_arxiv_id': '2506.23286', 'version': '1', 'title': 'Not All Explanations for Deep Learning Phenomena Are Equally Valuable', 'abstract': 'Developing a better understanding of surprising or counterintuitive phenomena\nhas constituted a significant portion of deep learning research in recent\nyears. These include double descent, grokking, and the lottery ticket\nhypothesis -- among many others. Works in this area often develop ad hoc\nhypotheses attempting to explain these observed phenomena on an isolated,\ncase-by-case basis. This position paper asserts that, in many prominent cases,\nthere is little evidence to suggest that these phenomena appear in real-world\napplications and these efforts may be inefficient in driving progress in the\nbroader field. Consequently, we argue against viewing them as isolated puzzles\nthat require bespoke resolutions or explanations. However, despite this, we\nsuggest that deep learning phenomena do still offer research value by providing\nunique settings in which we can refine our broad explanatory theories of more\ngeneral deep learning principles. This position is reinforced by analyzing the\nresearch outcomes of several prominent examples of these phenomena from the\nrecent literature. We revisit the current norms in the research community in\napproaching these problems and propose practical recommendations for future\nresearch, aiming to ensure that progress on deep learning phenomena is well\naligned with the ultimate pragmatic goal of progress in the broader field of\ndeep learning.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23286v1', 'title': 'Not All Explanations for Deep Learning Phenomena Are Equally Valuable', 'abstract': 'Developing a better understanding of surprising or counterintuitive phenomena\\nhas constituted a significant portion of deep learning research in recent\\nyears. These include double descent, grokking, and the lottery ticket\\nhypothesis -- among many others. Works in this area often develop ad hoc\\nhypotheses attempting to explain these observed phenomena on an isolated,\\ncase-by-case basis. This position paper asserts that, in many prominent cases,\\nthere is little evidence to suggest that these phenomena appear in real-world\\napplications and these efforts may be inefficient in driving progress in the\\nbroader field. Consequently, we argue against viewing them as isolated puzzles\\nthat require bespoke resolutions or explanations. However, despite this, we\\nsuggest that deep learning phenomena do still offer research value by providing\\nunique settings in which we can refine our broad explanatory theories of more\\ngeneral deep learning principles. This position is reinforced by analyzing the\\nresearch outcomes of several prominent examples of these phenomena from the\\nrecent literature. We revisit the current norms in the research community in\\napproaching these problems and propose practical recommendations for future\\nresearch, aiming to ensure that progress on deep learning phenomena is well\\naligned with the ultimate pragmatic goal of progress in the broader field of\\ndeep learning.', 'authors': ['Alan Jeffares', 'Mihaela van der Schaar'], 'published': '2025-06-29T15:18:56+00:00', 'categories': ['cs.LG', 'cs.AI', 'stat.ML'], 'url': 'http://arxiv.org/abs/2506.23286v1'}"}, {'id': 102, 'arxiv_id': '2506.23276v1', 'base_arxiv_id': '2506.23276', 'version': '1', 'title': 'Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games', 'abstract': 'As large language models (LLMs) are increasingly deployed as autonomous\nagents, understanding their cooperation and social mechanisms is becoming\nincreasingly important. In particular, how LLMs balance self-interest and\ncollective well-being is a critical challenge for ensuring alignment,\nrobustness, and safe deployment. In this paper, we examine the challenge of\ncostly sanctioning in multi-agent LLM systems, where an agent must decide\nwhether to invest its own resources to incentivize cooperation or penalize\ndefection. To study this, we adapt a public goods game with institutional\nchoice from behavioral economics, allowing us to observe how different LLMs\nnavigate social dilemmas over repeated interactions. Our analysis reveals four\ndistinct behavioral patterns among models: some consistently establish and\nsustain high levels of cooperation, others fluctuate between engagement and\ndisengagement, some gradually decline in cooperative behavior over time, and\nothers rigidly follow fixed strategies regardless of outcomes. Surprisingly, we\nfind that reasoning LLMs, such as the o1 series, struggle significantly with\ncooperation, whereas some traditional LLMs consistently achieve high levels of\ncooperation. These findings suggest that the current approach to improving\nLLMs, which focuses on enhancing their reasoning capabilities, does not\nnecessarily lead to cooperation, providing valuable insights for deploying LLM\nagents in environments that require sustained collaboration. Our code is\navailable at https://github.com/davidguzmanp/SanctSim', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23276v1', 'title': 'Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games', 'abstract': 'As large language models (LLMs) are increasingly deployed as autonomous\\nagents, understanding their cooperation and social mechanisms is becoming\\nincreasingly important. In particular, how LLMs balance self-interest and\\ncollective well-being is a critical challenge for ensuring alignment,\\nrobustness, and safe deployment. In this paper, we examine the challenge of\\ncostly sanctioning in multi-agent LLM systems, where an agent must decide\\nwhether to invest its own resources to incentivize cooperation or penalize\\ndefection. To study this, we adapt a public goods game with institutional\\nchoice from behavioral economics, allowing us to observe how different LLMs\\nnavigate social dilemmas over repeated interactions. Our analysis reveals four\\ndistinct behavioral patterns among models: some consistently establish and\\nsustain high levels of cooperation, others fluctuate between engagement and\\ndisengagement, some gradually decline in cooperative behavior over time, and\\nothers rigidly follow fixed strategies regardless of outcomes. Surprisingly, we\\nfind that reasoning LLMs, such as the o1 series, struggle significantly with\\ncooperation, whereas some traditional LLMs consistently achieve high levels of\\ncooperation. These findings suggest that the current approach to improving\\nLLMs, which focuses on enhancing their reasoning capabilities, does not\\nnecessarily lead to cooperation, providing valuable insights for deploying LLM\\nagents in environments that require sustained collaboration. Our code is\\navailable at https://github.com/davidguzmanp/SanctSim', 'authors': ['David Guzman Piedrahita', 'Yongjin Yang', 'Mrinmaya Sachan', 'Giorgia Ramponi', 'Bernhard Sch√∂lkopf', 'Zhijing Jin'], 'published': '2025-06-29T15:02:47+00:00', 'categories': ['cs.AI', 'cs.CL'], 'url': 'http://arxiv.org/abs/2506.23276v1'}"}, {'id': 105, 'arxiv_id': '2506.23273v1', 'base_arxiv_id': '2506.23273', 'version': '1', 'title': 'FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis', 'abstract': 'Despite the advancements of large language models, text2sql still faces many\nchallenges, particularly with complex and domain-specific queries. In finance,\ndatabase designs and financial reporting layouts vary widely between financial\nentities and countries, making text2sql even more challenging. We present\nFinStat2SQL, a lightweight text2sql pipeline enabling natural language queries\nover financial statements. Tailored to local standards like VAS, it combines\nlarge and small language models in a multi-agent setup for entity extraction,\nSQL generation, and self-correction. We build a domain-specific database and\nevaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves\n61.33\\% accuracy with sub-4-second response times on consumer hardware,\noutperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient\nsolution for financial analysis, making AI-powered querying accessible to\nVietnamese enterprises.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23273v1', 'title': 'FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis', 'abstract': 'Despite the advancements of large language models, text2sql still faces many\\nchallenges, particularly with complex and domain-specific queries. In finance,\\ndatabase designs and financial reporting layouts vary widely between financial\\nentities and countries, making text2sql even more challenging. We present\\nFinStat2SQL, a lightweight text2sql pipeline enabling natural language queries\\nover financial statements. Tailored to local standards like VAS, it combines\\nlarge and small language models in a multi-agent setup for entity extraction,\\nSQL generation, and self-correction. We build a domain-specific database and\\nevaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves\\n61.33\\\\% accuracy with sub-4-second response times on consumer hardware,\\noutperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient\\nsolution for financial analysis, making AI-powered querying accessible to\\nVietnamese enterprises.', 'authors': ['Quang Hung Nguyen', 'Phuong Anh Trinh', 'Phan Quoc Hung Mai', 'Tuan Phong Trinh'], 'published': '2025-06-29T14:55:21+00:00', 'categories': ['cs.AI'], 'url': 'http://arxiv.org/abs/2506.23273v1'}"}, {'id': 106, 'arxiv_id': '2506.23270v1', 'base_arxiv_id': '2506.23270', 'version': '1', 'title': 'Token Activation Map to Visually Explain Multimodal LLMs', 'abstract': 'Multimodal large language models (MLLMs) are broadly empowering various\nfields. Despite their advancements, the explainability of MLLMs remains less\nexplored, hindering deeper understanding, model credibility, and effective\nvisualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that\nproduce a single output, MLLMs generate sequences of tokens progressively,\nwhere each generated token depends on the previous context. Therefore, earlier\ncontext tokens can introduce redundant activations that interfere with the\nexplanation of later tokens beyond their original information. Existing studies\noften overlook this issue, but our observations reveal that these redundant\ncorrelations can significantly hurt the reliability of explanations. To address\nthis, we propose an estimated causal inference method to mitigate the\ninterference of context to achieve high-quality MLLM explanation, with a novel\nrank Gaussian filter to further reduce activation noises. We term this method\nToken Activation Map (TAM) to highlight the consideration of interactions\nbetween tokens. TAM also indicates that it excels at explaining multiple tokens\nof MLLM, which is different from the Class Activation Map (CAM) for a single\nprediction. Our TAM method significantly outperforms existing SoTA methods,\nshowcasing high-quality visualization results that can be utilized for various\nscenarios, such as object localization, failure case analysis, video\nvisualization, MLLMs visual comparison, and model understanding (e.g., color,\nshape, action, location, visual reasoning, multi-turn conversation, etc). The\ncode is available atgithub.com/xmed-lab/TAM.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23270v1', 'title': 'Token Activation Map to Visually Explain Multimodal LLMs', 'abstract': 'Multimodal large language models (MLLMs) are broadly empowering various\\nfields. Despite their advancements, the explainability of MLLMs remains less\\nexplored, hindering deeper understanding, model credibility, and effective\\nvisualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that\\nproduce a single output, MLLMs generate sequences of tokens progressively,\\nwhere each generated token depends on the previous context. Therefore, earlier\\ncontext tokens can introduce redundant activations that interfere with the\\nexplanation of later tokens beyond their original information. Existing studies\\noften overlook this issue, but our observations reveal that these redundant\\ncorrelations can significantly hurt the reliability of explanations. To address\\nthis, we propose an estimated causal inference method to mitigate the\\ninterference of context to achieve high-quality MLLM explanation, with a novel\\nrank Gaussian filter to further reduce activation noises. We term this method\\nToken Activation Map (TAM) to highlight the consideration of interactions\\nbetween tokens. TAM also indicates that it excels at explaining multiple tokens\\nof MLLM, which is different from the Class Activation Map (CAM) for a single\\nprediction. Our TAM method significantly outperforms existing SoTA methods,\\nshowcasing high-quality visualization results that can be utilized for various\\nscenarios, such as object localization, failure case analysis, video\\nvisualization, MLLMs visual comparison, and model understanding (e.g., color,\\nshape, action, location, visual reasoning, multi-turn conversation, etc). The\\ncode is available atgithub.com/xmed-lab/TAM.', 'authors': ['Yi Li', 'Hualiang Wang', 'Xinpeng Ding', 'Haonan Wang', 'Xiaomeng Li'], 'published': '2025-06-29T14:50:45+00:00', 'categories': ['cs.CV', 'cs.AI', 'cs.LG'], 'url': 'http://arxiv.org/abs/2506.23270v1'}"}, {'id': 107, 'arxiv_id': '2506.23260v1', 'base_arxiv_id': '2506.23260', 'version': '1', 'title': 'From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows', 'abstract': 'Autonomous AI agents powered by large language models (LLMs) with structured\nfunction-calling interfaces have dramatically expanded capabilities for\nreal-time data retrieval, complex computation, and multi-step orchestration.\nYet, the explosive proliferation of plugins, connectors, and inter-agent\nprotocols has outpaced discovery mechanisms and security practices, resulting\nin brittle integrations vulnerable to diverse threats. In this survey, we\nintroduce the first unified, end-to-end threat model for LLM-agent ecosystems,\nspanning host-to-tool and agent-to-agent communications, formalize adversary\ncapabilities and attacker objectives, and catalog over thirty attack\ntechniques. Specifically, we organized the threat model into four domains:\nInput Manipulation (e.g., prompt injections, long-context hijacks, multimodal\nadversarial inputs), Model Compromise (e.g., prompt- and parameter-level\nbackdoors, composite and encrypted multi-backdoors, poisoning strategies),\nSystem and Privacy Attacks (e.g., speculative side-channels, membership\ninference, retrieval poisoning, social-engineering simulations), and Protocol\nVulnerabilities (e.g., exploits in Model Context Protocol (MCP), Agent\nCommunication Protocol (ACP), Agent Network Protocol (ANP), and Agent-to-Agent\n(A2A) protocol). For each category, we review representative scenarios, assess\nreal-world feasibility, and evaluate existing defenses. Building on our threat\ntaxonomy, we identify key open challenges and future research directions, such\nas securing MCP deployments through dynamic trust management and cryptographic\nprovenance tracking; designing and hardening Agentic Web Interfaces; and\nachieving resilience in multi-agent and federated environments. Our work\nprovides a comprehensive reference to guide the design of robust defense\nmechanisms and establish best practices for resilient LLM-agent workflows.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23260v1', 'title': 'From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows', 'abstract': 'Autonomous AI agents powered by large language models (LLMs) with structured\\nfunction-calling interfaces have dramatically expanded capabilities for\\nreal-time data retrieval, complex computation, and multi-step orchestration.\\nYet, the explosive proliferation of plugins, connectors, and inter-agent\\nprotocols has outpaced discovery mechanisms and security practices, resulting\\nin brittle integrations vulnerable to diverse threats. In this survey, we\\nintroduce the first unified, end-to-end threat model for LLM-agent ecosystems,\\nspanning host-to-tool and agent-to-agent communications, formalize adversary\\ncapabilities and attacker objectives, and catalog over thirty attack\\ntechniques. Specifically, we organized the threat model into four domains:\\nInput Manipulation (e.g., prompt injections, long-context hijacks, multimodal\\nadversarial inputs), Model Compromise (e.g., prompt- and parameter-level\\nbackdoors, composite and encrypted multi-backdoors, poisoning strategies),\\nSystem and Privacy Attacks (e.g., speculative side-channels, membership\\ninference, retrieval poisoning, social-engineering simulations), and Protocol\\nVulnerabilities (e.g., exploits in Model Context Protocol (MCP), Agent\\nCommunication Protocol (ACP), Agent Network Protocol (ANP), and Agent-to-Agent\\n(A2A) protocol). For each category, we review representative scenarios, assess\\nreal-world feasibility, and evaluate existing defenses. Building on our threat\\ntaxonomy, we identify key open challenges and future research directions, such\\nas securing MCP deployments through dynamic trust management and cryptographic\\nprovenance tracking; designing and hardening Agentic Web Interfaces; and\\nachieving resilience in multi-agent and federated environments. Our work\\nprovides a comprehensive reference to guide the design of robust defense\\nmechanisms and establish best practices for resilient LLM-agent workflows.', 'authors': ['Mohamed Amine Ferrag', 'Norbert Tihanyi', 'Djallel Hamouda', 'Leandros Maglaras', 'Merouane Debbah'], 'published': '2025-06-29T14:32:32+00:00', 'categories': ['cs.CR', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23260v1'}"}, {'id': 108, 'arxiv_id': '2506.23247v1', 'base_arxiv_id': '2506.23247', 'version': '1', 'title': 'Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification', 'abstract': 'Deep learning dominates image classification tasks, yet understanding how\nmodels arrive at predictions remains a challenge. Much research focuses on\nlocal explanations of individual predictions, such as saliency maps, which\nvisualise the influence of specific pixels on a model\'s prediction. However,\nreviewing many of these explanations to identify recurring patterns is\ninfeasible, while global methods often oversimplify and miss important local\nbehaviours. To address this, we propose Segment Attribution Tables (SATs), a\nmethod for summarising local saliency explanations into (semi-)global insights.\nSATs take image segments (such as "eyes" in Chihuahuas) and leverage saliency\nmaps to quantify their influence. These segments highlight concepts the model\nrelies on across instances and reveal spurious correlations, such as reliance\non backgrounds or watermarks, even when out-of-distribution test performance\nsees little change. SATs can explain any classifier for which a form of\nsaliency map can be produced, using segmentation maps that provide named\nsegments. SATs bridge the gap between oversimplified global summaries and\noverly detailed local explanations, offering a practical tool for analysing and\ndebugging image classifiers.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': '{\'id\': \'2506.23247v1\', \'title\': \'Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification\', \'abstract\': \'Deep learning dominates image classification tasks, yet understanding how\\nmodels arrive at predictions remains a challenge. Much research focuses on\\nlocal explanations of individual predictions, such as saliency maps, which\\nvisualise the influence of specific pixels on a model\\\'s prediction. However,\\nreviewing many of these explanations to identify recurring patterns is\\ninfeasible, while global methods often oversimplify and miss important local\\nbehaviours. To address this, we propose Segment Attribution Tables (SATs), a\\nmethod for summarising local saliency explanations into (semi-)global insights.\\nSATs take image segments (such as "eyes" in Chihuahuas) and leverage saliency\\nmaps to quantify their influence. These segments highlight concepts the model\\nrelies on across instances and reveal spurious correlations, such as reliance\\non backgrounds or watermarks, even when out-of-distribution test performance\\nsees little change. SATs can explain any classifier for which a form of\\nsaliency map can be produced, using segmentation maps that provide named\\nsegments. SATs bridge the gap between oversimplified global summaries and\\noverly detailed local explanations, offering a practical tool for analysing and\\ndebugging image classifiers.\', \'authors\': [\'James Hinns\', \'David Martens\'], \'published\': \'2025-06-29T14:11:02+00:00\', \'categories\': [\'cs.CV\', \'cs.AI\', \'cs.LG\'], \'url\': \'http://arxiv.org/abs/2506.23247v1\'}'}, {'id': 109, 'arxiv_id': '2506.23236v1', 'base_arxiv_id': '2506.23236', 'version': '1', 'title': 'VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions', 'abstract': "Parametric human body models play a crucial role in computer graphics and\nvision, enabling applications ranging from human motion analysis to\nunderstanding human-environment interactions. Traditionally, these models use\nsurface meshes, which pose challenges in efficiently handling interactions with\nother geometric entities, such as objects and scenes, typically represented as\nmeshes or point clouds. To address this limitation, recent research has\nexplored volumetric neural implicit body models. However, existing works are\neither insufficiently robust for complex human articulations or impose high\ncomputational and memory costs, limiting their widespread use. To this end, we\nintroduce VolumetricSMPL, a neural volumetric body model that leverages Neural\nBlend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike\nprior approaches that rely on large MLPs, NBW dynamically blends a small set of\nlearned weight matrices using predicted shape- and pose-dependent coefficients,\nsignificantly improving computational efficiency while preserving\nexpressiveness. VolumetricSMPL outperforms prior volumetric occupancy model\nCOAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy,\nand a Signed Distance Function (SDF) for efficient and differentiable contact\nmodeling. We demonstrate VolumetricSMPL's strengths across four challenging\ntasks: (1) reconstructing human-object interactions from in-the-wild images,\n(2) recovering human meshes in 3D scenes from egocentric views, (3)\nscene-constrained motion synthesis, and (4) resolving self-intersections. Our\nresults highlight its broad applicability and significant performance and\nefficiency gains.", 'submit_date': datetime.date(2025, 6, 29), 'metadata': '{\'id\': \'2506.23236v1\', \'title\': \'VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions\', \'abstract\': "Parametric human body models play a crucial role in computer graphics and\\nvision, enabling applications ranging from human motion analysis to\\nunderstanding human-environment interactions. Traditionally, these models use\\nsurface meshes, which pose challenges in efficiently handling interactions with\\nother geometric entities, such as objects and scenes, typically represented as\\nmeshes or point clouds. To address this limitation, recent research has\\nexplored volumetric neural implicit body models. However, existing works are\\neither insufficiently robust for complex human articulations or impose high\\ncomputational and memory costs, limiting their widespread use. To this end, we\\nintroduce VolumetricSMPL, a neural volumetric body model that leverages Neural\\nBlend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike\\nprior approaches that rely on large MLPs, NBW dynamically blends a small set of\\nlearned weight matrices using predicted shape- and pose-dependent coefficients,\\nsignificantly improving computational efficiency while preserving\\nexpressiveness. VolumetricSMPL outperforms prior volumetric occupancy model\\nCOAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy,\\nand a Signed Distance Function (SDF) for efficient and differentiable contact\\nmodeling. We demonstrate VolumetricSMPL\'s strengths across four challenging\\ntasks: (1) reconstructing human-object interactions from in-the-wild images,\\n(2) recovering human meshes in 3D scenes from egocentric views, (3)\\nscene-constrained motion synthesis, and (4) resolving self-intersections. Our\\nresults highlight its broad applicability and significant performance and\\nefficiency gains.", \'authors\': [\'Marko Mihajlovic\', \'Siwei Zhang\', \'Gen Li\', \'Kaifeng Zhao\', \'Lea M√ºller\', \'Siyu Tang\'], \'published\': \'2025-06-29T13:48:38+00:00\', \'categories\': [\'cs.CV\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23236v1\'}'}, {'id': 110, 'arxiv_id': '2506.23219v1', 'base_arxiv_id': '2506.23219', 'version': '1', 'title': 'UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding', 'abstract': 'Urban research involves a wide range of scenarios and tasks that require the\nunderstanding of multi-modal data. Current methods often focus on specific data\ntypes and lack a unified framework in urban field for processing them\ncomprehensively. The recent success of multi-modal large language models\n(MLLMs) presents a promising opportunity to overcome this limitation. In this\npaper, we introduce $UrbanLLaVA$, a multi-modal large language model\ndesigned to process these four types of data simultaneously and achieve strong\nperformance across diverse urban tasks compared with general MLLMs. In\n$UrbanLLaVA$, we first curate a diverse urban instruction dataset\nencompassing both single-modal and cross-modal urban data, spanning from\nlocation view to global view of urban environment. Additionally, we propose a\nmulti-stage training framework that decouples spatial reasoning enhancement\nfrom domain knowledge learning, thereby improving the compatibility and\ndownstream performance of $UrbanLLaVA$ across diverse urban tasks.\nFinally, we also extend existing benchmark for urban research to assess the\nperformance of MLLMs across a wide range of urban tasks. Experimental results\nfrom three cities demonstrate that $UrbanLLaVA$ outperforms\nopen-source and proprietary MLLMs in both single-modal tasks and complex\ncross-modal tasks and shows robust generalization abilities across cities.\nSource codes and data are openly accessible to the research community via\nhttps://github.com/tsinghua-fib-lab/UrbanLLaVA.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23219v1', 'title': 'UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding', 'abstract': 'Urban research involves a wide range of scenarios and tasks that require the\\nunderstanding of multi-modal data. Current methods often focus on specific data\\ntypes and lack a unified framework in urban field for processing them\\ncomprehensively. The recent success of multi-modal large language models\\n(MLLMs) presents a promising opportunity to overcome this limitation. In this\\npaper, we introduce $\\\\textit{UrbanLLaVA}$, a multi-modal large language model\\ndesigned to process these four types of data simultaneously and achieve strong\\nperformance across diverse urban tasks compared with general MLLMs. In\\n$\\\\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset\\nencompassing both single-modal and cross-modal urban data, spanning from\\nlocation view to global view of urban environment. Additionally, we propose a\\nmulti-stage training framework that decouples spatial reasoning enhancement\\nfrom domain knowledge learning, thereby improving the compatibility and\\ndownstream performance of $\\\\textit{UrbanLLaVA}$ across diverse urban tasks.\\nFinally, we also extend existing benchmark for urban research to assess the\\nperformance of MLLMs across a wide range of urban tasks. Experimental results\\nfrom three cities demonstrate that $\\\\textit{UrbanLLaVA}$ outperforms\\nopen-source and proprietary MLLMs in both single-modal tasks and complex\\ncross-modal tasks and shows robust generalization abilities across cities.\\nSource codes and data are openly accessible to the research community via\\nhttps://github.com/tsinghua-fib-lab/UrbanLLaVA.', 'authors': ['Jie Feng', 'Shengyuan Wang', 'Tianhui Liu', 'Yanxin Xi', 'Yong Li'], 'published': '2025-06-29T13:04:27+00:00', 'categories': ['cs.CV', 'cs.AI', 'cs.CL'], 'url': 'http://arxiv.org/abs/2506.23219v1'}"}, {'id': 111, 'arxiv_id': '2506.23210v1', 'base_arxiv_id': '2506.23210', 'version': '1', 'title': 'FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model', 'abstract': "Federated learning(FL) is used for distributed scenarios to train artificial\nintelligence(AI) models while ensuring users' privacy. In federated learning\nscenario, the server generally never knows about users' data. This type of\nconcept makes the AI training process efficient in terms of data privacy.\nHowever, regarding model performance, federated AI models may not sufficiently\nsatisfy AI users' expectations. Furthermore, AI users have a wide range of\ndifferent needs. It is not easy to satisfy the whole users needs. These types\nof issues can be addressed through AI model optimization, fine-tuning, or\npersonalization to achieve optimal model performance. To address model\noptimization challenges, we propose reference model-based federated learning\nfor optimal fine-tuning, which overcomes catastrophic forgetting in each round.\nThis method is derived from Bayesian parameter-efficient transfer learning,\nwhich includes an optimal proximal term and enables overcoming the catastrophic\nforgetting issue in each round by utilizing a reference model that incorporates\nprevious model parameters. As a result, this method achieves both high model\nperformance and low computing cost.", 'submit_date': datetime.date(2025, 6, 29), 'metadata': '{\'id\': \'2506.23210v1\', \'title\': \'FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model\', \'abstract\': "Federated learning(FL) is used for distributed scenarios to train artificial\\nintelligence(AI) models while ensuring users\' privacy. In federated learning\\nscenario, the server generally never knows about users\' data. This type of\\nconcept makes the AI training process efficient in terms of data privacy.\\nHowever, regarding model performance, federated AI models may not sufficiently\\nsatisfy AI users\' expectations. Furthermore, AI users have a wide range of\\ndifferent needs. It is not easy to satisfy the whole users needs. These types\\nof issues can be addressed through AI model optimization, fine-tuning, or\\npersonalization to achieve optimal model performance. To address model\\noptimization challenges, we propose reference model-based federated learning\\nfor optimal fine-tuning, which overcomes catastrophic forgetting in each round.\\nThis method is derived from Bayesian parameter-efficient transfer learning,\\nwhich includes an optimal proximal term and enables overcoming the catastrophic\\nforgetting issue in each round by utilizing a reference model that incorporates\\nprevious model parameters. As a result, this method achieves both high model\\nperformance and low computing cost.", \'authors\': [\'Taehwan Yoon\', \'Bongjun Choi\'], \'published\': \'2025-06-29T12:41:11+00:00\', \'categories\': [\'cs.LG\', \'cs.AI\', \'cs.DC\'], \'url\': \'http://arxiv.org/abs/2506.23210v1\'}'}, {'id': 112, 'arxiv_id': '2506.23203v1', 'base_arxiv_id': '2506.23203', 'version': '1', 'title': 'Multi-Branch DNN and CRLB-Ratio-Weight Fusion for Enhanced DOA Sensing via a Massive H$^2$AD MIMO Receiver', 'abstract': 'As a green MIMO structure, massive H$^2$AD is viewed as a potential\ntechnology for the future 6G wireless network. For such a structure, it is a\nchallenging task to design a low-complexity and high-performance fusion of\ntarget direction values sensed by different sub-array groups with fewer use of\nprior knowledge. To address this issue, a lightweight Cramer-Rao lower bound\n(CRLB)-ratio-weight fusion (WF) method is proposed, which approximates inverse\nCRLB of each subarray using antenna number reciprocals to eliminate real-time\nCRLB computation. This reduces complexity and prior knowledge dependence while\npreserving fusion performance. Moreover, a multi-branch deep neural network\n(MBDNN) is constructed to further enhance direction-of-arrival (DOA) sensing by\nleveraging candidate angles from multiple subarrays. The subarray-specific\nbranch networks are integrated with a shared regression module to effectively\neliminate pseudo-solutions and fuse true angles. Simulation results show that\nthe proposed CRLB-ratio-WF method achieves DOA sensing performance comparable\nto CRLB-based methods, while significantly reducing the reliance on prior\nknowledge. More notably, the proposed MBDNN has superior performance in low-SNR\nranges. At SNR $= -15$ dB, it achieves an order-of-magnitude improvement in\nestimation accuracy compared to CRLB-ratio-WF method.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23203v1', 'title': 'Multi-Branch DNN and CRLB-Ratio-Weight Fusion for Enhanced DOA Sensing via a Massive H$^2$AD MIMO Receiver', 'abstract': 'As a green MIMO structure, massive H$^2$AD is viewed as a potential\\ntechnology for the future 6G wireless network. For such a structure, it is a\\nchallenging task to design a low-complexity and high-performance fusion of\\ntarget direction values sensed by different sub-array groups with fewer use of\\nprior knowledge. To address this issue, a lightweight Cramer-Rao lower bound\\n(CRLB)-ratio-weight fusion (WF) method is proposed, which approximates inverse\\nCRLB of each subarray using antenna number reciprocals to eliminate real-time\\nCRLB computation. This reduces complexity and prior knowledge dependence while\\npreserving fusion performance. Moreover, a multi-branch deep neural network\\n(MBDNN) is constructed to further enhance direction-of-arrival (DOA) sensing by\\nleveraging candidate angles from multiple subarrays. The subarray-specific\\nbranch networks are integrated with a shared regression module to effectively\\neliminate pseudo-solutions and fuse true angles. Simulation results show that\\nthe proposed CRLB-ratio-WF method achieves DOA sensing performance comparable\\nto CRLB-based methods, while significantly reducing the reliance on prior\\nknowledge. More notably, the proposed MBDNN has superior performance in low-SNR\\nranges. At SNR $= -15$ dB, it achieves an order-of-magnitude improvement in\\nestimation accuracy compared to CRLB-ratio-WF method.', 'authors': ['Feng Shu', 'Jiatong Bai', 'Di Wu', 'Wei Zhu', 'Bin Deng', 'Fuhui Zhou', 'Jiangzhou Wang'], 'published': '2025-06-29T12:14:59+00:00', 'categories': ['eess.SP', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23203v1'}"}, {'id': 113, 'arxiv_id': '2506.23184v1', 'base_arxiv_id': '2506.23184', 'version': '1', 'title': 'Score-based Diffusion Model for Unpaired Virtual Histology Staining', 'abstract': 'Hematoxylin and eosin (H&E) staining visualizes histology but lacks\nspecificity for diagnostic markers. Immunohistochemistry (IHC) staining\nprovides protein-targeted staining but is restricted by tissue availability and\nantibody specificity. Virtual staining, i.e., computationally translating the\nH&E image to its IHC counterpart while preserving the tissue structure, is\npromising for efficient IHC generation. Existing virtual staining methods still\nface key challenges: 1) effective decomposition of staining style and tissue\nstructure, 2) controllable staining process adaptable to diverse tissue and\nproteins, and 3) rigorous structural consistency modelling to handle the\nnon-pixel-aligned nature of paired H&E and IHC images. This study proposes a\nmutual-information (MI)-guided score-based diffusion model for unpaired virtual\nstaining. Specifically, we design 1) a global MI-guided energy function that\ndisentangles the tissue structure and staining characteristics across\nmodalities, 2) a novel timestep-customized reverse diffusion process for\nprecise control of the staining intensity and structural reconstruction, and 3)\na local MI-driven contrastive learning strategy to ensure the cellular level\nstructural consistency between H&E-IHC images. Extensive experiments\ndemonstrate the our superiority over state-of-the-art approaches, highlighting\nits biomedical potential. Codes will be open-sourced upon acceptance.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23184v1', 'title': 'Score-based Diffusion Model for Unpaired Virtual Histology Staining', 'abstract': 'Hematoxylin and eosin (H&E) staining visualizes histology but lacks\\nspecificity for diagnostic markers. Immunohistochemistry (IHC) staining\\nprovides protein-targeted staining but is restricted by tissue availability and\\nantibody specificity. Virtual staining, i.e., computationally translating the\\nH&E image to its IHC counterpart while preserving the tissue structure, is\\npromising for efficient IHC generation. Existing virtual staining methods still\\nface key challenges: 1) effective decomposition of staining style and tissue\\nstructure, 2) controllable staining process adaptable to diverse tissue and\\nproteins, and 3) rigorous structural consistency modelling to handle the\\nnon-pixel-aligned nature of paired H&E and IHC images. This study proposes a\\nmutual-information (MI)-guided score-based diffusion model for unpaired virtual\\nstaining. Specifically, we design 1) a global MI-guided energy function that\\ndisentangles the tissue structure and staining characteristics across\\nmodalities, 2) a novel timestep-customized reverse diffusion process for\\nprecise control of the staining intensity and structural reconstruction, and 3)\\na local MI-driven contrastive learning strategy to ensure the cellular level\\nstructural consistency between H&E-IHC images. Extensive experiments\\ndemonstrate the our superiority over state-of-the-art approaches, highlighting\\nits biomedical potential. Codes will be open-sourced upon acceptance.', 'authors': ['Anran Liu', 'Xiaofei Wang', 'Jing Cai', 'Chao Li'], 'published': '2025-06-29T11:02:45+00:00', 'categories': ['eess.IV', 'cs.AI', 'cs.CV'], 'url': 'http://arxiv.org/abs/2506.23184v1'}"}, {'id': 114, 'arxiv_id': '2506.23174v1', 'base_arxiv_id': '2506.23174', 'version': '1', 'title': 'Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data', 'abstract': "Generative models have gained significant attention for their ability to\nproduce realistic synthetic data that supplements the quantity of real-world\ndatasets. While recent studies show performance improvements in wireless\nsensing tasks by incorporating all synthetic data into training sets, the\nquality of synthetic data remains unpredictable and the resulting performance\ngains are not guaranteed. To address this gap, we propose tractable and\ngeneralizable metrics to quantify quality attributes of synthetic data -\naffinity and diversity. Our assessment reveals prevalent affinity limitation in\ncurrent wireless synthetic data, leading to mislabeled data and degraded task\nperformance. We attribute the quality limitation to generative models' lack of\nawareness of untrained conditions and domain-specific processing. To mitigate\nthese issues, we introduce SynCheck, a quality-guided synthetic data\nutilization scheme that refines synthetic data quality during task model\ntraining. Our evaluation demonstrates that SynCheck consistently outperforms\nquality-oblivious utilization of synthetic data, and achieves 4.3% performance\nimprovement even when the previous utilization degrades performance by 13.4%.", 'submit_date': datetime.date(2025, 6, 29), 'metadata': '{\'id\': \'2506.23174v1\', \'title\': \'Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data\', \'abstract\': "Generative models have gained significant attention for their ability to\\nproduce realistic synthetic data that supplements the quantity of real-world\\ndatasets. While recent studies show performance improvements in wireless\\nsensing tasks by incorporating all synthetic data into training sets, the\\nquality of synthetic data remains unpredictable and the resulting performance\\ngains are not guaranteed. To address this gap, we propose tractable and\\ngeneralizable metrics to quantify quality attributes of synthetic data -\\naffinity and diversity. Our assessment reveals prevalent affinity limitation in\\ncurrent wireless synthetic data, leading to mislabeled data and degraded task\\nperformance. We attribute the quality limitation to generative models\' lack of\\nawareness of untrained conditions and domain-specific processing. To mitigate\\nthese issues, we introduce SynCheck, a quality-guided synthetic data\\nutilization scheme that refines synthetic data quality during task model\\ntraining. Our evaluation demonstrates that SynCheck consistently outperforms\\nquality-oblivious utilization of synthetic data, and achieves 4.3% performance\\nimprovement even when the previous utilization degrades performance by 13.4%.", \'authors\': [\'Chen Gong\', \'Bo Liang\', \'Wei Gao\', \'Chenren Xu\'], \'published\': \'2025-06-29T10:17:39+00:00\', \'categories\': [\'cs.LG\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23174v1\'}'}, {'id': 115, 'arxiv_id': '2506.23173v1', 'base_arxiv_id': '2506.23173', 'version': '1', 'title': 'Deep Learning for Optical Misalignment Diagnostics in Multi-Lens Imaging Systems', 'abstract': 'In the rapidly evolving field of optical engineering, precise alignment of\nmulti-lens imaging systems is critical yet challenging, as even minor\nmisalignments can significantly degrade performance. Traditional alignment\nmethods rely on specialized equipment and are time-consuming processes,\nhighlighting the need for automated and scalable solutions. We present two\ncomplementary deep learning-based inverse-design methods for diagnosing\nmisalignments in multi-element lens systems using only optical measurements.\nFirst, we use ray-traced spot diagrams to predict five-degree-of-freedom\n(5-DOF) errors in a 6-lens photographic prime, achieving a mean absolute error\nof 0.031mm in lateral translation and 0.011$^$ in tilt. We also introduce\na physics-based simulation pipeline that utilizes grayscale synthetic camera\nimages, enabling a deep learning model to estimate 4-DOF, decenter and tilt\nerrors in both two- and six-lens multi-lens systems. These results show the\npotential to reshape manufacturing and quality control in precision imaging.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23173v1', 'title': 'Deep Learning for Optical Misalignment Diagnostics in Multi-Lens Imaging Systems', 'abstract': 'In the rapidly evolving field of optical engineering, precise alignment of\\nmulti-lens imaging systems is critical yet challenging, as even minor\\nmisalignments can significantly degrade performance. Traditional alignment\\nmethods rely on specialized equipment and are time-consuming processes,\\nhighlighting the need for automated and scalable solutions. We present two\\ncomplementary deep learning-based inverse-design methods for diagnosing\\nmisalignments in multi-element lens systems using only optical measurements.\\nFirst, we use ray-traced spot diagrams to predict five-degree-of-freedom\\n(5-DOF) errors in a 6-lens photographic prime, achieving a mean absolute error\\nof 0.031mm in lateral translation and 0.011$^\\\\circ$ in tilt. We also introduce\\na physics-based simulation pipeline that utilizes grayscale synthetic camera\\nimages, enabling a deep learning model to estimate 4-DOF, decenter and tilt\\nerrors in both two- and six-lens multi-lens systems. These results show the\\npotential to reshape manufacturing and quality control in precision imaging.', 'authors': ['Tomer Slor', 'Dean Oren', 'Shira Baneth', 'Tom Coen', 'Haim Suchowski'], 'published': '2025-06-29T10:13:40+00:00', 'categories': ['physics.optics', 'cs.AI', 'cs.LG'], 'url': 'http://arxiv.org/abs/2506.23173v1'}"}, {'id': 116, 'arxiv_id': '2506.23168v1', 'base_arxiv_id': '2506.23168', 'version': '1', 'title': 'Rises for Measuring Local Distributivity in Lattices', 'abstract': 'Distributivity is a well-established and extensively studied notion in\nlattice theory. In the context of data analysis, particularly within Formal\nConcept Analysis (FCA), lattices are often observed to exhibit a high degree of\ndistributivity. However, no standardized measure exists to quantify this\nproperty. In this paper, we introduce the notion of rises in (concept) lattices\nas a means to assess distributivity. Rises capture how the number of attributes\nor objects in covering concepts change within the concept lattice. We show that\na lattice is distributive if and only if no non-unit rises occur. Furthermore,\nwe relate rises to the classical notion of meet- and join distributivity. We\nobserve that concept lattices from real-world data are to a high degree\njoin-distributive, but much less meet-distributive. We additionally study how\njoin-distributivity manifests on the level of ordered sets.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23168v1', 'title': 'Rises for Measuring Local Distributivity in Lattices', 'abstract': 'Distributivity is a well-established and extensively studied notion in\\nlattice theory. In the context of data analysis, particularly within Formal\\nConcept Analysis (FCA), lattices are often observed to exhibit a high degree of\\ndistributivity. However, no standardized measure exists to quantify this\\nproperty. In this paper, we introduce the notion of rises in (concept) lattices\\nas a means to assess distributivity. Rises capture how the number of attributes\\nor objects in covering concepts change within the concept lattice. We show that\\na lattice is distributive if and only if no non-unit rises occur. Furthermore,\\nwe relate rises to the classical notion of meet- and join distributivity. We\\nobserve that concept lattices from real-world data are to a high degree\\njoin-distributive, but much less meet-distributive. We additionally study how\\njoin-distributivity manifests on the level of ordered sets.', 'authors': ['Mohammad Abdulla', 'Tobias Hille', 'Dominik D√ºrrschnabel', 'Gerd Stumme'], 'published': '2025-06-29T10:03:51+00:00', 'categories': ['cs.AI', 'cs.DM', 'math.CO', 'math.RA', '06B99', 'G.2.1'], 'url': 'http://arxiv.org/abs/2506.23168v1'}"}, {'id': 117, 'arxiv_id': '2506.23164v1', 'base_arxiv_id': '2506.23164', 'version': '1', 'title': 'Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models', 'abstract': 'Autonomous Vehicle decisions rely on multimodal prediction models that\naccount for multiple route options and the inherent uncertainty in human\nbehavior. However, models can suffer from mode collapse, where only the most\nlikely mode is predicted, posing significant safety risks. While existing\nmethods employ various strategies to generate diverse predictions, they often\noverlook the diversity in interaction modes among agents. Additionally,\ntraditional metrics for evaluating prediction models are dataset-dependent and\ndo not evaluate inter-agent interactions quantitatively. To our knowledge, none\nof the existing metrics explicitly evaluates mode collapse. In this paper, we\npropose a novel evaluation framework that assesses mode collapse in joint\ntrajectory predictions, focusing on safety-critical interactions. We introduce\nmetrics for mode collapse, mode correctness, and coverage, emphasizing the\nsequential dimension of predictions. By testing four multi-agent trajectory\nprediction models, we demonstrate that mode collapse indeed happens. When\nlooking at the sequential dimension, although prediction accuracy improves\ncloser to interaction events, there are still cases where the models are unable\nto predict the correct interaction mode, even just before the interaction mode\nbecomes inevitable. We hope that our framework can help researchers gain new\ninsights and advance the development of more consistent and accurate prediction\nmodels, thus enhancing the safety of autonomous driving systems.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23164v1', 'title': 'Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models', 'abstract': 'Autonomous Vehicle decisions rely on multimodal prediction models that\\naccount for multiple route options and the inherent uncertainty in human\\nbehavior. However, models can suffer from mode collapse, where only the most\\nlikely mode is predicted, posing significant safety risks. While existing\\nmethods employ various strategies to generate diverse predictions, they often\\noverlook the diversity in interaction modes among agents. Additionally,\\ntraditional metrics for evaluating prediction models are dataset-dependent and\\ndo not evaluate inter-agent interactions quantitatively. To our knowledge, none\\nof the existing metrics explicitly evaluates mode collapse. In this paper, we\\npropose a novel evaluation framework that assesses mode collapse in joint\\ntrajectory predictions, focusing on safety-critical interactions. We introduce\\nmetrics for mode collapse, mode correctness, and coverage, emphasizing the\\nsequential dimension of predictions. By testing four multi-agent trajectory\\nprediction models, we demonstrate that mode collapse indeed happens. When\\nlooking at the sequential dimension, although prediction accuracy improves\\ncloser to interaction events, there are still cases where the models are unable\\nto predict the correct interaction mode, even just before the interaction mode\\nbecomes inevitable. We hope that our framework can help researchers gain new\\ninsights and advance the development of more consistent and accurate prediction\\nmodels, thus enhancing the safety of autonomous driving systems.', 'authors': ['Maarten Hugenholtz', 'Anna Meszaros', 'Jens Kober', 'Zlatan Ajanovic'], 'published': '2025-06-29T09:53:12+00:00', 'categories': ['cs.RO', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23164v1'}"}, {'id': 118, 'arxiv_id': '2506.23151v1', 'base_arxiv_id': '2506.23151', 'version': '1', 'title': 'MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation', 'abstract': 'Recent advances in optical flow estimation have prioritized accuracy at the\ncost of growing GPU memory consumption, particularly for high-resolution\n(FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical\nflow method that identifies a favorable trade-off between multi-frame\nestimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU\nmemory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely\npositions our method to be trained at native 1080p without the need for\ncropping or downsampling. We systematically revisit design choices from\nRAFT-like architectures, integrating reduced correlation volumes and\nhigh-resolution training protocols alongside multi-frame estimation, to achieve\nstate-of-the-art performance across multiple benchmarks while substantially\nreducing memory overhead. Our method outperforms more resource-intensive\nalternatives in both accuracy and runtime efficiency, validating its robustness\nfor flow estimation at high resolutions. At the time of submission, our method\nranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289,\nleads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the\nbest Fl-all error on KITTI-2015 at 2.94%. The code is available at\nhttps://github.com/msu-video-group/memfof.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23151v1', 'title': 'MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation', 'abstract': 'Recent advances in optical flow estimation have prioritized accuracy at the\\ncost of growing GPU memory consumption, particularly for high-resolution\\n(FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical\\nflow method that identifies a favorable trade-off between multi-frame\\nestimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU\\nmemory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely\\npositions our method to be trained at native 1080p without the need for\\ncropping or downsampling. We systematically revisit design choices from\\nRAFT-like architectures, integrating reduced correlation volumes and\\nhigh-resolution training protocols alongside multi-frame estimation, to achieve\\nstate-of-the-art performance across multiple benchmarks while substantially\\nreducing memory overhead. Our method outperforms more resource-intensive\\nalternatives in both accuracy and runtime efficiency, validating its robustness\\nfor flow estimation at high resolutions. At the time of submission, our method\\nranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289,\\nleads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the\\nbest Fl-all error on KITTI-2015 at 2.94%. The code is available at\\nhttps://github.com/msu-video-group/memfof.', 'authors': ['Vladislav Bargatin', 'Egor Chistov', 'Alexander Yakovenko', 'Dmitriy Vatolin'], 'published': '2025-06-29T09:01:42+00:00', 'categories': ['cs.CV', 'cs.AI', 'cs.MM'], 'url': 'http://arxiv.org/abs/2506.23151v1'}"}, {'id': 119, 'arxiv_id': '2506.23141v1', 'base_arxiv_id': '2506.23141', 'version': '1', 'title': 'Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing', 'abstract': "Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge\nGraph Completion (KGC), providing vital cues for prediction. However,\ntraditional node-based message passing mechanisms, when applied to knowledge\ngraphs, often introduce noise and suffer from information dilution or\nover-smoothing by indiscriminately aggregating information from all neighboring\nedges. To address this challenge, we propose a semantic-aware relational\nmessage passing. A core innovation of this framework is the introduction of a\nsemantic-aware Top-K neighbor selection strategy. Specifically, this\nstrategy first evaluates the semantic relevance between a central node and its\nincident edges within a shared latent space, selecting only the Top-K most\npertinent ones. Subsequently, information from these selected edges is\neffectively fused with the central node's own representation using a\nmulti-head attention aggregator to generate a semantically focused\nnode message. In this manner, our model not only leverages the structure and\nfeatures of edges within the knowledge graph but also more accurately captures\nand propagates the contextual information most relevant to the specific link\nprediction task, thereby effectively mitigating interference from irrelevant\ninformation. Extensive experiments demonstrate that our method achieves\nsuperior performance compared to existing approaches on several established\nbenchmarks.", 'submit_date': datetime.date(2025, 6, 29), 'metadata': '{\'id\': \'2506.23141v1\', \'title\': \'Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing\', \'abstract\': "Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge\\nGraph Completion (KGC), providing vital cues for prediction. However,\\ntraditional node-based message passing mechanisms, when applied to knowledge\\ngraphs, often introduce noise and suffer from information dilution or\\nover-smoothing by indiscriminately aggregating information from all neighboring\\nedges. To address this challenge, we propose a semantic-aware relational\\nmessage passing. A core innovation of this framework is the introduction of a\\n\\\\textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this\\nstrategy first evaluates the semantic relevance between a central node and its\\nincident edges within a shared latent space, selecting only the Top-K most\\npertinent ones. Subsequently, information from these selected edges is\\neffectively fused with the central node\'s own representation using a\\n\\\\textbf{multi-head attention aggregator} to generate a semantically focused\\nnode message. In this manner, our model not only leverages the structure and\\nfeatures of edges within the knowledge graph but also more accurately captures\\nand propagates the contextual information most relevant to the specific link\\nprediction task, thereby effectively mitigating interference from irrelevant\\ninformation. Extensive experiments demonstrate that our method achieves\\nsuperior performance compared to existing approaches on several established\\nbenchmarks.", \'authors\': [\'Siyuan Li\', \'Ruitong Liu\', \'Yan Wen\', \'Te Sun\'], \'published\': \'2025-06-29T08:37:48+00:00\', \'categories\': [\'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23141v1\'}'}, {'id': 120, 'arxiv_id': '2506.23139v1', 'base_arxiv_id': '2506.23139', 'version': '1', 'title': 'Benchmarking Deep Search over Heterogeneous Enterprise Data', 'abstract': 'We present a new benchmark for evaluating Deep Search--a realistic and\ncomplex form of retrieval-augmented generation (RAG) that requires\nsource-aware, multi-hop reasoning over diverse, sparsed, but related sources.\nThese include documents, meeting transcripts, Slack messages, GitHub, and URLs,\nwhich vary in structure and often contain human-to-human interactions. We build\nit using a synthetic data pipeline that simulates business workflows across\nproduct planning, development, and support stages, generating interconnected\ncontent with realistic noise and multi-hop questions with guaranteed\nground-truth answers. We release our benchmark with both answerable and\nunanswerable queries, and retrieval pool of 39,190 enterprise artifacts,\nenabling fine-grained evaluation of long-context LLM and RAG systems. Our\nexperiments reveal that even the best-performing agentic RAG methods achieve an\naverage performance score of 32.96 on our benchmark. With further analysis, we\nhighlight retrieval as the main bottleneck: existing methods struggle to\nconduct deep searches and retrieve all necessary evidence. Consequently, they\noften reason over partial context, leading to significant performance\ndegradation.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23139v1', 'title': 'Benchmarking Deep Search over Heterogeneous Enterprise Data', 'abstract': 'We present a new benchmark for evaluating Deep Search--a realistic and\\ncomplex form of retrieval-augmented generation (RAG) that requires\\nsource-aware, multi-hop reasoning over diverse, sparsed, but related sources.\\nThese include documents, meeting transcripts, Slack messages, GitHub, and URLs,\\nwhich vary in structure and often contain human-to-human interactions. We build\\nit using a synthetic data pipeline that simulates business workflows across\\nproduct planning, development, and support stages, generating interconnected\\ncontent with realistic noise and multi-hop questions with guaranteed\\nground-truth answers. We release our benchmark with both answerable and\\nunanswerable queries, and retrieval pool of 39,190 enterprise artifacts,\\nenabling fine-grained evaluation of long-context LLM and RAG systems. Our\\nexperiments reveal that even the best-performing agentic RAG methods achieve an\\naverage performance score of 32.96 on our benchmark. With further analysis, we\\nhighlight retrieval as the main bottleneck: existing methods struggle to\\nconduct deep searches and retrieve all necessary evidence. Consequently, they\\noften reason over partial context, leading to significant performance\\ndegradation.', 'authors': ['Prafulla Kumar Choubey', 'Xiangyu Peng', 'Shilpa Bhagavath', 'Kung-Hsiang Huang', 'Caiming Xiong', 'Chien-Sheng Wu'], 'published': '2025-06-29T08:34:59+00:00', 'categories': ['cs.CL', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23139v1'}"}, {'id': 121, 'arxiv_id': '2506.23137v1', 'base_arxiv_id': '2506.23137', 'version': '1', 'title': 'Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion', 'abstract': 'Effective modeling of multifaceted relations is pivotal for Knowledge Graph\nCompletion (KGC). However, a majority of existing approaches are predicated on\nstatic, embedding-based scoring, exhibiting inherent limitations in capturing\ncontextual dependencies and relational dynamics. Addressing this gap, we\npropose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal\ncomponents: (1) a semantic context learning module that encodes\ncontext-sensitive entity representations, and (2) a conditional flow-matching\nmodule designed to learn the dynamic transformation from a head to a tail\nembedding, governed by the aforementioned context. The resultant predictive\nvector field, representing the context-informed relational path, serves to\ndynamically refine the initial static score of an entity pair. Through this\nsynergy of context-aware static representations and conditioned dynamic\ninformation, FMS facilitates a more profound modeling of relational semantics.\nComprehensive evaluations on several standard benchmarks demonstrate that our\nproposed method surpasses prior state-of-the-art results.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23137v1', 'title': 'Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion', 'abstract': 'Effective modeling of multifaceted relations is pivotal for Knowledge Graph\\nCompletion (KGC). However, a majority of existing approaches are predicated on\\nstatic, embedding-based scoring, exhibiting inherent limitations in capturing\\ncontextual dependencies and relational dynamics. Addressing this gap, we\\npropose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal\\ncomponents: (1) a semantic context learning module that encodes\\ncontext-sensitive entity representations, and (2) a conditional flow-matching\\nmodule designed to learn the dynamic transformation from a head to a tail\\nembedding, governed by the aforementioned context. The resultant predictive\\nvector field, representing the context-informed relational path, serves to\\ndynamically refine the initial static score of an entity pair. Through this\\nsynergy of context-aware static representations and conditioned dynamic\\ninformation, FMS facilitates a more profound modeling of relational semantics.\\nComprehensive evaluations on several standard benchmarks demonstrate that our\\nproposed method surpasses prior state-of-the-art results.', 'authors': ['Siyuan Li', 'Ruitong Liu', 'Yan Wen', 'Te Sun'], 'published': '2025-06-29T08:22:04+00:00', 'categories': ['cs.CL', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23137v1'}"}, {'id': 122, 'arxiv_id': '2506.23128v1', 'base_arxiv_id': '2506.23128', 'version': '1', 'title': 'Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons', 'abstract': "How far are Large Language Models (LLMs) in performing deep relational\nreasoning? In this paper, we evaluate and compare the reasoning capabilities of\nthree cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a\nsuite of carefully designed benchmark tasks in family tree and general graph\nreasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the\nhighest F1-scores across multiple tasks and problem sizes, demonstrating strong\naptitude in logical deduction and relational inference. However, all evaluated\nmodels, including DeepSeek-R1, struggle significantly as problem complexity\nincreases, largely due to token length limitations and incomplete output\nstructures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought\nresponses uncovers its unique planning and verification strategies, but also\nhighlights instances of incoherent or incomplete reasoning, calling attention\nto the need for deeper scrutiny into LLMs' internal inference dynamics. We\nfurther discuss key directions for future work, including the role of\nmultimodal reasoning and the systematic examination of reasoning failures. Our\nfindings provide both empirical insights and theoretical implications for\nadvancing LLMs' reasoning abilities, particularly in tasks that demand\nstructured, multi-step logical inference. Our code repository will be publicly\navailable at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.", 'submit_date': datetime.date(2025, 6, 29), 'metadata': '{\'id\': \'2506.23128v1\', \'title\': \'Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons\', \'abstract\': "How far are Large Language Models (LLMs) in performing deep relational\\nreasoning? In this paper, we evaluate and compare the reasoning capabilities of\\nthree cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a\\nsuite of carefully designed benchmark tasks in family tree and general graph\\nreasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the\\nhighest F1-scores across multiple tasks and problem sizes, demonstrating strong\\naptitude in logical deduction and relational inference. However, all evaluated\\nmodels, including DeepSeek-R1, struggle significantly as problem complexity\\nincreases, largely due to token length limitations and incomplete output\\nstructures. A detailed analysis of DeepSeek-R1\'s long Chain-of-Thought\\nresponses uncovers its unique planning and verification strategies, but also\\nhighlights instances of incoherent or incomplete reasoning, calling attention\\nto the need for deeper scrutiny into LLMs\' internal inference dynamics. We\\nfurther discuss key directions for future work, including the role of\\nmultimodal reasoning and the systematic examination of reasoning failures. Our\\nfindings provide both empirical insights and theoretical implications for\\nadvancing LLMs\' reasoning abilities, particularly in tasks that demand\\nstructured, multi-step logical inference. Our code repository will be publicly\\navailable at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.", \'authors\': [\'Chi Chiu So\', \'Yueyue Sun\', \'Jun-Min Wang\', \'Siu Pang Yung\', \'Anthony Wai Keung Loh\', \'Chun Pong Chau\'], \'published\': \'2025-06-29T07:37:49+00:00\', \'categories\': [\'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23128v1\'}'}, {'id': 123, 'arxiv_id': '2506.23127v1', 'base_arxiv_id': '2506.23127', 'version': '1', 'title': 'Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they face significant challenges in embodied task planning\nscenarios that require continuous environmental understanding and action\ngeneration. Existing approaches generate open-loop action scripts based on\nstatic knowledge, making it difficult to learn causal relationships between\nactions and environmental feedback, particularly in partially observable\nenvironments. We introduce Embodied Planner-R1, a novel outcome-driven\nreinforcement learning framework that enables LLMs to develop interactive\ncapabilities through autonomous exploration with minimal supervision. Our\nframework incorporates three key innovations: (1) Without human annotations, we\nemploy pure reinforcement learning with group rollout, incorporating\nin-environment interaction through parallel exploration; (2) completion-driven\nsparse reward; and (3) Interactive Policy Optimization (IPO) for efficient\nlearning from grouped trajectories. Across two challenging text-based Embodied\nplanning benchmarks, Embodied Planner-R1 achieves impressive completion rates\nof 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a\nlarge margin, and suffers only a -3.66% drop in previously unseen environments,\nevidencing strong generalization.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23127v1', 'title': 'Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning', 'abstract': 'Large Language Models (LLMs) have demonstrated remarkable capabilities across\\nvarious tasks, yet they face significant challenges in embodied task planning\\nscenarios that require continuous environmental understanding and action\\ngeneration. Existing approaches generate open-loop action scripts based on\\nstatic knowledge, making it difficult to learn causal relationships between\\nactions and environmental feedback, particularly in partially observable\\nenvironments. We introduce Embodied Planner-R1, a novel outcome-driven\\nreinforcement learning framework that enables LLMs to develop interactive\\ncapabilities through autonomous exploration with minimal supervision. Our\\nframework incorporates three key innovations: (1) Without human annotations, we\\nemploy pure reinforcement learning with group rollout, incorporating\\nin-environment interaction through parallel exploration; (2) completion-driven\\nsparse reward; and (3) Interactive Policy Optimization (IPO) for efficient\\nlearning from grouped trajectories. Across two challenging text-based Embodied\\nplanning benchmarks, Embodied Planner-R1 achieves impressive completion rates\\nof 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a\\nlarge margin, and suffers only a -3.66% drop in previously unseen environments,\\nevidencing strong generalization.', 'authors': ['Zhaoye Fei', 'Li Ji', 'Siyin Wang', 'Junhao Shi', 'Jingjing Gong', 'Xipeng Qiu'], 'published': '2025-06-29T07:31:24+00:00', 'categories': ['cs.CL', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23127v1'}"}, {'id': 124, 'arxiv_id': '2506.23123v1', 'base_arxiv_id': '2506.23123', 'version': '1', 'title': 'The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy', 'abstract': "Artificial intelligence is humanity's most promising technology because of\nthe remarkable capabilities offered by foundation models. Yet, the same\ntechnology brings confusion and consternation: foundation models are poorly\nunderstood and they may precipitate a wide array of harms. This dissertation\nexplains how technology and society coevolve in the age of AI, organized around\nthree themes. First, the conceptual framing: the capabilities, risks, and the\nsupply chain that grounds foundation models in the broader economy. Second, the\nempirical insights that enrich the conceptual foundations: transparency created\nvia evaluations at the model level and indexes at the organization level.\nFinally, the transition from understanding to action: superior understanding of\nthe societal impact of foundation models advances evidence-based AI policy.\nView together, this dissertation makes inroads into achieving better societal\noutcomes in the age of AI by building the scientific foundations and\nresearch-policy interface required for better AI governance.", 'submit_date': datetime.date(2025, 6, 29), 'metadata': '{\'id\': \'2506.23123v1\', \'title\': \'The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy\', \'abstract\': "Artificial intelligence is humanity\'s most promising technology because of\\nthe remarkable capabilities offered by foundation models. Yet, the same\\ntechnology brings confusion and consternation: foundation models are poorly\\nunderstood and they may precipitate a wide array of harms. This dissertation\\nexplains how technology and society coevolve in the age of AI, organized around\\nthree themes. First, the conceptual framing: the capabilities, risks, and the\\nsupply chain that grounds foundation models in the broader economy. Second, the\\nempirical insights that enrich the conceptual foundations: transparency created\\nvia evaluations at the model level and indexes at the organization level.\\nFinally, the transition from understanding to action: superior understanding of\\nthe societal impact of foundation models advances evidence-based AI policy.\\nView together, this dissertation makes inroads into achieving better societal\\noutcomes in the age of AI by building the scientific foundations and\\nresearch-policy interface required for better AI governance.", \'authors\': [\'Rishi Bommasani\'], \'published\': \'2025-06-29T07:16:48+00:00\', \'categories\': [\'cs.AI\', \'cs.CY\', \'cs.ET\'], \'url\': \'http://arxiv.org/abs/2506.23123v1\'}'}, {'id': 125, 'arxiv_id': '2506.23121v1', 'base_arxiv_id': '2506.23121', 'version': '1', 'title': 'CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation', 'abstract': 'Multi-organ medical segmentation is a crucial component of medical image\nprocessing, essential for doctors to make accurate diagnoses and develop\neffective treatment plans. Despite significant progress in this field, current\nmulti-organ segmentation models often suffer from inaccurate details,\ndependence on geometric prompts and loss of spatial information. Addressing\nthese challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal\nInteraction and Semantic Prompting based on SAM2. This model represents a\npromising approach to multi-organ medical segmentation guided by textual\ndescriptions of organs. Our method begins by converting visual and textual\ninputs into cross-modal contextualized semantics using a progressive\ncross-attention interaction mechanism. These semantics are then injected into\nthe image encoder to enhance the detailed understanding of visual information.\nTo eliminate reliance on geometric prompts, we use a semantic prompting\nstrategy, replacing the original prompt encoder to sharpen the perception of\nchallenging targets. In addition, a similarity-sorting self-updating strategy\nfor memory and a mask-refining process is applied to further adapt to medical\nimaging and enhance localized details. Comparative experiments conducted on\nseven public datasets indicate that CRISP-SAM2 outperforms existing models.\nExtensive analysis also demonstrates the effectiveness of our method, thereby\nconfirming its superior performance, especially in addressing the limitations\nmentioned earlier. Our code is available at:\nhttps://github.com/YU-deep/CRISP\\_SAM2.git.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23121v1', 'title': 'CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation', 'abstract': 'Multi-organ medical segmentation is a crucial component of medical image\\nprocessing, essential for doctors to make accurate diagnoses and develop\\neffective treatment plans. Despite significant progress in this field, current\\nmulti-organ segmentation models often suffer from inaccurate details,\\ndependence on geometric prompts and loss of spatial information. Addressing\\nthese challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal\\nInteraction and Semantic Prompting based on SAM2. This model represents a\\npromising approach to multi-organ medical segmentation guided by textual\\ndescriptions of organs. Our method begins by converting visual and textual\\ninputs into cross-modal contextualized semantics using a progressive\\ncross-attention interaction mechanism. These semantics are then injected into\\nthe image encoder to enhance the detailed understanding of visual information.\\nTo eliminate reliance on geometric prompts, we use a semantic prompting\\nstrategy, replacing the original prompt encoder to sharpen the perception of\\nchallenging targets. In addition, a similarity-sorting self-updating strategy\\nfor memory and a mask-refining process is applied to further adapt to medical\\nimaging and enhance localized details. Comparative experiments conducted on\\nseven public datasets indicate that CRISP-SAM2 outperforms existing models.\\nExtensive analysis also demonstrates the effectiveness of our method, thereby\\nconfirming its superior performance, especially in addressing the limitations\\nmentioned earlier. Our code is available at:\\nhttps://github.com/YU-deep/CRISP\\\\_SAM2.git.', 'authors': ['Xinlei Yu', 'Chanmiao Wang', 'Hui Jin', 'Ahmed Elazab', 'Gangyong Jia', 'Xiang Wan', 'Changqing Zou', 'Ruiquan Ge'], 'published': '2025-06-29T07:05:27+00:00', 'categories': ['eess.IV', 'cs.AI', 'cs.CV'], 'url': 'http://arxiv.org/abs/2506.23121v1'}"}, {'id': 126, 'arxiv_id': '2506.23115v1', 'base_arxiv_id': '2506.23115', 'version': '1', 'title': 'MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings', 'abstract': 'Multimodal embedding models, built upon causal Vision Language Models (VLMs),\nhave shown promise in various tasks. However, current approaches face three key\nlimitations: the use of causal attention in VLM backbones is suboptimal for\nembedding tasks; scalability issues due to reliance on high-quality labeled\npaired data for contrastive learning; and limited diversity in training\nobjectives and data. To address these issues, we propose MoCa, a two-stage\nframework for transforming pre-trained VLMs into effective bidirectional\nmultimodal embedding models. The first stage, Modality-aware Continual\nPre-training, introduces a joint reconstruction objective that simultaneously\ndenoises interleaved text and image inputs, enhancing bidirectional\ncontext-aware reasoning. The second stage, Heterogeneous Contrastive\nFine-tuning, leverages diverse, semantically rich multimodal data beyond simple\nimage-caption pairs to enhance generalization and alignment. Our method\naddresses the stated limitations by introducing bidirectional attention through\ncontinual pre-training, scaling effectively with massive unlabeled datasets via\njoint reconstruction objectives, and utilizing diverse multimodal data for\nenhanced representation robustness. Experiments demonstrate that MoCa\nconsistently improves performance across MMEB and ViDoRe-v2 benchmarks,\nachieving new state-of-the-art results, and exhibits strong scalability with\nboth model size and training data on MMEB.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23115v1', 'title': 'MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings', 'abstract': 'Multimodal embedding models, built upon causal Vision Language Models (VLMs),\\nhave shown promise in various tasks. However, current approaches face three key\\nlimitations: the use of causal attention in VLM backbones is suboptimal for\\nembedding tasks; scalability issues due to reliance on high-quality labeled\\npaired data for contrastive learning; and limited diversity in training\\nobjectives and data. To address these issues, we propose MoCa, a two-stage\\nframework for transforming pre-trained VLMs into effective bidirectional\\nmultimodal embedding models. The first stage, Modality-aware Continual\\nPre-training, introduces a joint reconstruction objective that simultaneously\\ndenoises interleaved text and image inputs, enhancing bidirectional\\ncontext-aware reasoning. The second stage, Heterogeneous Contrastive\\nFine-tuning, leverages diverse, semantically rich multimodal data beyond simple\\nimage-caption pairs to enhance generalization and alignment. Our method\\naddresses the stated limitations by introducing bidirectional attention through\\ncontinual pre-training, scaling effectively with massive unlabeled datasets via\\njoint reconstruction objectives, and utilizing diverse multimodal data for\\nenhanced representation robustness. Experiments demonstrate that MoCa\\nconsistently improves performance across MMEB and ViDoRe-v2 benchmarks,\\nachieving new state-of-the-art results, and exhibits strong scalability with\\nboth model size and training data on MMEB.', 'authors': ['Haonan Chen', 'Hong Liu', 'Yuping Luo', 'Liang Wang', 'Nan Yang', 'Furu Wei', 'Zhicheng Dou'], 'published': '2025-06-29T06:41:00+00:00', 'categories': ['cs.CV', 'cs.AI', 'cs.CL'], 'url': 'http://arxiv.org/abs/2506.23115v1'}"}, {'id': 127, 'arxiv_id': '2506.23107v1', 'base_arxiv_id': '2506.23107', 'version': '1', 'title': 'Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study', 'abstract': 'Large language models (LLMs) have made significant strides, extending their\napplications to dialogue systems, automated content creation, and\ndomain-specific advisory tasks. However, as their use grows, concerns have\nemerged regarding their reliability in simulating complex decision-making\nbehavior, such as risky decision-making, where a single choice can lead to\nmultiple outcomes. This study investigates the ability of LLMs to simulate\nrisky decision-making scenarios. We compare model-generated decisions with\nactual human responses in a series of lottery-based tasks, using transportation\nstated preference survey data from participants in Sydney, Dhaka, Hong Kong,\nand Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and\nChatGPT o1-mini -- which were tasked with predicting individual choices. Risk\npreferences were analyzed using the Constant Relative Risk Aversion (CRRA)\nframework. Results show that both models exhibit more risk-averse behavior than\nhuman participants, with o1-mini aligning more closely with observed human\ndecisions. Further analysis of multilingual data from Nanjing and Hong Kong\nindicates that model predictions in Chinese deviate more from actual responses\ncompared to English, suggesting that prompt language may influence simulation\nperformance. These findings highlight both the promise and the current\nlimitations of LLMs in replicating human-like risk behavior, particularly in\nlinguistic and cultural settings.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23107v1', 'title': 'Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study', 'abstract': 'Large language models (LLMs) have made significant strides, extending their\\napplications to dialogue systems, automated content creation, and\\ndomain-specific advisory tasks. However, as their use grows, concerns have\\nemerged regarding their reliability in simulating complex decision-making\\nbehavior, such as risky decision-making, where a single choice can lead to\\nmultiple outcomes. This study investigates the ability of LLMs to simulate\\nrisky decision-making scenarios. We compare model-generated decisions with\\nactual human responses in a series of lottery-based tasks, using transportation\\nstated preference survey data from participants in Sydney, Dhaka, Hong Kong,\\nand Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and\\nChatGPT o1-mini -- which were tasked with predicting individual choices. Risk\\npreferences were analyzed using the Constant Relative Risk Aversion (CRRA)\\nframework. Results show that both models exhibit more risk-averse behavior than\\nhuman participants, with o1-mini aligning more closely with observed human\\ndecisions. Further analysis of multilingual data from Nanjing and Hong Kong\\nindicates that model predictions in Chinese deviate more from actual responses\\ncompared to English, suggesting that prompt language may influence simulation\\nperformance. These findings highlight both the promise and the current\\nlimitations of LLMs in replicating human-like risk behavior, particularly in\\nlinguistic and cultural settings.', 'authors': ['Bing Song', 'Jianing Liu', 'Sisi Jian', 'Chenyang Wu', 'Vinayak Dixit'], 'published': '2025-06-29T06:16:57+00:00', 'categories': ['cs.AI'], 'url': 'http://arxiv.org/abs/2506.23107v1'}"}, {'id': 128, 'arxiv_id': '2506.23101v1', 'base_arxiv_id': '2506.23101', 'version': '1', 'title': 'From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship', 'abstract': 'Multimodal large language models (MLLMs) have shown impressive capabilities\nacross tasks involving both visual and textual modalities. However, growing\nconcerns remain about their potential to encode and amplify gender bias,\nparticularly in socially sensitive applications. Existing benchmarks\npredominantly evaluate bias in isolated scenarios, overlooking how bias may\nemerge subtly through interpersonal interactions. We fill this gap by going\nbeyond single-entity evaluation and instead focusing on a deeper examination of\nrelational and contextual gender bias in dual-individual interactions. We\nintroduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs\nthrough the lens of social relationships in generated narratives. Genres\nassesses gender bias through a dual-character profile and narrative generation\ntask that captures rich interpersonal dynamics and supports a fine-grained bias\nevaluation suite across multiple dimensions. Experiments on both open- and\nclosed-source MLLMs reveal persistent, context-sensitive gender biases that are\nnot evident in single-character settings. Our findings underscore the\nimportance of relationship-aware benchmarks for diagnosing subtle,\ninteraction-driven gender bias in MLLMs and provide actionable insights for\nfuture bias mitigation.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23101v1', 'title': 'From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship', 'abstract': 'Multimodal large language models (MLLMs) have shown impressive capabilities\\nacross tasks involving both visual and textual modalities. However, growing\\nconcerns remain about their potential to encode and amplify gender bias,\\nparticularly in socially sensitive applications. Existing benchmarks\\npredominantly evaluate bias in isolated scenarios, overlooking how bias may\\nemerge subtly through interpersonal interactions. We fill this gap by going\\nbeyond single-entity evaluation and instead focusing on a deeper examination of\\nrelational and contextual gender bias in dual-individual interactions. We\\nintroduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs\\nthrough the lens of social relationships in generated narratives. Genres\\nassesses gender bias through a dual-character profile and narrative generation\\ntask that captures rich interpersonal dynamics and supports a fine-grained bias\\nevaluation suite across multiple dimensions. Experiments on both open- and\\nclosed-source MLLMs reveal persistent, context-sensitive gender biases that are\\nnot evident in single-character settings. Our findings underscore the\\nimportance of relationship-aware benchmarks for diagnosing subtle,\\ninteraction-driven gender bias in MLLMs and provide actionable insights for\\nfuture bias mitigation.', 'authors': ['Yue Xu', 'Wenjie Wang'], 'published': '2025-06-29T06:03:21+00:00', 'categories': ['cs.CL', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23101v1'}"}, {'id': 129, 'arxiv_id': '2506.23094v1', 'base_arxiv_id': '2506.23094', 'version': '1', 'title': 'TOMI: Transforming and Organizing Music Ideas for Multi-Track Compositions with Full-Song Structure', 'abstract': 'Hierarchical planning is a powerful approach to model long sequences\nstructurally. Aside from considering hierarchies in the temporal structure of\nmusic, this paper explores an even more important aspect: concept hierarchy,\nwhich involves generating music ideas, transforming them, and ultimately\norganizing them--across musical time and space--into a complete composition. To\nthis end, we introduce TOMI (Transforming and Organizing Music Ideas) as a\nnovel approach in deep music generation and develop a TOMI-based model via\ninstruction-tuned foundation LLM. Formally, we represent a multi-track\ncomposition process via a sparse, four-dimensional space characterized by clips\n(short audio or MIDI segments), sections (temporal positions), tracks\n(instrument layers), and transformations (elaboration methods). Our model is\ncapable of generating multi-track electronic music with full-song structure,\nand we further integrate the TOMI-based model with the REAPER digital audio\nworkstation, enabling interactive human-AI co-creation. Experimental results\ndemonstrate that our approach produces higher-quality electronic music with\nstronger structural coherence compared to baselines.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23094v1', 'title': 'TOMI: Transforming and Organizing Music Ideas for Multi-Track Compositions with Full-Song Structure', 'abstract': 'Hierarchical planning is a powerful approach to model long sequences\\nstructurally. Aside from considering hierarchies in the temporal structure of\\nmusic, this paper explores an even more important aspect: concept hierarchy,\\nwhich involves generating music ideas, transforming them, and ultimately\\norganizing them--across musical time and space--into a complete composition. To\\nthis end, we introduce TOMI (Transforming and Organizing Music Ideas) as a\\nnovel approach in deep music generation and develop a TOMI-based model via\\ninstruction-tuned foundation LLM. Formally, we represent a multi-track\\ncomposition process via a sparse, four-dimensional space characterized by clips\\n(short audio or MIDI segments), sections (temporal positions), tracks\\n(instrument layers), and transformations (elaboration methods). Our model is\\ncapable of generating multi-track electronic music with full-song structure,\\nand we further integrate the TOMI-based model with the REAPER digital audio\\nworkstation, enabling interactive human-AI co-creation. Experimental results\\ndemonstrate that our approach produces higher-quality electronic music with\\nstronger structural coherence compared to baselines.', 'authors': ['Qi He', 'Gus Xia', 'Ziyu Wang'], 'published': '2025-06-29T05:15:41+00:00', 'categories': ['cs.SD', 'cs.AI', 'eess.AS'], 'url': 'http://arxiv.org/abs/2506.23094v1'}"}, {'id': 130, 'arxiv_id': '2506.23080v1', 'base_arxiv_id': '2506.23080', 'version': '1', 'title': "AI's Euclid's Elements Moment: From Language Models to Computable Thought", 'abstract': 'This paper presents a comprehensive five-stage evolutionary framework for\nunderstanding the development of artificial intelligence, arguing that its\ntrajectory mirrors the historical progression of human cognitive technologies.\nWe posit that AI is advancing through distinct epochs, each defined by a\nrevolutionary shift in its capacity for representation and reasoning, analogous\nto the inventions of cuneiform, the alphabet, grammar and logic, mathematical\ncalculus, and formal logical systems. This "Geometry of Cognition" framework\nmoves beyond mere metaphor to provide a systematic, cross-disciplinary model\nthat not only explains AI\'s past architectural shifts-from expert systems to\nTransformers-but also charts a concrete and prescriptive path forward.\nCrucially, we demonstrate that this evolution is not merely linear but\nreflexive: as AI advances through these stages, the tools and insights it\ndevelops create a feedback loop that fundamentally reshapes its own underlying\narchitecture. We are currently transitioning into a "Metalinguistic Moment,"\ncharacterized by the emergence of self-reflective capabilities like\nChain-of-Thought prompting and Constitutional AI. The subsequent stages, the\n"Mathematical Symbolism Moment" and the "Formal Logic System Moment," will be\ndefined by the development of a computable calculus of thought, likely through\nneuro-symbolic architectures and program synthesis, culminating in provably\naligned and reliable AI that reconstructs its own foundational representations.\nThis work serves as the methodological capstone to our trilogy, which\npreviously explored the economic drivers ("why") and cognitive nature ("what")\nof AI. Here, we address the "how," providing a theoretical foundation for\nfuture research and offering concrete, actionable strategies for startups and\ndevelopers aiming to build the next generation of intelligent systems.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': '{\'id\': \'2506.23080v1\', \'title\': "AI\'s Euclid\'s Elements Moment: From Language Models to Computable Thought", \'abstract\': \'This paper presents a comprehensive five-stage evolutionary framework for\\nunderstanding the development of artificial intelligence, arguing that its\\ntrajectory mirrors the historical progression of human cognitive technologies.\\nWe posit that AI is advancing through distinct epochs, each defined by a\\nrevolutionary shift in its capacity for representation and reasoning, analogous\\nto the inventions of cuneiform, the alphabet, grammar and logic, mathematical\\ncalculus, and formal logical systems. This "Geometry of Cognition" framework\\nmoves beyond mere metaphor to provide a systematic, cross-disciplinary model\\nthat not only explains AI\\\'s past architectural shifts-from expert systems to\\nTransformers-but also charts a concrete and prescriptive path forward.\\nCrucially, we demonstrate that this evolution is not merely linear but\\nreflexive: as AI advances through these stages, the tools and insights it\\ndevelops create a feedback loop that fundamentally reshapes its own underlying\\narchitecture. We are currently transitioning into a "Metalinguistic Moment,"\\ncharacterized by the emergence of self-reflective capabilities like\\nChain-of-Thought prompting and Constitutional AI. The subsequent stages, the\\n"Mathematical Symbolism Moment" and the "Formal Logic System Moment," will be\\ndefined by the development of a computable calculus of thought, likely through\\nneuro-symbolic architectures and program synthesis, culminating in provably\\naligned and reliable AI that reconstructs its own foundational representations.\\nThis work serves as the methodological capstone to our trilogy, which\\npreviously explored the economic drivers ("why") and cognitive nature ("what")\\nof AI. Here, we address the "how," providing a theoretical foundation for\\nfuture research and offering concrete, actionable strategies for startups and\\ndevelopers aiming to build the next generation of intelligent systems.\', \'authors\': [\'Xinmin Fang\', \'Lingfeng Tao\', \'Zhengxiong Li\'], \'published\': \'2025-06-29T04:14:19+00:00\', \'categories\': [\'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23080v1\'}'}, {'id': 131, 'arxiv_id': '2506.23068v1', 'base_arxiv_id': '2506.23068', 'version': '1', 'title': 'Curious Causality-Seeking Agents Learn Meta Causal World', 'abstract': "When building a world model, a common assumption is that the environment has\na single, unchanging underlying causal rule, like applying Newton's laws to\nevery situation. In reality, what appears as a drifting causal mechanism is\noften the manifestation of a fixed underlying mechanism seen through a narrow\nobservational window. This brings about a problem that, when building a world\nmodel, even subtle shifts in policy or environment states can alter the very\nobserved causal mechanisms. In this work, we introduce the Meta-Causal\nGraph as world models, a minimal unified representation that efficiently\nencodes the transformation rules governing how causal structures shift across\ndifferent latent world states. A single Meta-Causal Graph is composed of\nmultiple causal subgraphs, each triggered by meta state, which is in the latent\nstate space. Building on this representation, we introduce a\nCausality-Seeking Agent whose objectives are to (1) identify the meta\nstates that trigger each subgraph, (2) discover the corresponding causal\nrelationships by agent curiosity-driven intervention policy, and (3)\niteratively refine the Meta-Causal Graph through ongoing curiosity-driven\nexploration and agent experiences. Experiments on both synthetic tasks and a\nchallenging robot arm manipulation task demonstrate that our method robustly\ncaptures shifts in causal dynamics and generalizes effectively to previously\nunseen contexts.", 'submit_date': datetime.date(2025, 6, 29), 'metadata': '{\'id\': \'2506.23068v1\', \'title\': \'Curious Causality-Seeking Agents Learn Meta Causal World\', \'abstract\': "When building a world model, a common assumption is that the environment has\\na single, unchanging underlying causal rule, like applying Newton\'s laws to\\nevery situation. In reality, what appears as a drifting causal mechanism is\\noften the manifestation of a fixed underlying mechanism seen through a narrow\\nobservational window. This brings about a problem that, when building a world\\nmodel, even subtle shifts in policy or environment states can alter the very\\nobserved causal mechanisms. In this work, we introduce the \\\\textbf{Meta-Causal\\nGraph} as world models, a minimal unified representation that efficiently\\nencodes the transformation rules governing how causal structures shift across\\ndifferent latent world states. A single Meta-Causal Graph is composed of\\nmultiple causal subgraphs, each triggered by meta state, which is in the latent\\nstate space. Building on this representation, we introduce a\\n\\\\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta\\nstates that trigger each subgraph, (2) discover the corresponding causal\\nrelationships by agent curiosity-driven intervention policy, and (3)\\niteratively refine the Meta-Causal Graph through ongoing curiosity-driven\\nexploration and agent experiences. Experiments on both synthetic tasks and a\\nchallenging robot arm manipulation task demonstrate that our method robustly\\ncaptures shifts in causal dynamics and generalizes effectively to previously\\nunseen contexts.", \'authors\': [\'Zhiyu Zhao\', \'Haoxuan Li\', \'Haifeng Zhang\', \'Jun Wang\', \'Francesco Faccio\', \'J√ºrgen Schmidhuber\', \'Mengyue Yang\'], \'published\': \'2025-06-29T03:05:25+00:00\', \'categories\': [\'cs.LG\', \'cs.AI\', \'stat.AP\'], \'url\': \'http://arxiv.org/abs/2506.23068v1\'}'}, {'id': 132, 'arxiv_id': '2506.23055v1', 'base_arxiv_id': '2506.23055', 'version': '1', 'title': 'Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis', 'abstract': "Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities\nin producing human-like text. However, it is unclear how accurately these\nmodels internalize concepts that shape human thought and behavior. Here, we\ndeveloped a quantitative framework to assess concept alignment between LLMs and\nhuman psychological dimensions using 43 standardized psychological\nquestionnaires, selected for their established validity in measuring distinct\npsychological constructs. Our method evaluates how accurately language models\nreconstruct and classify questionnaire items through pairwise similarity\nanalysis. We compared resulting cluster structures with the original\ncategorical labels using hierarchical clustering. A GPT-4 model achieved\nsuperior classification accuracy (66.2\\%), significantly outperforming GPT-3.5\n(55.9\\%) and BERT (48.1\\%), all exceeding random baseline performance (31.9\\%).\nWe also demonstrated that the estimated semantic similarity from GPT-4 is\nassociated with Pearson's correlation coefficients of human responses in\nmultiple psychological questionnaires. This framework provides a novel approach\nto evaluate the alignment of the human-LLM concept and identify potential\nrepresentational biases. Our findings demonstrate that modern LLMs can\napproximate human psychological constructs with measurable accuracy, offering\ninsights for developing more interpretable AI systems.", 'submit_date': datetime.date(2025, 6, 29), 'metadata': '{\'id\': \'2506.23055v1\', \'title\': \'Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis\', \'abstract\': "Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities\\nin producing human-like text. However, it is unclear how accurately these\\nmodels internalize concepts that shape human thought and behavior. Here, we\\ndeveloped a quantitative framework to assess concept alignment between LLMs and\\nhuman psychological dimensions using 43 standardized psychological\\nquestionnaires, selected for their established validity in measuring distinct\\npsychological constructs. Our method evaluates how accurately language models\\nreconstruct and classify questionnaire items through pairwise similarity\\nanalysis. We compared resulting cluster structures with the original\\ncategorical labels using hierarchical clustering. A GPT-4 model achieved\\nsuperior classification accuracy (66.2\\\\%), significantly outperforming GPT-3.5\\n(55.9\\\\%) and BERT (48.1\\\\%), all exceeding random baseline performance (31.9\\\\%).\\nWe also demonstrated that the estimated semantic similarity from GPT-4 is\\nassociated with Pearson\'s correlation coefficients of human responses in\\nmultiple psychological questionnaires. This framework provides a novel approach\\nto evaluate the alignment of the human-LLM concept and identify potential\\nrepresentational biases. Our findings demonstrate that modern LLMs can\\napproximate human psychological constructs with measurable accuracy, offering\\ninsights for developing more interpretable AI systems.", \'authors\': [\'Hiro Taiyo Hamada\', \'Ippei Fujisawa\', \'Genji Kawakita\', \'Yuki Yamada\'], \'published\': \'2025-06-29T01:56:56+00:00\', \'categories\': [\'cs.LG\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23055v1\'}'}, {'id': 133, 'arxiv_id': '2506.23049v1', 'base_arxiv_id': '2506.23049', 'version': '1', 'title': 'AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks', 'abstract': 'Despite advances in language and speech technologies, no open-source system\nenables full speech-to-speech, multi-turn dialogue with integrated tool use and\nagentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and\nAutomated Tool Use), the first open-source, speech-native assistant capable of\ncompleting complex, goal-driven tasks through dynamic tool invocation and\nmulti-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a\ncascaded pipeline and supports tools such as calendar booking, contact lookup,\nweb search, and email. Its modular design allows easy integration of new tools\nusing natural language prompts and action classes. On VoiceBench, AURA scores\n92.75% on OpenBookQA-outperforming all open-weight systems and nearing\nGPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.\nHuman evaluation shows 90% task success on complex, multi-turn speech tasks.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23049v1', 'title': 'AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks', 'abstract': 'Despite advances in language and speech technologies, no open-source system\\nenables full speech-to-speech, multi-turn dialogue with integrated tool use and\\nagentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and\\nAutomated Tool Use), the first open-source, speech-native assistant capable of\\ncompleting complex, goal-driven tasks through dynamic tool invocation and\\nmulti-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a\\ncascaded pipeline and supports tools such as calendar booking, contact lookup,\\nweb search, and email. Its modular design allows easy integration of new tools\\nusing natural language prompts and action classes. On VoiceBench, AURA scores\\n92.75% on OpenBookQA-outperforming all open-weight systems and nearing\\nGPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.\\nHuman evaluation shows 90% task success on complex, multi-turn speech tasks.', 'authors': ['Leander Melroy Maben', 'Gayathri Ganesh Lakshmy', 'Srijith Radhakrishnan', 'Siddhant Arora', 'Shinji Watanabe'], 'published': '2025-06-29T01:13:15+00:00', 'categories': ['cs.AI', 'cs.CL', 'cs.SD', 'eess.AS', '68T42, 68T50,', 'I.2.7; I.2.11; H.5.5'], 'url': 'http://arxiv.org/abs/2506.23049v1'}"}, {'id': 134, 'arxiv_id': '2506.23046v1', 'base_arxiv_id': '2506.23046', 'version': '1', 'title': 'SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions', 'abstract': "Humans continuously infer the states, goals, and behaviors of others by\nperceiving their surroundings in dynamic, real-world social interactions.\nHowever, most Theory of Mind (ToM) benchmarks only evaluate static, text-based\nscenarios, which have a significant gap compared to real interactions. We\npropose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in\nembodied multi-agent complex social interactions. This benchmark is based on\nrich multimodal interaction data generated by the interaction environment SoMi,\ncovering diverse crafting goals and social relationships. Our framework\nsupports multi-level evaluation: (1) first-person evaluation provides\nmultimodal (visual, dialogue, action, etc.) input from a first-person\nperspective during a task for real-time state inference, (2) third-person\nevaluation provides complete third-person perspective video and text records\nafter a task for goal and behavior inference. This evaluation method allows for\na more comprehensive examination of a model's ToM capabilities from both the\nsubjective immediate experience and the objective global observation. We\nconstructed a challenging dataset containing 35 third-person perspective\nvideos, 363 first-person perspective images, and 1225 expert-annotated\nmultiple-choice questions (three options). On this dataset, we systematically\nevaluated the performance of human subjects and several state-of-the-art large\nvision-language models (LVLMs). The results show that LVLMs perform\nsignificantly worse than humans on SoMi-ToM: the average accuracy gap between\nhumans and models is 40.1% in first-person evaluation and 26.4% in third-person\nevaluation. This indicates that future LVLMs need to further improve their ToM\ncapabilities in embodied, complex social interactions.", 'submit_date': datetime.date(2025, 6, 29), 'metadata': '{\'id\': \'2506.23046v1\', \'title\': \'SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions\', \'abstract\': "Humans continuously infer the states, goals, and behaviors of others by\\nperceiving their surroundings in dynamic, real-world social interactions.\\nHowever, most Theory of Mind (ToM) benchmarks only evaluate static, text-based\\nscenarios, which have a significant gap compared to real interactions. We\\npropose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in\\nembodied multi-agent complex social interactions. This benchmark is based on\\nrich multimodal interaction data generated by the interaction environment SoMi,\\ncovering diverse crafting goals and social relationships. Our framework\\nsupports multi-level evaluation: (1) first-person evaluation provides\\nmultimodal (visual, dialogue, action, etc.) input from a first-person\\nperspective during a task for real-time state inference, (2) third-person\\nevaluation provides complete third-person perspective video and text records\\nafter a task for goal and behavior inference. This evaluation method allows for\\na more comprehensive examination of a model\'s ToM capabilities from both the\\nsubjective immediate experience and the objective global observation. We\\nconstructed a challenging dataset containing 35 third-person perspective\\nvideos, 363 first-person perspective images, and 1225 expert-annotated\\nmultiple-choice questions (three options). On this dataset, we systematically\\nevaluated the performance of human subjects and several state-of-the-art large\\nvision-language models (LVLMs). The results show that LVLMs perform\\nsignificantly worse than humans on SoMi-ToM: the average accuracy gap between\\nhumans and models is 40.1% in first-person evaluation and 26.4% in third-person\\nevaluation. This indicates that future LVLMs need to further improve their ToM\\ncapabilities in embodied, complex social interactions.", \'authors\': [\'Xianzhe Fan\', \'Xuhui Zhou\', \'Chuanyang Jin\', \'Kolby Nottingham\', \'Hao Zhu\', \'Maarten Sap\'], \'published\': \'2025-06-29T00:54:13+00:00\', \'categories\': [\'cs.CL\', \'cs.AI\', \'cs.CV\', \'cs.RO\'], \'url\': \'http://arxiv.org/abs/2506.23046v1\'}'}, {'id': 135, 'arxiv_id': '2506.23044v1', 'base_arxiv_id': '2506.23044', 'version': '1', 'title': 'Ovis-U1 Technical Report', 'abstract': 'In this report, we introduce Ovis-U1, a 3-billion-parameter unified model\nthat integrates multimodal understanding, text-to-image generation, and image\nediting capabilities. Building on the foundation of the Ovis series, Ovis-U1\nincorporates a diffusion-based visual decoder paired with a bidirectional token\nrefiner, enabling image generation tasks comparable to leading models like\nGPT-4o. Unlike some previous models that use a frozen MLLM for generation\ntasks, Ovis-U1 utilizes a new unified training approach starting from a\nlanguage model. Compared to training solely on understanding or generation\ntasks, unified training yields better performance, demonstrating the\nenhancement achieved by integrating these two tasks. Ovis-U1 achieves a score\nof 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent\nstate-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In\ntext-to-image generation, it excels with scores of 83.72 and 0.89 on the\nDPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves\n4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the\ninitial version of the Ovis unified model series, Ovis-U1 pushes the boundaries\nof multimodal understanding, generation, and editing.', 'submit_date': datetime.date(2025, 6, 29), 'metadata': "{'id': '2506.23044v1', 'title': 'Ovis-U1 Technical Report', 'abstract': 'In this report, we introduce Ovis-U1, a 3-billion-parameter unified model\\nthat integrates multimodal understanding, text-to-image generation, and image\\nediting capabilities. Building on the foundation of the Ovis series, Ovis-U1\\nincorporates a diffusion-based visual decoder paired with a bidirectional token\\nrefiner, enabling image generation tasks comparable to leading models like\\nGPT-4o. Unlike some previous models that use a frozen MLLM for generation\\ntasks, Ovis-U1 utilizes a new unified training approach starting from a\\nlanguage model. Compared to training solely on understanding or generation\\ntasks, unified training yields better performance, demonstrating the\\nenhancement achieved by integrating these two tasks. Ovis-U1 achieves a score\\nof 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent\\nstate-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In\\ntext-to-image generation, it excels with scores of 83.72 and 0.89 on the\\nDPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves\\n4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the\\ninitial version of the Ovis unified model series, Ovis-U1 pushes the boundaries\\nof multimodal understanding, generation, and editing.', 'authors': ['Guo-Hua Wang', 'Shanshan Zhao', 'Xinjie Zhang', 'Liangfu Cao', 'Pengxin Zhan', 'Lunhao Duan', 'Shiyin Lu', 'Minghao Fu', 'Xiaohao Chen', 'Jianshan Zhao', 'Yang Li', 'Qing-Guo Chen'], 'published': '2025-06-29T00:40:17+00:00', 'categories': ['cs.CV', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23044v1'}"}, {'id': 136, 'arxiv_id': '2506.23040v1', 'base_arxiv_id': '2506.23040', 'version': '1', 'title': 'Treatment, evidence, imitation, and chat', 'abstract': "Large language models are thought to have potential to aid in medical\ndecision making. We investigate this here. We start with the treatment problem,\nthe patient's core medical decision-making task, which is solved in\ncollaboration with a healthcare provider. We discuss approaches to solving the\ntreatment problem, including -- within evidence-based medicine -- trials and\nobservational data. We then discuss the chat problem, and how this differs from\nthe treatment problem -- in particular as it relates to imitation. We then\ndiscuss how a large language model might be used to solve the treatment problem\nand highlight some of the challenges that emerge. We finally discuss how these\nchallenges relate to evidence-based medicine, and how this might inform next\nsteps.", 'submit_date': datetime.date(2025, 6, 29), 'metadata': '{\'id\': \'2506.23040v1\', \'title\': \'Treatment, evidence, imitation, and chat\', \'abstract\': "Large language models are thought to have potential to aid in medical\\ndecision making. We investigate this here. We start with the treatment problem,\\nthe patient\'s core medical decision-making task, which is solved in\\ncollaboration with a healthcare provider. We discuss approaches to solving the\\ntreatment problem, including -- within evidence-based medicine -- trials and\\nobservational data. We then discuss the chat problem, and how this differs from\\nthe treatment problem -- in particular as it relates to imitation. We then\\ndiscuss how a large language model might be used to solve the treatment problem\\nand highlight some of the challenges that emerge. We finally discuss how these\\nchallenges relate to evidence-based medicine, and how this might inform next\\nsteps.", \'authors\': [\'Samuel J. Weisenthal\'], \'published\': \'2025-06-29T00:23:06+00:00\', \'categories\': [\'stat.OT\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23040v1\'}'}, {'id': 137, 'arxiv_id': '2506.23030v1', 'base_arxiv_id': '2506.23030', 'version': '1', 'title': 'VisionScores -- A system-segmented image score dataset for deep learning tasks', 'abstract': "VisionScores presents a novel proposal being the first system-segmented image\nscore dataset, aiming to offer structure-rich, high information-density images\nfor machine and deep learning tasks. Delimited to two-handed piano pieces, it\nwas built to consider not only certain graphic similarity but also composition\npatterns, as this creative process is highly instrument-dependent. It provides\ntwo scenarios in relation to composer and composition type. The first, formed\nby 14k samples, considers works from different authors but the same composition\ntype, specifically, Sonatinas. The latter, consisting of 10.8K samples,\npresents the opposite case, various composition types from the same author,\nbeing the one selected Franz Liszt. All of the 24.8k samples are formatted as\ngrayscale jpg images of $128 512$ pixels. VisionScores supplies the\nusers not only the formatted samples but the systems' order and pieces'\nmetadata. Moreover, unsegmented full-page scores and the pre-formatted images\nare included for further analysis.", 'submit_date': datetime.date(2025, 6, 28), 'metadata': '{\'id\': \'2506.23030v1\', \'title\': \'VisionScores -- A system-segmented image score dataset for deep learning tasks\', \'abstract\': "VisionScores presents a novel proposal being the first system-segmented image\\nscore dataset, aiming to offer structure-rich, high information-density images\\nfor machine and deep learning tasks. Delimited to two-handed piano pieces, it\\nwas built to consider not only certain graphic similarity but also composition\\npatterns, as this creative process is highly instrument-dependent. It provides\\ntwo scenarios in relation to composer and composition type. The first, formed\\nby 14k samples, considers works from different authors but the same composition\\ntype, specifically, Sonatinas. The latter, consisting of 10.8K samples,\\npresents the opposite case, various composition types from the same author,\\nbeing the one selected Franz Liszt. All of the 24.8k samples are formatted as\\ngrayscale jpg images of $128 \\\\times 512$ pixels. VisionScores supplies the\\nusers not only the formatted samples but the systems\' order and pieces\'\\nmetadata. Moreover, unsegmented full-page scores and the pre-formatted images\\nare included for further analysis.", \'authors\': [\'Alejandro Romero Amezcua\', \'Mariano Jos√© Juan Rivera Meraz\'], \'published\': \'2025-06-28T22:29:23+00:00\', \'categories\': [\'cs.CV\', \'cs.AI\', \'cs.LG\', \'cs.SD\', \'eess.AS\'], \'url\': \'http://arxiv.org/abs/2506.23030v1\'}'}, {'id': 138, 'arxiv_id': '2506.23025v1', 'base_arxiv_id': '2506.23025', 'version': '1', 'title': 'Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models', 'abstract': 'Large language models (LLMs) are increasingly used across research and\nindustry applications, yet their inference efficiency remains a significant\nchallenge. As the computational power of modern GPU architectures continuously\nimproves, their memory bandwidth and capacity have not scaled proportionally,\ncreating a critical bottleneck during inference. To address this, we\ninvestigate ternary language models (TriLMs) that employ quantization-aware\ntraining to significantly reduce memory requirements. We first analyze the\nscalability of TriLMs by conducting a scaling law analysis, revealing that\nTriLMs benefit more from increasing training data than from scaling model\nparameters. Based on this observation, we introduce Spectra-1.1, an open suite\nof TriLMs trained on up to 1.2 trillion tokens, demonstrating sustained\nperformance gains at scale. Furthermore, to improve inference efficiency, we\npropose novel 2-bit and 1.6-bit packing schemes for ternary weights, which\ndemonstrate accelerated inference across various CPU architectures. Also,\nbuilding on the 2-bit packing, we develop a GPU kernel called TriRun that\naccelerates end-to-end model inference by up to 5 times compared to\nfloating-point baselines. To encourage further exploration and development of\nTriLMs, we will release the Spectra-1.1 suite and TriRun inference kernels.\nOverall, our work lays the foundation for building and deploying efficient\nLLMs, providing a valuable resource for the research community.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.23025v1', 'title': 'Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models', 'abstract': 'Large language models (LLMs) are increasingly used across research and\\nindustry applications, yet their inference efficiency remains a significant\\nchallenge. As the computational power of modern GPU architectures continuously\\nimproves, their memory bandwidth and capacity have not scaled proportionally,\\ncreating a critical bottleneck during inference. To address this, we\\ninvestigate ternary language models (TriLMs) that employ quantization-aware\\ntraining to significantly reduce memory requirements. We first analyze the\\nscalability of TriLMs by conducting a scaling law analysis, revealing that\\nTriLMs benefit more from increasing training data than from scaling model\\nparameters. Based on this observation, we introduce Spectra-1.1, an open suite\\nof TriLMs trained on up to 1.2 trillion tokens, demonstrating sustained\\nperformance gains at scale. Furthermore, to improve inference efficiency, we\\npropose novel 2-bit and 1.6-bit packing schemes for ternary weights, which\\ndemonstrate accelerated inference across various CPU architectures. Also,\\nbuilding on the 2-bit packing, we develop a GPU kernel called TriRun that\\naccelerates end-to-end model inference by up to 5 times compared to\\nfloating-point baselines. To encourage further exploration and development of\\nTriLMs, we will release the Spectra-1.1 suite and TriRun inference kernels.\\nOverall, our work lays the foundation for building and deploying efficient\\nLLMs, providing a valuable resource for the research community.', 'authors': ['Tejas Vaidhya', 'Ayush Kaushal', 'Vineet Jain', 'Francis Couture Harpin', 'Prashant Shishodia', 'Majid Behbahani', 'Yuriy Nevmyvaka', 'Irina Rish'], 'published': '2025-06-28T22:13:43+00:00', 'categories': ['cs.LG', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.23025v1'}"}, {'id': 139, 'arxiv_id': '2506.23024v1', 'base_arxiv_id': '2506.23024', 'version': '1', 'title': 'BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs', 'abstract': "Physics-informed neural networks (PINNs) offer a flexible way to solve\npartial differential equations (PDEs) with machine learning, yet they still\nfall well short of the machine-precision accuracy many scientific tasks demand.\nIn this work, we investigate whether the precision ceiling comes from the\nill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP)\narchitecture. We introduce the Barycentric Weight Layer (BWLer), which models\nthe PDE solution through barycentric polynomial interpolation. A BWLer can be\nadded on top of an existing MLP (a BWLer-hat) or replace it completely\n(explicit BWLer), cleanly separating how we represent the solution from how we\ntake derivatives for the PDE loss. Using BWLer, we identify fundamental\nprecision limitations within the MLP: on a simple 1-D interpolation task, even\nMLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above\nfloat64 machine precision -- before any PDE terms are added. In PDE learning,\nadding a BWLer lifts this ceiling and exposes a tradeoff between achievable\naccuracy and the conditioning of the PDE loss. For linear PDEs we fully\ncharacterize this tradeoff with an explicit error decomposition and navigate it\nduring training with spectral derivatives and preconditioning. Across five\nbenchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for\nconvection, 10x for reaction, and 1800x for wave equations while remaining\ncompatible with first-order optimizers. Replacing the MLP entirely lets an\nexplicit BWLer reach near-machine-precision on convection, reaction, and wave\nproblems (up to 10 billion times better than prior results) and match the\nperformance of standard PINNs on stiff Burgers' and irregular-geometry Poisson\nproblems. Together, these findings point to a practical path for combining the\nflexibility of PINNs with the precision of classical spectral solvers.", 'submit_date': datetime.date(2025, 6, 28), 'metadata': '{\'id\': \'2506.23024v1\', \'title\': \'BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs\', \'abstract\': "Physics-informed neural networks (PINNs) offer a flexible way to solve\\npartial differential equations (PDEs) with machine learning, yet they still\\nfall well short of the machine-precision accuracy many scientific tasks demand.\\nIn this work, we investigate whether the precision ceiling comes from the\\nill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP)\\narchitecture. We introduce the Barycentric Weight Layer (BWLer), which models\\nthe PDE solution through barycentric polynomial interpolation. A BWLer can be\\nadded on top of an existing MLP (a BWLer-hat) or replace it completely\\n(explicit BWLer), cleanly separating how we represent the solution from how we\\ntake derivatives for the PDE loss. Using BWLer, we identify fundamental\\nprecision limitations within the MLP: on a simple 1-D interpolation task, even\\nMLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above\\nfloat64 machine precision -- before any PDE terms are added. In PDE learning,\\nadding a BWLer lifts this ceiling and exposes a tradeoff between achievable\\naccuracy and the conditioning of the PDE loss. For linear PDEs we fully\\ncharacterize this tradeoff with an explicit error decomposition and navigate it\\nduring training with spectral derivatives and preconditioning. Across five\\nbenchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for\\nconvection, 10x for reaction, and 1800x for wave equations while remaining\\ncompatible with first-order optimizers. Replacing the MLP entirely lets an\\nexplicit BWLer reach near-machine-precision on convection, reaction, and wave\\nproblems (up to 10 billion times better than prior results) and match the\\nperformance of standard PINNs on stiff Burgers\' and irregular-geometry Poisson\\nproblems. Together, these findings point to a practical path for combining the\\nflexibility of PINNs with the precision of classical spectral solvers.", \'authors\': [\'Jerry Liu\', \'Yasa Baig\', \'Denise Hui Jean Lee\', \'Rajat Vadiraj Dwaraknath\', \'Atri Rudra\', \'Chris R√©\'], \'published\': \'2025-06-28T22:11:00+00:00\', \'categories\': [\'cs.LG\', \'cs.AI\', \'cs.NA\', \'math.NA\'], \'url\': \'http://arxiv.org/abs/2506.23024v1\'}'}, {'id': 141, 'arxiv_id': '2506.23014v1', 'base_arxiv_id': '2506.23014', 'version': '1', 'title': 'Generating Privacy Stories From Software Documentation', 'abstract': "Research shows that analysts and developers consider privacy as a security\nconcept or as an afterthought, which may lead to non-compliance and violation\nof users' privacy. Most current approaches, however, focus on extracting legal\nrequirements from the regulations and evaluating the compliance of software and\nprocesses with them. In this paper, we develop a novel approach based on\nchain-of-thought prompting (CoT), in-context-learning (ICL), and Large Language\nModels (LLMs) to extract privacy behaviors from various software documents\nprior to and during software development, and then generate privacy\nrequirements in the format of user stories. Our results show that most commonly\nused LLMs, such as GPT-4o and Llama 3, can identify privacy behaviors and\ngenerate privacy user stories with F1 scores exceeding 0.8. We also show that\nthe performance of these models could be improved through parameter-tuning. Our\nfindings provide insight into using and optimizing LLMs for generating privacy\nrequirements given software documents created prior to or throughout the\nsoftware development lifecycle.", 'submit_date': datetime.date(2025, 6, 28), 'metadata': '{\'id\': \'2506.23014v1\', \'title\': \'Generating Privacy Stories From Software Documentation\', \'abstract\': "Research shows that analysts and developers consider privacy as a security\\nconcept or as an afterthought, which may lead to non-compliance and violation\\nof users\' privacy. Most current approaches, however, focus on extracting legal\\nrequirements from the regulations and evaluating the compliance of software and\\nprocesses with them. In this paper, we develop a novel approach based on\\nchain-of-thought prompting (CoT), in-context-learning (ICL), and Large Language\\nModels (LLMs) to extract privacy behaviors from various software documents\\nprior to and during software development, and then generate privacy\\nrequirements in the format of user stories. Our results show that most commonly\\nused LLMs, such as GPT-4o and Llama 3, can identify privacy behaviors and\\ngenerate privacy user stories with F1 scores exceeding 0.8. We also show that\\nthe performance of these models could be improved through parameter-tuning. Our\\nfindings provide insight into using and optimizing LLMs for generating privacy\\nrequirements given software documents created prior to or throughout the\\nsoftware development lifecycle.", \'authors\': [\'Wilder Baldwin\', \'Shashank Chintakuntla\', \'Shreyah Parajuli\', \'Ali Pourghasemi\', \'Ryan Shanz\', \'Sepideh Ghanavati\'], \'published\': \'2025-06-28T20:55:21+00:00\', \'categories\': [\'cs.SE\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.23014v1\'}'}, {'id': 142, 'arxiv_id': '2506.22992v1', 'base_arxiv_id': '2506.22992', 'version': '1', 'title': 'MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning', 'abstract': 'The ability to process information from multiple modalities and to reason\nthrough it step-by-step remains a critical challenge in advancing artificial\nintelligence. However, existing reasoning benchmarks focus on text-only\nreasoning, or employ multimodal questions that can be answered by directly\nretrieving information from a non-text modality. Thus, complex reasoning\nremains poorly understood in multimodal domains. Here, we present MARBLE, a\nchallenging multimodal reasoning benchmark that is designed to scrutinize\nmultimodal language models (MLLMs) in their ability to carefully reason\nstep-by-step through complex multimodal problems and environments. MARBLE is\ncomposed of two highly challenging tasks, M-Portal and M-Cube, that require the\ncrafting and understanding of multistep plans under spatial, visual, and\nphysical constraints. We find that current MLLMs perform poorly on MARBLE --\nall the 12 advanced models obtain near-random performance on M-Portal and 0%\naccuracy on M-Cube. Only in simplified subtasks some models outperform the\nrandom baseline, indicating that complex reasoning is still a challenge for\nexisting MLLMs. Moreover, we show that perception remains a bottleneck, where\nMLLMs occasionally fail to extract information from the visual inputs. By\nshedding a light on the limitations of MLLMs, we hope that MARBLE will spur the\ndevelopment of the next generation of models with the ability to reason and\nplan across many, multimodal reasoning steps.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22992v1', 'title': 'MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning', 'abstract': 'The ability to process information from multiple modalities and to reason\\nthrough it step-by-step remains a critical challenge in advancing artificial\\nintelligence. However, existing reasoning benchmarks focus on text-only\\nreasoning, or employ multimodal questions that can be answered by directly\\nretrieving information from a non-text modality. Thus, complex reasoning\\nremains poorly understood in multimodal domains. Here, we present MARBLE, a\\nchallenging multimodal reasoning benchmark that is designed to scrutinize\\nmultimodal language models (MLLMs) in their ability to carefully reason\\nstep-by-step through complex multimodal problems and environments. MARBLE is\\ncomposed of two highly challenging tasks, M-Portal and M-Cube, that require the\\ncrafting and understanding of multistep plans under spatial, visual, and\\nphysical constraints. We find that current MLLMs perform poorly on MARBLE --\\nall the 12 advanced models obtain near-random performance on M-Portal and 0%\\naccuracy on M-Cube. Only in simplified subtasks some models outperform the\\nrandom baseline, indicating that complex reasoning is still a challenge for\\nexisting MLLMs. Moreover, we show that perception remains a bottleneck, where\\nMLLMs occasionally fail to extract information from the visual inputs. By\\nshedding a light on the limitations of MLLMs, we hope that MARBLE will spur the\\ndevelopment of the next generation of models with the ability to reason and\\nplan across many, multimodal reasoning steps.', 'authors': ['Yulun Jiang', 'Yekun Chai', 'Maria Brbiƒá', 'Michael Moor'], 'published': '2025-06-28T19:44:32+00:00', 'categories': ['cs.AI', 'cs.CL', 'cs.CV'], 'url': 'http://arxiv.org/abs/2506.22992v1'}"}, {'id': 143, 'arxiv_id': '2506.22978v1', 'base_arxiv_id': '2506.22978', 'version': '1', 'title': 'A Systematic Study of Compositional Syntactic Transformer Language Models', 'abstract': 'Syntactic language models (SLMs) enhance Transformers by incorporating\nsyntactic biases through the modeling of linearized syntactic parse trees\nalongside surface sentences. This paper focuses on compositional SLMs that are\nbased on constituency parse trees and contain explicit bottom-up composition of\nconstituent representations. We identify key aspects of design choices in\nexisting compositional SLMs and propose a unified framework encompassing both\nexisting models and novel variants. We conduct a comprehensive empirical\nevaluation of all the variants in our framework across language modeling,\nsyntactic generalization, summarization, dialogue, and inference efficiency.\nBased on the experimental results, we make multiple recommendations on the\ndesign of compositional SLMs. Our code is released at\nhttps://github.com/zhaoyd1/compositional_SLMs.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22978v1', 'title': 'A Systematic Study of Compositional Syntactic Transformer Language Models', 'abstract': 'Syntactic language models (SLMs) enhance Transformers by incorporating\\nsyntactic biases through the modeling of linearized syntactic parse trees\\nalongside surface sentences. This paper focuses on compositional SLMs that are\\nbased on constituency parse trees and contain explicit bottom-up composition of\\nconstituent representations. We identify key aspects of design choices in\\nexisting compositional SLMs and propose a unified framework encompassing both\\nexisting models and novel variants. We conduct a comprehensive empirical\\nevaluation of all the variants in our framework across language modeling,\\nsyntactic generalization, summarization, dialogue, and inference efficiency.\\nBased on the experimental results, we make multiple recommendations on the\\ndesign of compositional SLMs. Our code is released at\\nhttps://github.com/zhaoyd1/compositional_SLMs.', 'authors': ['Yida Zhao', 'Hao Xve', 'Xiang Hu', 'Kewei Tu'], 'published': '2025-06-28T18:32:23+00:00', 'categories': ['cs.CL', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.22978v1'}"}, {'id': 144, 'arxiv_id': '2506.22968v1', 'base_arxiv_id': '2506.22968', 'version': '1', 'title': "Against 'softmaxing' culture", 'abstract': 'AI is flattening culture. Evaluations of "culture" are showing the myriad\nways in which large AI models are homogenizing language and culture, averaging\nout rich linguistic differences into generic expressions. I call this\nphenomenon "softmaxing culture," and it is one of the fundamental challenges\nfacing AI evaluations today. Efforts to improve and strengthen evaluations of\nculture are central to the project of cultural alignment in large AI systems.\nThis position paper argues that machine learning (ML) and human-computer\ninteraction (HCI) approaches to evaluation are limited. I propose two key\nshifts. First, instead of asking "what is culture?" at the start of system\nevaluations, I propose beginning with the question: "when is culture?" Second,\nwhile I acknowledge the philosophical claim that cultural universals exist, the\nchallenge is not simply to describe them, but to situate them in relation to\ntheir particulars. Taken together, these conceptual shifts invite evaluation\napproaches that move beyond technical requirements, toward perspectives more\nresponsive to the complexities of culture.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': '{\'id\': \'2506.22968v1\', \'title\': "Against \'softmaxing\' culture", \'abstract\': \'AI is flattening culture. Evaluations of "culture" are showing the myriad\\nways in which large AI models are homogenizing language and culture, averaging\\nout rich linguistic differences into generic expressions. I call this\\nphenomenon "softmaxing culture," and it is one of the fundamental challenges\\nfacing AI evaluations today. Efforts to improve and strengthen evaluations of\\nculture are central to the project of cultural alignment in large AI systems.\\nThis position paper argues that machine learning (ML) and human-computer\\ninteraction (HCI) approaches to evaluation are limited. I propose two key\\nshifts. First, instead of asking "what is culture?" at the start of system\\nevaluations, I propose beginning with the question: "when is culture?" Second,\\nwhile I acknowledge the philosophical claim that cultural universals exist, the\\nchallenge is not simply to describe them, but to situate them in relation to\\ntheir particulars. Taken together, these conceptual shifts invite evaluation\\napproaches that move beyond technical requirements, toward perspectives more\\nresponsive to the complexities of culture.\', \'authors\': [\'Daniel Mwesigwa\'], \'published\': \'2025-06-28T17:59:17+00:00\', \'categories\': [\'cs.HC\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.22968v1\'}'}, {'id': 145, 'arxiv_id': '2506.22957v1', 'base_arxiv_id': '2506.22957', 'version': '1', 'title': 'Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models', 'abstract': "As large language models (LLMs) are increasingly integrated into multi-agent\nand human-AI systems, understanding their awareness of both self-context and\nconversational partners is essential for ensuring reliable performance and\nrobust safety. While prior work has extensively studied situational awareness\nwhich refers to an LLM's ability to recognize its operating phase and\nconstraints, it has largely overlooked the complementary capacity to identify\nand adapt to the identity and characteristics of a dialogue partner. In this\npaper, we formalize this latter capability as interlocutor awareness and\npresent the first systematic evaluation of its emergence in contemporary LLMs.\nWe examine interlocutor inference across three dimensions-reasoning patterns,\nlinguistic style, and alignment preferences-and show that LLMs reliably\nidentify same-family peers and certain prominent model families, such as GPT\nand Claude. To demonstrate its practical significance, we develop three case\nstudies in which interlocutor awareness both enhances multi-LLM collaboration\nthrough prompt adaptation and introduces new alignment and safety\nvulnerabilities, including reward-hacking behaviors and increased jailbreak\nsusceptibility. Our findings highlight the dual promise and peril of\nidentity-sensitive behavior in LLMs, underscoring the need for further\nunderstanding of interlocutor awareness and new safeguards in multi-agent\ndeployments. Our code is open-sourced at\nhttps://github.com/younwoochoi/InterlocutorAwarenessLLM.", 'submit_date': datetime.date(2025, 6, 28), 'metadata': '{\'id\': \'2506.22957v1\', \'title\': \'Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models\', \'abstract\': "As large language models (LLMs) are increasingly integrated into multi-agent\\nand human-AI systems, understanding their awareness of both self-context and\\nconversational partners is essential for ensuring reliable performance and\\nrobust safety. While prior work has extensively studied situational awareness\\nwhich refers to an LLM\'s ability to recognize its operating phase and\\nconstraints, it has largely overlooked the complementary capacity to identify\\nand adapt to the identity and characteristics of a dialogue partner. In this\\npaper, we formalize this latter capability as interlocutor awareness and\\npresent the first systematic evaluation of its emergence in contemporary LLMs.\\nWe examine interlocutor inference across three dimensions-reasoning patterns,\\nlinguistic style, and alignment preferences-and show that LLMs reliably\\nidentify same-family peers and certain prominent model families, such as GPT\\nand Claude. To demonstrate its practical significance, we develop three case\\nstudies in which interlocutor awareness both enhances multi-LLM collaboration\\nthrough prompt adaptation and introduces new alignment and safety\\nvulnerabilities, including reward-hacking behaviors and increased jailbreak\\nsusceptibility. Our findings highlight the dual promise and peril of\\nidentity-sensitive behavior in LLMs, underscoring the need for further\\nunderstanding of interlocutor awareness and new safeguards in multi-agent\\ndeployments. Our code is open-sourced at\\nhttps://github.com/younwoochoi/InterlocutorAwarenessLLM.", \'authors\': [\'Younwoo Choi\', \'Changling Li\', \'Yongjin Yang\', \'Zhijing Jin\'], \'published\': \'2025-06-28T17:22:59+00:00\', \'categories\': [\'cs.CL\', \'cs.AI\', \'cs.CY\', \'cs.MA\'], \'url\': \'http://arxiv.org/abs/2506.22957v1\'}'}, {'id': 146, 'arxiv_id': '2506.22949v1', 'base_arxiv_id': '2506.22949', 'version': '1', 'title': 'A Study on Semi-Supervised Detection of DDoS Attacks under Class Imbalance', 'abstract': 'One of the most difficult challenges in cybersecurity is eliminating\nDistributed Denial of Service (DDoS) attacks. Automating this task using\nartificial intelligence is a complex process due to the inherent class\nimbalance and lack of sufficient labeled samples of real-world datasets. This\nresearch investigates the use of Semi-Supervised Learning (SSL) techniques to\nimprove DDoS attack detection when data is imbalanced and partially labeled. In\nthis process, 13 state-of-the-art SSL algorithms are evaluated for detecting\nDDoS attacks in several scenarios. We evaluate their practical efficacy and\nshortcomings, including the extent to which they work in extreme environments.\nThe results will offer insight into designing intelligent Intrusion Detection\nSystems (IDSs) that are robust against class imbalance and handle partially\nlabeled data.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22949v1', 'title': 'A Study on Semi-Supervised Detection of DDoS Attacks under Class Imbalance', 'abstract': 'One of the most difficult challenges in cybersecurity is eliminating\\nDistributed Denial of Service (DDoS) attacks. Automating this task using\\nartificial intelligence is a complex process due to the inherent class\\nimbalance and lack of sufficient labeled samples of real-world datasets. This\\nresearch investigates the use of Semi-Supervised Learning (SSL) techniques to\\nimprove DDoS attack detection when data is imbalanced and partially labeled. In\\nthis process, 13 state-of-the-art SSL algorithms are evaluated for detecting\\nDDoS attacks in several scenarios. We evaluate their practical efficacy and\\nshortcomings, including the extent to which they work in extreme environments.\\nThe results will offer insight into designing intelligent Intrusion Detection\\nSystems (IDSs) that are robust against class imbalance and handle partially\\nlabeled data.', 'authors': ['Ehsan Hallaji', 'Vaishnavi Shanmugam', 'Roozbeh Razavi-Far', 'Mehrdad Saif'], 'published': '2025-06-28T16:47:39+00:00', 'categories': ['cs.CR', 'cs.AI', 'cs.LG'], 'url': 'http://arxiv.org/abs/2506.22949v1'}"}, {'id': 147, 'arxiv_id': '2506.22941v1', 'base_arxiv_id': '2506.22941', 'version': '1', 'title': 'Positioning AI Tools to Support Online Harm Reduction Practice: Applications and Design Directions', 'abstract': 'Access to accurate and actionable harm reduction information can directly\nimpact the health outcomes of People Who Use Drugs (PWUD), yet existing online\nchannels often fail to meet their diverse and dynamic needs due to limitations\nin adaptability, accessibility, and the pervasive impact of stigma. Large\nLanguage Models (LLMs) present a novel opportunity to enhance information\nprovision, but their application in such a high-stakes domain is under-explored\nand presents socio-technical challenges. This paper investigates how LLMs can\nbe responsibly designed to support the information needs of PWUD. Through a\nqualitative workshop involving diverse stakeholder groups (academics, harm\nreduction practitioners, and an online community moderator), we explored LLM\ncapabilities, identified potential use cases, and delineated core design\nconsiderations. Our findings reveal that while LLMs can address some existing\ninformation barriers (e.g., by offering responsive, multilingual, and\npotentially less stigmatising interactions), their effectiveness is contingent\nupon overcoming challenges related to ethical alignment with harm reduction\nprinciples, nuanced contextual understanding, effective communication, and\nclearly defined operational boundaries. We articulate design pathways\nemphasising collaborative co-design with experts and PWUD to develop LLM\nsystems that are helpful, safe, and responsibly governed. This work contributes\nempirically grounded insights and actionable design considerations for the\nresponsible development of LLMs as supportive tools within the harm reduction\necosystem.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22941v1', 'title': 'Positioning AI Tools to Support Online Harm Reduction Practice: Applications and Design Directions', 'abstract': 'Access to accurate and actionable harm reduction information can directly\\nimpact the health outcomes of People Who Use Drugs (PWUD), yet existing online\\nchannels often fail to meet their diverse and dynamic needs due to limitations\\nin adaptability, accessibility, and the pervasive impact of stigma. Large\\nLanguage Models (LLMs) present a novel opportunity to enhance information\\nprovision, but their application in such a high-stakes domain is under-explored\\nand presents socio-technical challenges. This paper investigates how LLMs can\\nbe responsibly designed to support the information needs of PWUD. Through a\\nqualitative workshop involving diverse stakeholder groups (academics, harm\\nreduction practitioners, and an online community moderator), we explored LLM\\ncapabilities, identified potential use cases, and delineated core design\\nconsiderations. Our findings reveal that while LLMs can address some existing\\ninformation barriers (e.g., by offering responsive, multilingual, and\\npotentially less stigmatising interactions), their effectiveness is contingent\\nupon overcoming challenges related to ethical alignment with harm reduction\\nprinciples, nuanced contextual understanding, effective communication, and\\nclearly defined operational boundaries. We articulate design pathways\\nemphasising collaborative co-design with experts and PWUD to develop LLM\\nsystems that are helpful, safe, and responsibly governed. This work contributes\\nempirically grounded insights and actionable design considerations for the\\nresponsible development of LLMs as supportive tools within the harm reduction\\necosystem.', 'authors': ['Kaixuan Wang', 'Jason T. Jacques', 'Chenxin Diao'], 'published': '2025-06-28T16:15:47+00:00', 'categories': ['cs.HC', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.22941v1'}"}, {'id': 148, 'arxiv_id': '2506.22929v1', 'base_arxiv_id': '2506.22929', 'version': '1', 'title': 'Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration', 'abstract': 'While deep learning excels in natural image and language processing, its\napplication to high-dimensional data faces computational challenges due to the\ndimensionality curse. Current large-scale data tools focus on business-oriented\ndescriptive statistics, lacking mathematical statistics support for advanced\nanalysis. We propose a parallel computation architecture based on space\ncompleteness, decomposing high-dimensional data into dimension-independent\nstructures for distributed processing. This framework enables seamless\nintegration of data mining and parallel-optimized machine learning methods,\nsupporting scientific computations across diverse data types like medical and\nnatural images within a unified system.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22929v1', 'title': 'Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration', 'abstract': 'While deep learning excels in natural image and language processing, its\\napplication to high-dimensional data faces computational challenges due to the\\ndimensionality curse. Current large-scale data tools focus on business-oriented\\ndescriptive statistics, lacking mathematical statistics support for advanced\\nanalysis. We propose a parallel computation architecture based on space\\ncompleteness, decomposing high-dimensional data into dimension-independent\\nstructures for distributed processing. This framework enables seamless\\nintegration of data mining and parallel-optimized machine learning methods,\\nsupporting scientific computations across diverse data types like medical and\\nnatural images within a unified system.', 'authors': ['Chen Zhang'], 'published': '2025-06-28T15:42:23+00:00', 'categories': ['cs.LG', 'cs.AI', 'eess.IV', 'eess.SP'], 'url': 'http://arxiv.org/abs/2506.22929v1'}"}, {'id': 149, 'arxiv_id': '2506.22920v1', 'base_arxiv_id': '2506.22920', 'version': '1', 'title': 'Improving Rationality in the Reasoning Process of Language Models through Self-playing Game', 'abstract': 'Large language models (LLMs) have demonstrated considerable reasoning\nabilities in various tasks such as mathematics and coding. However, recent\nstudies indicate that even the best models lack true comprehension of their\nreasoning processes. In this paper, we explore how self-play can enhance the\nrationality of models in the reasoning process without supervision from humans\nor superior models. We design a Critic-Discernment Game(CDG) in which a prover\nfirst provides a solution to a given problem and is subsequently challenged by\ncritiques of its solution. These critiques either aim to assist or mislead the\nprover. The objective of the prover is to maintain the correct answer when\nfaced with misleading comments, while correcting errors in response to\nconstructive feedback. Our experiments on tasks involving mathematical\nreasoning, stepwise error detection, self-correction, and long-chain reasoning\ndemonstrate that CDG training can significantly improve the ability of\nwell-aligned LLMs to comprehend their reasoning process.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22920v1', 'title': 'Improving Rationality in the Reasoning Process of Language Models through Self-playing Game', 'abstract': 'Large language models (LLMs) have demonstrated considerable reasoning\\nabilities in various tasks such as mathematics and coding. However, recent\\nstudies indicate that even the best models lack true comprehension of their\\nreasoning processes. In this paper, we explore how self-play can enhance the\\nrationality of models in the reasoning process without supervision from humans\\nor superior models. We design a Critic-Discernment Game(CDG) in which a prover\\nfirst provides a solution to a given problem and is subsequently challenged by\\ncritiques of its solution. These critiques either aim to assist or mislead the\\nprover. The objective of the prover is to maintain the correct answer when\\nfaced with misleading comments, while correcting errors in response to\\nconstructive feedback. Our experiments on tasks involving mathematical\\nreasoning, stepwise error detection, self-correction, and long-chain reasoning\\ndemonstrate that CDG training can significantly improve the ability of\\nwell-aligned LLMs to comprehend their reasoning process.', 'authors': ['Pinzheng Wang', 'Juntao Li', 'Zecheng Tang', 'Haijia Gui', 'Min zhang'], 'published': '2025-06-28T15:11:23+00:00', 'categories': ['cs.AI'], 'url': 'http://arxiv.org/abs/2506.22920v1'}"}, {'id': 150, 'arxiv_id': '2506.22919v1', 'base_arxiv_id': '2506.22919', 'version': '1', 'title': 'Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning', 'abstract': "Mixture-of-Experts (MoE) models enable conditional computation by routing\ninputs to specialized experts, but these experts rely on identical inductive\nbiases, thus limiting representational diversity. This static computation\npathway is inefficient for inputs that require different types of reasoning and\nlimits specialization and interpretability. We propose Hecto, a lightweight MoE\narchitecture that leverages architectural heterogeneity by combining a GRU\nexpert for temporal reasoning and an FFNN expert for static abstraction under a\nsparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG\nNews, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely\ntrails homogeneous baselines in performance despite receiving isolated input\nrepresentations, while achieving clear expert specialization, with each expert\naligning to distinct reasoning types (temporal vs static). At larger batch\nsizes, Hecto exhibits improved performance, benefiting from relaxed\ncomputational constraints that allow its heterogeneous architecture to optimize\nmore effectively. Ablation results isolate architectural diversity as the\nsource of Hecto's stability and interpretability across diverse reasoning\ntasks. Overall, Hecto establishes itself as a new benchmark for conditional\ncomputation, offering a principled framework for specialized reasoning in\nlow-resource regimes with its model strength derived from principled\nspecialization.", 'submit_date': datetime.date(2025, 6, 28), 'metadata': '{\'id\': \'2506.22919v1\', \'title\': \'Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning\', \'abstract\': "Mixture-of-Experts (MoE) models enable conditional computation by routing\\ninputs to specialized experts, but these experts rely on identical inductive\\nbiases, thus limiting representational diversity. This static computation\\npathway is inefficient for inputs that require different types of reasoning and\\nlimits specialization and interpretability. We propose Hecto, a lightweight MoE\\narchitecture that leverages architectural heterogeneity by combining a GRU\\nexpert for temporal reasoning and an FFNN expert for static abstraction under a\\nsparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG\\nNews, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely\\ntrails homogeneous baselines in performance despite receiving isolated input\\nrepresentations, while achieving clear expert specialization, with each expert\\naligning to distinct reasoning types (temporal vs static). At larger batch\\nsizes, Hecto exhibits improved performance, benefiting from relaxed\\ncomputational constraints that allow its heterogeneous architecture to optimize\\nmore effectively. Ablation results isolate architectural diversity as the\\nsource of Hecto\'s stability and interpretability across diverse reasoning\\ntasks. Overall, Hecto establishes itself as a new benchmark for conditional\\ncomputation, offering a principled framework for specialized reasoning in\\nlow-resource regimes with its model strength derived from principled\\nspecialization.", \'authors\': [\'Sanskar Pandey\', \'Ruhaan Chopra\', \'Saad Murtaza Bhat\', \'Ark Abhyudaya\'], \'published\': \'2025-06-28T15:03:43+00:00\', \'categories\': [\'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.22919v1\'}'}, {'id': 151, 'arxiv_id': '2506.22911v1', 'base_arxiv_id': '2506.22911', 'version': '1', 'title': 'Learning Truthful Mechanisms without Discretization', 'abstract': 'This paper introduces TEDI (Truthful, Expressive, and Dimension-Insensitive\napproach), a discretization-free algorithm to learn truthful and\nutility-maximizing mechanisms. Existing learning-based approaches often rely on\ndiscretization of outcome spaces to ensure truthfulness, which leads to\ninefficiency with increasing problem size. To address this limitation, we\nformalize the concept of pricing rules, defined as functions that map outcomes\nto prices. Based on this concept, we propose a novel menu mechanism, which can\nbe equivalent to a truthful direct mechanism under specific conditions. The\ncore idea of TEDI lies in its parameterization of pricing rules using Partial\nGroupMax Network, a new network architecture designed to universally\napproximate partial convex functions. To learn optimal pricing rules, we\ndevelop novel training techniques, including covariance trick and continuous\nsampling, to derive unbiased gradient estimators compatible with first-order\noptimization. Theoretical analysis establishes that TEDI guarantees\ntruthfulness, full expressiveness, and dimension-insensitivity. Experimental\nevaluation in the studied auction setting demonstrates that TEDI achieves\nstrong performance, competitive with or exceeding state-of-the-art methods.\n  This work presents the first approaches to learn truthful mechanisms without\noutcome discretization, thereby enhancing algorithmic efficiency. The proposed\nconcepts, network architecture, and learning techniques might offer potential\nvalue and provide new insights for automated mechanism design and\ndifferentiable economics.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22911v1', 'title': 'Learning Truthful Mechanisms without Discretization', 'abstract': 'This paper introduces TEDI (Truthful, Expressive, and Dimension-Insensitive\\napproach), a discretization-free algorithm to learn truthful and\\nutility-maximizing mechanisms. Existing learning-based approaches often rely on\\ndiscretization of outcome spaces to ensure truthfulness, which leads to\\ninefficiency with increasing problem size. To address this limitation, we\\nformalize the concept of pricing rules, defined as functions that map outcomes\\nto prices. Based on this concept, we propose a novel menu mechanism, which can\\nbe equivalent to a truthful direct mechanism under specific conditions. The\\ncore idea of TEDI lies in its parameterization of pricing rules using Partial\\nGroupMax Network, a new network architecture designed to universally\\napproximate partial convex functions. To learn optimal pricing rules, we\\ndevelop novel training techniques, including covariance trick and continuous\\nsampling, to derive unbiased gradient estimators compatible with first-order\\noptimization. Theoretical analysis establishes that TEDI guarantees\\ntruthfulness, full expressiveness, and dimension-insensitivity. Experimental\\nevaluation in the studied auction setting demonstrates that TEDI achieves\\nstrong performance, competitive with or exceeding state-of-the-art methods.\\n  This work presents the first approaches to learn truthful mechanisms without\\noutcome discretization, thereby enhancing algorithmic efficiency. The proposed\\nconcepts, network architecture, and learning techniques might offer potential\\nvalue and provide new insights for automated mechanism design and\\ndifferentiable economics.', 'authors': ['Yunxuan Ma', 'Siqiang Wang', 'Zhijian Duan', 'Yukun Cheng', 'Xiaotie Deng'], 'published': '2025-06-28T14:50:29+00:00', 'categories': ['cs.GT', 'cs.AI', 'cs.LG'], 'url': 'http://arxiv.org/abs/2506.22911v1'}"}, {'id': 152, 'arxiv_id': '2506.22901v1', 'base_arxiv_id': '2506.22901', 'version': '1', 'title': 'Missing-Modality-Aware Graph Neural Network for Cancer Classification', 'abstract': "A key challenge in learning from multimodal biological data is missing\nmodalities, where all data from some modalities are missing for some patients.\nCurrent fusion methods address this by excluding patients with missing\nmodalities, imputing missing modalities, or making predictions directly with\npartial modalities. However, they often struggle with diverse missing-modality\npatterns and the exponential growth of the number of such patterns as the\nnumber of modalities increases. To address these limitations, we propose MAGNET\n(Missing-modality-Aware Graph neural NETwork) for direct prediction with\npartial modalities, which introduces a patient-modality multi-head attention\nmechanism to fuse lower-dimensional modality embeddings based on their\nimportance and missingness. MAGNET's complexity increases linearly with the\nnumber of modalities while adapting to missing-pattern variability. To generate\npredictions, MAGNET further constructs a patient graph with fused multimodal\nembeddings as node features and the connectivity determined by the modality\nmissingness, followed by a conventional graph neural network. Experiments on\nthree public multiomics datasets for cancer classification, with real-world\ninstead of artificial missingness, show that MAGNET outperforms the\nstate-of-the-art fusion methods. The data and code are available at\nhttps://github.com/SinaTabakhi/MAGNET.", 'submit_date': datetime.date(2025, 6, 28), 'metadata': '{\'id\': \'2506.22901v1\', \'title\': \'Missing-Modality-Aware Graph Neural Network for Cancer Classification\', \'abstract\': "A key challenge in learning from multimodal biological data is missing\\nmodalities, where all data from some modalities are missing for some patients.\\nCurrent fusion methods address this by excluding patients with missing\\nmodalities, imputing missing modalities, or making predictions directly with\\npartial modalities. However, they often struggle with diverse missing-modality\\npatterns and the exponential growth of the number of such patterns as the\\nnumber of modalities increases. To address these limitations, we propose MAGNET\\n(Missing-modality-Aware Graph neural NETwork) for direct prediction with\\npartial modalities, which introduces a patient-modality multi-head attention\\nmechanism to fuse lower-dimensional modality embeddings based on their\\nimportance and missingness. MAGNET\'s complexity increases linearly with the\\nnumber of modalities while adapting to missing-pattern variability. To generate\\npredictions, MAGNET further constructs a patient graph with fused multimodal\\nembeddings as node features and the connectivity determined by the modality\\nmissingness, followed by a conventional graph neural network. Experiments on\\nthree public multiomics datasets for cancer classification, with real-world\\ninstead of artificial missingness, show that MAGNET outperforms the\\nstate-of-the-art fusion methods. The data and code are available at\\nhttps://github.com/SinaTabakhi/MAGNET.", \'authors\': [\'Sina Tabakhi\', \'Haiping Lu\'], \'published\': \'2025-06-28T14:31:00+00:00\', \'categories\': [\'cs.LG\', \'cs.AI\', \'q-bio.BM\', \'q-bio.GN\'], \'url\': \'http://arxiv.org/abs/2506.22901v1\'}'}, {'id': 153, 'arxiv_id': '2506.22895v1', 'base_arxiv_id': '2506.22895', 'version': '1', 'title': 'Interpretable Time Series Autoregression for Periodicity Quantification', 'abstract': 'Time series autoregression is a classical statistical model for capturing\nauto-correlations and identifying temporal patterns such as periodicity and\nseasonality. In this work, we propose a novel sparse autoregression framework\nfrom an interpretable machine learning perspective and the model\ninterpretability for periodicity quantification is reinforced by $_0$-norm\ninduced sparsity constraints. On the time-varying time series data, we\nreformulate the sparse autoregression and convert the involved optimization\nproblem into a mixed-integer optimization (MIO). To accelerate it, we develop a\nsubspace pursuit based decision variable pruning (DVP) strategy to reduce the\nsearch space. On the multidimensional time series that involves complicated\nspatial and temporal dimensions, we propose a spatially- and time-varying\nsparse autoregression model and resolve the corresponding MIO problem by\ndeveloping a two-stage optimization scheme. In particular, the proposed scheme\nmakes the model scalable to large problems even with millions of decision\nvariables. Empirically, we conduct extensive experiments to evaluate the\nproposed models on real-world time series data. First, we demonstrate that the\nMIO solver can be drastically accelerated through the DVP strategy, while\nmaintaining the same solution quality as a full MIO solver. Applying the\ntime-varying sparse autoregression model to ridesharing trip data, we uncover\nboth daily and weekly periodicities and reveal long-term changes in regularity\nof human mobility. Second, we demonstrate the spatial patterns of yearly\nseasonality in climate variable time series such as temperature and\nprecipitation across the past four decades, and our model allows to discover\ndynamic climate patterns and identify climate phenomena such as El Nino in sea\nsurface temperature.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22895v1', 'title': 'Interpretable Time Series Autoregression for Periodicity Quantification', 'abstract': 'Time series autoregression is a classical statistical model for capturing\\nauto-correlations and identifying temporal patterns such as periodicity and\\nseasonality. In this work, we propose a novel sparse autoregression framework\\nfrom an interpretable machine learning perspective and the model\\ninterpretability for periodicity quantification is reinforced by $\\\\ell_0$-norm\\ninduced sparsity constraints. On the time-varying time series data, we\\nreformulate the sparse autoregression and convert the involved optimization\\nproblem into a mixed-integer optimization (MIO). To accelerate it, we develop a\\nsubspace pursuit based decision variable pruning (DVP) strategy to reduce the\\nsearch space. On the multidimensional time series that involves complicated\\nspatial and temporal dimensions, we propose a spatially- and time-varying\\nsparse autoregression model and resolve the corresponding MIO problem by\\ndeveloping a two-stage optimization scheme. In particular, the proposed scheme\\nmakes the model scalable to large problems even with millions of decision\\nvariables. Empirically, we conduct extensive experiments to evaluate the\\nproposed models on real-world time series data. First, we demonstrate that the\\nMIO solver can be drastically accelerated through the DVP strategy, while\\nmaintaining the same solution quality as a full MIO solver. Applying the\\ntime-varying sparse autoregression model to ridesharing trip data, we uncover\\nboth daily and weekly periodicities and reveal long-term changes in regularity\\nof human mobility. Second, we demonstrate the spatial patterns of yearly\\nseasonality in climate variable time series such as temperature and\\nprecipitation across the past four decades, and our model allows to discover\\ndynamic climate patterns and identify climate phenomena such as El Nino in sea\\nsurface temperature.', 'authors': ['Xinyu Chen', 'Vassilis Digalakis Jr', 'Lijun Ding', 'Dingyi Zhuang', 'Jinhua Zhao'], 'published': '2025-06-28T14:17:11+00:00', 'categories': ['cs.LG', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.22895v1'}"}, {'id': 154, 'arxiv_id': '2506.22893v1', 'base_arxiv_id': '2506.22893', 'version': '1', 'title': 'Agentic Enterprise: AI-Centric User to User-Centric AI', 'abstract': 'After a very long winter, the Artificial Intelligence (AI) spring is here.\nOr, so it seems over the last three years. AI has the potential to impact many\nareas of human life - personal, social, health, education, professional. In\nthis paper, we take a closer look at the potential of AI for Enterprises, where\ndecision-making plays a crucial and repeated role across functions, tasks, and\noperations. We consider Agents imbued with AI as means to increase\ndecision-productivity of enterprises. We highlight six tenets for Agentic\nsuccess in enterprises, by drawing attention to what the current, AI-Centric\nUser paradigm misses, in the face of persistent needs of and usefulness for\nEnterprise Decision-Making. In underscoring a shift to User-Centric AI, we\noffer six tenets and promote market mechanisms for platforms, aligning the\ndesign of AI and its delivery by Agents to the cause of enterprise users.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22893v1', 'title': 'Agentic Enterprise: AI-Centric User to User-Centric AI', 'abstract': 'After a very long winter, the Artificial Intelligence (AI) spring is here.\\nOr, so it seems over the last three years. AI has the potential to impact many\\nareas of human life - personal, social, health, education, professional. In\\nthis paper, we take a closer look at the potential of AI for Enterprises, where\\ndecision-making plays a crucial and repeated role across functions, tasks, and\\noperations. We consider Agents imbued with AI as means to increase\\ndecision-productivity of enterprises. We highlight six tenets for Agentic\\nsuccess in enterprises, by drawing attention to what the current, AI-Centric\\nUser paradigm misses, in the face of persistent needs of and usefulness for\\nEnterprise Decision-Making. In underscoring a shift to User-Centric AI, we\\noffer six tenets and promote market mechanisms for platforms, aligning the\\ndesign of AI and its delivery by Agents to the cause of enterprise users.', 'authors': ['Arpit Narechania', 'Alex Endert', 'Atanu R Sinha'], 'published': '2025-06-28T14:05:59+00:00', 'categories': ['cs.AI', 'cs.HC'], 'url': 'http://arxiv.org/abs/2506.22893v1'}"}, {'id': 155, 'arxiv_id': '2506.22884v1', 'base_arxiv_id': '2506.22884', 'version': '1', 'title': 'Performance Measurements in the AI-Centric Computing Continuum Systems', 'abstract': 'Over the Eight decades, computing paradigms have shifted from large,\ncentralized systems to compact, distributed architectures, leading to the rise\nof the Distributed Computing Continuum (DCC). In this model, multiple layers\nsuch as cloud, edge, Internet of Things (IoT), and mobile platforms work\ntogether to support a wide range of applications. Recently, the emergence of\nGenerative AI and large language models has further intensified the demand for\ncomputational resources across this continuum. Although traditional performance\nmetrics have provided a solid foundation, they need to be revisited and\nexpanded to keep pace with changing computational demands and application\nrequirements. Accurate performance measurements benefit both system designers\nand users by supporting improvements in efficiency and promoting alignment with\nsystem goals. In this context, we review commonly used metrics in DCC and IoT\nenvironments. We also discuss emerging performance dimensions that address\nevolving computing needs, such as sustainability, energy efficiency, and system\nobservability. We also outline criteria and considerations for selecting\nappropriate metrics, aiming to inspire future research and development in this\ncritical area.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22884v1', 'title': 'Performance Measurements in the AI-Centric Computing Continuum Systems', 'abstract': 'Over the Eight decades, computing paradigms have shifted from large,\\ncentralized systems to compact, distributed architectures, leading to the rise\\nof the Distributed Computing Continuum (DCC). In this model, multiple layers\\nsuch as cloud, edge, Internet of Things (IoT), and mobile platforms work\\ntogether to support a wide range of applications. Recently, the emergence of\\nGenerative AI and large language models has further intensified the demand for\\ncomputational resources across this continuum. Although traditional performance\\nmetrics have provided a solid foundation, they need to be revisited and\\nexpanded to keep pace with changing computational demands and application\\nrequirements. Accurate performance measurements benefit both system designers\\nand users by supporting improvements in efficiency and promoting alignment with\\nsystem goals. In this context, we review commonly used metrics in DCC and IoT\\nenvironments. We also discuss emerging performance dimensions that address\\nevolving computing needs, such as sustainability, energy efficiency, and system\\nobservability. We also outline criteria and considerations for selecting\\nappropriate metrics, aiming to inspire future research and development in this\\ncritical area.', 'authors': ['Praveen Kumar Donta', 'Qiyang Zhang', 'Schahram Dustdar'], 'published': '2025-06-28T13:46:07+00:00', 'categories': ['cs.DC', 'cs.AI', 'cs.ET', 'cs.NI', 'cs.SY', 'eess.SY'], 'url': 'http://arxiv.org/abs/2506.22884v1'}"}, {'id': 156, 'arxiv_id': '2506.22880v1', 'base_arxiv_id': '2506.22880', 'version': '1', 'title': 'Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and Grounder', 'abstract': "Existing video segmenter and grounder approaches, exemplified by Sa2VA,\ndirectly fuse features within segmentation models. This often results in an\nundesirable entanglement of dynamic visual information and static semantics,\nthereby degrading segmentation accuracy. To systematically mitigate this issue,\nwe propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text\npre-training and a linear decoupling module to address the information\nprocessing limitations inherent in SAM-2. Specifically, first, we devise a\npre-training paradigm that converts textual ground-truth labels into\npoint-level prompts while generating corresponding text masks. These masks are\nrefined through a hybrid loss function to strengthen the model's semantic\ngrounding capabilities. Next, we employ linear projection to disentangle hidden\nstates that generated by a large language model into distinct textual and\nvisual feature subspaces. Finally, a dynamic mask fusion strategy\nsynergistically combines these decoupled features through triple supervision\nfrom predicted text/visual masks and ground-truth annotations. Extensive\nexperiments demonstrate state-of-the-art performance across diverse tasks,\nincluding image segmentation, image question answering, video segmentation, and\nvideo question answering. Our codes are available at\nhttps://github.com/longmalongma/DeSa2VA.", 'submit_date': datetime.date(2025, 6, 28), 'metadata': '{\'id\': \'2506.22880v1\', \'title\': \'Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and Grounder\', \'abstract\': "Existing video segmenter and grounder approaches, exemplified by Sa2VA,\\ndirectly fuse features within segmentation models. This often results in an\\nundesirable entanglement of dynamic visual information and static semantics,\\nthereby degrading segmentation accuracy. To systematically mitigate this issue,\\nwe propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text\\npre-training and a linear decoupling module to address the information\\nprocessing limitations inherent in SAM-2. Specifically, first, we devise a\\npre-training paradigm that converts textual ground-truth labels into\\npoint-level prompts while generating corresponding text masks. These masks are\\nrefined through a hybrid loss function to strengthen the model\'s semantic\\ngrounding capabilities. Next, we employ linear projection to disentangle hidden\\nstates that generated by a large language model into distinct textual and\\nvisual feature subspaces. Finally, a dynamic mask fusion strategy\\nsynergistically combines these decoupled features through triple supervision\\nfrom predicted text/visual masks and ground-truth annotations. Extensive\\nexperiments demonstrate state-of-the-art performance across diverse tasks,\\nincluding image segmentation, image question answering, video segmentation, and\\nvideo question answering. Our codes are available at\\nhttps://github.com/longmalongma/DeSa2VA.", \'authors\': [\'Dang Jisheng\', \'Wu Xudong\', \'Wang Bimei\', \'Lv Ning\', \'Chen Jiayu\', \'Jingwen Zhao\', \'Yichu liu\', \'Jizhao Liu\', \'Juncheng Li\', \'Teng Wang\'], \'published\': \'2025-06-28T13:30:36+00:00\', \'categories\': [\'cs.CV\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.22880v1\'}'}, {'id': 157, 'arxiv_id': '2506.22868v1', 'base_arxiv_id': '2506.22868', 'version': '1', 'title': 'STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing', 'abstract': 'Previous text-guided video editing methods often suffer from temporal\ninconsistency, motion distortion, and-most notably-limited domain\ntransformation. We attribute these limitations to insufficient modeling of\nspatiotemporal pixel relevance during the editing process. To address this, we\npropose STR-Match, a training-free video editing algorithm that produces\nvisually appealing and spatiotemporally coherent videos through latent\noptimization guided by our novel STR score. The score captures spatiotemporal\npixel relevance across adjacent frames by leveraging 2D spatial attention and\n1D temporal modules in text-to-video (T2V) diffusion models, without the\noverhead of computationally expensive 3D attention mechanisms. Integrated into\na latent optimization framework with a latent mask, STR-Match generates\ntemporally consistent and visually faithful videos, maintaining strong\nperformance even under significant domain transformations while preserving key\nvisual attributes of the source. Extensive experiments demonstrate that\nSTR-Match consistently outperforms existing methods in both visual quality and\nspatiotemporal consistency.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22868v1', 'title': 'STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing', 'abstract': 'Previous text-guided video editing methods often suffer from temporal\\ninconsistency, motion distortion, and-most notably-limited domain\\ntransformation. We attribute these limitations to insufficient modeling of\\nspatiotemporal pixel relevance during the editing process. To address this, we\\npropose STR-Match, a training-free video editing algorithm that produces\\nvisually appealing and spatiotemporally coherent videos through latent\\noptimization guided by our novel STR score. The score captures spatiotemporal\\npixel relevance across adjacent frames by leveraging 2D spatial attention and\\n1D temporal modules in text-to-video (T2V) diffusion models, without the\\noverhead of computationally expensive 3D attention mechanisms. Integrated into\\na latent optimization framework with a latent mask, STR-Match generates\\ntemporally consistent and visually faithful videos, maintaining strong\\nperformance even under significant domain transformations while preserving key\\nvisual attributes of the source. Extensive experiments demonstrate that\\nSTR-Match consistently outperforms existing methods in both visual quality and\\nspatiotemporal consistency.', 'authors': ['Junsung Lee', 'Junoh Kang', 'Bohyung Han'], 'published': '2025-06-28T12:36:19+00:00', 'categories': ['cs.CV', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.22868v1'}"}, {'id': 158, 'arxiv_id': '2506.22866v1', 'base_arxiv_id': '2506.22866', 'version': '1', 'title': 'Region-Aware CAM: High-Resolution Weakly-Supervised Defect Segmentation via Salient Region Perception', 'abstract': "Surface defect detection plays a critical role in industrial quality\ninspection. Recent advances in artificial intelligence have significantly\nenhanced the automation level of detection processes. However, conventional\nsemantic segmentation and object detection models heavily rely on large-scale\nannotated datasets, which conflicts with the practical requirements of defect\ndetection tasks. This paper proposes a novel weakly supervised semantic\nsegmentation framework comprising two key components: a region-aware class\nactivation map (CAM) and pseudo-label training. To address the limitations of\nexisting CAM methods, especially low-resolution thermal maps, and insufficient\ndetail preservation, we introduce filtering-guided backpropagation (FGBP),\nwhich refines target regions by filtering gradient magnitudes to identify areas\nwith higher relevance to defects. Building upon this, we further develop a\nregion-aware weighted module to enhance spatial precision. Finally,\npseudo-label segmentation is implemented to refine the model's performance\niteratively. Comprehensive experiments on industrial defect datasets\ndemonstrate the superiority of our method. The proposed framework effectively\nbridges the gap between weakly supervised learning and high-precision defect\nsegmentation, offering a practical solution for resource-constrained industrial\nscenarios.", 'submit_date': datetime.date(2025, 6, 28), 'metadata': '{\'id\': \'2506.22866v1\', \'title\': \'Region-Aware CAM: High-Resolution Weakly-Supervised Defect Segmentation via Salient Region Perception\', \'abstract\': "Surface defect detection plays a critical role in industrial quality\\ninspection. Recent advances in artificial intelligence have significantly\\nenhanced the automation level of detection processes. However, conventional\\nsemantic segmentation and object detection models heavily rely on large-scale\\nannotated datasets, which conflicts with the practical requirements of defect\\ndetection tasks. This paper proposes a novel weakly supervised semantic\\nsegmentation framework comprising two key components: a region-aware class\\nactivation map (CAM) and pseudo-label training. To address the limitations of\\nexisting CAM methods, especially low-resolution thermal maps, and insufficient\\ndetail preservation, we introduce filtering-guided backpropagation (FGBP),\\nwhich refines target regions by filtering gradient magnitudes to identify areas\\nwith higher relevance to defects. Building upon this, we further develop a\\nregion-aware weighted module to enhance spatial precision. Finally,\\npseudo-label segmentation is implemented to refine the model\'s performance\\niteratively. Comprehensive experiments on industrial defect datasets\\ndemonstrate the superiority of our method. The proposed framework effectively\\nbridges the gap between weakly supervised learning and high-precision defect\\nsegmentation, offering a practical solution for resource-constrained industrial\\nscenarios.", \'authors\': [\'Hang-Cheng Dong\', \'Lu Zou\', \'Bingguo Liu\', \'Dong Ye\', \'Guodong Liu\'], \'published\': \'2025-06-28T12:24:45+00:00\', \'categories\': [\'cs.CV\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.22866v1\'}'}, {'id': 159, 'arxiv_id': '2506.22865v1', 'base_arxiv_id': '2506.22865', 'version': '1', 'title': 'ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language Models', 'abstract': 'Recent advancements in Large Language Models (LLMs) have revealed a\nsignificant performance gap between closed-source and open-source models,\nparticularly in tasks requiring complex reasoning and precise instruction\nfollowing. This paper introduces ReasonBridge, a methodology that efficiently\ntransfers reasoning capabilities from powerful closed-source to open-source\nmodels through a novel hierarchical knowledge distillation framework. We\ndevelop a tailored dataset Reason1K with only 1,000 carefully curated reasoning\ntraces emphasizing difficulty, diversity, and quality. These traces are\nfiltered from across multiple domains using a structured multi-criteria\nselection algorithm. Our transfer learning approach incorporates: (1) a\nhierarchical distillation process capturing both strategic abstraction and\ntactical implementation patterns, (2) a sparse reasoning-focused adapter\narchitecture requiring only 0.3% additional trainable parameters, and (3) a\ntest-time compute scaling mechanism using guided inference interventions.\nComprehensive evaluations demonstrate that ReasonBridge improves reasoning\ncapabilities in open-source models by up to 23% on benchmark tasks,\nsignificantly narrowing the gap with closed-source models. Notably, the\nenhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its\nperformance on competition-level AIME problems. Our methodology generalizes\neffectively across diverse reasoning domains and model architectures,\nestablishing a sample-efficient approach to reasoning enhancement for\ninstruction following.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22865v1', 'title': 'ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language Models', 'abstract': 'Recent advancements in Large Language Models (LLMs) have revealed a\\nsignificant performance gap between closed-source and open-source models,\\nparticularly in tasks requiring complex reasoning and precise instruction\\nfollowing. This paper introduces ReasonBridge, a methodology that efficiently\\ntransfers reasoning capabilities from powerful closed-source to open-source\\nmodels through a novel hierarchical knowledge distillation framework. We\\ndevelop a tailored dataset Reason1K with only 1,000 carefully curated reasoning\\ntraces emphasizing difficulty, diversity, and quality. These traces are\\nfiltered from across multiple domains using a structured multi-criteria\\nselection algorithm. Our transfer learning approach incorporates: (1) a\\nhierarchical distillation process capturing both strategic abstraction and\\ntactical implementation patterns, (2) a sparse reasoning-focused adapter\\narchitecture requiring only 0.3% additional trainable parameters, and (3) a\\ntest-time compute scaling mechanism using guided inference interventions.\\nComprehensive evaluations demonstrate that ReasonBridge improves reasoning\\ncapabilities in open-source models by up to 23% on benchmark tasks,\\nsignificantly narrowing the gap with closed-source models. Notably, the\\nenhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its\\nperformance on competition-level AIME problems. Our methodology generalizes\\neffectively across diverse reasoning domains and model architectures,\\nestablishing a sample-efficient approach to reasoning enhancement for\\ninstruction following.', 'authors': ['Ziqi Zhong', 'Xunzhu Tang'], 'published': '2025-06-28T12:22:55+00:00', 'categories': ['cs.AI'], 'url': 'http://arxiv.org/abs/2506.22865v1'}"}, {'id': 160, 'arxiv_id': '2506.22864v1', 'base_arxiv_id': '2506.22864', 'version': '1', 'title': 'Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval', 'abstract': 'Text-to-image retrieval (TIR) aims to find relevant images based on a textual\nquery, but existing approaches are primarily based on whole-image captions and\nlack interpretability. Meanwhile, referring expression segmentation (RES)\nenables precise object localization based on natural language descriptions but\nis computationally expensive when applied across large image collections. To\nbridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies\nTIR and RES, requiring both efficient image search and accurate object\nsegmentation. To address this task, we propose a two-stage framework,\ncomprising a first stage for segmentation-aware image retrieval and a second\nstage for reranking and object grounding with a multimodal large language model\n(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract\nregion-level embeddings offline at first, enabling effective and scalable\nonline retrieval. Secondly, MLLM is used to refine retrieval rankings and\ngenerate bounding boxes, which are matched to segmentation masks. We evaluate\nour approach on COCO and D$^3$ datasets, demonstrating significant improvements\nin both retrieval accuracy and segmentation quality over previous methods.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22864v1', 'title': 'Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval', 'abstract': 'Text-to-image retrieval (TIR) aims to find relevant images based on a textual\\nquery, but existing approaches are primarily based on whole-image captions and\\nlack interpretability. Meanwhile, referring expression segmentation (RES)\\nenables precise object localization based on natural language descriptions but\\nis computationally expensive when applied across large image collections. To\\nbridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies\\nTIR and RES, requiring both efficient image search and accurate object\\nsegmentation. To address this task, we propose a two-stage framework,\\ncomprising a first stage for segmentation-aware image retrieval and a second\\nstage for reranking and object grounding with a multimodal large language model\\n(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract\\nregion-level embeddings offline at first, enabling effective and scalable\\nonline retrieval. Secondly, MLLM is used to refine retrieval rankings and\\ngenerate bounding boxes, which are matched to segmentation masks. We evaluate\\nour approach on COCO and D$^3$ datasets, demonstrating significant improvements\\nin both retrieval accuracy and segmentation quality over previous methods.', 'authors': ['Li-Cheng Shen', 'Jih-Kang Hsieh', 'Wei-Hua Li', 'Chu-Song Chen'], 'published': '2025-06-28T12:19:49+00:00', 'categories': ['cs.CV', 'cs.AI', 'cs.CL'], 'url': 'http://arxiv.org/abs/2506.22864v1'}"}, {'id': 161, 'arxiv_id': '2506.22853v1', 'base_arxiv_id': '2506.22853', 'version': '1', 'title': 'DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues', 'abstract': 'Existing function-calling benchmarks focus on single-turn interactions.\nHowever, they overlook the complexity of real-world scenarios. To quantify how\nexisting benchmarks address practical applications, we introduce DICE-SCORE, a\nmetric that evaluates the dispersion of tool-related information such as\nfunction name and parameter values throughout the dialogue. Analyzing existing\nbenchmarks through DICE-SCORE reveals notably low scores, highlighting the need\nfor more realistic scenarios. To address this gap, we present DICE-BENCH, a\nframework that constructs practical function-calling datasets by synthesizing\nconversations through a tool graph that maintains dependencies across rounds\nand a multi-agent system with distinct personas to enhance dialogue\nnaturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our\nexperiments on 19 LLMs with DICE-BENCH show that significant advances are still\nrequired before such models can be deployed effectively in real-world settings.\nOur code and data are all publicly available:\nhttps://snuhcc.github.io/DICE-Bench/.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22853v1', 'title': 'DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues', 'abstract': 'Existing function-calling benchmarks focus on single-turn interactions.\\nHowever, they overlook the complexity of real-world scenarios. To quantify how\\nexisting benchmarks address practical applications, we introduce DICE-SCORE, a\\nmetric that evaluates the dispersion of tool-related information such as\\nfunction name and parameter values throughout the dialogue. Analyzing existing\\nbenchmarks through DICE-SCORE reveals notably low scores, highlighting the need\\nfor more realistic scenarios. To address this gap, we present DICE-BENCH, a\\nframework that constructs practical function-calling datasets by synthesizing\\nconversations through a tool graph that maintains dependencies across rounds\\nand a multi-agent system with distinct personas to enhance dialogue\\nnaturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our\\nexperiments on 19 LLMs with DICE-BENCH show that significant advances are still\\nrequired before such models can be deployed effectively in real-world settings.\\nOur code and data are all publicly available:\\nhttps://snuhcc.github.io/DICE-Bench/.', 'authors': ['Kyochul Jang', 'Donghyeon Lee', 'Kyusik Kim', 'Dongseok Heo', 'Taewhoo Lee', 'Woojeong Kim', 'Bongwon Suh'], 'published': '2025-06-28T11:28:04+00:00', 'categories': ['cs.CL', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.22853v1'}"}, {'id': 162, 'arxiv_id': '2506.22848v1', 'base_arxiv_id': '2506.22848', 'version': '1', 'title': 'Scalable Structure Learning of Bayesian Networks by Learning Algorithm Ensembles', 'abstract': 'Learning the structure of Bayesian networks (BNs) from data is challenging,\nespecially for datasets involving a large number of variables. The recently\nproposed divide-and-conquer (D\\&D) strategies present a promising approach for\nlearning large BNs. However, they still face a main issue of unstable learning\naccuracy across subproblems. In this work, we introduce the idea of employing\nstructure learning ensemble (SLE), which combines multiple BN structure\nlearning algorithms, to consistently achieve high learning accuracy. We further\npropose an automatic approach called Auto-SLE for learning near-optimal SLEs,\naddressing the challenge of manually designing high-quality SLEs. The learned\nSLE is then integrated into a D\\&D method. Extensive experiments firmly show\nthe superiority of our method over D\\&D methods with single BN structure\nlearning algorithm in learning large BNs, achieving accuracy improvement\nusually by 30\\%$$225\\% on datasets involving 10,000 variables. Furthermore,\nour method generalizes well to datasets with many more (e.g., 30000) variables\nand different network characteristics than those present in the training data\nfor learning the SLE. These results indicate the significant potential of\nemploying (automatic learning of) SLEs for scalable BN structure learning.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22848v1', 'title': 'Scalable Structure Learning of Bayesian Networks by Learning Algorithm Ensembles', 'abstract': 'Learning the structure of Bayesian networks (BNs) from data is challenging,\\nespecially for datasets involving a large number of variables. The recently\\nproposed divide-and-conquer (D\\\\&D) strategies present a promising approach for\\nlearning large BNs. However, they still face a main issue of unstable learning\\naccuracy across subproblems. In this work, we introduce the idea of employing\\nstructure learning ensemble (SLE), which combines multiple BN structure\\nlearning algorithms, to consistently achieve high learning accuracy. We further\\npropose an automatic approach called Auto-SLE for learning near-optimal SLEs,\\naddressing the challenge of manually designing high-quality SLEs. The learned\\nSLE is then integrated into a D\\\\&D method. Extensive experiments firmly show\\nthe superiority of our method over D\\\\&D methods with single BN structure\\nlearning algorithm in learning large BNs, achieving accuracy improvement\\nusually by 30\\\\%$\\\\sim$225\\\\% on datasets involving 10,000 variables. Furthermore,\\nour method generalizes well to datasets with many more (e.g., 30000) variables\\nand different network characteristics than those present in the training data\\nfor learning the SLE. These results indicate the significant potential of\\nemploying (automatic learning of) SLEs for scalable BN structure learning.', 'authors': ['Shengcai Liu', 'Hui Ou-yang', 'Zhiyuan Wang', 'Cheng Chen', 'Qijun Cai', 'Yew-Soon Ong', 'Ke Tang'], 'published': '2025-06-28T11:05:08+00:00', 'categories': ['cs.LG', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.22848v1'}"}, {'id': 163, 'arxiv_id': '2506.22845v1', 'base_arxiv_id': '2506.22845', 'version': '1', 'title': 'Quantum Neural Networks for Wind Energy Forecasting: A Comparative Study of Performance and Scalability with Classical Models', 'abstract': 'Quantum Neural Networks (QNNs), a prominent approach in Quantum Machine\nLearning (QML), are emerging as a powerful alternative to classical machine\nlearning methods. Recent studies have focused on the applicability of QNNs to\nvarious tasks, such as time-series forecasting, prediction, and classification,\nacross a wide range of applications, including cybersecurity and medical\nimaging. With the increased use of smart grids driven by the integration of\nrenewable energy systems, machine learning plays an important role in\npredicting power demand and detecting system disturbances. This study provides\nan in-depth investigation of QNNs for predicting the power output of a wind\nturbine. We assess the predictive performance and simulation time of six QNN\nconfigurations that are based on the Z Feature Map for data encoding and\nvarying ansatz structures. Through detailed cross-validation experiments and\ntests on an unseen hold-out dataset, we experimentally demonstrate that QNNs\ncan achieve predictive performance that is competitive with, and in some cases\nmarginally better than, the benchmarked classical approaches. Our results also\nreveal the effects of dataset size and circuit complexity on predictive\nperformance and simulation time. We believe our findings will offer valuable\ninsights for researchers in the energy domain who wish to incorporate quantum\nmachine learning into their work.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22845v1', 'title': 'Quantum Neural Networks for Wind Energy Forecasting: A Comparative Study of Performance and Scalability with Classical Models', 'abstract': 'Quantum Neural Networks (QNNs), a prominent approach in Quantum Machine\\nLearning (QML), are emerging as a powerful alternative to classical machine\\nlearning methods. Recent studies have focused on the applicability of QNNs to\\nvarious tasks, such as time-series forecasting, prediction, and classification,\\nacross a wide range of applications, including cybersecurity and medical\\nimaging. With the increased use of smart grids driven by the integration of\\nrenewable energy systems, machine learning plays an important role in\\npredicting power demand and detecting system disturbances. This study provides\\nan in-depth investigation of QNNs for predicting the power output of a wind\\nturbine. We assess the predictive performance and simulation time of six QNN\\nconfigurations that are based on the Z Feature Map for data encoding and\\nvarying ansatz structures. Through detailed cross-validation experiments and\\ntests on an unseen hold-out dataset, we experimentally demonstrate that QNNs\\ncan achieve predictive performance that is competitive with, and in some cases\\nmarginally better than, the benchmarked classical approaches. Our results also\\nreveal the effects of dataset size and circuit complexity on predictive\\nperformance and simulation time. We believe our findings will offer valuable\\ninsights for researchers in the energy domain who wish to incorporate quantum\\nmachine learning into their work.', 'authors': ['Batuhan Hangun', 'Oguz Altun', 'Onder Eyecioglu'], 'published': '2025-06-28T10:51:27+00:00', 'categories': ['cs.LG', 'cs.AI', 'cs.PF'], 'url': 'http://arxiv.org/abs/2506.22845v1'}"}, {'id': 164, 'arxiv_id': '2506.22837v1', 'base_arxiv_id': '2506.22837', 'version': '1', 'title': 'xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection', 'abstract': 'The recently proposed xLSTM is a powerful model that leverages expressive\nmultiplicative gating and residual connections, providing the temporal capacity\nneeded for long-horizon forecasting and representation learning. This\narchitecture has demonstrated success in time series forecasting, lossless\ncompression, and even large-scale language modeling tasks, where its linear\nmemory footprint and fast inference make it a viable alternative to\nTransformers. Despite its growing popularity, no prior work has explored xLSTM\nfor anomaly detection. In this work, we fill this gap by proposing xLSTMAD, the\nfirst anomaly detection method that integrates a full encoder-decoder xLSTM\narchitecture, purpose-built for multivariate time series data. Our encoder\nprocesses input sequences to capture historical context, while the decoder is\ndevised in two separate variants of the method. In the forecasting approach,\nthe decoder iteratively generates forecasted future values xLSTMAD-F, while the\nreconstruction approach reconstructs the input time series from its encoded\ncounterpart xLSTMAD-R. We investigate the performance of two loss functions:\nMean Squared Error (MSE), and Soft Dynamic Time Warping (SoftDTW) to consider\nlocal reconstruction fidelity and global sequence alignment, respectively. We\nevaluate our method on the comprehensive TSB-AD-M benchmark, which spans 17\nreal-world datasets, using state-of-the-art challenging metrics such as VUS-PR.\nIn our results, xLSTM showcases state-of-the-art accuracy, outperforming 23\npopular anomaly detection baselines. Our paper is the first work revealing the\npowerful modeling capabilities of xLSTM for anomaly detection, paving the way\nfor exciting new developments on this subject. Our code is available at:\nhttps://github.com/Nyderx/xlstmad', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22837v1', 'title': 'xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection', 'abstract': 'The recently proposed xLSTM is a powerful model that leverages expressive\\nmultiplicative gating and residual connections, providing the temporal capacity\\nneeded for long-horizon forecasting and representation learning. This\\narchitecture has demonstrated success in time series forecasting, lossless\\ncompression, and even large-scale language modeling tasks, where its linear\\nmemory footprint and fast inference make it a viable alternative to\\nTransformers. Despite its growing popularity, no prior work has explored xLSTM\\nfor anomaly detection. In this work, we fill this gap by proposing xLSTMAD, the\\nfirst anomaly detection method that integrates a full encoder-decoder xLSTM\\narchitecture, purpose-built for multivariate time series data. Our encoder\\nprocesses input sequences to capture historical context, while the decoder is\\ndevised in two separate variants of the method. In the forecasting approach,\\nthe decoder iteratively generates forecasted future values xLSTMAD-F, while the\\nreconstruction approach reconstructs the input time series from its encoded\\ncounterpart xLSTMAD-R. We investigate the performance of two loss functions:\\nMean Squared Error (MSE), and Soft Dynamic Time Warping (SoftDTW) to consider\\nlocal reconstruction fidelity and global sequence alignment, respectively. We\\nevaluate our method on the comprehensive TSB-AD-M benchmark, which spans 17\\nreal-world datasets, using state-of-the-art challenging metrics such as VUS-PR.\\nIn our results, xLSTM showcases state-of-the-art accuracy, outperforming 23\\npopular anomaly detection baselines. Our paper is the first work revealing the\\npowerful modeling capabilities of xLSTM for anomaly detection, paving the way\\nfor exciting new developments on this subject. Our code is available at:\\nhttps://github.com/Nyderx/xlstmad', 'authors': ['Kamil Faber', 'Marcin Pietro≈Ñ', 'Dominik ≈ªurek', 'Roberto Corizzo'], 'published': '2025-06-28T10:39:09+00:00', 'categories': ['cs.LG', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.22837v1'}"}, {'id': 165, 'arxiv_id': '2506.22832v1', 'base_arxiv_id': '2506.22832', 'version': '1', 'title': 'Listener-Rewarded Thinking in VLMs for Image Preferences', 'abstract': 'Training robust and generalizable reward models for human visual preferences\nis essential for aligning text-to-image and text-to-video generative models\nwith human intent. However, current reward models often fail to generalize, and\nsupervised fine-tuning leads to memorization, demanding complex annotation\npipelines. While reinforcement learning (RL), specifically Group Relative\nPolicy Optimization (GRPO), improves generalization, we uncover a key failure\nmode: a significant drop in reasoning accuracy occurs when a model\'s reasoning\ntrace contradicts that of an independent, frozen vision-language model\n("listener") evaluating the same output. To address this, we introduce a\nlistener-augmented GRPO framework. Here, the listener re-evaluates the\nreasoner\'s chain-of-thought to provide a dense, calibrated confidence score,\nshaping the RL reward signal. This encourages the reasoner not only to answer\ncorrectly, but to produce explanations that are persuasive to an independent\nmodel. Our listener-shaped reward scheme achieves best accuracy on the\nImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)\nperformance on a large-scale human preference dataset (1.2M votes, up to +6%\nover naive reasoner), and reduces reasoning contradictions compared to strong\nGRPO and SFT baselines. These results demonstrate that listener-based rewards\nprovide a scalable, data-efficient path to aligning vision-language models with\nnuanced human preferences. We will release our reasoning model here:\nhttps://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': '{\'id\': \'2506.22832v1\', \'title\': \'Listener-Rewarded Thinking in VLMs for Image Preferences\', \'abstract\': \'Training robust and generalizable reward models for human visual preferences\\nis essential for aligning text-to-image and text-to-video generative models\\nwith human intent. However, current reward models often fail to generalize, and\\nsupervised fine-tuning leads to memorization, demanding complex annotation\\npipelines. While reinforcement learning (RL), specifically Group Relative\\nPolicy Optimization (GRPO), improves generalization, we uncover a key failure\\nmode: a significant drop in reasoning accuracy occurs when a model\\\'s reasoning\\ntrace contradicts that of an independent, frozen vision-language model\\n("listener") evaluating the same output. To address this, we introduce a\\nlistener-augmented GRPO framework. Here, the listener re-evaluates the\\nreasoner\\\'s chain-of-thought to provide a dense, calibrated confidence score,\\nshaping the RL reward signal. This encourages the reasoner not only to answer\\ncorrectly, but to produce explanations that are persuasive to an independent\\nmodel. Our listener-shaped reward scheme achieves best accuracy on the\\nImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)\\nperformance on a large-scale human preference dataset (1.2M votes, up to +6%\\nover naive reasoner), and reduces reasoning contradictions compared to strong\\nGRPO and SFT baselines. These results demonstrate that listener-based rewards\\nprovide a scalable, data-efficient path to aligning vision-language models with\\nnuanced human preferences. We will release our reasoning model here:\\nhttps://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.\', \'authors\': [\'Alexander Gambashidze\', \'Li Pengyi\', \'Matvey Skripkin\', \'Andrey Galichin\', \'Anton Gusarov\', \'Konstantin Sobolev\', \'Andrey Kuznetsov\', \'Ivan Oseledets\'], \'published\': \'2025-06-28T09:53:17+00:00\', \'categories\': [\'cs.CV\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.22832v1\'}'}, {'id': 166, 'arxiv_id': '2506.22818v1', 'base_arxiv_id': '2506.22818', 'version': '1', 'title': 'TriADA: Massively Parallel Trilinear Matrix-by-Tensor Multiply-Add Algorithm and Device Architecture for the Acceleration of 3D Discrete Transformations', 'abstract': 'Multilinear transformations are key in high-performance computing (HPC) and\nartificial intelligence (AI) workloads, where data is represented as tensors.\nHowever, their high computational and memory demands, which grow with\ndimensionality, often slow down critical tasks. Moreover, scaling computation\nby enlarging the number of parallel processing units substantially increases\nenergy consumption, limiting widespread adoption, especially for sparse data,\nwhich is common in HPC and AI applications. This paper introduces the Trilinear\nAlgorithm and isomorphic to algorithm Device Architecture (TriADA) to address\nthese challenges with the following innovations: (1) a massively parallel,\nlow-rank algorithm for computing a family of trilinear (3D) discrete orthogonal\ntransformations (3D-DXTs), which is a special case of the more general 3-mode\nmatrix-by-tensor multiplication (3D-GEMT); (2) a new outer-product-based GEMM\nkernel with decoupled streaming active memory, specially designed to accelerate\n3D-GEMT operation; (3) an isomorphic to the proposed algorithm, fully\ndistributed 3D network of mesh interconnected processing elements or cells with\na coordinate-free, data-driven local processing activity, which is independent\nof problem size; (4) an elastic sparse outer-product (ESOP) method that avoids\nunnecessary computing and communication operations with zero-valued operands,\nthereby enhancing energy efficiency, computational accuracy, and stability.\nTriADA is capable of performing a variety of trilinear transformations with\nhypercubic arithmetic complexity in a linear number of time-steps. The\nmassively parallel, scalable, and energy-efficient architecture of TriADA is\nideal for accelerating multilinear tensor operations, which are the most\ndemanding parts of AI and HPC workloads.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22818v1', 'title': 'TriADA: Massively Parallel Trilinear Matrix-by-Tensor Multiply-Add Algorithm and Device Architecture for the Acceleration of 3D Discrete Transformations', 'abstract': 'Multilinear transformations are key in high-performance computing (HPC) and\\nartificial intelligence (AI) workloads, where data is represented as tensors.\\nHowever, their high computational and memory demands, which grow with\\ndimensionality, often slow down critical tasks. Moreover, scaling computation\\nby enlarging the number of parallel processing units substantially increases\\nenergy consumption, limiting widespread adoption, especially for sparse data,\\nwhich is common in HPC and AI applications. This paper introduces the Trilinear\\nAlgorithm and isomorphic to algorithm Device Architecture (TriADA) to address\\nthese challenges with the following innovations: (1) a massively parallel,\\nlow-rank algorithm for computing a family of trilinear (3D) discrete orthogonal\\ntransformations (3D-DXTs), which is a special case of the more general 3-mode\\nmatrix-by-tensor multiplication (3D-GEMT); (2) a new outer-product-based GEMM\\nkernel with decoupled streaming active memory, specially designed to accelerate\\n3D-GEMT operation; (3) an isomorphic to the proposed algorithm, fully\\ndistributed 3D network of mesh interconnected processing elements or cells with\\na coordinate-free, data-driven local processing activity, which is independent\\nof problem size; (4) an elastic sparse outer-product (ESOP) method that avoids\\nunnecessary computing and communication operations with zero-valued operands,\\nthereby enhancing energy efficiency, computational accuracy, and stability.\\nTriADA is capable of performing a variety of trilinear transformations with\\nhypercubic arithmetic complexity in a linear number of time-steps. The\\nmassively parallel, scalable, and energy-efficient architecture of TriADA is\\nideal for accelerating multilinear tensor operations, which are the most\\ndemanding parts of AI and HPC workloads.', 'authors': ['Stanislav Sedukhin', 'Yoichi Tomioka', 'Kazuya Matsumoto', 'Yuichi Okuyama'], 'published': '2025-06-28T08:42:01+00:00', 'categories': ['cs.DC', 'cs.AI', 'cs.AR', 'cs.ET', 'eess.SP', 'C.1.4; C.3; F.2.1; G.1.3; G.4'], 'url': 'http://arxiv.org/abs/2506.22818v1'}"}, {'id': 179, 'arxiv_id': '2506.22716v1', 'base_arxiv_id': '2506.22716', 'version': '1', 'title': 'BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute', 'abstract': 'Large language models (LLMs) are powerful tools but are often expensive to\ndeploy at scale. LLM query routing mitigates this by dynamically assigning\nqueries to models of varying cost and quality to obtain a desired trade-off.\nPrior query routing approaches generate only one response from the selected\nmodel and a single response from a small (inexpensive) model was often not good\nenough to beat a response from a large (expensive) model due to which they end\nup overusing the large model and missing out on potential cost savings.\nHowever, it is well known that for small models, generating multiple responses\nand selecting the best can enhance quality while remaining cheaper than a\nsingle large-model response. We leverage this idea to propose BEST-Route, a\nnovel routing framework that chooses a model and the number of responses to\nsample from it based on query difficulty and the quality thresholds.\nExperiments on real-world datasets demonstrate that our method reduces costs by\nup to 60% with less than 1% performance drop.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22716v1', 'title': 'BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute', 'abstract': 'Large language models (LLMs) are powerful tools but are often expensive to\\ndeploy at scale. LLM query routing mitigates this by dynamically assigning\\nqueries to models of varying cost and quality to obtain a desired trade-off.\\nPrior query routing approaches generate only one response from the selected\\nmodel and a single response from a small (inexpensive) model was often not good\\nenough to beat a response from a large (expensive) model due to which they end\\nup overusing the large model and missing out on potential cost savings.\\nHowever, it is well known that for small models, generating multiple responses\\nand selecting the best can enhance quality while remaining cheaper than a\\nsingle large-model response. We leverage this idea to propose BEST-Route, a\\nnovel routing framework that chooses a model and the number of responses to\\nsample from it based on query difficulty and the quality thresholds.\\nExperiments on real-world datasets demonstrate that our method reduces costs by\\nup to 60% with less than 1% performance drop.', 'authors': ['Dujian Ding', 'Ankur Mallick', 'Shaokun Zhang', 'Chi Wang', 'Daniel Madrigal', 'Mirian Del Carmen Hipolito Garcia', 'Menglin Xia', 'Laks V. S. Lakshmanan', 'Qingyun Wu', 'Victor R√ºhle'], 'published': '2025-06-28T01:52:50+00:00', 'categories': ['cs.LG', 'cs.AI', 'cs.CL', 'cs.DB'], 'url': 'http://arxiv.org/abs/2506.22716v1'}"}, {'id': 167, 'arxiv_id': '2506.22809v1', 'base_arxiv_id': '2506.22809', 'version': '1', 'title': 'BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters', 'abstract': 'We propose BayesLoRA, a task-specific uncertainty quantification framework\nthat integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike\ngeneral-purpose transformer uncertainty methods, BayesLoRA provides guardrails\ntailored to downstream workflows, enabling agents to introspect and modulate\nbehavior under uncertainty. We demonstrate mathematically and empirically that\nLoRA adapters exhibit amplified variance outside fine-tuning distributions,\nyielding reliable confidence estimates for agentic decision-making.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22809v1', 'title': 'BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters', 'abstract': 'We propose BayesLoRA, a task-specific uncertainty quantification framework\\nthat integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike\\ngeneral-purpose transformer uncertainty methods, BayesLoRA provides guardrails\\ntailored to downstream workflows, enabling agents to introspect and modulate\\nbehavior under uncertainty. We demonstrate mathematically and empirically that\\nLoRA adapters exhibit amplified variance outside fine-tuning distributions,\\nyielding reliable confidence estimates for agentic decision-making.', 'authors': ['Cooper Doyle'], 'published': '2025-06-28T08:22:02+00:00', 'categories': ['cs.LG', 'cs.AI', 'cs.CL'], 'url': 'http://arxiv.org/abs/2506.22809v1'}"}, {'id': 168, 'arxiv_id': '2506.22808v1', 'base_arxiv_id': '2506.22808', 'version': '1', 'title': 'MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs', 'abstract': 'While Medical Large Language Models (MedLLMs) have demonstrated remarkable\npotential in clinical tasks, their ethical safety remains insufficiently\nexplored. This paper introduces $MedEthicsQA$, a comprehensive\nbenchmark comprising $5,623$ multiple-choice questions and\n$5,351$ open-ended questions for evaluation of medical ethics in LLMs.\nWe systematically establish a hierarchical taxonomy integrating global medical\nethical standards. The benchmark encompasses widely used medical datasets,\nauthoritative question banks, and scenarios derived from PubMed literature.\nRigorous quality control involving multi-stage filtering and multi-faceted\nexpert validation ensures the reliability of the dataset with a low error rate\n($2.72\\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance\nin answering medical ethics questions compared to their foundation\ncounterparts, elucidating the deficiencies of medical ethics alignment. The\ndataset, registered under CC BY-NC 4.0 license, is available at\nhttps://github.com/JianhuiWei7/MedEthicsQA.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22808v1', 'title': 'MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs', 'abstract': 'While Medical Large Language Models (MedLLMs) have demonstrated remarkable\\npotential in clinical tasks, their ethical safety remains insufficiently\\nexplored. This paper introduces $\\\\textbf{MedEthicsQA}$, a comprehensive\\nbenchmark comprising $\\\\textbf{5,623}$ multiple-choice questions and\\n$\\\\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs.\\nWe systematically establish a hierarchical taxonomy integrating global medical\\nethical standards. The benchmark encompasses widely used medical datasets,\\nauthoritative question banks, and scenarios derived from PubMed literature.\\nRigorous quality control involving multi-stage filtering and multi-faceted\\nexpert validation ensures the reliability of the dataset with a low error rate\\n($2.72\\\\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance\\nin answering medical ethics questions compared to their foundation\\ncounterparts, elucidating the deficiencies of medical ethics alignment. The\\ndataset, registered under CC BY-NC 4.0 license, is available at\\nhttps://github.com/JianhuiWei7/MedEthicsQA.', 'authors': ['Jianhui Wei', 'Zijie Meng', 'Zikai Xiao', 'Tianxiang Hu', 'Yang Feng', 'Zhijie Zhou', 'Jian Wu', 'Zuozhu Liu'], 'published': '2025-06-28T08:21:35+00:00', 'categories': ['cs.CL', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.22808v1'}"}, {'id': 169, 'arxiv_id': '2506.22793v1', 'base_arxiv_id': '2506.22793', 'version': '1', 'title': 'Offline Reinforcement Learning for Mobility Robustness Optimization', 'abstract': 'In this work we revisit the Mobility Robustness Optimisation (MRO) algorithm\nand study the possibility of learning the optimal Cell Individual Offset tuning\nusing offline Reinforcement Learning. Such methods make use of collected\noffline datasets to learn the optimal policy, without further exploration. We\nadapt and apply a sequence-based method called Decision Transformers as well as\na value-based method called Conservative Q-Learning to learn the optimal policy\nfor the same target reward as the vanilla rule-based MRO. The same input\nfeatures related to failures, ping-pongs, and other handover issues are used.\nEvaluation for realistic New Radio networks with 3500 MHz carrier frequency on\na traffic mix including diverse user service types and a specific tunable\ncell-pair shows that offline-RL methods outperform rule-based MRO, offering up\nto 7% improvement. Furthermore, offline-RL can be trained for diverse objective\nfunctions using the same available dataset, thus offering operational\nflexibility compared to rule-based methods.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22793v1', 'title': 'Offline Reinforcement Learning for Mobility Robustness Optimization', 'abstract': 'In this work we revisit the Mobility Robustness Optimisation (MRO) algorithm\\nand study the possibility of learning the optimal Cell Individual Offset tuning\\nusing offline Reinforcement Learning. Such methods make use of collected\\noffline datasets to learn the optimal policy, without further exploration. We\\nadapt and apply a sequence-based method called Decision Transformers as well as\\na value-based method called Conservative Q-Learning to learn the optimal policy\\nfor the same target reward as the vanilla rule-based MRO. The same input\\nfeatures related to failures, ping-pongs, and other handover issues are used.\\nEvaluation for realistic New Radio networks with 3500 MHz carrier frequency on\\na traffic mix including diverse user service types and a specific tunable\\ncell-pair shows that offline-RL methods outperform rule-based MRO, offering up\\nto 7% improvement. Furthermore, offline-RL can be trained for diverse objective\\nfunctions using the same available dataset, thus offering operational\\nflexibility compared to rule-based methods.', 'authors': ['Pegah Alizadeh', 'Anastasios Giovanidis', 'Pradeepa Ramachandra', 'Vasileios Koutsoukis', 'Osama Arouk'], 'published': '2025-06-28T07:31:01+00:00', 'categories': ['cs.NI', 'cs.AI', 'cs.PF'], 'url': 'http://arxiv.org/abs/2506.22793v1'}"}, {'id': 170, 'arxiv_id': '2506.22789v1', 'base_arxiv_id': '2506.22789', 'version': '1', 'title': 'WavShape: Information-Theoretic Speech Representation Learning for Fair and Privacy-Aware Audio Processing', 'abstract': 'Speech embeddings often retain sensitive attributes such as speaker identity,\naccent, or demographic information, posing risks in biased model training and\nprivacy leakage. We propose WavShape, an information-theoretic speech\nrepresentation learning framework that optimizes embeddings for fairness and\nprivacy while preserving task-relevant information. We leverage mutual\ninformation (MI) estimation using the Donsker-Varadhan formulation to guide an\nMI-based encoder that systematically filters sensitive attributes while\nmaintaining speech content essential for downstream tasks. Experimental results\non three known datasets show that WavShape reduces MI between embeddings and\nsensitive attributes by up to 81% while retaining 97% of task-relevant\ninformation. By integrating information theory with self-supervised speech\nmodels, this work advances the development of fair, privacy-aware, and\nresource-efficient speech systems.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22789v1', 'title': 'WavShape: Information-Theoretic Speech Representation Learning for Fair and Privacy-Aware Audio Processing', 'abstract': 'Speech embeddings often retain sensitive attributes such as speaker identity,\\naccent, or demographic information, posing risks in biased model training and\\nprivacy leakage. We propose WavShape, an information-theoretic speech\\nrepresentation learning framework that optimizes embeddings for fairness and\\nprivacy while preserving task-relevant information. We leverage mutual\\ninformation (MI) estimation using the Donsker-Varadhan formulation to guide an\\nMI-based encoder that systematically filters sensitive attributes while\\nmaintaining speech content essential for downstream tasks. Experimental results\\non three known datasets show that WavShape reduces MI between embeddings and\\nsensitive attributes by up to 81% while retaining 97% of task-relevant\\ninformation. By integrating information theory with self-supervised speech\\nmodels, this work advances the development of fair, privacy-aware, and\\nresource-efficient speech systems.', 'authors': ['Oguzhan Baser', 'Ahmet Ege Tanriverdi', 'Kaan Kale', 'Sandeep P. Chinchali', 'Sriram Vishwanath'], 'published': '2025-06-28T07:03:55+00:00', 'categories': ['cs.SD', 'cs.AI', 'eess.AS'], 'url': 'http://arxiv.org/abs/2506.22789v1'}"}, {'id': 171, 'arxiv_id': '2506.22784v1', 'base_arxiv_id': '2506.22784', 'version': '1', 'title': 'Single-Frame Point-Pixel Registration via Supervised Cross-Modal Feature Matching', 'abstract': 'Point-pixel registration between LiDAR point clouds and camera images is a\nfundamental yet challenging task in autonomous driving and robotic perception.\nA key difficulty lies in the modality gap between unstructured point clouds and\nstructured images, especially under sparse single-frame LiDAR settings.\nExisting methods typically extract features separately from point clouds and\nimages, then rely on hand-crafted or learned matching strategies. This separate\nencoding fails to bridge the modality gap effectively, and more critically,\nthese methods struggle with the sparsity and noise of single-frame LiDAR, often\nrequiring point cloud accumulation or additional priors to improve reliability.\nInspired by recent progress in detector-free matching paradigms (e.g.\nMatchAnything), we revisit the projection-based approach and introduce the\ndetector-free framework for direct point-pixel matching between LiDAR and\ncamera views. Specifically, we project the LiDAR intensity map into a 2D view\nfrom the LiDAR perspective and feed it into an attention-based detector-free\nmatching network, enabling cross-modal correspondence estimation without\nrelying on multi-frame accumulation. To further enhance matching reliability,\nwe introduce a repeatability scoring mechanism that acts as a soft visibility\nprior. This guides the network to suppress unreliable matches in regions with\nlow intensity variation, improving robustness under sparse input. Extensive\nexperiments on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks demonstrate that\nour method achieves state-of-the-art performance, outperforming prior\napproaches on nuScenes (even those relying on accumulated point clouds),\ndespite using only single-frame LiDAR.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22784v1', 'title': 'Single-Frame Point-Pixel Registration via Supervised Cross-Modal Feature Matching', 'abstract': 'Point-pixel registration between LiDAR point clouds and camera images is a\\nfundamental yet challenging task in autonomous driving and robotic perception.\\nA key difficulty lies in the modality gap between unstructured point clouds and\\nstructured images, especially under sparse single-frame LiDAR settings.\\nExisting methods typically extract features separately from point clouds and\\nimages, then rely on hand-crafted or learned matching strategies. This separate\\nencoding fails to bridge the modality gap effectively, and more critically,\\nthese methods struggle with the sparsity and noise of single-frame LiDAR, often\\nrequiring point cloud accumulation or additional priors to improve reliability.\\nInspired by recent progress in detector-free matching paradigms (e.g.\\nMatchAnything), we revisit the projection-based approach and introduce the\\ndetector-free framework for direct point-pixel matching between LiDAR and\\ncamera views. Specifically, we project the LiDAR intensity map into a 2D view\\nfrom the LiDAR perspective and feed it into an attention-based detector-free\\nmatching network, enabling cross-modal correspondence estimation without\\nrelying on multi-frame accumulation. To further enhance matching reliability,\\nwe introduce a repeatability scoring mechanism that acts as a soft visibility\\nprior. This guides the network to suppress unreliable matches in regions with\\nlow intensity variation, improving robustness under sparse input. Extensive\\nexperiments on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks demonstrate that\\nour method achieves state-of-the-art performance, outperforming prior\\napproaches on nuScenes (even those relying on accumulated point clouds),\\ndespite using only single-frame LiDAR.', 'authors': ['Yu Han', 'Zhiwei Huang', 'Yanting Zhang', 'Fangjun Ding', 'Shen Cai', 'Rui Fan'], 'published': '2025-06-28T06:57:13+00:00', 'categories': ['cs.CV', 'cs.AI', 'cs.RO'], 'url': 'http://arxiv.org/abs/2506.22784v1'}"}, {'id': 172, 'arxiv_id': '2506.22783v1', 'base_arxiv_id': '2506.22783', 'version': '1', 'title': 'PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection', 'abstract': 'Deepfake (DF) attacks pose a growing threat as generative models become\nincreasingly advanced. However, our study reveals that existing DF datasets\nfail to deceive human perception, unlike real DF attacks that influence public\ndiscourse. It highlights the need for more realistic DF attack vectors. We\nintroduce PhonemeFake (PF), a DF attack that manipulates critical speech\nsegments using language reasoning, significantly reducing human perception by\nup to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF\ndataset on HuggingFace and open-source bilevel DF segment detection model that\nadaptively prioritizes compute on manipulated regions. Our extensive\nexperiments across three known DF datasets reveal that our detection model\nreduces EER by 91% while achieving up to 90% speed-up, with minimal compute\noverhead and precise localization beyond existing models as a scalable\nsolution.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22783v1', 'title': 'PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection', 'abstract': 'Deepfake (DF) attacks pose a growing threat as generative models become\\nincreasingly advanced. However, our study reveals that existing DF datasets\\nfail to deceive human perception, unlike real DF attacks that influence public\\ndiscourse. It highlights the need for more realistic DF attack vectors. We\\nintroduce PhonemeFake (PF), a DF attack that manipulates critical speech\\nsegments using language reasoning, significantly reducing human perception by\\nup to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF\\ndataset on HuggingFace and open-source bilevel DF segment detection model that\\nadaptively prioritizes compute on manipulated regions. Our extensive\\nexperiments across three known DF datasets reveal that our detection model\\nreduces EER by 91% while achieving up to 90% speed-up, with minimal compute\\noverhead and precise localization beyond existing models as a scalable\\nsolution.', 'authors': ['Oguzhan Baser', 'Ahmet Ege Tanriverdi', 'Sriram Vishwanath', 'Sandeep P. Chinchali'], 'published': '2025-06-28T06:56:41+00:00', 'categories': ['cs.CV', 'cs.AI', 'cs.CL'], 'url': 'http://arxiv.org/abs/2506.22783v1'}"}, {'id': 173, 'arxiv_id': '2506.22777v1', 'base_arxiv_id': '2506.22777', 'version': '1', 'title': 'Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning', 'abstract': 'Language models trained with RL can engage in reward hacking--exploiting\nunintended strategies for high reward--without revealing this behavior in their\nchain-of-thought reasoning, making detection difficult and posing risks for\nhigh-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL\nintervention that trains models to explicitly acknowledge when they are\ninfluenced by prompt cues--hints which point to incorrect answers (e.g., "a\nStanford professor thinks the answer is A"). To evaluate VFT, we subsequently\ntrain models with RL on environments where held-out prompt cues signal which\nincorrect answers will receive high reward, incentivizing models to reward hack\nby exploiting cues instead of reasoning correctly. We measure how often models\nexploit these cues without verbalizing it. After RL, only 6% of the VFT-trained\nmodel\'s responses consist of undetected reward hacks. In comparison, when we\nperform RL without VFT, the rate of undetected reward hacks goes up to 88%;\nwith a debiasing baseline intervention, this increases further to 99%. VFT\nachieves this by substantially increasing how often models verbalize the\ninfluence of cues--from 8% to 42% after VFT, and up to 94% after RL--while\nbaselines remain low even after RL (10% and 1%). Our results show that teaching\nmodels to explicitly verbalize reward hacking behavior before RL significantly\nimproves their detection, offering a practical path toward more transparent and\nsafe AI systems.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': '{\'id\': \'2506.22777v1\', \'title\': \'Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning\', \'abstract\': \'Language models trained with RL can engage in reward hacking--exploiting\\nunintended strategies for high reward--without revealing this behavior in their\\nchain-of-thought reasoning, making detection difficult and posing risks for\\nhigh-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL\\nintervention that trains models to explicitly acknowledge when they are\\ninfluenced by prompt cues--hints which point to incorrect answers (e.g., "a\\nStanford professor thinks the answer is A"). To evaluate VFT, we subsequently\\ntrain models with RL on environments where held-out prompt cues signal which\\nincorrect answers will receive high reward, incentivizing models to reward hack\\nby exploiting cues instead of reasoning correctly. We measure how often models\\nexploit these cues without verbalizing it. After RL, only 6% of the VFT-trained\\nmodel\\\'s responses consist of undetected reward hacks. In comparison, when we\\nperform RL without VFT, the rate of undetected reward hacks goes up to 88%;\\nwith a debiasing baseline intervention, this increases further to 99%. VFT\\nachieves this by substantially increasing how often models verbalize the\\ninfluence of cues--from 8% to 42% after VFT, and up to 94% after RL--while\\nbaselines remain low even after RL (10% and 1%). Our results show that teaching\\nmodels to explicitly verbalize reward hacking behavior before RL significantly\\nimproves their detection, offering a practical path toward more transparent and\\nsafe AI systems.\', \'authors\': [\'Miles Turpin\', \'Andy Arditi\', \'Marvin Li\', \'Joe Benton\', \'Julian Michael\'], \'published\': \'2025-06-28T06:37:10+00:00\', \'categories\': [\'cs.CL\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.22777v1\'}'}, {'id': 174, 'arxiv_id': '2506.22776v1', 'base_arxiv_id': '2506.22776', 'version': '1', 'title': 'Smaller = Weaker? Benchmarking Robustness of Quantized LLMs in Code Generation', 'abstract': "Quantization has emerged as a mainstream method for compressing Large\nLanguage Models (LLMs), reducing memory requirements and accelerating inference\nwithout architectural modifications. While existing research primarily focuses\non evaluating the effectiveness of quantized LLMs compared to their original\ncounterparts, the impact on robustness remains largely unexplored.In this\npaper, we present the first systematic investigation of how quantization\naffects the robustness of LLMs in code generation tasks. Through extensive\nexperiments across four prominent LLM families (LLaMA, DeepSeek, CodeGen, and\nStarCoder) with parameter scales ranging from 350M to 33B, we evaluate\nrobustness from dual perspectives: adversarial attacks on input prompts and\nnoise perturbations on model architecture. Our findings challenge conventional\nwisdom by demonstrating that quantized LLMs often exhibit superior robustness\ncompared to their full-precision counterparts, with 51.59% versus 42.86% of our\nadversarial experiments showing better resilience in quantized LLMs. Similarly,\nour noise perturbation experiments also confirm that LLMs after quantitation\ngenerally withstand higher levels of weight disturbances. These results suggest\nthat quantization not only reduces computational requirements but can actually\nenhance LLMs' reliability in code generation tasks, providing valuable insights\nfor developing more robust and efficient LLM deployment strategies.", 'submit_date': datetime.date(2025, 6, 28), 'metadata': '{\'id\': \'2506.22776v1\', \'title\': \'Smaller = Weaker? Benchmarking Robustness of Quantized LLMs in Code Generation\', \'abstract\': "Quantization has emerged as a mainstream method for compressing Large\\nLanguage Models (LLMs), reducing memory requirements and accelerating inference\\nwithout architectural modifications. While existing research primarily focuses\\non evaluating the effectiveness of quantized LLMs compared to their original\\ncounterparts, the impact on robustness remains largely unexplored.In this\\npaper, we present the first systematic investigation of how quantization\\naffects the robustness of LLMs in code generation tasks. Through extensive\\nexperiments across four prominent LLM families (LLaMA, DeepSeek, CodeGen, and\\nStarCoder) with parameter scales ranging from 350M to 33B, we evaluate\\nrobustness from dual perspectives: adversarial attacks on input prompts and\\nnoise perturbations on model architecture. Our findings challenge conventional\\nwisdom by demonstrating that quantized LLMs often exhibit superior robustness\\ncompared to their full-precision counterparts, with 51.59% versus 42.86% of our\\nadversarial experiments showing better resilience in quantized LLMs. Similarly,\\nour noise perturbation experiments also confirm that LLMs after quantitation\\ngenerally withstand higher levels of weight disturbances. These results suggest\\nthat quantization not only reduces computational requirements but can actually\\nenhance LLMs\' reliability in code generation tasks, providing valuable insights\\nfor developing more robust and efficient LLM deployment strategies.", \'authors\': [\'Sen Fang\', \'Weiyuan Ding\', \'Antonio Mastropaolo\', \'Bowen Xu\'], \'published\': \'2025-06-28T06:32:25+00:00\', \'categories\': [\'cs.SE\', \'cs.AI\', \'cs.PL\'], \'url\': \'http://arxiv.org/abs/2506.22776v1\'}'}, {'id': 175, 'arxiv_id': '2506.22771v1', 'base_arxiv_id': '2506.22771', 'version': '1', 'title': 'FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision', 'abstract': 'Backpropagation has been the cornerstone of neural network training for\ndecades, yet its inefficiencies in time and energy consumption limit its\nsuitability for resource-constrained edge devices. While low-precision neural\nnetwork quantization has been extensively researched to speed up model\ninference, its application in training has been less explored. Recently, the\nForward-Forward (FF) algorithm has emerged as a promising alternative to\nbackpropagation, replacing the backward pass with an additional forward pass.\nBy avoiding the need to store intermediate activations for backpropagation, FF\ncan reduce memory footprint, making it well-suited for embedded devices. This\npaper presents an INT8 quantized training approach that leverages FF\'s\nlayer-by-layer strategy to stabilize gradient quantization. Furthermore, we\npropose a novel "look-ahead" scheme to address limitations of FF and improve\nmodel accuracy. Experiments conducted on NVIDIA Jetson Orin Nano board\ndemonstrate 4.6% faster training, 8.3% energy savings, and 27.0% reduction in\nmemory usage, while maintaining competitive accuracy compared to the\nstate-of-the-art.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': '{\'id\': \'2506.22771v1\', \'title\': \'FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision\', \'abstract\': \'Backpropagation has been the cornerstone of neural network training for\\ndecades, yet its inefficiencies in time and energy consumption limit its\\nsuitability for resource-constrained edge devices. While low-precision neural\\nnetwork quantization has been extensively researched to speed up model\\ninference, its application in training has been less explored. Recently, the\\nForward-Forward (FF) algorithm has emerged as a promising alternative to\\nbackpropagation, replacing the backward pass with an additional forward pass.\\nBy avoiding the need to store intermediate activations for backpropagation, FF\\ncan reduce memory footprint, making it well-suited for embedded devices. This\\npaper presents an INT8 quantized training approach that leverages FF\\\'s\\nlayer-by-layer strategy to stabilize gradient quantization. Furthermore, we\\npropose a novel "look-ahead" scheme to address limitations of FF and improve\\nmodel accuracy. Experiments conducted on NVIDIA Jetson Orin Nano board\\ndemonstrate 4.6% faster training, 8.3% energy savings, and 27.0% reduction in\\nmemory usage, while maintaining competitive accuracy compared to the\\nstate-of-the-art.\', \'authors\': [\'Jingxiao Ma\', \'Priyadarshini Panda\', \'Sherief Reda\'], \'published\': \'2025-06-28T06:16:26+00:00\', \'categories\': [\'cs.LG\', \'cs.AI\', \'cs.NE\', \'I.2.0; I.2.6\'], \'url\': \'http://arxiv.org/abs/2506.22771v1\'}'}, {'id': 176, 'arxiv_id': '2506.22742v1', 'base_arxiv_id': '2506.22742', 'version': '1', 'title': 'RAILS: Retrieval-Augmented Intelligence for Learning Software Development', 'abstract': 'Large Language Models (LLMs) like GPT-3.5-Turbo are increasingly used to\nassist software development, yet they often produce incomplete code or\nincorrect imports, especially when lacking access to external or\nproject-specific documentation. We introduce RAILS (Retrieval-Augmented\nIntelligence for Learning Software Development), a framework that augments LLM\nprompts with semantically retrieved context from curated Java resources using\nFAISS and OpenAI embeddings. RAILS incorporates an iterative validation loop\nguided by compiler feedback to refine suggestions. We evaluated RAILS on 78\nreal-world Java import error cases spanning standard libraries, GUI APIs,\nexternal tools, and custom utilities. Despite using the same LLM, RAILS\noutperforms baseline prompting by preserving intent, avoiding hallucinations,\nand surfacing correct imports even when libraries are unavailable locally.\nFuture work will integrate symbolic filtering via PostgreSQL and extend support\nto other languages and IDEs.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22742v1', 'title': 'RAILS: Retrieval-Augmented Intelligence for Learning Software Development', 'abstract': 'Large Language Models (LLMs) like GPT-3.5-Turbo are increasingly used to\\nassist software development, yet they often produce incomplete code or\\nincorrect imports, especially when lacking access to external or\\nproject-specific documentation. We introduce RAILS (Retrieval-Augmented\\nIntelligence for Learning Software Development), a framework that augments LLM\\nprompts with semantically retrieved context from curated Java resources using\\nFAISS and OpenAI embeddings. RAILS incorporates an iterative validation loop\\nguided by compiler feedback to refine suggestions. We evaluated RAILS on 78\\nreal-world Java import error cases spanning standard libraries, GUI APIs,\\nexternal tools, and custom utilities. Despite using the same LLM, RAILS\\noutperforms baseline prompting by preserving intent, avoiding hallucinations,\\nand surfacing correct imports even when libraries are unavailable locally.\\nFuture work will integrate symbolic filtering via PostgreSQL and extend support\\nto other languages and IDEs.', 'authors': ['Wali Mohammad Abdullah', 'Md. Morshedul Islam', 'Devraj Parmar', 'Happy Hasmukhbhai Patel', 'Sindhuja Prabhakaran', 'Baidya Saha'], 'published': '2025-06-28T03:30:04+00:00', 'categories': ['cs.SE', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.22742v1'}"}, {'id': 177, 'arxiv_id': '2506.22740v1', 'base_arxiv_id': '2506.22740', 'version': '1', 'title': 'Explanations are a means to an end', 'abstract': 'Modern methods for explainable machine learning are designed to describe how\nmodels map inputs to outputs--without deep consideration of how these\nexplanations will be used in practice. This paper argues that explanations\nshould be designed and evaluated with a specific end in mind. We describe how\nto formalize this end in a framework based in statistical decision theory. We\nshow how this functionally-grounded approach can be applied across diverse use\ncases, such as clinical decision support, providing recourse, or debugging. We\ndemonstrate its use to characterize the maximum "boost" in performance on a\nparticular task that an explanation could provide an idealized decision-maker,\npreventing misuse due to ambiguity by forcing researchers to specify concrete\nuse cases that can be analyzed in light of models of expected explanation use.\nWe argue that evaluation should meld theoretical and empirical perspectives on\nthe value of explanation, and contribute definitions that span these\nperspectives.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': '{\'id\': \'2506.22740v1\', \'title\': \'Explanations are a means to an end\', \'abstract\': \'Modern methods for explainable machine learning are designed to describe how\\nmodels map inputs to outputs--without deep consideration of how these\\nexplanations will be used in practice. This paper argues that explanations\\nshould be designed and evaluated with a specific end in mind. We describe how\\nto formalize this end in a framework based in statistical decision theory. We\\nshow how this functionally-grounded approach can be applied across diverse use\\ncases, such as clinical decision support, providing recourse, or debugging. We\\ndemonstrate its use to characterize the maximum "boost" in performance on a\\nparticular task that an explanation could provide an idealized decision-maker,\\npreventing misuse due to ambiguity by forcing researchers to specify concrete\\nuse cases that can be analyzed in light of models of expected explanation use.\\nWe argue that evaluation should meld theoretical and empirical perspectives on\\nthe value of explanation, and contribute definitions that span these\\nperspectives.\', \'authors\': [\'Jessica Hullman\', \'Ziyang Guo\', \'Berk Ustun\'], \'published\': \'2025-06-28T03:04:21+00:00\', \'categories\': [\'cs.AI\', \'stat.ML\'], \'url\': \'http://arxiv.org/abs/2506.22740v1\'}'}, {'id': 178, 'arxiv_id': '2506.22722v1', 'base_arxiv_id': '2506.22722', 'version': '1', 'title': 'Kill Two Birds with One Stone! Trajectory enabled Unified Online Detection of Adversarial Examples and Backdoor Attacks', 'abstract': "The proposed UniGuard is the first unified online detection framework capable\nof simultaneously addressing adversarial examples and backdoor attacks.\nUniGuard builds upon two key insights: first, both AE and backdoor attacks have\nto compromise the inference phase, making it possible to tackle them\nsimultaneously during run-time via online detection. Second, an adversarial\ninput, whether a perturbed sample in AE attacks or a trigger-carrying sample in\nbackdoor attacks, exhibits distinctive trajectory signatures from a benign\nsample as it propagates through the layers of a DL model in forward inference.\nThe propagation trajectory of the adversarial sample must deviate from that of\nits benign counterpart; otherwise, the adversarial objective cannot be\nfulfilled. Detecting these trajectory signatures is inherently challenging due\nto their subtlety; UniGuard overcomes this by treating the propagation\ntrajectory as a time-series signal, leveraging LSTM and spectrum transformation\nto amplify differences between adversarial and benign trajectories that are\nsubtle in the time domain. UniGuard exceptional efficiency and effectiveness\nhave been extensively validated across various modalities (image, text, and\naudio) and tasks (classification and regression), ranging from diverse model\narchitectures against a wide range of AE attacks and backdoor attacks,\nincluding challenging partial backdoors and dynamic triggers. When compared to\nSOTA methods, including ContraNet (NDSS 22) specific for AE detection and TED\n(IEEE SP 24) specific for backdoor detection, UniGuard consistently\ndemonstrates superior performance, even when matched against each method's\nstrengths in addressing their respective threats-each SOTA fails to parts of\nattack strategies while UniGuard succeeds for all.", 'submit_date': datetime.date(2025, 6, 28), 'metadata': '{\'id\': \'2506.22722v1\', \'title\': \'Kill Two Birds with One Stone! Trajectory enabled Unified Online Detection of Adversarial Examples and Backdoor Attacks\', \'abstract\': "The proposed UniGuard is the first unified online detection framework capable\\nof simultaneously addressing adversarial examples and backdoor attacks.\\nUniGuard builds upon two key insights: first, both AE and backdoor attacks have\\nto compromise the inference phase, making it possible to tackle them\\nsimultaneously during run-time via online detection. Second, an adversarial\\ninput, whether a perturbed sample in AE attacks or a trigger-carrying sample in\\nbackdoor attacks, exhibits distinctive trajectory signatures from a benign\\nsample as it propagates through the layers of a DL model in forward inference.\\nThe propagation trajectory of the adversarial sample must deviate from that of\\nits benign counterpart; otherwise, the adversarial objective cannot be\\nfulfilled. Detecting these trajectory signatures is inherently challenging due\\nto their subtlety; UniGuard overcomes this by treating the propagation\\ntrajectory as a time-series signal, leveraging LSTM and spectrum transformation\\nto amplify differences between adversarial and benign trajectories that are\\nsubtle in the time domain. UniGuard exceptional efficiency and effectiveness\\nhave been extensively validated across various modalities (image, text, and\\naudio) and tasks (classification and regression), ranging from diverse model\\narchitectures against a wide range of AE attacks and backdoor attacks,\\nincluding challenging partial backdoors and dynamic triggers. When compared to\\nSOTA methods, including ContraNet (NDSS 22) specific for AE detection and TED\\n(IEEE SP 24) specific for backdoor detection, UniGuard consistently\\ndemonstrates superior performance, even when matched against each method\'s\\nstrengths in addressing their respective threats-each SOTA fails to parts of\\nattack strategies while UniGuard succeeds for all.", \'authors\': [\'Anmin Fu\', \'Fanyu Meng\', \'Huaibing Peng\', \'Hua Ma\', \'Zhi Zhang\', \'Yifeng Zheng\', \'Willy Susilo\', \'Yansong Gao\'], \'published\': \'2025-06-28T02:06:23+00:00\', \'categories\': [\'cs.CR\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.22722v1\'}'}, {'id': 180, 'arxiv_id': '2506.22706v1', 'base_arxiv_id': '2506.22706', 'version': '1', 'title': 'General Autonomous Cybersecurity Defense: Learning Robust Policies for Dynamic Topologies and Diverse Attackers', 'abstract': 'In the face of evolving cyber threats such as malware, ransomware and\nphishing, autonomous cybersecurity defense (ACD) systems have become essential\nfor real-time threat detection and response with optional human intervention.\nHowever, existing ACD systems rely on limiting assumptions, particularly the\nstationarity of the underlying network dynamics. In real-world scenarios,\nnetwork topologies can change due to actions taken by attackers or defenders,\nsystem failures, or time evolution of networks, leading to failures in the\nadaptive capabilities of current defense agents. Moreover, many agents are\ntrained on static environments, resulting in overfitting to specific\ntopologies, which hampers their ability to generalize to out-of-distribution\nnetwork topologies. This work addresses these challenges by exploring methods\nfor developing agents to learn generalizable policies across dynamic network\nenvironments -- general ACD (GACD).', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22706v1', 'title': 'General Autonomous Cybersecurity Defense: Learning Robust Policies for Dynamic Topologies and Diverse Attackers', 'abstract': 'In the face of evolving cyber threats such as malware, ransomware and\\nphishing, autonomous cybersecurity defense (ACD) systems have become essential\\nfor real-time threat detection and response with optional human intervention.\\nHowever, existing ACD systems rely on limiting assumptions, particularly the\\nstationarity of the underlying network dynamics. In real-world scenarios,\\nnetwork topologies can change due to actions taken by attackers or defenders,\\nsystem failures, or time evolution of networks, leading to failures in the\\nadaptive capabilities of current defense agents. Moreover, many agents are\\ntrained on static environments, resulting in overfitting to specific\\ntopologies, which hampers their ability to generalize to out-of-distribution\\nnetwork topologies. This work addresses these challenges by exploring methods\\nfor developing agents to learn generalizable policies across dynamic network\\nenvironments -- general ACD (GACD).', 'authors': ['Arun Ramamurthy', 'Neil Dhir'], 'published': '2025-06-28T01:12:13+00:00', 'categories': ['cs.CR', 'cs.AI', 'cs.CV', 'stat.ML'], 'url': 'http://arxiv.org/abs/2506.22706v1'}"}, {'id': 181, 'arxiv_id': '2506.22703v1', 'base_arxiv_id': '2506.22703', 'version': '1', 'title': 'P4OMP: Retrieval-Augmented Prompting for OpenMP Parallelism in Serial Code', 'abstract': 'We present P4OMP, a retrieval-augmented framework for transforming serial\nC/C++ code into OpenMP-annotated parallel code using large language models\n(LLMs). To our knowledge, this is the first system to apply retrieval-based\nprompting for OpenMP pragma correctness without model fine-tuning or compiler\ninstrumentation. P4OMP leverages Retrieval-Augmented Generation (RAG) with\nstructured instructional knowledge from OpenMP tutorials to improve the\nreliability of prompt-driven code generation. By grounding generation in the\nretrieved context, P4OMP improves syntactic correctness compared to baseline\nprompting with GPT-3.5-Turbo. We evaluate P4OMP against a baseline,\nGPT-3.5-Turbo without retrieval, on a comprehensive benchmark of 108 real-world\nC++ programs drawn from Stack Overflow, PolyBench, and NAS benchmark suites.\nP4OMP achieves 100% compilation success on all parallelizable cases, while the\nbaseline fails to compile in 20 out of 108 cases. Six cases that rely on\nnon-random-access iterators or thread-unsafe constructs are excluded due to\nfundamental OpenMP limitations. A detailed analysis demonstrates how P4OMP\nconsistently avoids scoping errors, syntactic misuse, and invalid directive\ncombinations that commonly affect baseline-generated code. We further\ndemonstrate strong runtime scaling across seven compute-intensive benchmarks on\nan HPC cluster. P4OMP offers a robust, modular pipeline that significantly\nimproves the reliability and applicability of LLM-generated OpenMP code.', 'submit_date': datetime.date(2025, 6, 28), 'metadata': "{'id': '2506.22703v1', 'title': 'P4OMP: Retrieval-Augmented Prompting for OpenMP Parallelism in Serial Code', 'abstract': 'We present P4OMP, a retrieval-augmented framework for transforming serial\\nC/C++ code into OpenMP-annotated parallel code using large language models\\n(LLMs). To our knowledge, this is the first system to apply retrieval-based\\nprompting for OpenMP pragma correctness without model fine-tuning or compiler\\ninstrumentation. P4OMP leverages Retrieval-Augmented Generation (RAG) with\\nstructured instructional knowledge from OpenMP tutorials to improve the\\nreliability of prompt-driven code generation. By grounding generation in the\\nretrieved context, P4OMP improves syntactic correctness compared to baseline\\nprompting with GPT-3.5-Turbo. We evaluate P4OMP against a baseline,\\nGPT-3.5-Turbo without retrieval, on a comprehensive benchmark of 108 real-world\\nC++ programs drawn from Stack Overflow, PolyBench, and NAS benchmark suites.\\nP4OMP achieves 100% compilation success on all parallelizable cases, while the\\nbaseline fails to compile in 20 out of 108 cases. Six cases that rely on\\nnon-random-access iterators or thread-unsafe constructs are excluded due to\\nfundamental OpenMP limitations. A detailed analysis demonstrates how P4OMP\\nconsistently avoids scoping errors, syntactic misuse, and invalid directive\\ncombinations that commonly affect baseline-generated code. We further\\ndemonstrate strong runtime scaling across seven compute-intensive benchmarks on\\nan HPC cluster. P4OMP offers a robust, modular pipeline that significantly\\nimproves the reliability and applicability of LLM-generated OpenMP code.', 'authors': ['Wali Mohammad Abdullah', 'Azmain Kabir'], 'published': '2025-06-28T01:06:34+00:00', 'categories': ['cs.SE', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.22703v1'}"}, {'id': 182, 'arxiv_id': '2506.22668v1', 'base_arxiv_id': '2506.22668', 'version': '1', 'title': 'DistShap: Scalable GNN Explanations with Distributed Shapley Values', 'abstract': 'With the growing adoption of graph neural networks (GNNs), explaining their\npredictions has become increasingly important. However, attributing predictions\nto specific edges or features remains computationally expensive. For example,\nclassifying a node with 100 neighbors using a 3-layer GNN may involve\nidentifying important edges from millions of candidates contributing to the\nprediction. To address this challenge, we propose DistShap, a parallel\nalgorithm that distributes Shapley value-based explanations across multiple\nGPUs. DistShap operates by sampling subgraphs in a distributed setting,\nexecuting GNN inference in parallel across GPUs, and solving a distributed\nleast squares problem to compute edge importance scores. DistShap outperforms\nmost existing GNN explanation methods in accuracy and is the first to scale to\nGNN models with millions of features by using up to 128 GPUs on the NERSC\nPerlmutter supercomputer.', 'submit_date': datetime.date(2025, 6, 27), 'metadata': "{'id': '2506.22668v1', 'title': 'DistShap: Scalable GNN Explanations with Distributed Shapley Values', 'abstract': 'With the growing adoption of graph neural networks (GNNs), explaining their\\npredictions has become increasingly important. However, attributing predictions\\nto specific edges or features remains computationally expensive. For example,\\nclassifying a node with 100 neighbors using a 3-layer GNN may involve\\nidentifying important edges from millions of candidates contributing to the\\nprediction. To address this challenge, we propose DistShap, a parallel\\nalgorithm that distributes Shapley value-based explanations across multiple\\nGPUs. DistShap operates by sampling subgraphs in a distributed setting,\\nexecuting GNN inference in parallel across GPUs, and solving a distributed\\nleast squares problem to compute edge importance scores. DistShap outperforms\\nmost existing GNN explanation methods in accuracy and is the first to scale to\\nGNN models with millions of features by using up to 128 GPUs on the NERSC\\nPerlmutter supercomputer.', 'authors': ['Selahattin Akkas', 'Aditya Devarakonda', 'Ariful Azad'], 'published': '2025-06-27T22:30:49+00:00', 'categories': ['cs.LG', 'cs.AI', 'cs.DC', 'stat.ML'], 'url': 'http://arxiv.org/abs/2506.22668v1'}"}, {'id': 183, 'arxiv_id': '2506.22656v1', 'base_arxiv_id': '2506.22656', 'version': '1', 'title': 'Knowledge-Guided Multi-Agent Framework for Automated Requirements Development: A Vision', 'abstract': 'This paper envisions a knowledge-guided multi-agent framework named KGMAF for\nautomated requirements development. KGMAF aims to address gaps in current\nautomation systems for SE, which prioritize code development and overlook the\ncomplexities of requirements tasks. KGMAF is composed of six specialized agents\nand an artifact pool to improve efficiency and accuracy. Specifically, KGMAF\noutlines the functionality, actions, and knowledge of each agent and provides\nthe conceptual design of the artifact pool. Our case study highlights the\npotential of KGMAF in real-world scenarios. Finally, we outline several\nresearch opportunities for implementing and enhancing automated requirements\ndevelopment using multi-agent systems. We believe that KGMAF will play a\npivotal role in shaping the future of automated requirements development in the\nera of LLMs.', 'submit_date': datetime.date(2025, 6, 27), 'metadata': "{'id': '2506.22656v1', 'title': 'Knowledge-Guided Multi-Agent Framework for Automated Requirements Development: A Vision', 'abstract': 'This paper envisions a knowledge-guided multi-agent framework named KGMAF for\\nautomated requirements development. KGMAF aims to address gaps in current\\nautomation systems for SE, which prioritize code development and overlook the\\ncomplexities of requirements tasks. KGMAF is composed of six specialized agents\\nand an artifact pool to improve efficiency and accuracy. Specifically, KGMAF\\noutlines the functionality, actions, and knowledge of each agent and provides\\nthe conceptual design of the artifact pool. Our case study highlights the\\npotential of KGMAF in real-world scenarios. Finally, we outline several\\nresearch opportunities for implementing and enhancing automated requirements\\ndevelopment using multi-agent systems. We believe that KGMAF will play a\\npivotal role in shaping the future of automated requirements development in the\\nera of LLMs.', 'authors': ['Jiangping Huang', 'Dongming Jin', 'Weisong Sun', 'Yang Liu', 'Zhi Jin'], 'published': '2025-06-27T21:57:53+00:00', 'categories': ['cs.SE', 'cs.AI', '68-04', 'D.2.3; I.2.7'], 'url': 'http://arxiv.org/abs/2506.22656v1'}"}, {'id': 184, 'arxiv_id': '2506.22653v1', 'base_arxiv_id': '2506.22653', 'version': '1', 'title': 'URSA: The Universal Research and Scientific Agent', 'abstract': 'Large language models (LLMs) have moved far beyond their initial form as\nsimple chatbots, now carrying out complex reasoning, planning, writing, coding,\nand research tasks. These skills overlap significantly with those that human\nscientists use day-to-day to solve complex problems that drive the cutting edge\nof research. Using LLMs in "agentic" AI has the potential to revolutionize\nmodern science and remove bottlenecks to progress. In this work, we present\nURSA, a scientific agent ecosystem for accelerating research tasks. URSA\nconsists of a set of modular agents and tools, including coupling to advanced\nphysics simulation codes, that can be combined to address scientific problems\nof varied complexity and impact. This work highlights the architecture of URSA,\nas well as examples that highlight the potential of the system.', 'submit_date': datetime.date(2025, 6, 27), 'metadata': '{\'id\': \'2506.22653v1\', \'title\': \'URSA: The Universal Research and Scientific Agent\', \'abstract\': \'Large language models (LLMs) have moved far beyond their initial form as\\nsimple chatbots, now carrying out complex reasoning, planning, writing, coding,\\nand research tasks. These skills overlap significantly with those that human\\nscientists use day-to-day to solve complex problems that drive the cutting edge\\nof research. Using LLMs in "agentic" AI has the potential to revolutionize\\nmodern science and remove bottlenecks to progress. In this work, we present\\nURSA, a scientific agent ecosystem for accelerating research tasks. URSA\\nconsists of a set of modular agents and tools, including coupling to advanced\\nphysics simulation codes, that can be combined to address scientific problems\\nof varied complexity and impact. This work highlights the architecture of URSA,\\nas well as examples that highlight the potential of the system.\', \'authors\': [\'Michael Grosskopf\', \'Russell Bent\', \'Rahul Somasundaram\', \'Isaac Michaud\', \'Arthur Lui\', \'Nathan Debardeleben\', \'Earl Lawrence\'], \'published\': \'2025-06-27T21:56:02+00:00\', \'categories\': [\'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.22653v1\'}'}, {'id': 185, 'arxiv_id': '2506.22638v1', 'base_arxiv_id': '2506.22638', 'version': '1', 'title': 'Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training', 'abstract': 'Large language models can exhibit improved mathematical reasoning\ncapabilities following post-training with instruction tuning, reinforcement\nlearning, or knowledge distillation. However, it remains unclear whether these\nimprovements are driven by major changes in transformer layers or from minor\nadjustments that leave the relative layer importance structures of the base\nmodel largely unchanged. We investigate this question through systematic\nlayer-wise ablation experiments, examining base, instruction-tuned,\nknowledge-distilled, and reinforcement learning variants on mathematical\nreasoning benchmarks. Our findings show that mathematical reasoning gives rise\nto a specific layer importance structure, and this structure persists across\nall post-training paradigms. Removal of such layers causes accuracy drops of up\nto 80%. In contrast, non-mathematical tasks like factual recall exhibit no\ncritical layers. This distinction suggests that mathematical reasoning requires\nspecialized layers that emerge during pre-training, while other non-reasoning\ntasks do not. From an information-theoretic perspective, we also observe that\nthese critical layers are the same layers where major representational\ntransformation occurs.', 'submit_date': datetime.date(2025, 6, 27), 'metadata': "{'id': '2506.22638v1', 'title': 'Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training', 'abstract': 'Large language models can exhibit improved mathematical reasoning\\ncapabilities following post-training with instruction tuning, reinforcement\\nlearning, or knowledge distillation. However, it remains unclear whether these\\nimprovements are driven by major changes in transformer layers or from minor\\nadjustments that leave the relative layer importance structures of the base\\nmodel largely unchanged. We investigate this question through systematic\\nlayer-wise ablation experiments, examining base, instruction-tuned,\\nknowledge-distilled, and reinforcement learning variants on mathematical\\nreasoning benchmarks. Our findings show that mathematical reasoning gives rise\\nto a specific layer importance structure, and this structure persists across\\nall post-training paradigms. Removal of such layers causes accuracy drops of up\\nto 80%. In contrast, non-mathematical tasks like factual recall exhibit no\\ncritical layers. This distinction suggests that mathematical reasoning requires\\nspecialized layers that emerge during pre-training, while other non-reasoning\\ntasks do not. From an information-theoretic perspective, we also observe that\\nthese critical layers are the same layers where major representational\\ntransformation occurs.', 'authors': ['Aadim Nepal', 'Safal Shrestha', 'Anubhav Shrestha', 'Minwu Kim', 'Keith Ross'], 'published': '2025-06-27T21:04:55+00:00', 'categories': ['cs.LG', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.22638v1'}"}, {'id': 186, 'arxiv_id': '2506.22623v1', 'base_arxiv_id': '2506.22623', 'version': '1', 'title': 'Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks', 'abstract': 'In the present-day scenario, Large Language Models (LLMs) are establishing\ntheir presence as powerful instruments permeating various sectors of society.\nWhile their utility offers valuable support to individuals, there are multiple\nconcerns over potential misuse. Consequently, some academic endeavors have\nsought to introduce watermarking techniques, characterized by the inclusion of\nmarkers within machine-generated text, to facilitate algorithmic\nidentification. This research project is focused on the development of a novel\nmethodology for the detection of synthetic text, with the overarching goal of\nensuring the ethical application of LLMs in AI-driven text generation. The\ninvestigation commences with replicating findings from a previous baseline\nstudy, thereby underscoring its susceptibility to variations in the underlying\ngeneration model. Subsequently, we propose an innovative watermarking approach\nand subject it to rigorous evaluation, employing paraphrased generated text to\nasses its robustness. Experimental results highlight the robustness of our\nproposal compared to the~aarson watermarking method.', 'submit_date': datetime.date(2025, 6, 27), 'metadata': "{'id': '2506.22623v1', 'title': 'Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks', 'abstract': 'In the present-day scenario, Large Language Models (LLMs) are establishing\\ntheir presence as powerful instruments permeating various sectors of society.\\nWhile their utility offers valuable support to individuals, there are multiple\\nconcerns over potential misuse. Consequently, some academic endeavors have\\nsought to introduce watermarking techniques, characterized by the inclusion of\\nmarkers within machine-generated text, to facilitate algorithmic\\nidentification. This research project is focused on the development of a novel\\nmethodology for the detection of synthetic text, with the overarching goal of\\nensuring the ethical application of LLMs in AI-driven text generation. The\\ninvestigation commences with replicating findings from a previous baseline\\nstudy, thereby underscoring its susceptibility to variations in the underlying\\ngeneration model. Subsequently, we propose an innovative watermarking approach\\nand subject it to rigorous evaluation, employing paraphrased generated text to\\nasses its robustness. Experimental results highlight the robustness of our\\nproposal compared to the~\\\\cite{aarson} watermarking method.', 'authors': ['Badr Youbi Idrissi', 'Monica Millunzi', 'Amelia Sorrenti', 'Lorenzo Baraldi', 'Daryna Dementieva'], 'published': '2025-06-27T20:39:35+00:00', 'categories': ['cs.CL', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.22623v1'}"}, {'id': 187, 'arxiv_id': '2506.22609v1', 'base_arxiv_id': '2506.22609', 'version': '1', 'title': 'Ludax: A GPU-Accelerated Domain Specific Language for Board Games', 'abstract': "Games have long been used as benchmarks and testing environments for research\nin artificial intelligence. A key step in supporting this research was the\ndevelopment of game description languages: frameworks that compile\ndomain-specific code into playable and simulatable game environments, allowing\nresearchers to generalize their algorithms and approaches across multiple games\nwithout having to manually implement each one. More recently, progress in\nreinforcement learning (RL) has been largely driven by advances in hardware\nacceleration. Libraries like JAX allow practitioners to take full advantage of\ncutting-edge computing hardware, often speeding up training and testing by\norders of magnitude. Here, we present a synthesis of these strands of research:\na domain-specific language for board games which automatically compiles into\nhardware-accelerated code. Our framework, Ludax, combines the generality of\ngame description languages with the speed of modern parallel processing\nhardware and is designed to fit neatly into existing deep learning pipelines.\nWe envision Ludax as a tool to help accelerate games research generally, from\nRL to cognitive science, by enabling rapid simulation and providing a flexible\nrepresentation scheme. We present a detailed breakdown of Ludax's description\nlanguage and technical notes on the compilation process, along with speed\nbenchmarking and a demonstration of training RL agents. The Ludax framework,\nalong with implementations of existing board games, is open-source and freely\navailable.", 'submit_date': datetime.date(2025, 6, 27), 'metadata': '{\'id\': \'2506.22609v1\', \'title\': \'Ludax: A GPU-Accelerated Domain Specific Language for Board Games\', \'abstract\': "Games have long been used as benchmarks and testing environments for research\\nin artificial intelligence. A key step in supporting this research was the\\ndevelopment of game description languages: frameworks that compile\\ndomain-specific code into playable and simulatable game environments, allowing\\nresearchers to generalize their algorithms and approaches across multiple games\\nwithout having to manually implement each one. More recently, progress in\\nreinforcement learning (RL) has been largely driven by advances in hardware\\nacceleration. Libraries like JAX allow practitioners to take full advantage of\\ncutting-edge computing hardware, often speeding up training and testing by\\norders of magnitude. Here, we present a synthesis of these strands of research:\\na domain-specific language for board games which automatically compiles into\\nhardware-accelerated code. Our framework, Ludax, combines the generality of\\ngame description languages with the speed of modern parallel processing\\nhardware and is designed to fit neatly into existing deep learning pipelines.\\nWe envision Ludax as a tool to help accelerate games research generally, from\\nRL to cognitive science, by enabling rapid simulation and providing a flexible\\nrepresentation scheme. We present a detailed breakdown of Ludax\'s description\\nlanguage and technical notes on the compilation process, along with speed\\nbenchmarking and a demonstration of training RL agents. The Ludax framework,\\nalong with implementations of existing board games, is open-source and freely\\navailable.", \'authors\': [\'Graham Todd\', \'Alexander G. Padula\', \'Dennis J. N. J. Soemers\', \'Julian Togelius\'], \'published\': \'2025-06-27T20:15:53+00:00\', \'categories\': [\'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.22609v1\'}'}, {'id': 188, 'arxiv_id': '2506.22604v1', 'base_arxiv_id': '2506.22604', 'version': '1', 'title': 'Bootstrapping Human-Like Planning via LLMs', 'abstract': "Robot end users increasingly require accessible means of specifying tasks for\nrobots to perform. Two common end-user programming paradigms include\ndrag-and-drop interfaces and natural language programming. Although natural\nlanguage interfaces harness an intuitive form of human communication,\ndrag-and-drop interfaces enable users to meticulously and precisely dictate the\nkey actions of the robot's task. In this paper, we investigate the degree to\nwhich both approaches can be combined. Specifically, we construct a large\nlanguage model (LLM)-based pipeline that accepts natural language as input and\nproduces human-like action sequences as output, specified at a level of\ngranularity that a human would produce. We then compare these generated action\nsequences to another dataset of hand-specified action sequences. Although our\nresults reveal that larger models tend to outperform smaller ones in the\nproduction of human-like action sequences, smaller models nonetheless achieve\nsatisfactory performance.", 'submit_date': datetime.date(2025, 6, 27), 'metadata': '{\'id\': \'2506.22604v1\', \'title\': \'Bootstrapping Human-Like Planning via LLMs\', \'abstract\': "Robot end users increasingly require accessible means of specifying tasks for\\nrobots to perform. Two common end-user programming paradigms include\\ndrag-and-drop interfaces and natural language programming. Although natural\\nlanguage interfaces harness an intuitive form of human communication,\\ndrag-and-drop interfaces enable users to meticulously and precisely dictate the\\nkey actions of the robot\'s task. In this paper, we investigate the degree to\\nwhich both approaches can be combined. Specifically, we construct a large\\nlanguage model (LLM)-based pipeline that accepts natural language as input and\\nproduces human-like action sequences as output, specified at a level of\\ngranularity that a human would produce. We then compare these generated action\\nsequences to another dataset of hand-specified action sequences. Although our\\nresults reveal that larger models tend to outperform smaller ones in the\\nproduction of human-like action sequences, smaller models nonetheless achieve\\nsatisfactory performance.", \'authors\': [\'David Porfirio\', \'Vincent Hsiao\', \'Morgan Fine-Morris\', \'Leslie Smith\', \'Laura M. Hiatt\'], \'published\': \'2025-06-27T20:00:51+00:00\', \'categories\': [\'cs.AI\', \'cs.HC\', \'cs.RO\'], \'url\': \'http://arxiv.org/abs/2506.22604v1\'}'}, {'id': 189, 'arxiv_id': '2506.22593v1', 'base_arxiv_id': '2506.22593', 'version': '1', 'title': 'Pixels-to-Graph: Real-time Integration of Building Information Models and Scene Graphs for Semantic-Geometric Human-Robot Understanding', 'abstract': 'Autonomous robots are increasingly playing key roles as support platforms for\nhuman operators in high-risk, dangerous applications. To accomplish challenging\ntasks, an efficient human-robot cooperation and understanding is required.\nWhile typically robotic planning leverages 3D geometric information, human\noperators are accustomed to a high-level compact representation of the\nenvironment, like top-down 2D maps representing the Building Information Model\n(BIM). 3D scene graphs have emerged as a powerful tool to bridge the gap\nbetween human readable 2D BIM and the robot 3D maps. In this work, we introduce\nPixels-to-Graph (Pix2G), a novel lightweight method to generate structured\nscene graphs from image pixels and LiDAR maps in real-time for the autonomous\nexploration of unknown environments on resource-constrained robot platforms. To\nsatisfy onboard compute constraints, the framework is designed to perform all\noperation on CPU only. The method output are a de-noised 2D top-down\nenvironment map and a structure-segmented 3D pointcloud which are seamlessly\nconnected using a multi-layer graph abstracting information from object-level\nup to the building-level. The proposed method is quantitatively and\nqualitatively evaluated during real-world experiments performed using the NASA\nJPL NeBula-Spot legged robot to autonomously explore and map cluttered garage\nand urban office like environments in real-time.', 'submit_date': datetime.date(2025, 6, 27), 'metadata': "{'id': '2506.22593v1', 'title': 'Pixels-to-Graph: Real-time Integration of Building Information Models and Scene Graphs for Semantic-Geometric Human-Robot Understanding', 'abstract': 'Autonomous robots are increasingly playing key roles as support platforms for\\nhuman operators in high-risk, dangerous applications. To accomplish challenging\\ntasks, an efficient human-robot cooperation and understanding is required.\\nWhile typically robotic planning leverages 3D geometric information, human\\noperators are accustomed to a high-level compact representation of the\\nenvironment, like top-down 2D maps representing the Building Information Model\\n(BIM). 3D scene graphs have emerged as a powerful tool to bridge the gap\\nbetween human readable 2D BIM and the robot 3D maps. In this work, we introduce\\nPixels-to-Graph (Pix2G), a novel lightweight method to generate structured\\nscene graphs from image pixels and LiDAR maps in real-time for the autonomous\\nexploration of unknown environments on resource-constrained robot platforms. To\\nsatisfy onboard compute constraints, the framework is designed to perform all\\noperation on CPU only. The method output are a de-noised 2D top-down\\nenvironment map and a structure-segmented 3D pointcloud which are seamlessly\\nconnected using a multi-layer graph abstracting information from object-level\\nup to the building-level. The proposed method is quantitatively and\\nqualitatively evaluated during real-world experiments performed using the NASA\\nJPL NeBula-Spot legged robot to autonomously explore and map cluttered garage\\nand urban office like environments in real-time.', 'authors': ['Antonello Longo', 'Chanyoung Chung', 'Matteo Palieri', 'Sung-Kyun Kim', 'Ali Agha', 'Cataldo Guaragnella', 'Shehryar Khattak'], 'published': '2025-06-27T19:23:31+00:00', 'categories': ['cs.RO', 'cs.AI', 'cs.CV'], 'url': 'http://arxiv.org/abs/2506.22593v1'}"}, {'id': 190, 'arxiv_id': '2506.22580v1', 'base_arxiv_id': '2506.22580', 'version': '1', 'title': 'FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for Federated Medical Image Segmentation', 'abstract': "Federated learning is a decentralized training approach that keeps data under\nstakeholder control while achieving superior performance over isolated\ntraining. While inter-institutional feature discrepancies pose a challenge in\nall federated settings, medical imaging is particularly affected due to diverse\nimaging devices and population variances, which can diminish the global model's\neffectiveness. Existing aggregation methods generally fail to adapt across\nvaried circumstances. To address this, we propose FedCLAM, which integrates\nclient-adaptive momentum terms derived from each client's loss\nreduction during local training, as well as a personalized dampening\nfactor to curb overfitting. We further introduce a novel intensity\nalignment loss that matches predicted and ground-truth foreground\ndistributions to handle heterogeneous image intensity profiles across\ninstitutions and devices. Extensive evaluations on two datasets show that\nFedCLAM surpasses eight cutting-edge methods in medical segmentation tasks,\nunderscoring its efficacy. The code is available at\nhttps://github.com/siomvas/FedCLAM.", 'submit_date': datetime.date(2025, 6, 27), 'metadata': '{\'id\': \'2506.22580v1\', \'title\': \'FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for Federated Medical Image Segmentation\', \'abstract\': "Federated learning is a decentralized training approach that keeps data under\\nstakeholder control while achieving superior performance over isolated\\ntraining. While inter-institutional feature discrepancies pose a challenge in\\nall federated settings, medical imaging is particularly affected due to diverse\\nimaging devices and population variances, which can diminish the global model\'s\\neffectiveness. Existing aggregation methods generally fail to adapt across\\nvaried circumstances. To address this, we propose FedCLAM, which integrates\\n\\\\textit{client-adaptive momentum} terms derived from each client\'s loss\\nreduction during local training, as well as a \\\\textit{personalized dampening\\nfactor} to curb overfitting. We further introduce a novel \\\\textit{intensity\\nalignment} loss that matches predicted and ground-truth foreground\\ndistributions to handle heterogeneous image intensity profiles across\\ninstitutions and devices. Extensive evaluations on two datasets show that\\nFedCLAM surpasses eight cutting-edge methods in medical segmentation tasks,\\nunderscoring its efficacy. The code is available at\\nhttps://github.com/siomvas/FedCLAM.", \'authors\': [\'Vasilis Siomos\', \'Jonathan Passerat-Palmbach\', \'Giacomo Tarroni\'], \'published\': \'2025-06-27T18:52:41+00:00\', \'categories\': [\'eess.IV\', \'cs.AI\', \'cs.CV\'], \'url\': \'http://arxiv.org/abs/2506.22580v1\'}'}, {'id': 191, 'arxiv_id': '2506.22578v1', 'base_arxiv_id': '2506.22578', 'version': '1', 'title': 'The Hidden Link Between RLHF and Contrastive Learning', 'abstract': 'Alignment of large language models (LLMs) with human values has recently\ngarnered significant attention, with prominent examples including the canonical\nyet costly Reinforcement Learning from Human Feedback (RLHF) and the simple\nDirect Preference Optimization (DPO). In this work, we demonstrate that both\nRLHF and DPO can be interpreted from the perspective of mutual information (MI)\nmaximization, uncovering a profound connection to contrastive learning. Within\nthis framework, both RLHF and DPO can be viewed as methods that perform\ncontrastive learning based on the positive and negative samples derived from\nthe base model, leveraging the Donsker-Varadhan (DV) lower bound on MI\n(equivalently, the MINE estimator). This paradigm further explains why RLHF may\nnot intrinsically incentivize reasoning capacities in LLMs beyond what is\nalready present in the base model. Building on this perspective, we replace the\nDV/MINE bound with the Jensen-Shannon MI estimator and propose Mutual\nInformation Optimization (MIO). Comprehensive theoretical analysis and\nextensive empirical evaluations demonstrate that MIO mitigates the late-stage\ndecline in chosen-likelihood observed in DPO, achieving competitive or superior\nperformance across various challenging reasoning and mathematical benchmarks.\nWe will release the model and code upon acceptance.', 'submit_date': datetime.date(2025, 6, 27), 'metadata': "{'id': '2506.22578v1', 'title': 'The Hidden Link Between RLHF and Contrastive Learning', 'abstract': 'Alignment of large language models (LLMs) with human values has recently\\ngarnered significant attention, with prominent examples including the canonical\\nyet costly Reinforcement Learning from Human Feedback (RLHF) and the simple\\nDirect Preference Optimization (DPO). In this work, we demonstrate that both\\nRLHF and DPO can be interpreted from the perspective of mutual information (MI)\\nmaximization, uncovering a profound connection to contrastive learning. Within\\nthis framework, both RLHF and DPO can be viewed as methods that perform\\ncontrastive learning based on the positive and negative samples derived from\\nthe base model, leveraging the Donsker-Varadhan (DV) lower bound on MI\\n(equivalently, the MINE estimator). This paradigm further explains why RLHF may\\nnot intrinsically incentivize reasoning capacities in LLMs beyond what is\\nalready present in the base model. Building on this perspective, we replace the\\nDV/MINE bound with the Jensen-Shannon MI estimator and propose Mutual\\nInformation Optimization (MIO). Comprehensive theoretical analysis and\\nextensive empirical evaluations demonstrate that MIO mitigates the late-stage\\ndecline in chosen-likelihood observed in DPO, achieving competitive or superior\\nperformance across various challenging reasoning and mathematical benchmarks.\\nWe will release the model and code upon acceptance.', 'authors': ['Xufei Lv', 'Haoyuan Sun', 'Xuefeng Bai', 'Min Zhang', 'Houde Liu', 'Kehai Chen'], 'published': '2025-06-27T18:51:25+00:00', 'categories': ['cs.LG', 'cs.AI', 'stat.ML'], 'url': 'http://arxiv.org/abs/2506.22578v1'}"}, {'id': 192, 'arxiv_id': '2506.22567v1', 'base_arxiv_id': '2506.22567', 'version': '1', 'title': 'Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation', 'abstract': 'CLIP models pretrained on natural images with billion-scale image-text pairs\nhave demonstrated impressive capabilities in zero-shot classification,\ncross-modal retrieval, and open-ended visual answering. However, transferring\nthis success to biomedicine is hindered by the scarcity of large-scale\nbiomedical image-text corpora, the heterogeneity of image modalities, and\nfragmented data standards across institutions. These limitations hinder the\ndevelopment of a unified and generalizable biomedical foundation model trained\nfrom scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical\nfoundation model developed via Multiple Medical CLIP Knowledge Distillation.\nRather than relying on billion-scale raw data, MMKD-CLIP distills knowledge\nfrom nine state-of-the-art domain-specific or generalist biomedical CLIP\nmodels, each pretrained on millions of biomedical image-text pairs. Our\ntwo-stage training pipeline first performs CLIP-style pretraining on over 2.9\nmillion biomedical image-text pairs from 26 image modalities, followed by\nfeature-level distillation using over 19.2 million feature pairs extracted from\nteacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets,\nencompassing over 10.8 million biomedical images across nine image modalities.\nThe evaluation spans six core task types: zero-shot classification, linear\nprobing, cross-modal retrieval, visual question answering, survival prediction,\nand cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models\nwhile demonstrating remarkable robustness and generalization across image\ndomains and task settings. These results underscore that multi-teacher\nknowledge distillation is a scalable and effective paradigm for building\nhigh-performing biomedical foundation models under the practical constraints of\nreal-world data availability.', 'submit_date': datetime.date(2025, 6, 27), 'metadata': "{'id': '2506.22567v1', 'title': 'Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation', 'abstract': 'CLIP models pretrained on natural images with billion-scale image-text pairs\\nhave demonstrated impressive capabilities in zero-shot classification,\\ncross-modal retrieval, and open-ended visual answering. However, transferring\\nthis success to biomedicine is hindered by the scarcity of large-scale\\nbiomedical image-text corpora, the heterogeneity of image modalities, and\\nfragmented data standards across institutions. These limitations hinder the\\ndevelopment of a unified and generalizable biomedical foundation model trained\\nfrom scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical\\nfoundation model developed via Multiple Medical CLIP Knowledge Distillation.\\nRather than relying on billion-scale raw data, MMKD-CLIP distills knowledge\\nfrom nine state-of-the-art domain-specific or generalist biomedical CLIP\\nmodels, each pretrained on millions of biomedical image-text pairs. Our\\ntwo-stage training pipeline first performs CLIP-style pretraining on over 2.9\\nmillion biomedical image-text pairs from 26 image modalities, followed by\\nfeature-level distillation using over 19.2 million feature pairs extracted from\\nteacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets,\\nencompassing over 10.8 million biomedical images across nine image modalities.\\nThe evaluation spans six core task types: zero-shot classification, linear\\nprobing, cross-modal retrieval, visual question answering, survival prediction,\\nand cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models\\nwhile demonstrating remarkable robustness and generalization across image\\ndomains and task settings. These results underscore that multi-teacher\\nknowledge distillation is a scalable and effective paradigm for building\\nhigh-performing biomedical foundation models under the practical constraints of\\nreal-world data availability.', 'authors': ['Shansong Wang', 'Zhecheng Jin', 'Mingzhe Hu', 'Mojtaba Safari', 'Feng Zhao', 'Chih-Wei Chang', 'Richard LJ Qiu', 'Justin Roper', 'David S. Yu', 'Xiaofeng Yang'], 'published': '2025-06-27T18:28:57+00:00', 'categories': ['cs.CV', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.22567v1'}"}, {'id': 193, 'arxiv_id': '2506.22566v1', 'base_arxiv_id': '2506.22566', 'version': '1', 'title': 'Exploration Behavior of Untrained Policies', 'abstract': 'Exploration remains a fundamental challenge in reinforcement learning (RL),\nparticularly in environments with sparse or adversarial reward structures. In\nthis work, we study how the architecture of deep neural policies implicitly\nshapes exploration before training. We theoretically and empirically\ndemonstrate strategies for generating ballistic or diffusive trajectories from\nuntrained policies in a toy model. Using the theory of infinite-width networks\nand a continuous-time limit, we show that untrained policies return correlated\nactions and result in non-trivial state-visitation distributions. We discuss\nthe distributions of the corresponding trajectories for a standard\narchitecture, revealing insights into inductive biases for tackling\nexploration. Our results establish a theoretical and experimental framework for\nusing policy initialization as a design tool to understand exploration behavior\nin early training.', 'submit_date': datetime.date(2025, 6, 27), 'metadata': "{'id': '2506.22566v1', 'title': 'Exploration Behavior of Untrained Policies', 'abstract': 'Exploration remains a fundamental challenge in reinforcement learning (RL),\\nparticularly in environments with sparse or adversarial reward structures. In\\nthis work, we study how the architecture of deep neural policies implicitly\\nshapes exploration before training. We theoretically and empirically\\ndemonstrate strategies for generating ballistic or diffusive trajectories from\\nuntrained policies in a toy model. Using the theory of infinite-width networks\\nand a continuous-time limit, we show that untrained policies return correlated\\nactions and result in non-trivial state-visitation distributions. We discuss\\nthe distributions of the corresponding trajectories for a standard\\narchitecture, revealing insights into inductive biases for tackling\\nexploration. Our results establish a theoretical and experimental framework for\\nusing policy initialization as a design tool to understand exploration behavior\\nin early training.', 'authors': ['Jacob Adamczyk'], 'published': '2025-06-27T18:28:41+00:00', 'categories': ['cs.LG', 'cs.AI', 'stat.ML'], 'url': 'http://arxiv.org/abs/2506.22566v1'}"}, {'id': 194, 'arxiv_id': '2506.22554v1', 'base_arxiv_id': '2506.22554', 'version': '1', 'title': 'Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset', 'abstract': 'Human communication involves a complex interplay of verbal and nonverbal\nsignals, essential for conveying meaning and achieving interpersonal goals. To\ndevelop socially intelligent AI technologies, it is crucial to develop models\nthat can both comprehend and generate dyadic behavioral dynamics. To this end,\nwe introduce the Seamless Interaction Dataset, a large-scale collection of over\n4,000 hours of face-to-face interaction footage from over 4,000 participants in\ndiverse contexts. This dataset enables the development of AI technologies that\nunderstand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,\ntelepresence experiences, and multimodal content analysis tools. We also\ndevelop a suite of models that utilize the dataset to generate dyadic motion\ngestures and facial expressions aligned with human speech. These models can\ntake as input both the speech and visual behavior of their interlocutors. We\npresent a variant with speech from an LLM model and integrations with 2D and 3D\nrendering methods, bringing us closer to interactive virtual agents.\nAdditionally, we describe controllable variants of our motion models that can\nadapt emotional responses and expressivity levels, as well as generating more\nsemantically-relevant gestures. Finally, we discuss methods for assessing the\nquality of these dyadic motion models, which are demonstrating the potential\nfor more intuitive and responsive human-AI interactions.', 'submit_date': datetime.date(2025, 6, 27), 'metadata': '{\'id\': \'2506.22554v1\', \'title\': \'Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset\', \'abstract\': \'Human communication involves a complex interplay of verbal and nonverbal\\nsignals, essential for conveying meaning and achieving interpersonal goals. To\\ndevelop socially intelligent AI technologies, it is crucial to develop models\\nthat can both comprehend and generate dyadic behavioral dynamics. To this end,\\nwe introduce the Seamless Interaction Dataset, a large-scale collection of over\\n4,000 hours of face-to-face interaction footage from over 4,000 participants in\\ndiverse contexts. This dataset enables the development of AI technologies that\\nunderstand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,\\ntelepresence experiences, and multimodal content analysis tools. We also\\ndevelop a suite of models that utilize the dataset to generate dyadic motion\\ngestures and facial expressions aligned with human speech. These models can\\ntake as input both the speech and visual behavior of their interlocutors. We\\npresent a variant with speech from an LLM model and integrations with 2D and 3D\\nrendering methods, bringing us closer to interactive virtual agents.\\nAdditionally, we describe controllable variants of our motion models that can\\nadapt emotional responses and expressivity levels, as well as generating more\\nsemantically-relevant gestures. Finally, we discuss methods for assessing the\\nquality of these dyadic motion models, which are demonstrating the potential\\nfor more intuitive and responsive human-AI interactions.\', \'authors\': [\'Vasu Agrawal\', \'Akinniyi Akinyemi\', \'Kathryn Alvero\', \'Morteza Behrooz\', \'Julia Buffalini\', \'Fabio Maria Carlucci\', \'Joy Chen\', \'Junming Chen\', \'Zhang Chen\', \'Shiyang Cheng\', \'Praveen Chowdary\', \'Joe Chuang\', "Antony D\'Avirro", \'Jon Daly\', \'Ning Dong\', \'Mark Duppenthaler\', \'Cynthia Gao\', \'Jeff Girard\', \'Martin Gleize\', \'Sahir Gomez\', \'Hongyu Gong\', \'Srivathsan Govindarajan\', \'Brandon Han\', \'Sen He\', \'Denise Hernandez\', \'Yordan Hristov\', \'Rongjie Huang\', \'Hirofumi Inaguma\', \'Somya Jain\', \'Raj Janardhan\', \'Qingyao Jia\', \'Christopher Klaiber\', \'Dejan Kovachev\', \'Moneish Kumar\', \'Hang Li\', \'Yilei Li\', \'Pavel Litvin\', \'Wei Liu\', \'Guangyao Ma\', \'Jing Ma\', \'Martin Ma\', \'Xutai Ma\', \'Lucas Mantovani\', \'Sagar Miglani\', \'Sreyas Mohan\', \'Louis-Philippe Morency\', \'Evonne Ng\', \'Kam-Woh Ng\', \'Tu Anh Nguyen\', \'Amia Oberai\', \'Benjamin Peloquin\', \'Juan Pino\', \'Jovan Popovic\', \'Omid Poursaeed\', \'Fabian Prada\', \'Alice Rakotoarison\', \'Alexander Richard\', \'Christophe Ropers\', \'Safiyyah Saleem\', \'Vasu Sharma\', \'Alex Shcherbyna\', \'Jia Shen\', \'Jie Shen\', \'Anastasis Stathopoulos\', \'Anna Sun\', \'Paden Tomasello\', \'Tuan Tran\', \'Arina Turkatenko\', \'Bo Wan\', \'Chao Wang\', \'Jeff Wang\', \'Mary Williamson\', \'Carleigh Wood\', \'Tao Xiang\', \'Yilin Yang\', \'Julien Yao\', \'Chen Zhang\', \'Jiemin Zhang\', \'Xinyue Zhang\', \'Jason Zheng\', \'Pavlo Zhyzheria\', \'Jan Zikes\', \'Michael Zollhoefer\'], \'published\': \'2025-06-27T18:09:49+00:00\', \'categories\': [\'cs.CV\', \'cs.AI\'], \'url\': \'http://arxiv.org/abs/2506.22554v1\'}'}, {'id': 195, 'arxiv_id': '2506.22427v1', 'base_arxiv_id': '2506.22427', 'version': '1', 'title': 'CLoVE: Personalized Federated Learning through Clustering of Loss Vector Embeddings', 'abstract': 'We propose CLoVE (Clustering of Loss Vector Embeddings), a novel algorithm\nfor Clustered Federated Learning (CFL). In CFL, clients are naturally grouped\ninto clusters based on their data distribution. However, identifying these\nclusters is challenging, as client assignments are unknown. CLoVE utilizes\nclient embeddings derived from model losses on client data, and leverages the\ninsight that clients in the same cluster share similar loss values, while those\nin different clusters exhibit distinct loss patterns. Based on these\nembeddings, CLoVE is able to iteratively identify and separate clients from\ndifferent clusters and optimize cluster-specific models through federated\naggregation. Key advantages of CLoVE over existing CFL algorithms are (1) its\nsimplicity, (2) its applicability to both supervised and unsupervised settings,\nand (3) the fact that it eliminates the need for near-optimal model\ninitialization, which makes it more robust and better suited for real-world\napplications. We establish theoretical convergence bounds, showing that CLoVE\ncan recover clusters accurately with high probability in a single round and\nconverges exponentially fast to optimal models in a linear setting. Our\ncomprehensive experiments comparing with a variety of both CFL and generic\nPersonalized Federated Learning (PFL) algorithms on different types of datasets\nand an extensive array of non-IID settings demonstrate that CLoVE achieves\nhighly accurate cluster recovery in just a few rounds of training, along with\nstate-of-the-art model accuracy, across a variety of both supervised and\nunsupervised PFL tasks.', 'submit_date': datetime.date(2025, 6, 27), 'metadata': "{'id': '2506.22427v1', 'title': 'CLoVE: Personalized Federated Learning through Clustering of Loss Vector Embeddings', 'abstract': 'We propose CLoVE (Clustering of Loss Vector Embeddings), a novel algorithm\\nfor Clustered Federated Learning (CFL). In CFL, clients are naturally grouped\\ninto clusters based on their data distribution. However, identifying these\\nclusters is challenging, as client assignments are unknown. CLoVE utilizes\\nclient embeddings derived from model losses on client data, and leverages the\\ninsight that clients in the same cluster share similar loss values, while those\\nin different clusters exhibit distinct loss patterns. Based on these\\nembeddings, CLoVE is able to iteratively identify and separate clients from\\ndifferent clusters and optimize cluster-specific models through federated\\naggregation. Key advantages of CLoVE over existing CFL algorithms are (1) its\\nsimplicity, (2) its applicability to both supervised and unsupervised settings,\\nand (3) the fact that it eliminates the need for near-optimal model\\ninitialization, which makes it more robust and better suited for real-world\\napplications. We establish theoretical convergence bounds, showing that CLoVE\\ncan recover clusters accurately with high probability in a single round and\\nconverges exponentially fast to optimal models in a linear setting. Our\\ncomprehensive experiments comparing with a variety of both CFL and generic\\nPersonalized Federated Learning (PFL) algorithms on different types of datasets\\nand an extensive array of non-IID settings demonstrate that CLoVE achieves\\nhighly accurate cluster recovery in just a few rounds of training, along with\\nstate-of-the-art model accuracy, across a variety of both supervised and\\nunsupervised PFL tasks.', 'authors': ['Randeep Bhatia', 'Nikos Papadis', 'Murali Kodialam', 'TV Lakshman', 'Sayak Chakrabarty'], 'published': '2025-06-27T17:52:16+00:00', 'categories': ['cs.LG', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.22427v1'}"}, {'id': 196, 'arxiv_id': '2506.22419v1', 'base_arxiv_id': '2506.22419', 'version': '1', 'title': 'The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements', 'abstract': 'Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.', 'submit_date': datetime.date(2025, 6, 27), 'metadata': "{'id': '2506.22419v1', 'title': 'The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements', 'abstract': 'Rapid advancements in large language models (LLMs) have the potential to\\nassist in scientific progress. A critical capability toward this endeavor is\\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\\nreproduce results in an active research area, we introduce the Automated LLM\\nSpeedrunning Benchmark, leveraging the research community contributions on the\\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\\nEach of the 19 speedrun tasks provides the agent with the previous records\\ntraining script, optionally paired with one of three hint formats, ranging from\\npseudocode to paper-like descriptions of the new records improvements. Records\\nexecute quickly by design and speedrun improvements encompass diverse\\ncode-level changes, ranging from high-level algorithmic advancements to\\nhardware-aware optimizations. These features make the benchmark both accessible\\nand realistic for the frontier problem of improving LLM training. We find that\\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\\nalready-known innovations in our benchmark, even when given detailed hints. Our\\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\\nautonomous research agent.', 'authors': ['Bingchen Zhao', 'Despoina Magka', 'Minqi Jiang', 'Xian Li', 'Roberta Raileanu', 'Tatiana Shavrina', 'Jean-Christophe Gagnon-Audet', 'Kelvin Niu', 'Shagun Sodhani', 'Michael Shvartsman', 'Andrei Lupu', 'Alisia Lupidi', 'Edan Toledo', 'Karen Hambardzumyan', 'Martin Josifoski', 'Thomas Foster', 'Lucia Cipolina-Kun', 'Abhishek Charnalia', 'Derek Dunfield', 'Alexander H. Miller', 'Oisin Mac Aodha', 'Jakob Foerster', 'Yoram Bachrach'], 'published': '2025-06-27T17:44:32+00:00', 'categories': ['cs.AI', 'cs.CL', 'cs.LG'], 'url': 'http://arxiv.org/abs/2506.22419v1'}"}, {'id': 197, 'arxiv_id': '2506.22403v1', 'base_arxiv_id': '2506.22403', 'version': '1', 'title': 'HyperCLOVA X THINK Technical Report', 'abstract': 'We introduce HyperCLOVA X THINK, the first reasoning-focused large language\nmodel in the HyperCLOVA X family, pre-trained on roughly $6$ trillion\nhigh-quality Korean, and English tokens, augmented with targeted synthetic\nKorean data. It was implemented as a compute-memory-balanced Peri-LN\nTransformer scaled with $$P, pre-trained through a three-stage curriculum\nthat expands the context window to $128$K tokens, and post-trained via\nsupervised fine-tuning with Reinforcement Learning from Verifiable Rewards\nsupports both detailed rationale and concise-answer modes. It delivers\ncompetitive performance against similarly sized models on Korea-focused\nbenchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while\npreserving robust bilingual consistency and translation quality. In addition, a\nvision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM\nbenchmark, all of which are achieved with substantially lower training compute\nthan existing models of similar sizes. We also present a pruning and\ndistillation technique that will soon be applied to HyperCLOVA X THINK for an\nopen-source and business-friendly foundation model. Altogether, these\ncapabilities position HyperCLOVA X THINK as a robust foundation for Korean AI\ninnovation and a valuable resource for the global research community.', 'submit_date': datetime.date(2025, 6, 27), 'metadata': "{'id': '2506.22403v1', 'title': 'HyperCLOVA X THINK Technical Report', 'abstract': 'We introduce HyperCLOVA X THINK, the first reasoning-focused large language\\nmodel in the HyperCLOVA X family, pre-trained on roughly $6$ trillion\\nhigh-quality Korean, and English tokens, augmented with targeted synthetic\\nKorean data. It was implemented as a compute-memory-balanced Peri-LN\\nTransformer scaled with $\\\\mu$P, pre-trained through a three-stage curriculum\\nthat expands the context window to $128$K tokens, and post-trained via\\nsupervised fine-tuning with Reinforcement Learning from Verifiable Rewards\\nsupports both detailed rationale and concise-answer modes. It delivers\\ncompetitive performance against similarly sized models on Korea-focused\\nbenchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while\\npreserving robust bilingual consistency and translation quality. In addition, a\\nvision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM\\nbenchmark, all of which are achieved with substantially lower training compute\\nthan existing models of similar sizes. We also present a pruning and\\ndistillation technique that will soon be applied to HyperCLOVA X THINK for an\\nopen-source and business-friendly foundation model. Altogether, these\\ncapabilities position HyperCLOVA X THINK as a robust foundation for Korean AI\\ninnovation and a valuable resource for the global research community.', 'authors': ['NAVER Cloud HyperCLOVA X Team'], 'published': '2025-06-27T17:23:12+00:00', 'categories': ['cs.CL', 'cs.AI'], 'url': 'http://arxiv.org/abs/2506.22403v1'}"}]}
