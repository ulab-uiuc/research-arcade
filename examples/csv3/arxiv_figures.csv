id,paper_arxiv_id,path,caption,label,name
2,1453.1644,path,Updated figure caption,\label{fig:explainer-intro},
3,1903.03894v4,figs/motivation-node-features.pdf,"\caption{\textbf{A.} \gnn computation graph $G_c$ (green and orange) for making prediction $\hat{y}$ at node $v$. Some edges in $G_c$ form important neural message-passing pathways (green), which allow useful node information to be propagated across $G_c$ and aggregated at $v$ for prediction, while other edges do not (orange). However, \gnn needs to aggregate important as well as unimportant messages  to form a prediction at node $v$, which can dilute the signal accumulated from $v$'s neighborhood.  The goal of \name is to identify a small set of important features and pathways (green) that are crucial for prediction. \textbf{B.} In addition to $G_S$ (green), \name identifies what feature dimensions of $G_S$'s nodes are important for prediction
    by learning a node feature mask.}",\label{fig:definition-node-features},
4,1903.03894v4,figs/including-node-features.pdf,"\caption{For $v$'s explanation $G_S$ (in green), \name identifies what feature dimensions of $G_S$'s nodes are essential for prediction at $v$
    by learning a node feature mask $M_T$.}",\label{fig:including-node-features},
5,1903.03894v4,figs/including-node-features.pdf,"\caption{For $v$'s explanation $G_S$ (in green), \name identifies what feature dimensions of $G_S$'s nodes are essential for prediction at $v$
    by learning a node feature mask $M_T$.}",\label{fig:including-node-features},
6,1903.03894v4,figs/fig3-node-cls-v3.pdf,\caption{Evaluation of single-instance explanations. \textbf{A-B.} Shown are exemplar explanation subgraphs for node classification task on four synthetic datasets. Each method provides explanation for the red node's prediction.},\label{fig:subgraph_node},
7,1903.03894v4,figs/fig3-graph-cls-v2.pdf,"\caption{Evaluation of single-instance explanations. \textbf{A-B.} Shown are exemplar explanation subgraphs for graph classification task on two datasets, \textsc{Mutag} and \textsc{Reddit-Binary}.}",\label{fig:subgraph_graph},
8,1903.03894v4,figs/feature_importance_v2.pdf,,,
9,1903.03894v4,figs/prototype,"\caption{\longname is able to provide a prototype for a given node class, which can help identify functional subgraphs, e.g. a mutagenic compound from the \textsc{Mutag} dataset.}",\label{fig:prototype},
10,1903.03894v4,figs/local_subgraph,,,
11,1903.03894v4,figs/grad_subgraph,,,
12,1903.03894v4,figs/local_subgraph,,,
13,1903.03894v4,figs/grad_subgraph,,,
14,1903.03894v4,figs/prototype,"\caption{\longname is able to provide a prototype for a given node class, which can help identify functional subgraphs, e.g. a mutagenic compound from the \textsc{Mutag} dataset.}",\label{fig:prototype},
15,1806.08804v4,figs/differentiable-pooling-V2.pdf,"\caption{High-level illustration of our proposed method \name. At each hierarchical layer, we run a GNN model to obtain embeddings of nodes. We then use these learned embeddings to cluster nodes together  and run another GNN layer on this coarsened graph. This whole process is repeated for $L$ layers and we use the final output representation to classify the graph. }",\label{fig:illustration},
16,1806.08804v4,figs/diffpool_vis.pdf,"\caption{Visualization of hierarchical cluster assignment over two \name\ layers, using an example graph from \textsc{Collab}.
      Nodes in the right-hand side plot correspond to cluster in the original graph (left-hand side). Colors are used to connect the nodes/clusters across the graphs, and dotted lines are used to indicate clusters.}",\label{fig:assignment_vis},
17,1806.08804v4,figs/diffpool_vis.pdf,\caption{},,
18,1806.08804v4,figs/vis_1_ell.png,\caption{},,
19,1806.08804v4,figs/vis_2_ell.png,\caption{},,
20,1806.08804v4,figs/diffpool_vis.pdf,\caption{},,
21,1806.08804v4,figs/vis_1_ell.png,\caption{},,
22,1806.08804v4,figs/vis_2_ell.png,\caption{},,
23,1706.03762v7,/figures/transformer_architecture.png,The Transformer model architecture. The left side shows the encoder stack and the right side shows the decoder stack.,fig:architecture,Figure 1
24,1706.03762v7,/figures/scaled_dot_product_attention.png,Scaled Dot-Product Attention and Multi-Head Attention mechanisms.,fig:attention,Figure 2
25,1706.03762v7,/figures/positional_encoding.png,Positional encoding visualization showing sine and cosine functions of different frequencies.,fig:positional,Figure 3
26,1903.03894v4,figs/explainer-motivation.pdf,"\caption{\gnn computation graph $G_c$ for making a prediction $\hat{y}$ at node $v$. Some edges form important message-passing pathways (green) while others do not (orange). \gnn model aggregates informative as well as non-informative messages to form a prediction at node $v$. The goal of \name is to identify a small set of important features and pathways (green) that are crucial for prediction.
    }",\label{fig:explainer-motivation},
27,1903.03894v4,figs/explainer-introduction_v2.pdf,"\caption{\name provides interpretable explanations for predictions made by any GNN model on any graph-based machine learning task. Shown is a hypothetical node classification task where a GNN model $\Phi$ is trained on a social interaction graph to predict future sport activities. Given a trained GNN $\Phi$ and a prediction $\hat{y}_i$ = ``Basketball'' for person $v_i$, \name generates an explanation by identifying a small subgraph of the input graph together with a small subset of node features (shown on the right) that are most influential for $\hat{y}_i$. Examining explanation for $\hat{y}_i$, we see that many friends in one part of $v_i$'s social circle enjoy ball games, and so the GNN predicts that $v_i$ will like basketball. Similarly, examining explanation for $\hat{y}_j$, we see that $v_j$'s friends and friends of his friends enjoy water and beach sports, and so the GNN predicts $\hat{y}_j$ = ``Sailing.''
 }",\label{fig:explainer-intro},
28,1903.03894v4,figs/motivation-node-features.pdf,"\caption{\textbf{A.} \gnn computation graph $G_c$ (green and orange) for making prediction $\hat{y}$ at node $v$. Some edges in $G_c$ form important neural message-passing pathways (green), which allow useful node information to be propagated across $G_c$ and aggregated at $v$ for prediction, while other edges do not (orange). However, \gnn needs to aggregate important as well as unimportant messages  to form a prediction at node $v$, which can dilute the signal accumulated from $v$'s neighborhood.  The goal of \name is to identify a small set of important features and pathways (green) that are crucial for prediction. \textbf{B.} In addition to $G_S$ (green), \name identifies what feature dimensions of $G_S$'s nodes are important for prediction
    by learning a node feature mask.}",\label{fig:definition-node-features},
29,1903.03894v4,figs/including-node-features.pdf,"\caption{For $v$'s explanation $G_S$ (in green), \name identifies what feature dimensions of $G_S$'s nodes are essential for prediction at $v$
    by learning a node feature mask $M_T$.}",\label{fig:including-node-features},
30,1903.03894v4,figs/including-node-features.pdf,"\caption{For $v$'s explanation $G_S$ (in green), \name identifies what feature dimensions of $G_S$'s nodes are essential for prediction at $v$
    by learning a node feature mask $M_T$.}",\label{fig:including-node-features},
31,1903.03894v4,figs/fig3-node-cls-v3.pdf,\caption{Evaluation of single-instance explanations. \textbf{A-B.} Shown are exemplar explanation subgraphs for node classification task on four synthetic datasets. Each method provides explanation for the red node's prediction.},\label{fig:subgraph_node},
32,1903.03894v4,figs/fig3-graph-cls-v2.pdf,"\caption{Evaluation of single-instance explanations. \textbf{A-B.} Shown are exemplar explanation subgraphs for graph classification task on two datasets, \textsc{Mutag} and \textsc{Reddit-Binary}.}",\label{fig:subgraph_graph},
33,1903.03894v4,figs/feature_importance_v2.pdf,,,
34,1903.03894v4,figs/prototype,"\caption{\longname is able to provide a prototype for a given node class, which can help identify functional subgraphs, e.g. a mutagenic compound from the \textsc{Mutag} dataset.}",\label{fig:prototype},
35,1903.03894v4,figs/local_subgraph,,,
36,1903.03894v4,figs/grad_subgraph,,,
37,1903.03894v4,figs/local_subgraph,,,
38,1903.03894v4,figs/grad_subgraph,,,
39,1903.03894v4,figs/prototype,"\caption{\longname is able to provide a prototype for a given node class, which can help identify functional subgraphs, e.g. a mutagenic compound from the \textsc{Mutag} dataset.}",\label{fig:prototype},
40,1806.08804v4,figs/differentiable-pooling-V2.pdf,"\caption{High-level illustration of our proposed method \name. At each hierarchical layer, we run a GNN model to obtain embeddings of nodes. We then use these learned embeddings to cluster nodes together  and run another GNN layer on this coarsened graph. This whole process is repeated for $L$ layers and we use the final output representation to classify the graph. }",\label{fig:illustration},
41,1806.08804v4,figs/diffpool_vis.pdf,"\caption{Visualization of hierarchical cluster assignment over two \name\ layers, using an example graph from \textsc{Collab}.
      Nodes in the right-hand side plot correspond to cluster in the original graph (left-hand side). Colors are used to connect the nodes/clusters across the graphs, and dotted lines are used to indicate clusters.}",\label{fig:assignment_vis},
42,1806.08804v4,figs/diffpool_vis.pdf,\caption{},,
43,1806.08804v4,figs/vis_1_ell.png,\caption{},,
44,1806.08804v4,figs/vis_2_ell.png,\caption{},,
45,1806.08804v4,figs/diffpool_vis.pdf,\caption{},,
46,1806.08804v4,figs/vis_1_ell.png,\caption{},,
47,1806.08804v4,figs/vis_2_ell.png,\caption{},,
48,1903.03894v4,figs/explainer-motivation.pdf,"\caption{\gnn computation graph $G_c$ for making a prediction $\hat{y}$ at node $v$. Some edges form important message-passing pathways (green) while others do not (orange). \gnn model aggregates informative as well as non-informative messages to form a prediction at node $v$. The goal of \name is to identify a small set of important features and pathways (green) that are crucial for prediction.
    }",\label{fig:explainer-motivation},
49,1903.03894v4,figs/explainer-introduction_v2.pdf,"\caption{\name provides interpretable explanations for predictions made by any GNN model on any graph-based machine learning task. Shown is a hypothetical node classification task where a GNN model $\Phi$ is trained on a social interaction graph to predict future sport activities. Given a trained GNN $\Phi$ and a prediction $\hat{y}_i$ = ``Basketball'' for person $v_i$, \name generates an explanation by identifying a small subgraph of the input graph together with a small subset of node features (shown on the right) that are most influential for $\hat{y}_i$. Examining explanation for $\hat{y}_i$, we see that many friends in one part of $v_i$'s social circle enjoy ball games, and so the GNN predicts that $v_i$ will like basketball. Similarly, examining explanation for $\hat{y}_j$, we see that $v_j$'s friends and friends of his friends enjoy water and beach sports, and so the GNN predicts $\hat{y}_j$ = ``Sailing.''
 }",\label{fig:explainer-intro},
50,1903.03894v4,figs/motivation-node-features.pdf,"\caption{\textbf{A.} \gnn computation graph $G_c$ (green and orange) for making prediction $\hat{y}$ at node $v$. Some edges in $G_c$ form important neural message-passing pathways (green), which allow useful node information to be propagated across $G_c$ and aggregated at $v$ for prediction, while other edges do not (orange). However, \gnn needs to aggregate important as well as unimportant messages  to form a prediction at node $v$, which can dilute the signal accumulated from $v$'s neighborhood.  The goal of \name is to identify a small set of important features and pathways (green) that are crucial for prediction. \textbf{B.} In addition to $G_S$ (green), \name identifies what feature dimensions of $G_S$'s nodes are important for prediction
    by learning a node feature mask.}",\label{fig:definition-node-features},
51,1903.03894v4,figs/including-node-features.pdf,"\caption{For $v$'s explanation $G_S$ (in green), \name identifies what feature dimensions of $G_S$'s nodes are essential for prediction at $v$
    by learning a node feature mask $M_T$.}",\label{fig:including-node-features},
52,1903.03894v4,figs/including-node-features.pdf,"\caption{For $v$'s explanation $G_S$ (in green), \name identifies what feature dimensions of $G_S$'s nodes are essential for prediction at $v$
    by learning a node feature mask $M_T$.}",\label{fig:including-node-features},
53,1903.03894v4,figs/fig3-node-cls-v3.pdf,\caption{Evaluation of single-instance explanations. \textbf{A-B.} Shown are exemplar explanation subgraphs for node classification task on four synthetic datasets. Each method provides explanation for the red node's prediction.},\label{fig:subgraph_node},
54,1903.03894v4,figs/fig3-graph-cls-v2.pdf,"\caption{Evaluation of single-instance explanations. \textbf{A-B.} Shown are exemplar explanation subgraphs for graph classification task on two datasets, \textsc{Mutag} and \textsc{Reddit-Binary}.}",\label{fig:subgraph_graph},
55,1903.03894v4,figs/feature_importance_v2.pdf,,,
56,1903.03894v4,figs/prototype,"\caption{\longname is able to provide a prototype for a given node class, which can help identify functional subgraphs, e.g. a mutagenic compound from the \textsc{Mutag} dataset.}",\label{fig:prototype},
57,1903.03894v4,figs/local_subgraph,,,
58,1903.03894v4,figs/grad_subgraph,,,
59,1903.03894v4,figs/local_subgraph,,,
60,1903.03894v4,figs/grad_subgraph,,,
61,1903.03894v4,figs/prototype,"\caption{\longname is able to provide a prototype for a given node class, which can help identify functional subgraphs, e.g. a mutagenic compound from the \textsc{Mutag} dataset.}",\label{fig:prototype},
62,1806.08804v4,figs/differentiable-pooling-V2.pdf,"\caption{High-level illustration of our proposed method \name. At each hierarchical layer, we run a GNN model to obtain embeddings of nodes. We then use these learned embeddings to cluster nodes together  and run another GNN layer on this coarsened graph. This whole process is repeated for $L$ layers and we use the final output representation to classify the graph. }",\label{fig:illustration},
63,1806.08804v4,figs/diffpool_vis.pdf,"\caption{Visualization of hierarchical cluster assignment over two \name\ layers, using an example graph from \textsc{Collab}.
      Nodes in the right-hand side plot correspond to cluster in the original graph (left-hand side). Colors are used to connect the nodes/clusters across the graphs, and dotted lines are used to indicate clusters.}",\label{fig:assignment_vis},
64,1806.08804v4,figs/diffpool_vis.pdf,\caption{},,
65,1806.08804v4,figs/vis_1_ell.png,\caption{},,
66,1806.08804v4,figs/vis_2_ell.png,\caption{},,
67,1806.08804v4,figs/diffpool_vis.pdf,\caption{},,
68,1806.08804v4,figs/vis_1_ell.png,\caption{},,
69,1806.08804v4,figs/vis_2_ell.png,\caption{},,
