[
  {
    "venue": "ICLR.cc/2025/Conference",
    "review_openreview_id": "Is5Qh2Gs5x",
    "replyto_openreview_id": "zzR1Uskhj0",
    "writer": "{\"Rating\": 6, \"Summary\": \"The submission studies contextual bandits with cross learning. Previously, the existing regret bound held in expectation. The submission refines the regret analysis so that the regret bound holds with high probability. The main contribution is to show how the weak dependency structure can be exploited to solve a concentration difficulty in the previous analysis.\", \"Questions\": \"- (1) Is the concept class $C$ a finite set? If so, what is the reason for assuming a finite concept class $C$? Practically speaking, the contextual information would be like a vector in a compact set, as it is very unlikely to see two identical users.- (2) Where is the variable in Theorem 1 that characterizes the property of $C$? How does this variable appear in the bound proved in this submission?- (3) Could you please provide more evidence or further discussion of the applicability of the technique developed here so that we can better appreciate its potential?\", \"Soundness\": 3, \"Strengths\": \"- The submission points out the difficulty that prevents the previous work from achieving a bound with high probability (lines 167\\u2013176).- Identify the weak dependency between epochs (line 386).- Devise a new technique to solve the unbounded issue induced by the weak dependency (the treatment for the Bias5e them in lines 395\\u2013411).\", \"Confidence\": 4, \"Weaknesses\": \"- (1) Section 3.2 contains several subtopics, such as the regret decomposition, the discussion of each decomposed term, and the analysis strategy for the challenging term. A better editorial layout would improve the readability.- (2) The sentence \\u201cNotably, \\u2026\\u201d in line 111 is confusing. It seems unrealistic to be able to observe the loss for every context $c$. It also does not match the algorithm\\u2019s (Algorithm 1) behavior.\", \"Contribution\": 3, \"Presentation\": 3}",
    "title": "Official Review by Reviewer_8LjP",
    "content": "{'Rating': 6, 'Summary': 'The submission studies contextual bandits with cross learning. Previously, the existing regret bound held in expectation. The submission refines the regret analysis so that the regret bound holds with high probability. The main contribution is to show how the weak dependency structure can be exploited to solve a concentration difficulty in the previous analysis.', 'Questions': '- (1) Is the concept class $C$ a finite set? If so, what is the reason for assuming a finite concept class $C$? Practically speaking, the contextual information would be like a vector in a compact set, as it is very unlikely to see two identical users.- (2) Where is the variable in Theorem 1 that characterizes the property of $C$? How does this variable appear in the bound proved in this submission?- (3) Could you please provide more evidence or further discussion of the applicability of the technique developed here so that we can better appreciate its potential?', 'Soundness': 3, 'Strengths': '- The submission points out the difficulty that prevents the previous work from achieving a bound with high probability (lines 167–176).- Identify the weak dependency between epochs (line 386).- Devise a new technique to solve the unbounded issue induced by the weak dependency (the treatment for the Bias5e them in lines 395–411).', 'Confidence': 4, 'Weaknesses': '- (1) Section 3.2 contains several subtopics, such as the regret decomposition, the discussion of each decomposed term, and the analysis strategy for the challenging term. A better editorial layout would improve the readability.- (2) The sentence “Notably, …” in line 111 is confusing. It seems unrealistic to be able to observe the loss for every context $c$. It also does not match the algorithm’s (Algorithm 1) behavior.', 'Contribution': 3, 'Presentation': 3}",
    "time": "2024-11-04 01:05:41"
  },
  {
    "venue": "ICLR.cc/2025/Conference",
    "review_openreview_id": "Hsumvt7DeH",
    "replyto_openreview_id": "zzR1Uskhj0",
    "writer": "{\"Rating\": 6, \"Summary\": \"The paper studied adversarial context bandits in a special setting where the losses of arm $a_i$ could be observed under all contexts when the algorithm plays arm $a_i$. The goal, like in classical adversarial bandit problems, is to minimize the regret compared to the loss of the best arm in hindsight. The paper focuses on the setting where the loss sequence is adversarial and the context is stochastic with an unknown distribution. A recent work of SZ [NeurIPS\\u201923] designed an algorithm with *expected* regret of $\\\\tilde{O}(\\\\sqrt{KT})$ in this setting, where $K$ is the number of arms. This paper conducted a renewed analysis of the algorithm in SZ  [NeurIPS\\u201923], and the main result is that the algorithm could actually achieve $\\\\tilde{O}(\\\\sqrt{KT})$ with high probability.The main technique of the paper is heavily influenced by the previous work of SZ [NeurIPS\\u201923]. In a nutshell, the low-regret guarantee of the algorithm crucially relies on the concentration of unbiased estimation of $E_{c}[\\\\ell_{t,c}(a)]$. Here, we cannot exactly compute the quantity since the distribution of the context is unknown. The key idea of SZ [NeurIPS\\u201923] is to commit two steps for each EXP3 step and use one of them to estimate the distribution of the context. On top of that, this paper further utilized the weak dependency between epochs, and derived a martingale argument to get high-probability regret.\", \"Questions\": \"- Should the definition of regret on page 3 be reversed? As in, you are subtracting the loss of the best arm with the loss of a policy, which should be a negative value (if with positive regret). This would change the decomposition of the regret on page 5 as well, but it seems nothing would affect the correctness.- I think the full algorithm description of the algorithm in SZ [NeurIPS\\u201923] (or some simpler version of the description) could be shown much earlier in the paper. This would be helpful for readers who are not familiar with the previous algorithm.- Also, stating the main theorem in a preliminary section looks very non-standard to me. I\\u2019m not letting this affect my score, but please consider re-organizing this.- The meaning of 'with high probability' was never explained in the paper -- as in, it could mean with probability $1-1/K$ or with probability $0.99$. I think your bound gives the former, and this should be stated explicitly.\", \"Soundness\": 4, \"Strengths\": \"In general, my opinion of this paper is positive. The paper appears to require a great deal of background to be able to parse. Despite this, I believe the paper did reasonably well in terms of explaining the existing work and its techniques. Getting high probability bounds in adversarial bandits usually requires some neat observations and technical steps. Although I\\u2019m not able to follow all the steps in the short time frame, I do think the paper contains some nice technical observations and ideas.\", \"Confidence\": 4, \"Weaknesses\": \"Although I'm mostly supportive, I think I couldn\\u2019t strongly champion the paper due to the following reasons:- The scope of contribution: although the paper does contain some neat technical observations, the contribution appears to be somehow incremental. After all, this is a new analysis of an existing algorithm, and the new analysis is not something that improves the previous bound (but instead is to get a high-probability bound). Again, I do acknowledge that such contributions are non-trivial. However, I do not think it\\u2019s enough for me to champion the paper.- If the paper is going to be mainly accepted due to the techniques: then, I do not think the paper contains a substantial amount of new ideas. I appreciate the technical observations, and I agree that the steps are non-trivial. However, if the conceptual message is not as strong, and the merits of the paper mainly lie in the techniques, then the bar would inevitably be higher. - For a conference like ICLR, the lack of experiments could be an issue. I am *not* letting this affect my score since I often advocate learning theory papers. However, I do want to raise this point since it is common for ML conferences to ask for experiments.\", \"Contribution\": 3, \"Presentation\": 3}",
    "title": "Official Review by Reviewer_Sstz",
    "content": "{'Rating': 6, 'Summary': 'The paper studied adversarial context bandits in a special setting where the losses of arm $a_i$ could be observed under all contexts when the algorithm plays arm $a_i$. The goal, like in classical adversarial bandit problems, is to minimize the regret compared to the loss of the best arm in hindsight. The paper focuses on the setting where the loss sequence is adversarial and the context is stochastic with an unknown distribution. A recent work of SZ [NeurIPS’23] designed an algorithm with *expected* regret of $\\\\tilde{O}(\\\\sqrt{KT})$ in this setting, where $K$ is the number of arms. This paper conducted a renewed analysis of the algorithm in SZ  [NeurIPS’23], and the main result is that the algorithm could actually achieve $\\\\tilde{O}(\\\\sqrt{KT})$ with high probability.The main technique of the paper is heavily influenced by the previous work of SZ [NeurIPS’23]. In a nutshell, the low-regret guarantee of the algorithm crucially relies on the concentration of unbiased estimation of $E_{c}[\\\\ell_{t,c}(a)]$. Here, we cannot exactly compute the quantity since the distribution of the context is unknown. The key idea of SZ [NeurIPS’23] is to commit two steps for each EXP3 step and use one of them to estimate the distribution of the context. On top of that, this paper further utilized the weak dependency between epochs, and derived a martingale argument to get high-probability regret.', 'Questions': \"- Should the definition of regret on page 3 be reversed? As in, you are subtracting the loss of the best arm with the loss of a policy, which should be a negative value (if with positive regret). This would change the decomposition of the regret on page 5 as well, but it seems nothing would affect the correctness.- I think the full algorithm description of the algorithm in SZ [NeurIPS’23] (or some simpler version of the description) could be shown much earlier in the paper. This would be helpful for readers who are not familiar with the previous algorithm.- Also, stating the main theorem in a preliminary section looks very non-standard to me. I’m not letting this affect my score, but please consider re-organizing this.- The meaning of 'with high probability' was never explained in the paper -- as in, it could mean with probability $1-1/K$ or with probability $0.99$. I think your bound gives the former, and this should be stated explicitly.\", 'Soundness': 4, 'Strengths': 'In general, my opinion of this paper is positive. The paper appears to require a great deal of background to be able to parse. Despite this, I believe the paper did reasonably well in terms of explaining the existing work and its techniques. Getting high probability bounds in adversarial bandits usually requires some neat observations and technical steps. Although I’m not able to follow all the steps in the short time frame, I do think the paper contains some nice technical observations and ideas.', 'Confidence': 4, 'Weaknesses': \"Although I'm mostly supportive, I think I couldn’t strongly champion the paper due to the following reasons:- The scope of contribution: although the paper does contain some neat technical observations, the contribution appears to be somehow incremental. After all, this is a new analysis of an existing algorithm, and the new analysis is not something that improves the previous bound (but instead is to get a high-probability bound). Again, I do acknowledge that such contributions are non-trivial. However, I do not think it’s enough for me to champion the paper.- If the paper is going to be mainly accepted due to the techniques: then, I do not think the paper contains a substantial amount of new ideas. I appreciate the technical observations, and I agree that the steps are non-trivial. However, if the conceptual message is not as strong, and the merits of the paper mainly lie in the techniques, then the bar would inevitably be higher. - For a conference like ICLR, the lack of experiments could be an issue. I am *not* letting this affect my score since I often advocate learning theory papers. However, I do want to raise this point since it is common for ML conferences to ask for experiments.\", 'Contribution': 3, 'Presentation': 3}",
    "time": "2024-11-07 22:01:20"
  },
  {
    "venue": "ICLR.cc/2025/Conference",
    "review_openreview_id": "smWIsNwjkv",
    "replyto_openreview_id": "zzR1Uskhj0",
    "writer": "{\"Rating\": 8, \"Summary\": \"The paper studies cross learning in contextual adversarial linear bandits where the learner observes the losses of all contexts in each round. Recent work in Schneider et al. proposed an algorithm with a regret upper bound only in expectation. The paper studies the same algorithm and proves that the regret upper bound holds with high probability.\", \"Questions\": \"please see weaknesses\", \"Soundness\": 4, \"Strengths\": \"- The paper proves a high probability lower bound which is stronger than the in expectation bound in the literature.- The analysis uses a nice observation that the different epochs in the algorithm are only weakly dependent which enables to prove a small bound for the cumulative bias across all epochs- While standard martingale inequalities cannot directly upper bound the cumulative bias, a novel technique is proposed to address this\", \"Confidence\": 3, \"Weaknesses\": \"Can the reduction in [1] be used to map the multi-context to a single context problem? The technique is proposed for non-adversarial losses, however, the action set map from distributional to fixed should not be affected by that.I understand that the paper only focuses on analyzing an existing algorithm. However, a comparison with such technique in the related work is needed to justify the use of such algorithm or suggest alternative techniques to address the problem.[1] \\\"Contexts can be cheap: Solving stochastic contextual bandits with linear bandit algorithms.\\\" The Thirty Sixth Annual Conference on Learning Theory. PMLR, 2023.\", \"Contribution\": 3, \"Presentation\": 3}",
    "title": "Official Review by Reviewer_uDyZ",
    "content": "{'Rating': 8, 'Summary': 'The paper studies cross learning in contextual adversarial linear bandits where the learner observes the losses of all contexts in each round. Recent work in Schneider et al. proposed an algorithm with a regret upper bound only in expectation. The paper studies the same algorithm and proves that the regret upper bound holds with high probability.', 'Questions': 'please see weaknesses', 'Soundness': 4, 'Strengths': '- The paper proves a high probability lower bound which is stronger than the in expectation bound in the literature.- The analysis uses a nice observation that the different epochs in the algorithm are only weakly dependent which enables to prove a small bound for the cumulative bias across all epochs- While standard martingale inequalities cannot directly upper bound the cumulative bias, a novel technique is proposed to address this', 'Confidence': 3, 'Weaknesses': 'Can the reduction in [1] be used to map the multi-context to a single context problem? The technique is proposed for non-adversarial losses, however, the action set map from distributional to fixed should not be affected by that.I understand that the paper only focuses on analyzing an existing algorithm. However, a comparison with such technique in the related work is needed to justify the use of such algorithm or suggest alternative techniques to address the problem.[1] \"Contexts can be cheap: Solving stochastic contextual bandits with linear bandit algorithms.\" The Thirty Sixth Annual Conference on Learning Theory. PMLR, 2023.', 'Contribution': 3, 'Presentation': 3}",
    "time": "2024-11-08 08:31:50"
  },
  {
    "venue": "ICLR.cc/2025/Conference",
    "review_openreview_id": "UfYuXDF8OO",
    "replyto_openreview_id": "zzR1Uskhj0",
    "writer": "{\"Rating\": 5, \"Summary\": \"This paper addresses the challenge of achieving high-probability regret bounds in the adversarial contextual bandit framework, where the learner encounters varying contexts and must minimize cumulative loss over time. The focus is on \\\"cross-learning\\\" contextual bandits, where learners can observe losses for all possible contexts, not just the current one. Results leverage weak dependencies between epochs and refine existing martingale inequalities, by exploiting interdependencies in observations. This analysis ultimately shows that the algorithm is effective in adversarial settings, even with unknown context distributions.\", \"Questions\": \"While the result is good, I am uncertain about its significance because neither the problem nor the algorithm proposed are new. There are no extensions of this result, no experiments. I am not sure if this is standalone result is significant enough to be published at a premier conference.\", \"Soundness\": 3, \"Strengths\": \"The paper proposes a new look at an existing problem and provides a completely novel analysis in their work. The ideas and techniques proposed are completely new and can be of independent interest. This is particularly true for the martingale concentration result.\", \"Confidence\": 3, \"Weaknesses\": \"See questions.\", \"Contribution\": 3, \"Presentation\": 3}",
    "title": "Official Review by Reviewer_NZtQ",
    "content": "{'Rating': 5, 'Summary': 'This paper addresses the challenge of achieving high-probability regret bounds in the adversarial contextual bandit framework, where the learner encounters varying contexts and must minimize cumulative loss over time. The focus is on \"cross-learning\" contextual bandits, where learners can observe losses for all possible contexts, not just the current one. Results leverage weak dependencies between epochs and refine existing martingale inequalities, by exploiting interdependencies in observations. This analysis ultimately shows that the algorithm is effective in adversarial settings, even with unknown context distributions.', 'Questions': 'While the result is good, I am uncertain about its significance because neither the problem nor the algorithm proposed are new. There are no extensions of this result, no experiments. I am not sure if this is standalone result is significant enough to be published at a premier conference.', 'Soundness': 3, 'Strengths': 'The paper proposes a new look at an existing problem and provides a completely novel analysis in their work. The ideas and techniques proposed are completely new and can be of independent interest. This is particularly true for the martingale concentration result.', 'Confidence': 3, 'Weaknesses': 'See questions.', 'Contribution': 3, 'Presentation': 3}",
    "time": "2024-11-08 09:01:46"
  },
  {
    "venue": "ICLR.cc/2025/Conference",
    "review_openreview_id": "NLrlOlSugS",
    "replyto_openreview_id": "zzR1Uskhj0",
    "writer": "{\"Rating\": 5, \"Summary\": \"The paper proposes an algorithm that achieves high probability regret bound (which is stronger than the expected regret bound) for the cross-learning contextual bandits under unknown context distribution by developing refined martingale inequalities.\", \"Questions\": \"1. What is the intuition behind the algorithm?2. How the indicator function $F_e$ resolves the unbounded martingale inequalties?\", \"Soundness\": 2, \"Strengths\": \"1. The paper clearly presents the challenging point with detailed technical expressions and the novelty of the analysis.\", \"Confidence\": 3, \"Weaknesses\": \"1. The explanation is only focused on technical side without the explanation of the algorithm. I suggest authors to spend more time including more explanations and organizing the paper.\", \"Contribution\": 2, \"Presentation\": 1}",
    "title": "Official Review by Reviewer_NaxM",
    "content": "{'Rating': 5, 'Summary': 'The paper proposes an algorithm that achieves high probability regret bound (which is stronger than the expected regret bound) for the cross-learning contextual bandits under unknown context distribution by developing refined martingale inequalities.', 'Questions': '1. What is the intuition behind the algorithm?2. How the indicator function $F_e$ resolves the unbounded martingale inequalties?', 'Soundness': 2, 'Strengths': '1. The paper clearly presents the challenging point with detailed technical expressions and the novelty of the analysis.', 'Confidence': 3, 'Weaknesses': '1. The explanation is only focused on technical side without the explanation of the algorithm. I suggest authors to spend more time including more explanations and organizing the paper.', 'Contribution': 2, 'Presentation': 1}",
    "time": "2024-11-12 13:14:27"
  },
  {
    "venue": "ICLR.cc/2025/Conference",
    "review_openreview_id": "WQLpTsquBi",
    "replyto_openreview_id": "NLrlOlSugS",
    "writer": "{\"Title\": \"Rebuttal by Authors\", \"Comment\": \"Dear reviewer NaxM:Thank you for your valuable feedback. We address the comments below in detail:---**Question 1: Lack of explanation of the algorithm**The reviewer suggested that we proposed an algorithm, but did not elaborate on its intuition, making it challenging to understand.**Response:**We would like to clarify that we did **not** propose a new algorithm. Instead, we provided a new and in-depth analysis of an **existing** algorithm, strengthening its result from an expected regret bound to a high-probability regret bound. This point was explicitly stated multiple times in the manuscript and was well understood by other reviewers.We are grateful for the reviewer\\u2019s feedback. To address this concern, we have rewritten the paper to make it clearer that our contribution lies in the re-analysis of an existing algorithm. A revised version of the paper will be uploaded soon. In the new version, we have included a dedicated section to introduce the existing algorithm in detail. Furthermore, to better clarify the intuition behind the work, we provide an explanation of the algorithm's underlying principles at the beginning of this section.---**Question 2: How the indicator function resolves unbounded martingale inequalities****Response:**As noted near the end of the original manuscript, we use the indicator function to associate the original random variable sequence with a new random variable sequence. This new random variable sequence forms a bounded martingale, allowing us to apply standard martingale concentration inequalities. Furthermore, through this indicator-based association, we demonstrate that the original and new random variable sequence coincide with high probability. This enables us to transfer the concentration inequalities from the new sequence back to the original.---Once again, we sincerely thank the reviewer for the constructive comments, which have helped us improve the clarity and readability of the manuscript. The revised version will be uploaded soon.  We hope our response has addressed the reviewer\\u2019s concerns and that you will consider increasing your support for the paper.\"}",
    "title": "Response by Authors",
    "content": "{'Title': 'Rebuttal by Authors', 'Comment': \"Dear reviewer NaxM:Thank you for your valuable feedback. We address the comments below in detail:---**Question 1: Lack of explanation of the algorithm**The reviewer suggested that we proposed an algorithm, but did not elaborate on its intuition, making it challenging to understand.**Response:**We would like to clarify that we did **not** propose a new algorithm. Instead, we provided a new and in-depth analysis of an **existing** algorithm, strengthening its result from an expected regret bound to a high-probability regret bound. This point was explicitly stated multiple times in the manuscript and was well understood by other reviewers.We are grateful for the reviewer’s feedback. To address this concern, we have rewritten the paper to make it clearer that our contribution lies in the re-analysis of an existing algorithm. A revised version of the paper will be uploaded soon. In the new version, we have included a dedicated section to introduce the existing algorithm in detail. Furthermore, to better clarify the intuition behind the work, we provide an explanation of the algorithm's underlying principles at the beginning of this section.---**Question 2: How the indicator function resolves unbounded martingale inequalities****Response:**As noted near the end of the original manuscript, we use the indicator function to associate the original random variable sequence with a new random variable sequence. This new random variable sequence forms a bounded martingale, allowing us to apply standard martingale concentration inequalities. Furthermore, through this indicator-based association, we demonstrate that the original and new random variable sequence coincide with high probability. This enables us to transfer the concentration inequalities from the new sequence back to the original.---Once again, we sincerely thank the reviewer for the constructive comments, which have helped us improve the clarity and readability of the manuscript. The revised version will be uploaded soon.  We hope our response has addressed the reviewer’s concerns and that you will consider increasing your support for the paper.\"}",
    "time": "2024-11-19 15:11:20"
  },
  {
    "venue": "ICLR.cc/2025/Conference",
    "review_openreview_id": "ozYvTN3mlR",
    "replyto_openreview_id": "wR6AlCqdjL",
    "writer": "{\"Title\": \"\", \"Comment\": \"Thank you for the feedback. After going through the reviews and all feedback replies, I will keep my score for now. Thank you!\"}",
    "title": "Response by Reviewer",
    "content": "{'Title': '', 'Comment': 'Thank you for the feedback. After going through the reviews and all feedback replies, I will keep my score for now. Thank you!'}",
    "time": "2024-11-26 12:01:35"
  },
  {
    "venue": "ICLR.cc/2025/Conference",
    "review_openreview_id": "rA3OQEiDA0",
    "replyto_openreview_id": "p1r3CeMT7X",
    "writer": "{\"Title\": \"\", \"Comment\": \"Thank you for your response.\"}",
    "title": "Response by Reviewer",
    "content": "{'Title': '', 'Comment': 'Thank you for your response.'}",
    "time": "2024-11-27 02:09:51"
  },
  {
    "venue": "ICLR.cc/2025/Conference",
    "review_openreview_id": "2LhrQBBkiK",
    "replyto_openreview_id": "UfYuXDF8OO",
    "writer": "{\"Title\": \"Rebuttal by Authors\", \"Comment\": \"Dear reviewer NZtQ:We sincerely thank the reviewer for their valuable suggestions. Below, we address the reviewer\\u2019s concerns in detail.  ---**Question: Significance of our results**  **Response:**  We appreciate the reviewer\\u2019s accurate understanding of our results and their concerns about our work's significance. We would like to highlight that it is a common practice in adversarial bandit research to focus solely on providing high-probability bounds, as this itself constitutes a significant contribution (e.g., [1,2,3]).  From a technical perspective, as noted by reviewer Sstz, deriving high-probability bounds for existing algorithms in bandit research often requires neat observations and techniques. From a results perspective, high-probability bounds are particularly important in adversarial bandit settings because these scenarios focus on worst-case outcomes, where even low-probability events cannot be ignored. Thus, providing high-probability bounds, rather than just expected regret bounds, is critical for addressing the worst-case nature of adversarial bandits.  Regarding the lack of experiments, we acknowledge that this is a limitation. However, we respectfully note that in theoretical works on adversarial bandits, especially those focusing on high-probability bounds, it is common practice to omit experiments. This is because our proofs are based on rigorous mathematical arguments without relying on any unrealistic assumptions or approximations, and they remain effective under worst-case scenarios. We hope the reviewer will consider this aspect.  To further emphasize our contributions, we have restructured the paper and added a Technical Overview section in the introduction to discuss our technical contributions in detail. The revised version of the manuscript will be uploaded shortly.  Once again, we thank the reviewer for their insightful suggestions, which have helped us improve the structure of our paper and better highlight its contributions.  We hope our response has addressed the reviewer\\u2019s concerns and that you will consider increasing your support for the paper.---References:[1] Luo, H., Tong, H., Zhang, M., & Zhang, Y. (2022). Improved High-Probability Regret for Adversarial Bandits with Time-Varying Feedback Graphs. International Conference on Algorithmic Learning Theory.[2] Neu, G. (2015). Explore no more: Improved high-probability regret bounds for non-stochastic bandits. Neural Information Processing Systems.[3] Bartlett, P.L., Dani, V., Hayes, T.P., Kakade, S.M., Rakhlin, A., & Tewari, A. (2008). High-Probability Regret Bounds for Bandit Online Linear Optimization. Annual Conference Computational Learning Theory.\"}",
    "title": "Response by Authors",
    "content": "{'Title': 'Rebuttal by Authors', 'Comment': \"Dear reviewer NZtQ:We sincerely thank the reviewer for their valuable suggestions. Below, we address the reviewer’s concerns in detail.  ---**Question: Significance of our results**  **Response:**  We appreciate the reviewer’s accurate understanding of our results and their concerns about our work's significance. We would like to highlight that it is a common practice in adversarial bandit research to focus solely on providing high-probability bounds, as this itself constitutes a significant contribution (e.g., [1,2,3]).  From a technical perspective, as noted by reviewer Sstz, deriving high-probability bounds for existing algorithms in bandit research often requires neat observations and techniques. From a results perspective, high-probability bounds are particularly important in adversarial bandit settings because these scenarios focus on worst-case outcomes, where even low-probability events cannot be ignored. Thus, providing high-probability bounds, rather than just expected regret bounds, is critical for addressing the worst-case nature of adversarial bandits.  Regarding the lack of experiments, we acknowledge that this is a limitation. However, we respectfully note that in theoretical works on adversarial bandits, especially those focusing on high-probability bounds, it is common practice to omit experiments. This is because our proofs are based on rigorous mathematical arguments without relying on any unrealistic assumptions or approximations, and they remain effective under worst-case scenarios. We hope the reviewer will consider this aspect.  To further emphasize our contributions, we have restructured the paper and added a Technical Overview section in the introduction to discuss our technical contributions in detail. The revised version of the manuscript will be uploaded shortly.  Once again, we thank the reviewer for their insightful suggestions, which have helped us improve the structure of our paper and better highlight its contributions.  We hope our response has addressed the reviewer’s concerns and that you will consider increasing your support for the paper.---References:[1] Luo, H., Tong, H., Zhang, M., & Zhang, Y. (2022). Improved High-Probability Regret for Adversarial Bandits with Time-Varying Feedback Graphs. International Conference on Algorithmic Learning Theory.[2] Neu, G. (2015). Explore no more: Improved high-probability regret bounds for non-stochastic bandits. Neural Information Processing Systems.[3] Bartlett, P.L., Dani, V., Hayes, T.P., Kakade, S.M., Rakhlin, A., & Tewari, A. (2008). High-Probability Regret Bounds for Bandit Online Linear Optimization. Annual Conference Computational Learning Theory.\"}",
    "time": "2024-11-20 04:39:57"
  },
  {
    "venue": "ICLR.cc/2025/Conference",
    "review_openreview_id": "wR6AlCqdjL",
    "replyto_openreview_id": "Is5Qh2Gs5x",
    "writer": "{\"Title\": \"Rebuttal by Authors\", \"Comment\": \"Dear reviewer 8LjP:We sincerely thank the reviewer for their suggestions and positive feedback. Below, we provide detailed responses to the reviewer\\u2019s comments.---**Question: What is the reason for assuming a finite concept class?**  **Response:**  We would like to point out that assuming a finite concept class is a common practice in contextual bandit research. While the real context space is often continuous, it can be discretized into a finite set. In this way, the finite concept class serves as a foundational model, much like the tabular MDP framework in reinforcement learning.  Of course, the discretization process involves a tradeoff: finer discretization reduces discretization error but increases the size of the concept class, leading to larger regret. However, the cross-learning structure in our setting entirely eliminates this issue, providing further justification for focusing on finite concept classes. Please see our response to the next question for more details.  ---**Question: Where is the variable in Theorem 1 that characterizes the property of $C$?**  **Response:**  We thank the reviewer for raising this excellent question, which touches on one of the most interesting aspects of cross-learning bandits. Indeed, in most cases, the results depend on the size of the concept class. In the vanilla contextual bandit setting, the results typically have a polynomial dependence on the size of the concept class $C$.  However, in our problem, thanks to the cross-learning structure, this polynomial dependence on the size of $C$ is entirely eliminated. As a result, our final regret bound is completely independent of the size of $C$, which is why Theorem 1 does not need to explicitly characterize the property of $C$.  As mentioned in the response to the previous question, the cross-learning structure allows us to bypass the discretization issue for finite concept classes. This is because our result is completely independent of the size of the concept class, enabling arbitrarily fine discretization and resolving this concern entirely.  ---**Question: The statement in line 111 that we can observe the loss for every context seems confusing and does not match the behavior of the algorithm.**  **Response:**  We would like to clarify that this is precisely the core of the cross-learning structure. The cross-learning structure explicitly assumes that we can observe the loss for every context. As discussed in the related works section, this structure is common in practice, with examples including bidding in online auctions, sleeping bandits, repeated Bayesian games, and dynamic pricing.  Regarding the algorithm\\u2019s behavior, we respectfully disagree with the reviewer\\u2019s assessment. The algorithm indeed matches this assumption since it is explicitly designed for the cross-learning structure.---**Question: Readability of Section 3.2**  **Response:**  We thank the reviewer for pointing this out. To improve readability, we have restructured the paper. In the revised version, Section 3.2 has been expanded into a standalone chapter, further divided into three subsections, each focusing on a single topic. This restructuring aims to make the paper more organized and easier to follow. The updated manuscript will be uploaded shortly.  ---We once again thank the reviewer for their valuable suggestions and positive feedback. Your comments have helped us improve the structure and readability of our paper. We hope our responses have addressed your concerns.\"}",
    "title": "Response by Authors",
    "content": "{'Title': 'Rebuttal by Authors', 'Comment': 'Dear reviewer 8LjP:We sincerely thank the reviewer for their suggestions and positive feedback. Below, we provide detailed responses to the reviewer’s comments.---**Question: What is the reason for assuming a finite concept class?**  **Response:**  We would like to point out that assuming a finite concept class is a common practice in contextual bandit research. While the real context space is often continuous, it can be discretized into a finite set. In this way, the finite concept class serves as a foundational model, much like the tabular MDP framework in reinforcement learning.  Of course, the discretization process involves a tradeoff: finer discretization reduces discretization error but increases the size of the concept class, leading to larger regret. However, the cross-learning structure in our setting entirely eliminates this issue, providing further justification for focusing on finite concept classes. Please see our response to the next question for more details.  ---**Question: Where is the variable in Theorem 1 that characterizes the property of $C$?**  **Response:**  We thank the reviewer for raising this excellent question, which touches on one of the most interesting aspects of cross-learning bandits. Indeed, in most cases, the results depend on the size of the concept class. In the vanilla contextual bandit setting, the results typically have a polynomial dependence on the size of the concept class $C$.  However, in our problem, thanks to the cross-learning structure, this polynomial dependence on the size of $C$ is entirely eliminated. As a result, our final regret bound is completely independent of the size of $C$, which is why Theorem 1 does not need to explicitly characterize the property of $C$.  As mentioned in the response to the previous question, the cross-learning structure allows us to bypass the discretization issue for finite concept classes. This is because our result is completely independent of the size of the concept class, enabling arbitrarily fine discretization and resolving this concern entirely.  ---**Question: The statement in line 111 that we can observe the loss for every context seems confusing and does not match the behavior of the algorithm.**  **Response:**  We would like to clarify that this is precisely the core of the cross-learning structure. The cross-learning structure explicitly assumes that we can observe the loss for every context. As discussed in the related works section, this structure is common in practice, with examples including bidding in online auctions, sleeping bandits, repeated Bayesian games, and dynamic pricing.  Regarding the algorithm’s behavior, we respectfully disagree with the reviewer’s assessment. The algorithm indeed matches this assumption since it is explicitly designed for the cross-learning structure.---**Question: Readability of Section 3.2**  **Response:**  We thank the reviewer for pointing this out. To improve readability, we have restructured the paper. In the revised version, Section 3.2 has been expanded into a standalone chapter, further divided into three subsections, each focusing on a single topic. This restructuring aims to make the paper more organized and easier to follow. The updated manuscript will be uploaded shortly.  ---We once again thank the reviewer for their valuable suggestions and positive feedback. Your comments have helped us improve the structure and readability of our paper. We hope our responses have addressed your concerns.'}",
    "time": "2024-11-20 06:04:06"
  }
]