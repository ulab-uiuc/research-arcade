venue,review_openreview_id,replyto_openreview_id,writer,title,content,time
ICLR.cc/2025/Conference,Is5Qh2Gs5x,zzR1Uskhj0,"{""Rating"": 6, ""Summary"": ""The submission studies contextual bandits with cross learning. Previously, the existing regret bound held in expectation. The submission refines the regret analysis so that the regret bound holds with high probability. The main contribution is to show how the weak dependency structure can be exploited to solve a concentration difficulty in the previous analysis."", ""Questions"": ""- (1) Is the concept class $C$ a finite set? If so, what is the reason for assuming a finite concept class $C$? Practically speaking, the contextual information would be like a vector in a compact set, as it is very unlikely to see two identical users.- (2) Where is the variable in Theorem 1 that characterizes the property of $C$? How does this variable appear in the bound proved in this submission?- (3) Could you please provide more evidence or further discussion of the applicability of the technique developed here so that we can better appreciate its potential?"", ""Soundness"": 3, ""Strengths"": ""- The submission points out the difficulty that prevents the previous work from achieving a bound with high probability (lines 167\u2013176).- Identify the weak dependency between epochs (line 386).- Devise a new technique to solve the unbounded issue induced by the weak dependency (the treatment for the Bias5e them in lines 395\u2013411)."", ""Confidence"": 4, ""Weaknesses"": ""- (1) Section 3.2 contains several subtopics, such as the regret decomposition, the discussion of each decomposed term, and the analysis strategy for the challenging term. A better editorial layout would improve the readability.- (2) The sentence \u201cNotably, \u2026\u201d in line 111 is confusing. It seems unrealistic to be able to observe the loss for every context $c$. It also does not match the algorithm\u2019s (Algorithm 1) behavior."", ""Contribution"": 3, ""Presentation"": 3}",Official Review by Reviewer_8LjP,"{""Rating"": 6, ""Summary"": ""The submission studies contextual bandits with cross learning. Previously, the existing regret bound held in expectation. The submission refines the regret analysis so that the regret bound holds with high probability. The main contribution is to show how the weak dependency structure can be exploited to solve a concentration difficulty in the previous analysis."", ""Questions"": ""- (1) Is the concept class $C$ a finite set? If so, what is the reason for assuming a finite concept class $C$? Practically speaking, the contextual information would be like a vector in a compact set, as it is very unlikely to see two identical users.- (2) Where is the variable in Theorem 1 that characterizes the property of $C$? How does this variable appear in the bound proved in this submission?- (3) Could you please provide more evidence or further discussion of the applicability of the technique developed here so that we can better appreciate its potential?"", ""Soundness"": 3, ""Strengths"": ""- The submission points out the difficulty that prevents the previous work from achieving a bound with high probability (lines 167\u2013176).- Identify the weak dependency between epochs (line 386).- Devise a new technique to solve the unbounded issue induced by the weak dependency (the treatment for the Bias5e them in lines 395\u2013411)."", ""Confidence"": 4, ""Weaknesses"": ""- (1) Section 3.2 contains several subtopics, such as the regret decomposition, the discussion of each decomposed term, and the analysis strategy for the challenging term. A better editorial layout would improve the readability.- (2) The sentence \u201cNotably, \u2026\u201d in line 111 is confusing. It seems unrealistic to be able to observe the loss for every context $c$. It also does not match the algorithm\u2019s (Algorithm 1) behavior."", ""Contribution"": 3, ""Presentation"": 3}",2024-11-04 01:05:41
ICLR.cc/2025/Conference,Hsumvt7DeH,zzR1Uskhj0,"{""Rating"": 6, ""Summary"": ""The paper studied adversarial context bandits in a special setting where the losses of arm $a_i$ could be observed under all contexts when the algorithm plays arm $a_i$. The goal, like in classical adversarial bandit problems, is to minimize the regret compared to the loss of the best arm in hindsight. The paper focuses on the setting where the loss sequence is adversarial and the context is stochastic with an unknown distribution. A recent work of SZ [NeurIPS\u201923] designed an algorithm with *expected* regret of $\\tilde{O}(\\sqrt{KT})$ in this setting, where $K$ is the number of arms. This paper conducted a renewed analysis of the algorithm in SZ  [NeurIPS\u201923], and the main result is that the algorithm could actually achieve $\\tilde{O}(\\sqrt{KT})$ with high probability.The main technique of the paper is heavily influenced by the previous work of SZ [NeurIPS\u201923]. In a nutshell, the low-regret guarantee of the algorithm crucially relies on the concentration of unbiased estimation of $E_{c}[\\ell_{t,c}(a)]$. Here, we cannot exactly compute the quantity since the distribution of the context is unknown. The key idea of SZ [NeurIPS\u201923] is to commit two steps for each EXP3 step and use one of them to estimate the distribution of the context. On top of that, this paper further utilized the weak dependency between epochs, and derived a martingale argument to get high-probability regret."", ""Questions"": ""- Should the definition of regret on page 3 be reversed? As in, you are subtracting the loss of the best arm with the loss of a policy, which should be a negative value (if with positive regret). This would change the decomposition of the regret on page 5 as well, but it seems nothing would affect the correctness.- I think the full algorithm description of the algorithm in SZ [NeurIPS\u201923] (or some simpler version of the description) could be shown much earlier in the paper. This would be helpful for readers who are not familiar with the previous algorithm.- Also, stating the main theorem in a preliminary section looks very non-standard to me. I\u2019m not letting this affect my score, but please consider re-organizing this.- The meaning of 'with high probability' was never explained in the paper -- as in, it could mean with probability $1-1/K$ or with probability $0.99$. I think your bound gives the former, and this should be stated explicitly."", ""Soundness"": 4, ""Strengths"": ""In general, my opinion of this paper is positive. The paper appears to require a great deal of background to be able to parse. Despite this, I believe the paper did reasonably well in terms of explaining the existing work and its techniques. Getting high probability bounds in adversarial bandits usually requires some neat observations and technical steps. Although I\u2019m not able to follow all the steps in the short time frame, I do think the paper contains some nice technical observations and ideas."", ""Confidence"": 4, ""Weaknesses"": ""Although I'm mostly supportive, I think I couldn\u2019t strongly champion the paper due to the following reasons:- The scope of contribution: although the paper does contain some neat technical observations, the contribution appears to be somehow incremental. After all, this is a new analysis of an existing algorithm, and the new analysis is not something that improves the previous bound (but instead is to get a high-probability bound). Again, I do acknowledge that such contributions are non-trivial. However, I do not think it\u2019s enough for me to champion the paper.- If the paper is going to be mainly accepted due to the techniques: then, I do not think the paper contains a substantial amount of new ideas. I appreciate the technical observations, and I agree that the steps are non-trivial. However, if the conceptual message is not as strong, and the merits of the paper mainly lie in the techniques, then the bar would inevitably be higher. - For a conference like ICLR, the lack of experiments could be an issue. I am *not* letting this affect my score since I often advocate learning theory papers. However, I do want to raise this point since it is common for ML conferences to ask for experiments."", ""Contribution"": 3, ""Presentation"": 3}",Official Review by Reviewer_Sstz,"{""Rating"": 6, ""Summary"": ""The paper studied adversarial context bandits in a special setting where the losses of arm $a_i$ could be observed under all contexts when the algorithm plays arm $a_i$. The goal, like in classical adversarial bandit problems, is to minimize the regret compared to the loss of the best arm in hindsight. The paper focuses on the setting where the loss sequence is adversarial and the context is stochastic with an unknown distribution. A recent work of SZ [NeurIPS\u201923] designed an algorithm with *expected* regret of $\\tilde{O}(\\sqrt{KT})$ in this setting, where $K$ is the number of arms. This paper conducted a renewed analysis of the algorithm in SZ  [NeurIPS\u201923], and the main result is that the algorithm could actually achieve $\\tilde{O}(\\sqrt{KT})$ with high probability.The main technique of the paper is heavily influenced by the previous work of SZ [NeurIPS\u201923]. In a nutshell, the low-regret guarantee of the algorithm crucially relies on the concentration of unbiased estimation of $E_{c}[\\ell_{t,c}(a)]$. Here, we cannot exactly compute the quantity since the distribution of the context is unknown. The key idea of SZ [NeurIPS\u201923] is to commit two steps for each EXP3 step and use one of them to estimate the distribution of the context. On top of that, this paper further utilized the weak dependency between epochs, and derived a martingale argument to get high-probability regret."", ""Questions"": ""- Should the definition of regret on page 3 be reversed? As in, you are subtracting the loss of the best arm with the loss of a policy, which should be a negative value (if with positive regret). This would change the decomposition of the regret on page 5 as well, but it seems nothing would affect the correctness.- I think the full algorithm description of the algorithm in SZ [NeurIPS\u201923] (or some simpler version of the description) could be shown much earlier in the paper. This would be helpful for readers who are not familiar with the previous algorithm.- Also, stating the main theorem in a preliminary section looks very non-standard to me. I\u2019m not letting this affect my score, but please consider re-organizing this.- The meaning of 'with high probability' was never explained in the paper -- as in, it could mean with probability $1-1/K$ or with probability $0.99$. I think your bound gives the former, and this should be stated explicitly."", ""Soundness"": 4, ""Strengths"": ""In general, my opinion of this paper is positive. The paper appears to require a great deal of background to be able to parse. Despite this, I believe the paper did reasonably well in terms of explaining the existing work and its techniques. Getting high probability bounds in adversarial bandits usually requires some neat observations and technical steps. Although I\u2019m not able to follow all the steps in the short time frame, I do think the paper contains some nice technical observations and ideas."", ""Confidence"": 4, ""Weaknesses"": ""Although I'm mostly supportive, I think I couldn\u2019t strongly champion the paper due to the following reasons:- The scope of contribution: although the paper does contain some neat technical observations, the contribution appears to be somehow incremental. After all, this is a new analysis of an existing algorithm, and the new analysis is not something that improves the previous bound (but instead is to get a high-probability bound). Again, I do acknowledge that such contributions are non-trivial. However, I do not think it\u2019s enough for me to champion the paper.- If the paper is going to be mainly accepted due to the techniques: then, I do not think the paper contains a substantial amount of new ideas. I appreciate the technical observations, and I agree that the steps are non-trivial. However, if the conceptual message is not as strong, and the merits of the paper mainly lie in the techniques, then the bar would inevitably be higher. - For a conference like ICLR, the lack of experiments could be an issue. I am *not* letting this affect my score since I often advocate learning theory papers. However, I do want to raise this point since it is common for ML conferences to ask for experiments."", ""Contribution"": 3, ""Presentation"": 3}",2024-11-07 22:01:20
ICLR.cc/2025/Conference,smWIsNwjkv,zzR1Uskhj0,"{""Rating"": 8, ""Summary"": ""The paper studies cross learning in contextual adversarial linear bandits where the learner observes the losses of all contexts in each round. Recent work in Schneider et al. proposed an algorithm with a regret upper bound only in expectation. The paper studies the same algorithm and proves that the regret upper bound holds with high probability."", ""Questions"": ""please see weaknesses"", ""Soundness"": 4, ""Strengths"": ""- The paper proves a high probability lower bound which is stronger than the in expectation bound in the literature.- The analysis uses a nice observation that the different epochs in the algorithm are only weakly dependent which enables to prove a small bound for the cumulative bias across all epochs- While standard martingale inequalities cannot directly upper bound the cumulative bias, a novel technique is proposed to address this"", ""Confidence"": 3, ""Weaknesses"": ""Can the reduction in [1] be used to map the multi-context to a single context problem? The technique is proposed for non-adversarial losses, however, the action set map from distributional to fixed should not be affected by that.I understand that the paper only focuses on analyzing an existing algorithm. However, a comparison with such technique in the related work is needed to justify the use of such algorithm or suggest alternative techniques to address the problem.[1] \""Contexts can be cheap: Solving stochastic contextual bandits with linear bandit algorithms.\"" The Thirty Sixth Annual Conference on Learning Theory. PMLR, 2023."", ""Contribution"": 3, ""Presentation"": 3}",Official Review by Reviewer_uDyZ,"{""Rating"": 8, ""Summary"": ""The paper studies cross learning in contextual adversarial linear bandits where the learner observes the losses of all contexts in each round. Recent work in Schneider et al. proposed an algorithm with a regret upper bound only in expectation. The paper studies the same algorithm and proves that the regret upper bound holds with high probability."", ""Questions"": ""please see weaknesses"", ""Soundness"": 4, ""Strengths"": ""- The paper proves a high probability lower bound which is stronger than the in expectation bound in the literature.- The analysis uses a nice observation that the different epochs in the algorithm are only weakly dependent which enables to prove a small bound for the cumulative bias across all epochs- While standard martingale inequalities cannot directly upper bound the cumulative bias, a novel technique is proposed to address this"", ""Confidence"": 3, ""Weaknesses"": ""Can the reduction in [1] be used to map the multi-context to a single context problem? The technique is proposed for non-adversarial losses, however, the action set map from distributional to fixed should not be affected by that.I understand that the paper only focuses on analyzing an existing algorithm. However, a comparison with such technique in the related work is needed to justify the use of such algorithm or suggest alternative techniques to address the problem.[1] \""Contexts can be cheap: Solving stochastic contextual bandits with linear bandit algorithms.\"" The Thirty Sixth Annual Conference on Learning Theory. PMLR, 2023."", ""Contribution"": 3, ""Presentation"": 3}",2024-11-08 08:31:50
ICLR.cc/2025/Conference,UfYuXDF8OO,zzR1Uskhj0,"{""Rating"": 5, ""Summary"": ""This paper addresses the challenge of achieving high-probability regret bounds in the adversarial contextual bandit framework, where the learner encounters varying contexts and must minimize cumulative loss over time. The focus is on \""cross-learning\"" contextual bandits, where learners can observe losses for all possible contexts, not just the current one. Results leverage weak dependencies between epochs and refine existing martingale inequalities, by exploiting interdependencies in observations. This analysis ultimately shows that the algorithm is effective in adversarial settings, even with unknown context distributions."", ""Questions"": ""While the result is good, I am uncertain about its significance because neither the problem nor the algorithm proposed are new. There are no extensions of this result, no experiments. I am not sure if this is standalone result is significant enough to be published at a premier conference."", ""Soundness"": 3, ""Strengths"": ""The paper proposes a new look at an existing problem and provides a completely novel analysis in their work. The ideas and techniques proposed are completely new and can be of independent interest. This is particularly true for the martingale concentration result."", ""Confidence"": 3, ""Weaknesses"": ""See questions."", ""Contribution"": 3, ""Presentation"": 3}",Official Review by Reviewer_NZtQ,"{""Rating"": 5, ""Summary"": ""This paper addresses the challenge of achieving high-probability regret bounds in the adversarial contextual bandit framework, where the learner encounters varying contexts and must minimize cumulative loss over time. The focus is on \""cross-learning\"" contextual bandits, where learners can observe losses for all possible contexts, not just the current one. Results leverage weak dependencies between epochs and refine existing martingale inequalities, by exploiting interdependencies in observations. This analysis ultimately shows that the algorithm is effective in adversarial settings, even with unknown context distributions."", ""Questions"": ""While the result is good, I am uncertain about its significance because neither the problem nor the algorithm proposed are new. There are no extensions of this result, no experiments. I am not sure if this is standalone result is significant enough to be published at a premier conference."", ""Soundness"": 3, ""Strengths"": ""The paper proposes a new look at an existing problem and provides a completely novel analysis in their work. The ideas and techniques proposed are completely new and can be of independent interest. This is particularly true for the martingale concentration result."", ""Confidence"": 3, ""Weaknesses"": ""See questions."", ""Contribution"": 3, ""Presentation"": 3}",2024-11-08 09:01:46
ICLR.cc/2025/Conference,NLrlOlSugS,zzR1Uskhj0,"{""Rating"": 5, ""Summary"": ""The paper proposes an algorithm that achieves high probability regret bound (which is stronger than the expected regret bound) for the cross-learning contextual bandits under unknown context distribution by developing refined martingale inequalities."", ""Questions"": ""1. What is the intuition behind the algorithm?2. How the indicator function $F_e$ resolves the unbounded martingale inequalties?"", ""Soundness"": 2, ""Strengths"": ""1. The paper clearly presents the challenging point with detailed technical expressions and the novelty of the analysis."", ""Confidence"": 3, ""Weaknesses"": ""1. The explanation is only focused on technical side without the explanation of the algorithm. I suggest authors to spend more time including more explanations and organizing the paper."", ""Contribution"": 2, ""Presentation"": 1}",Official Review by Reviewer_NaxM,"{""Rating"": 5, ""Summary"": ""The paper proposes an algorithm that achieves high probability regret bound (which is stronger than the expected regret bound) for the cross-learning contextual bandits under unknown context distribution by developing refined martingale inequalities."", ""Questions"": ""1. What is the intuition behind the algorithm?2. How the indicator function $F_e$ resolves the unbounded martingale inequalties?"", ""Soundness"": 2, ""Strengths"": ""1. The paper clearly presents the challenging point with detailed technical expressions and the novelty of the analysis."", ""Confidence"": 3, ""Weaknesses"": ""1. The explanation is only focused on technical side without the explanation of the algorithm. I suggest authors to spend more time including more explanations and organizing the paper."", ""Contribution"": 2, ""Presentation"": 1}",2024-11-12 13:14:27
ICLR.cc/2025/Conference,WQLpTsquBi,NLrlOlSugS,"{""Title"": ""Rebuttal by Authors"", ""Comment"": ""Dear reviewer NaxM:Thank you for your valuable feedback. We address the comments below in detail:---**Question 1: Lack of explanation of the algorithm**The reviewer suggested that we proposed an algorithm, but did not elaborate on its intuition, making it challenging to understand.**Response:**We would like to clarify that we did **not** propose a new algorithm. Instead, we provided a new and in-depth analysis of an **existing** algorithm, strengthening its result from an expected regret bound to a high-probability regret bound. This point was explicitly stated multiple times in the manuscript and was well understood by other reviewers.We are grateful for the reviewer\u2019s feedback. To address this concern, we have rewritten the paper to make it clearer that our contribution lies in the re-analysis of an existing algorithm. A revised version of the paper will be uploaded soon. In the new version, we have included a dedicated section to introduce the existing algorithm in detail. Furthermore, to better clarify the intuition behind the work, we provide an explanation of the algorithm's underlying principles at the beginning of this section.---**Question 2: How the indicator function resolves unbounded martingale inequalities****Response:**As noted near the end of the original manuscript, we use the indicator function to associate the original random variable sequence with a new random variable sequence. This new random variable sequence forms a bounded martingale, allowing us to apply standard martingale concentration inequalities. Furthermore, through this indicator-based association, we demonstrate that the original and new random variable sequence coincide with high probability. This enables us to transfer the concentration inequalities from the new sequence back to the original.---Once again, we sincerely thank the reviewer for the constructive comments, which have helped us improve the clarity and readability of the manuscript. The revised version will be uploaded soon.  We hope our response has addressed the reviewer\u2019s concerns and that you will consider increasing your support for the paper.""}",Response by Authors,"{""Title"": ""Rebuttal by Authors"", ""Comment"": ""Dear reviewer NaxM:Thank you for your valuable feedback. We address the comments below in detail:---**Question 1: Lack of explanation of the algorithm**The reviewer suggested that we proposed an algorithm, but did not elaborate on its intuition, making it challenging to understand.**Response:**We would like to clarify that we did **not** propose a new algorithm. Instead, we provided a new and in-depth analysis of an **existing** algorithm, strengthening its result from an expected regret bound to a high-probability regret bound. This point was explicitly stated multiple times in the manuscript and was well understood by other reviewers.We are grateful for the reviewer\u2019s feedback. To address this concern, we have rewritten the paper to make it clearer that our contribution lies in the re-analysis of an existing algorithm. A revised version of the paper will be uploaded soon. In the new version, we have included a dedicated section to introduce the existing algorithm in detail. Furthermore, to better clarify the intuition behind the work, we provide an explanation of the algorithm's underlying principles at the beginning of this section.---**Question 2: How the indicator function resolves unbounded martingale inequalities****Response:**As noted near the end of the original manuscript, we use the indicator function to associate the original random variable sequence with a new random variable sequence. This new random variable sequence forms a bounded martingale, allowing us to apply standard martingale concentration inequalities. Furthermore, through this indicator-based association, we demonstrate that the original and new random variable sequence coincide with high probability. This enables us to transfer the concentration inequalities from the new sequence back to the original.---Once again, we sincerely thank the reviewer for the constructive comments, which have helped us improve the clarity and readability of the manuscript. The revised version will be uploaded soon.  We hope our response has addressed the reviewer\u2019s concerns and that you will consider increasing your support for the paper.""}",2024-11-19 15:11:20
ICLR.cc/2025/Conference,ozYvTN3mlR,wR6AlCqdjL,"{""Title"": """", ""Comment"": ""Thank you for the feedback. After going through the reviews and all feedback replies, I will keep my score for now. Thank you!""}",Response by Reviewer,"{""Title"": """", ""Comment"": ""Thank you for the feedback. After going through the reviews and all feedback replies, I will keep my score for now. Thank you!""}",2024-11-26 12:01:35
ICLR.cc/2025/Conference,rA3OQEiDA0,p1r3CeMT7X,"{""Title"": """", ""Comment"": ""Thank you for your response.""}",Response by Reviewer,"{""Title"": """", ""Comment"": ""Thank you for your response.""}",2024-11-27 02:09:51
ICLR.cc/2025/Conference,2LhrQBBkiK,UfYuXDF8OO,"{""Title"": ""Rebuttal by Authors"", ""Comment"": ""Dear reviewer NZtQ:We sincerely thank the reviewer for their valuable suggestions. Below, we address the reviewer\u2019s concerns in detail.  ---**Question: Significance of our results**  **Response:**  We appreciate the reviewer\u2019s accurate understanding of our results and their concerns about our work's significance. We would like to highlight that it is a common practice in adversarial bandit research to focus solely on providing high-probability bounds, as this itself constitutes a significant contribution (e.g., [1,2,3]).  From a technical perspective, as noted by reviewer Sstz, deriving high-probability bounds for existing algorithms in bandit research often requires neat observations and techniques. From a results perspective, high-probability bounds are particularly important in adversarial bandit settings because these scenarios focus on worst-case outcomes, where even low-probability events cannot be ignored. Thus, providing high-probability bounds, rather than just expected regret bounds, is critical for addressing the worst-case nature of adversarial bandits.  Regarding the lack of experiments, we acknowledge that this is a limitation. However, we respectfully note that in theoretical works on adversarial bandits, especially those focusing on high-probability bounds, it is common practice to omit experiments. This is because our proofs are based on rigorous mathematical arguments without relying on any unrealistic assumptions or approximations, and they remain effective under worst-case scenarios. We hope the reviewer will consider this aspect.  To further emphasize our contributions, we have restructured the paper and added a Technical Overview section in the introduction to discuss our technical contributions in detail. The revised version of the manuscript will be uploaded shortly.  Once again, we thank the reviewer for their insightful suggestions, which have helped us improve the structure of our paper and better highlight its contributions.  We hope our response has addressed the reviewer\u2019s concerns and that you will consider increasing your support for the paper.---References:[1] Luo, H., Tong, H., Zhang, M., & Zhang, Y. (2022). Improved High-Probability Regret for Adversarial Bandits with Time-Varying Feedback Graphs. International Conference on Algorithmic Learning Theory.[2] Neu, G. (2015). Explore no more: Improved high-probability regret bounds for non-stochastic bandits. Neural Information Processing Systems.[3] Bartlett, P.L., Dani, V., Hayes, T.P., Kakade, S.M., Rakhlin, A., & Tewari, A. (2008). High-Probability Regret Bounds for Bandit Online Linear Optimization. Annual Conference Computational Learning Theory.""}",Response by Authors,"{""Title"": ""Rebuttal by Authors"", ""Comment"": ""Dear reviewer NZtQ:We sincerely thank the reviewer for their valuable suggestions. Below, we address the reviewer\u2019s concerns in detail.  ---**Question: Significance of our results**  **Response:**  We appreciate the reviewer\u2019s accurate understanding of our results and their concerns about our work's significance. We would like to highlight that it is a common practice in adversarial bandit research to focus solely on providing high-probability bounds, as this itself constitutes a significant contribution (e.g., [1,2,3]).  From a technical perspective, as noted by reviewer Sstz, deriving high-probability bounds for existing algorithms in bandit research often requires neat observations and techniques. From a results perspective, high-probability bounds are particularly important in adversarial bandit settings because these scenarios focus on worst-case outcomes, where even low-probability events cannot be ignored. Thus, providing high-probability bounds, rather than just expected regret bounds, is critical for addressing the worst-case nature of adversarial bandits.  Regarding the lack of experiments, we acknowledge that this is a limitation. However, we respectfully note that in theoretical works on adversarial bandits, especially those focusing on high-probability bounds, it is common practice to omit experiments. This is because our proofs are based on rigorous mathematical arguments without relying on any unrealistic assumptions or approximations, and they remain effective under worst-case scenarios. We hope the reviewer will consider this aspect.  To further emphasize our contributions, we have restructured the paper and added a Technical Overview section in the introduction to discuss our technical contributions in detail. The revised version of the manuscript will be uploaded shortly.  Once again, we thank the reviewer for their insightful suggestions, which have helped us improve the structure of our paper and better highlight its contributions.  We hope our response has addressed the reviewer\u2019s concerns and that you will consider increasing your support for the paper.---References:[1] Luo, H., Tong, H., Zhang, M., & Zhang, Y. (2022). Improved High-Probability Regret for Adversarial Bandits with Time-Varying Feedback Graphs. International Conference on Algorithmic Learning Theory.[2] Neu, G. (2015). Explore no more: Improved high-probability regret bounds for non-stochastic bandits. Neural Information Processing Systems.[3] Bartlett, P.L., Dani, V., Hayes, T.P., Kakade, S.M., Rakhlin, A., & Tewari, A. (2008). High-Probability Regret Bounds for Bandit Online Linear Optimization. Annual Conference Computational Learning Theory.""}",2024-11-20 04:39:57
ICLR.cc/2025/Conference,wR6AlCqdjL,Is5Qh2Gs5x,"{""Title"": ""Rebuttal by Authors"", ""Comment"": ""Dear reviewer 8LjP:We sincerely thank the reviewer for their suggestions and positive feedback. Below, we provide detailed responses to the reviewer\u2019s comments.---**Question: What is the reason for assuming a finite concept class?**  **Response:**  We would like to point out that assuming a finite concept class is a common practice in contextual bandit research. While the real context space is often continuous, it can be discretized into a finite set. In this way, the finite concept class serves as a foundational model, much like the tabular MDP framework in reinforcement learning.  Of course, the discretization process involves a tradeoff: finer discretization reduces discretization error but increases the size of the concept class, leading to larger regret. However, the cross-learning structure in our setting entirely eliminates this issue, providing further justification for focusing on finite concept classes. Please see our response to the next question for more details.  ---**Question: Where is the variable in Theorem 1 that characterizes the property of $C$?**  **Response:**  We thank the reviewer for raising this excellent question, which touches on one of the most interesting aspects of cross-learning bandits. Indeed, in most cases, the results depend on the size of the concept class. In the vanilla contextual bandit setting, the results typically have a polynomial dependence on the size of the concept class $C$.  However, in our problem, thanks to the cross-learning structure, this polynomial dependence on the size of $C$ is entirely eliminated. As a result, our final regret bound is completely independent of the size of $C$, which is why Theorem 1 does not need to explicitly characterize the property of $C$.  As mentioned in the response to the previous question, the cross-learning structure allows us to bypass the discretization issue for finite concept classes. This is because our result is completely independent of the size of the concept class, enabling arbitrarily fine discretization and resolving this concern entirely.  ---**Question: The statement in line 111 that we can observe the loss for every context seems confusing and does not match the behavior of the algorithm.**  **Response:**  We would like to clarify that this is precisely the core of the cross-learning structure. The cross-learning structure explicitly assumes that we can observe the loss for every context. As discussed in the related works section, this structure is common in practice, with examples including bidding in online auctions, sleeping bandits, repeated Bayesian games, and dynamic pricing.  Regarding the algorithm\u2019s behavior, we respectfully disagree with the reviewer\u2019s assessment. The algorithm indeed matches this assumption since it is explicitly designed for the cross-learning structure.---**Question: Readability of Section 3.2**  **Response:**  We thank the reviewer for pointing this out. To improve readability, we have restructured the paper. In the revised version, Section 3.2 has been expanded into a standalone chapter, further divided into three subsections, each focusing on a single topic. This restructuring aims to make the paper more organized and easier to follow. The updated manuscript will be uploaded shortly.  ---We once again thank the reviewer for their valuable suggestions and positive feedback. Your comments have helped us improve the structure and readability of our paper. We hope our responses have addressed your concerns.""}",Response by Authors,"{""Title"": ""Rebuttal by Authors"", ""Comment"": ""Dear reviewer 8LjP:We sincerely thank the reviewer for their suggestions and positive feedback. Below, we provide detailed responses to the reviewer\u2019s comments.---**Question: What is the reason for assuming a finite concept class?**  **Response:**  We would like to point out that assuming a finite concept class is a common practice in contextual bandit research. While the real context space is often continuous, it can be discretized into a finite set. In this way, the finite concept class serves as a foundational model, much like the tabular MDP framework in reinforcement learning.  Of course, the discretization process involves a tradeoff: finer discretization reduces discretization error but increases the size of the concept class, leading to larger regret. However, the cross-learning structure in our setting entirely eliminates this issue, providing further justification for focusing on finite concept classes. Please see our response to the next question for more details.  ---**Question: Where is the variable in Theorem 1 that characterizes the property of $C$?**  **Response:**  We thank the reviewer for raising this excellent question, which touches on one of the most interesting aspects of cross-learning bandits. Indeed, in most cases, the results depend on the size of the concept class. In the vanilla contextual bandit setting, the results typically have a polynomial dependence on the size of the concept class $C$.  However, in our problem, thanks to the cross-learning structure, this polynomial dependence on the size of $C$ is entirely eliminated. As a result, our final regret bound is completely independent of the size of $C$, which is why Theorem 1 does not need to explicitly characterize the property of $C$.  As mentioned in the response to the previous question, the cross-learning structure allows us to bypass the discretization issue for finite concept classes. This is because our result is completely independent of the size of the concept class, enabling arbitrarily fine discretization and resolving this concern entirely.  ---**Question: The statement in line 111 that we can observe the loss for every context seems confusing and does not match the behavior of the algorithm.**  **Response:**  We would like to clarify that this is precisely the core of the cross-learning structure. The cross-learning structure explicitly assumes that we can observe the loss for every context. As discussed in the related works section, this structure is common in practice, with examples including bidding in online auctions, sleeping bandits, repeated Bayesian games, and dynamic pricing.  Regarding the algorithm\u2019s behavior, we respectfully disagree with the reviewer\u2019s assessment. The algorithm indeed matches this assumption since it is explicitly designed for the cross-learning structure.---**Question: Readability of Section 3.2**  **Response:**  We thank the reviewer for pointing this out. To improve readability, we have restructured the paper. In the revised version, Section 3.2 has been expanded into a standalone chapter, further divided into three subsections, each focusing on a single topic. This restructuring aims to make the paper more organized and easier to follow. The updated manuscript will be uploaded shortly.  ---We once again thank the reviewer for their valuable suggestions and positive feedback. Your comments have helped us improve the structure and readability of our paper. We hope our responses have addressed your concerns.""}",2024-11-20 06:04:06
xujj_test,xujj_test,xujj_test,xujj_test,xujj_test,"{""title"": ""xujj_test"", ""content"": ""xujj_test""}",xujj_test
ICLR.cc/2025/Conference,DHwZxFryth,Yqbllggrmw,test,Response by Authors,"{""Title"": ""Response to Reviewer 7i95 (1/2)"", ""Comment"": ""> The method does not improve much in the AlpacaEval 2.0 Score. The author should give a detailed explanation. And why not use metrics like length-controlled win rate?**Response:** Thank you for your careful observation and question. We would like to clarify that we are already using the length-controlled (LC) AlpacaEval 2.0 win-rate metric in our evaluations. We will make this clearer in the table header of Table 3.Regarding the fact that the AlpacaEval 2.0 scores on LLama-3 (8B) do not improve compared to the baselines, we believe this is because our base model, the instruction-finetuned LLama-3 (8B), is already trained to perform exceptionally well in terms of helpfulness, which is the focus of the AlpacaEval benchmark. Additionally, the preference dataset we used, UltraFeedback, may not provide significant further enhancement in the helpfulness aspect. This is supported by the slight decrease observed in the AlpacaEval score for the standard DPO baseline as well (see Table 3, results on LLama-3). Therefore, we think these AlpacaEval 2.0 results on LLama-3 (8B) may not indicate that SAIL is ineffective; it may be simply caused by an ill-suited combination of base model, finetuning dataset, and evaluation benchmark.We also further conducted experiments on the Zephyr (7B) model as the backbone, whose AlpacaEval 2.0 win-rate is lower. We still train on the UltraFeedback preference dataset and the other experiment setups are unchanged. In this experiment, we see a larger improvement of the SAIL method compared to the standard DPO baseline (Zephyr-7B-Beta).|             | AlpacaEval 2.0 (LC) Win-Rate ||--------------------|------------------------------|| Base (Zephyr-7B-SFT-Full) | 6.4 %                        || DPO (Zephyr-7B-Beta)   | 13.2 %                       || SAIL-PP  | 15.9 %                       |> Authors should compare more advanced preference optimization algorithms like ORPO and SimPO. And current results are not impressive for the alignment community.**Response:** Thank you for raising this insightful point. We see ORPO and SimPO are two recent work which propose a different objective than the standard RLHF, and achieve remarkable improvements in terms of alignment performance and efficiency.Our work focus more on bringing standard RLHF to a bilevel optimization framework and propose an effective and efficient approximate algorithm on top of it. We can see some new preference optimization methods including ORPO and SimPO have one fundamental difference from our approach: they do not explicitly incorporate the KL regularization term. The absence of the KL regularization term allows these methods to optimize more aggressively for the reward function by deviating significantly from the reference model. In contrast, our approach is specifically grounded in the standard RLHF, where the KL regularization term ensures that the model remains aligned with the reference distribution while optimizing for the reward function. This distinction makes direct comparisons with ORPO or SimPO less meaningful theoretically, as those methods omit the KL regularization and adopt a fundamentally different optimization objective design.However, we think our work, although developed adhering to the standard RLHF setup, can be compatible and combined with some recent advanced preference optimization algorithms, despite their differences in optimization setups and objectives. This is because we can reformulate their alignment problem as bilevel optimization, and go through the derivation as done in the paper. Taking SimPO as an example, we can treat their reward model definition (Equation (4) in the SimPO paper) as the solution of the upper level optimization (replacing Equation (4) in our manuscript), and adopt their modified Bradley-Terry objective with reward margin (Equation (5) in the SimPO paper) to replace the standard one (Equation (10) in our manuscript). By applying these changes and rederiving the extra gradient terms, we can formulate an adaptation of our method to the SimPO objective. We will implement this combined algorithm, which adapt our methodology to the SimPO objective, and compare with the SimPO as a baseline.Recently many different alignment objectives and algorithms have emerged; it is an interesting question to discuss the compatibility and combination of our method with each objective. We will add more relevant discussions to the appendices, but due to the fact that the compatibility problem with each design is a non-trivial question, this process may incur considerably more work, and we hope the reviewer understands that this effort cannot be fully reflected by the rebuttal period. But we will continue to expand the discussion as the wide compatibility to other designs also strengthens our contribution to the community. We thank the reviewer for raising this insightful point.""}",2024-11-26 15:27:26
