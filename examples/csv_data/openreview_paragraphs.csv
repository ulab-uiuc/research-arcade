venue,paper_openreview_id,paragraph_idx,section,content
ICLR.cc/2025/Conference,zkNCWtw2fd,1,Title,"SYNERGISTIC APPROACH FOR SIMULTANEOUSOPTIMIZATION OF MONOLINGUAL, CROSS-LINGUAL,AND MULTILINGUAL INFORMATION RETRIEVAL"
ICLR.cc/2025/Conference,zkNCWtw2fd,2,Abstract,"Information retrieval across different languages is an increasingly important challenge in natural language processing. Recent approaches based on multilingualpre-trained language models have achieved remarkable success, yet they oftenoptimize for either monolingual, cross-lingual, or multilingual retrieval performance at the expense of others. This paper proposes a novel hybrid batch trainingstrategy to simultaneously improve zero-shot retrieval performance across monolingual, cross-lingual, and multilingual settings while mitigating language bias.The approach fine-tunes multilingual language models using a mix of monolingualand cross-lingual question-answer pair batches sampled based on dataset size.Experiments on XQuAD-R, MLQA-R, and MIRACL benchmark datasets showthat the proposed method consistently achieves comparable or superior resultsin zero-shot retrieval across various languages and retrieval tasks compared tomonolingual-only or cross-lingual-only training. Hybrid batch training also substantially reduces language bias in multilingual retrieval compared to monolingualtraining. These results demonstrate the effectiveness of the proposed approach forlearning language-agnostic representations that enable strong zero-shot retrievalperformance across diverse languages."
ICLR.cc/2025/Conference,zkNCWtw2fd,3,1 INTRODUCTION,"Information retrieval (IR) across different languages is an increasingly important challenge in naturallanguage processing. However, optimizing information retrieval systems for multilingual scenarios isnot a straightforward task, as it requires considering multiple distinct retrieval settings, each withits own set of challenges and requirements, including monolingual retrieval, cross-lingual retrieval,and multilingual retrieval. Monolingual retrieval refers to the task of retrieving documents in thesame language as the user’s query, focusing on developing effective ranking algorithms and relevancematching techniques. Cross-lingual retrieval involves queries and documents in different languages,requiring the system to bridge the language gap by employing techniques such as query translation,document translation, or cross-lingual representation learning. Multilingual retrieval requires thecreation of a single ranked list of documents in multiple languages for a given query, addressingchallenges such as language disparity, varying document lengths, and potential differences in contentquality and relevance across languages while providing users with a unified and coherent ranked listof results."
ICLR.cc/2025/Conference,zkNCWtw2fd,4,1 INTRODUCTION,"Recent approaches to multilingual information retrieval have leveraged multilingual pre-trainedlanguage models such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) to encodequeries and documents (Karpukhin et al., 2020). While these models can transfer relevance matchingcapabilities across languages, their performance tends to underperform on cross-lingual retrievalbenchmarks due to the lack of explicit alignment between languages during pretraining (Zhang et al.,2023). LaREQA, introduced by (Roy et al., 2020), targets strong alignment, requiring semanticallyrelated pairs across languages to be closer in representation space than unrelated pairs within the samelanguage. (Roy et al., 2020) found that augmenting the training data through machine translationproved effective in achieving robust alignment for MLIR. However, this approach compromisesperformance in monolingual retrieval tasks. Alternative approaches using parallel corpora, such asInfoXLM (Chi et al., 2021) and LaBSE (Feng et al., 2022), have been proposed to align sentences across languages. However, the scarcity of parallel data, especially for low-resource languages,remains a substantial challenge. To address these limitations, (Lawrie et al., 2023) introduced aMultilingual Translate-Train approach using translated datasets, (Hu et al., 2023) proposed contrastivelosses to align representations and remove language-specific information, (Huang et al., 2023a)presented a knowledge distillation framework for multilingual dense retrieval, and (Lin et al., 2023a)extended Aggretriever (Lin et al., 2023b) for multilingual retrieval using semantic and lexical features.While the methods proposed in (Hu et al., 2023) and (Huang et al., 2023a) attempt to mitigatelanguage bias, we raise the question: Is there a straightforward approach that addresses this issue bymodifying the training data batches without necessitating the introduction of loss functions or newarchitectural components? In this paper, we propose a novel hybrid batch training strategy that simultaneously optimizes retrievalperformance across monolingual, cross-lingual, and multilingual settings while also mitigatinglanguage bias. Our approach fine-tunes multilingual language models using a balanced mix ofmonolingual and cross-lingual question-answer pair batches. We collect a diverse set of Englishquestion-answer datasets and use machine translation to generate parallel question-answer pairsacross several languages, including low-resource languages where parallel corpora may be limited(Fan et al., 2021; Kim et al., 2021; Costa-juss`a et al., 2022). Our hybrid batch training approachsignificantly reduces the language bias that hinders the performance of multilingual retrieval systemsby training the models on a diverse set of language pairs and encouraging the learning of languageagnostic representations. This mitigates the tendency of models to favor certain languages overothers, ensuring that documents from multiple languages are fairly ranked based on their relevanceto the query, regardless of the language. Extensive experiments on XQuAD-R, MLQA-R, andMIRACL benchmark datasets demonstrate the effectiveness of our proposed approach, with modelstrained using the hybrid batch strategy consistently achieving competitive results in zero-shot retrievalacross various languages and retrieval tasks, outperforming models trained with only monolingual orcross-lingual data. Our approach also exhibits strong zero-shot generalization to unseen languagesnot included in the training data, highlighting its potential to expand the linguistic coverage ofmultilingual information retrieval systems."
ICLR.cc/2025/Conference,zkNCWtw2fd,5,2.1 CONTRASTIVE LEARNING,"Throughout the paper, we utilize the dual-encoder architecture with shared parameters, which iscommonly used for dense retrieval (DR; Ni et al., 2022). Contrastive learning is a method for trainingDR models by contrasting positive pairs against negatives. Specifically, given a batch of triplets, eachof which consists of a query and its relevant and irrelevant documents: (qn, d+n ); 1 ≤ n ≤ |B|.We minimize the InfoNCE loss for each query qn: n , d− L = |B|(cid:88) i=1 − log esθ(qi,d+i )|B|(cid:80)j=1 i ) + esθ(qi,d+ esθ(qi,d−j ) . (1) (a) Proposed hybrid batching We use cosine similarity as the scoring function: sθ(q, d) = cos (Eθ(q), Eθ(d)), where Eθ is theencoder parametrized by θ. Following Wang et al. (2022), we incorporate prefix identifiers “Query:”and “Passage:” for queries and passages, respectively. As shown in prior work (Hofst¨atter et al.,2021; Lin et al., 2021), in-batch negatives mining, the second term of the denominator in Eq (1), playsa crucial role in dense retrieval training. In this work, we study different batch sampling approachesto control in-batch negative mining."
ICLR.cc/2025/Conference,zkNCWtw2fd,6,2.2 BATCH SAMPLING,"Baseline Batch Sampling. We study the following training batching procedures introduced by(Roy et al., 2020). (i) Monolingual batching (coined as X-X-mono model) creates each batch withmono language, where all the triplets consist of queries and passages in the same language. Notethat we sample the language used to create the batch equally among all possible languages in ourtraining data. (ii) Cross-lingual batching (coined as X-Y model) creates each batch, where all thetriplets consist of queries and passages in different languages. Monolingual batching only focuseson contrastive learning for query-passage pairs in the same languages while cross-lingual batchingmines positives and in-batch negatives from diverse languages."
ICLR.cc/2025/Conference,zkNCWtw2fd,7,2.2 BATCH SAMPLING,"As shown in (Roy et al., 2020), the X-Y model is more effective in cross-lingual retrieval scenariosand shows reduced language bias; however, the X-X-mono surpasses the X-Y model in monolingualretrieval. These results inspire us to explore whether simply combining the two batch samplingapproaches can achieve improvement in both monolingual and cross-lingual retrieval effectiveness."
ICLR.cc/2025/Conference,zkNCWtw2fd,8,2.2 BATCH SAMPLING,"Figure 1: Illustrative example of monolingual, cross-lingual, and multilingual information retrieval."
ICLR.cc/2025/Conference,zkNCWtw2fd,9,2.2 BATCH SAMPLING,"Figure 2: Illustrations of the proposed hybrid batch sampling (assuming we only have training datain English, Arabic, and Japanese), where our model is exposed to monolingual and cross-lingualbatches with the respective probability of α and β = 1 − α."
ICLR.cc/2025/Conference,zkNCWtw2fd,10,2.2 BATCH SAMPLING,"Hybrid Batch Sampling.In this work, we propose to combine the two aforementioned baselinesampling strategies. Specifically, when creating batch training data, we set α and β = 1 − α as therespective probability of using monolingual and cross-lingual batching as shown in Fig. 2.1"
