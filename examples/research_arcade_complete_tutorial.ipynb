{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ResearchArcade Complete Tutorial\n",
    "\n",
    "This tutorial demonstrates how to work with the ResearchArcade database, covering all node types and edge relationships.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#setup)\n",
    "2. [OpenReview Data](#openreview)\n",
    "3. [ArXiv Papers](#arxiv-papers)\n",
    "4. [ArXiv Authors](#arxiv-authors)\n",
    "5. [ArXiv Categories](#arxiv-categories)\n",
    "6. [ArXiv Figures](#arxiv-figures)\n",
    "7. [ArXiv Tables](#arxiv-tables)\n",
    "8. [ArXiv Sections](#arxiv-sections)\n",
    "9. [ArXiv Paragraphs](#arxiv-paragraphs)\n",
    "10. [Relationships/Edges](#relationships)\n",
    "11. [Advanced Queries](#advanced-queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## 1. Setup <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from research_arcade.research_arcade import ResearchArcade\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db-backend",
   "metadata": {},
   "source": [
    "### Choose Database Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "csv-backend",
   "metadata": {},
   "source": [
    "#### CSV Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "csv-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_type = \"csv\"\n",
    "config = {\n",
    "    \"csv_dir\": \"../data/my_research_arcade_data/\"\n",
    "}\n",
    "\n",
    "research_arcade = ResearchArcade(db_type=db_type, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sql-backend",
   "metadata": {},
   "source": [
    "#### SQL Based (PostgreSQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sql-config",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "connection to server at \"localhost\" (::1), port 5432 failed: fe_sendauth: no password supplied\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m db_type = \u001b[33m\"\u001b[39m\u001b[33msql\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m config = {\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhost\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mlocalhost\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdbname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33miclr_openreview_database\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mport\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m5432\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m research_arcade = \u001b[43mResearchArcade\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/uiuc/research-arcade/research_arcade/research_arcade.py:98\u001b[39m, in \u001b[36mResearchArcade.__init__\u001b[39m\u001b[34m(self, db_type, config)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28mself\u001b[39m.arxiv_paragraph_reference = SQLArxivParagraphReference(**config)\n\u001b[32m     95\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[33;03mBelow is the openreview dataset\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28mself\u001b[39m.openreview_arxiv = \u001b[43mSQLOpenReviewArxiv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28mself\u001b[39m.openreview_authors = SQLOpenReviewAuthors(**config)\n\u001b[32m    100\u001b[39m \u001b[38;5;28mself\u001b[39m.openreview_papers_authors = SQLOpenReviewPapersAuthors(**config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Research/uiuc/research-arcade/research_arcade/sql_database/sql_openreview_arxiv.py:12\u001b[39m, in \u001b[36mSQLOpenReviewArxiv.__init__\u001b[39m\u001b[34m(self, host, dbname, user, password, port)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, host: \u001b[38;5;28mstr\u001b[39m, dbname: \u001b[38;5;28mstr\u001b[39m, user: \u001b[38;5;28mstr\u001b[39m, password: \u001b[38;5;28mstr\u001b[39m, port: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Store connection and cursor for reuse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28mself\u001b[39m.conn = \u001b[43mpsycopg2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdbname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mport\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Enable autocommit\u001b[39;00m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mself\u001b[39m.conn.autocommit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/research_arcade/lib/python3.12/site-packages/psycopg2/__init__.py:122\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     kwasync[\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m] = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    121\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    124\u001b[39m     conn.cursor_factory = cursor_factory\n",
      "\u001b[31mOperationalError\u001b[39m: connection to server at \"localhost\" (::1), port 5432 failed: fe_sendauth: no password supplied\n"
     ]
    }
   ],
   "source": [
    "db_type = \"sql\"\n",
    "config = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"dbname\": \"iclr_openreview_database\",\n",
    "    \"user\": \"jingjunx\",\n",
    "    \"password\": \"\",\n",
    "    \"port\": \"5432\"\n",
    "}\n",
    "\n",
    "research_arcade = ResearchArcade(db_type=db_type, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-papers-section",
   "metadata": {},
   "source": [
    "## 3. ArXiv Papers <a name=\"arxiv-papers\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `arxiv_id` (VARCHAR, unique) - e.g., 1802.08773v3\n",
    "- `base_arxiv_id` (VARCHAR) - e.g., 1802.08773\n",
    "- `version` (INT) - e.g., 3\n",
    "- `title` (TEXT)\n",
    "- `abstract` (TEXT)\n",
    "- `submit_date` (DATE)\n",
    "- `metadata` (JSONB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8e215",
   "metadata": {},
   "source": [
    "Construct Table from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaeefb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"start time\": \"\", \"end time\": \"\"} # TO BE FILLED IN\n",
    "research_arcade.construct_table_from_api(\"papers\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-papers-insert",
   "metadata": {},
   "source": [
    "### Insert a Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "insert-paper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Insert the famous \"Attention is All You Need\" paper\n",
    "new_paper = {\n",
    "    'arxiv_id': '1706.03762v7',\n",
    "    'base_arxiv_id': '1706.03762',\n",
    "    'version': 7,\n",
    "    'title': 'Attention Is All You Need',\n",
    "    'abstract': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.',\n",
    "    'submit_date': '2017-06-12',\n",
    "    'metadata': {'venue': 'NeurIPS 2017', 'pdf_url': 'https://arxiv.org/pdf/1706.03762.pdf'}\n",
    "}\n",
    "\n",
    "research_arcade.insert_node(\"arxiv_papers\", node_features=new_paper)\n",
    "print(\"Paper inserted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "insert-paper-bert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT paper inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Insert BERT paper\n",
    "bert_paper = {\n",
    "    'arxiv_id': '1810.04805v2',\n",
    "    'base_arxiv_id': '1810.04805',\n",
    "    'version': 2,\n",
    "    'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n",
    "    'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.',\n",
    "    'submit_date': '2018-10-11',\n",
    "    'metadata': {'venue': 'NAACL 2019', 'citations': 50000}\n",
    "}\n",
    "\n",
    "research_arcade.insert_node(\"arxiv_papers\", node_features=bert_paper)\n",
    "print(\"BERT paper inserted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-papers-get-all",
   "metadata": {},
   "source": [
    "### Get All Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "get-all-papers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers in database: 4\n",
      "\n",
      "First 5 papers:\n",
      "   id      arxiv_id  base_arxiv_id  version  \\\n",
      "0   2  1810.04805v2     1810.04805        2   \n",
      "1   3   1409.0473v7     1409.04730        7   \n",
      "2   4  1512.03385v1     1512.03385        1   \n",
      "3   5  1706.03762v7     1706.03762        7   \n",
      "\n",
      "                                               title  \\\n",
      "0  BERT: Pre-training of Deep Bidirectional Trans...   \n",
      "1  Neural Machine Translation by Jointly Learning...   \n",
      "2       Deep Residual Learning for Image Recognition   \n",
      "3                          Attention Is All You Need   \n",
      "\n",
      "                                            abstract submit_date  \\\n",
      "0  We introduce a new language representation mod...  2018-10-11   \n",
      "1  Neural machine translation is a recently propo...  2014-09-01   \n",
      "2  Deeper neural networks are more difficult to t...  2015-12-10   \n",
      "3  The dominant sequence transduction models are ...  2017-06-12   \n",
      "\n",
      "                                            metadata  \n",
      "0        {\"venue\": \"NAACL 2019\", \"citations\": 50000}  \n",
      "1                             {\"venue\": \"ICLR 2015\"}  \n",
      "2                             {\"venue\": \"CVPR 2016\"}  \n",
      "3  {\"venue\": \"NeurIPS 2017\", \"pdf_url\": \"https://...  \n"
     ]
    }
   ],
   "source": [
    "arxiv_papers_df = research_arcade.get_all_node_features(\"arxiv_papers\")\n",
    "print(f\"Total papers in database: {len(arxiv_papers_df)}\")\n",
    "print(\"\\nFirst 5 papers:\")\n",
    "print(arxiv_papers_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-papers-get-by-id",
   "metadata": {},
   "source": [
    "### Get Specific Paper by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "get-paper-by-id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper details:\n",
      "{'id': 5, 'arxiv_id': '1706.03762v7', 'base_arxiv_id': 1706.03762, 'version': 7, 'title': 'Attention Is All You Need', 'abstract': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.', 'submit_date': '2017-06-12', 'metadata': '{\"venue\": \"NeurIPS 2017\", \"pdf_url\": \"https://arxiv.org/pdf/1706.03762.pdf\"}'}\n"
     ]
    }
   ],
   "source": [
    "paper_id = {\"arxiv_id\": \"1706.03762v7\"}\n",
    "paper_features = research_arcade.get_node_features_by_id(\"arxiv_papers\", paper_id)\n",
    "print(\"Paper details:\")\n",
    "print(paper_features.to_dict(orient=\"records\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-papers-update",
   "metadata": {},
   "source": [
    "### Update a Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "update-paper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper updated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Update metadata for a paper\n",
    "updated_paper = {\n",
    "    'arxiv_id': '1706.03762v7',\n",
    "    'metadata': {\n",
    "        'venue': 'NeurIPS 2017',\n",
    "        'pdf_url': 'https://arxiv.org/pdf/1706.03762.pdf',\n",
    "        'citations': 75000,\n",
    "        'influential': True\n",
    "    }\n",
    "}\n",
    "\n",
    "research_arcade.update_node(\"arxiv_papers\", node_features=updated_paper)\n",
    "print(\"Paper updated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-papers-delete",
   "metadata": {},
   "source": [
    "### Delete a Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "delete-paper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted paper:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Delete a paper by ID\n",
    "paper_id = {\"arxiv_id\": \"1706.03762v7\"}\n",
    "deleted_paper = research_arcade.delete_node_by_id(\"arxiv_papers\", paper_id)\n",
    "print(\"Deleted paper:\")\n",
    "print(deleted_paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-authors-section",
   "metadata": {},
   "source": [
    "## 4. ArXiv Authors <a name=\"arxiv-authors\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `semantic_scholar_id` (VARCHAR, unique)\n",
    "- `name` (VARCHAR)\n",
    "- `homepage` (VARCHAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-authors-insert",
   "metadata": {},
   "source": [
    "### Insert Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "insert-authors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted author: Ashish Vaswani\n",
      "Inserted author: Noam Shazeer\n",
      "Inserted author: Niki Parmar\n",
      "Inserted author: Jakob Uszkoreit\n",
      "Inserted author: Llion Jones\n"
     ]
    }
   ],
   "source": [
    "# Insert authors from the Transformer paper\n",
    "authors = [\n",
    "    {\n",
    "        'semantic_scholar_id': 'ss_ashish_vaswani',\n",
    "        'name': 'Ashish Vaswani',\n",
    "        'homepage': 'https://scholar.google.com/citations?user=oR9sCGYAAAAJ'\n",
    "    },\n",
    "    {\n",
    "        'semantic_scholar_id': 'ss_noam_shazeer',\n",
    "        'name': 'Noam Shazeer',\n",
    "        'homepage': 'https://scholar.google.com/citations?user=oR9sCGYAAAAJ'\n",
    "    },\n",
    "    {\n",
    "        'semantic_scholar_id': 'ss_niki_parmar',\n",
    "        'name': 'Niki Parmar',\n",
    "        'homepage': 'https://scholar.google.com/citations?user=oR9sCGYAAAAJ'\n",
    "    },\n",
    "    {\n",
    "        'semantic_scholar_id': 'ss_jakob_uszkoreit',\n",
    "        'name': 'Jakob Uszkoreit',\n",
    "        'homepage': 'https://scholar.google.com/citations?user=oR9sCGYAAAAJ'\n",
    "    },\n",
    "    {\n",
    "        'semantic_scholar_id': 'ss_llion_jones',\n",
    "        'name': 'Llion Jones',\n",
    "        'homepage': 'https://scholar.google.com/citations?user=oR9sCGYAAAAJ'\n",
    "    }\n",
    "]\n",
    "\n",
    "for author in authors:\n",
    "    research_arcade.insert_node(\"arxiv_authors\", node_features=author)\n",
    "    print(f\"Inserted author: {author['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-authors-get-all",
   "metadata": {},
   "source": [
    "### Get All Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "get-all-authors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total authors in database: 5\n",
      "\n",
      "All authors:\n",
      "   id semantic_scholar_id             name  \\\n",
      "0   1   ss_ashish_vaswani   Ashish Vaswani   \n",
      "1   2     ss_noam_shazeer     Noam Shazeer   \n",
      "2   3      ss_niki_parmar      Niki Parmar   \n",
      "3   4  ss_jakob_uszkoreit  Jakob Uszkoreit   \n",
      "4   5      ss_llion_jones      Llion Jones   \n",
      "\n",
      "                                            homepage  \n",
      "0                          https://ashishvaswani.com  \n",
      "1  https://scholar.google.com/citations?user=oR9s...  \n",
      "2  https://scholar.google.com/citations?user=oR9s...  \n",
      "3  https://scholar.google.com/citations?user=oR9s...  \n",
      "4  https://scholar.google.com/citations?user=oR9s...  \n"
     ]
    }
   ],
   "source": [
    "authors_df = research_arcade.get_all_node_features(\"arxiv_authors\")\n",
    "print(f\"Total authors in database: {len(authors_df)}\")\n",
    "print(\"\\nAll authors:\")\n",
    "print(authors_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-authors-get-by-id",
   "metadata": {},
   "source": [
    "### Get Specific Author by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "get-author-by-id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author details:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "author_id = {\"semantic_scholar_id\": \"ss_ashish_vaswani\"}\n",
    "author_features = research_arcade.get_node_features_by_id(\"arxiv_authors\", author_id)\n",
    "print(\"Author details:\")\n",
    "print(author_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-authors-update",
   "metadata": {},
   "source": [
    "### Update an Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "update-author",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author updated successfully!\n"
     ]
    }
   ],
   "source": [
    "updated_author = {\n",
    "    'semantic_scholar_id': 'ss_ashish_vaswani',\n",
    "    'homepage': 'https://ashishvaswani.com'\n",
    "}\n",
    "\n",
    "research_arcade.update_node(\"arxiv_authors\", node_features=updated_author)\n",
    "print(\"Author updated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-categories-section",
   "metadata": {},
   "source": [
    "## 5. ArXiv Categories <a name=\"arxiv-categories\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `name` (VARCHAR, unique)\n",
    "- `description` (TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-categories-insert",
   "metadata": {},
   "source": [
    "### Insert Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "insert-categories",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted category: cs.CL\n",
      "Inserted category: cs.LG\n",
      "Inserted category: cs.AI\n",
      "Inserted category: cs.CV\n",
      "Inserted category: stat.ML\n"
     ]
    }
   ],
   "source": [
    "categories = [\n",
    "    {\n",
    "        'name': 'cs.CL',\n",
    "        'description': 'Computation and Language (Natural Language Processing)'\n",
    "    },\n",
    "    {\n",
    "        'name': 'cs.LG',\n",
    "        'description': 'Machine Learning'\n",
    "    },\n",
    "    {\n",
    "        'name': 'cs.AI',\n",
    "        'description': 'Artificial Intelligence'\n",
    "    },\n",
    "    {\n",
    "        'name': 'cs.CV',\n",
    "        'description': 'Computer Vision and Pattern Recognition'\n",
    "    },\n",
    "    {\n",
    "        'name': 'stat.ML',\n",
    "        'description': 'Machine Learning (Statistics)'\n",
    "    }\n",
    "]\n",
    "\n",
    "for category in categories:\n",
    "    research_arcade.insert_node(\"arxiv_categories\", node_features=category)\n",
    "    print(f\"Inserted category: {category['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-categories-get-all",
   "metadata": {},
   "source": [
    "### Get All Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "get-all-categories",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total categories: 5\n",
      "\n",
      "All categories:\n",
      "   id     name                                        description\n",
      "0   1    cs.CL  Computation and Language (Natural Language Pro...\n",
      "1   2    cs.LG                                   Machine Learning\n",
      "2   3    cs.AI                            Artificial Intelligence\n",
      "3   4    cs.CV            Computer Vision and Pattern Recognition\n",
      "4   5  stat.ML                      Machine Learning (Statistics)\n"
     ]
    }
   ],
   "source": [
    "categories_df = research_arcade.get_all_node_features(\"arxiv_categories\")\n",
    "print(f\"Total categories: {len(categories_df)}\")\n",
    "print(\"\\nAll categories:\")\n",
    "print(categories_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-figures-section",
   "metadata": {},
   "source": [
    "## 6. ArXiv Figures <a name=\"arxiv-figures\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `paper_arxiv_id` (VARCHAR FK → papers.arxiv_id)\n",
    "- `path` (VARCHAR)\n",
    "- `caption` (TEXT)\n",
    "- `label` (TEXT)\n",
    "- `name` (TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-figures-insert",
   "metadata": {},
   "source": [
    "### Insert Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "insert-figures",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted Figure 1\n",
      "Inserted Figure 2\n",
      "Inserted Figure 3\n"
     ]
    }
   ],
   "source": [
    "# Insert figures for the Transformer paper\n",
    "figures = [\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'path': '/figures/transformer_architecture.png',\n",
    "        'caption': 'The Transformer model architecture. The left side shows the encoder stack and the right side shows the decoder stack.',\n",
    "        'label': 'fig:architecture',\n",
    "        'name': 'Figure 1'\n",
    "    },\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'path': '/figures/scaled_dot_product_attention.png',\n",
    "        'caption': 'Scaled Dot-Product Attention and Multi-Head Attention mechanisms.',\n",
    "        'label': 'fig:attention',\n",
    "        'name': 'Figure 2'\n",
    "    },\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'path': '/figures/positional_encoding.png',\n",
    "        'caption': 'Positional encoding visualization showing sine and cosine functions of different frequencies.',\n",
    "        'label': 'fig:positional',\n",
    "        'name': 'Figure 3'\n",
    "    }\n",
    "]\n",
    "\n",
    "for figure in figures:\n",
    "    research_arcade.insert_node(\"arxiv_figures\", node_features=figure)\n",
    "    print(f\"Inserted {figure['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-figures-get-all",
   "metadata": {},
   "source": [
    "### Get All Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "get-all-figures",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total figures: 3\n",
      "\n",
      "All figures:\n",
      "       name                                            caption  \\\n",
      "0  Figure 1  The Transformer model architecture. The left s...   \n",
      "1  Figure 2  Scaled Dot-Product Attention and Multi-Head At...   \n",
      "2  Figure 3  Positional encoding visualization showing sine...   \n",
      "\n",
      "              label  \n",
      "0  fig:architecture  \n",
      "1     fig:attention  \n",
      "2    fig:positional  \n"
     ]
    }
   ],
   "source": [
    "figures_df = research_arcade.get_all_node_features(\"arxiv_figures\")\n",
    "print(f\"Total figures: {len(figures_df)}\")\n",
    "print(\"\\nAll figures:\")\n",
    "print(figures_df[['name', 'caption', 'label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-tables-section",
   "metadata": {},
   "source": [
    "## 7. ArXiv Tables <a name=\"arxiv-tables\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `paper_arxiv_id` (VARCHAR FK → papers.arxiv_id)\n",
    "- `path` (VARCHAR)\n",
    "- `caption` (TEXT)\n",
    "- `label` (TEXT)\n",
    "- `table_text` (TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-tables-insert",
   "metadata": {},
   "source": [
    "### Insert Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "insert-tables",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted table: tab:variations\n",
      "Inserted table: tab:wmt\n",
      "Inserted table: tab:parsing\n"
     ]
    }
   ],
   "source": [
    "# Insert tables for the Transformer paper\n",
    "tables = [\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'path': '/tables/model_variations.tex',\n",
    "        'caption': 'Variations on the Transformer architecture with different hyperparameters.',\n",
    "        'label': 'tab:variations',\n",
    "        'table_text': 'Model | N | d_model | d_ff | h | d_k | d_v | P_drop | train time\\nbase | 6 | 512 | 2048 | 8 | 64 | 64 | 0.1 | 12 hrs'\n",
    "    },\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'path': '/tables/wmt_results.tex',\n",
    "        'caption': 'Performance of the Transformer on WMT 2014 English-German and English-French translation tasks.',\n",
    "        'label': 'tab:wmt',\n",
    "        'table_text': 'Model | EN-DE BLEU | EN-FR BLEU\\nTransformer (base) | 27.3 | 38.1\\nTransformer (big) | 28.4 | 41.8'\n",
    "    },\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'path': '/tables/parsing_results.tex',\n",
    "        'caption': 'English constituency parsing results on WSJ test set.',\n",
    "        'label': 'tab:parsing',\n",
    "        'table_text': 'Model | WSJ 23 F1\\nTransformer | 91.3'\n",
    "    }\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "    research_arcade.insert_node(\"arxiv_tables\", node_features=table)\n",
    "    print(f\"Inserted table: {table['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-tables-get-all",
   "metadata": {},
   "source": [
    "### Get All Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "get-all-tables",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tables: 3\n",
      "\n",
      "All tables:\n",
      "            label                                            caption\n",
      "0  tab:variations  Variations on the Transformer architecture wit...\n",
      "1         tab:wmt  Performance of the Transformer on WMT 2014 Eng...\n",
      "2     tab:parsing  English constituency parsing results on WSJ te...\n"
     ]
    }
   ],
   "source": [
    "tables_df = research_arcade.get_all_node_features(\"arxiv_tables\")\n",
    "print(f\"Total tables: {len(tables_df)}\")\n",
    "print(\"\\nAll tables:\")\n",
    "print(tables_df[['label', 'caption']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-sections-section",
   "metadata": {},
   "source": [
    "## 8. ArXiv Sections <a name=\"arxiv-sections\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `content` (TEXT)\n",
    "- `title` (TEXT)\n",
    "- `appendix` (BOOLEAN)\n",
    "- `paper_arxiv_id` (VARCHAR FK → papers.arxiv_id)\n",
    "- `section_in_paper_id` (INT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-sections-insert",
   "metadata": {},
   "source": [
    "### Insert Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "insert-sections",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted section: Introduction\n",
      "Inserted section: Background\n",
      "Inserted section: Model Architecture\n",
      "Inserted section: Training\n",
      "Inserted section: Results\n",
      "Inserted section: Conclusion\n"
     ]
    }
   ],
   "source": [
    "# Insert sections for the Transformer paper\n",
    "sections = [\n",
    "    {\n",
    "        'content': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder...',\n",
    "        'title': 'Introduction',\n",
    "        'appendix': False,\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'section_in_paper_id': 1\n",
    "    },\n",
    "    {\n",
    "        'content': 'Most competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps an input sequence of symbol representations...',\n",
    "        'title': 'Background',\n",
    "        'appendix': False,\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'section_in_paper_id': 2\n",
    "    },\n",
    "    {\n",
    "        'content': 'Most neural sequence transduction models have an encoder-decoder structure. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers...',\n",
    "        'title': 'Model Architecture',\n",
    "        'appendix': False,\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'section_in_paper_id': 3\n",
    "    },\n",
    "    {\n",
    "        'content': 'In this section we describe the training regime for our models...',\n",
    "        'title': 'Training',\n",
    "        'appendix': False,\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'section_in_paper_id': 4\n",
    "    },\n",
    "    {\n",
    "        'content': 'On the WMT 2014 English-to-German translation task, the big transformer model outperforms the best previously reported models...',\n",
    "        'title': 'Results',\n",
    "        'appendix': False,\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'section_in_paper_id': 5\n",
    "    },\n",
    "    {\n",
    "        'content': 'In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers...',\n",
    "        'title': 'Conclusion',\n",
    "        'appendix': False,\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'section_in_paper_id': 6\n",
    "    }\n",
    "]\n",
    "\n",
    "for section in sections:\n",
    "    research_arcade.insert_node(\"arxiv_sections\", node_features=section)\n",
    "    print(f\"Inserted section: {section['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-sections-get-all",
   "metadata": {},
   "source": [
    "### Get All Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "get-all-sections",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sections:    id                                            content               title  \\\n",
      "0   1  The dominant sequence transduction models are ...        Introduction   \n",
      "1   2  Most competitive neural sequence transduction ...          Background   \n",
      "2   3  Most neural sequence transduction models have ...  Model Architecture   \n",
      "3   4  In this section we describe the training regim...            Training   \n",
      "4   5  On the WMT 2014 English-to-German translation ...             Results   \n",
      "5   6  In this work, we presented the Transformer, th...          Conclusion   \n",
      "\n",
      "   appendix paper_arxiv_id  section_in_paper_id  \n",
      "0     False   1706.03762v7                  1.0  \n",
      "1     False   1706.03762v7                  2.0  \n",
      "2     False   1706.03762v7                  3.0  \n",
      "3     False   1706.03762v7                  4.0  \n",
      "4     False   1706.03762v7                  5.0  \n",
      "5     False   1706.03762v7                  6.0  \n",
      "\n",
      "All sections:\n",
      "                title  section_in_paper_id  appendix\n",
      "0        Introduction                  1.0     False\n",
      "1          Background                  2.0     False\n",
      "2  Model Architecture                  3.0     False\n",
      "3            Training                  4.0     False\n",
      "4             Results                  5.0     False\n",
      "5          Conclusion                  6.0     False\n"
     ]
    }
   ],
   "source": [
    "sections_df = research_arcade.get_all_node_features(\"arxiv_sections\")\n",
    "print(f\"Total sections: {sections_df}\")\n",
    "print(\"\\nAll sections:\")\n",
    "print(sections_df[['title', 'section_in_paper_id', 'appendix']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-paragraphs-section",
   "metadata": {},
   "source": [
    "## 9. ArXiv Paragraphs <a name=\"arxiv-paragraphs\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `paragraph_id` (INT)\n",
    "- `content` (TEXT)\n",
    "- `paper_arxiv_id` (VARCHAR FK → papers.arxiv_id)\n",
    "- `paper_section` (TEXT)\n",
    "- `section_id` (INT)\n",
    "- `paragraph_in_paper_id` (INT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-paragraphs-insert",
   "metadata": {},
   "source": [
    "### Insert Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "insert-paragraphs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted paragraph 1 from Introduction\n",
      "Inserted paragraph 2 from Introduction\n",
      "Inserted paragraph 3 from Introduction\n",
      "Inserted paragraph 4 from Introduction\n",
      "Inserted paragraph 5 from Introduction\n"
     ]
    }
   ],
   "source": [
    "# Insert paragraphs from the Introduction section\n",
    "paragraphs = [\n",
    "    {\n",
    "        'paragraph_id': 1,\n",
    "        'content': 'Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation.',\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'paper_section': 'Introduction',\n",
    "        'section_id': 1,\n",
    "        'paragraph_in_paper_id': 1\n",
    "    },\n",
    "    {\n",
    "        'paragraph_id': 2,\n",
    "        'content': 'Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures. Recurrent models typically factor computation along the symbol positions of the input and output sequences.',\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'paper_section': 'Introduction',\n",
    "        'section_id': 1,\n",
    "        'paragraph_in_paper_id': 2\n",
    "    },\n",
    "    {\n",
    "        'paragraph_id': 3,\n",
    "        'content': 'Aligning the positions to steps in computation time, they generate a sequence of hidden states h_t, as a function of the previous hidden state h_{t-1} and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.',\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'paper_section': 'Introduction',\n",
    "        'section_id': 1,\n",
    "        'paragraph_in_paper_id': 3\n",
    "    },\n",
    "    {\n",
    "        'paragraph_id': 4,\n",
    "        'content': 'Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences.',\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'paper_section': 'Introduction',\n",
    "        'section_id': 1,\n",
    "        'paragraph_in_paper_id': 4\n",
    "    },\n",
    "    {\n",
    "        'paragraph_id': 5,\n",
    "        'content': 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.',\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'paper_section': 'Introduction',\n",
    "        'section_id': 1,\n",
    "        'paragraph_in_paper_id': 5\n",
    "    }\n",
    "]\n",
    "\n",
    "for paragraph in paragraphs:\n",
    "    research_arcade.insert_node(\"arxiv_paragraphs\", node_features=paragraph)\n",
    "    print(f\"Inserted paragraph {paragraph['paragraph_id']} from {paragraph['paper_section']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-paragraphs-get-all",
   "metadata": {},
   "source": [
    "### Get All Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "get-all-paragraphs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total paragraphs: 5\n",
      "\n",
      "First 3 paragraphs:\n",
      "   paragraph_id paper_section  \\\n",
      "0             1  Introduction   \n",
      "1             2  Introduction   \n",
      "2             3  Introduction   \n",
      "\n",
      "                                             content  \n",
      "0  Recurrent neural networks, long short-term mem...  \n",
      "1  Numerous efforts have since continued to push ...  \n",
      "2  Aligning the positions to steps in computation...  \n"
     ]
    }
   ],
   "source": [
    "paragraphs_df = research_arcade.get_all_node_features(\"arxiv_paragraphs\")\n",
    "print(f\"Total paragraphs: {len(paragraphs_df)}\")\n",
    "print(\"\\nFirst 3 paragraphs:\")\n",
    "print(paragraphs_df[['paragraph_id', 'paper_section', 'content']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relationships-section",
   "metadata": {},
   "source": [
    "## 10. Relationships/Edges <a name=\"relationships\"></a>\n",
    "\n",
    "This section demonstrates how to create relationships between different entities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paper-authors-edge",
   "metadata": {},
   "source": [
    "### Paper-Author Relationships (arxiv_paper_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "insert-paper-authors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table arxiv_paper_authors not found.\n",
      "Linked author ss_ashish_vaswani to paper (position 1)\n",
      "Table arxiv_paper_authors not found.\n",
      "Linked author ss_noam_shazeer to paper (position 2)\n",
      "Table arxiv_paper_authors not found.\n",
      "Linked author ss_niki_parmar to paper (position 3)\n",
      "Table arxiv_paper_authors not found.\n",
      "Linked author ss_jakob_uszkoreit to paper (position 4)\n",
      "Table arxiv_paper_authors not found.\n",
      "Linked author ss_llion_jones to paper (position 5)\n"
     ]
    }
   ],
   "source": [
    "# Create authorship relationships for the Transformer paper\n",
    "paper_authors = [\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'author_id': 'ss_ashish_vaswani',\n",
    "        'author_sequence': 1\n",
    "    },\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'author_id': 'ss_noam_shazeer',\n",
    "        'author_sequence': 2\n",
    "    },\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'author_id': 'ss_niki_parmar',\n",
    "        'author_sequence': 3\n",
    "    },\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'author_id': 'ss_jakob_uszkoreit',\n",
    "        'author_sequence': 4\n",
    "    },\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'author_id': 'ss_llion_jones',\n",
    "        'author_sequence': 5\n",
    "    }\n",
    "]\n",
    "\n",
    "for relation in paper_authors:\n",
    "    research_arcade.insert_edge(\"arxiv_paper_authors\", edge_features=relation)\n",
    "    print(f\"Linked author {relation['author_id']} to paper (position {relation['author_sequence']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paper-categories-edge",
   "metadata": {},
   "source": [
    "### Paper-Category Relationships (arxiv_paper_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial has covered:\n",
    "\n",
    "1. Setting up the ResearchArcade database connection\n",
    "2. Working with OpenReview data\n",
    "3. CRUD operations for all ArXiv entity types:\n",
    "   - Papers\n",
    "   - Authors\n",
    "   - Categories\n",
    "   - Figures\n",
    "   - Tables\n",
    "   - Sections\n",
    "   - Paragraphs\n",
    "4. Creating relationships between entities:\n",
    "   - Authorship\n",
    "   - Citations\n",
    "   - Paper-Category links\n",
    "   - Paper-Figure/Table links\n",
    "   - Paragraph-level references\n",
    "5. Advanced querying patterns\n",
    "6. Best practices for data validation\n",
    "\n",
    "For more information, refer to the ResearchArcade documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_arcade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
