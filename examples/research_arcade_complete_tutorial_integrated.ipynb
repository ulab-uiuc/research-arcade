{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ResearchArcade Complete Tutorial\n",
    "\n",
    "This tutorial demonstrates how to work with the ResearchArcade database, covering all node types and edge relationships.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#setup)\n",
    "2. [OpenReview Data](#openreview)\n",
    "3. [ArXiv Papers](#arxiv-papers)\n",
    "4. [ArXiv Authors](#arxiv-authors)\n",
    "5. [ArXiv Categories](#arxiv-categories)\n",
    "6. [ArXiv Figures](#arxiv-figures)\n",
    "7. [ArXiv Tables](#arxiv-tables)\n",
    "8. [ArXiv Sections](#arxiv-sections)\n",
    "9. [ArXiv Paragraphs](#arxiv-paragraphs)\n",
    "10. [Relationships/Edges](#relationships)\n",
    "11. [Advanced Queries](#advanced-queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## 1. Setup <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05b9b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from research_arcade.research_arcade import ResearchArcade\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f1a1e4",
   "metadata": {},
   "source": [
    "### Choose Database Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9672a27",
   "metadata": {},
   "source": [
    "#### CSV Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169f7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_type = \"csv\"\n",
    "config = {\n",
    "    \"csv_dir\": \"../data/my_research_arcade_data/\"\n",
    "}\n",
    "\n",
    "research_arcade = ResearchArcade(db_type=db_type, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-papers-section",
   "metadata": {},
   "source": [
    "## 3. ArXiv Papers <a name=\"arxiv-papers\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `arxiv_id` (VARCHAR, unique) - e.g., 1802.08773v3\n",
    "- `base_arxiv_id` (VARCHAR) - e.g., 1802.08773\n",
    "- `version` (INT) - e.g., 3\n",
    "- `title` (TEXT)\n",
    "- `abstract` (TEXT)\n",
    "- `submit_date` (DATE)\n",
    "- `metadata` (JSONB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8e215",
   "metadata": {},
   "source": [
    "### Construct Table from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccaeefb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"arxiv_ids\": [\"1806.08804v4\", \"1903.03894v4\"], \"dest_dir\": \"./download\"}\n",
    "research_arcade.construct_table_from_api(\"arxiv_papers\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-papers-insert",
   "metadata": {},
   "source": [
    "### Insert a Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "insert-paper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Insert the famous \"Attention is All You Need\" paper\n",
    "new_paper = {\n",
    "    'arxiv_id': '1706.03762v7',\n",
    "    'base_arxiv_id': '1706.03762',\n",
    "    'version': 7,\n",
    "    'title': 'Attention Is All You Need',\n",
    "    'abstract': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.',\n",
    "    'submit_date': '2017-06-12',\n",
    "    'metadata': {'venue': 'NeurIPS 2017', 'pdf_url': 'https://arxiv.org/pdf/1706.03762.pdf'}\n",
    "}\n",
    "\n",
    "research_arcade.insert_node(\"arxiv_papers\", node_features=new_paper)\n",
    "print(\"Paper inserted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "insert-paper-bert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT paper inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Insert BERT paper\n",
    "bert_paper = {\n",
    "    'arxiv_id': '1810.04805v2',\n",
    "    'base_arxiv_id': '1810.04805',\n",
    "    'version': 2,\n",
    "    'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n",
    "    'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.',\n",
    "    'submit_date': '2018-10-11',\n",
    "    'metadata': {'venue': 'NAACL 2019', 'citations': 50000}\n",
    "}\n",
    "\n",
    "research_arcade.insert_node(\"arxiv_papers\", node_features=bert_paper)\n",
    "print(\"BERT paper inserted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-papers-get-all",
   "metadata": {},
   "source": [
    "### Get All Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "get-all-papers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers in database: 6\n",
      "\n",
      "First 5 papers:\n",
      "   id      arxiv_id  base_arxiv_id  version  \\\n",
      "0   2  1810.04805v2     1810.04805        2   \n",
      "1   3   1409.0473v7     1409.04730        7   \n",
      "2   4  1512.03385v1     1512.03385        1   \n",
      "3   5  1806.08804v4     1806.08804        4   \n",
      "4   6  1903.03894v4     1903.03894        4   \n",
      "\n",
      "                                               title  \\\n",
      "0  BERT: Pre-training of Deep Bidirectional Trans...   \n",
      "1  Neural Machine Translation by Jointly Learning...   \n",
      "2       Deep Residual Learning for Image Recognition   \n",
      "3  Hierarchical Graph Representation Learning wit...   \n",
      "4  GNNExplainer: Generating Explanations for Grap...   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  We introduce a new language representation mod...   \n",
      "1  Neural machine translation is a recently propo...   \n",
      "2  Deeper neural networks are more difficult to t...   \n",
      "3  Recently, graph neural networks (GNNs) have re...   \n",
      "4  Graph Neural Networks (GNNs) are a powerful to...   \n",
      "\n",
      "                 submit_date  \\\n",
      "0                 2018-10-11   \n",
      "1                 2014-09-01   \n",
      "2                 2015-12-10   \n",
      "3  2018-06-22 18:04:46+00:00   \n",
      "4  2019-03-10 00:56:26+00:00   \n",
      "\n",
      "                                            metadata  \n",
      "0        {\"venue\": \"NAACL 2019\", \"citations\": 50000}  \n",
      "1                             {\"venue\": \"ICLR 2015\"}  \n",
      "2                             {\"venue\": \"CVPR 2016\"}  \n",
      "3  {\"id\": \"1806.08804v4\", \"title\": \"Hierarchical ...  \n",
      "4  {\"id\": \"1903.03894v4\", \"title\": \"GNNExplainer:...  \n"
     ]
    }
   ],
   "source": [
    "arxiv_papers_df = research_arcade.get_all_node_features(\"arxiv_papers\")\n",
    "print(f\"Total papers in database: {len(arxiv_papers_df)}\")\n",
    "print(\"\\nFirst 5 papers:\")\n",
    "print(arxiv_papers_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-papers-get-by-id",
   "metadata": {},
   "source": [
    "### Get Specific Paper by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "get-paper-by-id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper details:\n",
      "{'id': 2, 'arxiv_id': '1810.04805v2', 'base_arxiv_id': 1810.04805, 'version': 2, 'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.', 'submit_date': '2018-10-11', 'metadata': '{\"venue\": \"NAACL 2019\", \"citations\": 50000}'}\n"
     ]
    }
   ],
   "source": [
    "paper_id = {\"arxiv_id\": \"1810.04805v2\"}\n",
    "paper_features = research_arcade.get_node_features_by_id(\"arxiv_papers\", paper_id)\n",
    "print(\"Paper details:\")\n",
    "print(paper_features.to_dict(orient=\"records\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-papers-update",
   "metadata": {},
   "source": [
    "### Update a Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "update-paper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper updated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Update metadata for a paper\n",
    "updated_paper = {\n",
    "    'arxiv_id': '1706.03762v7',\n",
    "    'metadata': {\n",
    "        'venue': 'NeurIPS 2017',\n",
    "        'pdf_url': 'https://arxiv.org/pdf/1706.03762.pdf',\n",
    "        'citations': 75000,\n",
    "        'influential': True\n",
    "    }\n",
    "}\n",
    "\n",
    "research_arcade.update_node(\"arxiv_papers\", node_features=updated_paper)\n",
    "print(\"Paper updated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-papers-delete",
   "metadata": {},
   "source": [
    "### Delete a Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "delete-paper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted paper:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Delete a paper by ID\n",
    "paper_id = {\"arxiv_id\": \"1706.03762v7\"}\n",
    "deleted_paper = research_arcade.delete_node_by_id(\"arxiv_papers\", paper_id)\n",
    "print(\"Deleted paper:\")\n",
    "print(deleted_paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-authors-section",
   "metadata": {},
   "source": [
    "## 4. ArXiv Authors <a name=\"arxiv-authors\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `semantic_scholar_id` (VARCHAR, unique)\n",
    "- `name` (VARCHAR)\n",
    "- `homepage` (VARCHAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e14ad06",
   "metadata": {},
   "source": [
    "### Construct Table from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c18c7737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\"arxiv_ids\": [\"1903.03894v4\", \"1806.08804v4\"], \"dest_dir\": \"./download\"}\n",
    "# research_arcade.construct_table_from_api(\"arxiv_authors\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-authors-insert",
   "metadata": {},
   "source": [
    "### Insert Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "insert-authors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted author: Ashish Vaswani\n",
      "Inserted author: Noam Shazeer\n",
      "Inserted author: Niki Parmar\n",
      "Inserted author: Jakob Uszkoreit\n",
      "Inserted author: Llion Jones\n"
     ]
    }
   ],
   "source": [
    "# Insert authors from the Transformer paper\n",
    "authors = [\n",
    "    {\n",
    "        'semantic_scholar_id': 'ss_ashish_vaswani',\n",
    "        'name': 'Ashish Vaswani',\n",
    "        'homepage': 'https://scholar.google.com/citations?user=oR9sCGYAAAAJ'\n",
    "    },\n",
    "    {\n",
    "        'semantic_scholar_id': 'ss_noam_shazeer',\n",
    "        'name': 'Noam Shazeer',\n",
    "        'homepage': 'https://scholar.google.com/citations?user=oR9sCGYAAAAJ'\n",
    "    },\n",
    "    {\n",
    "        'semantic_scholar_id': 'ss_niki_parmar',\n",
    "        'name': 'Niki Parmar',\n",
    "        'homepage': 'https://scholar.google.com/citations?user=oR9sCGYAAAAJ'\n",
    "    },\n",
    "    {\n",
    "        'semantic_scholar_id': 'ss_jakob_uszkoreit',\n",
    "        'name': 'Jakob Uszkoreit',\n",
    "        'homepage': 'https://scholar.google.com/citations?user=oR9sCGYAAAAJ'\n",
    "    },\n",
    "    {\n",
    "        'semantic_scholar_id': 'ss_llion_jones',\n",
    "        'name': 'Llion Jones',\n",
    "        'homepage': 'https://scholar.google.com/citations?user=oR9sCGYAAAAJ'\n",
    "    }\n",
    "]\n",
    "\n",
    "for author in authors:\n",
    "    research_arcade.insert_node(\"arxiv_authors\", node_features=author)\n",
    "    print(f\"Inserted author: {author['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-authors-get-all",
   "metadata": {},
   "source": [
    "### Get All Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "get-all-authors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total authors in database: 13\n",
      "\n",
      "All authors:\n",
      "    id semantic_scholar_id                 name  \\\n",
      "0    1   ss_ashish_vaswani       Ashish Vaswani   \n",
      "1    2     ss_noam_shazeer         Noam Shazeer   \n",
      "2    3      ss_niki_parmar          Niki Parmar   \n",
      "3    4  ss_jakob_uszkoreit      Jakob Uszkoreit   \n",
      "4    5      ss_llion_jones          Llion Jones   \n",
      "5    6            83539859             Rex Ying   \n",
      "6    7            40974349      Dylan Bourgeois   \n",
      "7    8           145829303          Jiaxuan You   \n",
      "8    9             2095762            M. Zitnik   \n",
      "9   10             1702139          J. Leskovec   \n",
      "10  11           143622465   Christopher Morris   \n",
      "11  12           145201124            Xiang Ren   \n",
      "12  13            49437682  William L. Hamilton   \n",
      "\n",
      "                                             homepage  \n",
      "0                           https://ashishvaswani.com  \n",
      "1   https://scholar.google.com/citations?user=oR9s...  \n",
      "2   https://scholar.google.com/citations?user=oR9s...  \n",
      "3   https://scholar.google.com/citations?user=oR9s...  \n",
      "4   https://scholar.google.com/citations?user=oR9s...  \n",
      "5     https://www.semanticscholar.org/author/83539859  \n",
      "6     https://www.semanticscholar.org/author/40974349  \n",
      "7    https://www.semanticscholar.org/author/145829303  \n",
      "8      https://www.semanticscholar.org/author/2095762  \n",
      "9      https://www.semanticscholar.org/author/1702139  \n",
      "10   https://www.semanticscholar.org/author/143622465  \n",
      "11   https://www.semanticscholar.org/author/145201124  \n",
      "12    https://www.semanticscholar.org/author/49437682  \n"
     ]
    }
   ],
   "source": [
    "authors_df = research_arcade.get_all_node_features(\"arxiv_authors\")\n",
    "print(f\"Total authors in database: {len(authors_df)}\")\n",
    "print(\"\\nAll authors:\")\n",
    "print(authors_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-authors-get-by-id",
   "metadata": {},
   "source": [
    "### Get Specific Author by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "get-author-by-id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author details:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "author_id = {\"semantic_scholar_id\": \"ss_ashish_vaswani\"}\n",
    "author_features = research_arcade.get_node_features_by_id(\"arxiv_authors\", author_id)\n",
    "print(\"Author details:\")\n",
    "print(author_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-authors-update",
   "metadata": {},
   "source": [
    "### Update an Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "update-author",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author updated successfully!\n"
     ]
    }
   ],
   "source": [
    "updated_author = {\n",
    "    'semantic_scholar_id': 'ss_ashish_vaswani',\n",
    "    'homepage': 'https://ashishvaswani.com'\n",
    "}\n",
    "\n",
    "research_arcade.update_node(\"arxiv_authors\", node_features=updated_author)\n",
    "print(\"Author updated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-categories-section",
   "metadata": {},
   "source": [
    "## 5. ArXiv Categories <a name=\"arxiv-categories\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `name` (VARCHAR, unique)\n",
    "- `description` (TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9eeea6",
   "metadata": {},
   "source": [
    "### Insert From API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "168633f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1903.03894v4', 'title': 'GNNExplainer: Generating Explanations for Graph Neural Networks', 'abstract': \"Graph Neural Networks (GNNs) are a powerful tool for machine learning on\\ngraphs.GNNs combine node feature information with the graph structure by\\nrecursively passing neural messages along edges of the input graph. However,\\nincorporating both graph structure and feature information leads to complex\\nmodels, and explaining predictions made by GNNs remains unsolved. Here we\\npropose GNNExplainer, the first general, model-agnostic approach for providing\\ninterpretable explanations for predictions of any GNN-based model on any\\ngraph-based machine learning task. Given an instance, GNNExplainer identifies a\\ncompact subgraph structure and a small subset of node features that have a\\ncrucial role in GNN's prediction. Further, GNNExplainer can generate consistent\\nand concise explanations for an entire class of instances. We formulate\\nGNNExplainer as an optimization task that maximizes the mutual information\\nbetween a GNN's prediction and distribution of possible subgraph structures.\\nExperiments on synthetic and real-world graphs show that our approach can\\nidentify important graph structures as well as node features, and outperforms\\nbaselines by 17.1% on average. GNNExplainer provides a variety of benefits,\\nfrom the ability to visualize semantically relevant structures to\\ninterpretability, to giving insights into errors of faulty GNNs.\", 'authors': ['Rex Ying', 'Dylan Bourgeois', 'Jiaxuan You', 'Marinka Zitnik', 'Jure Leskovec'], 'published': '2019-03-10 00:56:26+00:00', 'categories': ['cs.LG', 'stat.ML'], 'url': 'http://arxiv.org/abs/1903.03894v4'}\n",
      "{'id': '1806.08804v4', 'title': 'Hierarchical Graph Representation Learning with Differentiable Pooling', 'abstract': 'Recently, graph neural networks (GNNs) have revolutionized the field of graph\\nrepresentation learning through effectively learned node embeddings, and\\nachieved state-of-the-art results in tasks such as node classification and link\\nprediction. However, current GNN methods are inherently flat and do not learn\\nhierarchical representations of graphs---a limitation that is especially\\nproblematic for the task of graph classification, where the goal is to predict\\nthe label associated with an entire graph. Here we propose DiffPool, a\\ndifferentiable graph pooling module that can generate hierarchical\\nrepresentations of graphs and can be combined with various graph neural network\\narchitectures in an end-to-end fashion. DiffPool learns a differentiable soft\\ncluster assignment for nodes at each layer of a deep GNN, mapping nodes to a\\nset of clusters, which then form the coarsened input for the next GNN layer.\\nOur experimental results show that combining existing GNN methods with DiffPool\\nyields an average improvement of 5-10% accuracy on graph classification\\nbenchmarks, compared to all existing pooling approaches, achieving a new\\nstate-of-the-art on four out of five benchmark data sets.', 'authors': ['Rex Ying', 'Jiaxuan You', 'Christopher Morris', 'Xiang Ren', 'William L. Hamilton', 'Jure Leskovec'], 'published': '2018-06-22 18:04:46+00:00', 'categories': ['cs.LG', 'cs.NE', 'cs.SI', 'stat.ML'], 'url': 'http://arxiv.org/abs/1806.08804v4'}\n"
     ]
    }
   ],
   "source": [
    "config = {\"arxiv_ids\": [\"1903.03894v4\", \"1806.08804v4\"], \"dest_dir\": \"./download\"}\n",
    "research_arcade.construct_table_from_api(\"arxiv_categories\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-categories-insert",
   "metadata": {},
   "source": [
    "### Insert Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "insert-categories",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted category: cs.CL\n",
      "Inserted category: cs.LG\n",
      "Inserted category: cs.AI\n",
      "Inserted category: cs.CV\n",
      "Inserted category: stat.ML\n"
     ]
    }
   ],
   "source": [
    "categories = [\n",
    "    {\n",
    "        'name': 'cs.CL',\n",
    "        'description': 'Computation and Language (Natural Language Processing)'\n",
    "    },\n",
    "    {\n",
    "        'name': 'cs.LG',\n",
    "        'description': 'Machine Learning'\n",
    "    },\n",
    "    {\n",
    "        'name': 'cs.AI',\n",
    "        'description': 'Artificial Intelligence'\n",
    "    },\n",
    "    {\n",
    "        'name': 'cs.CV',\n",
    "        'description': 'Computer Vision and Pattern Recognition'\n",
    "    },\n",
    "    {\n",
    "        'name': 'stat.ML',\n",
    "        'description': 'Machine Learning (Statistics)'\n",
    "    }\n",
    "]\n",
    "\n",
    "for category in categories:\n",
    "    research_arcade.insert_node(\"arxiv_categories\", node_features=category)\n",
    "    print(f\"Inserted category: {category['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-categories-get-all",
   "metadata": {},
   "source": [
    "### Get All Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "get-all-categories",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total categories: 7\n",
      "\n",
      "All categories:\n",
      "   id     name                                        description\n",
      "0   1    cs.CL  Computation and Language (Natural Language Pro...\n",
      "1   2    cs.LG                                   Machine Learning\n",
      "2   3    cs.AI                            Artificial Intelligence\n",
      "3   4    cs.CV            Computer Vision and Pattern Recognition\n",
      "4   5  stat.ML                      Machine Learning (Statistics)\n",
      "5   6    cs.NE                                                NaN\n",
      "6   7    cs.SI                                                NaN\n"
     ]
    }
   ],
   "source": [
    "categories_df = research_arcade.get_all_node_features(\"arxiv_categories\")\n",
    "print(f\"Total categories: {len(categories_df)}\")\n",
    "print(\"\\nAll categories:\")\n",
    "print(categories_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-figures-section",
   "metadata": {},
   "source": [
    "## 6. ArXiv Figures <a name=\"arxiv-figures\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `paper_arxiv_id` (VARCHAR FK → papers.arxiv_id)\n",
    "- `path` (VARCHAR)\n",
    "- `caption` (TEXT)\n",
    "- `label` (TEXT)\n",
    "- `name` (TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-figures-insert",
   "metadata": {},
   "source": [
    "### Insert Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "insert-figures",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted Figure 1\n",
      "Inserted Figure 2\n",
      "Inserted Figure 3\n"
     ]
    }
   ],
   "source": [
    "# Insert figures for the Transformer paper\n",
    "figures = [\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'path': '/figures/transformer_architecture.png',\n",
    "        'caption': 'The Transformer model architecture. The left side shows the encoder stack and the right side shows the decoder stack.',\n",
    "        'label': 'fig:architecture',\n",
    "        'name': 'Figure 1'\n",
    "    },\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'path': '/figures/scaled_dot_product_attention.png',\n",
    "        'caption': 'Scaled Dot-Product Attention and Multi-Head Attention mechanisms.',\n",
    "        'label': 'fig:attention',\n",
    "        'name': 'Figure 2'\n",
    "    },\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'path': '/figures/positional_encoding.png',\n",
    "        'caption': 'Positional encoding visualization showing sine and cosine functions of different frequencies.',\n",
    "        'label': 'fig:positional',\n",
    "        'name': 'Figure 3'\n",
    "    }\n",
    "]\n",
    "\n",
    "for figure in figures:\n",
    "    research_arcade.insert_node(\"arxiv_figures\", node_features=figure)\n",
    "    print(f\"Inserted {figure['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-figures-get-all",
   "metadata": {},
   "source": [
    "### Get All Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "get-all-figures",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total figures: 135\n",
      "\n",
      "All figures:\n",
      "         name                                            caption  \\\n",
      "0    Figure 1  The Transformer model architecture. The left s...   \n",
      "1    Figure 2  Scaled Dot-Product Attention and Multi-Head At...   \n",
      "2    Figure 3  Positional encoding visualization showing sine...   \n",
      "3         NaN  \\caption{\\gnn computation graph $G_c$ for maki...   \n",
      "4         NaN  \\caption{\\name provides interpretable explanat...   \n",
      "..        ...                                                ...   \n",
      "130       NaN                                         \\caption{}   \n",
      "131       NaN                                         \\caption{}   \n",
      "132       NaN                                         \\caption{}   \n",
      "133       NaN                                         \\caption{}   \n",
      "134       NaN                                         \\caption{}   \n",
      "\n",
      "                                label  \n",
      "0                    fig:architecture  \n",
      "1                       fig:attention  \n",
      "2                      fig:positional  \n",
      "3    \\label{fig:explainer-motivation}  \n",
      "4         \\label{fig:explainer-intro}  \n",
      "..                                ...  \n",
      "130                               NaN  \n",
      "131                               NaN  \n",
      "132                               NaN  \n",
      "133                               NaN  \n",
      "134                               NaN  \n",
      "\n",
      "[135 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "figures_df = research_arcade.get_all_node_features(\"arxiv_figures\")\n",
    "print(f\"Total figures: {len(figures_df)}\")\n",
    "print(\"\\nAll figures:\")\n",
    "print(figures_df[['name', 'caption', 'label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-tables-section",
   "metadata": {},
   "source": [
    "## 7. ArXiv Tables <a name=\"arxiv-tables\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `paper_arxiv_id` (VARCHAR FK → papers.arxiv_id)\n",
    "- `path` (VARCHAR)\n",
    "- `caption` (TEXT)\n",
    "- `label` (TEXT)\n",
    "- `table_text` (TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240602b",
   "metadata": {},
   "source": [
    "### Insert From API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54a13d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"arxiv_ids\": [\"1903.03894v4\", \"1806.08804v4\"], \"dest_dir\": \"./download\"}\n",
    "research_arcade.construct_table_from_api(\"arxiv_tables\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7002162",
   "metadata": {},
   "source": [
    "### Insert Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b809fdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted category: cs.CL\n",
      "Inserted category: cs.LG\n",
      "Inserted category: cs.AI\n",
      "Inserted category: cs.CV\n",
      "Inserted category: stat.ML\n"
     ]
    }
   ],
   "source": [
    "categories = [\n",
    "    {\n",
    "        'name': 'cs.CL',\n",
    "        'description': 'Computation and Language (Natural Language Processing)'\n",
    "    },\n",
    "    {\n",
    "        'name': 'cs.LG',\n",
    "        'description': 'Machine Learning'\n",
    "    },\n",
    "    {\n",
    "        'name': 'cs.AI',\n",
    "        'description': 'Artificial Intelligence'\n",
    "    },\n",
    "    {\n",
    "        'name': 'cs.CV',\n",
    "        'description': 'Computer Vision and Pattern Recognition'\n",
    "    },\n",
    "    {\n",
    "        'name': 'stat.ML',\n",
    "        'description': 'Machine Learning (Statistics)'\n",
    "    }\n",
    "]\n",
    "\n",
    "for category in categories:\n",
    "    research_arcade.insert_node(\"arxiv_categories\", node_features=category)\n",
    "    print(f\"Inserted category: {category['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3771de83",
   "metadata": {},
   "source": [
    "### Get All Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f1357fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total categories: 7\n",
      "\n",
      "All categories:\n",
      "   id     name                                        description\n",
      "0   1    cs.CL  Computation and Language (Natural Language Pro...\n",
      "1   2    cs.LG                                   Machine Learning\n",
      "2   3    cs.AI                            Artificial Intelligence\n",
      "3   4    cs.CV            Computer Vision and Pattern Recognition\n",
      "4   5  stat.ML                      Machine Learning (Statistics)\n",
      "5   6    cs.NE                                                NaN\n",
      "6   7    cs.SI                                                NaN\n"
     ]
    }
   ],
   "source": [
    "categories_df = research_arcade.get_all_node_features(\"arxiv_categories\")\n",
    "print(f\"Total categories: {len(categories_df)}\")\n",
    "print(\"\\nAll categories:\")\n",
    "print(categories_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d6714",
   "metadata": {},
   "source": [
    "## 6. ArXiv Figures <a name=\"arxiv-figures\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `paper_arxiv_id` (VARCHAR FK → papers.arxiv_id)\n",
    "- `path` (VARCHAR)\n",
    "- `caption` (TEXT)\n",
    "- `label` (TEXT)\n",
    "- `name` (TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d33eb",
   "metadata": {},
   "source": [
    "### Insert From API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "195e218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"arxiv_ids\": [\"1903.03894v4\", \"1806.08804v4\"], \"dest_dir\": \"./download\"}\n",
    "research_arcade.construct_table_from_api(\"arxiv_figures\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-tables-insert",
   "metadata": {},
   "source": [
    "### Insert Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "insert-tables",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted table: tab:variations\n",
      "Inserted table: tab:wmt\n",
      "Inserted table: tab:parsing\n"
     ]
    }
   ],
   "source": [
    "# Insert tables for the Transformer paper\n",
    "tables = [\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'path': '/tables/model_variations.tex',\n",
    "        'caption': 'Variations on the Transformer architecture with different hyperparameters.',\n",
    "        'label': 'tab:variations',\n",
    "        'table_text': 'Model | N | d_model | d_ff | h | d_k | d_v | P_drop | train time\\nbase | 6 | 512 | 2048 | 8 | 64 | 64 | 0.1 | 12 hrs'\n",
    "    },\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'path': '/tables/wmt_results.tex',\n",
    "        'caption': 'Performance of the Transformer on WMT 2014 English-German and English-French translation tasks.',\n",
    "        'label': 'tab:wmt',\n",
    "        'table_text': 'Model | EN-DE BLEU | EN-FR BLEU\\nTransformer (base) | 27.3 | 38.1\\nTransformer (big) | 28.4 | 41.8'\n",
    "    },\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'path': '/tables/parsing_results.tex',\n",
    "        'caption': 'English constituency parsing results on WSJ test set.',\n",
    "        'label': 'tab:parsing',\n",
    "        'table_text': 'Model | WSJ 23 F1\\nTransformer | 91.3'\n",
    "    }\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "    research_arcade.insert_node(\"arxiv_tables\", node_features=table)\n",
    "    print(f\"Inserted table: {table['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-tables-get-all",
   "metadata": {},
   "source": [
    "### Get All Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "get-all-tables",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tables: 59\n",
      "\n",
      "All tables:\n",
      "                         label  \\\n",
      "0               tab:variations   \n",
      "1                      tab:wmt   \n",
      "2                  tab:parsing   \n",
      "3   \\label{fig:synth_datasets}   \n",
      "4       \\label{tab:results_pr}   \n",
      "5       \\label{tab:results_pr}   \n",
      "6          \\label{tab:results}   \n",
      "7         \\label{tab:results2}   \n",
      "8               tab:variations   \n",
      "9                      tab:wmt   \n",
      "10                 tab:parsing   \n",
      "11  \\label{fig:synth_datasets}   \n",
      "12      \\label{tab:results_pr}   \n",
      "13      \\label{tab:results_pr}   \n",
      "14         \\label{tab:results}   \n",
      "15        \\label{tab:results2}   \n",
      "16              tab:variations   \n",
      "17                     tab:wmt   \n",
      "18                 tab:parsing   \n",
      "19  \\label{fig:synth_datasets}   \n",
      "20      \\label{tab:results_pr}   \n",
      "21      \\label{tab:results_pr}   \n",
      "22         \\label{tab:results}   \n",
      "23        \\label{tab:results2}   \n",
      "24              tab:variations   \n",
      "25                     tab:wmt   \n",
      "26                 tab:parsing   \n",
      "27  \\label{fig:synth_datasets}   \n",
      "28      \\label{tab:results_pr}   \n",
      "29      \\label{tab:results_pr}   \n",
      "30         \\label{tab:results}   \n",
      "31        \\label{tab:results2}   \n",
      "32              tab:variations   \n",
      "33                     tab:wmt   \n",
      "34                 tab:parsing   \n",
      "35  \\label{fig:synth_datasets}   \n",
      "36      \\label{tab:results_pr}   \n",
      "37      \\label{tab:results_pr}   \n",
      "38         \\label{tab:results}   \n",
      "39        \\label{tab:results2}   \n",
      "40              tab:variations   \n",
      "41                     tab:wmt   \n",
      "42                 tab:parsing   \n",
      "43  \\label{fig:synth_datasets}   \n",
      "44      \\label{tab:results_pr}   \n",
      "45      \\label{tab:results_pr}   \n",
      "46         \\label{tab:results}   \n",
      "47        \\label{tab:results2}   \n",
      "48              tab:variations   \n",
      "49                     tab:wmt   \n",
      "50                 tab:parsing   \n",
      "51  \\label{fig:synth_datasets}   \n",
      "52      \\label{tab:results_pr}   \n",
      "53      \\label{tab:results_pr}   \n",
      "54         \\label{tab:results}   \n",
      "55        \\label{tab:results2}   \n",
      "56              tab:variations   \n",
      "57                     tab:wmt   \n",
      "58                 tab:parsing   \n",
      "\n",
      "                                              caption  \n",
      "0   Variations on the Transformer architecture wit...  \n",
      "1   Performance of the Transformer on WMT 2014 Eng...  \n",
      "2   English constituency parsing results on WSJ te...  \n",
      "3   \\caption{Illustration of synthetic datasets (r...  \n",
      "4   \\caption{\\namelong compared to baselines in id...  \n",
      "5   \\caption{\\namelong compared to \\textsc{Grad} b...  \n",
      "6   \\caption{Classification accuracies in percent....  \n",
      "7   \\caption{Accuracy results of applying \\name to...  \n",
      "8   Variations on the Transformer architecture wit...  \n",
      "9   Performance of the Transformer on WMT 2014 Eng...  \n",
      "10  English constituency parsing results on WSJ te...  \n",
      "11  \\caption{Illustration of synthetic datasets (r...  \n",
      "12  \\caption{\\namelong compared to baselines in id...  \n",
      "13  \\caption{\\namelong compared to \\textsc{Grad} b...  \n",
      "14  \\caption{Classification accuracies in percent....  \n",
      "15  \\caption{Accuracy results of applying \\name to...  \n",
      "16  Variations on the Transformer architecture wit...  \n",
      "17  Performance of the Transformer on WMT 2014 Eng...  \n",
      "18  English constituency parsing results on WSJ te...  \n",
      "19  \\caption{Illustration of synthetic datasets (r...  \n",
      "20  \\caption{\\namelong compared to baselines in id...  \n",
      "21  \\caption{\\namelong compared to \\textsc{Grad} b...  \n",
      "22  \\caption{Classification accuracies in percent....  \n",
      "23  \\caption{Accuracy results of applying \\name to...  \n",
      "24  Variations on the Transformer architecture wit...  \n",
      "25  Performance of the Transformer on WMT 2014 Eng...  \n",
      "26  English constituency parsing results on WSJ te...  \n",
      "27  \\caption{Illustration of synthetic datasets (r...  \n",
      "28  \\caption{\\namelong compared to baselines in id...  \n",
      "29  \\caption{\\namelong compared to \\textsc{Grad} b...  \n",
      "30  \\caption{Classification accuracies in percent....  \n",
      "31  \\caption{Accuracy results of applying \\name to...  \n",
      "32  Variations on the Transformer architecture wit...  \n",
      "33  Performance of the Transformer on WMT 2014 Eng...  \n",
      "34  English constituency parsing results on WSJ te...  \n",
      "35  \\caption{Illustration of synthetic datasets (r...  \n",
      "36  \\caption{\\namelong compared to baselines in id...  \n",
      "37  \\caption{\\namelong compared to \\textsc{Grad} b...  \n",
      "38  \\caption{Classification accuracies in percent....  \n",
      "39  \\caption{Accuracy results of applying \\name to...  \n",
      "40  Variations on the Transformer architecture wit...  \n",
      "41  Performance of the Transformer on WMT 2014 Eng...  \n",
      "42  English constituency parsing results on WSJ te...  \n",
      "43  \\caption{Illustration of synthetic datasets (r...  \n",
      "44  \\caption{\\namelong compared to baselines in id...  \n",
      "45  \\caption{\\namelong compared to \\textsc{Grad} b...  \n",
      "46  \\caption{Classification accuracies in percent....  \n",
      "47  \\caption{Accuracy results of applying \\name to...  \n",
      "48  Variations on the Transformer architecture wit...  \n",
      "49  Performance of the Transformer on WMT 2014 Eng...  \n",
      "50  English constituency parsing results on WSJ te...  \n",
      "51  \\caption{Illustration of synthetic datasets (r...  \n",
      "52  \\caption{\\namelong compared to baselines in id...  \n",
      "53  \\caption{\\namelong compared to \\textsc{Grad} b...  \n",
      "54  \\caption{Classification accuracies in percent....  \n",
      "55  \\caption{Accuracy results of applying \\name to...  \n",
      "56  Variations on the Transformer architecture wit...  \n",
      "57  Performance of the Transformer on WMT 2014 Eng...  \n",
      "58  English constituency parsing results on WSJ te...  \n"
     ]
    }
   ],
   "source": [
    "tables_df = research_arcade.get_all_node_features(\"arxiv_tables\")\n",
    "print(f\"Total tables: {len(tables_df)}\")\n",
    "print(\"\\nAll tables:\")\n",
    "print(tables_df[['label', 'caption']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-sections-section",
   "metadata": {},
   "source": [
    "## 8. ArXiv Sections <a name=\"arxiv-sections\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `content` (TEXT)\n",
    "- `title` (TEXT)\n",
    "- `appendix` (BOOLEAN)\n",
    "- `paper_arxiv_id` (VARCHAR FK → papers.arxiv_id)\n",
    "- `section_in_paper_id` (INT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9890560",
   "metadata": {},
   "source": [
    "### Insert From API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1735fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"arxiv_ids\": [\"1903.03894v4\", \"1806.08804v4\"], \"dest_dir\": \"./download\"}\n",
    "research_arcade.construct_table_from_api(\"arxiv_sections\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-sections-insert",
   "metadata": {},
   "source": [
    "### Insert Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "insert-sections",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted section: Introduction\n",
      "Inserted section: Background\n",
      "Inserted section: Model Architecture\n",
      "Inserted section: Training\n",
      "Inserted section: Results\n",
      "Inserted section: Conclusion\n"
     ]
    }
   ],
   "source": [
    "# Insert sections for the Transformer paper\n",
    "sections = [\n",
    "    {\n",
    "        'content': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder...',\n",
    "        'title': 'Introduction',\n",
    "        'appendix': False,\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'section_in_paper_id': 1\n",
    "    },\n",
    "    {\n",
    "        'content': 'Most competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps an input sequence of symbol representations...',\n",
    "        'title': 'Background',\n",
    "        'appendix': False,\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'section_in_paper_id': 2\n",
    "    },\n",
    "    {\n",
    "        'content': 'Most neural sequence transduction models have an encoder-decoder structure. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers...',\n",
    "        'title': 'Model Architecture',\n",
    "        'appendix': False,\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'section_in_paper_id': 3\n",
    "    },\n",
    "    {\n",
    "        'content': 'In this section we describe the training regime for our models...',\n",
    "        'title': 'Training',\n",
    "        'appendix': False,\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'section_in_paper_id': 4\n",
    "    },\n",
    "    {\n",
    "        'content': 'On the WMT 2014 English-to-German translation task, the big transformer model outperforms the best previously reported models...',\n",
    "        'title': 'Results',\n",
    "        'appendix': False,\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'section_in_paper_id': 5\n",
    "    },\n",
    "    {\n",
    "        'content': 'In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers...',\n",
    "        'title': 'Conclusion',\n",
    "        'appendix': False,\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'section_in_paper_id': 6\n",
    "    }\n",
    "]\n",
    "\n",
    "for section in sections:\n",
    "    research_arcade.insert_node(\"arxiv_sections\", node_features=section)\n",
    "    print(f\"Inserted section: {section['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-sections-get-all",
   "metadata": {},
   "source": [
    "### Get All Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "get-all-sections",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sections:       id                                            content  \\\n",
      "0      1  The dominant sequence transduction models are ...   \n",
      "1      2  Most competitive neural sequence transduction ...   \n",
      "2      3  Most neural sequence transduction models have ...   \n",
      "3      4  In this section we describe the training regim...   \n",
      "4      5  On the WMT 2014 English-to-German translation ...   \n",
      "..   ...                                                ...   \n",
      "141  142  Most competitive neural sequence transduction ...   \n",
      "142  143  Most neural sequence transduction models have ...   \n",
      "143  144  In this section we describe the training regim...   \n",
      "144  145  On the WMT 2014 English-to-German translation ...   \n",
      "145  146  In this work, we presented the Transformer, th...   \n",
      "\n",
      "                  title  appendix paper_arxiv_id  section_in_paper_id  \n",
      "0          Introduction     False   1706.03762v7                  1.0  \n",
      "1            Background     False   1706.03762v7                  2.0  \n",
      "2    Model Architecture     False   1706.03762v7                  3.0  \n",
      "3              Training     False   1706.03762v7                  4.0  \n",
      "4               Results     False   1706.03762v7                  5.0  \n",
      "..                  ...       ...            ...                  ...  \n",
      "141          Background     False   1706.03762v7                  2.0  \n",
      "142  Model Architecture     False   1706.03762v7                  3.0  \n",
      "143            Training     False   1706.03762v7                  4.0  \n",
      "144             Results     False   1706.03762v7                  5.0  \n",
      "145          Conclusion     False   1706.03762v7                  6.0  \n",
      "\n",
      "[146 rows x 6 columns]\n",
      "\n",
      "All sections:\n",
      "                  title  section_in_paper_id  appendix\n",
      "0          Introduction                  1.0     False\n",
      "1            Background                  2.0     False\n",
      "2    Model Architecture                  3.0     False\n",
      "3              Training                  4.0     False\n",
      "4               Results                  5.0     False\n",
      "..                  ...                  ...       ...\n",
      "141          Background                  2.0     False\n",
      "142  Model Architecture                  3.0     False\n",
      "143            Training                  4.0     False\n",
      "144             Results                  5.0     False\n",
      "145          Conclusion                  6.0     False\n",
      "\n",
      "[146 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "sections_df = research_arcade.get_all_node_features(\"arxiv_sections\")\n",
    "print(f\"Total sections: {sections_df}\")\n",
    "print(\"\\nAll sections:\")\n",
    "print(sections_df[['title', 'section_in_paper_id', 'appendix']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-paragraphs-section",
   "metadata": {},
   "source": [
    "## 9. ArXiv Paragraphs <a name=\"arxiv-paragraphs\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `paragraph_id` (INT)\n",
    "- `content` (TEXT)\n",
    "- `paper_arxiv_id` (VARCHAR FK → papers.arxiv_id)\n",
    "- `paper_section` (TEXT)\n",
    "- `section_id` (INT)\n",
    "- `paragraph_in_paper_id` (INT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3fd1cc",
   "metadata": {},
   "source": [
    "### Insert From API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "480dabdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 430.47it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 146.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1903.03894v4\n",
      "Key to References: {'fig:explainer-motivation': 'figures_3', 'fig:explainer-intro': 'figures_4', 'fig:definition-node-features': 'figures_5', 'fig:including-node-features': 'figures_7', 'fig:subgraph_node': 'figures_8', 'fig:subgraph_graph': 'figures_9', 'fig:prototype': 'figures_12', 'fig:my_label': 'figures_11', 'fig:synth_datasets': 'table_13', 'tab:results_pr': 'table_15'}\n",
      "No paper found for  cho2011friendship\n",
      "No paper found for  you2018graph\n",
      "No paper found for  zitnik2018decagon\n",
      "No paper found for  zhang_deep_2018\n",
      "No paper found for  zhou_graph_2018\n",
      "No paper found for  graphsage\n",
      "No paper found for  kipf2016semi\n",
      "No paper found for  ying2018hierarchical\n",
      "No paper found for  zhang2018link\n",
      "No paper found for  doshi-velez_towards_2017\n",
      "No paper found for  lakkaraju_interpretable_2017\n",
      "No paper found for  ribeiro_why_2016\n",
      "No paper found for  schmitz_ann-dt:_1999\n",
      "No paper found for  chen2018learning\n",
      "No paper found for  Erhan2009VisualizingHF\n",
      "No paper found for  lundberg_unified_2017\n",
      "No paper found for  sundararajan_axiomatic_nodate\n",
      "No paper found for  koh_understanding_2017\n",
      "No paper found for  DBLP:journals/corr/abs-1811-09720\n",
      "No paper found for  ribeiro_why_2016\n",
      "No paper found for  augasta_reverse_2012\n",
      "No paper found for  lakkaraju_interpretable_2017\n",
      "No paper found for  calders_deepred_2016\n",
      "No paper found for  Erhan2009VisualizingHF\n",
      "No paper found for  fleet_visualizing_2014\n",
      "No paper found for  chen2018learning\n",
      "No paper found for  shrikumar_learning_2017\n",
      "No paper found for  sundararajan_axiomatic_nodate\n",
      "No paper found for  Kang2019explaine\n",
      "No paper found for  fleet_visualizing_2014\n",
      "No paper found for  2018sanity\n",
      "No paper found for  shrikumar_learning_2017\n",
      "No paper found for  sundararajan_axiomatic_nodate\n",
      "No paper found for  adadi_peeking_2018\n",
      "No paper found for  fisher_all_2018\n",
      "No paper found for  guidotti_survey_2018\n",
      "No paper found for  hooker_discovering_2004\n",
      "No paper found for  koh_understanding_2017\n",
      "No paper found for  DBLP:journals/corr/abs-1811-09720\n",
      "No paper found for  mutag\n",
      "No paper found for  duvenaud_convolutional_2015\n",
      "No paper found for  neil2018interpretable\n",
      "No paper found for  velickovic2018graph\n",
      "No paper found for  PhysRevLett.120.145301\n",
      "No paper found for  battaglia\n",
      "No paper found for  zhang_deep_2018\n",
      "No paper found for  zhou_graph_2018\n",
      "No paper found for  graphsage\n",
      "No paper found for  xu2018powerful\n",
      "No paper found for  kipf2016semi\n",
      "No paper found for  xujumping\n",
      "No paper found for  chen2018supervised\n",
      "No paper found for  kipf2016semi\n",
      "No paper found for  xujumping\n",
      "No paper found for  chen2018supervised\n",
      "No paper found for  mutag\n",
      "No paper found for  yanardag2015deep\n",
      "fig:synth_datasets\n",
      "fig:synth_datasets\n",
      "No paper found for  velickovic2018graph\n",
      "No paper found for  velickovic2018graph\n",
      "fig:synth_datasets\n",
      "fig:synth_datasets\n",
      "fig:synth_datasets\n",
      "fig:synth_datasets\n",
      "No paper found for  kumar2018community\n",
      "No paper found for  kumar2018community\n",
      "1806.08804v4\n",
      "Key to References: {'fig:assignment_vis': 'figures_157', 'tab:results': 'table_158', 'tab:results2': 'table_159'}\n",
      "tab:results\n",
      "tab:results2\n",
      "Paper count:  2\n",
      "Total nodes:  254\n",
      "Total edges:  474\n",
      "Paper nodes:  2\n",
      "Figure nodes:  0\n",
      "Table nodes:  2\n",
      "Text nodes:  250\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\"arxiv_ids\": [\"1903.03894v4\", \"1806.08804v4\"], \"dest_dir\": \"./download\"}\n",
    "research_arcade.construct_table_from_api(\"arxiv_paragraphs\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-paragraphs-insert",
   "metadata": {},
   "source": [
    "### Insert Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "insert-paragraphs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted paragraph 1 from Introduction\n",
      "Inserted paragraph 2 from Introduction\n",
      "Inserted paragraph 3 from Introduction\n",
      "Inserted paragraph 4 from Introduction\n",
      "Inserted paragraph 5 from Introduction\n"
     ]
    }
   ],
   "source": [
    "# Insert paragraphs from the Introduction section\n",
    "paragraphs = [\n",
    "    {\n",
    "        'paragraph_id': 1,\n",
    "        'content': 'Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation.',\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'paper_section': 'Introduction',\n",
    "        'section_id': 1,\n",
    "        'paragraph_in_paper_id': 1\n",
    "    },\n",
    "    {\n",
    "        'paragraph_id': 2,\n",
    "        'content': 'Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures. Recurrent models typically factor computation along the symbol positions of the input and output sequences.',\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'paper_section': 'Introduction',\n",
    "        'section_id': 1,\n",
    "        'paragraph_in_paper_id': 2\n",
    "    },\n",
    "    {\n",
    "        'paragraph_id': 3,\n",
    "        'content': 'Aligning the positions to steps in computation time, they generate a sequence of hidden states h_t, as a function of the previous hidden state h_{t-1} and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.',\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'paper_section': 'Introduction',\n",
    "        'section_id': 1,\n",
    "        'paragraph_in_paper_id': 3\n",
    "    },\n",
    "    {\n",
    "        'paragraph_id': 4,\n",
    "        'content': 'Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences.',\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'paper_section': 'Introduction',\n",
    "        'section_id': 1,\n",
    "        'paragraph_in_paper_id': 4\n",
    "    },\n",
    "    {\n",
    "        'paragraph_id': 5,\n",
    "        'content': 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.',\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'paper_section': 'Introduction',\n",
    "        'section_id': 1,\n",
    "        'paragraph_in_paper_id': 5\n",
    "    }\n",
    "]\n",
    "\n",
    "for paragraph in paragraphs:\n",
    "    research_arcade.insert_node(\"arxiv_paragraphs\", node_features=paragraph)\n",
    "    print(f\"Inserted paragraph {paragraph['paragraph_id']} from {paragraph['paper_section']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-paragraphs-get-all",
   "metadata": {},
   "source": [
    "### Get All Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "get-all-paragraphs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total paragraphs: 255\n",
      "\n",
      "First 3 paragraphs:\n",
      "   paragraph_id paper_section  \\\n",
      "0             1  Introduction   \n",
      "1             2  Introduction   \n",
      "2             3  Introduction   \n",
      "\n",
      "                                             content  \n",
      "0  Recurrent neural networks, long short-term mem...  \n",
      "1  Numerous efforts have since continued to push ...  \n",
      "2  Aligning the positions to steps in computation...  \n"
     ]
    }
   ],
   "source": [
    "paragraphs_df = research_arcade.get_all_node_features(\"arxiv_paragraphs\")\n",
    "print(f\"Total paragraphs: {len(paragraphs_df)}\")\n",
    "print(\"\\nFirst 3 paragraphs:\")\n",
    "print(paragraphs_df[['paragraph_id', 'paper_section', 'content']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Relationships/Edges <a name=\"relationships\"></a>\n",
    "\n",
    "This section demonstrates how to create and manage relationships between different entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 ArXiv Citations (arxiv_citation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citation created!\n"
     ]
    }
   ],
   "source": [
    "citation = {\n",
    "    'citing_arxiv_id': '1810.04805v2',\n",
    "    'cited_arxiv_id': '1706.03762v7',\n",
    "    'bib_title': 'attention is all you need',\n",
    "    'bib_key': 'something',\n",
    "    'citing_sections': 'citing_sections',\n",
    "}\n",
    "research_arcade.insert_edge(\"arxiv_citation\", edge_features=citation)\n",
    "print(\"Citation created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get All Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total citations: 1\n",
      "   id citing_arxiv_id cited_arxiv_id                  bib_title    bib_key  \\\n",
      "0   1    1810.04805v2   1706.03762v7  attention is all you need  something   \n",
      "\n",
      "   author_cited_paper    citing_sections citing_paragraphs  \n",
      "0                 NaN  \"citing_sections\"                []  \n"
     ]
    }
   ],
   "source": [
    "all_citations = research_arcade.get_all_edge_features(\"arxiv_citation\")\n",
    "print(f\"Total citations: {len(all_citations)}\")\n",
    "print(all_citations.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Cited Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers cited:\n",
      "   id citing_arxiv_id cited_arxiv_id                  bib_title    bib_key  \\\n",
      "0   1    1810.04805v2   1706.03762v7  attention is all you need  something   \n",
      "\n",
      "   author_cited_paper    citing_sections citing_paragraphs  \n",
      "0                 NaN  \"citing_sections\"                []  \n"
     ]
    }
   ],
   "source": [
    "citing_paper = {'citing_paper_id': '1810.04805v2'}\n",
    "cited_papers = research_arcade.get_neighborhood(\"arxiv_citation\", primary_key=citing_paper)\n",
    "print(\"Papers cited:\")\n",
    "print(cited_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Citing Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers that cite:\n",
      "   id citing_arxiv_id cited_arxiv_id                  bib_title    bib_key  \\\n",
      "0   1    1810.04805v2   1706.03762v7  attention is all you need  something   \n",
      "\n",
      "   author_cited_paper    citing_sections citing_paragraphs  \n",
      "0                 NaN  \"citing_sections\"                []  \n"
     ]
    }
   ],
   "source": [
    "cited_paper = {'cited_paper_id': '1706.03762v7'}\n",
    "citing_papers = research_arcade.get_neighborhood(\"arxiv_citation\", primary_key=cited_paper)\n",
    "print(\"Papers that cite:\")\n",
    "print(citing_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted citation: 1810.04805v2 -> 1706.03762v7\n",
      "Citation deleted!\n"
     ]
    }
   ],
   "source": [
    "citation_id = {\n",
    "    'citing_paper_id': '1810.04805v2',\n",
    "    'cited_paper_id': '1706.03762v7'\n",
    "}\n",
    "research_arcade.delete_edge_by_id(\"arxiv_citation\", primary_key=citation_id)\n",
    "print(\"Citation deleted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 ArXiv Paper-Author (arxiv_paper_author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Paper-Author Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked author ss_ashish_vaswani (position 1)\n",
      "Linked author ss_noam_shazeer (position 2)\n",
      "Linked author ss_niki_parmar (position 3)\n"
     ]
    }
   ],
   "source": [
    "paper_authors = [\n",
    "    {'paper_arxiv_id': '1706.03762v7', 'author_id': 'ss_ashish_vaswani', 'author_sequence': 1},\n",
    "    {'paper_arxiv_id': '1706.03762v7', 'author_id': 'ss_noam_shazeer', 'author_sequence': 2},\n",
    "    {'paper_arxiv_id': '1706.03762v7', 'author_id': 'ss_niki_parmar', 'author_sequence': 3}\n",
    "]\n",
    "for relation in paper_authors:\n",
    "    research_arcade.insert_edge(\"arxiv_paper_author\", edge_features=relation)\n",
    "    print(f\"Linked author {relation['author_id']} (position {relation['author_sequence']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get All Paper-Author Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total relationships: 3\n",
      "  paper_arxiv_id          author_id  author_sequence\n",
      "0   1706.03762v7  ss_ashish_vaswani                1\n",
      "1   1706.03762v7    ss_noam_shazeer                2\n",
      "2   1706.03762v7     ss_niki_parmar                3\n"
     ]
    }
   ],
   "source": [
    "all_relations = research_arcade.get_all_edge_features(\"arxiv_paper_author\")\n",
    "print(f\"Total relationships: {len(all_relations)}\")\n",
    "print(all_relations.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Authors for a Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors:\n",
      "  paper_arxiv_id          author_id  author_sequence\n",
      "0   1706.03762v7  ss_ashish_vaswani                1\n",
      "1   1706.03762v7    ss_noam_shazeer                2\n",
      "2   1706.03762v7     ss_niki_parmar                3\n"
     ]
    }
   ],
   "source": [
    "paper_id = {'paper_arxiv_id': '1706.03762v7'}\n",
    "authors = research_arcade.get_neighborhood(\"arxiv_paper_author\", primary_key=paper_id)\n",
    "print(\"Authors:\")\n",
    "print(authors.sort_values('author_sequence'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Papers by Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers by author:\n",
      "  paper_arxiv_id          author_id  author_sequence\n",
      "0   1706.03762v7  ss_ashish_vaswani                1\n"
     ]
    }
   ],
   "source": [
    "author_id = {'author_id': 'ss_ashish_vaswani'}\n",
    "papers = research_arcade.get_neighborhood(\"arxiv_paper_author\", primary_key=author_id)\n",
    "print(\"Papers by author:\")\n",
    "print(papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Paper-Author Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationship deleted!\n"
     ]
    }
   ],
   "source": [
    "relation_id = {'paper_arxiv_id': '1706.03762v7', 'author_id': 'ss_ashish_vaswani'}\n",
    "research_arcade.delete_edge_by_id(\"arxiv_paper_author\", primary_key=relation_id)\n",
    "print(\"Relationship deleted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4 ArXiv Paper-Category (arxiv_paper_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Paper-Category Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked 1\n",
      "Linked 1\n",
      "Linked 2\n"
     ]
    }
   ],
   "source": [
    "paper_categories = [\n",
    "    {'paper_arxiv_id': '1706.03762v7', 'category_id': '1'},\n",
    "    {'paper_arxiv_id': '1706.03762v7', 'category_id': '1'},\n",
    "    {'paper_arxiv_id': '1706.03762v7', 'category_id': '2'}\n",
    "]\n",
    "for relation in paper_categories:\n",
    "    research_arcade.insert_edge(\"arxiv_paper_category\", edge_features=relation)\n",
    "    print(f\"Linked {relation['category_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get All Paper-Category Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total relationships: 15\n",
      "  paper_arxiv_id  category_id\n",
      "0   1706.03762v7            1\n",
      "1   1706.03762v7            2\n",
      "2   1810.04805v2            1\n",
      "3   1810.04805v2            3\n",
      "4   1706.03762v7            1\n"
     ]
    }
   ],
   "source": [
    "all_relations = research_arcade.get_all_edge_features(\"arxiv_paper_category\")\n",
    "print(f\"Total relationships: {len(all_relations)}\")\n",
    "print(all_relations.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Categories for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories:\n",
      "   paper_arxiv_id  category_id\n",
      "0    1706.03762v7            1\n",
      "1    1706.03762v7            2\n",
      "2    1706.03762v7            1\n",
      "3    1706.03762v7            1\n",
      "4    1706.03762v7            1\n",
      "5    1706.03762v7            1\n",
      "6    1706.03762v7            2\n",
      "7    1706.03762v7            1\n",
      "8    1706.03762v7            1\n",
      "9    1706.03762v7            2\n",
      "10   1706.03762v7            1\n",
      "11   1706.03762v7            1\n",
      "12   1706.03762v7            2\n"
     ]
    }
   ],
   "source": [
    "paper_id = {'paper_arxiv_id': '1706.03762v7'}\n",
    "categories = research_arcade.get_neighborhood(\"arxiv_paper_category\", primary_key=paper_id)\n",
    "print(\"Categories:\")\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Papers in Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers in category:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "category_id = {'category_id': 'cs.LG'}\n",
    "papers = research_arcade.get_neighborhood(\"arxiv_paper_category\", primary_key=category_id)\n",
    "print(\"Papers in category:\")\n",
    "print(papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Paper-Category Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationship deleted!\n"
     ]
    }
   ],
   "source": [
    "relation_id = {'paper_arxiv_id': '1706.03762v7', 'category_id': 'cs.AI'}\n",
    "research_arcade.delete_edge_by_id(\"arxiv_paper_category\", primary_key=relation_id)\n",
    "print(\"Relationship deleted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5 ArXiv Paper-Figure (arxiv_paper_figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Paper-Figure Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked figure 1)\n",
      "Linked figure 2)\n"
     ]
    }
   ],
   "source": [
    "paper_figures = [\n",
    "    {'paper_arxiv_id': '1706.03762v7', 'figure_id': 1},\n",
    "    {'paper_arxiv_id': '1706.03762v7', 'figure_id': 2}\n",
    "]\n",
    "for relation in paper_figures:\n",
    "    research_arcade.insert_edge(\"arxiv_paper_figure\", edge_features=relation)\n",
    "    print(f\"Linked figure {relation['figure_id']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Figures for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figures:\n",
      "  paper_arxiv_id  figure_id\n",
      "0   1706.03762v7          1\n",
      "1   1706.03762v7          2\n"
     ]
    }
   ],
   "source": [
    "paper_id = {'paper_arxiv_id': '1706.03762v7'}\n",
    "figures = research_arcade.get_neighborhood(\"arxiv_paper_figure\", primary_key=paper_id)\n",
    "print(\"Figures:\")\n",
    "print(figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.6 ArXiv Paper-Table (arxiv_paper_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Paper-Table Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked table 1\n",
      "Linked table 2\n"
     ]
    }
   ],
   "source": [
    "paper_tables = [\n",
    "    {'paper_arxiv_id': '1706.03762v7', 'table_id': 1},\n",
    "    {'paper_arxiv_id': '1706.03762v7', 'table_id': 2}\n",
    "]\n",
    "for relation in paper_tables:\n",
    "    research_arcade.insert_edge(\"arxiv_paper_table\", edge_features=relation)\n",
    "    print(f\"Linked table {relation['table_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Tables for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables:\n"
     ]
    }
   ],
   "source": [
    "paper_id = {'paper_arxiv_id': '1706.03762v7'}\n",
    "tables = research_arcade.get_neighborhood(\"arxiv_paper_table\", primary_key=paper_id)\n",
    "print(\"Tables:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.7 ArXiv Paragraph-Reference (arxiv_paragraph_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Paragraph-Reference Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_references = [\n",
    "    {'paragraph_id': 1, 'paper_section': 'established approaches', 'paper_arxiv_id': '1706.03762v7', 'reference_label': \"{something}\", 'reference_type': 'figure'}\n",
    "]\n",
    "\n",
    "for relation in paragraph_references:\n",
    "    research_arcade.insert_edge(\"arxiv_paragraph_reference\", edge_features=relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get References in Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "References:\n",
      "   id  paragraph_id           paper_section paper_arxiv_id reference_label  \\\n",
      "0   1             1  established approaches   1706.03762v7     {something}   \n",
      "1   2             1  established approaches   1706.03762v7     {something}   \n",
      "2   3             1  established approaches   1706.03762v7     {something}   \n",
      "3   4             1  established approaches   1706.03762v7     {something}   \n",
      "\n",
      "  reference_type  \n",
      "0         figure  \n",
      "1         figure  \n",
      "2         figure  \n",
      "3         figure  \n"
     ]
    }
   ],
   "source": [
    "paragraph_id = {'paragraph_id': 1}\n",
    "references = research_arcade.get_neighborhood(\"arxiv_paragraph_reference\", primary_key=paragraph_id)\n",
    "print(\"References:\")\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial has covered:\n",
    "\n",
    "1. Setting up the ResearchArcade database connection\n",
    "2. Working with OpenReview data\n",
    "3. CRUD operations for all ArXiv entity types:\n",
    "   - Papers\n",
    "   - Authors\n",
    "   - Categories\n",
    "   - Figures\n",
    "   - Tables\n",
    "   - Sections\n",
    "   - Paragraphs\n",
    "4. Creating relationships between entities:\n",
    "   - Authorship\n",
    "   - Citations\n",
    "   - Paper-Category links\n",
    "   - Paper-Figure/Table links\n",
    "   - Paragraph-level references\n",
    "5. Advanced querying patterns\n",
    "6. Best practices for data validation\n",
    "\n",
    "For more information, refer to the ResearchArcade documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_arcade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
