{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# ResearchArcade Complete Tutorial\n",
    "\n",
    "This tutorial demonstrates how to work with the ResearchArcade database, covering all node types and edge relationships.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#setup)\n",
    "2. [OpenReview Data](#openreview)\n",
    "3. [ArXiv Papers](#arxiv-papers)\n",
    "4. [ArXiv Authors](#arxiv-authors)\n",
    "5. [ArXiv Categories](#arxiv-categories)\n",
    "6. [ArXiv Figures](#arxiv-figures)\n",
    "7. [ArXiv Tables](#arxiv-tables)\n",
    "8. [ArXiv Sections](#arxiv-sections)\n",
    "9. [ArXiv Paragraphs](#arxiv-paragraphs)\n",
    "10. [Relationships/Edges](#relationships)\n",
    "11. [Advanced Queries](#advanced-queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## 1. Setup <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b9b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from research_arcade.research_arcade import ResearchArcade\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f1a1e4",
   "metadata": {},
   "source": [
    "### Choose Database Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9672a27",
   "metadata": {},
   "source": [
    "#### CSV Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169f7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_type = \"csv\"\n",
    "# config = {\n",
    "#     \"csv_dir\": \"../data/my_research_arcade_data/\"\n",
    "# }\n",
    "\n",
    "# research_arcade = ResearchArcade(db_type=db_type, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea147c5f",
   "metadata": {},
   "source": [
    "#### SQL Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5a88db",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_type = \"sql\"\n",
    "config = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"dbname\": \"postgres\",\n",
    "    \"user\": \"cl195\",\n",
    "    \"password\": \"\",\n",
    "    \"port\": \"5433\"\n",
    "}\n",
    "\n",
    "research_arcade = ResearchArcade(db_type=db_type, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-papers-section",
   "metadata": {},
   "source": [
    "## 3. ArXiv Papers <a name=\"arxiv-papers\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `arxiv_id` (VARCHAR, unique) - e.g., 1802.08773v3\n",
    "- `base_arxiv_id` (VARCHAR) - e.g., 1802.08773\n",
    "- `version` (INT) - e.g., 3\n",
    "- `title` (TEXT)\n",
    "- `abstract` (TEXT)\n",
    "- `submit_date` (DATE)\n",
    "- `metadata` (JSONB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8e215",
   "metadata": {},
   "source": [
    "### Construct Table from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccaeefb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2505.23559 does not have metadata downloaded\n"
     ]
    }
   ],
   "source": [
    "config = {\"arxiv_ids\": [\"2505.23559\", \"1903.03894v4\"], \"dest_dir\": \"./download\"}\n",
    "research_arcade.construct_table_from_api(\"arxiv_papers\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: CSV file ./examples/csv_data/csv_arxiv_papers_example.csv does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"csv_file\": \"./examples/csv_data/csv_arxiv_papers_example.csv\"}\n",
    "research_arcade.construct_table_from_csv(\"arxiv_papers\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: JSON file ./examples/json_data/json_arxiv_papers_example.json does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"json_file\": \"./examples/json_data/json_arxiv_papers_example.json\"}\n",
    "research_arcade.construct_table_from_json(\"arxiv_papers\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-papers-insert",
   "metadata": {},
   "source": [
    "### Insert a Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "insert-paper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Insert the famous \"Attention is All You Need\" paper\n",
    "new_paper = {\n",
    "    'arxiv_id': '1706.03762v7',\n",
    "    'base_arxiv_id': '1706.03762',\n",
    "    'version': 7,\n",
    "    'title': 'Attention Is All You Need',\n",
    "    'abstract': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.',\n",
    "    'submit_date': '2017-06-12',\n",
    "    'metadata': {'venue': 'NeurIPS 2017', 'pdf_url': 'https://arxiv.org/pdf/1706.03762.pdf'}\n",
    "}\n",
    "\n",
    "research_arcade.insert_node(\"arxiv_papers\", node_features=new_paper)\n",
    "print(\"Paper inserted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "insert-paper-bert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT paper inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Insert BERT paper\n",
    "bert_paper = {\n",
    "    'arxiv_id': '1810.04805v2',\n",
    "    'base_arxiv_id': '1810.04805',\n",
    "    'version': 2,\n",
    "    'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',\n",
    "    'abstract': 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.',\n",
    "    'submit_date': '2018-10-11',\n",
    "    'metadata': {'venue': 'NAACL 2019', 'citations': 50000}\n",
    "}\n",
    "\n",
    "research_arcade.insert_node(\"arxiv_papers\", node_features=bert_paper)\n",
    "print(\"BERT paper inserted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-papers-get-all",
   "metadata": {},
   "source": [
    "### Get All Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "get-all-papers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers in database: 3\n",
      "\n",
      "First 5 papers:\n",
      "[(2, '1903.03894v4', '1903.03894', '4', 'GNNExplainer: Generating Explanations for Graph Neural Networks', \"Graph Neural Networks (GNNs) are a powerful tool for machine learning on\\ngraphs.GNNs combine node feature information with the graph structure by\\nrecursively passing neural messages along edges of the input graph. However,\\nincorporating both graph structure and feature information leads to complex\\nmodels, and explaining predictions made by GNNs remains unsolved. Here we\\npropose GNNExplainer, the first general, model-agnostic approach for providing\\ninterpretable explanations for predictions of any GNN-based model on any\\ngraph-based machine learning task. Given an instance, GNNExplainer identifies a\\ncompact subgraph structure and a small subset of node features that have a\\ncrucial role in GNN's prediction. Further, GNNExplainer can generate consistent\\nand concise explanations for an entire class of instances. We formulate\\nGNNExplainer as an optimization task that maximizes the mutual information\\nbetween a GNN's prediction and distribution of possible subgraph structures.\\nExperiments on synthetic and real-world graphs show that our approach can\\nidentify important graph structures as well as node features, and outperforms\\nbaselines by 17.1% on average. GNNExplainer provides a variety of benefits,\\nfrom the ability to visualize semantically relevant structures to\\ninterpretability, to giving insights into errors of faulty GNNs.\", datetime.datetime(2019, 3, 10, 0, 56, 26), {'id': '1903.03894v4', 'url': 'http://arxiv.org/abs/1903.03894v4', 'title': 'GNNExplainer: Generating Explanations for Graph Neural Networks', 'authors': ['Rex Ying', 'Dylan Bourgeois', 'Jiaxuan You', 'Marinka Zitnik', 'Jure Leskovec'], 'abstract': \"Graph Neural Networks (GNNs) are a powerful tool for machine learning on\\ngraphs.GNNs combine node feature information with the graph structure by\\nrecursively passing neural messages along edges of the input graph. However,\\nincorporating both graph structure and feature information leads to complex\\nmodels, and explaining predictions made by GNNs remains unsolved. Here we\\npropose GNNExplainer, the first general, model-agnostic approach for providing\\ninterpretable explanations for predictions of any GNN-based model on any\\ngraph-based machine learning task. Given an instance, GNNExplainer identifies a\\ncompact subgraph structure and a small subset of node features that have a\\ncrucial role in GNN's prediction. Further, GNNExplainer can generate consistent\\nand concise explanations for an entire class of instances. We formulate\\nGNNExplainer as an optimization task that maximizes the mutual information\\nbetween a GNN's prediction and distribution of possible subgraph structures.\\nExperiments on synthetic and real-world graphs show that our approach can\\nidentify important graph structures as well as node features, and outperforms\\nbaselines by 17.1% on average. GNNExplainer provides a variety of benefits,\\nfrom the ability to visualize semantically relevant structures to\\ninterpretability, to giving insights into errors of faulty GNNs.\", 'published': '2019-03-10 00:56:26+00:00', 'categories': ['cs.LG', 'stat.ML']}), (3, '1706.03762v7', '1706.03762', '7', 'Attention Is All You Need', 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.', datetime.datetime(2017, 6, 12, 0, 0), {'venue': 'NeurIPS 2017', 'pdf_url': 'https://arxiv.org/pdf/1706.03762.pdf'}), (4, '1810.04805v2', '1810.04805', '2', 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.', datetime.datetime(2018, 10, 11, 0, 0), {'venue': 'NAACL 2019', 'citations': 50000})]\n"
     ]
    }
   ],
   "source": [
    "arxiv_papers_df = research_arcade.get_all_node_features(\"arxiv_papers\")\n",
    "print(f\"Total papers in database: {len(arxiv_papers_df)}\")\n",
    "print(\"\\nFirst 5 papers:\")\n",
    "print(arxiv_papers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-papers-get-by-id",
   "metadata": {},
   "source": [
    "### Get Specific Paper by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "get-paper-by-id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper details:\n",
      "(4, '1810.04805v2', '1810.04805', '2', 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.', datetime.datetime(2018, 10, 11, 0, 0), {'venue': 'NAACL 2019', 'citations': 50000})\n"
     ]
    }
   ],
   "source": [
    "paper_id = {\"arxiv_id\": \"1810.04805v2\"}\n",
    "paper_features = research_arcade.get_node_features_by_id(\"arxiv_papers\", paper_id)\n",
    "print(\"Paper details:\")\n",
    "print(paper_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-papers-update",
   "metadata": {},
   "source": [
    "### Update a Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "update-paper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper updated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Update metadata for a paper\n",
    "updated_paper = {\n",
    "    'arxiv_id': '1706.03762v7',\n",
    "    'metadata': {\n",
    "        'venue': 'NeurIPS 2017',\n",
    "        'pdf_url': 'https://arxiv.org/pdf/1706.03762.pdf',\n",
    "        'citations': 75000,\n",
    "        'influential': True\n",
    "    }\n",
    "}\n",
    "\n",
    "research_arcade.update_node(\"arxiv_papers\", node_features=updated_paper)\n",
    "print(\"Paper updated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-papers-delete",
   "metadata": {},
   "source": [
    "### Delete a Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "delete-paper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted paper:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Delete a paper by ID\n",
    "paper_id = {\"arxiv_id\": \"1706.03762v7\"}\n",
    "deleted_paper = research_arcade.delete_node_by_id(\"arxiv_papers\", paper_id)\n",
    "print(\"Deleted paper:\")\n",
    "print(deleted_paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-authors-section",
   "metadata": {},
   "source": [
    "## 4. ArXiv Authors <a name=\"arxiv-authors\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `semantic_scholar_id` (VARCHAR, unique)\n",
    "- `name` (VARCHAR)\n",
    "- `homepage` (VARCHAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e14ad06",
   "metadata": {},
   "source": [
    "### Construct Table from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c18c7737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = {\"arxiv_ids\": [\"1903.03894v4\", \"1806.08804v4\"], \"dest_dir\": \"./download\"}\n",
    "# research_arcade.construct_table_from_api(\"arxiv_authors\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: CSV file ./examples/csv_data/csv_arxiv_authors_example.csv does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"csv_file\": \"./examples/csv_data/csv_arxiv_authors_example.csv\"}\n",
    "research_arcade.construct_table_from_csv(\"arxiv_authors\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: JSON file ./examples/json_data/json_arxiv_authors_example.json does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"json_file\": \"./examples/json_data/json_arxiv_authors_example.json\"}\n",
    "research_arcade.construct_table_from_json(\"arxiv_authors\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-authors-insert",
   "metadata": {},
   "source": [
    "### Insert Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "insert-authors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted author: Ashish Vaswani\n",
      "Inserted author: Noam Shazeer\n",
      "Inserted author: Niki Parmar\n",
      "Inserted author: Jakob Uszkoreit\n",
      "Inserted author: Llion Jones\n"
     ]
    }
   ],
   "source": [
    "# Insert authors from the Transformer paper\n",
    "authors = [\n",
    "    {\n",
    "        'semantic_scholar_id': 'ss_ashish_vaswani',\n",
    "        'name': 'Ashish Vaswani',\n",
    "        'homepage': 'https://scholar.google.com/citations?user=oR9sCGYAAAAJ'\n",
    "    },\n",
    "    {\n",
    "        'semantic_scholar_id': 'ss_noam_shazeer',\n",
    "        'name': 'Noam Shazeer',\n",
    "        'homepage': 'https://scholar.google.com/citations?user=oR9sCGYAAAAJ'\n",
    "    },\n",
    "    {\n",
    "        'semantic_scholar_id': 'ss_niki_parmar',\n",
    "        'name': 'Niki Parmar',\n",
    "        'homepage': 'https://scholar.google.com/citations?user=oR9sCGYAAAAJ'\n",
    "    },\n",
    "    {\n",
    "        'semantic_scholar_id': 'ss_jakob_uszkoreit',\n",
    "        'name': 'Jakob Uszkoreit',\n",
    "        'homepage': 'https://scholar.google.com/citations?user=oR9sCGYAAAAJ'\n",
    "    },\n",
    "    {\n",
    "        'semantic_scholar_id': 'ss_llion_jones',\n",
    "        'name': 'Llion Jones',\n",
    "        'homepage': 'https://scholar.google.com/citations?user=oR9sCGYAAAAJ'\n",
    "    }\n",
    "]\n",
    "\n",
    "for author in authors:\n",
    "    research_arcade.insert_node(\"arxiv_authors\", node_features=author)\n",
    "    print(f\"Inserted author: {author['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-authors-get-all",
   "metadata": {},
   "source": [
    "### Get All Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "get-all-authors",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total authors in database: 5\n",
      "\n",
      "All authors:\n",
      "[(1, 'ss_ashish_vaswani', 'Ashish Vaswani', 'https://scholar.google.com/citations?user=oR9sCGYAAAAJ'), (2, 'ss_noam_shazeer', 'Noam Shazeer', 'https://scholar.google.com/citations?user=oR9sCGYAAAAJ'), (3, 'ss_niki_parmar', 'Niki Parmar', 'https://scholar.google.com/citations?user=oR9sCGYAAAAJ'), (4, 'ss_jakob_uszkoreit', 'Jakob Uszkoreit', 'https://scholar.google.com/citations?user=oR9sCGYAAAAJ'), (5, 'ss_llion_jones', 'Llion Jones', 'https://scholar.google.com/citations?user=oR9sCGYAAAAJ')]\n"
     ]
    }
   ],
   "source": [
    "authors_df = research_arcade.get_all_node_features(\"arxiv_authors\")\n",
    "print(f\"Total authors in database: {len(authors_df)}\")\n",
    "print(\"\\nAll authors:\")\n",
    "print(authors_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-authors-get-by-id",
   "metadata": {},
   "source": [
    "### Get Specific Author by ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "get-author-by-id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author details:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "author_id = {\"semantic_scholar_id\": '2288033664'}\n",
    "author_features = research_arcade.get_node_features_by_id(\"arxiv_authors\", author_id)\n",
    "print(\"Author details:\")\n",
    "print(author_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-authors-update",
   "metadata": {},
   "source": [
    "### Update an Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "update-author",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author updated successfully!\n"
     ]
    }
   ],
   "source": [
    "updated_author = {\n",
    "    'semantic_scholar_id': 'ss_ashish_vaswani',\n",
    "    'homepage': 'https://ashishvaswani.com'\n",
    "}\n",
    "\n",
    "research_arcade.update_node(\"arxiv_authors\", node_features=updated_author)\n",
    "print(\"Author updated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-categories-section",
   "metadata": {},
   "source": [
    "## 5. ArXiv Categories <a name=\"arxiv-categories\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `name` (VARCHAR, unique)\n",
    "- `description` (TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9eeea6",
   "metadata": {},
   "source": [
    "### Insert From API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "168633f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1903.03894v4', 'title': 'GNNExplainer: Generating Explanations for Graph Neural Networks', 'abstract': \"Graph Neural Networks (GNNs) are a powerful tool for machine learning on\\ngraphs.GNNs combine node feature information with the graph structure by\\nrecursively passing neural messages along edges of the input graph. However,\\nincorporating both graph structure and feature information leads to complex\\nmodels, and explaining predictions made by GNNs remains unsolved. Here we\\npropose GNNExplainer, the first general, model-agnostic approach for providing\\ninterpretable explanations for predictions of any GNN-based model on any\\ngraph-based machine learning task. Given an instance, GNNExplainer identifies a\\ncompact subgraph structure and a small subset of node features that have a\\ncrucial role in GNN's prediction. Further, GNNExplainer can generate consistent\\nand concise explanations for an entire class of instances. We formulate\\nGNNExplainer as an optimization task that maximizes the mutual information\\nbetween a GNN's prediction and distribution of possible subgraph structures.\\nExperiments on synthetic and real-world graphs show that our approach can\\nidentify important graph structures as well as node features, and outperforms\\nbaselines by 17.1% on average. GNNExplainer provides a variety of benefits,\\nfrom the ability to visualize semantically relevant structures to\\ninterpretability, to giving insights into errors of faulty GNNs.\", 'authors': ['Rex Ying', 'Dylan Bourgeois', 'Jiaxuan You', 'Marinka Zitnik', 'Jure Leskovec'], 'published': '2019-03-10 00:56:26+00:00', 'categories': ['cs.LG', 'stat.ML'], 'url': 'http://arxiv.org/abs/1903.03894v4'}\n",
      "{'id': '1806.08804v4', 'title': 'Hierarchical Graph Representation Learning with Differentiable Pooling', 'abstract': 'Recently, graph neural networks (GNNs) have revolutionized the field of graph\\nrepresentation learning through effectively learned node embeddings, and\\nachieved state-of-the-art results in tasks such as node classification and link\\nprediction. However, current GNN methods are inherently flat and do not learn\\nhierarchical representations of graphs---a limitation that is especially\\nproblematic for the task of graph classification, where the goal is to predict\\nthe label associated with an entire graph. Here we propose DiffPool, a\\ndifferentiable graph pooling module that can generate hierarchical\\nrepresentations of graphs and can be combined with various graph neural network\\narchitectures in an end-to-end fashion. DiffPool learns a differentiable soft\\ncluster assignment for nodes at each layer of a deep GNN, mapping nodes to a\\nset of clusters, which then form the coarsened input for the next GNN layer.\\nOur experimental results show that combining existing GNN methods with DiffPool\\nyields an average improvement of 5-10% accuracy on graph classification\\nbenchmarks, compared to all existing pooling approaches, achieving a new\\nstate-of-the-art on four out of five benchmark data sets.', 'authors': ['Rex Ying', 'Jiaxuan You', 'Christopher Morris', 'Xiang Ren', 'William L. Hamilton', 'Jure Leskovec'], 'published': '2018-06-22 18:04:46+00:00', 'categories': ['cs.LG', 'cs.NE', 'cs.SI', 'stat.ML'], 'url': 'http://arxiv.org/abs/1806.08804v4'}\n"
     ]
    }
   ],
   "source": [
    "config = {\"arxiv_ids\": [\"1903.03894v4\", \"1806.08804v4\"], \"dest_dir\": \"./download\"}\n",
    "research_arcade.construct_table_from_api(\"arxiv_categories\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: CSV file ./examples/csv_data/csv_arxiv_categories_example.csv does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"csv_file\": \"./examples/csv_data/csv_arxiv_categories_example.csv\"}\n",
    "research_arcade.construct_table_from_csv(\"arxiv_categories\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: JSON file ./examples/json_data/json_arxiv_categories_example.json does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"json_file\": \"./examples/json_data/json_arxiv_categories_example.json\"}\n",
    "research_arcade.construct_table_from_json(\"arxiv_categories\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-categories-insert",
   "metadata": {},
   "source": [
    "### Insert Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "insert-categories",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted category: cs.CL\n",
      "Inserted category: cs.LG\n",
      "Inserted category: cs.AI\n",
      "Inserted category: cs.CV\n",
      "Inserted category: stat.ML\n"
     ]
    }
   ],
   "source": [
    "categories = [\n",
    "    {\n",
    "        'name': 'cs.CL',\n",
    "        'description': 'Computation and Language (Natural Language Processing)'\n",
    "    },\n",
    "    {\n",
    "        'name': 'cs.LG',\n",
    "        'description': 'Machine Learning'\n",
    "    },\n",
    "    {\n",
    "        'name': 'cs.AI',\n",
    "        'description': 'Artificial Intelligence'\n",
    "    },\n",
    "    {\n",
    "        'name': 'cs.CV',\n",
    "        'description': 'Computer Vision and Pattern Recognition'\n",
    "    },\n",
    "    {\n",
    "        'name': 'stat.ML',\n",
    "        'description': 'Machine Learning (Statistics)'\n",
    "    }\n",
    "]\n",
    "\n",
    "for category in categories:\n",
    "    research_arcade.insert_node(\"arxiv_categories\", node_features=category)\n",
    "    print(f\"Inserted category: {category['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-categories-get-all",
   "metadata": {},
   "source": [
    "### Get All Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "get-all-categories",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total categories: 7\n",
      "\n",
      "All categories:\n",
      "[(1, 'cs.LG', None), (2, 'stat.ML', None), (4, 'cs.NE', None), (5, 'cs.SI', None), (7, 'cs.CL', 'Computation and Language (Natural Language Processing)'), (9, 'cs.AI', 'Artificial Intelligence'), (10, 'cs.CV', 'Computer Vision and Pattern Recognition')]\n"
     ]
    }
   ],
   "source": [
    "categories_df = research_arcade.get_all_node_features(\"arxiv_categories\")\n",
    "print(f\"Total categories: {len(categories_df)}\")\n",
    "print(\"\\nAll categories:\")\n",
    "print(categories_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-figures-section",
   "metadata": {},
   "source": [
    "## 6. ArXiv Figures <a name=\"arxiv-figures\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `paper_arxiv_id` (VARCHAR FK → papers.arxiv_id)\n",
    "- `path` (VARCHAR)\n",
    "- `caption` (TEXT)\n",
    "- `label` (TEXT)\n",
    "- `name` (TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-figures-insert",
   "metadata": {},
   "source": [
    "### Insert Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "insert-figures",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted Figure 1\n",
      "Inserted Figure 2\n",
      "Inserted Figure 3\n"
     ]
    }
   ],
   "source": [
    "# Insert figures for the Transformer paper\n",
    "figures = [\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'path': '/figures/transformer_architecture.png',\n",
    "        'caption': 'The Transformer model architecture. The left side shows the encoder stack and the right side shows the decoder stack.',\n",
    "        'label': 'fig:architecture',\n",
    "        'name': 'Figure 1'\n",
    "    },\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'path': '/figures/scaled_dot_product_attention.png',\n",
    "        'caption': 'Scaled Dot-Product Attention and Multi-Head Attention mechanisms.',\n",
    "        'label': 'fig:attention',\n",
    "        'name': 'Figure 2'\n",
    "    },\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'path': '/figures/positional_encoding.png',\n",
    "        'caption': 'Positional encoding visualization showing sine and cosine functions of different frequencies.',\n",
    "        'label': 'fig:positional',\n",
    "        'name': 'Figure 3'\n",
    "    }\n",
    "]\n",
    "\n",
    "for figure in figures:\n",
    "    research_arcade.insert_node(\"arxiv_figures\", node_features=figure)\n",
    "    print(f\"Inserted {figure['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-figures-get-all",
   "metadata": {},
   "source": [
    "### Get All Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "get-all-figures",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total figures: 3\n",
      "\n",
      "All figures:\n",
      "[(1, '1706.03762v7', '/figures/transformer_architecture.png', 'The Transformer model architecture. The left side shows the encoder stack and the right side shows the decoder stack.', 'fig:architecture', 'Figure 1'), (2, '1706.03762v7', '/figures/scaled_dot_product_attention.png', 'Scaled Dot-Product Attention and Multi-Head Attention mechanisms.', 'fig:attention', 'Figure 2'), (3, '1706.03762v7', '/figures/positional_encoding.png', 'Positional encoding visualization showing sine and cosine functions of different frequencies.', 'fig:positional', 'Figure 3')]\n"
     ]
    }
   ],
   "source": [
    "figures_df = research_arcade.get_all_node_features(\"arxiv_figures\")\n",
    "print(f\"Total figures: {len(figures_df)}\")\n",
    "print(\"\\nAll figures:\")\n",
    "print(figures_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-tables-section",
   "metadata": {},
   "source": [
    "## 7. ArXiv Tables <a name=\"arxiv-tables\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `paper_arxiv_id` (VARCHAR FK → papers.arxiv_id)\n",
    "- `path` (VARCHAR)\n",
    "- `caption` (TEXT)\n",
    "- `label` (TEXT)\n",
    "- `table_text` (TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c240602b",
   "metadata": {},
   "source": [
    "### Insert From API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54a13d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: ['1903.03894v4']\n",
      "BFS_que.qsize(): 1\n",
      "current paper: 1903.03894v4\n",
      "Thread 131687254062656 Processing 1903.03894v4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "gzip: stdin: unexpected end of file\n",
      "tar: Unexpected EOF in archive\n",
      "tar: Unexpected EOF in archive\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 131687254062656 Finished processing 1903.03894v4 (1/999999999) Time elapsed: 1.11s\n",
      "'NoneType' object is not subscriptable\n",
      "Thread 131687254062656 Failed to process 1903.03894v4\n",
      "Thread 131691059222336 Finished processing 1 papers\n",
      "Error: The file './download/output/1903.03894v4.json' was not found.\n"
     ]
    }
   ],
   "source": [
    "config = {\"arxiv_ids\": [\"1903.03894v4\", \"1806.08804v4\"], \"dest_dir\": \"./download\"}\n",
    "research_arcade.construct_table_from_api(\"arxiv_tables\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: CSV file ./examples/csv_data/csv_arxiv_tables_example.csv does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"csv_file\": \"./examples/csv_data/csv_arxiv_tables_example.csv\"}\n",
    "research_arcade.construct_table_from_csv(\"arxiv_tables\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7002162",
   "metadata": {},
   "source": [
    "### Insert Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b809fdd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted category: cs.CL\n",
      "Inserted category: cs.LG\n",
      "Inserted category: cs.AI\n",
      "Inserted category: cs.CV\n",
      "Inserted category: stat.ML\n"
     ]
    }
   ],
   "source": [
    "categories = [\n",
    "    {\n",
    "        'name': 'cs.CL',\n",
    "        'description': 'Computation and Language (Natural Language Processing)'\n",
    "    },\n",
    "    {\n",
    "        'name': 'cs.LG',\n",
    "        'description': 'Machine Learning'\n",
    "    },\n",
    "    {\n",
    "        'name': 'cs.AI',\n",
    "        'description': 'Artificial Intelligence'\n",
    "    },\n",
    "    {\n",
    "        'name': 'cs.CV',\n",
    "        'description': 'Computer Vision and Pattern Recognition'\n",
    "    },\n",
    "    {\n",
    "        'name': 'stat.ML',\n",
    "        'description': 'Machine Learning (Statistics)'\n",
    "    }\n",
    "]\n",
    "\n",
    "for category in categories:\n",
    "    research_arcade.insert_node(\"arxiv_categories\", node_features=category)\n",
    "    print(f\"Inserted category: {category['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3771de83",
   "metadata": {},
   "source": [
    "### Get All Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f1357fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total categories: 7\n",
      "\n",
      "All categories:\n",
      "[(1, 'cs.LG', None), (2, 'stat.ML', None), (4, 'cs.NE', None), (5, 'cs.SI', None), (7, 'cs.CL', 'Computation and Language (Natural Language Processing)'), (9, 'cs.AI', 'Artificial Intelligence'), (10, 'cs.CV', 'Computer Vision and Pattern Recognition')]\n"
     ]
    }
   ],
   "source": [
    "categories_df = research_arcade.get_all_node_features(\"arxiv_categories\")\n",
    "print(f\"Total categories: {len(categories_df)}\")\n",
    "print(\"\\nAll categories:\")\n",
    "print(categories_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827d6714",
   "metadata": {},
   "source": [
    "## 6. ArXiv Figures <a name=\"arxiv-figures\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `paper_arxiv_id` (VARCHAR FK → papers.arxiv_id)\n",
    "- `path` (VARCHAR)\n",
    "- `caption` (TEXT)\n",
    "- `label` (TEXT)\n",
    "- `name` (TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d33eb",
   "metadata": {},
   "source": [
    "### Insert From API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "195e218d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: ['1903.03894v4']\n",
      "BFS_que.qsize(): 1\n",
      "current paper: 1903.03894v4\n",
      "Thread 131687254062656 Processing 1903.03894v4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "gzip: stdin: unexpected end of file\n",
      "tar: Unexpected EOF in archive\n",
      "tar: Unexpected EOF in archive\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 131687254062656 Finished processing 1903.03894v4 (1/999999999) Time elapsed: 1.23s\n",
      "'NoneType' object is not subscriptable\n",
      "Thread 131687254062656 Failed to process 1903.03894v4\n",
      "Thread 131691059222336 Finished processing 1 papers\n",
      "Error: The file './download/output/1903.03894v4.json' was not found.\n"
     ]
    }
   ],
   "source": [
    "config = {\"arxiv_ids\": [\"1903.03894v4\", \"1806.08804v4\"], \"dest_dir\": \"./download\"}\n",
    "research_arcade.construct_table_from_api(\"arxiv_figures\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: CSV file ./examples/csv_data/csv_arxiv_figures_example.csv does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"csv_file\": \"./examples/csv_data/csv_arxiv_figures_example.csv\"}\n",
    "research_arcade.construct_table_from_csv(\"arxiv_figures\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: JSON file ./examples/json_data/json_arxiv_figures_example.json does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"json_file\": \"./examples/json_data/json_arxiv_figures_example.json\"}\n",
    "research_arcade.construct_table_from_json(\"arxiv_figures\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-tables-insert",
   "metadata": {},
   "source": [
    "### Insert Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "insert-tables",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted table: tab:variations\n",
      "Inserted table: tab:wmt\n",
      "Inserted table: tab:parsing\n"
     ]
    }
   ],
   "source": [
    "# Insert tables for the Transformer paper\n",
    "tables = [\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'path': '/tables/model_variations.tex',\n",
    "        'caption': 'Variations on the Transformer architecture with different hyperparameters.',\n",
    "        'label': 'tab:variations',\n",
    "        'table_text': 'Model | N | d_model | d_ff | h | d_k | d_v | P_drop | train time\\nbase | 6 | 512 | 2048 | 8 | 64 | 64 | 0.1 | 12 hrs'\n",
    "    },\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'path': '/tables/wmt_results.tex',\n",
    "        'caption': 'Performance of the Transformer on WMT 2014 English-German and English-French translation tasks.',\n",
    "        'label': 'tab:wmt',\n",
    "        'table_text': 'Model | EN-DE BLEU | EN-FR BLEU\\nTransformer (base) | 27.3 | 38.1\\nTransformer (big) | 28.4 | 41.8'\n",
    "    },\n",
    "    {\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'path': '/tables/parsing_results.tex',\n",
    "        'caption': 'English constituency parsing results on WSJ test set.',\n",
    "        'label': 'tab:parsing',\n",
    "        'table_text': 'Model | WSJ 23 F1\\nTransformer | 91.3'\n",
    "    }\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "    research_arcade.insert_node(\"arxiv_tables\", node_features=table)\n",
    "    print(f\"Inserted table: {table['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-tables-get-all",
   "metadata": {},
   "source": [
    "### Get All Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "get-all-tables",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tables: 5\n",
      "\n",
      "All tables:\n",
      "[(1, '1806.08804v4', None, '\\\\caption{Classification accuracies in percent. The far-right column gives the relative increase in accuracy compared to the baseline \\\\textsc{GraphSage} approach.}', '\\\\label{tab:results}', '\\\\begin{tabular}{@{}clcccccc@{}}\\\\cmidrule[\\\\heavyrulewidth]{2-8}\\n& \\\\multirow{3}{*}{\\\\vspace*{8pt}\\\\textbf{Method}}&\\\\multicolumn{5}{c}{\\\\textbf{Data Set}}\\\\\\\\\\\\cmidrule{3-8}\\n& & {\\\\textsc{Enzymes}} & {\\\\textsc{D\\\\&D}} & {\\\\textsc{Reddit-Multi-12k}} & {\\\\textsc{Collab}} & {\\\\textsc{Proteins}} & {\\\\text{Gain}}\\n\\\\\\\\ \\\\cmidrule{2-8}\\n\\\\multirow{4}{*}{\\\\rotatebox{90}{\\\\hspace*{-6pt}Kernel}} \\n& \\\\textsc{Graphlet}  & 41.03 & 74.85 &  21.73 & 64.66 & 72.91 &  \\\\\\\\ \\n& \\\\textsc{Shortest-path} & 42.32 & 78.86 & 36.93 & 59.10  & 76.43 &   \\\\\\\\     \\n& \\\\text{1-WL} &  53.43 & 74.02 &  39.03 &  78.61 & 73.76 &  \\\\\\\\     \\n& \\\\text{WL-OA} & 60.13  & 79.04\\t & 44.38  & 80.74  & 75.26  &   \\\\\\\\       \\\\cmidrule{2-8}\\n% GNN\\n& \\\\textsc{PatchySan} & -- & 76.27\\t & 41.32   & 72.60 &  75.00  & 4.17 \\\\\\\\ \\n\\\\multirow{7}{*}{\\\\rotatebox{90}{GNN}} \\n& \\\\textsc{GraphSage} &  54.25 & 75.42 \\t & 42.24  & 68.25  & 70.48 &  --\\\\\\\\ \\n& \\\\textsc{ECC}  & 53.50  & 74.10 & 41.73  & 67.79 &   72.65  & 0.11 \\\\\\\\\\t\\n& \\\\textsc{Set2set} &  60.15 & 78.12  & 43.49 & 71.75 & 74.29 & 3.32 \\\\\\\\ \\n& \\\\textsc{SortPool} & 57.12 & 79.37  & 41.82 & 73.76  & 75.54  & 3.39 \\\\\\\\     \\n& \\\\textsc{\\\\name-Det} & 58.33 & 75.47 & 46.18 & \\\\textbf{82.13} & 75.62 & 5.42 \\\\\\\\ \\n& \\\\textsc{\\\\name-NoLP} & 61.95  & 79.98\\t & 46.65  & 75.58   &  76.22  &  5.95 \\\\\\\\ \\n& \\\\textsc{\\\\name} & \\\\textbf{62.53}  & \\\\textbf{80.64}\\t & \\\\textbf{47.08}  & 75.48   &  \\\\textbf{76.25}  & \\\\textbf{6.27}\\\\\\\\     \\n\\\\cmidrule[\\\\heavyrulewidth]{2-8}\\n\\\\end{tabular}'), (2, '1806.08804v4', None, '\\\\caption{Accuracy results of applying \\\\name to \\\\textsc{S2V}.}', '\\\\label{tab:results2}', '\\\\begin{tabular}{@{}clccc@{}}\\\\cmidrule[\\\\heavyrulewidth]{2-5}\\n& \\\\multirow{3}{*}{\\\\vspace*{8pt}\\\\textbf{Data Set}}&\\\\multicolumn{3}{c}{\\\\textbf{Method}}\\\\\\\\\\\\cmidrule{3-5}\\n& & {\\\\textsc{S2V}} & {\\\\textsc{S2V with 1 DiffPool}} & {\\\\textsc{S2V with 2 DiffPool}}\\n\\\\\\\\ \\\\cmidrule{2-5}\\n& \\\\textsc{Enzymes}  & \\t61.10 & 62.86   & \\\\textbf{63.33}  \\\\\\\\ \\n& \\\\textsc{D\\\\&D} & 78.92 & 80.75 &   \\\\textbf{82.07}  \\\\\\\\     \\n\\\\cmidrule[\\\\heavyrulewidth]{2-5}\\n\\\\end{tabular}'), (3, '1706.03762v7', '/tables/model_variations.tex', 'Variations on the Transformer architecture with different hyperparameters.', 'tab:variations', 'Model | N | d_model | d_ff | h | d_k | d_v | P_drop | train time\\nbase | 6 | 512 | 2048 | 8 | 64 | 64 | 0.1 | 12 hrs'), (4, '1706.03762v7', '/tables/wmt_results.tex', 'Performance of the Transformer on WMT 2014 English-German and English-French translation tasks.', 'tab:wmt', 'Model | EN-DE BLEU | EN-FR BLEU\\nTransformer (base) | 27.3 | 38.1\\nTransformer (big) | 28.4 | 41.8'), (5, '1706.03762v7', '/tables/parsing_results.tex', 'English constituency parsing results on WSJ test set.', 'tab:parsing', 'Model | WSJ 23 F1\\nTransformer | 91.3')]\n"
     ]
    }
   ],
   "source": [
    "tables_df = research_arcade.get_all_node_features(\"arxiv_tables\")\n",
    "print(f\"Total tables: {len(tables_df)}\")\n",
    "print(\"\\nAll tables:\")\n",
    "print(tables_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-sections-section",
   "metadata": {},
   "source": [
    "## 8. ArXiv Sections <a name=\"arxiv-sections\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `content` (TEXT)\n",
    "- `title` (TEXT)\n",
    "- `appendix` (BOOLEAN)\n",
    "- `paper_arxiv_id` (VARCHAR FK → papers.arxiv_id)\n",
    "- `section_in_paper_id` (INT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9890560",
   "metadata": {},
   "source": [
    "### Insert From API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c1735fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: ['1903.03894v4']\n",
      "BFS_que.qsize(): 1\n",
      "current paper: 1903.03894v4\n",
      "Thread 131687254062656 Processing 1903.03894v4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "gzip: stdin: unexpected end of file\n",
      "tar: Unexpected EOF in archive\n",
      "tar: Unexpected EOF in archive\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 131687254062656 Finished processing 1903.03894v4 (1/999999999) Time elapsed: 1.10s\n",
      "'NoneType' object is not subscriptable\n",
      "Thread 131687254062656 Failed to process 1903.03894v4\n",
      "Thread 131691059222336 Finished processing 1 papers\n",
      "Error: The file './download/output/1903.03894v4.json' was not found.\n",
      "An unexpected error occurred: SQLArxivSections.insert_section() got an unexpected keyword argument 'is_appendix'\n"
     ]
    }
   ],
   "source": [
    "config = {\"arxiv_ids\": [\"1903.03894v4\", \"1806.08804v4\"], \"dest_dir\": \"./download\"}\n",
    "research_arcade.construct_table_from_api(\"arxiv_sections\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: CSV file ./examples/csv_data/csv_arxiv_sections_example.csv does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"csv_file\": \"./examples/csv_data/csv_arxiv_sections_example.csv\"}\n",
    "research_arcade.construct_table_from_csv(\"arxiv_sections\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: JSON file ./examples/json_data/json_arxiv_sections_example.json does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"json_file\": \"./examples/json_data/json_arxiv_sections_example.json\"}\n",
    "research_arcade.construct_table_from_json(\"arxiv_sections\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-sections-insert",
   "metadata": {},
   "source": [
    "### Insert Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "insert-sections",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted section: Introduction\n",
      "Inserted section: Background\n",
      "Inserted section: Model Architecture\n",
      "Inserted section: Training\n",
      "Inserted section: Results\n",
      "Inserted section: Conclusion\n"
     ]
    }
   ],
   "source": [
    "# Insert sections for the Transformer paper\n",
    "sections = [\n",
    "    {\n",
    "        'content': 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder...',\n",
    "        'title': 'Introduction',\n",
    "        'appendix': False,\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'section_in_paper_id': 1\n",
    "    },\n",
    "    {\n",
    "        'content': 'Most competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps an input sequence of symbol representations...',\n",
    "        'title': 'Background',\n",
    "        'appendix': False,\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'section_in_paper_id': 2\n",
    "    },\n",
    "    {\n",
    "        'content': 'Most neural sequence transduction models have an encoder-decoder structure. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers...',\n",
    "        'title': 'Model Architecture',\n",
    "        'appendix': False,\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'section_in_paper_id': 3\n",
    "    },\n",
    "    {\n",
    "        'content': 'In this section we describe the training regime for our models...',\n",
    "        'title': 'Training',\n",
    "        'appendix': False,\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'section_in_paper_id': 4\n",
    "    },\n",
    "    {\n",
    "        'content': 'On the WMT 2014 English-to-German translation task, the big transformer model outperforms the best previously reported models...',\n",
    "        'title': 'Results',\n",
    "        'appendix': False,\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'section_in_paper_id': 5\n",
    "    },\n",
    "    {\n",
    "        'content': 'In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers...',\n",
    "        'title': 'Conclusion',\n",
    "        'appendix': False,\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'section_in_paper_id': 6\n",
    "    }\n",
    "]\n",
    "\n",
    "for section in sections:\n",
    "    research_arcade.insert_node(\"arxiv_sections\", node_features=section)\n",
    "    print(f\"Inserted section: {section['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-sections-get-all",
   "metadata": {},
   "source": [
    "### Get All Sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "get-all-sections",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sections: [(1, 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder...', 'Introduction', False, '1706.03762v7'), (2, 'Most competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps an input sequence of symbol representations...', 'Background', False, '1706.03762v7'), (3, 'Most neural sequence transduction models have an encoder-decoder structure. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers...', 'Model Architecture', False, '1706.03762v7'), (4, 'In this section we describe the training regime for our models...', 'Training', False, '1706.03762v7'), (5, 'On the WMT 2014 English-to-German translation task, the big transformer model outperforms the best previously reported models...', 'Results', False, '1706.03762v7'), (6, 'In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers...', 'Conclusion', False, '1706.03762v7')]\n",
      "\n",
      "All sections:\n",
      "[(1, 'The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder...', 'Introduction', False, '1706.03762v7'), (2, 'Most competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps an input sequence of symbol representations...', 'Background', False, '1706.03762v7'), (3, 'Most neural sequence transduction models have an encoder-decoder structure. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers...', 'Model Architecture', False, '1706.03762v7'), (4, 'In this section we describe the training regime for our models...', 'Training', False, '1706.03762v7'), (5, 'On the WMT 2014 English-to-German translation task, the big transformer model outperforms the best previously reported models...', 'Results', False, '1706.03762v7'), (6, 'In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers...', 'Conclusion', False, '1706.03762v7')]\n"
     ]
    }
   ],
   "source": [
    "sections_df = research_arcade.get_all_node_features(\"arxiv_sections\")\n",
    "print(f\"Total sections: {sections_df}\")\n",
    "print(\"\\nAll sections:\")\n",
    "print(sections_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-paragraphs-section",
   "metadata": {},
   "source": [
    "## 9. ArXiv Paragraphs <a name=\"arxiv-paragraphs\"></a>\n",
    "\n",
    "### Table Schema\n",
    "- `id` (SERIAL PK)\n",
    "- `paragraph_id` (INT)\n",
    "- `content` (TEXT)\n",
    "- `paper_arxiv_id` (VARCHAR FK → papers.arxiv_id)\n",
    "- `paper_section` (TEXT)\n",
    "- `section_id` (INT)\n",
    "- `paragraph_in_paper_id` (INT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3fd1cc",
   "metadata": {},
   "source": [
    "### Insert From API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "480dabdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: ['1903.03894v4']\n",
      "BFS_que.qsize(): 1\n",
      "current paper: 1903.03894v4\n",
      "Thread 131687254062656 Processing 1903.03894v4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "gzip: stdin: unexpected end of file\n",
      "tar: Unexpected EOF in archive\n",
      "tar: Unexpected EOF in archive\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 131687254062656 Finished processing 1903.03894v4 (1/999999999) Time elapsed: 1.11s\n",
      "'NoneType' object is not subscriptable\n",
      "Thread 131687254062656 Failed to process 1903.03894v4\n",
      "Thread 131691059222336 Finished processing 1 papers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 671.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading ./download/output/1903.03894v4.json: [Errno 2] No such file or directory: './download/output/1903.03894v4.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 156.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading ./download/output/1903.03894v4.json: [Errno 2] No such file or directory: './download/output/1903.03894v4.json'\n",
      "1806.08804v4\n",
      "Key to References: {'fig:assignment_vis': 'figures_3', 'tab:results': 'table_4', 'tab:results2': 'table_5'}\n",
      "tab:results\n",
      "tab:results2\n",
      "Paper count:  1\n",
      "Total nodes:  113\n",
      "Total edges:  210\n",
      "Paper nodes:  1\n",
      "Figure nodes:  0\n",
      "Table nodes:  2\n",
      "Text nodes:  110\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config = {\"arxiv_ids\": [\"1903.03894v4\", \"1806.08804v4\"], \"dest_dir\": \"./download\"}\n",
    "research_arcade.construct_table_from_api(\"arxiv_paragraphs\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: CSV file ./examples/csv_data/csv_arxiv_paragraphs_example.csv does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"csv_file\": \"./examples/csv_data/csv_arxiv_paragraphs_example.csv\"}\n",
    "research_arcade.construct_table_from_csv(\"arxiv_paragraphs\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: JSON file ./examples/json_data/json_arxiv_paragraphs_example.json does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"json_file\": \"./examples/json_data/json_arxiv_paragraphs_example.json\"}\n",
    "research_arcade.construct_table_from_json(\"arxiv_paragraphs\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-paragraphs-insert",
   "metadata": {},
   "source": [
    "### Insert Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "insert-paragraphs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted paragraph 1 from Introduction\n",
      "Inserted paragraph 2 from Introduction\n",
      "Inserted paragraph 3 from Introduction\n",
      "Inserted paragraph 4 from Introduction\n",
      "Inserted paragraph 5 from Introduction\n"
     ]
    }
   ],
   "source": [
    "# Insert paragraphs from the Introduction section\n",
    "paragraphs = [\n",
    "    {\n",
    "        'paragraph_id': 1,\n",
    "        'content': 'Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation.',\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'paper_section': 'Introduction',\n",
    "        'section_id': 1,\n",
    "        'paragraph_in_paper_id': 1\n",
    "    },\n",
    "    {\n",
    "        'paragraph_id': 2,\n",
    "        'content': 'Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures. Recurrent models typically factor computation along the symbol positions of the input and output sequences.',\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'paper_section': 'Introduction',\n",
    "        'section_id': 1,\n",
    "        'paragraph_in_paper_id': 2\n",
    "    },\n",
    "    {\n",
    "        'paragraph_id': 3,\n",
    "        'content': 'Aligning the positions to steps in computation time, they generate a sequence of hidden states h_t, as a function of the previous hidden state h_{t-1} and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.',\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'paper_section': 'Introduction',\n",
    "        'section_id': 1,\n",
    "        'paragraph_in_paper_id': 3\n",
    "    },\n",
    "    {\n",
    "        'paragraph_id': 4,\n",
    "        'content': 'Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences.',\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'paper_section': 'Introduction',\n",
    "        'section_id': 1,\n",
    "        'paragraph_in_paper_id': 4\n",
    "    },\n",
    "    {\n",
    "        'paragraph_id': 5,\n",
    "        'content': 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.',\n",
    "        'paper_arxiv_id': '1706.03762v7',\n",
    "        'paper_section': 'Introduction',\n",
    "        'section_id': 1,\n",
    "        'paragraph_in_paper_id': 5\n",
    "    }\n",
    "]\n",
    "\n",
    "for paragraph in paragraphs:\n",
    "    research_arcade.insert_node(\"arxiv_paragraphs\", node_features=paragraph)\n",
    "    print(f\"Inserted paragraph {paragraph['paragraph_id']} from {paragraph['paper_section']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arxiv-paragraphs-get-all",
   "metadata": {},
   "source": [
    "### Get All Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "get-all-paragraphs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total paragraphs: 115\n",
      "\n",
      "First 3 paragraphs:\n",
      "[(1, '0', '\\\\label{sec:intro}\\nIn recent years there has been a surge of interest in developing graph neural networks (GNNs)---general deep learning architectures that can operate over graph structured data, such as social network data \\\\cite{hamilton2017inductive,kipf2017semi,Vel+2018} or graph-based representations of molecules \\\\cite{dai2016discriminative,Duv+2015,Gil+2017}.\\nThe general approach with GNNs is to view the underlying graph as a computation graph and learn neural network primitives that generate individual node embeddings by passing, transforming, and aggregating node feature information across the graph~\\\\cite{Gil+2017,hamilton2017inductive}.\\nThe generated node embeddings can then be used as input to any differentiable prediction layer, e.g., for node classification \\\\cite{hamilton2017inductive} or link prediction \\\\cite{Sch+2017}, and the whole model can be trained in an end-to-end fashion.', '1806.08804v4', 'Introduction'), (2, '1', 'However, a major limitation of current GNN architectures is that they are inherently {\\\\em flat}\\\\em flat as they only propagate information across the edges of the graph and are unable to infer and aggregate the information in a \\\\textit{hierarchical} way. \\nFor example, in order to successfully encode the graph structure of organic molecules, one would ideally want to encode the local molecular structure (e.g., individual atoms and their direct bonds) as well as the coarse-grained structure of the molecular graph (e.g., groups of atoms and bonds representing functional units in a molecule).\\nThis lack of hierarchical structure is especially problematic for the task of graph classification, where the goal is to predict the label associated with an \\\\textit{entire graph}. When applying GNNs to graph classification, the standard approach is to generate embeddings for all the nodes in the graph and then to {\\\\em globally pool}\\\\em globally pool all these node embeddings together, e.g., using a simple summation or neural network that operates over sets \\\\cite{dai2016discriminative,Duv+2015,Gil+2017,Li+2016}. This global pooling approach ignores any hierarchical structure that might be present in the graph, and it prevents researchers from building effective GNN models for predictive tasks over entire graphs.', '1806.08804v4', 'Introduction'), (3, '2', \"Here we propose \\\\name, a differentiable graph pooling module that can be adapted to various graph neural network architectures in an hierarchical and end-to-end fashion (Figure~\\\\ref{fig:illustration}). \\n\\\\name allows for developing deeper GNN models that can learn to operate on hierarchical representations of a graph. We develop a graph analogue of the spatial pooling operation in CNNs \\\\cite{krizhevsky2012imagenet}, which allows for deep CNN architectures to iteratively operate on coarser and coarser representations of an image. The challenge in the GNN setting---compared to standard CNNs---is that graphs contain no natural notion of spatial locality, i.e., one cannot simply pool together all nodes in a ``$m \\\\times m$m \\\\times m patch'' on a graph, because the complex topological structure of graphs precludes any straightforward, deterministic definition of a ``patch''. Moreover, unlike image data, graph data sets often contain graphs with varying numbers of nodes and edges, which makes defining a general graph pooling operator even more challenging.\", '1806.08804v4', 'Introduction'), (4, '3', \"In order to solve the above challenges, we require a model that learns how to cluster together nodes to build a hierarchical multi-layer scaffold on top of the underlying graph. \\nOur approach \\\\name learns a differentiable soft assignment at each layer of a deep GNN, mapping nodes to a set of clusters based on their learned embeddings. \\nIn this framework, we generate deep GNNs by ``stacking'' GNN layers in a hierarchical fashion (Figure~\\\\ref{fig:illustration}): the input nodes at the layer $l$l GNN module correspond to the clusters learned at the layer $l-1$l-1 GNN module. \\nThus, each layer of \\\\name coarsens the input graph more and more, and \\\\name is able to generate a hierarchical representation of any input graph after training.\\nWe show that \\\\name\\\\ can be combined with various GNN approaches, resulting in an average 7\\\\% gain in accuracy and a new state of the art on four out of five benchmark graph classification tasks. \\nFinally,  we show that \\\\name\\\\ can learn interpretable hierarchical clusters that correspond to well-defined communities in the input graphs.\\n\\\\cut{\\n\\\\jure{Say much much more about our method, how it works and why it is cool.}\", '1806.08804v4', 'Introduction'), (5, '4', '\\\\chris{Maybe add highlevel visualization of the idea.}', '1806.08804v4', 'Introduction'), (6, '5', '\\\\jure{ I strongly agree, we need a Figure 1 with an illustration of the method.\\nFor example, a graph and then a hierarchical multi-layer scaffold on top of it.\\nWill, you are great at creating good figures. Do you want to give it a try?}\\n\\\\will{Yes, I will brainstorm with Rex today about a figure and put something together :)}', '1806.08804v4', 'Introduction'), (7, '6', '\\\\chris{Quickly sketched this. I think it somehow explains the high-level idea. Guess the equations are too much.}\\n}\\n\\\\jure{Say much much more about our method, how it works and why it is cool.}Say much much more about our method, how it works and why it is cool.', '1806.08804v4', 'Introduction'), (8, '7', '\\\\chris{Maybe add highlevel visualization of the idea.}Maybe add highlevel visualization of the idea.', '1806.08804v4', 'Introduction'), (9, '8', '\\\\jure{ I strongly agree, we need a Figure 1 with an illustration of the method.\\nFor example, a graph and then a hierarchical multi-layer scaffold on top of it.\\nWill, you are great at creating good figures. Do you want to give it a try?} I strongly agree, we need a Figure 1 with an illustration of the method.\\nFor example, a graph and then a hierarchical multi-layer scaffold on top of it.\\nWill, you are great at creating good figures. Do you want to give it a try?\\n\\\\will{Yes, I will brainstorm with Rex today about a figure and put something together :)}Yes, I will brainstorm with Rex today about a figure and put something together :)', '1806.08804v4', 'Introduction'), (10, '9', '\\\\chris{Quickly sketched this. I think it somehow explains the high-level idea. Guess the equations are too much.}Quickly sketched this. I think it somehow explains the high-level idea. Guess the equations are too much.', '1806.08804v4', 'Introduction'), (11, '0', 'Our work builds upon a rich line of recent research on graph neural networks and graph classification.', '1806.08804v4', 'Related Work'), (12, '1', \"\\\\xhdr{General graph neural networks}General graph neural networks\\nA wide variety of graph neural network (GNN) models have been proposed in recent years, including methods inspired by convolutional neural networks \\\\cite{Bru+2014,Def+2015,Duv+2015,hamilton2017inductive,kipf2017semi,Lei+2017,niepert2016learning, Vel+2018}, recurrent neural networks \\\\cite{Li+2016}, recursive neural networks~\\\\cite{bianchini2001,Sca+2009} and loopy belief propagation \\\\cite{dai2016discriminative}. \\nMost of these approaches fit within the framework of ``neural message passing'' proposed by Gilmer \\\\etal~\\\\cite{Gil+2017}. \\nIn the message passing framework, a GNN is viewed as a message passing algorithm where node representations are iteratively computed from the features of their neighbor nodes using a differentiable aggregation function.\\nHamilton \\\\etal~\\\\cite{Ham+2017a} provides a conceptual review of recent advancements in this area, and Bronstein \\\\etal \\\\cite{bronstein2017geometric} outlines connections to spectral graph convolutions.\", '1806.08804v4', 'Related Work'), (13, '2', \"\\\\xhdr{Graph classification with graph neural networks}Graph classification with graph neural networks\\nGNNs have been applied to a wide variety of tasks, including node classification \\\\cite{hamilton2017inductive,kipf2017semi}, link prediction \\\\cite{kipf2018}, graph classification \\\\cite{dai2016discriminative,Duv+2015,zhang2018end}, and chemoinformatics~\\\\cite{Mer+2005,Lus+2013,Fou+2017,Jin+2018,Sch+2017}. \\nIn the context of graph classification---the task that we study here---a major challenge in applying GNNs is going from node embeddings, which are the output of GNNs, to a representation of the entire graph. \\nCommon approaches to this problem include simply summing up or averaging all the node embeddings in a final layer \\\\cite{Duv+2015}, introducing a ``virtual node'' that is connected to all the nodes in the graph \\\\cite{Li+2016}, or aggregating the node embeddings using a deep learning architecture that operates over sets \\\\cite{Gil+2017}.\\nHowever, all of these methods have the limitation that they do not learn hierarchical representations (i.e., all the node embeddings are globally pooled together in a single layer), and thus are unable to capture the natural structures of many real-world graphs.\\nSome recent approaches have also proposed applying CNN architectures to the concatenation of all the node embeddings \\\\cite{niepert2016learning,zhang2018end}, but this requires a specifying (or learning) a canonical ordering over nodes, which is in general very difficult and equivalent to solving graph isomorphism.\", '1806.08804v4', 'Related Work'), (14, '3', 'Lastly, there are some recent works that learn hierarchical graph representations by combining GNNs with deterministic graph clustering algorithms \\\\cite{Def+2015,simonovsky2017dynamic,Fey+2018}, following a two-stage approach. However, unlike these previous approaches, we seek to {\\\\em learn}\\\\em learn the hierarchical structure in an end-to-end fashion, rather than relying on a deterministic graph clustering subroutine.', '1806.08804v4', 'Related Work'), (15, '4', \"\\\\cut{\\n% Will: Let's try to move some of this to related work\\n\\\\rex{need to be removed probably. there are some points i think are important which is not illustrated in related work}\\nTo achieve this task. recent works have made several attempts.\\nRecent attempts include the use of linearization of graphs. A graph can be linearized to a vector representation using ``canonical ordering'' \\\\cite{niepert2016learning}. Pooling or striding is easy to carry out on the 1D representation. However, it is non-trivial to specify a canonical ordering that also preserves graph structure. For example, nodes that are distant in graph might be adjacent to each other in the linearized representation.\", '1806.08804v4', 'Related Work'), (16, '5', \"Another intuitive pooling strategy is to pool using a graph clustering or coarsening strategy \\\\cite{safro2015advanced}, which can be applied hierarchically and produce coarser and coarser graphs.\\nHowever, a variety of clustering and coarsening strategies have been designed, and significant hand-engineering is required to find the best strategy for a specific classification task at hand.\\n}\\n\\\\rex{need to be removed probably. there are some points i think are important which is not illustrated in related work}need to be removed probably. there are some points i think are important which is not illustrated in related work\\nTo achieve this task. recent works have made several attempts.\\nRecent attempts include the use of linearization of graphs. A graph can be linearized to a vector representation using ``canonical ordering'' \\\\cite{niepert2016learning}. Pooling or striding is easy to carry out on the 1D representation. However, it is non-trivial to specify a canonical ordering that also preserves graph structure. For example, nodes that are distant in graph might be adjacent to each other in the linearized representation.\", '1806.08804v4', 'Related Work'), (17, '6', 'Another intuitive pooling strategy is to pool using a graph clustering or coarsening strategy \\\\cite{safro2015advanced}, which can be applied hierarchically and produce coarser and coarser graphs.\\nHowever, a variety of clustering and coarsening strategies have been designed, and significant hand-engineering is required to find the best strategy for a specific classification task at hand.', '1806.08804v4', 'Related Work'), (18, '7', '\\\\cut{\\n\\\\xhdr{Graph kernels}', '1806.08804v4', 'Related Work'), (19, '8', 'In recent years, various graph kernels have been proposed, which\\nimplicitly or explicitly map graphs to a Hilbert space. Gärtner \\\\etal\\\\cite{Gae+2003}\\nand Kashima \\\\etal~\\\\cite{Kas+2003} simultaneously proposed\\ngraph kernels based on random walks, which count the number of walks\\ntwo graphs have in common. Since then, random walk kernels have been\\nstudied intensively, e.g., see~\\\\cite{Sug+2015}.\\nKernels based on shortest paths were first proposed in~\\\\cite{Borgwardt2005}.', '1806.08804v4', 'Related Work'), (20, '9', 'A different line in the development of graph kernels focused\\nparticularly on scalable graph kernels. These kernels are typically\\ncomputed efficiently by explicit feature maps, which allow to bypass\\nthe computation of a gram matrix, and allow applying scalable linear\\nclassification algorithms. Prominent examples are kernels based on\\nsubgraphs up to a fixed size, so called\\n{graphlets}~\\\\cite{She+2009}. Other approaches of this category encode\\nthe neighborhood of every node by different techniques, e.g.,\\nsee~\\\\cite{Hid+2009, Neu+2016}, and most notably the\\nWeisfeiler-Lehman subtree kernel~\\\\cite{She+2011} and its higher-order\\nvariants~\\\\cite{Mor+2017}. Subgraph and Weisfeiler-Lehman kernels have\\nbeen successfully employed within frameworks for smoothed and deep\\ngraph kernels~\\\\cite{Yan+2015,Yan+2015a}. Recent developments include\\nassignment-based approaches~\\\\cite{kriege2016valid,Nik+2017,Joh+2015} and\\nspectral approaches~\\\\cite{Kon+2016}. Although graph kernels have shown good performance in the past, they lack the ability to adapt to they give data distribution at hand, since they mostly rely on precomputed features. Moreover, they may not scale to large data sets due to the quadratic overhead to compute the gram matrix.', '1806.08804v4', 'Related Work'), (21, '10', \"Niepert \\\\etal~\\\\cite{niepert2016learning} extracted canonical vector representations of neighborhoods of nodes,  based on heuristics such as the Weisfeiler-Lehman algorithm~\\\\cite{She+2011}, to compute a graph embeddings and then used neural networks for the classification task. Moreover, recently graph neural networks (GNN) for node and graph classification became popular. Most of these\\napproaches fit into the framework proposed by~\\\\cite{Gil+2017}. Here a GNN is viewed as a message passing approach where node\\nfeatures are iteratively computed from the features of the\\nnode's neighbors by using a differentialable neighborhood aggregation function. The parameters of this function are \\nlearned together with the parameters of the neural network used for the\\nclassification task in an end-to-end fashion. Up to now, naïve global mean pooling or precomputed clustering methods were applied to compute graph embeddings from the features of each single node.\", '1806.08804v4', 'Related Work'), (34, '4', \"\\\\xhdr{Graph neural networks}Graph neural networks\\nIn this work, we build upon graph neural networks in order to learn useful representations for graph classification in an end-to-end fashion. \\nIn particular, we consider GNNs that employ the following general ``message-passing'' architecture:\\n\\\\begin{equation}\\\\label{eq:gnn}\\n     H^{(k)} = M(A,H^{(k-1)};\\\\theta^{(k)}),\\n\\\\end{equation}\\\\begin{equation}\\\\label{eq:gnn}\\n     H^{(k)} = M(A,H^{(k-1)};\\\\theta^{(k)}),\\n\\\\end{equation}\\\\label{eq:gnn}\\n     H^{(k)}(k) = M(A,H^{(k-1)}(k-1);\\\\theta^{(k)}(k)),\", '1806.08804v4', 'Proposed Method'), (22, '11', 'Notable instances of this model include Neural\\nFingerprints~\\\\cite{Duv+2015}, Gated Graph Neural\\nNetworks~\\\\cite{Li+2016}, and spectral approaches proposed\\nin~\\\\cite{Bru+2014,Def+2015,kipf2017semi}. Dai \\\\etal\\\\cite{dai2016discriminative}\\nproposed an approach inspired by mean-field inference\\nand Lei \\\\etal\\\\cite{Lei+2017} showed that the generated\\nfeature maps lie in the same Hilbert space as some popular graph\\nkernels and successfully applied them in the domain of\\nchemoinformatics~\\\\cite{Jin+2018}. In~\\\\cite{simonovsky2017dynamic} GNN were extended to include edge features and various precomputed pooling heuristics based on clustering methods were applied. Attention-based extensions were explored in~\\\\cite{Vel+2018}. In order to make GNNs scale to large graphs Hamilton \\\\etal\\\\cite{hamilton2017inductive} and Chen \\\\etal\\\\cite{Che+2018} devised stochastic versions of GNNs. Recently, a differentiable pooling mechanism to compute graph embeddings based on node features using differentiable sorting was proposed~\\\\cite{zhang2018end}.', '1806.08804v4', 'Related Work'), (23, '12', 'Moreover, GNNs were applied for protein-protein\\ninteraction prediction~\\\\cite{Fou+2017} and quantum interactions in\\nmolecules~\\\\cite{Sch+2017}. An approach for unsupervised learning based\\non GNNs was presented in~\\\\cite{Gar+2017}. Early\\napproaches were published in~\\\\cite{Mer+2005} and~\\\\cite{Sca+2009,\\nSca+2009a}. A survey can be found in~\\\\cite{Ham+2017a}.', '1806.08804v4', 'Related Work'), (24, '13', '}\\n\\\\xhdr{Graph kernels}Graph kernels', '1806.08804v4', 'Related Work'), (25, '14', 'In recent years, various graph kernels have been proposed, which\\nimplicitly or explicitly map graphs to a Hilbert space. Gärtner \\\\etal\\\\cite{Gae+2003}\\nand Kashima \\\\etal~\\\\cite{Kas+2003} simultaneously proposed\\ngraph kernels based on random walks, which count the number of walks\\ntwo graphs have in common. Since then, random walk kernels have been\\nstudied intensively, e.g., see~\\\\cite{Sug+2015}.\\nKernels based on shortest paths were first proposed in~\\\\cite{Borgwardt2005}.', '1806.08804v4', 'Related Work'), (26, '15', 'A different line in the development of graph kernels focused\\nparticularly on scalable graph kernels. These kernels are typically\\ncomputed efficiently by explicit feature maps, which allow to bypass\\nthe computation of a gram matrix, and allow applying scalable linear\\nclassification algorithms. Prominent examples are kernels based on\\nsubgraphs up to a fixed size, so called\\n{graphlets}graphlets~\\\\cite{She+2009}. Other approaches of this category encode\\nthe neighborhood of every node by different techniques, e.g.,\\nsee~\\\\cite{Hid+2009, Neu+2016}, and most notably the\\nWeisfeiler-Lehman subtree kernel~\\\\cite{She+2011} and its higher-order\\nvariants~\\\\cite{Mor+2017}. Subgraph and Weisfeiler-Lehman kernels have\\nbeen successfully employed within frameworks for smoothed and deep\\ngraph kernels~\\\\cite{Yan+2015,Yan+2015a}. Recent developments include\\nassignment-based approaches~\\\\cite{kriege2016valid,Nik+2017,Joh+2015} and\\nspectral approaches~\\\\cite{Kon+2016}. Although graph kernels have shown good performance in the past, they lack the ability to adapt to they give data distribution at hand, since they mostly rely on precomputed features. Moreover, they may not scale to large data sets due to the quadratic overhead to compute the gram matrix.', '1806.08804v4', 'Related Work'), (27, '16', \"Niepert \\\\etal~\\\\cite{niepert2016learning} extracted canonical vector representations of neighborhoods of nodes,  based on heuristics such as the Weisfeiler-Lehman algorithm~\\\\cite{She+2011}, to compute a graph embeddings and then used neural networks for the classification task. Moreover, recently graph neural networks (GNN) for node and graph classification became popular. Most of these\\napproaches fit into the framework proposed by~\\\\cite{Gil+2017}. Here a GNN is viewed as a message passing approach where node\\nfeatures are iteratively computed from the features of the\\nnode's neighbors by using a differentialable neighborhood aggregation function. The parameters of this function are \\nlearned together with the parameters of the neural network used for the\\nclassification task in an end-to-end fashion. Up to now, naïve global mean pooling or precomputed clustering methods were applied to compute graph embeddings from the features of each single node.\", '1806.08804v4', 'Related Work'), (28, '17', 'Notable instances of this model include Neural\\nFingerprints~\\\\cite{Duv+2015}, Gated Graph Neural\\nNetworks~\\\\cite{Li+2016}, and spectral approaches proposed\\nin~\\\\cite{Bru+2014,Def+2015,kipf2017semi}. Dai \\\\etal\\\\cite{dai2016discriminative}\\nproposed an approach inspired by mean-field inference\\nand Lei \\\\etal\\\\cite{Lei+2017} showed that the generated\\nfeature maps lie in the same Hilbert space as some popular graph\\nkernels and successfully applied them in the domain of\\nchemoinformatics~\\\\cite{Jin+2018}. In~\\\\cite{simonovsky2017dynamic} GNN were extended to include edge features and various precomputed pooling heuristics based on clustering methods were applied. Attention-based extensions were explored in~\\\\cite{Vel+2018}. In order to make GNNs scale to large graphs Hamilton \\\\etal\\\\cite{hamilton2017inductive} and Chen \\\\etal\\\\cite{Che+2018} devised stochastic versions of GNNs. Recently, a differentiable pooling mechanism to compute graph embeddings based on node features using differentiable sorting was proposed~\\\\cite{zhang2018end}.', '1806.08804v4', 'Related Work'), (29, '18', 'Moreover, GNNs were applied for protein-protein\\ninteraction prediction~\\\\cite{Fou+2017} and quantum interactions in\\nmolecules~\\\\cite{Sch+2017}. An approach for unsupervised learning based\\non GNNs was presented in~\\\\cite{Gar+2017}. Early\\napproaches were published in~\\\\cite{Mer+2005} and~\\\\cite{Sca+2009,\\nSca+2009a}. A survey can be found in~\\\\cite{Ham+2017a}.', '1806.08804v4', 'Related Work'), (30, '0', '\\\\label{sec:proposed}', '1806.08804v4', 'Proposed Method'), (31, '1', 'The key idea of \\\\name is that it enables the construction of deep, multi-layer GNN models by providing a differentiable module to hierarchically pool graph nodes. \\nIn this section, we outline the \\\\name module and show how it is applied in a deep GNN architecture.', '1806.08804v4', 'Proposed Method'), (32, '2', '\\\\subsection{Preliminaries}\\n\\\\label{sec:setting}', '1806.08804v4', 'Proposed Method'), (33, '3', 'We represent a graph $G$G as $(A,F)$(A,F), where $A\\\\in \\\\{0, 1\\\\}^{n\\\\times n}$A\\\\in \\\\{0, 1\\\\}^{n\\\\times n}n\\\\times n is the adjacency matrix, and $F \\\\in \\\\mathbb{R}^{n\\\\times d}$F \\\\in \\\\mathbb{R}^{n\\\\times d}n\\\\times d is the node feature matrix assuming each node has $d$d features.\\\\footnote{We do not consider edge features, although one can easily extend the algorithm to support edge features using techniques introduced in \\\\cite{simonovsky2017dynamic}.}\\nGiven a set of labeled graphs $\\\\mathcal{D}=\\\\{(G_1,y_1),(G_2,y_2),...\\\\}$\\\\mathcal{D}=\\\\{(G_1,y_1),(G_2,y_2),...\\\\} where $y_i \\\\in \\\\mathcal{Y}$y_i \\\\in \\\\mathcal{Y} is the label corresponding to graph $G_i \\\\in \\\\mathcal{G}$G_i \\\\in \\\\mathcal{G}, the goal of graph classification is to learn a mapping $f : \\\\mathcal{G} \\\\to \\\\mathcal{Y}$f : \\\\mathcal{G} \\\\to \\\\mathcal{Y} that maps graphs to the set of labels. \\nThe challenge---compared to standard supervised machine learning setup---is that we need a way to extract useful feature vectors from these input graphs.\\nThat is, in order to apply standard machine learning methods for classification, e.g., neural networks, we need a procedure to convert each graph to an finite dimensional vector in $\\\\mathbb{R}^D$\\\\mathbb{R}^D.', '1806.08804v4', 'Proposed Method'), (111, '1', 'Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation.', '1706.03762v7', 'Introduction'), (35, '5', \"where $H^{(k)} \\\\in \\\\mathbb{R}^{n \\\\times d}$H^{(k)}(k) \\\\in \\\\mathbb{R}^{n \\\\times d}n \\\\times d are the node embeddings (i.e., ``messages'') computed after $k$k steps of the GNN and $M$M is the message propagation function, which depends on the adjacency matrix, trainable parameters $\\\\theta^{(k)}$\\\\theta^{(k)}(k), and the node embeddings $H^{(k-1)}$H^{(k-1)}(k-1) generated from the previous message-passing step.\\\\footnote{For notational convenience, we assume that the embedding dimension is $d$ for all $H^{(k)}$; however, in general this restriction is not necessary.}\\nThe input node embeddings $H^{(0)}$H^{(0)}(0) at the initial message-passing iteration $(k=1)$(k=1), are initialized using\\nthe node features on the graph, $H^{(0)} = F$H^{(0)}(0) = F.\", '1806.08804v4', 'Proposed Method'), (36, '6', \"There are many possible implementations of the propagation function $M$M \\\\cite{Gil+2017,hamilton2017inductive}.\\nFor example, one popular variant of GNNs---Kipf's \\\\etal \\\\cite{kipf2017semi} Graph Convolutional Networks (GCNs)---implements $M$M using a combination of linear transformations and ReLU non-linearities:\\n\\\\begin{equation}\\n    \\\\label{eq:gcn}\\n    H^{(k)} =  M(A,H^{(k-1)};W^{(k)}) = \\\\textrm{ReLU}(\\\\tilde{D}^{-\\\\frac{1}{2}}\\\\tilde{A}\\\\tilde{D}^{-\\\\frac{1}{2}} H^{(k-1)} W^{(k-1)}),\\n\\\\end{equation}\\\\begin{equation}\\n    \\\\label{eq:gcn}\\n    H^{(k)} =  M(A,H^{(k-1)};W^{(k)}) = \\\\textrm{ReLU}(\\\\tilde{D}^{-\\\\frac{1}{2}}\\\\tilde{A}\\\\tilde{D}^{-\\\\frac{1}{2}} H^{(k-1)} W^{(k-1)}),\\n\\\\end{equation}\\n    \\\\label{eq:gcn}\\n    H^{(k)}(k) =  M(A,H^{(k-1)}(k-1);W^{(k)}(k)) = \\\\textrm{ReLU}(\\\\tilde{D}^{-\\\\frac{1}{2}}-\\\\frac{1}{2}\\\\tilde{A}\\\\tilde{D}^{-\\\\frac{1}{2}}-\\\\frac{1}{2} H^{(k-1)}(k-1) W^{(k-1)}(k-1)),\", '1806.08804v4', 'Proposed Method'), (37, '7', 'where $\\\\tilde{A}=A+I$\\\\tilde{A}=A+I, $\\\\tilde{D}=\\\\sum_j\\\\tilde{A}_{ij}$\\\\tilde{D}=\\\\sum_j\\\\tilde{A}_{ij}ij and $W^{(k)} \\\\in \\\\mathbb{R}^{d \\\\times d}$W^{(k)}(k) \\\\in \\\\mathbb{R}^{d \\\\times d}d \\\\times d is a trainable weight matrix.\\nThe differentiable pooling model we propose can be applied to any GNN model implementing Equation \\\\eqref{eq:gnn}, and is agnostic with regards to the specifics of how $M$M is implemented.', '1806.08804v4', 'Proposed Method'), (38, '8', 'A full GNN module will run $K$K iterations of Equation \\\\eqref{eq:gnn} to generate the final output node embeddings $Z=H^{(K)} \\\\in \\\\mathbb{R}^{n \\\\times d}$Z=H^{(K)}(K) \\\\in \\\\mathbb{R}^{n \\\\times d}n \\\\times d, where $K$K is usually in the range 2-6.\\nFor simplicity, in the following sections we will abstract away the internal structure of the GNNs and use $Z = \\\\gnn(A, X)$Z = \\\\gnn(A, X) to denote an arbitrary GNN module implementing $K$K iterations of message passing according to some adjacency matrix $A$A and initial input node features $X$X.', '1806.08804v4', 'Proposed Method'), (39, '9', \"\\\\xhdr{Stacking GNNs and pooling layers}Stacking GNNs and pooling layers\\nGNNs implementing Equation \\\\eqref{eq:gnn} are inherently flat, as they only propagate information across edges of a graph.\\nThe goal of this work is to define a general, end-to-end differentiable strategy that allows one to {\\\\em stack}\\\\em stack multiple GNN modules in a hierarchical fashion. \\nFormally, given $Z = \\\\gnn(A, X)$Z = \\\\gnn(A, X), the output of a GNN module, and a graph adjacency matrix $A \\\\in \\\\mathbb{R}^{n \\\\times n}$A \\\\in \\\\mathbb{R}^{n \\\\times n}n \\\\times n, we seek to define a strategy to output a new coarsened graph containing $m<n$m<n nodes, with weighted adjacency matrix $A^{'} \\\\in \\\\R^{m \\\\times m}$A^{'}' \\\\in \\\\R^{m \\\\times m}m \\\\times m and node embeddings $Z^{'} \\\\in \\\\mathbb{R}^{m\\\\times d}$Z^{'}' \\\\in \\\\mathbb{R}^{m\\\\times d}m\\\\times d.\\nThis new coarsened graph can then be used as input to another GNN layer, and this whole process can be repeated $L$L times, generating a model with $L$L GNN layers that operate on a series of coarser and coarser versions of the input graph (Figure~\\\\ref{fig:illustration}).\\nThus, our goal is to learn how to cluster or pool together nodes using the output of a GNN, so that we can use this coarsened graph as input to another GNN layer.\\nWhat makes designing such a pooling layer for GNNs especially challenging---compared to the usual graph coarsening task---is that our goal is not to simply cluster the nodes in one graph, but to provide a general recipe to hierarchically pool nodes across a broad set of input graphs. \\nThat is, we need our model to learn a pooling strategy that will generalize across graphs with different nodes, edges, and that can adapt to the various graph structures during inference.\", '1806.08804v4', 'Proposed Method'), (40, '10', '\\\\cut{\\n\\\\chris{I think the paragraph below repeats some of the points of the prev. paragraph}\\nThe key challenge for graph pooling with GNNs is designing a pooling layer that respects the hierarchical structure of the input graph.\\nSimilar to how CNNs on images stack filters with increasingly large receptive fields, the pooling layers in a GNN should be stacked hierarchically, extracting coarser and coarser subgraph structures to allow the GNN to obtain a more global view of the graph at the final layers. \\n%Firstly, the pooling layer should respect the hierarchical structure of graph.\\n%In the context of ConvNets for images, a deep neural network is only effective if the architecture allows a larger and larger receptive field.\\n%Similarly, it is desirable to have graph pooling layers to be stacked hierarchically, extracting larger subgraph structures and allow GNN to obtain a more global view at higher level.\\n%At the same time, the pooling layer should retain the structural information of the graph.\\n%Intuitively, each pooling results in a more coarsened graph representation, where another layer of graph convolution can be applied.\\n%There are several key ingredients for an effective pooling strategy. \\n%Thus, in terms of graph structure, the pooling layer should effectively partition the graph into modular components . \\n%As a result, a good pooling strategy should generally comply with the property of homophily on graphs,\\n%\\\\emph{i.e.} the idea that nodes that are close to each other in graph should be pooled.\\nMoreover, what makes designing a pooling layer for GNNs especially challenging---compared to the usual graph coarsening task---is that our goal is not to simply cluster the nodes in one graph, but to provide a general recipe to hierarchically pool nodes across a broad set of input graphs. \\nThat is, we need our model to encode a pooling strategy that will generalize across graphs with different nodes, edges, and that can adapt to the various graph structures during inference. \\nFinally, a key desideratum of a pooling module is that we want it to be able to {\\\\em learn} a good strategy from the training data, rather than relying on deterministic graph coarsening functions. \\nFor instance, one could simply use edge contractions or non-negative matrix factorization to assign nodes to clusters, but these approaches are incapable of adapting their pooling strategy based on training data. \\n}\\n\\\\chris{I think the paragraph below repeats some of the points of the prev. paragraph}I think the paragraph below repeats some of the points of the prev. paragraph\\nThe key challenge for graph pooling with GNNs is designing a pooling layer that respects the hierarchical structure of the input graph.\\nSimilar to how CNNs on images stack filters with increasingly large receptive fields, the pooling layers in a GNN should be stacked hierarchically, extracting coarser and coarser subgraph structures to allow the GNN to obtain a more global view of the graph at the final layers. \\nMoreover, what makes designing a pooling layer for GNNs especially challenging---compared to the usual graph coarsening task---is that our goal is not to simply cluster the nodes in one graph, but to provide a general recipe to hierarchically pool nodes across a broad set of input graphs. \\nThat is, we need our model to encode a pooling strategy that will generalize across graphs with different nodes, edges, and that can adapt to the various graph structures during inference. \\nFinally, a key desideratum of a pooling module is that we want it to be able to {\\\\em learn}\\\\em learn a good strategy from the training data, rather than relying on deterministic graph coarsening functions. \\nFor instance, one could simply use edge contractions or non-negative matrix factorization to assign nodes to clusters, but these approaches are incapable of adapting their pooling strategy based on training data.', '1806.08804v4', 'Proposed Method'), (41, '11', \"\\\\subsection{Differentiable Pooling via Learned Assignments}\\n\\\\cut{\\n\\\\jure{This is our main contribution. We need to discuss why our approach is sound and what is the intuition behind it.\\nIt would be good to talk about possible strategies to learn this (factorization, etc.) but why our approach of learning to assign nodes to clusters is appropriate one.\\nWe need a model that can for any network structure learn what is the right hierarchical pooling structure in order to aggregate information. \\nI think such discussion is important as it let's reader to think and appreciate the problem. If we simply introduce the solution too quickly people won't appreciate it.}\\n\\\\will{I tried to add more motivation in the paragraph above, and Rex and I are trying to add in more intuition behind the method in the paragraphs below.}\\n}\\n\\\\jure{This is our main contribution. We need to discuss why our approach is sound and what is the intuition behind it.\\nIt would be good to talk about possible strategies to learn this (factorization, etc.) but why our approach of learning to assign nodes to clusters is appropriate one.\\nWe need a model that can for any network structure learn what is the right hierarchical pooling structure in order to aggregate information. \\nI think such discussion is important as it let's reader to think and appreciate the problem. If we simply introduce the solution too quickly people won't appreciate it.}This is our main contribution. We need to discuss why our approach is sound and what is the intuition behind it.\\nIt would be good to talk about possible strategies to learn this (factorization, etc.) but why our approach of learning to assign nodes to clusters is appropriate one.\\nWe need a model that can for any network structure learn what is the right hierarchical pooling structure in order to aggregate information. \\nI think such discussion is important as it let's reader to think and appreciate the problem. If we simply introduce the solution too quickly people won't appreciate it.\\n\\\\will{I tried to add more motivation in the paragraph above, and Rex and I are trying to add in more intuition behind the method in the paragraphs below.}I tried to add more motivation in the paragraph above, and Rex and I are trying to add in more intuition behind the method in the paragraphs below.\", '1806.08804v4', 'Proposed Method'), (42, '12', 'Our proposed approach, \\\\name, addresses the above challenges by learning a cluster assignment matrix over the nodes using the output of a GNN model.\\nThe key intuition is that we stack $L$L GNN modules and learn to assign nodes to clusters at layer $l$l in an end-to-end fashion, using embeddings generated from a GNN at layer $l-1$l-1.\\nThus, we are using GNNs to both extract node embeddings that are useful for graph classification, as well to extract node embeddings that are useful for hierarchical pooling.\\nUsing this construction, the GNNs in \\\\name learn to encode a general pooling strategy that is useful for a large set of training graphs. \\nWe first describe how the \\\\name\\\\ module pools nodes at each layer given an assignment matrix; following this, we discuss how we generate the assignment matrix using a GNN architecture.', '1806.08804v4', 'Proposed Method'), (43, '13', '\\\\xhdr{Pooling with an assignment matrix}Pooling with an assignment matrix\\nWe denote the learned cluster assignment matrix at layer $l$l as $S^{(l)} \\\\in \\\\mathbb{R}^{n_l \\\\times n_{l+1}}$S^{(l)}(l) \\\\in \\\\mathbb{R}^{n_l \\\\times n_{l+1}}n_l \\\\times n_{l+1}l+1. \\nEach row of $S^{(l)}$S^{(l)}(l) corresponds to one of the $n_l$n_l nodes (or clusters) at layer $l$l, and each column of $S^{(l)}$S^{(l)}(l) corresponds to one of the $n_{l+1}$n_{l+1}l+1 clusters at the next layer $l+1$l+1. \\nIntuitively, $S^{(l)}$S^{(l)}(l) provides a soft assignment of each node at layer $l$l to a cluster in the next coarsened layer $l+1$l+1.', '1806.08804v4', 'Proposed Method'), (44, '14', 'Suppose that $S^{(l)}$S^{(l)}(l) has already been computed, i.e., that we have computed the assignment matrix at the $l$l-th layer of our model.\\nWe denote the input adjacency matrix at this layer as $A^{(l)}$A^{(l)}(l) and denote the input node embedding matrix at this layer as $Z^{(l)}$Z^{(l)}(l).\\nGiven these inputs, the \\\\name layer $(A^{(l+1)},X^{(l+1)}) = \\\\textsc{DiffPool}(A^{(l)},Z^{(l)})$(A^{(l+1)}(l+1),X^{(l+1)}(l+1)) = \\\\textsc{DiffPool}(A^{(l)}(l),Z^{(l)}(l)) coarsens the input graph, generating a new coarsened adjacency matrix $A^{(l+1)}$A^{(l+1)}(l+1) and a new matrix of embeddings $X^{(l+1)}$X^{(l+1)}(l+1) for each of the nodes/clusters in this coarsened graph.\\nIn particular, we apply the two following equations:\\n\\\\begin{align}\\n\\\\label{eq:4}\\n&X^{(l+1)} = {S^{(l)}}^T Z^{(l)}\\\\in \\\\mathbb{R}^{n_{l+1} \\\\times d},\\\\\\\\\\n\\\\label{eq:5}\\n&A^{(l+1)} = {S^{(l)}}^T A^{(l)}{S^{(l)}} \\\\in \\\\mathbb{R}^{n_{l+1} \\\\times n_{l+1}}.\\n\\\\end{align}\\\\begin{align}\\n\\\\label{eq:4}\\n&X^{(l+1)} = {S^{(l)}}^T Z^{(l)}\\\\in \\\\mathbb{R}^{n_{l+1} \\\\times d},\\\\\\\\\\n\\\\label{eq:5}\\n&A^{(l+1)} = {S^{(l)}}^T A^{(l)}{S^{(l)}} \\\\in \\\\mathbb{R}^{n_{l+1} \\\\times n_{l+1}}.\\n\\\\end{align}\\n\\\\label{eq:4}\\n&X^{(l+1)}(l+1) = {S^{(l)}}S^{(l)}(l)^T Z^{(l)}(l)\\\\in \\\\mathbb{R}^{n_{l+1} \\\\times d}n_{l+1}l+1 \\\\times d,\\\\\\\\\\n\\\\label{eq:5}\\n&A^{(l+1)}(l+1) = {S^{(l)}}S^{(l)}(l)^T A^{(l)}(l){S^{(l)}}S^{(l)}(l) \\\\in \\\\mathbb{R}^{n_{l+1} \\\\times n_{l+1}}n_{l+1}l+1 \\\\times n_{l+1}l+1.', '1806.08804v4', 'Proposed Method'), (45, '15', 'Equation \\\\eqref{eq:4} takes the node embeddings $Z^{(l)}$Z^{(l)}(l) and aggregates these embeddings according to the cluster assignments $S^{(l)}$S^{(l)}(l), generating embeddings for each of the $n_{l+1}$n_{l+1}l+1 clusters.\\nSimilarly, Equation \\\\eqref{eq:5} takes the adjacency matrix $A^{(l)}$A^{(l)}(l) and generates a coarsened adjacency matrix denoting the connectivity strength between each pair of clusters.', '1806.08804v4', 'Proposed Method'), (46, '16', 'Through Equations \\\\eqref{eq:4} and \\\\eqref{eq:5}, the \\\\name layer coarsens the graph: the next layer adjacency matrix $A^{(l+1)}$A^{(l+1)}(l+1) represents a coarsened graph with $n_{l+1}$n_{l+1}l+1 nodes or {\\\\em cluster nodes}\\\\em cluster nodes, where each individual cluster node in the new coarsened graph corresponds to a cluster of nodes in the graph at layer $l$l.\\nNote that $A^{(l+1)}$A^{(l+1)}(l+1) is a real matrix and represents a fully connected edge-weighted  graph; each entry $A^{(l+1)}_{ij}$A^{(l+1)}(l+1)_{ij}ij can be viewed as the connectivity strength between cluster $i$i and cluster $j$j. \\nSimilarly, the $i$i-th row of $X^{(l+1)}$X^{(l+1)}(l+1) corresponds to the embedding of cluster $i$i. \\nTogether, the coarsened adjacency matrix $A^{(l+1)}$A^{(l+1)}(l+1) and cluster embeddings $X^{(l+1)}$X^{(l+1)}(l+1) can be used as input to another GNN layer, a process which we describe in detail below.', '1806.08804v4', 'Proposed Method'), (47, '17', '\\\\xhdr{Learning the assignment matrix}Learning the assignment matrix\\nIn the following we describe the architecture of \\\\name, i.e., how \\\\name\\\\ generates the assignment matrix $S^{(l)}$S^{(l)}(l) and embedding matrices $Z^{(l)}$Z^{(l)}(l) that are used in Equations \\\\eqref{eq:4} and \\\\eqref{eq:5}.\\nWe generate these two matrices using two separate GNNs that are both applied to the input cluster node features $X^{(l)}$X^{(l)}(l) and coarsened adjacency matrix $A^{(l)}$A^{(l)}(l).\\nThe {\\\\em embedding GNN}\\\\em embedding GNN at layer $l$l is a standard GNN module applied to these inputs:\\n\\\\begin{align}\\\\label{eq:embedgnn}\\n   Z^{(l)} = \\\\gnn_{l, \\\\textrm{embed}}(A^{(l)}, X^{(l)}),\\n\\\\end{align}\\\\begin{align}\\\\label{eq:embedgnn}\\n   Z^{(l)} = \\\\gnn_{l, \\\\textrm{embed}}(A^{(l)}, X^{(l)}),\\n\\\\end{align}\\\\label{eq:embedgnn}\\n   Z^{(l)}(l) = \\\\gnn_{l, \\\\textrm{embed}}l, \\\\textrm{embed}(A^{(l)}(l), X^{(l)}(l)),', '1806.08804v4', 'Proposed Method'), (48, '18', 'i.e., we take the adjacency matrix between the cluster nodes at layer $l$l (from Equation \\\\ref{eq:5}) and the pooled features for the clusters (from Equation \\\\ref{eq:4}) and pass these matrices through a standard GNN to get new embeddings $Z^{(l)}$Z^{(l)}(l) for the cluster nodes. \\nIn contrast, the {\\\\em pooling GNN}\\\\em pooling GNN at layer $l$l, uses the input cluster features $X^{(l)}$X^{(l)}(l) and cluster adjacency matrix $A^{(l)}$A^{(l)}(l) to generate an assignment matrix:\\n\\\\begin{align}\\\\label{eq:poolgnn}\\n    S^{(l)} = \\\\textrm{softmax}\\\\left(\\\\gnn_{l,\\\\text{pool}}(A^{(l)}, X^{(l)})\\\\right),\\n\\\\end{align}\\\\begin{align}\\\\label{eq:poolgnn}\\n    S^{(l)} = \\\\textrm{softmax}\\\\left(\\\\gnn_{l,\\\\text{pool}}(A^{(l)}, X^{(l)})\\\\right),\\n\\\\end{align}\\\\label{eq:poolgnn}\\n    S^{(l)}(l) = \\\\textrm{softmax}\\\\left(\\\\gnn_{l,\\\\text{pool}}l,\\\\text{pool}(A^{(l)}(l), X^{(l)}(l))\\\\right),', '1806.08804v4', 'Proposed Method'), (49, '19', 'where the softmax function is applied in a row-wise fashion.\\nThe output dimension of $\\\\gnn_{l,\\\\text{pool}}$\\\\gnn_{l,\\\\text{pool}}l,\\\\text{pool} corresponds to a pre-defined maximum number of clusters in layer $l$l, and is a hyperparameter of the model.', '1806.08804v4', 'Proposed Method'), (50, '20', 'Note that these two GNNs consume the same input data but have distinct parameterizations and play separate roles:\\nThe embedding GNN generates new embeddings for the input nodes at this layer, while the pooling GNN generates a probabilistic assignment of the input nodes to $n_{l+1}$n_{l+1}l+1 clusters.', '1806.08804v4', 'Proposed Method'), (51, '21', \"In the base case, the inputs to Equations \\\\eqref{eq:embedgnn} and Equations \\\\eqref{eq:poolgnn} at layer $l=0$l=0 are simply the input adjacency matrix $A$A and the node features $F$F for the original graph.\\nAt the penultimate layer $L-1$L-1 of a deep GNN model using \\\\name, we set the assignment matrix $S^{(L-1)}$S^{(L-1)}(L-1) be a vector of $1$1's, i.e., all nodes at the final layer $L$L are assigned to a single cluster, generating a final embedding vector corresponding to the entire graph.\\nThis final output embedding can then be used as feature input to a differentiable classifier (e.g., a softmax layer), and the entire system can be trained end-to-end using stochastic gradient descent.\", '1806.08804v4', 'Proposed Method'), (69, '39', '\\\\xhdr{Representation dimension}\\nAt deeper layers, the embedding matrix $Z$ provides representations for hyper-nodes corresponding to larger subgraphs. Therefore, more dimensions should be used when encoding larger subgraphs. This is analogous to CNNs for images~\\\\cite{?}, where the number of channels increases after each convolutional layer.  In contrast, the assignment matrix $S$ aims to map hyper-nodes into a fewer number of clusters, allowing hyper-nodes to gain a more global \\\\rex{what is better wording?} \\\\chris{higher-order?} view of connectivities between subgraphs. Therefore, at deeper layers, the output dimension of $g_\\\\phi$ decreases exponentially. Here the dimension reduction ratio $\\\\alpha$ is a hyper-parameter. In practice, we discover that performance is insensitive for $0.1 < \\\\alpha < 0.5$. The network outputs a sparse $S$ if the number of intuitive clusters are much less than the output dimension.', '1806.08804v4', 'Proposed Method'), (52, '22', '\\\\xhdr{Permutation invariance}Permutation invariance\\nNote that in order to be useful for graph classification, the pooling layer should be invariant under node permutations. For \\\\name we get the following positive result, which shows that any deep GNN model based on \\\\name\\\\ is permutation invariant, as long as the component GNNs are permutation invariant. \\n\\\\begin{proposition}\\n\\\\label{prop:permute}\\nLet $P\\\\in \\\\{0,1\\\\}^{n\\\\times n}$ be any permutation matrix, then $\\\\text{\\\\sc \\\\name}(A, Z) = \\\\text{\\\\sc \\\\name}(PAP^T,PX)$ as long as $\\\\gnn(A, X) = \\\\gnn(PAP^T, X)$ (i.e., as long as the GNN method used is permutation invariant).\\n\\\\end{proposition}\\\\begin{proposition}\\n\\\\label{prop:permute}\\nLet $P\\\\in \\\\{0,1\\\\}^{n\\\\times n}$ be any permutation matrix, then $\\\\text{\\\\sc \\\\name}(A, Z) = \\\\text{\\\\sc \\\\name}(PAP^T,PX)$ as long as $\\\\gnn(A, X) = \\\\gnn(PAP^T, X)$ (i.e., as long as the GNN method used is permutation invariant).\\n\\\\end{proposition}\\n\\\\label{prop:permute}\\nLet $P\\\\in \\\\{0,1\\\\}^{n\\\\times n}$P\\\\in \\\\{0,1\\\\}^{n\\\\times n}n\\\\times n be any permutation matrix, then $\\\\text{\\\\sc \\\\name}(A, Z) = \\\\text{\\\\sc \\\\name}(PAP^T,PX)$\\\\text{\\\\sc \\\\name}(A, Z) = \\\\text{\\\\sc \\\\name}(PAP^T,PX) as long as $\\\\gnn(A, X) = \\\\gnn(PAP^T, X)$\\\\gnn(A, X) = \\\\gnn(PAP^T, X) (i.e., as long as the GNN method used is permutation invariant).', '1806.08804v4', 'Proposed Method'), (53, '23', '\\\\begin{proof}\\nEquations \\\\eqref{eq:embedgnn} and \\\\eqref{eq:poolgnn} are permutation invariant by the assumption that the GNN module is permutation invariant. \\nAnd since any permutation matrix is orthogonal, applying $P^T P=I$ to Equation (\\\\ref{eq:4}) and (\\\\ref{eq:5}) finishes the proof.\\n\\\\end{proof}\\\\begin{proof}\\nEquations \\\\eqref{eq:embedgnn} and \\\\eqref{eq:poolgnn} are permutation invariant by the assumption that the GNN module is permutation invariant. \\nAnd since any permutation matrix is orthogonal, applying $P^T P=I$ to Equation (\\\\ref{eq:4}) and (\\\\ref{eq:5}) finishes the proof.\\n\\\\end{proof}\\nEquations \\\\eqref{eq:embedgnn} and \\\\eqref{eq:poolgnn} are permutation invariant by the assumption that the GNN module is permutation invariant. \\nAnd since any permutation matrix is orthogonal, applying $P^T P=I$P^T P=I to Equation (\\\\ref{eq:4}) and (\\\\ref{eq:5}) finishes the proof.', '1806.08804v4', 'Proposed Method'), (54, '24', '\\\\subsection{Auxiliary Link Prediction Objective and Entropy Regularization}', '1806.08804v4', 'Proposed Method'), (55, '25', 'In practice, it can be difficult to train the pooling GNN (Equation \\\\ref{eq:5}) using only gradient signal from the graph classification task.\\nIntuitively, we have a non-convex optimization problem and it can be difficult to push the pooling GNN away from spurious local minima early in training.\\nTo alleviate this issue, we train the pooling GNN with an auxiliary link prediction objective, which encodes the intuition that nearby nodes should be pooled together. \\n\\\\cut{\\nControlling input features and representation dimension is still insufficient for $g_\\\\phi$ to learn and extract meaningful cluster information from a graph. We additionally supply side objectives as regularizations for $g_\\\\phi$. \\nIntuitively, $g_\\\\phi$ should learn to assign nodes that are close together in terms of connectivity into the same clusters. \\nHence, we use a link prediction objective to further encourage similarity of cluster assignments.}\\nControlling input features and representation dimension is still insufficient for $g_\\\\phi$g_\\\\phi to learn and extract meaningful cluster information from a graph. We additionally supply side objectives as regularizations for $g_\\\\phi$g_\\\\phi. \\nIntuitively, $g_\\\\phi$g_\\\\phi should learn to assign nodes that are close together in terms of connectivity into the same clusters. \\nHence, we use a link prediction objective to further encourage similarity of cluster assignments.\\nIn particular, at each layer $l$l, we minimize\\n$L_{\\\\text{LP}} = ||A^{(l)}, S^{(l)} S^{{(l)}^T}||_F$L_{\\\\text{LP}}\\\\text{LP} = ||A^{(l)}(l), S^{(l)}(l) S^{{(l)}^T}{(l)}(l)^T||_F, where $||\\\\cdot||_F$||\\\\cdot||_F denotes the Frobenius norm. \\nNote that the adjacency matrix $A^{(l)}$A^{(l)}(l) at deeper layers is a function of lower level assignment matrices, and changes during training.', '1806.08804v4', 'Proposed Method'), (56, '26', 'Another important characteristic of the pooling GNN (Equation \\\\ref{eq:5})  is that the output cluster assignment for each node should generally be close to a one-hot vector, so that the membership for each cluster or subgraph is clearly defined. We therefore regularize the entropy of the cluster assignment by minimizing $L_{\\\\text{E}} = \\\\frac{1}{n} \\\\sum_{i=1}^n H(S_i)$L_{\\\\text{E}}\\\\text{E} = \\\\frac{1}{n} \\\\sum_{i=1}i=1^n H(S_i), where $H$H denotes the entropy function, and $S_i$S_i is the $i$i-th row of $S$S.', '1806.08804v4', 'Proposed Method'), (57, '27', 'During training, $L_{\\\\text{LP}}$L_{\\\\text{LP}}\\\\text{LP} and $L_{\\\\text{E}}$L_{\\\\text{E}}\\\\text{E} from each layer are added to the classification loss.\\nIn practice we observe that training with the side objective takes longer to converge, but nevertheless achieves better performance and more interpretable cluster assignments.', '1806.08804v4', 'Proposed Method'), (58, '28', '\\\\cut{\\n\\\\subsection{Behavior on sparse and dense graphs}\\n\\\\label{sec:sparse_dense}\\nThe sparsity of (sub)graphs has a large impact on the behavior of the assignment layer  $M(A^{(l)},H^{(l)};\\\\phi^{(l)})$.\\nIn particular, we find that \\\\name\\\\ is most effective in sparse graphs that exhibit hierarchical partitions, whereas in very dense (sub)graphs \\\\name\\\\ simply learns to map all nodes to a single cluster.\\nMoreover, within a particular layer of a deep GNN model, \\\\name\\\\ will tend to collapse densely-connected connected subgraphs into a single hyper-node. \\nThis trend is supported by our empirical studies (Section \\\\ref{sec:ex})---e.g., where \\\\name\\\\ leads to state-of-the-art results on all data sets except \\\\textsc{Collab}, which involves exceptionally dense graphs---and this trend can be understood based on the following theoretical intuitions.', '1806.08804v4', 'Proposed Method'), (59, '29', 'First, note that if a subgraph of the input graph is dense, then the entries of the corresponding subgraph adjacency matrix will be mostly ones. \\nAnd since  $\\\\mathbf{1} \\\\mathbf{1}^T$ is a matrix of all ones, an assignment matrix for the subgraph, where one column is a vector of ones and the others are zero vectors, will be close to a minimum of the link prediction auxiliary objective.\\nThus, the objective will tend to group all nodes of a dense subgraph into one cluster.', '1806.08804v4', 'Proposed Method'), (60, '30', '\\\\rex{formalizing: for a graph with n cliques, the network with linkpred objective will almost deterministically assign all nodes in each clique to a distinct cluster}', '1806.08804v4', 'Proposed Method'), (70, '40', '\\\\xhdr{Auxiliary objectives}\\nControlling input features and representation dimension is still insufficient for $g_\\\\phi$ to learn and extract meaningful cluster information from a graph. We additionally supply side objectives as regularizations for $g_\\\\phi$. \\nIntuitively, $g_\\\\phi$ should learn to assign nodes that are close together in terms of connectivity into the same clusters. \\nHence, we use a link prediction objective to further encourage similarity of cluster assignments.\\nParticularly, at each layer $l$, we minimize\\n$L_{\\\\text{LP}} ||A^{(l)}, S^{(l)} S^{{(l)}^T}||_F$, where $||\\\\cdot||_F$ denotes the Frobinus norm. \\nNote that the (soft) adjacency matrix $A^{(l)}$ at deeper layers is a function of lower level assignment matrices, and changes during training.', '1806.08804v4', 'Proposed Method'), (112, '2', 'Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures. Recurrent models typically factor computation along the symbol positions of the input and output sequences.', '1706.03762v7', 'Introduction'), (61, '31', 'Moreover, in terms of GNN computation, collapsing dense subgraphs in this way is intuitively an optimal pooling (or partitioning) strategy.\\nIn particular, it is known that GNNs can efficiently perform message-passing on dense, clique-like subgraphs (due to their small diameter) \\\\cite{liao2018graph}, and hence \\\\name\\\\ can pool together nodes in such a dense subgraph without losing any significant structural information.\\nIn contrast, sparse subgraphs may contain many interesting structures, including path-, cycle- and tree-like structures, and given the high-diameter induced by sparsity, GNN message-passing may fail to capture these structures. \\nThus, by separately pooling distinct parts of a sparse subgraph, \\\\name can learn to capture the meaningful structures present \\nin sparse graph regions. \\n}\\n\\\\subsection{Behavior on sparse and dense graphs}\\n\\\\label{sec:sparse_dense}\\nThe sparsity of (sub)graphs has a large impact on the behavior of the assignment layer  $M(A^{(l)},H^{(l)};\\\\phi^{(l)})$M(A^{(l)}(l),H^{(l)}(l);\\\\phi^{(l)}(l)).\\nIn particular, we find that \\\\name\\\\ is most effective in sparse graphs that exhibit hierarchical partitions, whereas in very dense (sub)graphs \\\\name\\\\ simply learns to map all nodes to a single cluster.\\nMoreover, within a particular layer of a deep GNN model, \\\\name\\\\ will tend to collapse densely-connected connected subgraphs into a single hyper-node. \\nThis trend is supported by our empirical studies (Section \\\\ref{sec:ex})---e.g., where \\\\name\\\\ leads to state-of-the-art results on all data sets except \\\\textsc{Collab}, which involves exceptionally dense graphs---and this trend can be understood based on the following theoretical intuitions.', '1806.08804v4', 'Proposed Method'), (62, '32', 'First, note that if a subgraph of the input graph is dense, then the entries of the corresponding subgraph adjacency matrix will be mostly ones. \\nAnd since  $\\\\mathbf{1} \\\\mathbf{1}^T$\\\\mathbf{1} \\\\mathbf{1}^T is a matrix of all ones, an assignment matrix for the subgraph, where one column is a vector of ones and the others are zero vectors, will be close to a minimum of the link prediction auxiliary objective.\\nThus, the objective will tend to group all nodes of a dense subgraph into one cluster.', '1806.08804v4', 'Proposed Method'), (63, '33', '\\\\rex{formalizing: for a graph with n cliques, the network with linkpred objective will almost deterministically assign all nodes in each clique to a distinct cluster}formalizing: for a graph with n cliques, the network with linkpred objective will almost deterministically assign all nodes in each clique to a distinct cluster', '1806.08804v4', 'Proposed Method'), (64, '34', 'Moreover, in terms of GNN computation, collapsing dense subgraphs in this way is intuitively an optimal pooling (or partitioning) strategy.\\nIn particular, it is known that GNNs can efficiently perform message-passing on dense, clique-like subgraphs (due to their small diameter) \\\\cite{liao2018graph}, and hence \\\\name\\\\ can pool together nodes in such a dense subgraph without losing any significant structural information.\\nIn contrast, sparse subgraphs may contain many interesting structures, including path-, cycle- and tree-like structures, and given the high-diameter induced by sparsity, GNN message-passing may fail to capture these structures. \\nThus, by separately pooling distinct parts of a sparse subgraph, \\\\name can learn to capture the meaningful structures present \\nin sparse graph regions.', '1806.08804v4', 'Proposed Method'), (65, '35', '\\\\cut{\\nThis interpretation matches our pooling objective well. Intuitively, a clique-like subgraph contains very little interesting structure, and message passing is efficient due to small graph diameter, and hence it could be directly pooled together without losing much structural information in the pooled embedding matrix $Z$. However, a sparse subgraph may contain many interesting structures, including path-, cycle- and tree-like ones. Therefore, message passing may miss these fine-grained structures. Subsequently, each part of the subgraph should be assigned and pooled differently, to capture these meaningful structures. }\\nThis interpretation matches our pooling objective well. Intuitively, a clique-like subgraph contains very little interesting structure, and message passing is efficient due to small graph diameter, and hence it could be directly pooled together without losing much structural information in the pooled embedding matrix $Z$Z. However, a sparse subgraph may contain many interesting structures, including path-, cycle- and tree-like ones. Therefore, message passing may miss these fine-grained structures. Subsequently, each part of the subgraph should be assigned and pooled differently, to capture these meaningful structures.', '1806.08804v4', 'Proposed Method'), (66, '36', '\\\\cut{\\nHence the objective will tend to group all nodes into one cluster. \\\\chris{We could also formalize it and prove it.}This  implies that all nodes in the dense subgraph are clustered into a single hyper-node. In contrast, when a subgraph is sparse, the resulting assignment tends to have higher entropy, and nodes could be assigned to different clusters, so that the elements of $S S^T$ are small.}\\nHence the objective will tend to group all nodes into one cluster. \\\\chris{We could also formalize it and prove it.}We could also formalize it and prove it.This  implies that all nodes in the dense subgraph are clustered into a single hyper-node. In contrast, when a subgraph is sparse, the resulting assignment tends to have higher entropy, and nodes could be assigned to different clusters, so that the elements of $S S^T$S S^T are small.\\n\\\\cut{\\n\\\\subsection{Learning to Pool with Side Information}\\n\\\\label{sec:pooling}', '1806.08804v4', 'Proposed Method'), (67, '37', 'Although the assignment matrix $S$ and the embedding matrix $Z$ are both computed using the GCN architecture, they have distinct interpretations. In particular, the embedding matrix at each layer is used as an intermediate representations of nodes and subgraphs at different coarsening scales. In comparison, the assignment matrix at each layer is used to determine the clustering assignment, and determines which nodes and subgraphs should be pooled together. Therefore it is important to create asymmetry in the computation of both $S$ and $Z$, in order to differentiate their purposes. We achieve this in three ways.', '1806.08804v4', 'Proposed Method'), (68, '38', '\\\\xhdr{Input Features}\\n\\\\note{note to rex himself: more experim - identity feat input?}\\nThe goal of $Z$ is to learn node and subgraph representations such that when pooled together, subsequent classifier can easily map the representations to labels. \\nSince the labels might be a complex function of all node features, e.g., homophily and structural properties of the input graph, we use a variety of features as inputs to $f_\\\\theta$ to compute $Z$, including structural features such as degree and clustering coefficient features, or node features.\\nIn comparison, the assignment network should learn to predict cluster indices mainly based on homophily property. Therefore structural features such as degree and clustering coefficients are removed from the input features, which is essential in obtaining meaningful clusters that also benefit the classification task.', '1806.08804v4', 'Proposed Method'), (71, '41', 'Another important characteristic of $g_\\\\phi$ is that the output cluster assignment for each node should generally be close to a one-hot vector, so that the membership for each cluster or subgraph is clearly defined, except in rare cases where a node serves as a bridge between multiple clusters. We therefore regularize the entropy of the cluster assignment by minimizing $L_{\\\\text{E}} = \\\\frac{1}{n} \\\\sum_{i=1}^n H(S_i)$, where $H$ denotes the entropy function, and $S_i$ is the $i$-th row of $S$.', '1806.08804v4', 'Proposed Method'), (72, '42', 'During training, $L_{\\\\text{LP}}$ and $L_{\\\\text{E}}$ from each layer are added to the classification loss, in order to obtain meaningful assignment matrices at all layers. In practice we observe that training with the side objective takes longer to converge, but nevertheless achieves better performance and more interpretable cluster assignment.', '1806.08804v4', 'Proposed Method'), (73, '43', '\\\\note{Note to Rex himself: try curriculum training}\\n\\\\chris{Where is $g_\\\\phi$ and $f_\\\\theta$ defined}', '1806.08804v4', 'Proposed Method'), (74, '44', '\\\\xhdr{Relation to low rank matrix factorization}\\nWe note that by approximating $A$ with $S S^T$, the link prediction auxiliary objective bears close resemblance to a low rank matrix factorization of $A$, and well-separated pair decomposition (WSPD). \\nSimilar to matrix factorization, we require that at each layer, $S$ is able to capture most of the distance information between nodes, while using less dimensions than that of $A$. However, low rank matrix factorization is non-convex and has many local minimums. When trained end-to-end in the graph classification task, \\\\name tends to find local minimum that is better suited for the task. This explains our observation that DiffPool consistently out-performs a two-step procedure of graph clustering followed by GCN that pools over these clusters.\\n$\\\\mathrm{WSPD}$ computes small number of pairs of clusters, such that for any pair of points $(p, q)$, we can find a pair of clusters $(A, B)$ such that $p\\\\in A, q\\\\in B$, and $d(p, q) \\\\approx d(A, B)$. In the case of \\\\name, the goal of the auxiliary objective is to let the connectivity strength between any node pair $(p, q)$ to be reflected by the connectivity strength between their corresponding clusters. However, unlike WSPD, the assignment in \\\\name is soft to allow differentiability, which enables end-to-end training and avoids the high computation cost of WSPD in high dimensions.', '1806.08804v4', 'Proposed Method'), (75, '45', '\\\\xhdr{Behavior on sparse and dense networks}\\nThe sparsity of graphs also affects the behavior of the assignment network $g_\\\\phi$.\\nSuppose a subgraph of the input graph is dense, therefore the entries of the corresponding subgraph adjacency matrix are mostly ones. Since  $\\\\mathbf{1} \\\\mathbf{1}^T$ is a matrix of all ones, an assignment matrix for the subgraph, where one column is a vector of ones and the others are zero vectors, will be close to a minimum of the link prediction auxiliary objective. Hence the objective will tend to group all nodes into one cluster. \\\\chris{We could also formalize it and prove it.}This  implies that all nodes in the dense subgraph are clustered into a single hyper-node. In contrast, when a subgraph is sparse, the resulting assignment tends to have higher entropy, and nodes could be assigned to different clusters, so that the elements of $S S^T$ are small.', '1806.08804v4', 'Proposed Method'), (76, '46', 'This interpretation matches our pooling objective well. Intuitively, a clique-like subgraph contains very little interesting structure, and message passing is efficient due to small graph diameter, and hence it could be directly pooled together without losing much structural information in the pooled embedding matrix. However, a sparse subgraph may contain many interesting structures, including path-, cycle- and tree-like ones. Therefore, message passing may miss these fine-grained structures. Subsequently, each part of the subgraph should be assigned and pooled differently, to capture these meaningful structures.', '1806.08804v4', 'Proposed Method'), (77, '47', '}\\n\\\\subsection{Learning to Pool with Side Information}\\n\\\\label{sec:pooling}', '1806.08804v4', 'Proposed Method'), (78, '48', 'Although the assignment matrix $S$S and the embedding matrix $Z$Z are both computed using the GCN architecture, they have distinct interpretations. In particular, the embedding matrix at each layer is used as an intermediate representations of nodes and subgraphs at different coarsening scales. In comparison, the assignment matrix at each layer is used to determine the clustering assignment, and determines which nodes and subgraphs should be pooled together. Therefore it is important to create asymmetry in the computation of both $S$S and $Z$Z, in order to differentiate their purposes. We achieve this in three ways.', '1806.08804v4', 'Proposed Method'), (79, '49', '\\\\xhdr{Input Features}Input Features\\n\\\\note{note to rex himself: more experim - identity feat input?}note to rex himself: more experim - identity feat input?\\nThe goal of $Z$Z is to learn node and subgraph representations such that when pooled together, subsequent classifier can easily map the representations to labels. \\nSince the labels might be a complex function of all node features, e.g., homophily and structural properties of the input graph, we use a variety of features as inputs to $f_\\\\theta$f_\\\\theta to compute $Z$Z, including structural features such as degree and clustering coefficient features, or node features.\\nIn comparison, the assignment network should learn to predict cluster indices mainly based on homophily property. Therefore structural features such as degree and clustering coefficients are removed from the input features, which is essential in obtaining meaningful clusters that also benefit the classification task.', '1806.08804v4', 'Proposed Method'), (80, '50', '\\\\xhdr{Representation dimension}Representation dimension\\nAt deeper layers, the embedding matrix $Z$Z provides representations for hyper-nodes corresponding to larger subgraphs. Therefore, more dimensions should be used when encoding larger subgraphs. This is analogous to CNNs for images~\\\\cite{?}, where the number of channels increases after each convolutional layer.  In contrast, the assignment matrix $S$S aims to map hyper-nodes into a fewer number of clusters, allowing hyper-nodes to gain a more global \\\\rex{what is better wording?}what is better wording? \\\\chris{higher-order?}higher-order? view of connectivities between subgraphs. Therefore, at deeper layers, the output dimension of $g_\\\\phi$g_\\\\phi decreases exponentially. Here the dimension reduction ratio $\\\\alpha$\\\\alpha is a hyper-parameter. In practice, we discover that performance is insensitive for $0.1 < \\\\alpha < 0.5$0.1 < \\\\alpha < 0.5. The network outputs a sparse $S$S if the number of intuitive clusters are much less than the output dimension.', '1806.08804v4', 'Proposed Method'), (81, '51', '\\\\xhdr{Auxiliary objectives}Auxiliary objectives\\nControlling input features and representation dimension is still insufficient for $g_\\\\phi$g_\\\\phi to learn and extract meaningful cluster information from a graph. We additionally supply side objectives as regularizations for $g_\\\\phi$g_\\\\phi. \\nIntuitively, $g_\\\\phi$g_\\\\phi should learn to assign nodes that are close together in terms of connectivity into the same clusters. \\nHence, we use a link prediction objective to further encourage similarity of cluster assignments.\\nParticularly, at each layer $l$l, we minimize\\n$L_{\\\\text{LP}} ||A^{(l)}, S^{(l)} S^{{(l)}^T}||_F$L_{\\\\text{LP}}\\\\text{LP} ||A^{(l)}(l), S^{(l)}(l) S^{{(l)}^T}{(l)}(l)^T||_F, where $||\\\\cdot||_F$||\\\\cdot||_F denotes the Frobinus norm. \\nNote that the (soft) adjacency matrix $A^{(l)}$A^{(l)}(l) at deeper layers is a function of lower level assignment matrices, and changes during training.', '1806.08804v4', 'Proposed Method'), (113, '3', 'Aligning the positions to steps in computation time, they generate a sequence of hidden states h_t, as a function of the previous hidden state h_{t-1} and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples.', '1706.03762v7', 'Introduction'), (82, '52', 'Another important characteristic of $g_\\\\phi$g_\\\\phi is that the output cluster assignment for each node should generally be close to a one-hot vector, so that the membership for each cluster or subgraph is clearly defined, except in rare cases where a node serves as a bridge between multiple clusters. We therefore regularize the entropy of the cluster assignment by minimizing $L_{\\\\text{E}} = \\\\frac{1}{n} \\\\sum_{i=1}^n H(S_i)$L_{\\\\text{E}}\\\\text{E} = \\\\frac{1}{n} \\\\sum_{i=1}i=1^n H(S_i), where $H$H denotes the entropy function, and $S_i$S_i is the $i$i-th row of $S$S.', '1806.08804v4', 'Proposed Method'), (83, '53', 'During training, $L_{\\\\text{LP}}$L_{\\\\text{LP}}\\\\text{LP} and $L_{\\\\text{E}}$L_{\\\\text{E}}\\\\text{E} from each layer are added to the classification loss, in order to obtain meaningful assignment matrices at all layers. In practice we observe that training with the side objective takes longer to converge, but nevertheless achieves better performance and more interpretable cluster assignment.', '1806.08804v4', 'Proposed Method'), (84, '54', '\\\\note{Note to Rex himself: try curriculum training}Note to Rex himself: try curriculum training\\n\\\\chris{Where is $g_\\\\phi$ and $f_\\\\theta$ defined}Where is $g_\\\\phi$g_\\\\phi and $f_\\\\theta$f_\\\\theta defined', '1806.08804v4', 'Proposed Method'), (85, '55', '\\\\xhdr{Relation to low rank matrix factorization}Relation to low rank matrix factorization\\nWe note that by approximating $A$A with $S S^T$S S^T, the link prediction auxiliary objective bears close resemblance to a low rank matrix factorization of $A$A, and well-separated pair decomposition (WSPD). \\nSimilar to matrix factorization, we require that at each layer, $S$S is able to capture most of the distance information between nodes, while using less dimensions than that of $A$A. However, low rank matrix factorization is non-convex and has many local minimums. When trained end-to-end in the graph classification task, \\\\name tends to find local minimum that is better suited for the task. This explains our observation that DiffPool consistently out-performs a two-step procedure of graph clustering followed by GCN that pools over these clusters.\\n$\\\\mathrm{WSPD}$\\\\mathrm{WSPD} computes small number of pairs of clusters, such that for any pair of points $(p, q)$(p, q), we can find a pair of clusters $(A, B)$(A, B) such that $p\\\\in A, q\\\\in B$p\\\\in A, q\\\\in B, and $d(p, q) \\\\approx d(A, B)$d(p, q) \\\\approx d(A, B). In the case of \\\\name, the goal of the auxiliary objective is to let the connectivity strength between any node pair $(p, q)$(p, q) to be reflected by the connectivity strength between their corresponding clusters. However, unlike WSPD, the assignment in \\\\name is soft to allow differentiability, which enables end-to-end training and avoids the high computation cost of WSPD in high dimensions.', '1806.08804v4', 'Proposed Method'), (86, '56', '\\\\xhdr{Behavior on sparse and dense networks}Behavior on sparse and dense networks\\nThe sparsity of graphs also affects the behavior of the assignment network $g_\\\\phi$g_\\\\phi.\\nSuppose a subgraph of the input graph is dense, therefore the entries of the corresponding subgraph adjacency matrix are mostly ones. Since  $\\\\mathbf{1} \\\\mathbf{1}^T$\\\\mathbf{1} \\\\mathbf{1}^T is a matrix of all ones, an assignment matrix for the subgraph, where one column is a vector of ones and the others are zero vectors, will be close to a minimum of the link prediction auxiliary objective. Hence the objective will tend to group all nodes into one cluster. \\\\chris{We could also formalize it and prove it.}We could also formalize it and prove it.This  implies that all nodes in the dense subgraph are clustered into a single hyper-node. In contrast, when a subgraph is sparse, the resulting assignment tends to have higher entropy, and nodes could be assigned to different clusters, so that the elements of $S S^T$S S^T are small.', '1806.08804v4', 'Proposed Method'), (87, '57', 'This interpretation matches our pooling objective well. Intuitively, a clique-like subgraph contains very little interesting structure, and message passing is efficient due to small graph diameter, and hence it could be directly pooled together without losing much structural information in the pooled embedding matrix. However, a sparse subgraph may contain many interesting structures, including path-, cycle- and tree-like ones. Therefore, message passing may miss these fine-grained structures. Subsequently, each part of the subgraph should be assigned and pooled differently, to capture these meaningful structures.', '1806.08804v4', 'Proposed Method'), (88, '0', '\\\\label{sec:ex}', '1806.08804v4', 'Experiments'), (89, '1', 'We evaluate the benefits of \\\\name\\\\ against a number of state-of-the-art graph classification approaches, with the goal of answering the following questions:\\n\\\\begin{enumerate}[leftmargin=20pt]\\n\\\\item[{\\\\bf Q1}] How does  \\\\name\\\\ compare to other pooling methods proposed for GNNs (e.g., using sort pooling~\\\\cite{zhang2018end} or the \\\\textsc{Set2Set} method \\\\cite{Gil+2017})?\\n\\\\item[{\\\\bf Q2}] How does  \\\\name\\\\ combined with GNNs compare to the state-of-the-art for graph classification task, including both GNNs and kernel-based methods?\\n\\\\item[{\\\\bf Q3}] Does  \\\\name  compute meaningful and interpretable clusters on the input graphs?\\n\\\\end{enumerate}\\\\begin{enumerate}[leftmargin=20pt]\\n\\\\item[{\\\\bf Q1}] How does  \\\\name\\\\ compare to other pooling methods proposed for GNNs (e.g., using sort pooling~\\\\cite{zhang2018end} or the \\\\textsc{Set2Set} method \\\\cite{Gil+2017})?\\n\\\\item[{\\\\bf Q2}] How does  \\\\name\\\\ combined with GNNs compare to the state-of-the-art for graph classification task, including both GNNs and kernel-based methods?\\n\\\\item[{\\\\bf Q3}] Does  \\\\name  compute meaningful and interpretable clusters on the input graphs?\\n\\\\end{enumerate}\\n\\\\item[{\\\\bf Q1}] How does  \\\\name\\\\ compare to other pooling methods proposed for GNNs (e.g., using sort pooling~\\\\cite{zhang2018end} or the \\\\textsc{Set2Set} method \\\\cite{Gil+2017})?\\n\\\\item[{\\\\bf Q2}] How does  \\\\name\\\\ combined with GNNs compare to the state-of-the-art for graph classification task, including both GNNs and kernel-based methods?\\n\\\\item[{\\\\bf Q3}] Does  \\\\name  compute meaningful and interpretable clusters on the input graphs?', '1806.08804v4', 'Experiments'), (97, '9', '\\\\cut{\\nGraphs in the \\\\textsc{Collab} data set, in contrast, are very dense and do not have a hierarchical structure. As illustrated in Section \\\\ref{sec:sparse_dense}, \\\\name tends to assign nodes in densely connected subgraphs into a single cluster. In practice, we observe that hierarchies deeper than two do not result in performance improvement in this data set, which again stresses that it does not contain any hierarchical structure.}\\nGraphs in the \\\\textsc{Collab} data set, in contrast, are very dense and do not have a hierarchical structure. As illustrated in Section \\\\ref{sec:sparse_dense}, \\\\name tends to assign nodes in densely connected subgraphs into a single cluster. In practice, we observe that hierarchies deeper than two do not result in performance improvement in this data set, which again stresses that it does not contain any hierarchical structure.', '1806.08804v4', 'Experiments'), (109, '0', 'This research has been supported in part by DARPA SIMPLEX, Stanford Data\\nScience Initiative, Huawei, JD and Chan Zuckerberg Biohub.\\nChristopher Morris is funded by the German Science Foundation (DFG) within the Collaborative Research Center SFB 876 “Providing Information by Resource-Constrained Data Analysis”, project A6 “Resource-efficient Graph Mining”. \\nThe authors also thank Marinka Zitnik for help in visualizing the high-level illustration of the proposed methods.', '1806.08804v4', 'Acknowledgement'), (90, '2', \"\\\\xhdr{Data sets}Data sets\\nTo probe the ability of \\\\name to learn complex hierarchical structures from graphs in different domains, we evaluate on a variety of relatively large graph data sets chosen from benchmarks commonly used in graph classification \\\\cite{KKMMN2016}. We use protein data sets including \\\\textsc{Enzymes}, \\\\textsc{Proteins}~\\\\cite{Borgwardt2005a, Fer+2013}, \\\\textsc{D\\\\&D}~\\\\cite{Dob+2003}, the social network data set \\\\textsc{Reddit-Multi-12k}~\\\\cite{Yan+2015a}, and the scientific collaboration data set \\\\textsc{Collab}~\\\\cite{Yan+2015a}. See Appendix A for statistics and properties.\\nFor all these data sets, we perform 10-fold cross-validation to evaluate model performance, and report the accuracy averaged over 10 folds. \\n\\\\xhdr{Model configurations}Model configurations\\nIn our experiments, the GNN model used for \\\\name\\\\ is built on top of the \\\\textsc{GraphSage} architecture, as we found this architecture to have superior performance compared to the standard GCN approach as introduced in \\\\cite{kipf2017semi}. \\nWe use the ``mean'' variant of \\\\textsc{GraphSage}~\\\\cite{hamilton2017inductive} and apply a \\\\name layer after every two \\\\textsc{GraphSage} layers in our architecture.\\nA total of 2 \\\\name layers are used for the datasets. For small datasets such as \\\\textsc{Enzymes} and \\\\textsc{Collab}, 1 \\\\name layer can achieve similar performance.\\nAfter each \\\\name layer, 3 layers of graph convolutions are performed, before the next \\\\name layer, or the readout layer.\\nThe embedding matrix and the assignment matrix are computed by two separate \\\\textsc{GraphSage} models respectively.\\nIn the 2 \\\\name layer architecture, the number of clusters is set as $25\\\\%$25\\\\% of the number of nodes before applying \\\\name,\\nwhile in the 1 \\\\name layer architecture, the number of clusters is set as $10\\\\%$10\\\\%.\\nBatch normalization \\\\cite{ioffe2015batch} is applied after every layer of \\\\textsc{GraphSage}. \\nWe also found that adding an $\\\\ell_2$\\\\ell_2 normalization to the node embeddings at each layer made the training more stable. \\nIn Section \\\\ref{sec:s2v}, we also test an analogous variant of \\\\name on the \\\\textsc{Structure2Vec} \\\\cite{dai2016discriminative} architecture, in order to demonstrate how \\\\name\\\\ can be applied on top of other GNN models. \\nAll models are trained for 3\\\\,000 epochs with early stopping applied when the validation loss starts to drop.\\nWe also evaluate two simplified versions of \\\\name:\\n\\\\begin{itemize}[leftmargin=15pt, topsep=-5pt, parsep=0pt]\\n \\\\item \\\\textsc{\\\\name-Det}, is a \\\\name\\\\ model where assignment matrices are generated using a deterministic graph clustering algorithm~\\\\cite{dhillon2007weighted}.% This follows the approach used in \\\\cite{Def+2015}, but uses a better performing GNN archicture and clustering algorithm. \\n    \\\\item\\n    \\\\textsc{DiffPool-NoLP} is a variant of \\\\textsc{\\\\name} where the link prediction side objective is turned off.\\n\\\\end{itemize}\\\\begin{itemize}[leftmargin=15pt, topsep=-5pt, parsep=0pt]\\n \\\\item \\\\textsc{\\\\name-Det}, is a \\\\name\\\\ model where assignment matrices are generated using a deterministic graph clustering algorithm~\\\\cite{dhillon2007weighted}.% This follows the approach used in \\\\cite{Def+2015}, but uses a better performing GNN archicture and clustering algorithm. \\n    \\\\item\\n    \\\\textsc{DiffPool-NoLP} is a variant of \\\\textsc{\\\\name} where the link prediction side objective is turned off.\\n\\\\end{itemize}\\n \\\\item \\\\textsc{\\\\name-Det}, is a \\\\name\\\\ model where assignment matrices are generated using a deterministic graph clustering algorithm~\\\\cite{dhillon2007weighted}.\\\\item\\n    \\\\textsc{DiffPool-NoLP} is a variant of \\\\textsc{\\\\name} where the link prediction side objective is turned off.\", '1806.08804v4', 'Experiments'), (91, '3', '\\\\subsection{Baseline Methods}\\nIn the performance comparison on graph classification, we consider baselines based upon GNNs (combined with different pooling methods) as well as state-of-the-art kernel-based approaches.', '1806.08804v4', 'Experiments'), (92, '4', '\\\\xhdr{GNN-based methods}GNN-based methods \\n\\\\begin{itemize}[leftmargin=15pt, topsep=-5pt, parsep=0pt]\\n    \\\\item \\\\textsc{GraphSage} with global mean-pooling \\\\cite{hamilton2017inductive}. Other GNN variants such as those proposed in \\\\cite{kipf2017semi} are omitted as empirically GraphSAGE obtained higher performance in the task.\\n    \\\\item \\\\textsc{Structure2Vec} (\\\\textsc{S2V})~\\\\cite{dai2016discriminative} is a state-of-the-art graph representation learning algorithm that combines a latent variable model with GNNs. It uses global mean pooling.\\n    \\\\item Edge-conditioned filters in CNN for graphs (\\\\textsc{ECC})~\\\\cite{simonovsky2017dynamic} incorporates edge information into the GCN model and performs pooling using a graph coarsening algorithm. %\\\\chris{Are we using the version with clustering here?}\\n    \\\\item \\\\textsc{PatchySan}~\\\\cite{niepert2016learning} defines a receptive field (neighborhood) for each node, and using a canonical node ordering, applies convolutions on linear sequences of node embeddings. \\n    \\\\item \\\\textsc{Set2Set} replaces the global mean-pooling in the traditional GNN architectures by the aggregation used in \\\\textsc{Set2Set}~\\\\cite{vinyals2015order}. Set2Set aggregation has been shown to perform better than mean pooling in previous work \\\\cite{Gil+2017}. We use \\\\textsc{GraphSage} as the base GNN model. \\n    \\\\item  \\\\textsc{SortPool}~\\\\cite{zhang2018end} applies a GNN architecture and then performs a single layer of soft pooling followed by 1D convolution on sorted node embeddings. \\n\\\\end{itemize}\\\\begin{itemize}[leftmargin=15pt, topsep=-5pt, parsep=0pt]\\n    \\\\item \\\\textsc{GraphSage} with global mean-pooling \\\\cite{hamilton2017inductive}. Other GNN variants such as those proposed in \\\\cite{kipf2017semi} are omitted as empirically GraphSAGE obtained higher performance in the task.\\n    \\\\item \\\\textsc{Structure2Vec} (\\\\textsc{S2V})~\\\\cite{dai2016discriminative} is a state-of-the-art graph representation learning algorithm that combines a latent variable model with GNNs. It uses global mean pooling.\\n    \\\\item Edge-conditioned filters in CNN for graphs (\\\\textsc{ECC})~\\\\cite{simonovsky2017dynamic} incorporates edge information into the GCN model and performs pooling using a graph coarsening algorithm. %\\\\chris{Are we using the version with clustering here?}\\n    \\\\item \\\\textsc{PatchySan}~\\\\cite{niepert2016learning} defines a receptive field (neighborhood) for each node, and using a canonical node ordering, applies convolutions on linear sequences of node embeddings. \\n    \\\\item \\\\textsc{Set2Set} replaces the global mean-pooling in the traditional GNN architectures by the aggregation used in \\\\textsc{Set2Set}~\\\\cite{vinyals2015order}. Set2Set aggregation has been shown to perform better than mean pooling in previous work \\\\cite{Gil+2017}. We use \\\\textsc{GraphSage} as the base GNN model. \\n    \\\\item  \\\\textsc{SortPool}~\\\\cite{zhang2018end} applies a GNN architecture and then performs a single layer of soft pooling followed by 1D convolution on sorted node embeddings. \\n\\\\end{itemize}\\n    \\\\item \\\\textsc{GraphSage} with global mean-pooling \\\\cite{hamilton2017inductive}. Other GNN variants such as those proposed in \\\\cite{kipf2017semi} are omitted as empirically GraphSAGE obtained higher performance in the task.\\n    \\\\item \\\\textsc{Structure2Vec} (\\\\textsc{S2V})~\\\\cite{dai2016discriminative} is a state-of-the-art graph representation learning algorithm that combines a latent variable model with GNNs. It uses global mean pooling.\\n    \\\\item Edge-conditioned filters in CNN for graphs (\\\\textsc{ECC})~\\\\cite{simonovsky2017dynamic} incorporates edge information into the GCN model and performs pooling using a graph coarsening algorithm. \\\\item \\\\textsc{PatchySan}~\\\\cite{niepert2016learning} defines a receptive field (neighborhood) for each node, and using a canonical node ordering, applies convolutions on linear sequences of node embeddings. \\n    \\\\item \\\\textsc{Set2Set} replaces the global mean-pooling in the traditional GNN architectures by the aggregation used in \\\\textsc{Set2Set}~\\\\cite{vinyals2015order}. Set2Set aggregation has been shown to perform better than mean pooling in previous work \\\\cite{Gil+2017}. We use \\\\textsc{GraphSage} as the base GNN model. \\n    \\\\item  \\\\textsc{SortPool}~\\\\cite{zhang2018end} applies a GNN architecture and then performs a single layer of soft pooling followed by 1D convolution on sorted node embeddings.', '1806.08804v4', 'Experiments'), (93, '5', \"\\\\medskip\\nFor all the GNN baselines, we use 10-fold cross validation numbers reported by the original authors when possible. \\nFor the \\\\textsc{GraphSage} and \\\\textsc{Set2Set} baselines, we use the base implementation and hyperparameter sweeps as in our \\\\name\\\\ approach.\\nWhen baseline approaches did not have the necessary published numbers, we contacted the original authors and used \\\\textbf{}their code (if available) to run the model, performing a hyperparameter search based on the original author's guidelines.\", '1806.08804v4', 'Experiments'), (94, '6', '\\\\xhdr{Kernel-based algorithms}Kernel-based algorithms\\nWe use the \\\\textsc{Graphlet}~\\\\cite{She+2009}, the \\\\textsc{Shortest-Path}~\\\\cite{Borgwardt2005}, \\\\textsc{Weisfeiler-Lehman} kernel (\\\\textsc{WL})~\\\\cite{She+2011}, and \\\\textsc{Weisfeiler-Lehman Optimal Assignment kernel} (\\\\textsc{WL-OA})~\\\\cite{kriege2016valid} as kernel baselines. For each kernel, we computed the normalized gram matrix. We computed the classification accuracies using the $C$C-SVM implementation of \\\\textsc{LibSvm}~\\\\cite{Cha+11}, using 10-fold cross validation. The $C$C parameter was selected from $\\\\{10^{-3}, 10^{-2}, \\\\dotsc, 10^{2},$\\\\{10^{-3}-3, 10^{-2}-2, \\\\dotsc, 10^{2}2, $10^{3}\\\\}$10^{3}3\\\\} by 10-fold cross validation on the training folds. Moreover, for \\\\textsc{WL} and \\\\textsc{WL-OA} we additionally selected the number of iteration from $\\\\{0, \\\\dots, 5\\\\}$\\\\{0, \\\\dots, 5\\\\}.', '1806.08804v4', 'Experiments'), (95, '7', \"\\\\subsection{Results for Graph Classification}\\\\label{sec:classification}\\nTable \\\\ref{tab:results} compares the performance of \\\\name\\\\ to these state-of-the-art graph classification baselines.\\nThese results provide positive answers to our motivating questions {\\\\bf Q1}\\\\bf Q1 and {\\\\bf Q2}\\\\bf Q2:\\nWe observe that our \\\\name approach obtains the highest average performance among all pooling approaches for GNNs, improves upon the base \\\\textsc{GraphSage} architecture by an average of $6.27\\\\%$6.27\\\\%, and achieves state-of-the-art results on 4 out of 5 benchmarks. Interestingly, our simplified model variant, \\\\textsc{\\\\name-Det}, achieves state-of-the-art performance on the \\\\textsc{Collab} benchmark. This is because many collaboration graphs in \\\\textsc{Collab} show only single-layer community structures, which can be captured well with pre-computed graph clustering algorithm~\\\\cite{dhillon2007weighted}.\\nOne observation is that despite significant performance improvement, \\\\name could be unstable to train, and there is significant variation in accuracy across different runs, even with the same hyperparameter setting. It is observed that adding the link predictioin objective makes training more stable, and reduces the standard deviation of accuracy across different runs.\\n\\\\cut{\\nAmong the baseline methods, the kernel-based \\\\textsc{WL-OA} also performs quite well, achieving the second-best accuracy on the \\\\textsc{Collab} benchmark, which contains exceptionally dense graphs. \\napplied on top of \\\\textsc{GraphSage} with GNNs using other pooling methods, as well as kernel-based methods. In the last column we report the percentage gain of each GNN pooling baseline over \\\\textsc{GraphSage} with naive mean pooling.\\n\\\\textsc{Set2Set} aggregation has shown to give significant gains in many data sets, achieving an average of $3.23\\\\%$ improvement compared to the naive baseline of \\\\textsc{GraphSage} with global pooling. However, \\\\textsc{Set2Set} aggregation has longer running time: it runs $12$ times slower than \\\\name on average.\\nIn comparison, \\\\textsc{PatchySan}, \\\\textsc{SortPool} and \\\\textsc{ClusterPool}  all achieve better results, due to their ability to pool according to structures of the graphs. \\n%Notably, ClusterPool achieves an improvement of XX$\\\\%$ over \\\\textbf{GraphSAGE}, due to it's ability to capture hierarchies of clusters.\\n}\\nAmong the baseline methods, the kernel-based \\\\textsc{WL-OA} also performs quite well, achieving the second-best accuracy on the \\\\textsc{Collab} benchmark, which contains exceptionally dense graphs. \\napplied on top of \\\\textsc{GraphSage} with GNNs using other pooling methods, as well as kernel-based methods. In the last column we report the percentage gain of each GNN pooling baseline over \\\\textsc{GraphSage} with naive mean pooling.\\n\\\\textsc{Set2Set} aggregation has shown to give significant gains in many data sets, achieving an average of $3.23\\\\%$3.23\\\\% improvement compared to the naive baseline of \\\\textsc{GraphSage} with global pooling. However, \\\\textsc{Set2Set} aggregation has longer running time: it runs $12$12 times slower than \\\\name on average.\\nIn comparison, \\\\textsc{PatchySan}, \\\\textsc{SortPool} and \\\\textsc{ClusterPool}  all achieve better results, due to their ability to pool according to structures of the graphs.\", '1806.08804v4', 'Experiments'), (96, '8', '\\\\cut{\\nOne notable data set that demonstrates the expressivity of \\\\name is \\\\textsc{Reddit-Multi-12k}, in which each graph represents an online discussion thread between users/nodes (an edge is formed when a user replies to another user). It contains rich hierarchical structures due to the tree-structured nature of Reddit discussions: there are small clusters of discussion among small numbers of users occurring at the leaves of the discussion threads, and the small clusters themselves are grouped into larger clusters based on higher-level threads/topics. \\n\\\\name significantly outperforms other methods on this data set, as it can automatically extract meaningful clusters (in a hierarchical fashion) from these natural thread-based graphs. \\n}\\nOne notable data set that demonstrates the expressivity of \\\\name is \\\\textsc{Reddit-Multi-12k}, in which each graph represents an online discussion thread between users/nodes (an edge is formed when a user replies to another user). It contains rich hierarchical structures due to the tree-structured nature of Reddit discussions: there are small clusters of discussion among small numbers of users occurring at the leaves of the discussion threads, and the small clusters themselves are grouped into larger clusters based on higher-level threads/topics. \\n\\\\name significantly outperforms other methods on this data set, as it can automatically extract meaningful clusters (in a hierarchical fashion) from these natural thread-based graphs.', '1806.08804v4', 'Experiments'), (98, '10', '\\\\xhdr{Differentiable Pooling on \\\\textsc{Structure2Vec}}Differentiable Pooling on \\\\textsc{Structure2Vec}\\\\label{sec:s2v}\\n\\\\name can be applied to other GNN architectures besides \\\\textsc{GraphSage} to capture hierarchical structure in the graph data.\\nTo further support answering {\\\\bf Q1}\\\\bf Q1, we also applied \\\\name on Structure2Vec (\\\\textsc{S2V}). \\nWe ran experiments using \\\\textsc{S2V} with three layer architecture, as reported in \\\\cite{dai2016discriminative}.\\nIn the first variant, one \\\\name layer is applied after the first layer of \\\\textsc{S2V}, and two more \\\\textsc{S2V} layers are stacked on top of the output of \\\\name. The second variant applies one \\\\name layer after the first and second layer of \\\\textsc{S2V} respectively. \\nIn both variants, \\\\textsc{S2V} model is used to compute the embedding matrix, while \\\\textsc{GraphSage} model is used to compute the assignment matrix.', '1806.08804v4', 'Experiments'), (99, '11', 'The results in terms of classification accuracy are summarized in Table \\\\ref{tab:results2}.\\nWe observe that \\\\name significantly improves the performance of S2V on both \\\\textsc{Enzymes} and \\\\textsc{D\\\\&D} data sets. Similar performance trends are also observed on other data sets.\\nThe results demonstrate that \\\\name is a general strategy to pool over hierarchical structure that can benefit different GNN architectures.', '1806.08804v4', 'Experiments'), (100, '12', '\\\\xhdr{Running time}Running time Although applying \\\\name requires additional computation of an assignment matrix, we observed that \\\\name did not incur substantial additional running time in practice.\\nThis is because each \\\\name layer reduces the size of graphs by extracting a coarser representation of the graph, which speeds up the graph convolution operation in the next layer.\\nConcretely, we found that \\\\textsc{GraphSage} with \\\\name\\\\ was 12$\\\\times$\\\\times faster than the $\\\\textsc{GraphSage}$\\\\textsc{GraphSage} model with $\\\\textsc{Set2Set}$\\\\textsc{Set2Set} pooling, while still achieving significantly higher accuracy on all benchmarks.', '1806.08804v4', 'Experiments'), (101, '13', '\\\\subsection{Analysis of Cluster Assignment in \\\\name}\\n\\\\label{sec:assignment_vis}', '1806.08804v4', 'Experiments'), (102, '14', '\\\\xhdr{Hierarchical cluster structure}Hierarchical cluster structure\\nTo address {\\\\bf Q3}\\\\bf Q3, we investigated the extent to which \\\\name learns meaningful node clusters by visualizing the cluster assignments in different layers. Figure \\\\ref{fig:assignment_vis} shows such a visualization of node assignments in the first and second layers on a graph from \\\\textsc{Collab} data set, where node color indicates its cluster membership. Node cluster membership is determined by taking the $\\\\argmax$\\\\argmax of its cluster assignment probabilities. We observe that even when learning cluster assignment based solely on the graph classification objective, \\\\name can still capture the hierarchical community structure. We also observe significant improvement in membership assignment quality with link prediction auxiliary objectives.', '1806.08804v4', 'Experiments'), (103, '15', '\\\\xhdr{Dense vs. sparse subgraph structure}Dense vs. sparse subgraph structure\\nIn addition, we observe that \\\\name learns to collapse nodes into soft clusters in a non-uniform way, with a tendency to collapse densely-connected subgraphs into clusters. \\nSince GNNs can efficiently perform message-passing on dense, clique-like subgraphs (due to their small diameters) \\\\cite{liao2018graph}, pooling together nodes in such a dense subgraph is not likely to lead to any loss of structural information. \\nThis intuitively explains why collapsing dense subgraphs is a useful pooling strategy for \\\\name. \\nIn contrast, sparse subgraphs may contain many interesting structures, including path-, cycle- and tree-like structures, and given the high-diameter induced by sparsity, GNN message-passing may fail to capture these structures. \\nThus, by separately pooling distinct parts of a sparse subgraph, \\\\name can learn to capture the meaningful structures present in sparse graph regions (e.g., as in Figure~\\\\ref{fig:assignment_vis}).', '1806.08804v4', 'Experiments'), (104, '16', '\\\\xhdr{Assignment for nodes with similar representations}Assignment for nodes with similar representations\\nSince the assignment network computes the soft cluster assignment based on features of input nodes and their neighbors, nodes with both similar input features and neighborhood structure will have similar cluster assignment.\\nIn fact, one can construct synthetic cases where 2 nodes, although far away, have exactly the same neighborhood structure and features for self and all neighbors. In this case the pooling network is forced to assign them into the same cluster, which is different from the concept of pooling in other architectures such as image ConvNets. In some cases we do observe that disconnected nodes are pooled together.', '1806.08804v4', 'Experiments'), (105, '17', 'In practice we rely on the identifiability assumption similar to Theorem 1 in GraphSAGE \\\\cite{hamilton2017inductive}, where nodes are identifiable via their features. This holds in many real datasets \\\\footnote{However, some chemistry molecular graph datasets contain many nodes that are structurally similar, and assignment network is observed to pool together nodes that are far away.}. \\nThe auxiliary link prediction objective is observed to also help discouraging nodes that are far away to be pooled together. Furthermore, it is possible to use more sophisticated GNN aggregation function such as high-order moments \\\\cite{verma2018graph} to distinguish nodes that are similar in structure and feature space. The overall framework remains unchanged.', '1806.08804v4', 'Experiments'), (106, '18', '\\\\xhdr{Sensitivity of the Pre-defined Maximum Number of Clusters}Sensitivity of the Pre-defined Maximum Number of Clusters\\nWe found that the assignment varies according to the depth of the network and $C$C, the maximum number of clusters. With larger $C$C, the pooling GNN can model more complex hierarchical structure. The trade-off is that very large $C$C results in more noise and less efficiency. \\nAlthough the value of $C$C is a pre-defined parameter, the pooling net learns to use the appropriate number of clusters by end-to-end training. \\nIn particular, some clusters might not be used by the assignment matrix. Column corresponding to unused cluster has low values for all nodes. This is observed in Figure \\\\ref{fig:assignment_vis}(c), where nodes are assigned predominantly into 3 clusters.', '1806.08804v4', 'Experiments'), (107, '19', '\\\\centering\\n    \\\\includegraphics[width=0.8\\\\textwidth]{figs/diffpool_vis.pdf}\\n    \\\\caption{Visualization of hierarchical cluster assignment over two \\\\name\\\\ layers, using an example graph from \\\\textsc{Collab}.\\n      Nodes in the right-hand side plot correspond to cluster in the original graph (left-hand side). Colors are used to connect the nodes/clusters across the graphs, and dotted lines are used to indicate clusters.}\\n    \\\\label{fig:assignment_vis}', '1806.08804v4', 'Experiments'), (108, '0', 'We introduced a differentiable pooling method for GNNs that is able to extract the complex hierarchical structure of real-world graphs. By using the proposed pooling layer in conjunction with existing GNN models, we achieved new state-of-the-art results on several graph classification benchmarks. \\nInteresting future directions include learning hard cluster assignments to further reduce computational cost in higher layers while also ensuring differentiability, and applying the hierarchical pooling method to other downstream tasks that require modeling of the entire graph structure.', '1806.08804v4', 'Conclusion'), (110, '1', '\\\\bibliography{refs}\\n\\\\bibliographystyle{abbrv}abbrv', '1806.08804v4', 'Acknowledgement'), (114, '4', 'Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences.', '1706.03762v7', 'Introduction'), (115, '5', 'In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.', '1706.03762v7', 'Introduction')]\n"
     ]
    }
   ],
   "source": [
    "paragraphs_df = research_arcade.get_all_node_features(\"arxiv_paragraphs\")\n",
    "print(f\"Total paragraphs: {len(paragraphs_df)}\")\n",
    "print(\"\\nFirst 3 paragraphs:\")\n",
    "print(paragraphs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Relationships/Edges <a name=\"relationships\"></a>\n",
    "\n",
    "This section demonstrates how to create and manage relationships between different entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 ArXiv Citations (arxiv_citation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citation created!\n"
     ]
    }
   ],
   "source": [
    "citation = {\n",
    "    'citing_arxiv_id': '1810.04805v2',\n",
    "    'cited_arxiv_id': '1706.03762v7',\n",
    "    'bib_title': 'attention is all you need',\n",
    "    'bib_key': 'something',\n",
    "    'citing_sections': ['something'],\n",
    "}\n",
    "research_arcade.insert_edge(\"arxiv_citation\", edge_features=citation)\n",
    "print(\"Citation created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: CSV file ./examples/csv_data/csv_arxiv_paper_citation_example.csv does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"csv_file\": \"./examples/csv_data/csv_arxiv_paper_citation_example.csv\"}\n",
    "research_arcade.construct_table_from_csv(\"arxiv_paper_citation\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: JSON file ./examples/json_data/json_arxiv_paper_citation_example.json does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"json_file\": \"./examples/json_data/json_arxiv_paper_citation_example.json\"}\n",
    "research_arcade.construct_table_from_json(\"arxiv_paper_citation\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get All Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total citations: 1\n",
      "[(1, '1810.04805v2', '1706.03762v7', 'attention is all you need', 'something', None, ['something'], [])]\n"
     ]
    }
   ],
   "source": [
    "all_citations = research_arcade.get_all_edge_features(\"arxiv_citation\")\n",
    "print(f\"Total citations: {len(all_citations)}\")\n",
    "print(all_citations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Cited Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers cited:\n",
      "[(1, '1810.04805v2', '1706.03762v7', 'attention is all you need', 'something', None, ['something'], [])]\n"
     ]
    }
   ],
   "source": [
    "citing_paper = {'citing_paper_id': '1810.04805v2'}\n",
    "cited_papers = research_arcade.get_neighborhood(\"arxiv_citation\", primary_key=citing_paper)\n",
    "print(\"Papers cited:\")\n",
    "print(cited_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Citing Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers that cite:\n",
      "[(1, '1810.04805v2', '1706.03762v7', 'attention is all you need', 'something', None, ['something'], [])]\n"
     ]
    }
   ],
   "source": [
    "cited_paper = {'cited_paper_id': '1706.03762v7'}\n",
    "citing_papers = research_arcade.get_neighborhood(\"arxiv_citation\", primary_key=cited_paper)\n",
    "print(\"Papers that cite:\")\n",
    "print(citing_papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted citation: 1810.04805v2 -> 1706.03762v7\n",
      "Citation deleted!\n"
     ]
    }
   ],
   "source": [
    "citation_id = {\n",
    "    'citing_paper_id': '1810.04805v2',\n",
    "    'cited_paper_id': '1706.03762v7'\n",
    "}\n",
    "research_arcade.delete_edge_by_id(\"arxiv_citation\", primary_key=citation_id)\n",
    "print(\"Citation deleted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 ArXiv Paper-Author (arxiv_paper_author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Paper-Author Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked author ss_ashish_vaswani (position 1)\n",
      "Linked author ss_noam_shazeer (position 2)\n",
      "Linked author ss_niki_parmar (position 3)\n"
     ]
    }
   ],
   "source": [
    "paper_authors = [\n",
    "    {'paper_arxiv_id': '1706.03762v7', 'author_id': 'ss_ashish_vaswani', 'author_sequence': 1},\n",
    "    {'paper_arxiv_id': '1706.03762v7', 'author_id': 'ss_noam_shazeer', 'author_sequence': 2},\n",
    "    {'paper_arxiv_id': '1706.03762v7', 'author_id': 'ss_niki_parmar', 'author_sequence': 3}\n",
    "]\n",
    "for relation in paper_authors:\n",
    "    research_arcade.insert_edge(\"arxiv_paper_author\", edge_features=relation)\n",
    "    print(f\"Linked author {relation['author_id']} (position {relation['author_sequence']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: CSV file ./examples/csv_data/csv_arxiv_paper_author_example.csv does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"csv_file\": \"./examples/csv_data/csv_arxiv_paper_author_example.csv\"}\n",
    "research_arcade.construct_table_from_csv(\"arxiv_paper_author\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: JSON file ./examples/json_data/json_arxiv_paper_author_example.json does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"json_file\": \"./examples/json_data/json_arxiv_paper_author_example.json\"}\n",
    "research_arcade.construct_table_from_json(\"arxiv_paper_author\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get All Paper-Author Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total relationships: 3\n",
      "[('1706.03762v7', 'ss_ashish_vaswani', 1), ('1706.03762v7', 'ss_noam_shazeer', 2), ('1706.03762v7', 'ss_niki_parmar', 3)]\n"
     ]
    }
   ],
   "source": [
    "all_relations = research_arcade.get_all_edge_features(\"arxiv_paper_author\")\n",
    "print(f\"Total relationships: {len(all_relations)}\")\n",
    "print(all_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Authors for a Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors:\n",
      "[('1706.03762v7', 'ss_ashish_vaswani', 1), ('1706.03762v7', 'ss_noam_shazeer', 2), ('1706.03762v7', 'ss_niki_parmar', 3)]\n"
     ]
    }
   ],
   "source": [
    "paper_id = {'paper_arxiv_id': '1706.03762v7'}\n",
    "authors = research_arcade.get_neighborhood(\"arxiv_paper_author\", primary_key=paper_id)\n",
    "print(\"Authors:\")\n",
    "print(authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Papers by Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers by author:\n",
      "[('1706.03762v7', 'ss_ashish_vaswani', 1)]\n"
     ]
    }
   ],
   "source": [
    "author_id = {'author_id': 'ss_ashish_vaswani'}\n",
    "papers = research_arcade.get_neighborhood(\"arxiv_paper_author\", primary_key=author_id)\n",
    "print(\"Papers by author:\")\n",
    "print(papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Paper-Author Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationship deleted!\n"
     ]
    }
   ],
   "source": [
    "relation_id = {'paper_arxiv_id': '1706.03762v7', 'author_id': 'ss_ashish_vaswani'}\n",
    "research_arcade.delete_edge_by_id(\"arxiv_paper_author\", primary_key=relation_id)\n",
    "print(\"Relationship deleted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4 ArXiv Paper-Category (arxiv_paper_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Paper-Category Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked 1\n",
      "Linked 1\n",
      "Linked 2\n"
     ]
    }
   ],
   "source": [
    "paper_categories = [\n",
    "    {'paper_arxiv_id': '1706.03762v7', 'category_id': '1'},\n",
    "    {'paper_arxiv_id': '1706.03762v7', 'category_id': '1'},\n",
    "    {'paper_arxiv_id': '1706.03762v7', 'category_id': '2'}\n",
    "]\n",
    "for relation in paper_categories:\n",
    "    research_arcade.insert_edge(\"arxiv_paper_category\", edge_features=relation)\n",
    "    print(f\"Linked {relation['category_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: CSV file ./examples/csv_data/csv_arxiv_paper_category_example.csv does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"csv_file\": \"./examples/csv_data/csv_arxiv_paper_category_example.csv\"}\n",
    "research_arcade.construct_table_from_csv(\"arxiv_paper_category\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: JSON file ./examples/json_data/json_arxiv_paper_category_example.json does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"json_file\": \"./examples/json_data/json_arxiv_paper_category_example.json\"}\n",
    "research_arcade.construct_table_from_json(\"arxiv_paper_category\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get All Paper-Category Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total relationships: 2\n",
      "[('1706.03762v7', '1'), ('1706.03762v7', '2')]\n"
     ]
    }
   ],
   "source": [
    "all_relations = research_arcade.get_all_edge_features(\"arxiv_paper_category\")\n",
    "print(f\"Total relationships: {len(all_relations)}\")\n",
    "print(all_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Categories for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories:\n",
      "[('1706.03762v7', '1'), ('1706.03762v7', '2')]\n"
     ]
    }
   ],
   "source": [
    "paper_id = {'paper_arxiv_id': '1706.03762v7'}\n",
    "categories = research_arcade.get_neighborhood(\"arxiv_paper_category\", primary_key=paper_id)\n",
    "print(\"Categories:\")\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Papers in Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers in category:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "category_id = {'category_id': 'cs.LG'}\n",
    "papers = research_arcade.get_neighborhood(\"arxiv_paper_category\", primary_key=category_id)\n",
    "print(\"Papers in category:\")\n",
    "print(papers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete Paper-Category Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationship deleted!\n"
     ]
    }
   ],
   "source": [
    "relation_id = {'paper_arxiv_id': '1706.03762v7', 'category_id': 'cs.AI'}\n",
    "research_arcade.delete_edge_by_id(\"arxiv_paper_category\", primary_key=relation_id)\n",
    "print(\"Relationship deleted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5 ArXiv Paper-Figure (arxiv_paper_figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Paper-Figure Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked figure 1)\n",
      "Linked figure 2)\n"
     ]
    }
   ],
   "source": [
    "paper_figures = [\n",
    "    {'paper_arxiv_id': '1706.03762v7', 'figure_id': 1},\n",
    "    {'paper_arxiv_id': '1706.03762v7', 'figure_id': 2}\n",
    "]\n",
    "for relation in paper_figures:\n",
    "    research_arcade.insert_edge(\"arxiv_paper_figure\", edge_features=relation)\n",
    "    print(f\"Linked figure {relation['figure_id']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: CSV file ./examples/csv_data/csv_arxiv_paper_figure_example.csv does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"csv_file\": \"./examples/csv_data/csv_arxiv_paper_figure_example.csv\"}\n",
    "research_arcade.construct_table_from_csv(\"arxiv_paper_figure\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: JSON file ./examples/json_data/json_arxiv_paper_figure_example.json does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"json_file\": \"./examples/json_data/json_arxiv_paper_figure_example.json\"}\n",
    "research_arcade.construct_table_from_json(\"arxiv_paper_figure\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Figures for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figures:\n",
      "[('1706.03762v7', 1), ('1706.03762v7', 2)]\n"
     ]
    }
   ],
   "source": [
    "paper_id = {'paper_arxiv_id': '1706.03762v7'}\n",
    "figures = research_arcade.get_neighborhood(\"arxiv_paper_figure\", primary_key=paper_id)\n",
    "print(\"Figures:\")\n",
    "print(figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.6 ArXiv Paper-Table (arxiv_paper_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Paper-Table Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linked table 1\n",
      "Linked table 2\n"
     ]
    }
   ],
   "source": [
    "paper_tables = [\n",
    "    {'paper_arxiv_id': '1706.03762v7', 'table_id': 1},\n",
    "    {'paper_arxiv_id': '1706.03762v7', 'table_id': 2}\n",
    "]\n",
    "for relation in paper_tables:\n",
    "    research_arcade.insert_edge(\"arxiv_paper_table\", edge_features=relation)\n",
    "    print(f\"Linked table {relation['table_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SQLArxivPaperTable' object has no attribute 'construct_table_from_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m config = {\u001b[33m\"\u001b[39m\u001b[33mcsv_file\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m./examples/csv_data/csv_arxiv_paper_table_example.csv\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mresearch_arcade\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconstruct_table_from_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marxiv_paper_table\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/cl195/paper-crawler/research_arcade/research_arcade.py:653\u001b[39m, in \u001b[36mResearchArcade.construct_table_from_csv\u001b[39m\u001b[34m(self, table, config)\u001b[39m\n\u001b[32m    651\u001b[39m     \u001b[38;5;28mself\u001b[39m.arxiv_paper_figure.construct_table_from_csv(**config)\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m table == \u001b[33m\"\u001b[39m\u001b[33marxiv_paper_table\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43marxiv_paper_table\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconstruct_table_from_csv\u001b[49m(**config)\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m table == \u001b[33m\"\u001b[39m\u001b[33marxiv_paragraph_reference\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    655\u001b[39m     \u001b[38;5;28mself\u001b[39m.arxiv_paragraph_reference.construct_table_from_csv(**config)\n",
      "\u001b[31mAttributeError\u001b[39m: 'SQLArxivPaperTable' object has no attribute 'construct_table_from_csv'"
     ]
    }
   ],
   "source": [
    "config = {\"csv_file\": \"./examples/csv_data/csv_arxiv_paper_table_example.csv\"}\n",
    "research_arcade.construct_table_from_csv(\"arxiv_paper_table\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SQLArxivPaperTable' object has no attribute 'construct_table_from_json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m config = {\u001b[33m\"\u001b[39m\u001b[33mjson_file\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m./examples/json_data/json_arxiv_paper_table_example.json\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mresearch_arcade\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconstruct_table_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marxiv_paper_table\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data/cl195/paper-crawler/research_arcade/research_arcade.py:706\u001b[39m, in \u001b[36mResearchArcade.construct_table_from_json\u001b[39m\u001b[34m(self, table, config)\u001b[39m\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m.arxiv_paper_figure.construct_table_from_json(**config)\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m table == \u001b[33m\"\u001b[39m\u001b[33marxiv_paper_table\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43marxiv_paper_table\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconstruct_table_from_json\u001b[49m(**config)\n\u001b[32m    707\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m table == \u001b[33m\"\u001b[39m\u001b[33marxiv_paragraph_reference\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    708\u001b[39m     \u001b[38;5;28mself\u001b[39m.arxiv_paragraph_reference.construct_table_from_json(**config)\n",
      "\u001b[31mAttributeError\u001b[39m: 'SQLArxivPaperTable' object has no attribute 'construct_table_from_json'"
     ]
    }
   ],
   "source": [
    "config = {\"json_file\": \"./examples/json_data/json_arxiv_paper_table_example.json\"}\n",
    "research_arcade.construct_table_from_json(\"arxiv_paper_table\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Tables for Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables:\n"
     ]
    }
   ],
   "source": [
    "paper_id = {'paper_arxiv_id': '1706.03762v7'}\n",
    "tables = research_arcade.get_neighborhood(\"arxiv_paper_table\", primary_key=paper_id)\n",
    "print(\"Tables:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.7 ArXiv Paragraph-Reference (arxiv_paragraph_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert Paragraph-Reference Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_references = [\n",
    "    {'paragraph_id': 1, 'paper_section': 'established approaches', 'paper_arxiv_id': '1706.03762v7', 'reference_label': \"{something}\", 'reference_type': 'figure'}\n",
    "]\n",
    "\n",
    "for relation in paragraph_references:\n",
    "    research_arcade.insert_edge(\"arxiv_paragraph_reference\", edge_features=relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: CSV file ./examples/csv_data/csv_arxiv_paragraph_reference_example.csv does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"csv_file\": \"./examples/csv_data/csv_arxiv_paragraph_reference_example.csv\"}\n",
    "research_arcade.construct_table_from_csv(\"arxiv_paragraph_reference\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct Table from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: JSON file ./examples/json_data/json_arxiv_paragraph_reference_example.json does not exist.\n"
     ]
    }
   ],
   "source": [
    "config = {\"json_file\": \"./examples/json_data/json_arxiv_paragraph_reference_example.json\"}\n",
    "research_arcade.construct_table_from_json(\"arxiv_paragraph_reference\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get References in Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "References:\n",
      "[(1, 1, 'established approaches', '1706.03762v7', '{something}', 'figure')]\n"
     ]
    }
   ],
   "source": [
    "paragraph_id = {'paragraph_id': 1}\n",
    "references = research_arcade.get_neighborhood(\"arxiv_paragraph_reference\", primary_key=paragraph_id)\n",
    "print(\"References:\")\n",
    "print(references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial has covered:\n",
    "\n",
    "1. Setting up the ResearchArcade database connection\n",
    "2. Working with OpenReview data\n",
    "3. CRUD operations for all ArXiv entity types:\n",
    "   - Papers\n",
    "   - Authors\n",
    "   - Categories\n",
    "   - Figures\n",
    "   - Tables\n",
    "   - Sections\n",
    "   - Paragraphs\n",
    "4. Creating relationships between entities:\n",
    "   - Authorship\n",
    "   - Citations\n",
    "   - Paper-Category links\n",
    "   - Paper-Figure/Table links\n",
    "   - Paragraph-level references\n",
    "\n",
    "For more information, refer to the ResearchArcade documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
