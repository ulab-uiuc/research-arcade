{"title": "Artificial Intelligence in Creative Industries: Advances Prior to 2025", "author": "Nantheera Anantrasirichai", "abstract": "\\begin{abstract}\n   The rapid advancements in artificial intelligence (AI), particularly in generative AI and large language models (LLMs), have profoundly impacted the creative industries by enabling innovative content creation, enhancing workflows, and democratizing access to creative tools. This paper explores the significant technological shifts since our previous review in 2022, highlighting how these developments have expanded creative opportunities and efficiency. These technological advancements have enhanced the capabilities of text-to-image, text-to-video, and multimodal generation technologies. In particular, key breakthroughs in LLMs have established new benchmarks in conversational AI, while advancements in image generators have revolutionized content creation. We also discuss AI integration into post-production workflows, which has significantly accelerated and refined traditional processes. Despite these innovations, challenges remain\u2014particularly for the media industry\u2014due to the demands on communication traffic from creative content. We therefore include data compression and quality assessment in this paper. Furthermore, we highlight the trend toward unified AI frameworks capable of addressing multiple creative tasks and underscore the importance of human oversight to mitigate AI-generated inaccuracies. Finally, we explore AI's future potential in the creative sector, stressing the need to navigate emerging challenges to maximize its benefits while addressing associated risks.\n\n\\end{abstract}", "citations": {"UK:Large:2024": {"bib_key": "UK:Large:2024", "bib_title": "Large language models and generative {AI}", "bib_author ": "{Communications", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "These advancements have been widely regarded as beneficial in many countries, such as the UK, as highlighted in a report published by the Authority of the House of Lords\\cite{UK:Large:2024}", "next_context": "."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "With the impact of these technologies, DMG Media, the Financial Times, and the Guardian Media Group have highlighted concerns about the potential impact on print journalism, particularly if AI tools reduce the need for users to click through to news websites, affecting advertising and subscription revenues\\cite{UK:Large:2024}", "next_context": "."}, {"section": "Future Challenges for AI in the Creative Sector", "subsection": "Ethical Issues, Fakes and Bias", "subsubsection": null, "prev_context": "The hallucinations is one of the issues that the UK government's viewpoint on LLMs highlights ongoing issues\\cite{UK:Large:2024}", "next_context": "."}], "importance_score": 3.0}, "Bai:Train:2021": {"bib_key": "Bai:Train:2021", "bib_title": "Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback", "bib_author ": "Bai, Yuntao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "\\cite{Bai:Train:2021}", "next_context": "."}], "importance_score": 1.0}, "openai:gpt4:2023": {"bib_key": "openai:gpt4:2023", "bib_title": "{GPT-4} Technical Report", "bib_author ": "{OpenAI}", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The next breakthrough happened in 2023 when OpenAI unveiled GPT-4, a significantly larger model with more parameters and improved performance compared to its predecessors\\cite{openai:gpt4:2023}", "next_context": "."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "More recently, inspired by the success of large language models (LLMs)\\cite{openai:gpt4:2023,touvron2023llama}", "next_context": "in other machine learning tasks, they have also been utilized in image and video quality assessment and demonstrate great potential to achieve better model generalization."}], "importance_score": 1.75}, "Wang:Painter:2023": {"bib_key": "Wang:Painter:2023", "bib_title": "Images Speak in Images: A Generalist Painter for In-Context Visual Learning", "bib_author ": "Wang, Xinlong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Take Painter by BAAI Vision\\cite{Wang:Painter:2023}", "next_context": "as an example, which employs an image pair as a task prompt (similar to a text prompt in LLMs), their model transfers the input image to produce a similar output as the task prompt, enabling it to undertake various tasks such as segmentation, low-light enhancement, rain removal, etc."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2}", "next_context": ""}], "importance_score": 1.1916666666666667}, "chung:human-loop:2021": {"bib_key": "chung:human-loop:2021", "bib_title": "Human in the Loop for Machine Creativity", "bib_author ": "Neo Christopher Chung", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "\\cite{chung:human-loop:2021}", "next_context": "is simplified through text prompts, as language enables artists to convey complex emotions and narratives."}], "importance_score": 1.0}, "Wu:brief:2023": {"bib_key": "Wu:brief:2023", "bib_title": "A Brief Overview of {ChatGPT}: The History, Status Quo and Potential Future Development", "bib_author ": "Wu, Tianyu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "This is important because generative AI could produce mistakes, known as hallucinations; hence, we, as humans, correct this through reinforcement learning with human feedback\\cite{Wu:brief:2023}", "next_context": "."}], "importance_score": 1.0}, "Anantrasirichai:AI:2022": {"bib_key": "Anantrasirichai:AI:2022", "bib_title": "Artificial intelligence in the creative industries: a review", "bib_author ": "Anantrasirichai, N.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "In this paper, the objective is to investigate the latest advancements in technology that have emerged since our previous review paper on AI in the creative industries before 2022\\cite{Anantrasirichai:AI:2022}", "next_context": "."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Similar to\\cite{Anantrasirichai:AI:2022}", "next_context": ", we first provide a high-level overview of current advanced AI technologies (Section\\ref{sec:overview}), followed by a selection of creative domain applications (Section\\ref{sec:existing}), where current technologies enabling AI in creative scenarios are described."}, {"section": "Current Advanced AI Technologies", "subsection": null, "subsubsection": null, "prev_context": "As this paper provides updates from our previous review\\cite{Anantrasirichai:AI:2022}", "next_context": ", an introduction to AI, basic neurons, convolutional neural networks (CNNs), generative adversarial networks (GANs), recurrent neural networks (RNNs), and deep reinforcement learning (DRL) can be found there."}, {"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "More details about AEs and GANs for creative technologies can be found in our previous review\\cite{Anantrasirichai:AI:2022}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "In our previous review paper\\cite{Anantrasirichai:AI:2022}", "next_context": ", we discussed AI technologies for contrast enhancement and colorization as separate topics, as methods were developed specifically for each task."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "In our previous review paper\\cite{Anantrasirichai:AI:2022}", "next_context": ", we categorized the work on restoration into several different types of distortions, including deblurring, denoising, dehazing, and mitigating atmospheric turbulence."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Inpainting", "prev_context": "CNNs and GANs have already achieved impressive results (see our previous review paper\\cite{Anantrasirichai:AI:2022}", "next_context": ")."}], "importance_score": 7.0}, "Vaswani:attention:2017": {"bib_key": "Vaswani:attention:2017", "bib_title": "Attention is All you Need", "bib_author ": "Vaswani, Ashish", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "However, the turning point was actually back in 2017 when Google AI introduced `Transformer' architectures in their publication `Attention Is All You Need'\\cite{Vaswani:attention:2017}", "next_context": "; since then, the performance of language models has dramatically improved."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "TrackFormer extracts visual features using a CNN-based encoder, which are then tracked using a vanilla transformer\\cite{Vaswani:attention:2017}", "next_context": "in a frame sequence, while MixFormer introduces cross-attention between the target and search regions."}], "importance_score": 2.25}, "Dosovitskiy:image:2021": {"bib_key": "Dosovitskiy:image:2021", "bib_title": "An Image is Worth 16$\\times$16 Words: {T}ransformers for Image Recognition at Scale", "bib_author ": "Alexey Dosovitskiy", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "The applications have been extended, including vision understanding\\cite{Dosovitskiy:image:2021}", "next_context": ", and even learning multiple tasks across multiple modalities simultaneously (e.g., Gato\\cite{Reed:Generalist:2022})."}, {"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "In 2020, the first successful training of a transformer encoder for image recognition was published in\\cite{Dosovitskiy:image:2021}", "next_context": ", known as Vision Transformer (ViT)."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024}", "next_context": ""}], "importance_score": 2.111111111111111}, "Reed:Generalist:2022": {"bib_key": "Reed:Generalist:2022", "bib_title": "A Generalist Agent", "bib_author ": "Scott Reed", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "The applications have been extended, including vision understanding\\cite{Dosovitskiy:image:2021}, and even learning multiple tasks across multiple modalities simultaneously (e.g., Gato\\cite{Reed:Generalist:2022}", "next_context": ")."}], "importance_score": 1.0}, "Li:HAM:2022": {"bib_key": "Li:HAM:2022", "bib_title": "{HAM: Hybrid} attention module in deep convolutional neural networks for image classification", "bib_author ": "Guoqiang Li", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "Attention modules have not only been using in the transformers, but they have also been integrated with other deep learning architectures, including with CNNs in particular, used for image classification\\cite{Li:HAM:2022}", "next_context": ", object detection\\cite{Woo:CBAM:2018}, and other computer vision tasks\\cite{Guo:Attention:2022}as providing performance improvement."}], "importance_score": 1.0}, "Woo:CBAM:2018": {"bib_key": "Woo:CBAM:2018", "bib_title": "{CBAM: Convolutional} Block Attention Module", "bib_author ": "Woo, Sanghyun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "Attention modules have not only been using in the transformers, but they have also been integrated with other deep learning architectures, including with CNNs in particular, used for image classification\\cite{Li:HAM:2022}, object detection\\cite{Woo:CBAM:2018}", "next_context": ", and other computer vision tasks\\cite{Guo:Attention:2022}as providing performance improvement."}], "importance_score": 1.0}, "Guo:Attention:2022": {"bib_key": "Guo:Attention:2022", "bib_title": "Attention mechanisms in computer vision: A survey", "bib_author ": "Guo, Ming-Hao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "Attention modules have not only been using in the transformers, but they have also been integrated with other deep learning architectures, including with CNNs in particular, used for image classification\\cite{Li:HAM:2022}, object detection\\cite{Woo:CBAM:2018}, and other computer vision tasks\\cite{Guo:Attention:2022}", "next_context": "as providing performance improvement."}], "importance_score": 1.0}, "Liu:Swin:2021": {"bib_key": "Liu:Swin:2021", "bib_title": "{Swin Transformer: Hierarchical} Vision Transformer using Shifted Windows", "bib_author ": "Liu, Ze", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "\\cite{Liu:Swin:2021}", "next_context": "by Microsoft."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "\\cite{Liang:SwinIR:2021}, employs several concatenated Swin Transformer blocks\\cite{Liu:Swin:2021}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "A survey until 2022 reported in\\cite{Zou:object:2023}show that Deformable DETR and Swin Transformer\\cite{Liu:Swin:2021}", "next_context": "outperform pure CNN-based YOLOv4\\cite{bochkovskiy2020yolov4}."}], "importance_score": 3.111111111111111}, "Liu:Swinv2:2022": {"bib_key": "Liu:Swinv2:2022", "bib_title": "Swin Transformer V2: Scaling Up Capacity and Resolution", "bib_author ": "Liu, Ze", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "Its version 2\\cite{Liu:Swinv2:2022}", "next_context": "applies cosine function in the attention module enabling the scaling up of capacity and resolution."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024}", "next_context": ""}], "importance_score": 1.1111111111111112}, "Fan:SUNet:2022": {"bib_key": "Fan:SUNet:2022", "bib_title": "{SUNet: swin} transformer UNet for image denoising", "bib_author ": "Fan, Chi-Mao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "To date, Swin Transformer has been widely adopted in various applications, including image restoration\\cite{Fan:SUNet:2022}", "next_context": ", where it is integrated with UNet."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "ii)\\textbf{Denoising}: SUNet\\cite{Fan:SUNet:2022}", "next_context": "applies Swin transformer blocks combined in a UNet-like architecture."}], "importance_score": 2.0666666666666664}, "Neimark:video:2021": {"bib_key": "Neimark:video:2021", "bib_title": "Video Transformer Network", "bib_author ": "Neimark, Daniel", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "\\cite{Neimark:video:2021}", "next_context": "or dividing the video into space-time blocks\\cite{Bertasius:Is:2021}."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024}", "next_context": ""}], "importance_score": 1.1111111111111112}, "Bertasius:Is:2021": {"bib_key": "Bertasius:Is:2021", "bib_title": "Is Space-Time Attention All You Need for Video Understanding?", "bib_author ": "Gedas Bertasius", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "\\cite{Neimark:video:2021}or dividing the video into space-time blocks\\cite{Bertasius:Is:2021}", "next_context": "."}], "importance_score": 1.0}, "Khan:Transformers:2022": {"bib_key": "Khan:Transformers:2022", "bib_title": "Transformers in Vision: A Survey", "bib_author ": "Khan, Salman", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "Comprehensive surveys on transformers for images and videos can be found in\\cite{Khan:Transformers:2022}", "next_context": "and\\cite{Selva:video:2023}, respectively."}], "importance_score": 1.0}, "Selva:video:2023": {"bib_key": "Selva:video:2023", "bib_title": "Video Transformers: A Survey", "bib_author ": "J. Selva", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "Comprehensive surveys on transformers for images and videos can be found in\\cite{Khan:Transformers:2022}and\\cite{Selva:video:2023}", "next_context": ", respectively."}], "importance_score": 1.0}, "gu2023mamba": {"bib_key": "gu2023mamba", "bib_title": "{Mamba: Linear}-time sequence modeling with selective state spaces", "bib_author ": "Gu, Albert", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "Note that in recent years, emerging technologies have adopted state space models\\cite{gu2023mamba, zhu2024vision}", "next_context": ", commonly known as `Mamba.' These models are recognized as a linear variant of Transformers and are distinguished by their linear complexity in attention modeling."}], "importance_score": 0.5}, "zhu2024vision": {"bib_key": "zhu2024vision", "bib_title": "Vision mamba: Efficient visual representation learning with bidirectional state space model", "bib_author ": "Zhu, Lianghui", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Transformers", "subsubsection": null, "prev_context": "Note that in recent years, emerging technologies have adopted state space models\\cite{gu2023mamba, zhu2024vision}", "next_context": ", commonly known as `Mamba.' These models are recognized as a linear variant of Transformers and are distinguished by their linear complexity in attention modeling."}], "importance_score": 0.5}, "Lester:power:2021": {"bib_key": "Lester:power:2021", "bib_title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "bib_author ": "Lester, Brian", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Large language models", "subsubsection": null, "prev_context": "\\cite{Lester:power:2021}", "next_context": "."}], "importance_score": 1.0}, "Jia:VPT:2022": {"bib_key": "Jia:VPT:2022", "bib_title": "Visual Prompt Tuning", "bib_author ": "Jia, Menglin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Large language models", "subsubsection": null, "prev_context": "This paradigm has however extended to other domains, such as visual prompt tuning\\cite{Jia:VPT:2022}", "next_context": "."}], "importance_score": 1.0}, "zhao:survey:2023": {"bib_key": "zhao:survey:2023", "bib_title": "A Survey of Large Language Models", "bib_author ": "Zhao, Wayne Xin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Large language models", "subsubsection": null, "prev_context": "For a comprehensive survey of LLMs, please refer to\\cite{zhao:survey:2023}", "next_context": "."}, {"section": "Current Advanced AI Technologies", "subsection": "Large language models", "subsubsection": null, "prev_context": "Many publications on surveys and evaluations of LLMs are available\\cite{zhao:survey:2023,Chang:Survey:2024,YAO:Survey:2024}", "next_context": "."}], "importance_score": 1.3333333333333333}, "Chang:Survey:2024": {"bib_key": "Chang:Survey:2024", "bib_title": "A Survey on Evaluation of Large Language Models", "bib_author ": "Chang, Yupeng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Large language models", "subsubsection": null, "prev_context": "Many publications on surveys and evaluations of LLMs are available\\cite{zhao:survey:2023,Chang:Survey:2024,YAO:Survey:2024}", "next_context": "."}], "importance_score": 0.3333333333333333}, "YAO:Survey:2024": {"bib_key": "YAO:Survey:2024", "bib_title": "A Survey on Large Language Model (LLM) Security and Privacy: The Good, The Bad, and The Ugly", "bib_author ": "Yifan Yao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Large language models", "subsubsection": null, "prev_context": "Many publications on surveys and evaluations of LLMs are available\\cite{zhao:survey:2023,Chang:Survey:2024,YAO:Survey:2024}", "next_context": "."}], "importance_score": 0.3333333333333333}, "Ye:FLASK:2024": {"bib_key": "Ye:FLASK:2024", "bib_title": "{FLASK}: Fine-grained Language Model Evaluation based on Alignment Skill Sets", "bib_author ": "Seonghyeon Ye", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Large language models", "subsubsection": null, "prev_context": "\\cite{Ye:FLASK:2024}", "next_context": "evaluating LLMs based on 12 fine-grained skills for comprehensive language model evaluation: logical correctness, logical robustness, logical efficiency, factuality, commonsense understanding, comprehension, insightfulness, completeness, metacognition, conciseness, readability, and harmlessness."}], "importance_score": 1.0}, "Kingma:auto:2014": {"bib_key": "Kingma:auto:2014", "bib_title": "Auto-Encoding Variational Bayes", "bib_author ": "D.P. Kingma", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "\\cite{Kingma:auto:2014}", "next_context": ", learns the latent space as statistic parameters of probabilistic distributions, leading to significant improvement of the generated results."}], "importance_score": 1.0}, "Goodfellow:GAN:2014": {"bib_key": "Goodfellow:GAN:2014", "bib_title": "Generative Adversarial Nets", "bib_author ": "Goodfellow, Ian", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "\\cite{Goodfellow:GAN:2014}", "next_context": "introduced an alternative architecture known as a Generative Adversarial Network (GAN)."}], "importance_score": 1.0}, "Dickstein:Deep:2015": {"bib_key": "Dickstein:Deep:2015", "bib_title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics", "bib_author ": "Sohl-Dickstein, Jascha", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "\\cite{Dickstein:Deep:2015}", "next_context": ", using Nonequilibrium Thermodynamics."}], "importance_score": 1.0}, "Ho:DDPM:2020": {"bib_key": "Ho:DDPM:2020", "bib_title": "Denoising Diffusion Probabilistic Models", "bib_author ": "Jonathan Ho", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "\\cite{Ho:DDPM:2020}", "next_context": "in 2020 and Score-based diffusion models proposed by Song et al."}], "importance_score": 1.0}, "Song:Score:2021": {"bib_key": "Song:Score:2021", "bib_title": "Score-Based Generative Modeling through Stochastic Differential Equations", "bib_author ": "Yang Song", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "\\cite{Song:Score:2021}", "next_context": "in 2021."}], "importance_score": 1.0}, "Dhariwal:Diffusion:2021": {"bib_key": "Dhariwal:Diffusion:2021", "bib_title": "Diffusion Models Beat {GAN}s on Image Synthesis", "bib_author ": "Prafulla Dhariwal", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "Comparing to GANs, the DMs provide higher diversity samples\\cite{Dhariwal:Diffusion:2021}", "next_context": "and the training process is much more stable and do not suffer from mode collapse; however, the DMs are computationally intensive and require longer training times compared to GANs."}, {"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "Classifier guidance was introduced in\\cite{Dhariwal:Diffusion:2021}", "next_context": "to improve a diffusion generator in generating images of a desired class."}], "importance_score": 2.0}, "Rombach:LDM:2022": {"bib_key": "Rombach:LDM:2022", "bib_title": "High-Resolution Image Synthesis with Latent Diffusion Models", "bib_author ": "Rombach, Robin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "Latent Diffusion Models (LDM)\\cite{Rombach:LDM:2022}", "next_context": "use pretrained networks to convert images to feature maps, and perform training on a low-dimensional space."}, {"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "The LDM\\cite{Rombach:LDM:2022}", "next_context": "offers more flexible conditional image generators by adding cross-attention layer (referred to Transformers in Section\\ref{ssec:transformers}) to the denoising autoencoder."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024}", "next_context": ""}], "importance_score": 2.142857142857143}, "Choi:ILVR:2021": {"bib_key": "Choi:ILVR:2021", "bib_title": "{ILVR: Conditioning} Method for Denoising Diffusion Probabilistic Models", "bib_author ": "Choi, Jooyoung", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "The work in\\cite{Choi:ILVR:2021}", "next_context": "simply refines the latent space of well-trained unconditional DDPM so that the higher-level semantics of the synthetic samples are similar to the reference (conditioning)."}], "importance_score": 1.0}, "Cao:survey:2024": {"bib_key": "Cao:survey:2024", "bib_title": "A Survey on Generative Diffusion Models", "bib_author ": "Cao, Hanqun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Diffusion Models", "subsubsection": null, "prev_context": "A survey on the methods and applications of DMs prior to 2024 can be found in\\cite{Cao:survey:2024}", "next_context": "."}], "importance_score": 1.0}, "xie2022neural": {"bib_key": "xie2022neural", "bib_title": "Neural fields in visual computing and beyond", "bib_author ": "Xie, Yiheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "They can be seen as fieldsx(represented by a scalar, vector, or tensor with a value, such as magnetic field in physics) that are fully or partially parameterized by a neural network\\Phi, typically an MLP\\cite{xie2022neural}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "They excel in neural rendering, particularly in view-dependent novel view synthesis, and have effectively tackled several challenges associated with automated 3D capture\\cite{xie2022neural}", "next_context": ", such as accurately representing the reflectance properties of the scene."}], "importance_score": 2.0}, "kwan2024hinerv": {"bib_key": "kwan2024hinerv", "bib_title": "{HiNeRV}: Video Compression with Hierarchical Encoding-based Neural Representation", "bib_author ": "Kwan, Ho Man", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "Hence, one of its applications is data compression\\cite{kwan2024hinerv}", "next_context": "."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "~\\cite{kwan2024hinerv,kim2024c3,leguay2024cool}", "next_context": "have content-specific embedding as inputs."}], "importance_score": 1.4444444444444444}, "sitzmann:siren:2020": {"bib_key": "sitzmann:siren:2020", "bib_title": "Implicit Neural Representations\nwith Periodic Activation Functions", "bib_author ": "Sitzmann, Vincent", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{sitzmann:siren:2020}", "next_context": "demonstrate that using periodic functions, such as sinusoids, is more suitable for representing complex natural signals, offering a better fit to the first- and second-order derivatives of the signals."}], "importance_score": 1.0}, "Saragadam:wire:2023": {"bib_key": "Saragadam:wire:2023", "bib_title": "WIRE: Wavelet Implicit Neural Representations", "bib_author ": "Saragadam, Vishwanath", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "instead proposed using complex Gabor wavelets\\cite{Saragadam:wire:2023}", "next_context": ", which learn to represent high frequencies better and simultaneously are robust to noise."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "INR with complex Gabor wavelets as activation functions show promising denoising results\\cite{Saragadam:wire:2023}", "next_context": "."}], "importance_score": 2.0}, "Mildenhall:NeRF:2020": {"bib_key": "Mildenhall:NeRF:2020", "bib_title": "{NeRF: Representing} Scenes as Neural Radiance Fields for View Synthesis", "bib_author ": "Mildenhall, Ben", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020}", "next_context": ", NeRF is a form of neural rendering, a subset of generative AI, that generates novel views of a scene based on a partial set of 2D images."}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, azzarelli:waveplanes:2023}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "Neural Radiance Fields (NeRFs), introduced in\\cite{Mildenhall:NeRF:2020}", "next_context": ", have demonstrated the ability to learn a 3D scene from a smaller number of images captured from various viewpoints, as opposed to photogrammetry."}], "importance_score": 2.125}, "Wang:SIMVLM:2022": {"bib_key": "Wang:SIMVLM:2022", "bib_title": "{SIMVLM: Simple} Visual Language Model Pretraining with Weak Supervision", "bib_author ": "Wang, Zirui", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "\\cite{Wang:SIMVLM:2022}", "next_context": "."}], "importance_score": 1.25}, "wei2024vary": {"bib_key": "wei2024vary", "bib_title": "{Vary: Scaling} up the Vision Vocabulary for Large Vision-Language Models", "bib_author ": "Wei, Haoran", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "More recent work in\\cite{wei2024vary}", "next_context": "scales up the vision vocabulary by incorporating new image features into the existing CLIP model, resulting in improved content understanding."}], "importance_score": 1.25}, "Alayrac:Flamingo:2022": {"bib_key": "Alayrac:Flamingo:2022", "bib_title": "Flamingo: a Visual Language Model for Few-Shot Learning", "bib_author ": "Alayrac, Jean-Baptiste", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Alayrac:Flamingo:2022, Huang:GenerSpeech:2022}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Alayrac:Flamingo:2022,esser:scaling:2024}", "next_context": ""}], "importance_score": 1.0}, "Huang:GenerSpeech:2022": {"bib_key": "Huang:GenerSpeech:2022", "bib_title": "{GenerSpeech: Towards} Style Transfer for Generalizable Out-Of-Domain Text-to-Speech", "bib_author ": "Huang, Rongjie", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Alayrac:Flamingo:2022, Huang:GenerSpeech:2022}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Audio and Music generation", "prev_context": "Voice style transfer often use zero-shot learning (a model is trained to recognize classes or categories it has never seen during training)\\cite{Huang:GenerSpeech:2022}", "next_context": "or few-shot learning (a model is trained with only one or a few examples per class)\\cite{Wang:One:2022}."}], "importance_score": 1.5}, "Li:Diffusion:2022": {"bib_key": "Li:Diffusion:2022", "bib_title": "Diffusion-LM Improves Controllable Text Generation", "bib_author ": "Li, Xiang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:Diffusion:2022, Yang:Diffsound:2023}", "next_context": ""}], "importance_score": 0.5}, "Yang:Diffsound:2023": {"bib_key": "Yang:Diffsound:2023", "bib_title": "Diffsound: Discrete Diffusion Model for Text-to-Sound Generation", "bib_author ": "Yang, Dongchao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:Diffusion:2022, Yang:Diffsound:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Audio and Music generation", "prev_context": "For example, the framework proposed in\\cite{Yang:Diffsound:2023}", "next_context": "uses a method based on DM with transformer backbone to turn text input into a mel-spectrogram using the vector quantized variational autoencoder (VQ-VAE)\\cite{Oord:Neural:2017}."}], "importance_score": 1.5}, "esser:scaling:2024": {"bib_key": "esser:scaling:2024", "bib_title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "bib_author ": "Esser, Patrick", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Alayrac:Flamingo:2022,esser:scaling:2024}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Image generation", "prev_context": "Released in March 2024, the latest version, Stable Diffusion 3 (SD3)\\cite{esser:scaling:2024}", "next_context": ", reports outperforming state-of-the-art text-to-image generation systems such as DALL\u00b7E 3 (released August 2023), Midjourney v6 (released December 2023), and Ideogram v1 (released February 2024) in terms of typography and prompt adherence, based on human preference evaluations."}], "importance_score": 1.6428571428571428}, "Brooks:InstructPix2Pix:2023": {"bib_key": "Brooks:InstructPix2Pix:2023", "bib_title": "{InstructPix2Pix: Learning} To Follow Image Editing Instructions", "bib_author ": "Brooks, Tim", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Image generation", "prev_context": "Recently, InstructPix2Pix\\cite{Brooks:InstructPix2Pix:2023}", "next_context": "proposed a conditional diffusion model to generate image editing examples without predefined editing areas."}], "importance_score": 1.1428571428571428}, "gal:Image:2023": {"bib_key": "gal:Image:2023", "bib_title": "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion", "bib_author ": "Rinon Gal", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Image generation", "prev_context": "Textual Inversion\\cite{gal:Image:2023}", "next_context": "personalizes large pre-trained text-to-image diffusion models based on specific objects and styles, using 3-5 images of a user-provided concept."}], "importance_score": 1.1428571428571428}, "Gandikota:Unified:2024": {"bib_key": "Gandikota:Unified:2024", "bib_title": "Unified Concept Editing in Diffusion Models", "bib_author ": "G", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024}", "next_context": ""}, {"section": "Future Challenges for AI in the Creative Sector", "subsection": "Ethical Issues, Fakes and Bias", "subsubsection": null, "prev_context": "\\cite{Gandikota:Unified:2024}", "next_context": "has been proposed for image generation in digital mediums."}], "importance_score": 1.1428571428571428}, "Lian:LLMG:2024": {"bib_key": "Lian:LLMG:2024", "bib_title": "{LLM}-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models", "bib_author ": "Long Lian", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Image generation", "prev_context": "Recently, LLM-grounded Diffusion\\cite{Lian:LLMG:2024}", "next_context": "enables instruction-based scene specification with multiple rounds of user requests without a need of manual selection on the image."}], "importance_score": 1.1428571428571428}, "ren:hypersd:2024": {"bib_key": "ren:hypersd:2024", "bib_title": "{Hyper-SD: Trajectory} Segmented Consistency Model for Efficient Image Synthesis", "bib_author ": "Yuxi Ren", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Image generation", "prev_context": "Recently, ByteDance announces Hyper-SD\\cite{ren:hypersd:2024}", "next_context": "proposed trajectory segmented consistency distillation that provides real-time high-resolution image generation from drawing with a control text prompt."}], "importance_score": 1.1428571428571428}, "hong:cogvideo:2023": {"bib_key": "hong:cogvideo:2023", "bib_title": "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers", "bib_author ": "Wenyi Hong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "As an example of a transformer-based approach, CogVideo\\cite{hong:cogvideo:2023}", "next_context": "employs VQ-VAE to convert input frames to tokens, which are then fused with text tokens to produce a new video."}], "importance_score": 1.0909090909090908}, "villegas:phenaki:2023": {"bib_key": "villegas:phenaki:2023", "bib_title": "{Phenaki: Variable} Length Video Generation from Open Domain Textual Descriptions", "bib_author ": "Ruben Villegas", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "\\cite{villegas:phenaki:2023}", "next_context": "exploits transformers to generate variable length videos, but the quality is lower than those based on DMs."}], "importance_score": 1.0909090909090908}, "Azadi:Make:2023": {"bib_key": "Azadi:Make:2023", "bib_title": "{Make-An-Animation: Large}-Scale Text-conditional {3D} Human Motion Generation", "bib_author ": "Azadi, Samaneh", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "Make-An-Animation\\cite{Azadi:Make:2023}", "next_context": "trains on image-text datasets and fine-tunes on motion capture data, adding additional layers to model the temporal dimension."}], "importance_score": 1.0909090909090908}, "Yu:Bidirectionally:2023": {"bib_key": "Yu:Bidirectionally:2023", "bib_title": "Bidirectionally Deformable Motion Modulation For Video-based Human Pose Transfer", "bib_author ": "Yu, Wing-Yin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "\\cite{wang:disco:2024}and Bidirectionally Deformable Motion Modulation (BDMM)\\cite{Yu:Bidirectionally:2023}", "next_context": "."}], "importance_score": 1.0909090909090908}, "Liu:FETV:2023": {"bib_key": "Liu:FETV:2023", "bib_title": "{FETV: A} Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation", "bib_author ": "Liu, Yuanxin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "Evaluations of these methods can be found in\\cite{Liu:FETV:2023}", "next_context": "."}], "importance_score": 1.2337662337662338}, "wang:disco:2024": {"bib_key": "wang:disco:2024", "bib_title": "DisCo: Disentangled Control for Realistic Human Dance Generation", "bib_author ": "Wang, Tan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "\\cite{wang:disco:2024}", "next_context": "and Bidirectionally Deformable Motion Modulation (BDMM)\\cite{Yu:Bidirectionally:2023}."}], "importance_score": 1.0909090909090908}, "xu:VASA-1:2024": {"bib_key": "xu:VASA-1:2024", "bib_title": "{VASA}-1: Lifelike Audio-Driven Talking Faces Generated in Real Time", "bib_author ": "Sicheng Xu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "In April 2024, Microsoft introduced VASA-1\\cite{xu:VASA-1:2024}", "next_context": ", which turns a single static image and a speech audio clip into a video clip of realistic talking faces mimicking human facial expressions and head movements, shown in Fig."}], "importance_score": 1.0909090909090908}, "corona:vlogger:2024": {"bib_key": "corona:vlogger:2024", "bib_title": "VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis", "bib_author ": "Corona, Enric", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "The overall quality of the generated videos is better than VLOGGER by Google\\cite{corona:vlogger:2024}", "next_context": ", which is based on similar technology--diffusion models."}], "importance_score": 1.0909090909090908}, "Gupta:Photorealistic:2024": {"bib_key": "Gupta:Photorealistic:2024", "bib_title": "Photorealistic Video Generation with Diffusion Models", "bib_author ": "Gupta, Agrim", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "The recent work applies spatiotemporal layers to model temporal dynamics\\cite{Gupta:Photorealistic:2024}", "next_context": "."}], "importance_score": 1.2337662337662338}, "Hu_2024_CVPR": {"bib_key": "Hu_2024_CVPR", "bib_title": "{Animate Anyone: Consistent} and Controllable Image-to-Video Synthesis for Character Animation", "bib_author ": "Hu, Li", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "Animate Anyone by Alibaba Group\\cite{Hu_2024_CVPR}", "next_context": "inputs a real photo or anime of a person with a sequence of guided poses."}], "importance_score": 1.0909090909090908}, "Zhu:INFP:2024": {"bib_key": "Zhu:INFP:2024", "bib_title": "{INFP: Audio}-Driven Interactive Head Generation in Dyadic Conversations", "bib_author ": "Yongming Zhu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "Recently, ByteDance introduced an audio-driven interactive head generation\\cite{Zhu:INFP:2024}", "next_context": "that offers listening and speaking states during multi-turn conversations."}], "importance_score": 1.2337662337662338}, "singer:Make:2023": {"bib_key": "singer:Make:2023", "bib_title": "{Make-A-Video: Text}-to-Video Generation without Text-Video Data", "bib_author ": "Uriel Singer", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "Make-A-Video\\cite{singer:Make:2023}", "next_context": ", through a spatiotemporally factorized diffusion model, leverages joint text-image priors and super-resolution in space and time."}], "importance_score": 1.1428571428571428}, "molad:dreamix:2023": {"bib_key": "molad:dreamix:2023", "bib_title": "Dreamix: Video Diffusion Models are General Video Editors", "bib_author ": "Eyal Molad", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "\\cite{molad:dreamix:2023}", "next_context": "videos do not have this issue, but they are very blurry."}], "importance_score": 1.1428571428571428}, "wang:modelscope:2023": {"bib_key": "wang:modelscope:2023", "bib_title": "ModelScope Text-to-Video Technical Report", "bib_author ": "Jiuniu Wang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "The main technologies of text-to-video and image-to-video tasks are based on DMs with a combination of 3D convolutions (or separately spatial and temporal convolutions), and spatial and temporal attention modules\\cite{wang:modelscope:2023}", "next_context": "."}], "importance_score": 1.1428571428571428}, "wu:tune:2023": {"bib_key": "wu:tune:2023", "bib_title": "{Tune-a-video: One-shot} tuning of image diffusion models for text-to-video generation", "bib_author ": "Wu, Jay Zhangjie", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Video generation and animation", "prev_context": "Tune-A-Video\\cite{wu:tune:2023}", "next_context": "modifies the style of an input video using a text prompt."}], "importance_score": 1.1428571428571428}, "yang:Holodeck:2024": {"bib_key": "yang:Holodeck:2024", "bib_title": "{Holodeck: Language} Guided Generation of {3D} Embodied {AI} Environments", "bib_author ": "Yue Yang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{yang:Holodeck:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Augmented, virtual and mixed reality, and 3D content", "prev_context": "\\cite{yang:Holodeck:2024}", "next_context": ", which automatically generates 3D embodied environments via text prompt conversation of a large language model (GPT-4)."}], "importance_score": 2.0}, "Xu:NeuralLift:2023": {"bib_key": "Xu:NeuralLift:2023", "bib_title": "{NeuralLift-360: Lifting} an in-the-Wild 2D Photo to A {3D} Object with 360\u00b0 Views", "bib_author ": "Xu, Dejia", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Augmented, virtual and mixed reality, and 3D content", "prev_context": "NeuralLift-360\\cite{Xu:NeuralLift:2023}", "next_context": "also uses diffusion models to generate prior for novel view synthesis."}], "importance_score": 1.25}, "Melas:RealFusion:2023": {"bib_key": "Melas:RealFusion:2023", "bib_title": "RealFusion 360$^\\circ$ Reconstruction of Any Object from a Single Image", "bib_author ": "Melas-Kyriazi, Luke", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Augmented, virtual and mixed reality, and 3D content", "prev_context": "RealFusion\\cite{Melas:RealFusion:2023}", "next_context": ", a single image to 3D object generator, merges 2D diffusion models with NeRF, improving Instant-NGP\\cite{mueller:instant:2022}, which provide API for VR controls."}], "importance_score": 1.25}, "Qian:Magic123:2024": {"bib_key": "Qian:Magic123:2024", "bib_title": "{Magic123: One} Image to High-Quality {3D} Object Generation Using Both 2D and {3D} Diffusion Priors", "bib_author ": "Qian, Guocheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Augmented, virtual and mixed reality, and 3D content", "prev_context": "Magic123\\cite{Qian:Magic123:2024}", "next_context": "is the latest image-to-3D tool that use  2D and 3D priors simultaneously  to produce high-quality high-resolution 3D geometry and textures."}], "importance_score": 1.25}, "tang:dreamgaussian:2024": {"bib_key": "tang:dreamgaussian:2024", "bib_title": "DreamGaussian: Generative Gaussian Splatting for Efficient {3D} Content Creation", "bib_author ": "Jiaxiang Tang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Augmented, virtual and mixed reality, and 3D content", "prev_context": "DreamGaussian\\cite{tang:dreamgaussian:2024}", "next_context": "offers text-to-3D and image-to-3D by adapting 3D Gaussian splatting (more in Section\\ref{sssec:3DGS}) into generative settings using diffusion prior."}], "importance_score": 1.5833333333333333}, "ren:dreamgaussian4d:2023": {"bib_key": "ren:dreamgaussian4d:2023", "bib_title": "{DreamGaussian4D: Generative} 4D Gaussian Splatting", "bib_author ": "Ren, Jiawei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Augmented, virtual and mixed reality, and 3D content", "prev_context": "DreamGaussian4D\\cite{ren:dreamgaussian4d:2023}", "next_context": "employs image-to-video diffusion and a 4D Gaussian Splatting representation to generate an image-to-4D model."}], "importance_score": 1.3333333333333333}, "zhao2024clear": {"bib_key": "zhao2024clear", "bib_title": "{CleAR}: Robust Context-Guided Generative Lighting Estimation for Mobile Augmented Reality", "bib_author ": "Zhao, Yiqin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Augmented, virtual and mixed reality, and 3D content", "prev_context": "There has also been an attempt to use autoregressive and generative models to estimate lighting, achieving a visually coherent environment between virtual and physical spaces in AR\\cite{zhao2024clear}", "next_context": "."}], "importance_score": 1.3333333333333333}, "sun:text:2023": {"bib_key": "sun:text:2023", "bib_title": "Text Classification via Large Language Models", "bib_author ": "Xiaofei Sun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{sun:text:2023, shi:chatgraph:2023, Hou:promptboosting:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Text categorization", "prev_context": "\\cite{sun:text:2023}", "next_context": "applies kNN to integrate diagnostic reasoning process for final decision."}], "importance_score": 1.3333333333333333}, "Hou:promptboosting:2023": {"bib_key": "Hou:promptboosting:2023", "bib_title": "{P}rompt{B}oosting: Black-Box Text Classification with Ten Forward Passes", "bib_author ": "Hou, Bairu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{sun:text:2023, shi:chatgraph:2023, Hou:promptboosting:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Text categorization", "prev_context": "Multiple learners are also used to enhance the performances\\cite{Hou:promptboosting:2023}", "next_context": "."}], "importance_score": 1.3333333333333333}, "Mao:Biases:2023": {"bib_key": "Mao:Biases:2023", "bib_title": "The Biases of Pre-Trained Language Models: An Empirical Study on Prompt-Based Sentiment Analysis and Emotion Detection", "bib_author ": "Mao, Rui", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Advertisements and film analysis", "prev_context": "Technically, LLMs learn complex patterns and relationships in text data for sentiment classification\\cite{Mao:Biases:2023, krugmann:sentiment:2024}", "next_context": "."}], "importance_score": 0.8333333333333333}, "krugmann:sentiment:2024": {"bib_key": "krugmann:sentiment:2024", "bib_title": "Sentiment Analysis in the Age of Generative {AI}", "bib_author ": "Krugmann, J. O.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Advertisements and film analysis", "prev_context": "Technically, LLMs learn complex patterns and relationships in text data for sentiment classification\\cite{Mao:Biases:2023, krugmann:sentiment:2024}", "next_context": "."}], "importance_score": 0.8333333333333333}, "Hartmann:More:2023": {"bib_key": "Hartmann:More:2023", "bib_title": "More than a Feeling: Accuracy and Application of Sentiment Analysis", "bib_author ": "Jochen Hartmann", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Advertisements and film analysis", "prev_context": "SiEBERT\\cite{Hartmann:More:2023}", "next_context": "provides pre-trained model with open-source scripts to be fine-tuned to further improve accuracy for novel applications."}], "importance_score": 1.3333333333333333}, "Metzler:Rethinking:2021": {"bib_key": "Metzler:Rethinking:2021", "bib_title": "Rethinking search: making domain experts out of dilettantes", "bib_author ": "Metzler, Donald", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Content retrieval and recommendation services", "prev_context": "\\cite{Metzler:Rethinking:2021}", "next_context": "."}], "importance_score": 1.1666666666666667}, "Yan:Universal:2023": {"bib_key": "Yan:Universal:2023", "bib_title": "Universal Instance Perception As Object Discovery and Retrieval", "bib_author ": "Yan, Bin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": null, "prev_context": "\\cite{Yan:Universal:2023}", "next_context": "have categorized information extraction tasks based on the Format-Time-Reference space, as illustrated in Fig."}], "importance_score": 1.1666666666666667}, "Lu:content:2023": {"bib_key": "Lu:content:2023", "bib_title": "Content-based Search for Deep Generative Models", "bib_author ": "Lu, Daohan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Content retrieval and recommendation services", "prev_context": "When retrieving visual content, recent work exploits generative models to enhance content-based model search\\cite{Lu:content:2023}", "next_context": "."}], "importance_score": 1.1666666666666667}, "Rajput:recommender:2023": {"bib_key": "Rajput:recommender:2023", "bib_title": "Recommender Systems with Generative Retrieval", "bib_author ": "Rajput, Shashank", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Content retrieval and recommendation services", "prev_context": "\\cite{Rajput:recommender:2023}", "next_context": ", which utilizes GR."}], "importance_score": 1.1666666666666667}, "li:unigen:2024": {"bib_key": "li:unigen:2024", "bib_title": "{UniGen: A} Unified Generative Framework for Retrieval and Question Answering with Large Language Models", "bib_author ": "Li, X", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Content retrieval and recommendation services", "prev_context": "Conversational question answering techniques have been integrated to enhance the document retrieval\\cite{li:unigen:2024}", "next_context": "."}], "importance_score": 1.1666666666666667}, "li2024learning": {"bib_key": "li2024learning", "bib_title": "Learning to Rank in Generative Retrieval", "bib_author ": "Li, Yongqi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Content retrieval and recommendation services", "prev_context": "\\cite{li2024learning}", "next_context": "introduces learning-to-rank training to enhance the performance system up typ 30\\%."}], "importance_score": 1.1666666666666667}, "Jin:DiffusionRet:2023": {"bib_key": "Jin:DiffusionRet:2023", "bib_title": "{DiffusionRet: Generative} Text-Video Retrieval with Diffusion Model", "bib_author ": "Jin, Peng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Jin:DiffusionRet:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Content retrieval and recommendation services", "prev_context": "DMs are also employed for visual retrieval tasks, where they learn joint data distributions between text queries and video candidates\\cite{Jin:DiffusionRet:2023}", "next_context": "."}], "importance_score": 2.0}, "King:Sasha:2024": {"bib_key": "King:Sasha:2024", "bib_title": "{Sasha: Creative} Goal-Oriented Reasoning in Smart Homes with Large Language Models", "bib_author ": "King, Evan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{King:Sasha:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Intelligent assistants", "prev_context": "Current LLMs obviously enhance the performance of intelligent assistants, designed to understand complex inquiries and generate more natural conversational responses, such as Sasha\\cite{King:Sasha:2024}", "next_context": "."}], "importance_score": 2.0}, "Xu:SNR:2022": {"bib_key": "Xu:SNR:2022", "bib_title": "SNR-Aware Low-light Image Enhancement", "bib_author ": "Xu, Xiaogang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "Meanwhile, SNR-Aware\\cite{Xu:SNR:2022}", "next_context": "estimates spatial-varying Signal-to-Noise Ratio (SNR) maps and proposes local and global learning branches using ResNet and transformer architectures, respectively."}], "importance_score": 1.1666666666666667}, "liang:RVRT:2022": {"bib_key": "liang:RVRT:2022", "bib_title": "Recurrent video restoration transformer with guided deformable attention", "bib_author ": "Liang, J.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "\\cite{Liang:VRT:2024}and its improved version with recurrent process (RVRT)\\cite{liang:RVRT:2022}", "next_context": ", have emerged as the state of the arts for video super-resolution, deblurring, denoising, and frame interpolation."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "The deblurring performance is comparable to RVRT\\cite{liang:RVRT:2022}", "next_context": ", but it is 5 times faster."}], "importance_score": 2.166666666666667}, "Wang:Ultra:2023": {"bib_key": "Wang:Ultra:2023", "bib_title": "Ultra-high-definition low-light image enhancement: A benchmark and transformer-based method", "bib_author ": "Wang, Tao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "LLFormer\\cite{Wang:Ultra:2023}", "next_context": ", and significantly better than  INR-based method, NeRCo\\cite{Yang:Implicit:2023}, on a real low-light image benchmarking dataset."}], "importance_score": 1.1666666666666667}, "Lin:SPATIO:2024": {"bib_key": "Lin:SPATIO:2024", "bib_title": "A SPATIO-TEMPORAL ALIGNED SUNET MODEL FOR LOW-LIGHT VIDEO ENHANCEMENT", "bib_author ": "Lin, Ruirui", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "STA-SUNet\\cite{Lin:SPATIO:2024}", "next_context": "has demonstrated that using transformers for low-light video enhancement outperforms CNN-based methods\\cite{anantrasirichai:BVI:2024}."}], "importance_score": 1.1666666666666667}, "Youk:FMA:2024": {"bib_key": "Youk:FMA:2024", "bib_title": "{FMA-Net: Flow}-Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring", "bib_author ": "Geunhyuk Youk", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "FMA-Net\\cite{Youk:FMA:2024}", "next_context": "proposed multi-attention for joint Video super-resolution and deblurring, achieving fast runtime with nearly 40\\%improvement over RVRT, and the restored quality was reported better by up to 3\\%."}], "importance_score": 1.1666666666666667}, "Liang:VRT:2024": {"bib_key": "Liang:VRT:2024", "bib_title": "VRT: A Video Restoration Transformer", "bib_author ": "Liang, Jingyun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "\\cite{Liang:VRT:2024}", "next_context": "and its improved version with recurrent process (RVRT)\\cite{liang:RVRT:2022}, have emerged as the state of the arts for video super-resolution, deblurring, denoising, and frame interpolation."}], "importance_score": 1.3583333333333334}, "HOU:Global:2023": {"bib_key": "HOU:Global:2023", "bib_title": "Global Structure-Aware Diffusion Process for Low-light Image Enhancement", "bib_author ": "HOU, Jinhui", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "Diffusion models (DMs) have also become popular choices for low-light image enhancement\\cite{HOU:Global:2023,Yi:Diff:2023,Jiang:Low:2023}", "next_context": "."}], "importance_score": 0.5833333333333333}, "Yi:Diff:2023": {"bib_key": "Yi:Diff:2023", "bib_title": "Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative Diffusion Model", "bib_author ": "Yi, Xunpeng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "Diffusion models (DMs) have also become popular choices for low-light image enhancement\\cite{HOU:Global:2023,Yi:Diff:2023,Jiang:Low:2023}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "Diff-Retinex\\cite{Yi:Diff:2023}", "next_context": "formulates the low-light image enhancement problem into Retinex decomposition, and employs multi-path generative diffusion networks to reconstruct the normal-light Retinex probability distribution."}], "importance_score": 1.5833333333333333}, "Jiang:Low:2023": {"bib_key": "Jiang:Low:2023", "bib_title": "Low-light image enhancement with wavelet-based diffusion models", "bib_author ": "Jiang, Hai", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "Diffusion models (DMs) have also become popular choices for low-light image enhancement\\cite{HOU:Global:2023,Yi:Diff:2023,Jiang:Low:2023}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "A recent state-of-the-art approach presented in\\cite{Jiang:Low:2023}", "next_context": "decomposes images into high and low frequencies using wavelet transform."}], "importance_score": 1.7833333333333332}, "lin2024lowlight": {"bib_key": "lin2024lowlight", "bib_title": "Low-light Video Enhancement with Conditional Diffusion Models and Wavelet Interscale Attentions", "bib_author ": "Lin, R.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "The technique has been extended for video enhancement in\\cite{lin2024lowlight}", "next_context": "."}], "importance_score": 1.25}, "Yang:Implicit:2023": {"bib_key": "Yang:Implicit:2023", "bib_title": "Implicit Neural Representation for Cooperative Low-light Image Enhancement", "bib_author ": "Yang, Shuzhou", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Yang:Implicit:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "NeRCo\\cite{Yang:Implicit:2023}", "next_context": "address low-light problem with INR, which unifies the diverse degradation factors of real-world scenes with a controllable fitting function."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "LLFormer\\cite{Wang:Ultra:2023}, and significantly better than  INR-based method, NeRCo\\cite{Yang:Implicit:2023}", "next_context": ", on a real low-light image benchmarking dataset."}], "importance_score": 3.0}, "Deng:StyTr2:2022": {"bib_key": "Deng:StyTr2:2022", "bib_title": "{StyTr2: Image} Style Transfer With Transformers", "bib_author ": "Deng, Yingying", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Style transfer", "prev_context": "\\cite{Deng:StyTr2:2022}", "next_context": "is the first transformer-based method for style transfer, applying content as a query and style as a key of attention."}], "importance_score": 1.3333333333333333}, "Moon:generalizable:2023": {"bib_key": "Moon:generalizable:2023", "bib_title": "Generalizable Style Transfer for Implicit Neural Representation", "bib_author ": "Jaeho Moon", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Moon:generalizable:2023,Kim:Controllable:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Style transfer", "prev_context": "Implicit Neural Representation (INR) is combined with vision transformers in\\cite{Moon:generalizable:2023}", "next_context": "for generalizable style transfer; however, the results are not very impressive."}], "importance_score": 1.8333333333333333}, "Chung_2024_CVPR": {"bib_key": "Chung_2024_CVPR", "bib_title": "Style Injection in Diffusion: A Training-free Approach for Adapting Large-scale Diffusion Models for Style Transfer", "bib_author ": "Chung, Jiwoo", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Style transfer", "prev_context": "\\cite{Chung_2024_CVPR}", "next_context": ", where the style is injected to manipulate the self-attention of the decoder."}], "importance_score": 1.3333333333333333}, "Zhang:Inversion:2023": {"bib_key": "Zhang:Inversion:2023", "bib_title": "Inversion-Based Style Transfer With Diffusion Models", "bib_author ": "Zhang, Yuxin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Zhang:Inversion:2023,Chai:StableVideo:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Style transfer", "prev_context": "InST\\cite{Zhang:Inversion:2023}", "next_context": "utilizes Stable Diffusion Models as the generative backbone and introduces an attention-based textual inversion module to learn the description of the content."}], "importance_score": 1.5}, "Chai:StableVideo:2023": {"bib_key": "Chai:StableVideo:2023", "bib_title": "{StableVideo: Text}-driven Consistency-aware Diffusion Video Editing", "bib_author ": "Chai, Wenhao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Zhang:Inversion:2023,Chai:StableVideo:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Style transfer", "prev_context": "StableVideo\\cite{Chai:StableVideo:2023}", "next_context": "uses a text prompt to describe the desired appearance of the output, transforming the input video to have a new look based on a diffusion model."}], "importance_score": 1.5}, "Kim:Controllable:2024": {"bib_key": "Kim:Controllable:2024", "bib_title": "Controllable Style Transfer via Test-time Training of Implicit Neural Representation", "bib_author ": "Sunwoo Kim", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Moon:generalizable:2023,Kim:Controllable:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Style transfer", "prev_context": "The method proposed in\\cite{Kim:Controllable:2024}", "next_context": "employs MLPs to map image coordinates to colors of the new style image, controlled by features from the content input and style input."}], "importance_score": 1.5}, "Liang:SwinIR:2021": {"bib_key": "Liang:SwinIR:2021", "bib_title": "SwinIR: Image Restoration Using Swin Transformer", "bib_author ": "Liang, Jingyun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "\\cite{Liang:SwinIR:2021}", "next_context": ", employs several concatenated Swin Transformer blocks\\cite{Liu:Swin:2021}."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "Denoising with diffusion models (DMs)\\cite{yang:realworld:2023}has been proposed by diffusing with estimated noise that is closer to real-world noise rather than Gaussian noise, achieving better performance than SwinIR\\cite{Liang:SwinIR:2021}", "next_context": "and Uformer\\cite{Wang:Uformer:2022}."}], "importance_score": 2.1916666666666664}, "Lu:Transformer:2022": {"bib_key": "Lu:Transformer:2022", "bib_title": "Transformer for Single Image Super-Resolution", "bib_author ": "Lu, Zhisheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "This was done in the feature domain extracted by a lightweight CNN module\\cite{Lu:Transformer:2022}", "next_context": ", outperforming those that use only CNNs."}], "importance_score": 1.125}, "Liu:Learning:2022": {"bib_key": "Liu:Learning:2022", "bib_title": "Learning Trajectory-Aware Transformer for Video Super-Resolution", "bib_author ": "Liu, Chengxu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "\\cite{Liu:Learning:2022}", "next_context": "."}], "importance_score": 1.125}, "Chen:Activating:2023": {"bib_key": "Chen:Activating:2023", "bib_title": "Activating More Pixels in Image Super-Resolution Transformer", "bib_author ": "Chen, Xiangyu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "The Hybrid Attention Transformer (HAT)\\cite{Chen:Activating:2023}", "next_context": "was introduced, which improves the SR quality over ESRT by more than 2dB when upscaling 2\\times-4\\times."}], "importance_score": 1.125}, "li:GRL:2023": {"bib_key": "li:GRL:2023", "bib_title": "Efficient and Explicit Modelling of Image Hierarchies for Image Restoration", "bib_author ": "Yawei Li", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "\\cite{li:GRL:2023}", "next_context": "exploits a hierarchy of features in a global, regional, and local range using different ways to compute self-attentions as an image often show similarity within itself in different scales and areas."}], "importance_score": 1.1916666666666667}, "kang:gigagan:2023": {"bib_key": "kang:gigagan:2023", "bib_title": "Scaling up GANs for Text-to-Image Synthesis", "bib_author ": "Kang, Minguk", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "This is achieved by adding flow estimation and temporal self-attention to the GigaGAN upsampler\\cite{kang:gigagan:2023}", "next_context": ", which is primarily used for image SR, and text-to-image synthesis."}], "importance_score": 1.125}, "xu:videogigagan:2024": {"bib_key": "xu:videogigagan:2024", "bib_title": "{VideoGigaGAN: Towards} Detail-rich Video Super-Resolution", "bib_author ": "Xu, Yiran", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "Recently, Adobe announced their VideoGigaGAN\\cite{xu:videogigagan:2024}", "next_context": ", which can perform 8\\timesupsampling."}], "importance_score": 1.125}, "Saharia:image:2023": {"bib_key": "Saharia:image:2023", "bib_title": "Image Super-Resolution via Iterative Refinement", "bib_author ": "Saharia, Chitwan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "For DMs, SR3 by Google\\cite{Saharia:image:2023}", "next_context": "has produced truly impressive results."}], "importance_score": 1.5333333333333332}, "Moliner:solving:2023": {"bib_key": "Moliner:solving:2023", "bib_title": "Solving Audio Inverse Problems with a Diffusion Model", "bib_author ": "Moliner, Eloi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Moliner:solving:2023, Fei:Generative:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "\\cite{Moliner:solving:2023}", "next_context": "tackle problems such as audio bandwidth extension, inpainting, and declipping by treating them as inverse problems using a diffusion model."}], "importance_score": 1.8333333333333333}, "Gao:Implicit:2023": {"bib_key": "Gao:Implicit:2023", "bib_title": "Implicit Diffusion Models for Continuous Super-Resolution", "bib_author ": "Gao, Sicheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "Later, IDM\\cite{Gao:Implicit:2023}", "next_context": "combines INR with a U-Net denoising model in the reverse process of the DM."}], "importance_score": 1.5333333333333332}, "Chen:Learning:2021": {"bib_key": "Chen:Learning:2021", "bib_title": "Learning Continuous Image Representation with Local Implicit Image Function", "bib_author ": "Chen, Yinbo", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "The later case exploits INR to upsample a continuous coordinate-based flow map, similar to SISR technique proposed in\\cite{Chen:Learning:2021}", "next_context": "."}], "importance_score": 1.2}, "Fei:Generative:2023": {"bib_key": "Fei:Generative:2023", "bib_title": "Generative Diffusion Prior for Unified Image Restoration and Enhancement", "bib_author ": "Fei, Ben", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Moliner:solving:2023, Fei:Generative:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "introduced the Generative Diffusion Prior\\cite{Fei:Generative:2023}", "next_context": "for unsupervised learning, aiming to model posterior distributions for image restoration and enhancement."}], "importance_score": 1.9}, "Yin:CLE:2023": {"bib_key": "Yin:CLE:2023", "bib_title": "{CLE Diffusion: Controllable} Light Enhancement Diffusion Model", "bib_author ": "Yin, Yuyang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "CLE Diffusion\\cite{Yin:CLE:2023}", "next_context": "offers lighting that is editable with user-friendly region controllability."}], "importance_score": 1.2}, "Wang:Uformer:2022": {"bib_key": "Wang:Uformer:2022", "bib_title": "Uformer: A General U-Shaped Transformer for Image Restoration", "bib_author ": "Wang, Zhendong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "Other two popular approaches that emerged in the same timeframe are Uformer\\cite{Wang:Uformer:2022}", "next_context": "and Restormer\\cite{Zamir:Restormer:2022}."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "Denoising with diffusion models (DMs)\\cite{yang:realworld:2023}has been proposed by diffusing with estimated noise that is closer to real-world noise rather than Gaussian noise, achieving better performance than SwinIR\\cite{Liang:SwinIR:2021}and Uformer\\cite{Wang:Uformer:2022}", "next_context": "."}], "importance_score": 2.0666666666666664}, "Zamir:Restormer:2022": {"bib_key": "Zamir:Restormer:2022", "bib_title": "Restormer: Efficient Transformer for High-Resolution Image Restoration", "bib_author ": "Zamir, Syed Waqas", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "Other two popular approaches that emerged in the same timeframe are Uformer\\cite{Wang:Uformer:2022}and Restormer\\cite{Zamir:Restormer:2022}", "next_context": "."}], "importance_score": 1.0666666666666667}, "yang:ldp:2023": {"bib_key": "yang:ldp:2023", "bib_title": "{LDP}: Language-driven Dual-Pixel Image Defocus Deblurring Network", "bib_author ": "Hao Yang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "\\cite{yang:ldp:2023}", "next_context": "."}], "importance_score": 1.0666666666666667}, "Morris:DaBiT:2024": {"bib_key": "Morris:DaBiT:2024", "bib_title": "{DaBiT: Depth} and Blur informed Transformer for Video Deblurring", "bib_author ": "C Morris", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "DaBiT\\cite{Morris:DaBiT:2024}", "next_context": "mitigates focal blur content with depth information and applies SR for further enhancing fine details."}], "importance_score": 1.0666666666666667}, "Yu:DBT:2022": {"bib_key": "Yu:DBT:2022", "bib_title": "DBT-Net: Dual-Branch Federative Magnitude and Phase Estimation With Attention-in-Attention Transformer for Monaural Speech Enhancement", "bib_author ": "Yu, Guochen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "There have been efforts to utilize AI for learning global contextual information to aid in the removal of unwanted sounds, leading to better final quality\\cite{Yu:DBT:2022}", "next_context": "."}], "importance_score": 1.0666666666666667}, "Song:vision:2023": {"bib_key": "Song:vision:2023", "bib_title": "Vision Transformers for Single Image Dehazing", "bib_author ": "Song, Yuda", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "iii)\\textbf{Dehazing}: Vision transformers for single image dehazing was proposed in DehazeFormer\\cite{Song:vision:2023}", "next_context": "."}], "importance_score": 1.0666666666666667}, "Xu:Video:2023": {"bib_key": "Xu:Video:2023", "bib_title": "Video Dehazing via a Multi-Range Temporal Alignment Network With Physical Prior", "bib_author ": "Xu, Jiaqi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "\\cite{Xu:Video:2023}", "next_context": "introduced a recurrent multi-range scene radiance recovery module with the space-time deformable attention."}], "importance_score": 1.0666666666666667}, "mao:single:2022": {"bib_key": "mao:single:2022", "bib_title": "Single Frame Atmospheric Turbulence Mitigation: A Benchmark Study and a New Physics-Inspired Transformer Model", "bib_author ": "Mao, Zhiyuan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb}", "next_context": ""}], "importance_score": 0.06666666666666667}, "Zhang:Image:2024": {"bib_key": "Zhang:Image:2024", "bib_title": "Imaging Through the Atmosphere Using Turbulence Mitigation Transformer", "bib_author ": "Zhang, Xingguang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "However, diffusion models outperform on a single image\\cite{Nair:AT-DDPM:2023}, and transformer-based methods remain state-of-the-art for restoring videos\\cite{Zhang:Image:2024, zou2024deturb}", "next_context": "."}], "importance_score": 0.5666666666666667}, "zou2024deturb": {"bib_key": "zou2024deturb", "bib_title": "{DeTurb: Atmospheric} Turbulence Mitigation with Deformable 3D Convolutions and 3D Swin Transformers", "bib_author ": "Zou, Z.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "However, diffusion models outperform on a single image\\cite{Nair:AT-DDPM:2023}, and transformer-based methods remain state-of-the-art for restoring videos\\cite{Zhang:Image:2024, zou2024deturb}", "next_context": "."}], "importance_score": 0.5666666666666667}, "yang:realworld:2023": {"bib_key": "yang:realworld:2023", "bib_title": "Real-World Denoising via Diffusion Model", "bib_author ": "Yang, Cheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "Denoising with diffusion models (DMs)\\cite{yang:realworld:2023}", "next_context": "has been proposed by diffusing with estimated noise that is closer to real-world noise rather than Gaussian noise, achieving better performance than SwinIR\\cite{Liang:SwinIR:2021}and Uformer\\cite{Wang:Uformer:2022}."}], "importance_score": 1.2}, "Nair:AT-DDPM:2023": {"bib_key": "Nair:AT-DDPM:2023", "bib_title": "{AT-DDPM: Restoring} Faces Degraded by Atmospheric Turbulence Using Denoising Diffusion Probabilistic Models", "bib_author ": "Nair, Nithin Gopalakrishnan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "However, diffusion models outperform on a single image\\cite{Nair:AT-DDPM:2023}", "next_context": ", and transformer-based methods remain state-of-the-art for restoring videos\\cite{Zhang:Image:2024, zou2024deturb}."}], "importance_score": 1.2}, "Jaiswal:Physics:2023": {"bib_key": "Jaiswal:Physics:2023", "bib_title": "Physics-Driven Turbulence Image Restoration with Stochastic Refinement", "bib_author ": "Jaiswal, Ajay", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "iv)\\textbf{Mitigating atmospheric turbulence}: Similar to dehazing, physics-inspired models have been widely developed to remove turbulence distortion\\cite{Jaiswal:Physics:2023,Jiang:NeRT:2023}", "next_context": ", while complex-valued CNNs have been proposed to exploit phase information\\cite{Atmospheric:2023}."}], "importance_score": 0.7}, "Jiang:NeRT:2023": {"bib_key": "Jiang:NeRT:2023", "bib_title": "{NeRT: Implicit} Neural Representations for Unsupervised Atmospheric Turbulence Mitigation", "bib_author ": "Jiang, Weiyun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Jiang:NeRT:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "iv)\\textbf{Mitigating atmospheric turbulence}: Similar to dehazing, physics-inspired models have been widely developed to remove turbulence distortion\\cite{Jaiswal:Physics:2023,Jiang:NeRT:2023}", "next_context": ", while complex-valued CNNs have been proposed to exploit phase information\\cite{Atmospheric:2023}."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "There was also an attempt to use instance normalization (INR) to solve this problem, offering tile and blur correction\\cite{Jiang:NeRT:2023}", "next_context": "."}], "importance_score": 2.5}, "Li:MAT:2022": {"bib_key": "Li:MAT:2022", "bib_title": "{MAT: Mask}-Aware Transformer for Large Hole Image Inpainting", "bib_author ": "Li, Wenbo", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Inpainting", "prev_context": "\\cite{Li:MAT:2022}", "next_context": "offers several outputs to fill a large missing area, consisting of a convolutional head, a transformer body, and a convolutional tail for reconstruction, along with a Conv-U-Net for refinement."}], "importance_score": 1.2}, "Liu:Reduce:2022": {"bib_key": "Liu:Reduce:2022", "bib_title": "Reduce Information Loss in Transformers for Pluralistic Image Inpainting", "bib_author ": "Liu, Qiankun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Inpainting", "prev_context": "PUT\\cite{Liu:Reduce:2022}", "next_context": "proposes a patch-based vector VQ-VAE and unquantized Transformer to minimize information loss."}], "importance_score": 1.2}, "Ren:DLFormer:2022": {"bib_key": "Ren:DLFormer:2022", "bib_title": "{DLFormer: Discrete} Latent Transformer for Video Inpainting", "bib_author ": "Ren, Jingjing", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Inpainting", "prev_context": "\\cite{Ren:DLFormer:2022}", "next_context": "and  ProPainter\\cite{Zhou:ProPainter:2023}."}], "importance_score": 1.2}, "Zhou:ProPainter:2023": {"bib_key": "Zhou:ProPainter:2023", "bib_title": "{ProPainter: Improving} Propagation and Transformer for Video Inpainting", "bib_author ": "Zhou, Shangchen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Inpainting", "prev_context": "\\cite{Ren:DLFormer:2022}and  ProPainter\\cite{Zhou:ProPainter:2023}", "next_context": "."}], "importance_score": 1.2}, "HUANG:Sparse:2024": {"bib_key": "HUANG:Sparse:2024", "bib_title": "Sparse self-attention transformer for image inpainting", "bib_author ": "Wenli Huang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Inpainting", "prev_context": "Spa-former\\cite{HUANG:Sparse:2024}", "next_context": "employs a UNet-like architecture, where each level performs transformer with sparse self-attention to remove coefficients with low or no correlation, leading to memory reduction, while improving result quality by up to 5\\%compared to PUT."}], "importance_score": 1.2}, "Ma:SwinFusion:2022": {"bib_key": "Ma:SwinFusion:2022", "bib_title": "{SwinFusion: Cross-domain} Long-range Learning for General Image Fusion via Swin Transformer", "bib_author ": "Ma, Jiayi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Image Fusion", "prev_context": "Most methods use CNNs for feature extraction, with transformers operating in the latent space\\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Image Fusion", "prev_context": "\\cite{Ma:SwinFusion:2022}", "next_context": ", which utilizes a self-attention-based intra-domain fusion unit and a cross-attention-based inter-domain fusion unit to achieve multi-modal and digital photography image fusion."}], "importance_score": 1.8333333333333333}, "Rao:TGFuse:2023": {"bib_key": "Rao:TGFuse:2023", "bib_title": "{TGFuse: An} Infrared and Visible Image Fusion Approach Based on Transformer and Generative Adversarial Network", "bib_author ": "Rao, Dongyu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Image Fusion", "prev_context": "Most methods use CNNs for feature extraction, with transformers operating in the latent space\\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023}", "next_context": "."}], "importance_score": 0.8333333333333333}, "Liu:Multi:2023": {"bib_key": "Liu:Multi:2023", "bib_title": "Multi-interactive Feature Learning and a Full-time Multi-modality Benchmark for Image Fusion and Segmentation", "bib_author ": "Liu, Jinyuan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Image Fusion", "prev_context": "Transformer-based image fusion has also been applied to downstream tasks like segmentation\\cite{Liu:Multi:2023}", "next_context": ", achieving superior results by leveraging the additional information."}], "importance_score": 1.3333333333333333}, "Zhao:DDFM:2023": {"bib_key": "Zhao:DDFM:2023", "bib_title": "{DDFM: Denoising} Diffusion Model for Multi-Modality Image Fusion", "bib_author ": "Zhao, Zixiang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Zhao:DDFM:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Image Fusion", "prev_context": "\\cite{Zhao:DDFM:2023}", "next_context": "."}], "importance_score": 2.0}, "Shi:Motion-I2V:2024": {"bib_key": "Shi:Motion-I2V:2024", "bib_title": "{Motion-I2V: Consistent} and Controllable Image-to-Video Generation with Explicit Motion Modeling", "bib_author ": "Shi, Xiaoyu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Shi:Motion-I2V:2024}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Shi:Motion-I2V:2024,guo2024liveportrait}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Editing and Visual Special Effects (VFX)", "prev_context": "Motion-I2V\\cite{Shi:Motion-I2V:2024}", "next_context": "provides motion blur and motion drag tools to control specific areas of an image to add motion."}], "importance_score": 2.5}, "guo2024liveportrait": {"bib_key": "guo2024liveportrait", "bib_title": "{LivePortrait: Efficient} Portrait Animation with Stitching and Retargeting Control", "bib_author ": "Guo, Jianzhu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Shi:Motion-I2V:2024,guo2024liveportrait}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Editing and Visual Special Effects (VFX)", "prev_context": "Editing or modifying specific areas of an image is much easier with DM technologies, particularly for headshot photos, such as targeting the eyes and mouth on the face\\cite{guo2024liveportrait}", "next_context": "."}], "importance_score": 1.5}, "Bowen:Marked:2022": {"bib_key": "Bowen:Marked:2022", "bib_title": "Masked-attention Mask Transformer for Universal Image Segmentation", "bib_author ": "Bowen Cheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "\\cite{Bowen:Marked:2022}", "next_context": "."}], "importance_score": 1.125}, "Kirillov:SAM:2023": {"bib_key": "Kirillov:SAM:2023", "bib_title": "Segment Anything", "bib_author ": "Kirillov, Alex", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "Among them, Segment Anything (SAM) by Meta AI\\cite{Kirillov:SAM:2023}", "next_context": "stands out as a pioneer in promptable segmentation approaches."}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "TAM combines SAM\\cite{Kirillov:SAM:2023}", "next_context": "and XMem\\cite{cheng:xmem:2022}, offering tracking and segmentation performance on the human-selected target."}], "importance_score": 2.125}, "Ke:SAM-HQ:2023": {"bib_key": "Ke:SAM-HQ:2023", "bib_title": "Segment Anything in High Quality", "bib_author ": "Ke, Lei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "HQ-SAM\\cite{Ke:SAM-HQ:2023}", "next_context": "enhances SAM by incorporating global-local feature fusion, leading to high-quality mask predictions."}], "importance_score": 1.125}, "Wang:SegGPT:2023": {"bib_key": "Wang:SegGPT:2023", "bib_title": "{SegGPT}: Towards Segmenting Everything In Context", "bib_author ": "Wang, Xinlong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "SegGPT\\cite{Wang:SegGPT:2023}", "next_context": "proposed context ensemble strategies and allows users to tune a prompt for a specific dataset, scene, or even a person, while SEEM\\cite{Zou:Segment:2023}provides a completely promptable and interactive segmentation interface."}], "importance_score": 1.125}, "Zou:Segment:2023": {"bib_key": "Zou:Segment:2023", "bib_title": "Segment Everything Everywhere All at Once", "bib_author ": "Zou, Xueyan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "SegGPT\\cite{Wang:SegGPT:2023}proposed context ensemble strategies and allows users to tune a prompt for a specific dataset, scene, or even a person, while SEEM\\cite{Zou:Segment:2023}", "next_context": "provides a completely promptable and interactive segmentation interface."}], "importance_score": 1.125}, "Oquab:DINOv2:2024": {"bib_key": "Oquab:DINOv2:2024", "bib_title": "{DINOv2: Learning} Robust Visual Features without Supervision", "bib_author ": "Maxime Oquab", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": null, "prev_context": "\\cite{Oquab:DINOv2:2024}", "next_context": "has introduced DINOv2, aimed at enriching information about visual content through self-supervised learning."}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Depth Estimation", "prev_context": "\\cite{Oquab:DINOv2:2024}", "next_context": ", which is trained with vision transformers."}], "importance_score": 2.4027777777777777}, "ravi2024sam2": {"bib_key": "ravi2024sam2", "bib_title": "SAM 2: Segment Anything in Images and Videos", "bib_author ": "Ravi, Nikhila", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "More recently, SAM2\\cite{ravi2024sam2}", "next_context": "introduced support for real-time video segmentation."}], "importance_score": 1.125}, "Wu:DiffuMask:2023": {"bib_key": "Wu:DiffuMask:2023", "bib_title": "{DiffuMask: Synthesizing} Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models", "bib_author ": "Wu, Weijia", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "DiffuMask\\cite{Wu:DiffuMask:2023}", "next_context": "automatically generate image and pixel-level semantic annotation using pre-trained Stable Diffusion with input as a text prompt."}], "importance_score": 1.3333333333333333}, "Xu:Open:2023": {"bib_key": "Xu:Open:2023", "bib_title": "Open-Vocabulary Panoptic Segmentation With Text-to-Image Diffusion Models", "bib_author ": "Xu, Jiarui", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "\\cite{Xu:Open:2023}", "next_context": ", outperforming the previous methods by up to 7.6\\%."}], "importance_score": 1.3333333333333333}, "Gu:Diffusioninst:2024": {"bib_key": "Gu:Diffusioninst:2024", "bib_title": "{Diffusioninst: Diffusion} Model for Instance Segmentation", "bib_author ": "Gu, Zhangxuan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024}", "next_context": ""}], "importance_score": 0.3333333333333333}, "Gong:Continuous:2023": {"bib_key": "Gong:Continuous:2023", "bib_title": "Continuous Pseudo-Label Rectified Domain Adaptive Semantic Segmentation With Implicit Neural Representations", "bib_author ": "Gong, Rui", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Gong:Continuous:2023, Cen:Segment:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "In creative technologies, unsupervised domain adaptation (UDA) and INR are used for continuous rectification function modeling in\\cite{Gong:Continuous:2023}", "next_context": ", achieving superior segmentation results in night vision."}], "importance_score": 1.5}, "Cen:Segment:2023": {"bib_key": "Cen:Segment:2023", "bib_title": "Segment Anything in {3D with NeRFs}", "bib_author ": "Cen, Jiazhong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Gong:Continuous:2023, Cen:Segment:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "However, the recent SA3D approach\\cite{Cen:Segment:2023}", "next_context": "segments 3D objects using NeRFs as the structural prior."}], "importance_score": 1.5}, "Carion:DERT:2020": {"bib_key": "Carion:DERT:2020", "bib_title": "End-to-end object detection with transformers", "bib_author ": "Carion, Nicolas", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "Back in 2020, DETR by Facebook AI\\cite{Carion:DERT:2020}", "next_context": "is one of the first to adopt the transformer architecture for object detection."}], "importance_score": 1.1111111111111112}, "Zhu:Deformable:2021": {"bib_key": "Zhu:Deformable:2021", "bib_title": "{Deformable DETR: Deformable} Transformers for End-to-End Object Detection", "bib_author ": "Zhu, Xizhou", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "Deformable convolution has been used, Deformable DETR\\cite {Zhu:Deformable:2021}", "next_context": ", resulting in training faster and approximately 5\\%accuracy improvement."}], "importance_score": 1.1111111111111112}, "Huang:MonoDTR:2022": {"bib_key": "Huang:MonoDTR:2022", "bib_title": "{MonoDTR: Monocular} {3D} Object Detection with Depth-Aware Transformer", "bib_author ": "Kuan-Chih Huang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "As estimating depth from a 2D image has been well established\\cite{Yang:depthanything:2024}, MonoDTR\\cite{Huang:MonoDTR:2022}", "next_context": "integrates such module to estimate 3D box of the detected object from one image."}], "importance_score": 1.1111111111111112}, "zhao:videoprism:2024": {"bib_key": "zhao:videoprism:2024", "bib_title": "{VideoPrism: A} Foundational Visual Encoder for Video Understanding", "bib_author ": "Zhao, Long", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": null, "prev_context": "Google introduced VideoPrism\\cite{zhao:videoprism:2024}", "next_context": ", a  solution for scene understanding tasks, including classification, localization, retrieval, captioning, and question answering (QA)."}], "importance_score": 1.1111111111111112}, "Li:Your:2023": {"bib_key": "Li:Your:2023", "bib_title": "Your Diffusion Model is Secretly a Zero-Shot Classifier", "bib_author ": "Li, Alex", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:Your:2023,Chen:DiffusionDet:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "\\cite{Li:Your:2023}", "next_context": "."}], "importance_score": 1.5}, "Chen:DiffusionDet:2023": {"bib_key": "Chen:DiffusionDet:2023", "bib_title": "DiffusionDet: Diffusion Model for Object Detection", "bib_author ": "Chen, Shoufa", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Li:Your:2023,Chen:DiffusionDet:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "DiffusionDet\\cite{Chen:DiffusionDet:2023}", "next_context": "formulates object detection as a denoising diffusion process from noisy boxes to object boxes, reporting performance that surpasses DETR."}], "importance_score": 1.5}, "Meinhardt:TrackFormer:2022": {"bib_key": "Meinhardt:TrackFormer:2022", "bib_title": "TrackFormer: Multi-Object Tracking With Transformers", "bib_author ": "Meinhardt, Tim", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "The first three tracking-by-attention approaches are TrackFormer\\cite{Meinhardt:TrackFormer:2022}", "next_context": ", MixFormer\\cite{cui:mixformer:2022}, and ToMP\\cite{Mayer:Transforming:2022}."}], "importance_score": 1.1111111111111112}, "zeng:motr:2022": {"bib_key": "zeng:motr:2022", "bib_title": "MOTR: End-to-End Multiple-Object Tracking with TRansformer", "bib_author ": "Zeng, Fangao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "MOTRv2\\cite{Zhang:MOTRv2:2023}combines YOLOX\\cite{ge:yolox:2021}for object recognition and MOTR\\cite{zeng:motr:2022}", "next_context": "for tracking, outperforming TrackFormer by 20\\%."}], "importance_score": 1.1111111111111112}, "cui:mixformer:2022": {"bib_key": "cui:mixformer:2022", "bib_title": "Mixformer: End-to-end tracking with iterative mixed attention", "bib_author ": "Cui, Yutao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "The first three tracking-by-attention approaches are TrackFormer\\cite{Meinhardt:TrackFormer:2022}, MixFormer\\cite{cui:mixformer:2022}", "next_context": ", and ToMP\\cite{Mayer:Transforming:2022}."}], "importance_score": 1.1111111111111112}, "Mayer:Transforming:2022": {"bib_key": "Mayer:Transforming:2022", "bib_title": "Transforming Model Prediction for Tracking", "bib_author ": "Mayer, Christoph", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "The first three tracking-by-attention approaches are TrackFormer\\cite{Meinhardt:TrackFormer:2022}, MixFormer\\cite{cui:mixformer:2022}, and ToMP\\cite{Mayer:Transforming:2022}", "next_context": "."}], "importance_score": 1.1111111111111112}, "yang:track:2023": {"bib_key": "yang:track:2023", "bib_title": "Track Anything: Segment Anything Meets Videos", "bib_author ": "Jinyu Yang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "Many more methods have been proposed, including popular ones like SeqTrack\\cite{Chen:SeqTrack:2023}and Track Anything Model (TAM)\\cite{yang:track:2023}", "next_context": "."}], "importance_score": 1.1111111111111112}, "Chen:SeqTrack:2023": {"bib_key": "Chen:SeqTrack:2023", "bib_title": "SeqTrack: Sequence to Sequence Learning for Visual Object Tracking", "bib_author ": "Chen, Xin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "Many more methods have been proposed, including popular ones like SeqTrack\\cite{Chen:SeqTrack:2023}", "next_context": "and Track Anything Model (TAM)\\cite{yang:track:2023}."}], "importance_score": 1.1111111111111112}, "Zhang:MOTRv2:2023": {"bib_key": "Zhang:MOTRv2:2023", "bib_title": "{MOTRv2: Bootstrapping} End-to-End Multi-Object Tracking by Pretrained Object Detectors", "bib_author ": "Y. Zhang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "MOTRv2\\cite{Zhang:MOTRv2:2023}", "next_context": "combines YOLOX\\cite{ge:yolox:2021}for object recognition and MOTR\\cite{zeng:motr:2022}for tracking, outperforming TrackFormer by 20\\%."}], "importance_score": 1.1111111111111112}, "Yi:Comprehensive:2024": {"bib_key": "Yi:Comprehensive:2024", "bib_title": "A Comprehensive Study of Object Tracking in Low-Light Environments", "bib_author ": "Yi, A", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "Additionally, some methods have been specifically proposed for challenging environments, such as low light\\cite{Yi:Comprehensive:2024}", "next_context": "and small objects, as seen in AnyFlow\\cite{Jung:AnyFlow:2023}."}], "importance_score": 1.1111111111111112}, "kang2025exploring": {"bib_key": "kang2025exploring", "bib_title": "Exploring Enhanced Contextual Information for Video-Level Object Tracking", "bib_author ": "Ben Kang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "The current state-of-the-art for single-object tracking\\footnote{https://paperswithcode.com/sota/visual-object-tracking-on-lasot}, however, is based on cross-attention and Mamba\\cite{kang2025exploring}", "next_context": "."}], "importance_score": 1.1111111111111112}, "Luo:DiffusionTrack:2024": {"bib_key": "Luo:DiffusionTrack:2024", "bib_title": "{DiffusionTrack: Diffusion} Model for Multi-Object Tracking", "bib_author ": "Luo, Run", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "However, a spatial-temporal fusion module has been added to the diffusion head to exploit temporal features of the videos\\cite{Luo:DiffusionTrack:2024}", "next_context": "."}], "importance_score": 1.3333333333333333}, "Xie:DiffusionTrack:2024": {"bib_key": "Xie:DiffusionTrack:2024", "bib_title": "{DiffusionTrack: Point} Set Diffusion Model for Visual Object Tracking", "bib_author ": "Xie, Fei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "DiffusionTrack\\cite{Xie:DiffusionTrack:2024}", "next_context": "localizes the target in a progressive diffusion manner, which is claimed to better handle challenging scenarios."}], "importance_score": 1.3333333333333333}, "Zhang:DiffusionTracker:2024": {"bib_key": "Zhang:DiffusionTracker:2024", "bib_title": "{DiffusionTracker: Targets} Denoising Based on Diffusion Model for Visual Tracking", "bib_author ": "Zhang, Runqing", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "The method in\\cite{Zhang:DiffusionTracker:2024}", "next_context": "exploits spatial-temporal weighting to suppress the probability of the tracker changing the target to the distractors."}], "importance_score": 1.3333333333333333}, "Jung:AnyFlow:2023": {"bib_key": "Jung:AnyFlow:2023", "bib_title": "{AnyFlow: Arbitrary} Scale Optical Flow With Implicit Neural Representation", "bib_author ": "Jung, Hyunyoung", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Jung:AnyFlow:2023}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "Additionally, some methods have been specifically proposed for challenging environments, such as low light\\cite{Yi:Comprehensive:2024}and small objects, as seen in AnyFlow\\cite{Jung:AnyFlow:2023}", "next_context": "."}], "importance_score": 2.0}, "Wang:multi:2021": {"bib_key": "Wang:multi:2021", "bib_title": "Multi-View {3D} Reconstruction With Transformers", "bib_author ": "Wang, Dan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024}", "next_context": ""}], "importance_score": 0.16666666666666666}, "Zhang:Lite:2023": {"bib_key": "Zhang:Lite:2023", "bib_title": "{Lite-Mono: A} Lightweight CNN and Transformer Architecture for Self-Supervised Monocular Depth Estimation", "bib_author ": "Zhang, Ning", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Depth Estimation", "prev_context": "There have been attempts to use transformers, such as\\cite{Zhang:Lite:2023}", "next_context": "and\\cite{Chen:Vision:2023}, and diffusion models, such as\\cite{Ji:DDP:2023}and\\cite{Ke:Repurposing:2024}."}], "importance_score": 1.1666666666666667}, "Chen:Vision:2023": {"bib_key": "Chen:Vision:2023", "bib_title": "Vision Transformer Adapter for Dense Predictions", "bib_author ": "Zhe Chen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Depth Estimation", "prev_context": "There have been attempts to use transformers, such as\\cite{Zhang:Lite:2023}and\\cite{Chen:Vision:2023}", "next_context": ", and diffusion models, such as\\cite{Ji:DDP:2023}and\\cite{Ke:Repurposing:2024}."}], "importance_score": 1.1666666666666667}, "Yang:depthanything:2024": {"bib_key": "Yang:depthanything:2024", "bib_title": "Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data", "bib_author ": "Yang, Lihe", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "As estimating depth from a 2D image has been well established\\cite{Yang:depthanything:2024}", "next_context": ", MonoDTR\\cite{Huang:MonoDTR:2022}integrates such module to estimate 3D box of the detected object from one image."}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Depth Estimation", "prev_context": "It is built on the previous version\\cite{Yang:depthanything:2024}", "next_context": ", jointly trained on large-scale labeled and unlabeled images and uses semantic priors from pretrained encoders."}], "importance_score": 2.166666666666667}, "Yang:depthanythingv2:2024": {"bib_key": "Yang:depthanythingv2:2024", "bib_title": "Depth Anything V2", "bib_author ": "Yang, Lihe", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Depth Estimation", "prev_context": "Amongst these, Depth Anything v2\\cite{Yang:depthanythingv2:2024}", "next_context": "has become a state-of-the-art monocular depth estimation method."}], "importance_score": 1.1666666666666667}, "Barron:Mip-NeRF360:2022": {"bib_key": "Barron:Mip-NeRF360:2022", "bib_title": "{Mip-NeRF 360: Unbounded} Anti-Aliased Neural Radiance Fields", "bib_author ": "Barron, Jonathan T.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024}", "next_context": ""}, {"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, azzarelli:waveplanes:2023}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "Mip-NeRF360\\cite{Barron:Mip-NeRF360:2022}", "next_context": "proposed unbounded anti-aliased technique achieving full 360 degree content."}], "importance_score": 1.375}, "Ji:DDP:2023": {"bib_key": "Ji:DDP:2023", "bib_title": "{DDP: Diffusion} Model for Dense Visual Prediction", "bib_author ": "Ji, Yuanfeng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Depth Estimation", "prev_context": "There have been attempts to use transformers, such as\\cite{Zhang:Lite:2023}and\\cite{Chen:Vision:2023}, and diffusion models, such as\\cite{Ji:DDP:2023}", "next_context": "and\\cite{Ke:Repurposing:2024}."}], "importance_score": 1.25}, "wynn:diffusionerf:2023": {"bib_key": "wynn:diffusionerf:2023", "bib_title": "{DiffusioNeRF: Regularizing Neural Radiance Fields with Denoising Diffusion Models}", "bib_author ": "Jamie Wynn", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "Diffusion models are integrated to regularize NeRF reconstructions\\cite{wynn:diffusionerf:2023}", "next_context": ", resulting in smoother depth continuity and clearer edges where depth discontinuities occur."}], "importance_score": 1.25}, "Ke:Repurposing:2024": {"bib_key": "Ke:Repurposing:2024", "bib_title": "Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation", "bib_author ": "Ke, Bingxin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Depth Estimation", "prev_context": "There have been attempts to use transformers, such as\\cite{Zhang:Lite:2023}and\\cite{Chen:Vision:2023}, and diffusion models, such as\\cite{Ji:DDP:2023}and\\cite{Ke:Repurposing:2024}", "next_context": "."}], "importance_score": 1.25}, "pumarola:DNeRF:2020": {"bib_key": "pumarola:DNeRF:2020", "bib_title": "{D-NeRF: Neural} Radiance Fields for Dynamic Scenes", "bib_author ": "Pumarola, Albert", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, azzarelli:waveplanes:2023}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "\\cite{pumarola:DNeRF:2020}", "next_context": ", known as D-NeRF."}], "importance_score": 1.125}, "mueller:instant:2022": {"bib_key": "mueller:instant:2022", "bib_title": "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding", "bib_author ": "Thomas M\\\"uller", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, azzarelli:waveplanes:2023}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Augmented, virtual and mixed reality, and 3D content", "prev_context": "RealFusion\\cite{Melas:RealFusion:2023}, a single image to 3D object generator, merges 2D diffusion models with NeRF, improving Instant-NGP\\cite{mueller:instant:2022}", "next_context": ", which provide API for VR controls."}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "The Instant-NGP tool developed by Nvidia\\cite{mueller:instant:2022}", "next_context": "enables real-time training of NeRFs by bypassing sampling in empty spaces and dense areas, and by incorporating multi-resolution hash encoding techniques."}], "importance_score": 2.125}, "Mildenhall:NeRFDark:2022": {"bib_key": "Mildenhall:NeRFDark:2022", "bib_title": "{NeRF in the Dark: High} Dynamic Range View Synthesis From Noisy Raw Images", "bib_author ": "Mildenhall, Ben", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, azzarelli:waveplanes:2023}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "Google Research\\cite{Mildenhall:NeRFDark:2022}", "next_context": "trains NeRF from noisy RAW images captured in the dark scene, allowing changing viewpoint, focus, exposure, and tone mapping simultaneously."}], "importance_score": 1.125}, "Fang:Fast:2022": {"bib_key": "Fang:Fast:2022", "bib_title": "Fast Dynamic Radiance Fields with Time-Aware Neural Voxels", "bib_author ": "Fang, Jiemin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, azzarelli:waveplanes:2023}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "However, the current leading method for generating high-quality novel views of real dynamic scenes is TiNeuVox\\cite{Fang:Fast:2022}", "next_context": "."}], "importance_score": 1.125}, "Guo:neural:2022": {"bib_key": "Guo:neural:2022", "bib_title": "Neural {3D} Scene Reconstruction With the Manhattan-World Assumption", "bib_author ": "Guo, Haoyu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, azzarelli:waveplanes:2023}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "With segmentation techniques significantly advanced (see Section\\ref{sssec:seg}), there have been integrations utilizing semantic segmentation to enhance 3D representation\\cite{Guo:neural:2022}", "next_context": "."}], "importance_score": 1.125}, "azzarelli:waveplanes:2023": {"bib_key": "azzarelli:waveplanes:2023", "bib_title": "{WavePlanes: A} Compact Wavelet Representation for Dynamic Neural Radiance Fields", "bib_author ": "Azzarelli, Adrian", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, azzarelli:waveplanes:2023}", "next_context": ",\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025}^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "Wavelet transform is employed in\\cite{azzarelli:waveplanes:2023}", "next_context": "to further reduce model size."}], "importance_score": 1.125}, "Fridovich:kplanes:2023": {"bib_key": "Fridovich:kplanes:2023", "bib_title": "K-Planes: Explicit Radiance Fields in Space, Time, and Appearance", "bib_author ": "{Sara Fridovich-Keil", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, azzarelli:waveplanes:2023},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025}", "next_context": "^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "Therefore,K-planes\\cite{Fridovich:kplanes:2023}", "next_context": "propose a simple planar factorization for volumetric rendering, achieving low memory usage (1000\\timescompression over a full 4D grid)."}], "importance_score": 1.1666666666666667}, "kerbl:3Dgaussians:2023": {"bib_key": "kerbl:3Dgaussians:2023", "bib_title": "{3D} Gaussian Splatting for Real-Time Radiance Field Rendering", "bib_author ": "Kerbl, Bernhard", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, azzarelli:waveplanes:2023},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025}", "next_context": "^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Gaussian Splatting", "prev_context": "3D Gaussian Splatting (3D-GS)\\cite{kerbl:3Dgaussians:2023}", "next_context": "has been introduced using anisotropic 3D Gaussians for a high-quality, unstructured representation of radiance fields."}], "importance_score": 1.1666666666666667}, "wu:4dgaussians:2024": {"bib_key": "wu:4dgaussians:2024", "bib_title": "4D Gaussian Splatting for Real-Time Dynamic Scene Rendering", "bib_author ": "Wu, Guanjun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, azzarelli:waveplanes:2023},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025}", "next_context": "^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Gaussian Splatting", "prev_context": "For dynamic scenes, 4D Gaussian Splatting (4D-GS)\\cite{wu:4dgaussians:2024}", "next_context": "proposes a Gaussian deformation field for motion and shape."}], "importance_score": 1.1666666666666667}, "Yu:CoGS:2024": {"bib_key": "Yu:CoGS:2024", "bib_title": "{CoGS: Controllable} Gaussian Splatting", "bib_author ": "Yu, Heng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, azzarelli:waveplanes:2023},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025}", "next_context": "^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Gaussian Splatting", "prev_context": "Instead of developing in 4D, CoGS\\cite{Yu:CoGS:2024}", "next_context": "exploits 3D-GS by integrating control mechanisms in separate regions to learn individual temporal dimensions."}], "importance_score": 1.1666666666666667}, "Huang:SCGS:2024": {"bib_key": "Huang:SCGS:2024", "bib_title": "{SC-GS: Sparse}-Controlled Gaussian Splatting for Editable Dynamic Scenes", "bib_author ": "Huang, Yi-Hua", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, azzarelli:waveplanes:2023},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025}", "next_context": "^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Gaussian Splatting", "prev_context": "SC-GS\\cite{Huang:SCGS:2024}", "next_context": "extracts sparse control points and uses an MLP to predict time-varying 6 DoF transformations."}], "importance_score": 1.1666666666666667}, "Wang2025": {"bib_key": "Wang2025", "bib_title": "{UW-GS: Distractor}-Aware 3D Gaussian Splatting for Enhanced Underwater Scene Reconstruction", "bib_author ": "H. Wang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, azzarelli:waveplanes:2023},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025}", "next_context": "^\\dag"}, {"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Gaussian Splatting", "prev_context": "Physics-inspired approaches are also integrated to improve 3D modeling in different media, such as 3D underwater scenes\\cite{Wang2025}", "next_context": "."}], "importance_score": 1.1666666666666667}, "zhu2022transformer": {"bib_key": "zhu2022transformer", "bib_title": "Transformer-based transform coding", "bib_author ": "Zhu, Yinhao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{zhu2022transformer,zou2022devil,liu2023learned}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "\\cite{zhu2022transformer}", "next_context": ", STF\\cite{zou2022devil}and LIC-TCM\\cite{liu2023learned}."}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "SwinT-ChARM\\cite{zhu2022transformer}", "next_context": "employs Swin transformers for non-linear transforms and outperforms the latest standard image codec, the Versatile Video Coding (VVC) Test Model (VTM, All Intra)."}], "importance_score": 2.333333333333333}, "zou2022devil": {"bib_key": "zou2022devil", "bib_title": "The devil is in the details: Window-based attention for image compression", "bib_author ": "Zou, Renjie", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{zhu2022transformer,zou2022devil,liu2023learned}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "\\cite{zhu2022transformer}, STF\\cite{zou2022devil}", "next_context": "and LIC-TCM\\cite{liu2023learned}."}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "STF\\cite{zou2022devil}", "next_context": "is based on a symmetrical transformer framework containing absolute transformer blocks in both the down-sampling encoder and the up-sampling decoder, which also shows improved rate quality performance over VTM."}], "importance_score": 2.333333333333333}, "liu2023learned": {"bib_key": "liu2023learned", "bib_title": "Learned image compression with mixed transformer-cnn architectures", "bib_author ": "Liu, Jinming", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{zhu2022transformer,zou2022devil,liu2023learned}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "\\cite{zhu2022transformer}, STF\\cite{zou2022devil}and LIC-TCM\\cite{liu2023learned}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "LIC-TCM\\cite{liu2023learned}", "next_context": "exploits the local modeling ability of CNN and the non-local modeling performance of transformers, and proposes a parallel transformer-CNN mixture block."}], "importance_score": 2.333333333333333}, "careil2023towards": {"bib_key": "careil2023towards", "bib_title": "Towards image compression with perfect realism at ultra-low bitrates", "bib_author ": "Careil, Marlene", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "More recently, diffusion models have been applied in image compression to allow realistic reconstruction at ultra-low bitrates\\cite{careil2023towards}", "next_context": "and achieve more competitive performance over GAN-based models\\cite{yang2024lossy}."}], "importance_score": 1.25}, "yang2024lossy": {"bib_key": "yang2024lossy", "bib_title": "Lossy image compression with conditional diffusion models", "bib_author ": "Yang, Ruihan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "More recently, diffusion models have been applied in image compression to allow realistic reconstruction at ultra-low bitrates\\cite{careil2023towards}and achieve more competitive performance over GAN-based models\\cite{yang2024lossy}", "next_context": "."}], "importance_score": 1.25}, "hoogeboom2023high": {"bib_key": "hoogeboom2023high", "bib_title": "High-fidelity image compression with score-based generative models", "bib_author ": "Hoogeboom, Emiel", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "\\cite{hoogeboom2023high}", "next_context": "and the diffusion-based residual augmentation codec (DIRAC)\\cite{ghouse2023residual}."}], "importance_score": 1.25}, "ghouse2023residual": {"bib_key": "ghouse2023residual", "bib_title": "A residual diffusion model for high perceptual quality codec augmentation", "bib_author ": "Ghouse, Noor Fathima", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "\\cite{hoogeboom2023high}and the diffusion-based residual augmentation codec (DIRAC)\\cite{ghouse2023residual}", "next_context": "."}], "importance_score": 1.25}, "sitzmann2020implicit": {"bib_key": "sitzmann2020implicit", "bib_title": "Implicit neural representations with periodic activation functions", "bib_author ": "Sitzmann, Vincent", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "\\cite{dupont2021coin,dupontcoin++}and\\cite{strumpler2022implicit}that combines SIREN networks\\cite{sitzmann2020implicit}", "next_context": "with positional encoding,   "}], "importance_score": 1.25}, "dupont2021coin": {"bib_key": "dupont2021coin", "bib_title": "{COIN}: COmpression with Implicit Neural representations", "bib_author ": "Dupont, Emilien", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "\\cite{dupont2021coin,dupontcoin++}", "next_context": "and\\cite{strumpler2022implicit}that combines SIREN networks\\cite{sitzmann2020implicit}with positional encoding,   "}], "importance_score": 0.75}, "dupontcoin++": {"bib_key": "dupontcoin++", "bib_title": "COIN++: Neural Compression Across Modalities", "bib_author ": "Dupont, Emilien", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "\\cite{dupont2021coin,dupontcoin++}", "next_context": "and\\cite{strumpler2022implicit}that combines SIREN networks\\cite{sitzmann2020implicit}with positional encoding,   "}], "importance_score": 0.75}, "strumpler2022implicit": {"bib_key": "strumpler2022implicit", "bib_title": "Implicit neural representations for image compression", "bib_author ": "Str{\\\"u}mpler, Yannick", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "\\cite{dupont2021coin,dupontcoin++}and\\cite{strumpler2022implicit}", "next_context": "that combines SIREN networks\\cite{sitzmann2020implicit}with positional encoding,   "}], "importance_score": 1.25}, "xiang2022mimt": {"bib_key": "xiang2022mimt", "bib_title": "Mimt: Masked image modeling transformer for video compression", "bib_author ": "Xiang, Jinxi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{xiang2022mimt,mentzer2022vct}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "New architectures have also been proposed such as CANF-VC~\\cite{ho2022canf}based on a video generative model, MTMT\\cite{xiang2022mimt}", "next_context": "using a masked image modeling transformer based entropy model and VCT\\cite{mentzer2022vct}based on a video compression transformer."}], "importance_score": 1.5}, "mentzer2022vct": {"bib_key": "mentzer2022vct", "bib_title": "VCT: A Video Compression Transformer", "bib_author ": "Mentzer, Fabian", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{xiang2022mimt,mentzer2022vct}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "New architectures have also been proposed such as CANF-VC~\\cite{ho2022canf}based on a video generative model, MTMT\\cite{xiang2022mimt}using a masked image modeling transformer based entropy model and VCT\\cite{mentzer2022vct}", "next_context": "based on a video compression transformer."}], "importance_score": 1.5}, "li2024extreme": {"bib_key": "li2024extreme", "bib_title": "Extreme video compression with pre-trained diffusion models", "bib_author ": "Li, Bohan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{li2024extreme}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "More recently, research has been conducted to further improve the performance of these learning-based coding tools utilizing more advanced network architectures, including ViTs\\cite{kathariya2023joint}, and diffusion models\\cite{li2024extreme}", "next_context": "."}], "importance_score": 2.0}, "chen2021nerv": {"bib_key": "chen2021nerv", "bib_title": "{NeRV}: Neural representations for videos", "bib_author ": "Chen, Hao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "~\\cite{chen2021nerv}", "next_context": ", patch~\\cite{bai2023ps}or disentangled spatial/grid coordinates~\\cite{li2022nerv}as model input, while content-based approaches~\\cite{kwan2024hinerv,kim2024c3,leguay2024cool}have content-specific embedding as inputs."}], "importance_score": 1.1111111111111112}, "bai2023ps": {"bib_key": "bai2023ps", "bib_title": "{PS-NeRV}: Patch-wise stylized neural representations for videos", "bib_author ": "Bai, Yunpeng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "~\\cite{chen2021nerv}, patch~\\cite{bai2023ps}", "next_context": "or disentangled spatial/grid coordinates~\\cite{li2022nerv}as model input, while content-based approaches~\\cite{kwan2024hinerv,kim2024c3,leguay2024cool}have content-specific embedding as inputs."}], "importance_score": 1.1111111111111112}, "kim2024c3": {"bib_key": "kim2024c3", "bib_title": "C3: High-performance and low-complexity neural compression from a single image or video", "bib_author ": "Kim, Hyunjik", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "~\\cite{kwan2024hinerv,kim2024c3,leguay2024cool}", "next_context": "have content-specific embedding as inputs."}], "importance_score": 0.4444444444444444}, "leguay2024cool": {"bib_key": "leguay2024cool", "bib_title": "{Cool-chic video: Learned} video coding with 800 parameters", "bib_author ": "Leguay, Thomas", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "~\\cite{kwan2024hinerv,kim2024c3,leguay2024cool}", "next_context": "have content-specific embedding as inputs."}], "importance_score": 0.4444444444444444}, "kwan2024nvrc": {"bib_key": "kwan2024nvrc", "bib_title": "{NVRC}: Neural Video Representation Compression", "bib_author ": "Kwan, Ho Man", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "\\cite{kwan2024nvrc}", "next_context": "has already achieved a performance similar to that of VVC VTM (RA), but with a much lower decoding complexity compared to autoencoder-based neural codecs."}], "importance_score": 1.1111111111111112}, "gao2024pnvc": {"bib_key": "gao2024pnvc", "bib_title": "{PNVC}: Towards Practical {INR}-based Video Compression", "bib_author ": "Gao, Ge", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "To address this limitation, efforts have been made~\\cite{gao2024pnvc}", "next_context": "towards more practical INR-based video compression (such as the Low Delay and Random Access modes in VVC VTM\\cite{bossen2023vtmctc}) by combining pre-training and online model overfitting."}], "importance_score": 1.1111111111111112}, "ruan2024point": {"bib_key": "ruan2024point", "bib_title": "Point Cloud Compression with Implicit Neural Representations: A Unified Framework", "bib_author ": "Ruan, Hongning", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "Some of these models have also been applied to volumetric video content~\\cite{ruan2024point,kwan2024immersive}", "next_context": ", demonstrating their great potential to compete with standard and other learning-based methods."}], "importance_score": 0.6111111111111112}, "kwan2024immersive": {"bib_key": "kwan2024immersive", "bib_title": "Immersive Video Compression using Implicit Neural Representations", "bib_author ": "Kwan, Ho Man", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "Some of these models have also been applied to volumetric video content~\\cite{ruan2024point,kwan2024immersive}", "next_context": ", demonstrating their great potential to compete with standard and other learning-based methods."}], "importance_score": 0.6111111111111112}, "cheon2021perceptual": {"bib_key": "cheon2021perceptual", "bib_title": "Perceptual image quality assessment with transformers", "bib_author ": "Cheon, Manri", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Important works in this class including IQT\\cite{cheon2021perceptual}", "next_context": ", TRes\\cite{golestaneh2022no}, SaTQA\\cite{shi2024transformer}, FastVQA\\cite{wu2022fast}and RankDVQA\\cite{feng2024rankdvqa}."}], "importance_score": 1.3333333333333333}, "golestaneh2022no": {"bib_key": "golestaneh2022no", "bib_title": "No-reference image quality assessment via transformers, relative ranking, and self-consistency", "bib_author ": "Golestaneh, S Alireza", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Important works in this class including IQT\\cite{cheon2021perceptual}, TRes\\cite{golestaneh2022no}", "next_context": ", SaTQA\\cite{shi2024transformer}, FastVQA\\cite{wu2022fast}and RankDVQA\\cite{feng2024rankdvqa}."}], "importance_score": 1.3333333333333333}, "shi2024transformer": {"bib_key": "shi2024transformer", "bib_title": "Transformer-based no-reference image quality assessment via supervised contrastive learning", "bib_author ": "Shi, Jinsong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Important works in this class including IQT\\cite{cheon2021perceptual}, TRes\\cite{golestaneh2022no}, SaTQA\\cite{shi2024transformer}", "next_context": ", FastVQA\\cite{wu2022fast}and RankDVQA\\cite{feng2024rankdvqa}."}], "importance_score": 1.3333333333333333}, "wu2022fast": {"bib_key": "wu2022fast", "bib_title": "{Fast-VQA}: Efficient end-to-end video quality assessment with fragment sampling", "bib_author ": "Wu, Haoning", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Important works in this class including IQT\\cite{cheon2021perceptual}, TRes\\cite{golestaneh2022no}, SaTQA\\cite{shi2024transformer}, FastVQA\\cite{wu2022fast}", "next_context": "and RankDVQA\\cite{feng2024rankdvqa}."}], "importance_score": 1.2}, "feng2024rankdvqa": {"bib_key": "feng2024rankdvqa", "bib_title": "Rankdvqa: Deep vqa based on ranking-inspired hybrid training", "bib_author ": "Feng, Chen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Important works in this class including IQT\\cite{cheon2021perceptual}, TRes\\cite{golestaneh2022no}, SaTQA\\cite{shi2024transformer}, FastVQA\\cite{wu2022fast}and RankDVQA\\cite{feng2024rankdvqa}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{liu2017rankiqa}and UNIQUE\\cite{zhang2021uncertainty}for the image quality assessment task, and VFIPS\\cite{hou2022perceptual}and RankDVQA\\cite{feng2024rankdvqa}", "next_context": "for video quality assessment."}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Performance and main challenges", "prev_context": "\\cite{feng2024rankdvqa}", "next_context": "."}], "importance_score": 3.2}, "wu2023exploringvideo": {"bib_key": "wu2023exploringvideo", "bib_title": "Exploring video quality assessment on user generated contents from aesthetic and technical perspectives", "bib_author ": "Wu, Haoning", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "The former has been further extended as DOVER\\cite{wu2023exploringvideo}", "next_context": "and COVER\\cite{he2024cover}when aesthetic and/or semantic aspects in the content are taken into account."}], "importance_score": 1.2}, "he2024cover": {"bib_key": "he2024cover", "bib_title": "{COVER}: A comprehensive video quality evaluator", "bib_author ": "He, Chenlong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "The former has been further extended as DOVER\\cite{wu2023exploringvideo}and COVER\\cite{he2024cover}", "next_context": "when aesthetic and/or semantic aspects in the content are taken into account."}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Similar works have also been proposed for video quality assessment, such as BVQI\\cite{wu2023exploring,wu2023towards}and COVER\\cite{he2024cover}", "next_context": "."}], "importance_score": 2.2}, "peng2024rmt": {"bib_key": "peng2024rmt", "bib_title": "{RMT-BVQA}: Recurrent memory transformer-based blind video quality assessment for enhanced video content", "bib_author ": "Peng, Tianhao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Current Advanced AI Technologies", "subsection": "Implicit Neural Representations", "subsubsection": null, "prev_context": "\\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}", "next_context": ""}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "More recently, quality-aware contrastive loss has been designed in\\cite{zhao2023quality,peng2024rmt}", "next_context": "to stabilize the learning process."}], "importance_score": 0.7}, "encyclopedia_ai_v1": {"bib_key": "encyclopedia_ai_v1", "bib_title": "Encyclopedia of Artificial Intelligence", "bib_author ": "", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": null, "prev_context": "The roots of AI art can be traced back to before the 2000s, exemplified by AARON, a computer program initiated in 1972 to autonomously produce paintings and drawings\\cite{encyclopedia_ai_v1}", "next_context": "."}], "importance_score": 1.0}, "Azzarelli:Reviewing:2024": {"bib_key": "Azzarelli:Reviewing:2024", "bib_title": "Reviewing Intelligent Cinematography: {AI} research for camera-based video production", "bib_author ": "A Azzarelli", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": null, "prev_context": "Note that when referring to real camera content acquisition using AI, commonly known as intelligent cinematography, we direct the reader to the review in\\cite{Azzarelli:Reviewing:2024}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "AI script generators serve as beneficial aids for writers, filmmakers, and game developers, offering inspiration, idea generation, and assistance in crafting entire scripts\\cite{Jeary2024, Azzarelli:Reviewing:2024}", "next_context": "."}], "importance_score": 1.5}, "ippolito:creative:2022": {"bib_key": "ippolito:creative:2022", "bib_title": "Creative Writing with an {AI}-Powered Writing Assistant: Perspectives from Professional Writers", "bib_author ": "Ippolito, Daphne", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "These tools go beyond mere grammar and spelling checks; they boast advancements enabling them to analyze the style and tone of written material, adding images, videos and tables, offering suggestions to enhance clarity, coherence, and overall readability\\cite{ippolito:creative:2022}", "next_context": "."}], "importance_score": 1.0}, "guo:exploring:2024": {"bib_key": "guo:exploring:2024", "bib_title": "Exploring the Interaction of Creative Writers with {AI}-Powered Writing Tools", "bib_author ": "Guo, Alicia", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "Human-AI brainstorming is helpful and saves time\\cite{guo:exploring:2024}", "next_context": "."}], "importance_score": 1.0}, "Mirowski:cowriting:2023": {"bib_key": "Mirowski:cowriting:2023", "bib_title": "Co-Writing Screenplays and Theatre Scripts with Language Models: Evaluation by Industry Professionals", "bib_author ": "Mirowski, Piotr", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "Dramatron, developed by Google\\cite{Mirowski:cowriting:2023}", "next_context": ", introduces hierarchical language generation, enabling the creation of cohesive scripts and screenplays spanning long ranges."}], "importance_score": 1.0}, "Beckett:Generating:2023": {"bib_key": "Beckett:Generating:2023", "bib_title": "Generating Change: {A} global survey of what news organisations are doing with {AI}", "bib_author ": "Charlie Beckett", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "\\cite{Beckett:Generating:2023}", "next_context": "."}], "importance_score": 1.0}, "Stefanini:From:2023": {"bib_key": "Stefanini:From:2023", "bib_title": "From Show to Tell: A Survey on Deep Learning-Based Image Captioning", "bib_author ": "Stefanini, Matteo", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "\\cite{Stefanini:From:2023}", "next_context": ") and with text prompts."}], "importance_score": 1.0}, "radford2021learning": {"bib_key": "radford2021learning", "bib_title": "Learning Transferable Visual Models From Natural Language Supervision", "bib_author ": "Alec Radford", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "The most well-known technique is Contrastive Language-Image Pre-training (CLIP)\\cite{radford2021learning}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Moreover, recent research works also focus on using pre-trained vision-language models, such as CLIP\\cite{radford2021learning}", "next_context": ", which align better image and text modalities."}], "importance_score": 2.0}, "Zhang:vision:2024": {"bib_key": "Zhang:vision:2024", "bib_title": "Vision-Language Models for Vision Tasks: A Survey", "bib_author ": "Zhang, Jingyi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Text generation, script and journalism", "prev_context": "The comprehensive survey of VLMs for vision tasks can be found in\\cite{Zhang:vision:2024}", "next_context": "."}], "importance_score": 1.0}, "Oord:Neural:2017": {"bib_key": "Oord:Neural:2017", "bib_title": "Neural discrete representation learning", "bib_author ": "van den Oord, Aaron", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Audio and Music generation", "prev_context": "\\cite{Oord:Neural:2017}", "next_context": "."}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Audio Compression", "prev_context": "Similarly to images and videos, learning-based solutions have also been researched to compress audio signals, and most neural audio codecs are based on VQ-VAE\\cite{Oord:Neural:2017}", "next_context": "."}], "importance_score": 2.0}, "Wang:One:2022": {"bib_key": "Wang:One:2022", "bib_title": "One-Shot Voice Conversion For Style Transfer Based On Speaker Adaptation", "bib_author ": "Wang, Zhichao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Audio and Music generation", "prev_context": "Voice style transfer often use zero-shot learning (a model is trained to recognize classes or categories it has never seen during training)\\cite{Huang:GenerSpeech:2022}or few-shot learning (a model is trained with only one or a few examples per class)\\cite{Wang:One:2022}", "next_context": "."}], "importance_score": 1.0}, "XU2025103402": {"bib_key": "XU2025103402", "bib_title": "Integrating augmented reality and LLM for enhanced cognitive support in critical audio communications", "bib_author ": "Fang Xu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Augmented, virtual and mixed reality, and 3D content", "prev_context": "While the benefits of LLMs in Augmented Reality (AR) directly target educational purposes, enhance cognitive support, and facilitate communication\\cite{XU2025103402}", "next_context": ", mixed reality (MR) has once again become exciting since the release of the Apple Vision Pro in February 2024."}], "importance_score": 1.0}, "deitke:Objaverse:2023": {"bib_key": "deitke:Objaverse:2023", "bib_title": "{Objaverse-XL: A} Universe of 10M+ {3D} Objects", "bib_author ": "Deitke, Matt", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content creation", "subsubsection": "Augmented, virtual and mixed reality, and 3D content", "prev_context": "3D objects are gathered from Objaverse\\cite{deitke:Objaverse:2023}", "next_context": ", a dataset with 800K+ annotated 3D objects."}], "importance_score": 1.0}, "feizi:Online:2023": {"bib_key": "feizi:Online:2023", "bib_title": "Online Advertisements with LLMs: Opportunities and Challenges", "bib_author ": "Feizi, Soheil", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Advertisements and film analysis", "prev_context": "Not only does AI assist in generating ideas and content, but it can also aid creators in effectively matching content to their audiences, particularly on an individual level\\cite{feizi:Online:2023}", "next_context": "."}], "importance_score": 1.0}, "CHUA:AI:2023": {"bib_key": "CHUA:AI:2023", "bib_title": "AI-enabled investment advice: Will users buy it?", "bib_author ": "Alton Y.K. Chua", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Content retrieval and recommendation services", "prev_context": "\\cite{CHUA:AI:2023}", "next_context": "has reported a positive association between buyers' attitudes toward AI and their behavioral intention to accept AI-based recommendations, with potential for further growth."}], "importance_score": 1.0}, "brynjolfsson:generative:2023": {"bib_key": "brynjolfsson:generative:2023", "bib_title": "Generative {AI} at Work", "bib_author ": "Brynjolfsson, Erik", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Intelligent assistants", "prev_context": "\\cite{brynjolfsson:generative:2023}", "next_context": "examined the implementation of a generative AI tool designed to offer conversational guidance to customer support agents."}], "importance_score": 1.0}, "Lee:design:2024": {"bib_key": "Lee:design:2024", "bib_title": "A Design Space for Intelligent and Interactive Writing Assistants", "bib_author ": "Lee, Mina", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Intelligent assistants", "prev_context": "AI-based intelligent assistants may currently be more focused on educational purposes, but they can clearly help artists write more efficiently\\cite{Lee:design:2024}", "next_context": "or assist in customizing personal requirements\\cite{sajja2024ai}."}], "importance_score": 1.0}, "sajja2024ai": {"bib_key": "sajja2024ai", "bib_title": "Artificial Intelligence-Enabled Intelligent Assistant for Personalized and Adaptive Learning in Higher Education", "bib_author ": "Sajja, R.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information analysis", "subsubsection": "Intelligent assistants", "prev_context": "AI-based intelligent assistants may currently be more focused on educational purposes, but they can clearly help artists write more efficiently\\cite{Lee:design:2024}or assist in customizing personal requirements\\cite{sajja2024ai}", "next_context": "."}], "importance_score": 1.0}, "Zhou:LEDNet:2022": {"bib_key": "Zhou:LEDNet:2022", "bib_title": "LEDNet: Joint Low-Light Enhancement and Deblurring in the Dark", "bib_author ": "Zhou, Shangchen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "To address this, LEDNet\\cite{Zhou:LEDNet:2022}", "next_context": "has introduced a synthetic dataset for such scenarios and incorporated a learnable non-linear activation function within the network to enhance feature intensities."}], "importance_score": 1.0}, "anantrasirichai:BVI:2024": {"bib_key": "anantrasirichai:BVI:2024", "bib_title": "{BVI-Lowlight: Fully} Registered Benchmark Dataset for Low-Light Video Enhancement", "bib_author ": "Anantrasirichai, Nantheera", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "STA-SUNet\\cite{Lin:SPATIO:2024}has demonstrated that using transformers for low-light video enhancement outperforms CNN-based methods\\cite{anantrasirichai:BVI:2024}", "next_context": "."}], "importance_score": 1.0}, "Lin:BVI-RLV:2024": {"bib_key": "Lin:BVI-RLV:2024", "bib_title": "{BVI-RLV: A} Fully Registered Dataset and Benchmarks for Low-Light Video Enhancement", "bib_author ": "R Lin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Enhancement", "prev_context": "While most training datasets use normal lighting conditions as ground truth\\cite{Lin:BVI-RLV:2024}", "next_context": ", the enhanced images and videos may alter the mood and tone of the content."}], "importance_score": 1.0}, "Conde:Efficient:2023": {"bib_key": "Conde:Efficient:2023", "bib_title": "Efficient Deep Models for Real-Time 4K Image Super-Resolution. NTIRE 2023 Benchmark and Report", "bib_author ": "Conde, Marcos V.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "However, the NTIRE 2023 Real-Time Super-Resolution Challenge\\cite{Conde:Efficient:2023}", "next_context": "showed that the winner, Bicubic++\\cite{Bilecen:Bicubic:2023}, uses only convolutional layers and achieves the fastest speed at 1.17ms in upscaling 720p to 4K images."}], "importance_score": 1.0}, "Bilecen:Bicubic:2023": {"bib_key": "Bilecen:Bicubic:2023", "bib_title": "{Bicubic++: Slim,} Slimmer, Slimmest Designing an Industry-Grade Super-Resolution Network", "bib_author ": "B. Bilecen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "However, the NTIRE 2023 Real-Time Super-Resolution Challenge\\cite{Conde:Efficient:2023}showed that the winner, Bicubic++\\cite{Bilecen:Bicubic:2023}", "next_context": ", uses only convolutional layers and achieves the fastest speed at 1.17ms in upscaling 720p to 4K images."}], "importance_score": 1.0}, "moser:diffusion:2023": {"bib_key": "moser:diffusion:2023", "bib_title": "Diffusion Models, Image Super-Resolution, and Everything: A Survey", "bib_author ": "Moser, Brian B.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "A survey in SISR using DMs can be found in\\cite{moser:diffusion:2023}", "next_context": "."}], "importance_score": 1.0}, "Chan:BasicVSR:2022": {"bib_key": "Chan:BasicVSR:2022", "bib_title": "BasicVSR++: Improving Video Super-Resolution With Enhanced Propagation and Alignment", "bib_author ": "Chan, Kelvin C.K.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "Although the results are slightly inferior to those of BasicVSR++\\cite{Chan:BasicVSR:2022}", "next_context": ", which employs CNN and was introduced around the same time, both methods significantly enhance detail and sharpness compared to previous approaches, albeit not in real time."}], "importance_score": 1.0}, "Fuoli:Fast:2023": {"bib_key": "Fuoli:Fast:2023", "bib_title": "Fast Online Video Super-Resolution With Deformable Attention Pyramid", "bib_author ": "Fuoli, Dario", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Upscaling imagery: super-resolution (SR)", "prev_context": "To address this limitation, the Deformable Attention Pyramid\\cite{Fuoli:Fast:2023}", "next_context": "has been introduced, offering slightly lower quality but a speed-up of over 3\\times."}], "importance_score": 1.0}, "Pan:Deep:2023": {"bib_key": "Pan:Deep:2023", "bib_title": "Deep Discriminative Spatial and Temporal Network for Efficient Video Deblurring", "bib_author ": "Pan, Jinshan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "i)\\textbf{Deblurring}: A lightweight deep CNN model was recently proposed in\\cite{Pan:Deep:2023}", "next_context": ", where a new discriminative temporal feature fusion has been introduced to select the most useful spatial and temporal features from adjacent frames."}], "importance_score": 1.0}, "Choi:Exploring:2023": {"bib_key": "Choi:Exploring:2023", "bib_title": "Exploring Positional Characteristics of Dual-Pixel Data for Camera Autofocus", "bib_author ": "Choi, Myungsub", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "\\cite{Choi:Exploring:2023}", "next_context": "."}], "importance_score": 1.0}, "Yang:K3DN:2023": {"bib_key": "Yang:K3DN:2023", "bib_title": "{K3DN: Disparity}-Aware Kernel Estimation for Dual-Pixel Defocus Deblurring", "bib_author ": "Yang, Yan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "Remarkably, their model achieves comparable results to previous state-of-the-art methods while being more lightweight\\cite{Yang:K3DN:2023}", "next_context": "."}], "importance_score": 1.0}, "Atmospheric:2023": {"bib_key": "Atmospheric:2023", "bib_title": "Atmospheric turbulence removal with complex-valued convolutional neural network", "bib_author ": "Nantheera Anantrasirichai", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "iv)\\textbf{Mitigating atmospheric turbulence}: Similar to dehazing, physics-inspired models have been widely developed to remove turbulence distortion\\cite{Jaiswal:Physics:2023,Jiang:NeRT:2023}, while complex-valued CNNs have been proposed to exploit phase information\\cite{Atmospheric:2023}", "next_context": "."}], "importance_score": 1.0}, "Hill2025": {"bib_key": "Hill2025", "bib_title": "Deep Learning Techniques for Atmospheric Turbulence Removal: A Review", "bib_author ": "Paul Hill", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Restoration", "prev_context": "A recent review can be found in\\cite{Hill2025}", "next_context": "."}], "importance_score": 1.0}, "zheng:pluralistic:2019": {"bib_key": "zheng:pluralistic:2019", "bib_title": "Pluralistic Image Completion", "bib_author ": "Zheng, Chuanxia", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Inpainting", "prev_context": "This means users can now mask large areas of an image, and AI tools generate multiple results for users to choose from, a technique known as pluralistic inpainting\\cite{zheng:pluralistic:2019}", "next_context": "."}], "importance_score": 1.0}, "Zhang:DINet:2023": {"bib_key": "Zhang:DINet:2023", "bib_title": "{DINet: Deformation} inpainting network for realistic face visually dubbing on high resolution video", "bib_author ": "Zhang, Zhimeng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Inpainting", "prev_context": "DINet\\cite{Zhang:DINet:2023}", "next_context": "replaces the mouth area to synchronize with a new language being spoken."}], "importance_score": 1.0}, "quan:deep:2024": {"bib_key": "quan:deep:2024", "bib_title": "Deep Learning-Based Image and Video Inpainting: A Survey", "bib_author ": "Quan, W.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Inpainting", "prev_context": "A survey of learning-based image and video inpainting,  including CNN, VAE, GAN, transformers and diffusion models can be found in\\cite{quan:deep:2024}", "next_context": "."}], "importance_score": 1.0}, "Karim:Current:2023": {"bib_key": "Karim:Current:2023", "bib_title": "Current advances and future perspectives of image fusion: A comprehensive review", "bib_author ": "Shahid Karim", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Image Fusion", "prev_context": "For an in-depth review, refer to recent works in\\cite{Karim:Current:2023, Zhang:Visible:2023}", "next_context": "."}], "importance_score": 0.5}, "Tous:Lester:2024": {"bib_key": "Tous:Lester:2024", "bib_title": "{Lester: Rotoscope} animation through video object segmentation and tracking", "bib_author ": "Ruben Tous", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Content enhancement and post production workflows", "subsubsection": "Editing and Visual Special Effects (VFX)", "prev_context": "Generative AI has also simplified and accelerated automated processes, such as rotoscoping\\cite{Tous:Lester:2024}", "next_context": ", an animation technique where animators trace over motion picture footage frame by frame to create realistic action."}], "importance_score": 1.0}, "Baranchuk:label:2022": {"bib_key": "Baranchuk:label:2022", "bib_title": "Label-Efficient Semantic Segmentation with Diffusion Models", "bib_author ": "Dmitry Baranchuk", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "", "next_context": "have investigated semantic representation, and found DMs outperform other few-shot learning approaches."}], "importance_score": 1.0}, "Lin:Feature:2024": {"bib_key": "Lin:Feature:2024", "bib_title": "Feature Denoising for Low-Light Instance Segmentation Using Weighted Non-Local Blocks", "bib_author ": "Lin, Joanne", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "Recently, the work integrated with a non-local means block in\\cite{Lin:Feature:2024}", "next_context": "has shown a significant improvement for instant segmentation in low-light scenes."}], "importance_score": 1.0}, "Goel:Interactive:2023": {"bib_key": "Goel:Interactive:2023", "bib_title": "Interactive Segmentation of Radiance Fields", "bib_author ": "Goel, Rahul", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Segmentation", "prev_context": "In radiance fields, earlier segmentation methods required additional modules to separate objects from the background, such as using k-means clustering as demonstrated in\\cite{Goel:Interactive:2023}", "next_context": "."}], "importance_score": 1.0}, "Ren:Faster:2027": {"bib_key": "Ren:Faster:2027", "bib_title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks", "bib_author ": "Ren, Shaoqing", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "The approach achieves comparable results to an optimized Faster R-CNN\\cite{Ren:Faster:2027}", "next_context": ", introduced in 2015."}], "importance_score": 1.0}, "Zou:object:2023": {"bib_key": "Zou:object:2023", "bib_title": "Object Detection in 20 Years: A Survey", "bib_author ": "Zou, Zhengxia", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "A survey until 2022 reported in\\cite{Zou:object:2023}", "next_context": "show that Deformable DETR and Swin Transformer\\cite{Liu:Swin:2021}outperform pure CNN-based YOLOv4\\cite{bochkovskiy2020yolov4}."}], "importance_score": 1.0}, "bochkovskiy2020yolov4": {"bib_key": "bochkovskiy2020yolov4", "bib_title": "{YOLOv4: Optimal} speed and accuracy of object detection", "bib_author ": "Bochkovskiy, A.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "A survey until 2022 reported in\\cite{Zou:object:2023}show that Deformable DETR and Swin Transformer\\cite{Liu:Swin:2021}outperform pure CNN-based YOLOv4\\cite{bochkovskiy2020yolov4}", "next_context": "."}], "importance_score": 1.0}, "lv2:detrs:2024": {"bib_key": "lv2:detrs:2024", "bib_title": "DETRs Beat YOLOs on Real-time Object Detection", "bib_author ": "Yian Zhao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "Later, RT-DETR\\cite{lv2:detrs:2024}", "next_context": "improves inference speed by decoupling the intra-scale interaction and cross-scale fusion of features with different scales."}], "importance_score": 1.0}, "wang:yolov10:2024": {"bib_key": "wang:yolov10:2024", "bib_title": "{YOLOv10: Real}-Time End-to-End Object Detection", "bib_author ": "Wang, Ao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "Recently, YOLOv10\\cite{wang:yolov10:2024}", "next_context": "has been released."}], "importance_score": 1.0}, "Li:Transformer:2023": {"bib_key": "Li:Transformer:2023", "bib_title": "Transformer for object detection: Review and benchmark", "bib_author ": "Yong Li", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "A review of transformer-based methods for object detection can be found in\\cite{Li:Transformer:2023}", "next_context": "."}], "importance_score": 1.0}, "Wu:datasetDM:2023": {"bib_key": "Wu:datasetDM:2023", "bib_title": "{DatasetDM: Synthesizing} Data with Perception Annotations Using Diffusion Models", "bib_author ": "Wu, Weijia", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "While DMs are primarily used to generate synthetic datasets\\cite{Wu:datasetDM:2023, Fang:Data:2024}", "next_context": ", they have also been demonstrated to function as zero-shot classifiers by Li et al."}], "importance_score": 0.5}, "Fang:Data:2024": {"bib_key": "Fang:Data:2024", "bib_title": "Data Augmentation for Object Detection via Controllable Diffusion Models", "bib_author ": "Fang, Haoyang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Detection and recognition", "prev_context": "While DMs are primarily used to generate synthetic datasets\\cite{Wu:datasetDM:2023, Fang:Data:2024}", "next_context": ", they have also been demonstrated to function as zero-shot classifiers by Li et al."}], "importance_score": 0.5}, "Kugarajeevan:Transformers:2023": {"bib_key": "Kugarajeevan:Transformers:2023", "bib_title": "Transformers in Single Object Tracking: An Experimental Survey", "bib_author ": "Kugarajeevan, Janani", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "An experimental survey cited in\\cite{Kugarajeevan:Transformers:2023}", "next_context": "reveals that transformer-based methods consistently rank at the top of the leaderboard across various datasets."}], "importance_score": 1.0}, "cheng:xmem:2022": {"bib_key": "cheng:xmem:2022", "bib_title": "Xmem: Long-term video object segmentation with an Atkinson-Shiffrin memory model", "bib_author ": "Cheng, Ho Kei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "TAM combines SAM\\cite{Kirillov:SAM:2023}and XMem\\cite{cheng:xmem:2022}", "next_context": ", offering tracking and segmentation performance on the human-selected target."}], "importance_score": 1.0}, "ge:yolox:2021": {"bib_key": "ge:yolox:2021", "bib_title": "YOLOX: Exceeding YOLO Series in 2021", "bib_author ": "Ge, Zheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Information Extraction and Understanding", "subsubsection": "Tracking", "prev_context": "MOTRv2\\cite{Zhang:MOTRv2:2023}combines YOLOX\\cite{ge:yolox:2021}", "next_context": "for object recognition and MOTR\\cite{zeng:motr:2022}for tracking, outperforming TrackFormer by 20\\%."}], "importance_score": 1.0}, "Azzarelli2024": {"bib_key": "Azzarelli2024", "bib_title": "Exploring Dynamic Novel View Synthesis Technologies for Cinematography", "bib_author ": "Adrian Azzarelli", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "They have hence gained lots of attention in cinematography\\cite{Azzarelli2024}", "next_context": ", as it potentially safe time and cost particularly for outdoor shooting."}], "importance_score": 1.0}, "schoenberger:sfm:2016": {"bib_key": "schoenberger:sfm:2016", "bib_title": "Structure-from-Motion Revisited", "bib_author ": "Sch\\\"{o}nberger, Johannes Lutz", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Neural Radiance Fields", "prev_context": "\\ref{fig:3Drepresentation}(a)), the camera positions and orientations are typically estimated from a series of 2D images using techniques like feature-mapping and Structure-from-Motion (SfM), as demonstrated in\\cite{schoenberger:sfm:2016}", "next_context": "."}], "importance_score": 1.0}, "10521791": {"bib_key": "10521791", "bib_title": "3D Gaussian Splatting as New Era: A Survey", "bib_author ": "Fei, Ben", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Gaussian Splatting", "prev_context": "A survey of 3D-GS can be found in\\cite{10521791}", "next_context": "."}], "importance_score": 1.0}, "jiang:vrgs:2024": {"bib_key": "jiang:vrgs:2024", "bib_title": "{VR-GS: A} Physical Dynamics-Aware Interactive Gaussian Splatting System in Virtual Reality", "bib_author ": "Jiang, Ying", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "3D Reconstruction and Rendering", "subsubsection": "Gaussian Splatting", "prev_context": "VR-GS\\cite{jiang:vrgs:2024}", "next_context": "offers intuitive and interactive physics-based game-play with deformable virtual objects and realistic environments represented with 3D-GS."}], "importance_score": 1.0}, "Bull:intelligent:2021": {"bib_key": "Bull:intelligent:2021", "bib_title": "Intelligent image and video compression: communicating pictures", "bib_author ": "Bull, David", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": null, "prev_context": "Data compression plays an important role in various applications of creative technologies, which effectively reduces memory space and bandwidth requirements during signal storage and transmission\\cite{Bull:intelligent:2021}", "next_context": "."}], "importance_score": 1.0}, "balle2016density": {"bib_key": "balle2016density", "bib_title": "Density modeling of images using a generalized normalization transformation", "bib_author ": "Ball{\\'e}, Johannes", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "Since the first neural image codec\\cite{balle2016density}", "next_context": "was proposed in 2016, numerous learning-based image compression methods have been developed, with significant performance improvements reported\\cite{balle2018variational,cheng2020learned}."}], "importance_score": 1.0}, "balle2018variational": {"bib_key": "balle2018variational", "bib_title": "Variational image compression with a scale hyperprior", "bib_author ": "Ball{\\'e}, Johannes", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "Since the first neural image codec\\cite{balle2016density}was proposed in 2016, numerous learning-based image compression methods have been developed, with significant performance improvements reported\\cite{balle2018variational,cheng2020learned}", "next_context": "."}], "importance_score": 0.5}, "cheng2020learned": {"bib_key": "cheng2020learned", "bib_title": "Learned image compression with discretized gaussian mixture likelihoods and attention modules", "bib_author ": "Cheng, Zhengxue", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "Since the first neural image codec\\cite{balle2016density}was proposed in 2016, numerous learning-based image compression methods have been developed, with significant performance improvements reported\\cite{balle2018variational,cheng2020learned}", "next_context": "."}], "importance_score": 0.5}, "agustsson2019generative": {"bib_key": "agustsson2019generative", "bib_title": "Generative adversarial networks for extreme learned image compression", "bib_author ": "Agustsson, Eirikur", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "Early works\\cite{agustsson2019generative,mentzer2020high}", "next_context": "employ GANs to generate more photo realistic results with improved visual quality."}], "importance_score": 0.5}, "mentzer2020high": {"bib_key": "mentzer2020high", "bib_title": "High-fidelity generative image compression", "bib_author ": "Mentzer, Fabian", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "Early works\\cite{agustsson2019generative,mentzer2020high}", "next_context": "employ GANs to generate more photo realistic results with improved visual quality."}], "importance_score": 0.5}, "Bovik_MSSSIM": {"bib_key": "Bovik_MSSSIM", "bib_title": "Multi-scale structural similarity for image quality assessment", "bib_author ": "Wang, Z.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "Although these models fail to outperform conventional, CNN-based or transformer-based approaches, when distortion-based quality metrics, e.g., PSNR, are used for performance evaluation, they have been reported to perform well when perceptual quality models, such as MS-SSIM\\cite{Bovik_MSSSIM}", "next_context": "and VMAF\\cite{VMAFblog}, or subjective tests are employed to measure perceived video quality."}], "importance_score": 1.0}, "VMAFblog": {"bib_key": "VMAFblog", "bib_title": "{The NETFLIX tech blog: Toward a practical perceptual video quality metric}", "bib_author ": "Z. Li", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "Although these models fail to outperform conventional, CNN-based or transformer-based approaches, when distortion-based quality metrics, e.g., PSNR, are used for performance evaluation, they have been reported to perform well when perceptual quality models, such as MS-SSIM\\cite{Bovik_MSSSIM}and VMAF\\cite{VMAFblog}", "next_context": ", or subjective tests are employed to measure perceived video quality."}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "These hand-crafted quality models have also been combined with features within a regression-based framework in order to achieve more accurate prediction performance - VMAF is one of such examples\\cite{VMAFblog}", "next_context": "."}], "importance_score": 2.0}, "clic": {"bib_key": "clic", "bib_title": "{ Challenge on Learned Image Compression}", "bib_author ": "", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "One of the most well-known ones is the Challenge on Learned Image Compression (CLIC)\\cite{clic}", "next_context": ", and in its latest competition, the best performing learned image codec offers up to 0.6dB PSNR gain over VTM (version 22.2, All Intra) at similar bitrates, which is based on an autoencoder architecture with latent refinement and perceptual losses."}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Performance and main challenges", "prev_context": "The Sixth Challenge on Learned Image Compression (CLIC)\\cite{clic}", "next_context": "associated with the Data Compression Conference 2024 is one of the latest examples which includes two quality assessment tracks for image and video compression."}], "importance_score": 2.0}, "ascenso2023jpeg": {"bib_key": "ascenso2023jpeg", "bib_title": "The {JPEG AI standard}: Providing efficient human and machine visual data consumption", "bib_author ": "Ascenso, Joao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "To support the deployment of neural image codecs, the International Organization for Standardization (ISO)/International Electrotechnical Commission(IEC) have jointly started to develop a royalty-free learned image coding standard, denoted as JPEG AI\\cite{ascenso2023jpeg}", "next_context": ", which aims to offer significant performance improvement over existing standards for both human and machine vision tasks."}], "importance_score": 1.0}, "JPEGAIN100634": {"bib_key": "JPEGAIN100634", "bib_title": "{JPEG AI: Future Plans and Timeline v2}", "bib_author ": "Elena Alshina", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "The Call for Proposals of JPEG AI was published in 2022, while the Working Draft and the Committee Draft outlining its core coding system were released in 2023\\cite{JPEGAIN100634}", "next_context": ", with its first version published in October 2024\\cite{JPEGAIN100634}."}, {"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "The Call for Proposals of JPEG AI was published in 2022, while the Working Draft and the Committee Draft outlining its core coding system were released in 2023\\cite{JPEGAIN100634}, with its first version published in October 2024\\cite{JPEGAIN100634}", "next_context": "."}], "importance_score": 2.0}, "JPEGAIM101081": {"bib_key": "JPEGAIM101081", "bib_title": "{JPEG AI sw v4.x status}", "bib_author ": "Alex", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Image Compression", "prev_context": "JPEG AI follows the same framework (the auto-encoder structure) as most existing neural image codecs, and its test model JPEG AI VM (version 4.3) has been reported to achieve up to 28.5\\%coding gains over VVC VTM (All Intra mode)\\cite{JPEGAIM101081}", "next_context": "."}], "importance_score": 1.0}, "li2021deepqtmt": {"bib_key": "li2021deepqtmt", "bib_title": "{DeepQTMT}: A deep learning approach for fast {QTMT-based CU} partition of intra-mode {VVC}", "bib_author ": "Li, Tianyi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "The enhancement of conventional coding tools focuses on employing deep learning techniques to improve the performance of a (or multiple) coding modules in a standard-applicant codec, including intra prediction\\cite{li2021deepqtmt}", "next_context": ", inter prediction\\cite{jin2021deep}, in-loop filtering\\cite{feng2024low}, post filtering\\cite{zhang2023wcdann}and resolution re-sampling\\cite{wang2023compression}."}], "importance_score": 1.0}, "jin2021deep": {"bib_key": "jin2021deep", "bib_title": "Deep affine motion compensation network for inter prediction in {VVC}", "bib_author ": "Jin, Dengchao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "The enhancement of conventional coding tools focuses on employing deep learning techniques to improve the performance of a (or multiple) coding modules in a standard-applicant codec, including intra prediction\\cite{li2021deepqtmt}, inter prediction\\cite{jin2021deep}", "next_context": ", in-loop filtering\\cite{feng2024low}, post filtering\\cite{zhang2023wcdann}and resolution re-sampling\\cite{wang2023compression}."}], "importance_score": 1.0}, "feng2024low": {"bib_key": "feng2024low", "bib_title": "Low Complexity In-Loop Filter for {VVC} Based on Convolution and Transformer", "bib_author ": "Feng, Zhen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "The enhancement of conventional coding tools focuses on employing deep learning techniques to improve the performance of a (or multiple) coding modules in a standard-applicant codec, including intra prediction\\cite{li2021deepqtmt}, inter prediction\\cite{jin2021deep}, in-loop filtering\\cite{feng2024low}", "next_context": ", post filtering\\cite{zhang2023wcdann}and resolution re-sampling\\cite{wang2023compression}."}], "importance_score": 1.0}, "zhang2023wcdann": {"bib_key": "zhang2023wcdann", "bib_title": "{WCDANN}: A Lightweight {CNN} Post-Processing Filter for {VVC-based} Video Compression", "bib_author ": "Zhang, Hao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "The enhancement of conventional coding tools focuses on employing deep learning techniques to improve the performance of a (or multiple) coding modules in a standard-applicant codec, including intra prediction\\cite{li2021deepqtmt}, inter prediction\\cite{jin2021deep}, in-loop filtering\\cite{feng2024low}, post filtering\\cite{zhang2023wcdann}", "next_context": "and resolution re-sampling\\cite{wang2023compression}."}], "importance_score": 1.0}, "wang2023compression": {"bib_key": "wang2023compression", "bib_title": "Compression-aware video super-resolution", "bib_author ": "Wang, Yingwei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "The enhancement of conventional coding tools focuses on employing deep learning techniques to improve the performance of a (or multiple) coding modules in a standard-applicant codec, including intra prediction\\cite{li2021deepqtmt}, inter prediction\\cite{jin2021deep}, in-loop filtering\\cite{feng2024low}, post filtering\\cite{zhang2023wcdann}and resolution re-sampling\\cite{wang2023compression}", "next_context": "."}], "importance_score": 1.0}, "li2023designs": {"bib_key": "li2023designs", "bib_title": "Designs and Implementations in Neural Network-based Video Coding", "bib_author ": "Li, Yue", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "To facilitate efficient integration, the MPEG Joint Video Experts Team (JVET)  has built a test model in 2022 based on VTM 11, named Neural Network-based Video Coding (NNVC)\\cite{li2023designs}", "next_context": ", with its latest version NNVC-7.1 containing two major learning-based coding tools, neural-network based intra prediction and in-loop filtering, which has achieved an up to 13\\%coding gain over VTM 11 (Random Access mode)\\cite{JVET-AG0014}."}], "importance_score": 1.0}, "JVET-AG0014": {"bib_key": "JVET-AG0014", "bib_title": "{NNVC} software development {AhG14} ", "bib_author ": "F. Galpin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "To facilitate efficient integration, the MPEG Joint Video Experts Team (JVET)  has built a test model in 2022 based on VTM 11, named Neural Network-based Video Coding (NNVC)\\cite{li2023designs}, with its latest version NNVC-7.1 containing two major learning-based coding tools, neural-network based intra prediction and in-loop filtering, which has achieved an up to 13\\%coding gain over VTM 11 (Random Access mode)\\cite{JVET-AG0014}", "next_context": "."}], "importance_score": 1.0}, "joshi2023switchable": {"bib_key": "joshi2023switchable", "bib_title": "Switchable CNNs for in-loop restoration and super-resolution for AV2", "bib_author ": "Joshi, Urvang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "The latest proposals focus on the trade-off between performance and complexity, with one of them based on inloop filtering and super-resolution, which achieves an average BD-rate saving of 3.9\\%(in PSNR) over AVM, the test model of AV2, but only requires a much lower computational complexity (below 1.5kMACs/pixel)\\cite{joshi2023switchable}", "next_context": "."}], "importance_score": 1.0}, "kathariya2023joint": {"bib_key": "kathariya2023joint", "bib_title": "Joint Pixel and Frequency Feature Learning and Fusion via Channel-wise Transformer for High-Efficiency Learned In-Loop Filter in VVC", "bib_author ": "Kathariya, Birendra", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "More recently, research has been conducted to further improve the performance of these learning-based coding tools utilizing more advanced network architectures, including ViTs\\cite{kathariya2023joint}", "next_context": ", and diffusion models\\cite{li2024extreme}."}], "importance_score": 1.0}, "chadha2021deep": {"bib_key": "chadha2021deep", "bib_title": "Deep perceptual preprocessing for video coding", "bib_author ": "Chadha, Aaron", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "There are also investigations on applying preprocessing before compression\\cite{chadha2021deep,tan2024joint}", "next_context": ", where the training of the deep preprocessors is based on proxy video codecs and/or rate-distortion loss functions to simulate the behavior of conventional video coding algorithms."}], "importance_score": 0.5}, "tan2024joint": {"bib_key": "tan2024joint", "bib_title": "Joint Frame-Level and Block-Level Rate-Perception Optimized Preprocessing for Video Coding", "bib_author ": "Tan, Huajie", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "There are also investigations on applying preprocessing before compression\\cite{chadha2021deep,tan2024joint}", "next_context": ", where the training of the deep preprocessors is based on proxy video codecs and/or rate-distortion loss functions to simulate the behavior of conventional video coding algorithms."}], "importance_score": 0.5}, "lu2019dvc": {"bib_key": "lu2019dvc", "bib_title": "{DVC}: An end-to-end deep video compression framework", "bib_author ": "Lu, Guo", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "The performance of these neural video codecs has improved significantly in the last five years, since the first attempt, DVC\\cite{lu2019dvc}", "next_context": ", was published, which only matches the performance of a fast implementation of H.264 (x264)."}], "importance_score": 1.0}, "li2024neural": {"bib_key": "li2024neural", "bib_title": "Neural video compression with feature modulation", "bib_author ": "Li, Jiahao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "When this paper was written, some latest well-performing learned video coding algorithms (e.g., DCVC-FM\\cite{li2024neural}", "next_context": "and DCVC-LCG\\cite{Qi2024longterm}) have already been able to compete or even outperform the state-of-the-art standard codecs, such as VVC VTM under certain coding configurations."}], "importance_score": 1.0}, "Qi2024longterm": {"bib_key": "Qi2024longterm", "bib_title": "Long-term Temporal Context Gathering for Neural Video Compression", "bib_author ": "Linfeng Qi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "When this paper was written, some latest well-performing learned video coding algorithms (e.g., DCVC-FM\\cite{li2024neural}and DCVC-LCG\\cite{Qi2024longterm}", "next_context": ") have already been able to compete or even outperform the state-of-the-art standard codecs, such as VVC VTM under certain coding configurations."}], "importance_score": 1.0}, "hu2021fvc": {"bib_key": "hu2021fvc", "bib_title": "{FVC}: A new framework towards deep video compression in feature space", "bib_author ": "Hu, Zhihao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "These learning-based methods often focus on enhancement from different perspectives, including feature space conditional coding (e.g., FVC~\\cite{hu2021fvc}", "next_context": "and DCVC\\cite{li2021deep}), instance adaptation~\\cite{khani2021efficient,yang2024parameter}, and motion estimation (e.g., DCVC-DC~\\cite{li2023neural})."}], "importance_score": 1.0}, "li2021deep": {"bib_key": "li2021deep", "bib_title": "{Deep contextual video compression}", "bib_author ": "Li, Jiahao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "These learning-based methods often focus on enhancement from different perspectives, including feature space conditional coding (e.g., FVC~\\cite{hu2021fvc}and DCVC\\cite{li2021deep}", "next_context": "), instance adaptation~\\cite{khani2021efficient,yang2024parameter}, and motion estimation (e.g., DCVC-DC~\\cite{li2023neural})."}], "importance_score": 1.0}, "khani2021efficient": {"bib_key": "khani2021efficient", "bib_title": "Efficient video compression via content-adaptive super-resolution", "bib_author ": "Khani, Mehrdad", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "These learning-based methods often focus on enhancement from different perspectives, including feature space conditional coding (e.g., FVC~\\cite{hu2021fvc}and DCVC\\cite{li2021deep}), instance adaptation~\\cite{khani2021efficient,yang2024parameter}", "next_context": ", and motion estimation (e.g., DCVC-DC~\\cite{li2023neural})."}], "importance_score": 0.5}, "yang2024parameter": {"bib_key": "yang2024parameter", "bib_title": "Parameter-Efficient Instance-Adaptive Neural Video Compression", "bib_author ": "Oh, Seungjun", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "These learning-based methods often focus on enhancement from different perspectives, including feature space conditional coding (e.g., FVC~\\cite{hu2021fvc}and DCVC\\cite{li2021deep}), instance adaptation~\\cite{khani2021efficient,yang2024parameter}", "next_context": ", and motion estimation (e.g., DCVC-DC~\\cite{li2023neural})."}], "importance_score": 0.5}, "li2023neural": {"bib_key": "li2023neural", "bib_title": "Neural Video Compression with Diverse Contexts", "bib_author ": "Jiahao Li", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "These learning-based methods often focus on enhancement from different perspectives, including feature space conditional coding (e.g., FVC~\\cite{hu2021fvc}and DCVC\\cite{li2021deep}), instance adaptation~\\cite{khani2021efficient,yang2024parameter}, and motion estimation (e.g., DCVC-DC~\\cite{li2023neural}", "next_context": ")."}], "importance_score": 1.0}, "ho2022canf": {"bib_key": "ho2022canf", "bib_title": "{CANF-VC}: Conditional augmented normalizing flows for video compression", "bib_author ": "Ho, Yung-Han", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "New architectures have also been proposed such as CANF-VC~\\cite{ho2022canf}", "next_context": "based on a video generative model, MTMT\\cite{xiang2022mimt}using a masked image modeling transformer based entropy model and VCT\\cite{mentzer2022vct}based on a video compression transformer."}], "importance_score": 1.0}, "guo2023evc": {"bib_key": "guo2023evc", "bib_title": "EVC: Towards Real-Time Neural Image Compression with Mask Decay", "bib_author ": "Guo-Hua, Wang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "To address this issue, researchers attempted to achieve complexity reduction while maintaining the coding performance through model pruning and knowledge distillation~\\cite{guo2023evc,peng2024accelerating}", "next_context": "."}], "importance_score": 0.5}, "peng2024accelerating": {"bib_key": "peng2024accelerating", "bib_title": "Accelerating learnt video codecs with gradient decay and layer-wise distillation", "bib_author ": "Peng, Tianhao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "To address this issue, researchers attempted to achieve complexity reduction while maintaining the coding performance through model pruning and knowledge distillation~\\cite{guo2023evc,peng2024accelerating}", "next_context": "."}], "importance_score": 0.5}, "nawala2024bvi": {"bib_key": "nawala2024bvi", "bib_title": "{BVI-AOM}: A New Training Dataset for Deep Video Compression Optimization", "bib_author ": "Nawa{\\l}a, Jakub", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "It should be noted that the neural codecs mentioned above are typically trained offline with diverse video content\\cite{nawala2024bvi}", "next_context": ", and deployed online for inference."}], "importance_score": 1.0}, "li2022nerv": {"bib_key": "li2022nerv", "bib_title": "{E-NeRV}: Expedite neural video representation with disentangled spatial-temporal context", "bib_author ": "Li, Zizhang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "~\\cite{chen2021nerv}, patch~\\cite{bai2023ps}or disentangled spatial/grid coordinates~\\cite{li2022nerv}", "next_context": "as model input, while content-based approaches~\\cite{kwan2024hinerv,kim2024c3,leguay2024cool}have content-specific embedding as inputs."}], "importance_score": 1.0}, "bossen2023vtmctc": {"bib_key": "bossen2023vtmctc", "bib_title": "{VTM} Common Test Conditions and Software Reference Configurations for {SDR} Video", "bib_author ": "F. Bossen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Video Compression", "prev_context": "To address this limitation, efforts have been made~\\cite{gao2024pnvc}towards more practical INR-based video compression (such as the Low Delay and Random Access modes in VVC VTM\\cite{bossen2023vtmctc}", "next_context": ") by combining pre-training and online model overfitting."}], "importance_score": 1.0}, "zeghidour2021soundstream": {"bib_key": "zeghidour2021soundstream", "bib_title": "Soundstream: An end-to-end neural audio codec", "bib_author ": "Zeghidour, Neil", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Audio Compression", "prev_context": "SoundStream\\cite{zeghidour2021soundstream}", "next_context": "is one of such models, which can encode audio content at various bitrate."}], "importance_score": 1.0}, "kumar2024high": {"bib_key": "kumar2024high", "bib_title": "High-fidelity audio compression with improved rvqgan", "bib_author ": "Kumar, Rithesh", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Audio Compression", "prev_context": "A more advanced universal model has been further developed\\cite{kumar2024high}", "next_context": "based on improved adversarial and reconstruction losses, which can compress different types of audio."}], "importance_score": 1.0}, "siuzdak2024snac": {"bib_key": "siuzdak2024snac", "bib_title": "{SNAC}: Multi-Scale Neural Audio Codec", "bib_author ": "Hubert Siuzdak", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Audio Compression", "prev_context": "RVQ has also been extended from a single scale to multiple scales\\cite{siuzdak2024snac}", "next_context": ", which performs hierarchical quantization at variable frame rates."}], "importance_score": 1.0}, "yang2024uniaudio": {"bib_key": "yang2024uniaudio", "bib_title": "{UniAudio 1.5: Large} Language Model-Driven Audio Codec is A Few-Shot Audio Task Learner", "bib_author ": "Dongchao Yang", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Audio Compression", "prev_context": "UniAudio 1.5\\cite{yang2024uniaudio}", "next_context": "is one of such attempts, which converts an audio into the textural space, which can be represented by a pre-trained LLM that shares a similar backbone of UniAudio\\cite{yang2023uniaudio}, an universal audio foundation model."}], "importance_score": 1.0}, "yang2023uniaudio": {"bib_key": "yang2023uniaudio", "bib_title": "{UniAudio}: An audio foundation model toward universal audio generation", "bib_author ": "Yang, Dongchao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Data Compression", "subsubsection": "Audio Compression", "prev_context": "UniAudio 1.5\\cite{yang2024uniaudio}is one of such attempts, which converts an audio into the textural space, which can be represented by a pre-trained LLM that shares a similar backbone of UniAudio\\cite{yang2023uniaudio}", "next_context": ", an universal audio foundation model."}], "importance_score": 1.0}, "zhai2020perceptual": {"bib_key": "zhai2020perceptual", "bib_title": "Perceptual image quality assessment: a survey", "bib_author ": "Zhai, Guangtao", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": null, "prev_context": "\\cite{zhai2020perceptual,zheng2024video,zhang2024quality}", "next_context": "."}], "importance_score": 0.3333333333333333}, "zheng2024video": {"bib_key": "zheng2024video", "bib_title": "Video Quality Assessment: A Comprehensive Survey", "bib_author ": "Zheng, Qi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": null, "prev_context": "\\cite{zhai2020perceptual,zheng2024video,zhang2024quality}", "next_context": "."}], "importance_score": 0.3333333333333333}, "zhang2024quality": {"bib_key": "zhang2024quality", "bib_title": "Quality assessment in the era of large models: A survey", "bib_author ": "Zhang, Zicheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": null, "prev_context": "\\cite{zhai2020perceptual,zheng2024video,zhang2024quality}", "next_context": "."}], "importance_score": 0.3333333333333333}, "Bovik_SSIM": {"bib_key": "Bovik_SSIM", "bib_title": "Image quality assessment: from error visibility to structural similarity", "bib_author ": "Z. {Wang}", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants\\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}", "next_context": "), distortion\\cite{chandler2007vsnr,larson2010most,STMAD}, and artifacts\\cite{ou2010perceptual,zhu2014no,zhang2015perception}."}], "importance_score": 0.3333333333333333}, "wang2003multiscale": {"bib_key": "wang2003multiscale", "bib_title": "Multiscale structural similarity for image quality assessment", "bib_author ": "Wang, Zhou", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants\\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}", "next_context": "), distortion\\cite{chandler2007vsnr,larson2010most,STMAD}, and artifacts\\cite{ou2010perceptual,zhu2014no,zhang2015perception}."}], "importance_score": 0.3333333333333333}, "rehman2015display": {"bib_key": "rehman2015display", "bib_title": "Display device-adapted video quality-of-experience assessment", "bib_author ": "Rehman, Abdul", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants\\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}", "next_context": "), distortion\\cite{chandler2007vsnr,larson2010most,STMAD}, and artifacts\\cite{ou2010perceptual,zhu2014no,zhang2015perception}."}], "importance_score": 0.3333333333333333}, "chandler2007vsnr": {"bib_key": "chandler2007vsnr", "bib_title": "{VSNR}: A wavelet-based visual signal-to-noise ratio for natural images", "bib_author ": "Ch", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants\\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}), distortion\\cite{chandler2007vsnr,larson2010most,STMAD}", "next_context": ", and artifacts\\cite{ou2010perceptual,zhu2014no,zhang2015perception}."}], "importance_score": 0.3333333333333333}, "larson2010most": {"bib_key": "larson2010most", "bib_title": "Most apparent distortion: full-reference image quality assessment and the role of strategy", "bib_author ": "Larson, Eric C", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants\\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}), distortion\\cite{chandler2007vsnr,larson2010most,STMAD}", "next_context": ", and artifacts\\cite{ou2010perceptual,zhu2014no,zhang2015perception}."}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{sheikh2006astatistical}, CSIQ\\cite{larson2010most}", "next_context": ", TID2013\\cite{ponomarenko2013color}, PieAPP and PIPAL, while video quality databases such as LIVE-VQA~\\cite{seshadrinathan2010study}, KoNViD-1K~\\cite{hosu2017konstanz}, YouTube UGC~\\cite{wang2019youtube}and LIVE-VQC~\\cite{sinno2018large}are typically employed for benchmarking in the literature."}], "importance_score": 1.3333333333333333}, "STMAD": {"bib_key": "STMAD", "bib_title": "A spatiotemporal most-apparent-distortion model for video quality assessment", "bib_author ": "P. V. {Vu}", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants\\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}), distortion\\cite{chandler2007vsnr,larson2010most,STMAD}", "next_context": ", and artifacts\\cite{ou2010perceptual,zhu2014no,zhang2015perception}."}], "importance_score": 0.3333333333333333}, "ou2010perceptual": {"bib_key": "ou2010perceptual", "bib_title": "Perceptual quality assessment of video considering both frame rate and quantization artifacts", "bib_author ": "Ou, Yen-Fu", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants\\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}), distortion\\cite{chandler2007vsnr,larson2010most,STMAD}, and artifacts\\cite{ou2010perceptual,zhu2014no,zhang2015perception}", "next_context": "."}], "importance_score": 0.3333333333333333}, "zhu2014no": {"bib_key": "zhu2014no", "bib_title": "No-reference video quality assessment based on artifact measurement and statistical analysis", "bib_author ": "Zhu, Kongfeng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants\\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}), distortion\\cite{chandler2007vsnr,larson2010most,STMAD}, and artifacts\\cite{ou2010perceptual,zhu2014no,zhang2015perception}", "next_context": "."}], "importance_score": 0.3333333333333333}, "zhang2015perception": {"bib_key": "zhang2015perception", "bib_title": "A perception-based hybrid model for video quality assessment", "bib_author ": "Zhang, Fan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants\\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}), distortion\\cite{chandler2007vsnr,larson2010most,STMAD}, and artifacts\\cite{ou2010perceptual,zhu2014no,zhang2015perception}", "next_context": "."}], "importance_score": 0.3333333333333333}, "helmholtz1896handbook": {"bib_key": "helmholtz1896handbook", "bib_title": "Handbook of Physiological Optics", "bib_author ": "H. L. F. von Helmholtz", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "In many cases, the extract features are further processed by models that simulate texture masking\\cite{helmholtz1896handbook}", "next_context": ", contract sensitivity\\cite{kelly1977visual}, and saliency\\cite{itti2001computational}."}], "importance_score": 1.0}, "kelly1977visual": {"bib_key": "kelly1977visual", "bib_title": "Visual contrast sensitivity", "bib_author ": "Kelly, DH", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "In many cases, the extract features are further processed by models that simulate texture masking\\cite{helmholtz1896handbook}, contract sensitivity\\cite{kelly1977visual}", "next_context": ", and saliency\\cite{itti2001computational}."}], "importance_score": 1.0}, "itti2001computational": {"bib_key": "itti2001computational", "bib_title": "Computational modelling of visual attention", "bib_author ": "Itti, Laurent", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "In many cases, the extract features are further processed by models that simulate texture masking\\cite{helmholtz1896handbook}, contract sensitivity\\cite{kelly1977visual}, and saliency\\cite{itti2001computational}", "next_context": "."}], "importance_score": 1.0}, "kim2017deep": {"bib_key": "kim2017deep", "bib_title": "Deep learning of human visual sensitivity in image quality assessment framework", "bib_author ": "Kim, Jongyoo", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Initially, convolution network networks were typically used, with notable examples such as DeepQA\\cite{kim2017deep}", "next_context": ", LPIPS\\cite{zhang2018unreasonable}and CONTRIQUE\\cite{madhusudana2022image}for image quality assessment, and TLVQA\\cite{korhonen2019two}, C3DVQA\\cite{xu2020c3dvqa}and DeepVQA\\cite{kim2018deep}for video quality assessment."}], "importance_score": 1.0}, "zhang2018unreasonable": {"bib_key": "zhang2018unreasonable", "bib_title": "The unreasonable effectiveness of deep features as a perceptual metric", "bib_author ": "Zhang, Richard", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Initially, convolution network networks were typically used, with notable examples such as DeepQA\\cite{kim2017deep}, LPIPS\\cite{zhang2018unreasonable}", "next_context": "and CONTRIQUE\\cite{madhusudana2022image}for image quality assessment, and TLVQA\\cite{korhonen2019two}, C3DVQA\\cite{xu2020c3dvqa}and DeepVQA\\cite{kim2018deep}for video quality assessment."}], "importance_score": 1.0}, "madhusudana2022image": {"bib_key": "madhusudana2022image", "bib_title": "Image quality assessment using contrastive learning", "bib_author ": "Madhusudana, Pavan C", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Initially, convolution network networks were typically used, with notable examples such as DeepQA\\cite{kim2017deep}, LPIPS\\cite{zhang2018unreasonable}and CONTRIQUE\\cite{madhusudana2022image}", "next_context": "for image quality assessment, and TLVQA\\cite{korhonen2019two}, C3DVQA\\cite{xu2020c3dvqa}and DeepVQA\\cite{kim2018deep}for video quality assessment."}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{madhusudana2022image}", "next_context": "learns relevant features from an unannotated image database based on the prediction of distortion types and degrees through contrastive learning."}], "importance_score": 2.0}, "korhonen2019two": {"bib_key": "korhonen2019two", "bib_title": "Two-level approach for no-reference consumer video quality assessment", "bib_author ": "Korhonen, Jari", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Initially, convolution network networks were typically used, with notable examples such as DeepQA\\cite{kim2017deep}, LPIPS\\cite{zhang2018unreasonable}and CONTRIQUE\\cite{madhusudana2022image}for image quality assessment, and TLVQA\\cite{korhonen2019two}", "next_context": ", C3DVQA\\cite{xu2020c3dvqa}and DeepVQA\\cite{kim2018deep}for video quality assessment."}], "importance_score": 1.0}, "xu2020c3dvqa": {"bib_key": "xu2020c3dvqa", "bib_title": "C3DVQA: Full-reference video quality assessment with 3D convolutional neural network", "bib_author ": "Xu, Munan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Initially, convolution network networks were typically used, with notable examples such as DeepQA\\cite{kim2017deep}, LPIPS\\cite{zhang2018unreasonable}and CONTRIQUE\\cite{madhusudana2022image}for image quality assessment, and TLVQA\\cite{korhonen2019two}, C3DVQA\\cite{xu2020c3dvqa}", "next_context": "and DeepVQA\\cite{kim2018deep}for video quality assessment."}], "importance_score": 1.0}, "kim2018deep": {"bib_key": "kim2018deep", "bib_title": "Deep video quality assessor: From spatio-temporal visual sensitivity to a convolutional neural aggregation network", "bib_author ": "Kim, Woojae", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Initially, convolution network networks were typically used, with notable examples such as DeepQA\\cite{kim2017deep}, LPIPS\\cite{zhang2018unreasonable}and CONTRIQUE\\cite{madhusudana2022image}for image quality assessment, and TLVQA\\cite{korhonen2019two}, C3DVQA\\cite{xu2020c3dvqa}and DeepVQA\\cite{kim2018deep}", "next_context": "for video quality assessment."}], "importance_score": 1.0}, "touvron2023llama": {"bib_key": "touvron2023llama", "bib_title": "Llama: Open and efficient foundation language models", "bib_author ": "Touvron, Hugo", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "More recently, inspired by the success of large language models (LLMs)\\cite{openai:gpt4:2023,touvron2023llama}", "next_context": "in other machine learning tasks, they have also been utilized in image and video quality assessment and demonstrate great potential to achieve better model generalization."}], "importance_score": 0.5}, "wu2024qbench": {"bib_key": "wu2024qbench", "bib_title": "Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision", "bib_author ": "Wu, Haoning", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Q-Bench\\cite{wu2024qbench}", "next_context": "is one of the first attempts that employs multimodality large language models to predict the perceptual quality of images based on prompt-driven evaluation."}], "importance_score": 1.0}, "wu2024qalign": {"bib_key": "wu2024qalign", "bib_title": "{Q-Align}: Teaching {LMMs} for Visual Scoring via Discrete Text-Defined Levels", "bib_author ": "Wu, Haoning", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{wu2024qalign}", "next_context": "."}], "importance_score": 1.0}, "chen2023x": {"bib_key": "chen2023x", "bib_title": "{X-iqe}: explainable image quality evaluation for text-to-image generation with visual large language models", "bib_author ": "Chen, Yixiong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{chen2023x}", "next_context": "that performs the quality prompt in a multi-iteration manner focusing on both image fidelity and aesthetics."}], "importance_score": 1.0}, "zhu20242afc": {"bib_key": "zhu20242afc", "bib_title": "{2AFC} Prompting of Large Multimodal Models for Image Quality Assessment", "bib_author ": "Zhu, Hanwei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Prompt-based approaches have also been proposed for differentiating the quality difference between multiple images, such as 2AFC-LMMs\\cite{zhu20242afc}", "next_context": "based on a two-alternative forced choice prompt and MAP (maximum a posteriori) estimation."}], "importance_score": 1.0}, "miyata2024zen": {"bib_key": "miyata2024zen", "bib_title": "ZEN-IQA: Zero-Shot Explainable and No-Reference Image Quality Assessment With Vision Language Model", "bib_author ": "Miyata, Takamichi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Important examples in this class for image quality assessment including ZEN-IQA\\cite{miyata2024zen}", "next_context": ", QA-CLIP\\cite{pan2023quality}and PromptIQA\\cite{chen2025promptiqa}."}], "importance_score": 1.0}, "pan2023quality": {"bib_key": "pan2023quality", "bib_title": "Quality-aware clip for blind image quality assessment", "bib_author ": "Pan, Wensheng", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Important examples in this class for image quality assessment including ZEN-IQA\\cite{miyata2024zen}, QA-CLIP\\cite{pan2023quality}", "next_context": "and PromptIQA\\cite{chen2025promptiqa}."}], "importance_score": 1.0}, "chen2025promptiqa": {"bib_key": "chen2025promptiqa", "bib_title": "Promptiqa: Boosting the performance and generalization for no-reference image quality assessment via prompts", "bib_author ": "Chen, Zewen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Important examples in this class for image quality assessment including ZEN-IQA\\cite{miyata2024zen}, QA-CLIP\\cite{pan2023quality}and PromptIQA\\cite{chen2025promptiqa}", "next_context": "."}], "importance_score": 1.0}, "wu2023exploring": {"bib_key": "wu2023exploring", "bib_title": "Exploring Opinion-Unaware Video Quality Assessment with Semantic Affinity Criterion", "bib_author ": "Wu, Haoning", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Similar works have also been proposed for video quality assessment, such as BVQI\\cite{wu2023exploring,wu2023towards}", "next_context": "and COVER\\cite{he2024cover}."}], "importance_score": 0.5}, "wu2023towards": {"bib_key": "wu2023towards", "bib_title": "Towards robust text-prompted semantic criterion for in-the-wild video quality assessment", "bib_author ": "Wu, Haoning", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "Similar works have also been proposed for video quality assessment, such as BVQI\\cite{wu2023exploring,wu2023towards}", "next_context": "and COVER\\cite{he2024cover}."}, {"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "There are also databases developed that investigate the impact of specific video formats and/or artifacts, such as LIVE-YT-HFR\\cite{madhusudana2021subjective}focusing on frame rates, VSR-QAD\\cite{zhou2024database}on spatial resolution (or super-resolution artifacts), BAND-2k\\cite{chen2024band2k}on banding artifacts and Maxwell\\cite{wu2023towards}", "next_context": "/BVI-Artifact\\cite{feng2024bvi}containing multiple artifacts commonly produced in video streaming."}], "importance_score": 1.5}, "sheikh2006astatistical": {"bib_key": "sheikh2006astatistical", "bib_title": "A statistical evaluation of recent full reference image quality assessment algorithms", "bib_author ": "Sheikh, Hamid R.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{sheikh2006astatistical}", "next_context": ", CSIQ\\cite{larson2010most}, TID2013\\cite{ponomarenko2013color}, PieAPP and PIPAL, while video quality databases such as LIVE-VQA~\\cite{seshadrinathan2010study}, KoNViD-1K~\\cite{hosu2017konstanz}, YouTube UGC~\\cite{wang2019youtube}and LIVE-VQC~\\cite{sinno2018large}are typically employed for benchmarking in the literature."}], "importance_score": 1.0}, "ponomarenko2013color": {"bib_key": "ponomarenko2013color", "bib_title": "Color image database TID2013: Peculiarities and preliminary results", "bib_author ": "Ponomarenko, Nikolay", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{sheikh2006astatistical}, CSIQ\\cite{larson2010most}, TID2013\\cite{ponomarenko2013color}", "next_context": ", PieAPP and PIPAL, while video quality databases such as LIVE-VQA~\\cite{seshadrinathan2010study}, KoNViD-1K~\\cite{hosu2017konstanz}, YouTube UGC~\\cite{wang2019youtube}and LIVE-VQC~\\cite{sinno2018large}are typically employed for benchmarking in the literature."}], "importance_score": 1.0}, "seshadrinathan2010study": {"bib_key": "seshadrinathan2010study", "bib_title": "Study of subjective and objective quality assessment of video", "bib_author ": "Seshadrinathan, Kalpana", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{sheikh2006astatistical}, CSIQ\\cite{larson2010most}, TID2013\\cite{ponomarenko2013color}, PieAPP and PIPAL, while video quality databases such as LIVE-VQA~\\cite{seshadrinathan2010study}", "next_context": ", KoNViD-1K~\\cite{hosu2017konstanz}, YouTube UGC~\\cite{wang2019youtube}and LIVE-VQC~\\cite{sinno2018large}are typically employed for benchmarking in the literature."}], "importance_score": 1.0}, "hosu2017konstanz": {"bib_key": "hosu2017konstanz", "bib_title": "The Konstanz natural video database \n(KoNViD-1k)", "bib_author ": "Hosu, Vlad", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{sheikh2006astatistical}, CSIQ\\cite{larson2010most}, TID2013\\cite{ponomarenko2013color}, PieAPP and PIPAL, while video quality databases such as LIVE-VQA~\\cite{seshadrinathan2010study}, KoNViD-1K~\\cite{hosu2017konstanz}", "next_context": ", YouTube UGC~\\cite{wang2019youtube}and LIVE-VQC~\\cite{sinno2018large}are typically employed for benchmarking in the literature."}], "importance_score": 1.0}, "wang2019youtube": {"bib_key": "wang2019youtube", "bib_title": "YouTube {UGC} dataset for video compression research", "bib_author ": "Wang, Yilin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{sheikh2006astatistical}, CSIQ\\cite{larson2010most}, TID2013\\cite{ponomarenko2013color}, PieAPP and PIPAL, while video quality databases such as LIVE-VQA~\\cite{seshadrinathan2010study}, KoNViD-1K~\\cite{hosu2017konstanz}, YouTube UGC~\\cite{wang2019youtube}", "next_context": "and LIVE-VQC~\\cite{sinno2018large}are typically employed for benchmarking in the literature."}], "importance_score": 1.0}, "sinno2018large": {"bib_key": "sinno2018large", "bib_title": "Large-scale study of perceptual video quality", "bib_author ": "Sinno, Zeina", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{sheikh2006astatistical}, CSIQ\\cite{larson2010most}, TID2013\\cite{ponomarenko2013color}, PieAPP and PIPAL, while video quality databases such as LIVE-VQA~\\cite{seshadrinathan2010study}, KoNViD-1K~\\cite{hosu2017konstanz}, YouTube UGC~\\cite{wang2019youtube}and LIVE-VQC~\\cite{sinno2018large}", "next_context": "are typically employed for benchmarking in the literature."}], "importance_score": 1.0}, "madhusudana2021subjective": {"bib_key": "madhusudana2021subjective", "bib_title": "Subjective and objective quality assessment of high frame rate videos", "bib_author ": "Madhusudana, Pavan C", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "There are also databases developed that investigate the impact of specific video formats and/or artifacts, such as LIVE-YT-HFR\\cite{madhusudana2021subjective}", "next_context": "focusing on frame rates, VSR-QAD\\cite{zhou2024database}on spatial resolution (or super-resolution artifacts), BAND-2k\\cite{chen2024band2k}on banding artifacts and Maxwell\\cite{wu2023towards}/BVI-Artifact\\cite{feng2024bvi}containing multiple artifacts commonly produced in video streaming."}], "importance_score": 1.0}, "zhou2024database": {"bib_key": "zhou2024database", "bib_title": "A Database and Model for the Visual Quality Assessment of Super-Resolution Videos", "bib_author ": "Zhou, Fei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "There are also databases developed that investigate the impact of specific video formats and/or artifacts, such as LIVE-YT-HFR\\cite{madhusudana2021subjective}focusing on frame rates, VSR-QAD\\cite{zhou2024database}", "next_context": "on spatial resolution (or super-resolution artifacts), BAND-2k\\cite{chen2024band2k}on banding artifacts and Maxwell\\cite{wu2023towards}/BVI-Artifact\\cite{feng2024bvi}containing multiple artifacts commonly produced in video streaming."}], "importance_score": 1.0}, "chen2024band2k": {"bib_key": "chen2024band2k", "bib_title": "BAND-2k: Banding Artifact Noticeable Database for Banding Detection and Quality Assessment", "bib_author ": "Chen, Zijian", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "There are also databases developed that investigate the impact of specific video formats and/or artifacts, such as LIVE-YT-HFR\\cite{madhusudana2021subjective}focusing on frame rates, VSR-QAD\\cite{zhou2024database}on spatial resolution (or super-resolution artifacts), BAND-2k\\cite{chen2024band2k}", "next_context": "on banding artifacts and Maxwell\\cite{wu2023towards}/BVI-Artifact\\cite{feng2024bvi}containing multiple artifacts commonly produced in video streaming."}], "importance_score": 1.0}, "feng2024bvi": {"bib_key": "feng2024bvi", "bib_title": "BVI-Artefact: An artefact detection benchmark dataset for streamed videos", "bib_author ": "Feng, Chen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "There are also databases developed that investigate the impact of specific video formats and/or artifacts, such as LIVE-YT-HFR\\cite{madhusudana2021subjective}focusing on frame rates, VSR-QAD\\cite{zhou2024database}on spatial resolution (or super-resolution artifacts), BAND-2k\\cite{chen2024band2k}on banding artifacts and Maxwell\\cite{wu2023towards}/BVI-Artifact\\cite{feng2024bvi}", "next_context": "containing multiple artifacts commonly produced in video streaming."}], "importance_score": 1.0}, "liu2017rankiqa": {"bib_key": "liu2017rankiqa", "bib_title": "RankIQA: Learning from Rankings for No-Reference Image Quality Assessment", "bib_author ": "Liu, Xialei", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{liu2017rankiqa}", "next_context": "and UNIQUE\\cite{zhang2021uncertainty}for the image quality assessment task, and VFIPS\\cite{hou2022perceptual}and RankDVQA\\cite{feng2024rankdvqa}for video quality assessment."}], "importance_score": 1.0}, "zhang2021uncertainty": {"bib_key": "zhang2021uncertainty", "bib_title": "Uncertainty-aware blind image quality assessment in the laboratory and wild", "bib_author ": "Zhang, Weixia", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{liu2017rankiqa}and UNIQUE\\cite{zhang2021uncertainty}", "next_context": "for the image quality assessment task, and VFIPS\\cite{hou2022perceptual}and RankDVQA\\cite{feng2024rankdvqa}for video quality assessment."}], "importance_score": 1.0}, "hou2022perceptual": {"bib_key": "hou2022perceptual", "bib_title": "A perceptual quality metric for video frame interpolation", "bib_author ": "Hou, Qiqi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "\\cite{liu2017rankiqa}and UNIQUE\\cite{zhang2021uncertainty}for the image quality assessment task, and VFIPS\\cite{hou2022perceptual}", "next_context": "and RankDVQA\\cite{feng2024rankdvqa}for video quality assessment."}], "importance_score": 1.0}, "madhusudana2023conviqt": {"bib_key": "madhusudana2023conviqt", "bib_title": "Conviqt: Contrastive video quality estimator", "bib_author ": "Madhusudana, Pavan C", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "This method has been further applied to video quality assessment, resulting in a contrastive video quality estimator, CONVIQT\\cite{madhusudana2023conviqt}", "next_context": "."}], "importance_score": 1.0}, "zhao2023quality": {"bib_key": "zhao2023quality", "bib_title": "Quality-aware pre-trained models for blind image quality assessment", "bib_author ": "Zhao, Kai", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Advanced AI for the creative industries", "subsection": "Visual Quality Assessment", "subsubsection": "Quality assessment models", "prev_context": "More recently, quality-aware contrastive loss has been designed in\\cite{zhao2023quality,peng2024rmt}", "next_context": "to stabilize the learning process."}], "importance_score": 0.5}, "zhong:LDB:2024": {"bib_key": "zhong:LDB:2024", "bib_title": "{LDB: A} Large Language Model Debugger via Verifying Runtime Execution Step-by-step", "bib_author ": "Li Zhong", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Future Challenges for AI in the Creative Sector", "subsection": null, "subsubsection": null, "prev_context": "\\cite{zhong:LDB:2024}", "next_context": "help software developers expedite their work, including tasks within creative sectors."}], "importance_score": 1.0}}, "refs": [], "table": [{"original": "\\begin{table}\n\\caption{Creative applications and corresponding AI-based methods mentioned in this paper}\n%\\tiny\n %\\hskip-5.0cm\n %\\begin{tabular}{p{1cm}p{1.4cm}|p{4cm}p{4cm}p{4cm}p{4cm}}\n \\resizebox{\\linewidth}{!}{\\begin{tabular}{lr|lll}\n \\\\\n \\toprule\n\\multicolumn{2}{c}{\\multirow{2}{*}{Application}} & \\multicolumn{3}{|c}{Technology} \\\\ \\cmidrule{3-5}\n& & Trans./Attn.$^1$ & Diffusion model$^2$  & INR\\\\\n\\midrule\n{\\bf Creation} & Text & \\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}  & \\\\\n& audio/music & \\cite{Alayrac:Flamingo:2022, Huang:GenerSpeech:2022} & \\cite{Li:Diffusion:2022, Yang:Diffsound:2023} & \\\\\n& Image &  \\cite{Alayrac:Flamingo:2022,esser:scaling:2024} & \\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024} & \\\\\n% ---------------------------------------------------------------\n& Animation/video & \\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024} & \\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024} &\\\\\n% ---------------------------------------------------------------\n & 3D/AR/VR & \\cite{yang:Holodeck:2024} & \\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024} & \\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear} \\\\\n% ---------------------------------------------------------------\n\\midrule\n{\\bf Information} & Text categorization & \\cite{sun:text:2023, shi:chatgraph:2023, Hou:promptboosting:2023} \\\\\n% ---------------------------------------------------------------\n{\\bf Analysis} & Film analysis & \\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}  \\\\\n % ---------------------------------------------------------------\n & Content retrieval & \\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning} & \\cite{Jin:DiffusionRet:2023} \\\\\n % ---------------------------------------------------------------\n & Intelligent assistants & \\cite{King:Sasha:2024} \\\\\n % ---------------------------------------------------------------\n \\midrule\n{\\bf Content} & Enhancement  & \\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024} & \\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight} & \\cite{Yang:Implicit:2023} \\\\\n{\\bf  Enhancement  } & Style transfer & \\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR} & \\cite{Zhang:Inversion:2023,Chai:StableVideo:2023} & \\cite{Moon:generalizable:2023,Kim:Controllable:2024} \\\\\n% ---------------------------------------------------------------\n {\\bf  and Post} & Super-resolution & \\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024} & \\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023} & \\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}\\\\\n% ---------------------------------------------------------------\n{\\bf Production} & {Restoration} & \\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb} & \\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023} & \\cite{Jiang:NeRT:2023} \\\\\n  % ---------------------------------------------------------------\n & Inpainting & \\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024} & \\cite{Moliner:solving:2023, Fei:Generative:2023} & \\\\\n % ---------------------------------------------------------------\n& Fusion & \\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023} & \\cite{Zhao:DDFM:2023} & \\\\\n & Editing/VFX & \\cite{Shi:Motion-I2V:2024} & \\cite{Shi:Motion-I2V:2024,guo2024liveportrait} \\\\\n \\midrule\n % ---------------------------------------------------------------\n{\\bf Information} & Segmentation & \\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2} & \\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024} & \\cite{Gong:Continuous:2023, Cen:Segment:2023} \\\\\n % ---------------------------------------------------------------\n{\\bf  Extraction} & Recognition &  \\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024} & \\cite{Li:Your:2023,Chen:DiffusionDet:2023}\\\\\n % ---------------------------------------------------------------\n{\\bf  and}  & Tracking &  \\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring} & \\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024} & \\cite{Jung:AnyFlow:2023}\\\\\n % ---------------------------------------------------------------\n{\\bf  Understanding}  & 3D Reconstruction  & \\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024} & \\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024} & \\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, azzarelli:waveplanes:2023},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025}$^\\dag$  \\\\\n\\midrule\n\n% ---------------------------------------------------------------\n{\\bf Compression} & Image$^\\ast$ & \\cite{zhu2022transformer,zou2022devil,liu2023learned}& \\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual} & \\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}\\\\\n& Video& \\cite{xiang2022mimt,mentzer2022vct}& \\cite{li2024extreme}&\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}\\\\\n&Audio$^\\ast$ & & &\\\\\n\\midrule\n% ---------------------------------------------------------------\n{\\bf Quailty}  &  Image$^\\ast$ & \\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}& &\\\\\n\\textbf{Assessment}& Video$^\\ast$ & \\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}& & \\\\\n\\bottomrule\n\\multicolumn{5}{l}{$^1$ Trans./Attn. include both transformers and CNN-based architectures that use attention module.} \\\\\n\\multicolumn{5}{l}{$^2$ Some diffusion models employ the transformer in their denoising autoencoders.}\n \\end{tabular}}\n \\footnotesize\n $^\\dag$ \nThese methods are based on explicit neural representations. \\\\\n$^*$ It is noted that for some compression and quality assessment tasks, there are other dominant network architectures in existing works. For example, LLMs have been used for image and audio compression, and visual quality assessment. Many neural audio codecs are also based on VQ-VAE models.\n \n\\label{tab:gather}\n\\end{table}", "caption": "\\caption{Creative applications and corresponding AI-based methods mentioned in this paper}", "label": "\\label{tab:gather}", "tabular": "\\begin{tabular}{lr|lll}\n \\\\\n \\toprule\n\\multicolumn{2}{c}{\\multirow{2}{*}{Application}} & \\multicolumn{3}{|c}{Technology} \\\\ \\cmidrule{3-5}\n& & Trans./Attn.$^1$ & Diffusion model$^2$  & INR\\\\\n\\midrule\n{\\bf Creation} & Text & \\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}  & \\\\\n& audio/music & \\cite{Alayrac:Flamingo:2022, Huang:GenerSpeech:2022} & \\cite{Li:Diffusion:2022, Yang:Diffsound:2023} & \\\\\n& Image &  \\cite{Alayrac:Flamingo:2022,esser:scaling:2024} & \\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024} & \\\\\n% ---------------------------------------------------------------\n& Animation/video & \\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024} & \\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024} &\\\\\n% ---------------------------------------------------------------\n & 3D/AR/VR & \\cite{yang:Holodeck:2024} & \\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024} & \\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear} \\\\\n% ---------------------------------------------------------------\n\\midrule\n{\\bf Information} & Text categorization & \\cite{sun:text:2023, shi:chatgraph:2023, Hou:promptboosting:2023} \\\\\n% ---------------------------------------------------------------\n{\\bf Analysis} & Film analysis & \\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}  \\\\\n % ---------------------------------------------------------------\n & Content retrieval & \\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning} & \\cite{Jin:DiffusionRet:2023} \\\\\n % ---------------------------------------------------------------\n & Intelligent assistants & \\cite{King:Sasha:2024} \\\\\n % ---------------------------------------------------------------\n \\midrule\n{\\bf Content} & Enhancement  & \\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024} & \\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight} & \\cite{Yang:Implicit:2023} \\\\\n{\\bf  Enhancement  } & Style transfer & \\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR} & \\cite{Zhang:Inversion:2023,Chai:StableVideo:2023} & \\cite{Moon:generalizable:2023,Kim:Controllable:2024} \\\\\n% ---------------------------------------------------------------\n {\\bf  and Post} & Super-resolution & \\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024} & \\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023} & \\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}\\\\\n% ---------------------------------------------------------------\n{\\bf Production} & {Restoration} & \\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb} & \\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023} & \\cite{Jiang:NeRT:2023} \\\\\n  % ---------------------------------------------------------------\n & Inpainting & \\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024} & \\cite{Moliner:solving:2023, Fei:Generative:2023} & \\\\\n % ---------------------------------------------------------------\n& Fusion & \\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023} & \\cite{Zhao:DDFM:2023} & \\\\\n & Editing/VFX & \\cite{Shi:Motion-I2V:2024} & \\cite{Shi:Motion-I2V:2024,guo2024liveportrait} \\\\\n \\midrule\n % ---------------------------------------------------------------\n{\\bf Information} & Segmentation & \\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2} & \\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024} & \\cite{Gong:Continuous:2023, Cen:Segment:2023} \\\\\n % ---------------------------------------------------------------\n{\\bf  Extraction} & Recognition &  \\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024} & \\cite{Li:Your:2023,Chen:DiffusionDet:2023}\\\\\n % ---------------------------------------------------------------\n{\\bf  and}  & Tracking &  \\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring} & \\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024} & \\cite{Jung:AnyFlow:2023}\\\\\n % ---------------------------------------------------------------\n{\\bf  Understanding}  & 3D Reconstruction  & \\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024} & \\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024} & \\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, azzarelli:waveplanes:2023},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025}$^\\dag$  \\\\\n\\midrule\n\n% ---------------------------------------------------------------\n{\\bf Compression} & Image$^\\ast$ & \\cite{zhu2022transformer,zou2022devil,liu2023learned}& \\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual} & \\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}\\\\\n& Video& \\cite{xiang2022mimt,mentzer2022vct}& \\cite{li2024extreme}&\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}\\\\\n&Audio$^\\ast$ & & &\\\\\n\\midrule\n% ---------------------------------------------------------------\n{\\bf Quailty}  &  Image$^\\ast$ & \\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}& &\\\\\n\\textbf{Assessment}& Video$^\\ast$ & \\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}& & \\\\\n\\bottomrule\n\\multicolumn{5}{l}{$^1$ Trans./Attn. include both transformers and CNN-based architectures that use attention module.} \\\\\n\\multicolumn{5}{l}{$^2$ Some diffusion models employ the transformer in their denoising autoencoders.}\n \\end{tabular}", "subtables": []}], "figure": [{"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/generativemodel.png}\n    \\caption{Generative AI. (a) Transformer architecture \\cite{Vaswani:attention:2017}. (b) The top row is diffusion process and the bottom row is generation process of new image \\cite{Yang:diffusion:2023}. (c) Latent Diffusion Models (LDM) \\cite{Rombach:LDM:2022}. }\n    \\label{fig:generativemodel}\n\\end{figure}", "caption": "\\caption{Generative AI. (a) Transformer architecture \\cite{Vaswani:attention:2017}. (b) The top row is diffusion process and the bottom row is generation process of new image \\cite{Yang:diffusion:2023}. (c) Latent Diffusion Models (LDM) \\cite{Rombach:LDM:2022}. }", "label": "\\label{fig:generativemodel}", "subfigures": [], "figure_paths": ["figures/generativemodel.png"]}, {"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/FLASK_LLM_and_history.png}\n    \\caption{(a) Timeline of large language models. (b) Performance comparison evaluated by FLASK \\cite{Ye:FLASK:2024}.}\n    \\label{fig:FLASK_LLM_and_history}\n\\end{figure}", "caption": "\\caption{(a) Timeline of large language models. (b) Performance comparison evaluated by FLASK \\cite{Ye:FLASK:2024}.}", "label": "\\label{fig:FLASK_LLM_and_history}", "subfigures": [], "figure_paths": ["figures/FLASK_LLM_and_history.png"]}, {"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/LLMGround.png}\n    \\caption{Text-to-image generation (generated on 27 November 2024). (a) text-to-image generation by Ideogram v1, DALL\u00b7E 3, Photoshop 2024, and sdxy-turbo by Nvidia. (b) The top-row images were generated by DALL\u00b7E in ChatGPT 4. The bottom-row images are generated by LLM-grounded Diffusion \\cite{Lian:LLMG:2024}. }\n    \\label{fig:LLMGround}\n\\end{figure}", "caption": "\\caption{Text-to-image generation (generated on 27 November 2024). (a) text-to-image generation by Ideogram v1, DALL\u00b7E 3, Photoshop 2024, and sdxy-turbo by Nvidia. (b) The top-row images were generated by DALL\u00b7E in ChatGPT 4. The bottom-row images are generated by LLM-grounded Diffusion \\cite{Lian:LLMG:2024}. }", "label": "\\label{fig:LLMGround}", "subfigures": [], "figure_paths": ["figures/LLMGround.png"]}, {"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/Deepmotion_Vasa.png}\n    \\caption{(Left) Video-to-3D post animation by DeepMotion. (Right) Image and audio to video by VASA-1 \\cite{xu:VASA-1:2024}}\n    \\label{fig:Deepmotion_Vasa}\n\\end{figure}", "caption": "\\caption{(Left) Video-to-3D post animation by DeepMotion. (Right) Image and audio to video by VASA-1 \\cite{xu:VASA-1:2024}}", "label": "\\label{fig:Deepmotion_Vasa}", "subfigures": [], "figure_paths": ["figures/Deepmotion_Vasa.png"]}, {"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/SR_results.png}\n    \\caption{(Left) Examples of SR ($\\times$4) using generative model. (Right) Real-time portrait editing with FacePoke.} \n    \\label{fig:SR}\n\\end{figure}", "caption": "\\caption{(Left) Examples of SR ($\\times$4) using generative model. (Right) Real-time portrait editing with FacePoke.}", "label": "\\label{fig:SR}", "subfigures": [], "figure_paths": ["figures/SR_results.png"]}, {"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation.png}\n    \\caption{(a) Tasks in Object-centric understanding defined by Yan et al. \\cite{Yan:Universal:2023} (REC:,  Referring Expression Comprehension, RES: Referring Expression Segmentation, VOS:  Video Object Segmentation, RVOS: Referring Video Object Segmentation, MOT: Multiple Object Tracking, MOTS: Multi-Object Tracking and Segmentation, VIS: Video Instance Segmentation, SOT: Single Object Tracking. (b) Current high-quality segmentation \\cite{Ke:SAM-HQ:2023}.}\n    \\label{fig:segmentation}\n\\end{figure}", "caption": "\\caption{(a) Tasks in Object-centric understanding defined by Yan et al. \\cite{Yan:Universal:2023} (REC:,  Referring Expression Comprehension, RES: Referring Expression Segmentation, VOS:  Video Object Segmentation, RVOS: Referring Video Object Segmentation, MOT: Multiple Object Tracking, MOTS: Multi-Object Tracking and Segmentation, VIS: Video Instance Segmentation, SOT: Single Object Tracking. (b) Current high-quality segmentation \\cite{Ke:SAM-HQ:2023}.}", "label": "\\label{fig:segmentation}", "subfigures": [], "figure_paths": ["figures/segmentation.png"]}, {"original": "\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/3Drepresentation.png}\n    \\caption{3D representation. (a) Neural Radiance Fields (NeRFs) \\cite{Mildenhall:NeRF:2020}. (b) Gaussian Splatting \\cite{kerbl:3Dgaussians:2023}. (c) Example scenes of VR-GS system for 3D content interaction in VR \\cite{jiang:vrgs:2024}. }\n    \\label{fig:3Drepresentation}\n\\end{figure}", "caption": "\\caption{3D representation. (a) Neural Radiance Fields (NeRFs) \\cite{Mildenhall:NeRF:2020}. (b) Gaussian Splatting \\cite{kerbl:3Dgaussians:2023}. (c) Example scenes of VR-GS system for 3D content interaction in VR \\cite{jiang:vrgs:2024}. }", "label": "\\label{fig:3Drepresentation}", "subfigures": [], "figure_paths": ["figures/3Drepresentation.png"]}], "equations": ["\\begin{equation}\n    \\text{Attention}(Q, K, V ) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) V,\n    \\label{eqn:attention}\n\\end{equation}", "\\begin{equation}\n\\begin{split}\n        \\text{MultiHead}(Q, K, V ) &= \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O,\\\\\n        \\text{head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V ). \n\\end{split}\n\\label{eqn:MultiHead}\n\\end{equation}", "\\begin{equation}\nF(x, \\Phi, \\nabla_x \\Phi, \\nabla_x^2 \\Phi, \\dots) = 0, \\quad \\Phi : x \\mapsto \\Phi(x).\n\\label{eqn:inr}\n\\end{equation}"], "algorithm": [], "sections": {"Acknowledgements": {"content": "\n\nThis work has been funded by the UKRI MyWorld Strength in Places Programme (SIPF00006/1).\n\n\n\\newpage\n\n\\tableofcontents\n\n\\newpage\n\n", "appendix": false}, "Introduction": {"content": "\n\nArtificial intelligence (AI) has grown dramatically over the past few years, particularly due to the rise of generative AI and large language models (LLMs). These advancements have been widely regarded as beneficial in many countries, such as the UK, as highlighted in a report published by the Authority of the House of Lords \\cite{UK:Large:2024}. These AI advancements have also had significant direct and indirect impacts on the creative industries, influencing the direction of their growth. Generative AI, for instance, primarily focuses on generating new data that is not identical to the training data but shares similarities. However, the training data can be very large and have a much broader range than what humans have encountered themselves. The resulting output may present entirely new work to an individual, serving as inspiration. \nIndeed, AI tools have also opened up opportunities to a wider range of users with different skill sets, enabling creative work to be done faster, more effectively, and with greater creativity. Moreover, these new technologies not only influence artists and creative industries but also open new ways for audiences to experience arts and culture \\cite{Jeary2024}.\n\nThe major breakthrough in generative AI has come from OpenAI\\footnote{\\url{https://openai.com/}}, an AI research and deployment company, with the introduction of Generative Pre-trained Transformer (GPT) models for LLMs. LLMs are specifically designed to understand and generate human language. They are characterized by their vast size in terms of parameters and the amount of training data. This breakthrough was particularly significant with the release of ChatGPT in 2022, which was fine-tuned from a model in the GPT-3.5 series. ChatGPT is a conversational model that includes advancements in safety features to mitigate the generation of inappropriate content. Several other LLM platforms were developed around the same time, such as LaMDA and PaLM by Google AI, Ernie Bot by Baidu, and BLOOM by BigScience. Additionally, Anthropic launched Claude, the LLM trained specifically to be harmless and honest, leveraging reinforcement learning from human feedback (RLHF) - a technique used to train AI systems to appear more human \\cite{Bai:Train:2021}. However, ChatGPT stands out as the most renowned, thanks to its quick and efficient responses, and notably, it is available for free. Another breakthrough in the same year was in text-to-image models. OpenAI achieved a significant milestone with DALL\u00b7E 2, producing impressive artworks and photorealistic images despite its limited language understanding. Midjourney by Midjourney, Inc., another well-known text-to-image generation, supports higher resolution images, which can go up to 4096$\\times$\\times4096 pixels. Stable Diffusion by Stability AI, for which the code and model weights are publicly available\\footnote{\\url{https://github.com/Stability-AI/stablediffusion}}, allows developers and artists to further adapt it to their own applications.\n\nThe next breakthrough happened in 2023 when OpenAI unveiled GPT-4, a significantly larger model with more parameters and improved performance compared to its predecessors \\cite{openai:gpt4:2023}. GPT-4 is a multimodal large language model that can generate responses to both text and images. It incorporates DALL\u00b7E 3, enabling it to comprehend a much broader range of nuances and details than earlier versions. In March 2024, Claude 3 Opus by Anthropic has been released, boasting multimodal capabilities in generating images, tables, graphs and diagrams. Moreover, Anthropic claims that Claude 3 Opus outperforms GPT-4 in generating human-like dialog and contextually aware responses. These AI advancements however make the media industry faces challenges. With the impact of these technologies, DMG Media, the Financial Times, and the Guardian Media Group have highlighted concerns about the potential impact on print journalism, particularly if AI tools reduce the need for users to click through to news websites, affecting advertising and subscription revenues \\cite{UK:Large:2024}.\n\nGenerating videos is significantly more challenging than generating images. In February 2024, Google announced Gemini 1.5 with a capability of processing a large data (approximately 8 times more than GPT-4), like video footage and audio recordings\\footnote{\\url{https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/}}. In the same month, OpenAI provided the first preview of Sora,  a model capable of generating impressive realistic videos up to 1 minute long. As of November 2024, Sora remains unreleased and unavailable to the public. Based on the videos released by OpenAI, Sora appears to outperform other text-to-video models. A month later, Gemini 1.5 has announced its support native audio understanding in 180$+$+ countries.  With the emergence of Gemini 1.5 and Sora, alongside other forthcoming AI tools, it is obvious that video content creation will significantly benefit, opening up the media landscape for creativity and providing more opportunities for diverse storytellers, while also saving time to produce. A recent example is the AI-generated Christmas commercial by Coca-Cola\\footnote{\\url{https://www.youtube.com/watch?v=4RSTupbfGog}}. These advertisements overcome the limitations of current technologies by using very short videos with quick scene transitions, ensuring that any artifacts, such as unnatural fingers, are not immediately noticeable.\n\nFor post-production workflow, generative AI may not have a direct impact, but the neural networks originally proposed for generative AI have been widely adapted to serve this purpose, leading to significant improvements in both output quality and computational speed. Moreover, in recent years, there has been a noticeable trend in technology towards adopting a unified framework rather than addressing individual challenges, as it better reflects real-world scenarios. For instance, natural history filmmaking involves challenging acquisition environments and high production standards. It is often filmed in low light, underwater, heat haze, or adverse weather conditions. This can lead to increased noise levels, focus issues, low contrast, color balance problems, and blurriness in the footage. Moreover, the unified models offer advantages in generalizing to diverse tasks and providing flexibility. Take Painter by BAAI Vision \\cite{Wang:Painter:2023} as an example, which employs an image pair as a task prompt (similar to a text prompt in LLMs), their model transfers the input image to produce a similar output as the task prompt, enabling it to undertake various tasks such as segmentation, low-light enhancement, rain removal, etc.\n\nWhile generative AI makes creating and post-processing digital content easier and faster, more efficient performance in video content streaming is consequently required. Although AI-based solutions have been proposed both for conventional coding tool enhancement and for new compression frameworks, they have not been fully adopted in practical applications due to hardware constraints and complexity issues. However, the latest learning-based video codecs have already demonstrated their potential to compete with conventional standard video codecs, in particular, with the latest advances in implicit neural representations. The techniques could emerge in the next few years and contribute to the future video coding standards.\n\nMoreover, in the past few years, we have also seen the impact of AI in the research area of visual quality assessment. The advances include new model architectures based on different attention mechanisms and the application of LLMs, which evidently improves model generalization. New training methodologies have also been proposed based on weakly/unsupervised learning, which address the issue with limited training content.\n\n\nOne of the exciting aspects of using LLMs in the creative sector is that `The human in the loop' \\cite{chung:human-loop:2021} is simplified through text prompts, as language enables artists to convey complex emotions and narratives. This is important because generative AI could produce mistakes, known as hallucinations; hence, we, as humans, correct this through reinforcement learning with human feedback \\cite{Wu:brief:2023}.\n\n\n\nIn this paper, the objective is to investigate the latest advancements in technology that have emerged since our previous review paper on AI in the creative industries before 2022 \\cite{Anantrasirichai:AI:2022}. Compared to the earlier paper, where most technologies were developed as support tools, this updated review paper reveals significant shifts in the technologies after 2022 due to generative AI and most recent AI-based technologies. We explore the new direction of how AI could be involved in the creative sector. Similar to \\cite{Anantrasirichai:AI:2022}, we first provide a high-level overview of current advanced AI technologies (Section \\ref{sec:overview}), followed by a selection of creative domain applications (Section \\ref{sec:existing}), where current technologies enabling AI in creative scenarios are described. Finally, we discuss challenges and the future potential of AI associated with the creative industries (Section \\ref{sec:discussion}).\n\n\n", "appendix": false}, "Current Advanced AI Technologies": {"content": "\n\\label{sec:overview}\n\nFoundation models (FMs) have been termed by The Stanford Institute for Human-Centered Artificial Intelligence in 2021 \\cite{Bommasani2021FoundationModels} which mean ``any model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g., fine-tuned) to a wide range of downstream tasks\". LLMs is one kind of the FMs.\n\nAs this paper provides updates from our previous review \\cite{Anantrasirichai:AI:2022}, an introduction to AI, basic neurons, convolutional neural networks (CNNs), generative adversarial networks (GANs), recurrent neural networks (RNNs), and deep reinforcement learning (DRL) can be found there. Here, we will emphasize four technologies that are rapidly growing and largely affecting creative industries, particularly during the year around 2023. This includes Transformers, Large language models (LLMs), Diffusion Models (DMs), and Implicit Neural Representations (INRs). It is important to note that while these newer technologies are gaining prominence, those from previous generations are still widely used, often in conjunction with the newer ones, as they offer distinct advantages. For instance, CNNs effectively capture local features and semantic meaning, while the attention mechanism in transformers offers global dependencies.\n\n\\subsection{Transformers}\n\\label{ssec:transformers}\n\nNo objection, the usage of AI has become widely accessible to everyone due to the launch of ChatGPT by OpenAI since November, 2022. ChatGPT has become the fastest-growing consumer software application in history\\footnote{\\url{https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/}}, marking a significant advancement in the performance of large language models (LLMs). However, the turning point was actually back in 2017 when Google AI introduced `Transformer' architectures in their publication `Attention Is All You Need' \\cite{Vaswani:attention:2017}; since then, the performance of language models has dramatically improved.  The applications have been extended, including vision understanding \\cite{Dosovitskiy:image:2021}, and even learning multiple tasks across multiple modalities simultaneously (e.g., Gato \\cite{Reed:Generalist:2022}). Here, we will provide a background of transformers. \n\nBefore the advent of transformers, natural language processing (NLP) was done using recurrent neural networks (RNNs), processing data sequences sequentially. In contrast, the transformers capture long-range dependencies through self-attention mechanisms, that go across all words in the sequence, weigh the importance of different words and find relationships between words regardless of their positions. This learns context-aware representation and enables parallel processing of the entire sequence, making the transformers computationally efficient. A set of several attention layers running in parallel is called Multi-Head Attention. In the technical overview level, Transformer architecture, shown in Fig. \\ref{fig:generativemodel} (a), comprises Encoder and Decoder, similar to many CNN-based generators. But the encoder is a stack of identical layers, concatenating a multi-head self-attention mechanism and a fully connected feed-forward network. The decoder is also a stack of identical layers, of which each layer has additional sub-layer to perform multi-head attention over the output of the encoder stack.\n\nMathematically, the attention function is computed from inputs: query $Q$Q, keys $K$K, and values $V$V. The matrix of outputs of  attention function is\n\\begin{equation}\n    \\text{Attention}(Q, K, V ) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) V,\n    \\label{eqn:attention}\n\\end{equation}\\begin{equation}\n    \\text{Attention}(Q, K, V ) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) V,\n    \\label{eqn:attention}\n\\end{equation}\n    \\text{Attention}(Q, K, V ) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}}) V,\n    \\label{eqn:attention}\n\n\\noindent where $d_k$d_k is a  dimension of $K$K. The term ${QK^T}${QK^T}QK^T is Dot-Product Attention, which yields a high similarity value when the two words are closely related.  If $Q$Q and $K$K are from the same sentence, Eq.~\\ref{eqn:attention} refers to self-attention, but if $Q$Q and $K$K are from different sentences, it is referred to as cross-attention. Within the network, multi-head attention is actually employed to concurrently process attention and enable the model to collectively focus on information from distinct representation subspaces at various positions through the learnable parameters $W$Ws.\n\\begin{equation}\n\\begin{split}\n        \\text{MultiHead}(Q, K, V ) &= \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O,\\\\\n        \\text{head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V ). \n\\end{split}\n\\label{eqn:MultiHead}\n\\end{equation}\\begin{equation}\n\\begin{split}\n        \\text{MultiHead}(Q, K, V ) &= \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O,\\\\\n        \\text{head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V ). \n\\end{split}\n\\label{eqn:MultiHead}\n\\end{equation}\n\n        \\text{MultiHead}(Q, K, V ) &= \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O,\\\\\n        \\text{head}_i &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V ). \n\n\\label{eqn:MultiHead}\n\nAttention modules have not only been using in the transformers, but they have also been integrated with other deep learning architectures, including with CNNs in particular, used for image classification \\cite{Li:HAM:2022}, object detection \\cite{Woo:CBAM:2018}, and other computer vision tasks \\cite{Guo:Attention:2022} as providing performance improvement.\n\nIn 2020, the first successful training of a transformer encoder for image recognition was published in \\cite{Dosovitskiy:image:2021}, known as Vision Transformer (ViT). ViT divides an input image into patches, similar to words in a sentence, and processes them through multi-head attention. Additionally, a Multilayer Perceptron (MLP) is employed as the feedforward network. The later work uses a hierarchical division of image inputs and the shifted window approach, called Swin Transformer \\cite{Liu:Swin:2021} by Microsoft. It reported outperforming ViT by 2.4\\% in ImageNet-22K classification (21,841 different categories). Its version 2 \\cite{Liu:Swinv2:2022} applies cosine function in the attention module enabling the scaling up of capacity and resolution. More detail on transformer-based object detection is discussed in Section \\ref{sssec:recog}. To date, Swin Transformer has been widely adopted in various applications, including image restoration \\cite{Fan:SUNet:2022}, where it is integrated with UNet. The application of transformers to video recognition has done by input as a sequence of frames  \\cite{Neimark:video:2021} or dividing the video into space-time blocks \\cite{Bertasius:Is:2021}.  Comprehensive surveys on transformers for images and videos can be found in \\cite{Khan:Transformers:2022} and \\cite{Selva:video:2023}, respectively.\n \nTransformers have been widely used and offer better performance across many tasks. One reason for this is the availability of the open-source Transformers library through Hugging  Face\\footnote{\\url{https://huggingface.co/}}, a platform that assists developers in building their own applications across various tasks, including computer vision, NLP, audio, tabular data, multimodal tasks, and reinforcement learning. The platform also provides model zoo (pretrained networks) and datasets. \n\nNote that in recent years, emerging technologies have adopted state space models \\cite{gu2023mamba, zhu2024vision}, commonly known as `Mamba.' These models are recognized as a linear variant of Transformers and are distinguished by their linear complexity in attention modeling. It is widely reported that they offer performance on par with, or even surpassing, traditional Transformers, while demanding fewer computational resources and less memory.\n\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/generativemodel.png}\n    \\caption{Generative AI. (a) Transformer architecture \\cite{Vaswani:attention:2017}. (b) The top row is diffusion process and the bottom row is generation process of new image \\cite{Yang:diffusion:2023}. (c) Latent Diffusion Models (LDM) \\cite{Rombach:LDM:2022}. }\n    \\label{fig:generativemodel}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/generativemodel.png}\n    \\caption{Generative AI. (a) Transformer architecture \\cite{Vaswani:attention:2017}. (b) The top row is diffusion process and the bottom row is generation process of new image \\cite{Yang:diffusion:2023}. (c) Latent Diffusion Models (LDM) \\cite{Rombach:LDM:2022}. }\n    \\label{fig:generativemodel}\n\n\n\\subsection{Large language models}\n\\label{ssec:llms}\n\nLLMs are based on transformer models with  self-attention mechanisms as core modules. Training LLMs comprises two steps: i) pre-training in unsupervised learning manner, and ii) fine-tuning to a specific task or prompt-tuning (or prompt engineering) for better user inputs. The models are first `pre-trained' with a large amount of unlabelled text data to learn the meaning of words, and the relationships between words before using it to adapt to a downstream task. This is why OpenAI calls their models generative pre-trained transformers (GPT). \n\nFine-tuning involves training the model on new datasets. The drawback is that these data need to be large enough to generalize a new task. Prompt-tuning and prompt engineering are relatively new disciplines for developing and optimizing prompts to efficiently use language models. Prompts is a component that guides the way AI models interpret and respond to user queries. Prompt engineering is the process of structuring text or phrasing that guide the model towards generating the desired output. This relies heavily on trial and error, and an understanding of how the model responds. \nPrompt-tuning, on the other hand, involves training a small set of parameters before utilizing the LLM, thus requiring a relatively small amount of new data. This approach essentially converts text inputs into task-specific virtual inputs, referred to as tokens, while the pre-trained LLM remains unchanged \\cite{Lester:power:2021}. As it is AI-based design, the main drawback of prompt-tuning is lack of interpretability. This paradigm has however extended to other domains, such as visual prompt tuning \\cite{Jia:VPT:2022}. For a comprehensive survey of LLMs, please refer to \\cite{zhao:survey:2023}.\n\nTo date, there are many LLM platforms as Fig. \\ref{fig:FLASK_LLM_and_history} (a) shows their timeline. Many publications on surveys and evaluations of LLMs are available \\cite{zhao:survey:2023,Chang:Survey:2024,YAO:Survey:2024}. This includes FLASK (Fine-grained Language Model Evaluation based on Alignment SKill Sets) \\cite{Ye:FLASK:2024} evaluating LLMs based on 12 fine-grained skills for comprehensive language model evaluation: logical correctness, logical robustness, logical efficiency, factuality, commonsense understanding, comprehension, insightfulness, completeness, metacognition, conciseness, readability, and harmlessness. Their evaluation results are shown in Fig. \\ref{fig:FLASK_LLM_and_history} (b).\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/FLASK_LLM_and_history.png}\n    \\caption{(a) Timeline of large language models. (b) Performance comparison evaluated by FLASK \\cite{Ye:FLASK:2024}.}\n    \\label{fig:FLASK_LLM_and_history}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/FLASK_LLM_and_history.png}\n    \\caption{(a) Timeline of large language models. (b) Performance comparison evaluated by FLASK \\cite{Ye:FLASK:2024}.}\n    \\label{fig:FLASK_LLM_and_history}\n\n\n\n\n\n\\subsection{Diffusion Models}\n\\label{ssec:DMs}\n\nA generative model, in the context of generative AI, exploits machine learning to learn a probability distribution of the training data to generate new data samples. The very first models are based on Autoencoders (AEs) that learn to encode input data into a lower-dimensional representation (latent space) and then decode it back to its original form. The specific type of AEs, Variational Autoencoders (VAEs) \\cite{Kingma:auto:2014}, learns the latent space as statistic parameters of probabilistic distributions, leading to significant improvement of the generated results. Concurrently, in the same year, Goodfellow et al. \\cite{Goodfellow:GAN:2014} introduced an alternative architecture known as a Generative Adversarial Network (GAN). GANs comprise two competing AI modules: the generator, which creates a sample, and the discriminator, which determines whether the received sample is real or generated. When comparing VAEs to GANs, VAEs exhibit greater stability during training, whereas GANs excel at producing realistic images. More details about AEs and GANs for creative technologies can be found in our previous review \\cite{Anantrasirichai:AI:2022}.\n\nThe important factor driving the rapid growth of generative AI is the development of diffusion probabilistic models, or in short, diffusion models (DMs). The first DM was introduced in 2015 by Sohl-Dickstein et al. \\cite{Dickstein:Deep:2015}, using Nonequilibrium Thermodynamics. However, it took 5 years for DMs to generate desirable results. The era of DMs began with Denoising Diffusion Probabilistic Models (DDPMs) proposed by Ho et al. \\cite{Ho:DDPM:2020} in 2020 and Score-based diffusion models proposed by Song et al. \\cite{Song:Score:2021} in 2021. These are a simplified process using a denoising autoencoder to approximate Bayesian inference. In brief, the models leverage a diffusion process to learn a probability distribution of the input data. As the name suggests, the data is diffused by gradually adding noise at each iteration step as shown in Fig. \\ref{fig:generativemodel} (b). A deep neural network (DNN) is then trained to remove this noise, called the denoising process or reverse process. Consequently, the trained model uses random noise to generate data with characteristics similar to those of the training samples. Comparing to GANs, the DMs provide higher diversity samples \\cite{Dhariwal:Diffusion:2021} and the training process is much more stable and do not suffer from mode collapse; however, the DMs are computationally intensive and require longer training times compared to GANs. The complexity is significantly reduced by training the DMs in latent space. Latent Diffusion Models (LDM) \\cite{Rombach:LDM:2022} use pretrained networks to convert images to feature maps, and perform training on a low-dimensional space. The diagram of LDM is shown in Fig. \\ref{fig:generativemodel} (c). \n\nGenerating a synthesized sample at random might not be particularly useful, especially for creative industries. Therefore, conditional diffusion models have been proposed, with a wide range of applications such as text-to-sound, text-to-images, and image-to-videos. For DMs, the conditional distributions are modelled using a conditional denoising autoencoder. Classifier guidance was introduced in \\cite{Dhariwal:Diffusion:2021} to improve a diffusion generator in generating images of a desired class. For example, when we provide the model with more information, such as `a flower', the DM will synthesize a variety of flower images, as the word `flower' guides the model toward the latent distribution that is formed by various images of flowers. The work in \\cite{Choi:ILVR:2021} simply refines the latent space of well-trained unconditional DDPM so that the higher-level semantics of the synthetic samples are similar to the reference (conditioning).\nThe LDM \\cite{Rombach:LDM:2022} offers more flexible conditional image generators by adding cross-attention layer (referred to Transformers in Section \\ref{ssec:transformers}) to the denoising autoencoder. A survey on the methods and applications of DMs prior to 2024 can be found in \\cite{Cao:survey:2024}.\n\n\n\n\\subsection{Implicit Neural Representations}\n\nImplicit Neural Representations (INR), also called neural fields, neural implicits, and coordinate-based neural networks, represent input content implicitly through learned functions $F$F, as shown in Eq.~\\ref{eqn:inr}. They can be seen as fields $x$x (represented by a scalar, vector, or tensor with a value, such as magnetic field in physics) that are fully or partially parameterized by a neural network $\\Phi$\\Phi, typically an MLP \\cite{xie2022neural}.\n\\begin{equation}\nF(x, \\Phi, \\nabla_x \\Phi, \\nabla_x^2 \\Phi, \\dots) = 0, \\quad \\Phi : x \\mapsto \\Phi(x).\n\\label{eqn:inr}\n\\end{equation}\\begin{equation}\nF(x, \\Phi, \\nabla_x \\Phi, \\nabla_x^2 \\Phi, \\dots) = 0, \\quad \\Phi : x \\mapsto \\Phi(x).\n\\label{eqn:inr}\n\\end{equation}\nF(x, \\Phi, \\nabla_x \\Phi, \\nabla_x^2 \\Phi, \\dots) = 0, \\quad \\Phi : x \\mapsto \\Phi(x).\n\\label{eqn:inr}\n\n\n The concept might sound complex, but the process is actually very straightforward. For example, in the case of an image, the coordinate of each pixel $(x, y)$(x, y) contains colors $(r, g, b)$(r, g, b). The INR inputs $(x, y)$(x, y) to the MLP and learns to give the output $(r, g, b)$(r, g, b). The weights and biases of the MLP now represent such an image. Usually, the number of parameters of the MLP is smaller than the total number of pixels multiplied by 3, accounting for the 3 color channels. Hence, one of its applications is data compression \\cite{kwan2024hinerv}. Moreover, the INR can handle complex and high-dimensional data efficiently, bringing attention in visual computing, such as 3D scene reconstruction.\n\nTraditional MLPs employ ReLU (rectified linear unit) for non-linear activation due to its simplicity. However, Sitzmann et al. \\cite{sitzmann:siren:2020} demonstrate that using periodic functions, such as sinusoids, is more suitable for representing complex natural signals, offering a better fit to the first- and second-order derivatives of the signals. However, this activation could cause ringing artifacts. Saragadam et al. instead proposed using complex Gabor wavelets \\cite{Saragadam:wire:2023}, which learn to represent high frequencies better and simultaneously are robust to noise.\n\nOne of the fastest-growing areas that exploits INRs is \\textbf{Neural Radiance Fields (NeRF)}, evidenced by 57 papers presented at CVPR, the largest annual conference in computer vision, in 2022 and which grew to 175 papers in 2023\\footnote{\\url{https://markboss.me/post/nerf_at_cvpr23/}}. First introduced in 2020 by Mildenhall et al. \\cite{Mildenhall:NeRF:2020}, NeRF is a form of neural rendering, a subset of generative AI, that generates novel views of a scene based on a partial set of 2D images. It achieves this by learning a mapping from 3D spatial coordinates and view directions $(x,y,z,\\theta,\\phi)$(x,y,z,\\theta,\\phi) to colors and density $(r,g,b,\\sigma)$(r,g,b,\\sigma). This implicit representation allows NeRF to handle complex scenes with varying geometry and appearance,  resulting in highly realistic renderings that include accurate lighting, shadows, and reflections. More detail can be found in Section \\ref{sssec:nerf}.\n\n\n\n\n\\begin{table}\n\\caption{Creative applications and corresponding AI-based methods mentioned in this paper}\n%\\tiny\n %\\hskip-5.0cm\n %\\begin{tabular}{p{1cm}p{1.4cm}|p{4cm}p{4cm}p{4cm}p{4cm}}\n \\resizebox{\\linewidth}{!}{\\begin{tabular}{lr|lll}\n \\\\\n \\toprule\n\\multicolumn{2}{c}{\\multirow{2}{*}{Application}} & \\multicolumn{3}{|c}{Technology} \\\\ \\cmidrule{3-5}\n& & Trans./Attn.$^1$ & Diffusion model$^2$  & INR\\\\\n\\midrule\n{\\bf Creation} & Text & \\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}  & \\\\\n& audio/music & \\cite{Alayrac:Flamingo:2022, Huang:GenerSpeech:2022} & \\cite{Li:Diffusion:2022, Yang:Diffsound:2023} & \\\\\n& Image &  \\cite{Alayrac:Flamingo:2022,esser:scaling:2024} & \\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024} & \\\\\n% ---------------------------------------------------------------\n& Animation/video & \\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024} & \\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024} &\\\\\n% ---------------------------------------------------------------\n & 3D/AR/VR & \\cite{yang:Holodeck:2024} & \\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024} & \\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear} \\\\\n% ---------------------------------------------------------------\n\\midrule\n{\\bf Information} & Text categorization & \\cite{sun:text:2023, shi:chatgraph:2023, Hou:promptboosting:2023} \\\\\n% ---------------------------------------------------------------\n{\\bf Analysis} & Film analysis & \\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}  \\\\\n % ---------------------------------------------------------------\n & Content retrieval & \\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning} & \\cite{Jin:DiffusionRet:2023} \\\\\n % ---------------------------------------------------------------\n & Intelligent assistants & \\cite{King:Sasha:2024} \\\\\n % ---------------------------------------------------------------\n \\midrule\n{\\bf Content} & Enhancement  & \\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024} & \\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight} & \\cite{Yang:Implicit:2023} \\\\\n{\\bf  Enhancement  } & Style transfer & \\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR} & \\cite{Zhang:Inversion:2023,Chai:StableVideo:2023} & \\cite{Moon:generalizable:2023,Kim:Controllable:2024} \\\\\n% ---------------------------------------------------------------\n {\\bf  and Post} & Super-resolution & \\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024} & \\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023} & \\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}\\\\\n% ---------------------------------------------------------------\n{\\bf Production} & {Restoration} & \\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb} & \\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023} & \\cite{Jiang:NeRT:2023} \\\\\n  % ---------------------------------------------------------------\n & Inpainting & \\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024} & \\cite{Moliner:solving:2023, Fei:Generative:2023} & \\\\\n % ---------------------------------------------------------------\n& Fusion & \\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023} & \\cite{Zhao:DDFM:2023} & \\\\\n & Editing/VFX & \\cite{Shi:Motion-I2V:2024} & \\cite{Shi:Motion-I2V:2024,guo2024liveportrait} \\\\\n \\midrule\n % ---------------------------------------------------------------\n{\\bf Information} & Segmentation & \\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2} & \\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024} & \\cite{Gong:Continuous:2023, Cen:Segment:2023} \\\\\n % ---------------------------------------------------------------\n{\\bf  Extraction} & Recognition &  \\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024} & \\cite{Li:Your:2023,Chen:DiffusionDet:2023}\\\\\n % ---------------------------------------------------------------\n{\\bf  and}  & Tracking &  \\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring} & \\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024} & \\cite{Jung:AnyFlow:2023}\\\\\n % ---------------------------------------------------------------\n{\\bf  Understanding}  & 3D Reconstruction  & \\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024} & \\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024} & \\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, azzarelli:waveplanes:2023},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025}$^\\dag$  \\\\\n\\midrule\n\n% ---------------------------------------------------------------\n{\\bf Compression} & Image$^\\ast$ & \\cite{zhu2022transformer,zou2022devil,liu2023learned}& \\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual} & \\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}\\\\\n& Video& \\cite{xiang2022mimt,mentzer2022vct}& \\cite{li2024extreme}&\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}\\\\\n&Audio$^\\ast$ & & &\\\\\n\\midrule\n% ---------------------------------------------------------------\n{\\bf Quailty}  &  Image$^\\ast$ & \\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}& &\\\\\n\\textbf{Assessment}& Video$^\\ast$ & \\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}& & \\\\\n\\bottomrule\n\\multicolumn{5}{l}{$^1$ Trans./Attn. include both transformers and CNN-based architectures that use attention module.} \\\\\n\\multicolumn{5}{l}{$^2$ Some diffusion models employ the transformer in their denoising autoencoders.}\n \\end{tabular}}\n \\footnotesize\n $^\\dag$ \nThese methods are based on explicit neural representations. \\\\\n$^*$ It is noted that for some compression and quality assessment tasks, there are other dominant network architectures in existing works. For example, LLMs have been used for image and audio compression, and visual quality assessment. Many neural audio codecs are also based on VQ-VAE models.\n \n\\label{tab:gather}\n\\end{table}\n\\caption{Creative applications and corresponding AI-based methods mentioned in this paper}\n\\resizebox{\\linewidth}\\linewidth{!}!{\\begin{tabular}{lr|lll}\n \\\\\n \\toprule\n\\multicolumn{2}{c}{\\multirow{2}{*}{Application}} & \\multicolumn{3}{|c}{Technology} \\\\ \\cmidrule{3-5}\n& & Trans./Attn.$^1$ & Diffusion model$^2$  & INR\\\\\n\\midrule\n{\\bf Creation} & Text & \\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}  & \\\\\n& audio/music & \\cite{Alayrac:Flamingo:2022, Huang:GenerSpeech:2022} & \\cite{Li:Diffusion:2022, Yang:Diffsound:2023} & \\\\\n& Image &  \\cite{Alayrac:Flamingo:2022,esser:scaling:2024} & \\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024} & \\\\\n% ---------------------------------------------------------------\n& Animation/video & \\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024} & \\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024} &\\\\\n% ---------------------------------------------------------------\n & 3D/AR/VR & \\cite{yang:Holodeck:2024} & \\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024} & \\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear} \\\\\n% ---------------------------------------------------------------\n\\midrule\n{\\bf Information} & Text categorization & \\cite{sun:text:2023, shi:chatgraph:2023, Hou:promptboosting:2023} \\\\\n% ---------------------------------------------------------------\n{\\bf Analysis} & Film analysis & \\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}  \\\\\n % ---------------------------------------------------------------\n & Content retrieval & \\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning} & \\cite{Jin:DiffusionRet:2023} \\\\\n % ---------------------------------------------------------------\n & Intelligent assistants & \\cite{King:Sasha:2024} \\\\\n % ---------------------------------------------------------------\n \\midrule\n{\\bf Content} & Enhancement  & \\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024} & \\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight} & \\cite{Yang:Implicit:2023} \\\\\n{\\bf  Enhancement  } & Style transfer & \\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR} & \\cite{Zhang:Inversion:2023,Chai:StableVideo:2023} & \\cite{Moon:generalizable:2023,Kim:Controllable:2024} \\\\\n% ---------------------------------------------------------------\n {\\bf  and Post} & Super-resolution & \\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024} & \\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023} & \\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}\\\\\n% ---------------------------------------------------------------\n{\\bf Production} & {Restoration} & \\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb} & \\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023} & \\cite{Jiang:NeRT:2023} \\\\\n  % ---------------------------------------------------------------\n & Inpainting & \\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024} & \\cite{Moliner:solving:2023, Fei:Generative:2023} & \\\\\n % ---------------------------------------------------------------\n& Fusion & \\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023} & \\cite{Zhao:DDFM:2023} & \\\\\n & Editing/VFX & \\cite{Shi:Motion-I2V:2024} & \\cite{Shi:Motion-I2V:2024,guo2024liveportrait} \\\\\n \\midrule\n % ---------------------------------------------------------------\n{\\bf Information} & Segmentation & \\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2} & \\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024} & \\cite{Gong:Continuous:2023, Cen:Segment:2023} \\\\\n % ---------------------------------------------------------------\n{\\bf  Extraction} & Recognition &  \\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024} & \\cite{Li:Your:2023,Chen:DiffusionDet:2023}\\\\\n % ---------------------------------------------------------------\n{\\bf  and}  & Tracking &  \\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring} & \\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024} & \\cite{Jung:AnyFlow:2023}\\\\\n % ---------------------------------------------------------------\n{\\bf  Understanding}  & 3D Reconstruction  & \\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024} & \\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024} & \\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, azzarelli:waveplanes:2023},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025}$^\\dag$  \\\\\n\\midrule\n\n% ---------------------------------------------------------------\n{\\bf Compression} & Image$^\\ast$ & \\cite{zhu2022transformer,zou2022devil,liu2023learned}& \\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual} & \\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}\\\\\n& Video& \\cite{xiang2022mimt,mentzer2022vct}& \\cite{li2024extreme}&\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}\\\\\n&Audio$^\\ast$ & & &\\\\\n\\midrule\n% ---------------------------------------------------------------\n{\\bf Quailty}  &  Image$^\\ast$ & \\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}& &\\\\\n\\textbf{Assessment}& Video$^\\ast$ & \\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}& & \\\\\n\\bottomrule\n\\multicolumn{5}{l}{$^1$ Trans./Attn. include both transformers and CNN-based architectures that use attention module.} \\\\\n\\multicolumn{5}{l}{$^2$ Some diffusion models employ the transformer in their denoising autoencoders.}\n \\end{tabular}}\n \\\\\n \\toprule\n\\multicolumn{2}2{c}c{\\multirow{2}{*}{Application}}\\multirow{2}2{*}*{Application}Application & \\multicolumn{3}3{|c}|c{Technology}Technology \\\\ \\cmidrule{3-5}3-5\n& & Trans./Attn.$^1$^1 & Diffusion model$^2$^2  & INR\\\\\n\\midrule\n{\\bf Creation}\\bf Creation & Text & \\cite{Vaswani:attention:2017,Wang:SIMVLM:2022, openai:gpt4:2023, wei2024vary}  & \\\\\n& audio/music & \\cite{Alayrac:Flamingo:2022, Huang:GenerSpeech:2022} & \\cite{Li:Diffusion:2022, Yang:Diffsound:2023} & \\\\\n& Image &  \\cite{Alayrac:Flamingo:2022,esser:scaling:2024} & \\cite{Rombach:LDM:2022, Brooks:InstructPix2Pix:2023, gal:Image:2023, Gandikota:Unified:2024, Lian:LLMG:2024,esser:scaling:2024,ren:hypersd:2024} & \\\\\n& Animation/video & \\cite{hong:cogvideo:2023,villegas:phenaki:2023,Azadi:Make:2023,Yu:Bidirectionally:2023,Liu:FETV:2023,wang:disco:2024, xu:VASA-1:2024,corona:vlogger:2024, Gupta:Photorealistic:2024, Hu_2024_CVPR, Zhu:INFP:2024} & \\cite{singer:Make:2023, molad:dreamix:2023, wang:modelscope:2023, wu:tune:2023,Liu:FETV:2023, Gupta:Photorealistic:2024, Zhu:INFP:2024} &\\\\\n& 3D/AR/VR & \\cite{yang:Holodeck:2024} & \\cite{Xu:NeuralLift:2023, Melas:RealFusion:2023, Qian:Magic123:2024,tang:dreamgaussian:2024} & \\cite{tang:dreamgaussian:2024, ren:dreamgaussian4d:2023, zhao2024clear} \\\\\n\\midrule\n{\\bf Information}\\bf Information & Text categorization & \\cite{sun:text:2023, shi:chatgraph:2023, Hou:promptboosting:2023} \\\\\n{\\bf Analysis}\\bf Analysis & Film analysis & \\cite{Mao:Biases:2023,krugmann:sentiment:2024,Hartmann:More:2023}  \\\\\n & Content retrieval & \\cite{Metzler:Rethinking:2021, Yan:Universal:2023, Lu:content:2023, Rajput:recommender:2023, li:unigen:2024, li2024learning} & \\cite{Jin:DiffusionRet:2023} \\\\\n & Intelligent assistants & \\cite{King:Sasha:2024} \\\\\n \\midrule\n{\\bf Content}\\bf Content & Enhancement  & \\cite{Xu:SNR:2022, liang:RVRT:2022, Wang:Ultra:2023, Lin:SPATIO:2024, Youk:FMA:2024, Liang:VRT:2024} & \\cite{HOU:Global:2023, Yi:Diff:2023, Jiang:Low:2023, lin2024lowlight} & \\cite{Yang:Implicit:2023} \\\\\n{\\bf  Enhancement  }\\bf  Enhancement   & Style transfer & \\cite{Deng:StyTr2:2022, Moon:generalizable:2023, Chung_2024_CVPR} & \\cite{Zhang:Inversion:2023,Chai:StableVideo:2023} & \\cite{Moon:generalizable:2023,Kim:Controllable:2024} \\\\\n{\\bf  and Post}\\bf  and Post & Super-resolution & \\cite{Liang:SwinIR:2021, Lu:Transformer:2022, Liu:Learning:2022, Chen:Activating:2023,li:GRL:2023, kang:gigagan:2023, Liang:VRT:2024, xu:videogigagan:2024} & \\cite{Saharia:image:2023, Moliner:solving:2023, Gao:Implicit:2023} & \\cite{Chen:Learning:2021, Saharia:image:2023, Fei:Generative:2023, Gao:Implicit:2023, Yin:CLE:2023}\\\\\n{\\bf Production}\\bf Production & {Restoration}Restoration & \\cite{Wang:Uformer:2022,Zamir:Restormer:2022, li:GRL:2023,yang:ldp:2023, Liang:VRT:2024, Morris:DaBiT:2024, Liang:SwinIR:2021,Fan:SUNet:2022, Yu:DBT:2022, Wang:Painter:2023, Song:vision:2023,Xu:Video:2023, mao:single:2022,Zhang:Image:2024,zou2024deturb} & \\cite{Jiang:Low:2023,Fei:Generative:2023, yang:realworld:2023, Nair:AT-DDPM:2023,Jaiswal:Physics:2023} & \\cite{Jiang:NeRT:2023} \\\\\n  & Inpainting & \\cite{Li:MAT:2022,Liu:Reduce:2022,Ren:DLFormer:2022,Zhou:ProPainter:2023,HUANG:Sparse:2024} & \\cite{Moliner:solving:2023, Fei:Generative:2023} & \\\\\n & Fusion & \\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023,Liu:Multi:2023} & \\cite{Zhao:DDFM:2023} & \\\\\n & Editing/VFX & \\cite{Shi:Motion-I2V:2024} & \\cite{Shi:Motion-I2V:2024,guo2024liveportrait} \\\\\n \\midrule\n {\\bf Information}\\bf Information & Segmentation & \\cite{Bowen:Marked:2022, Kirillov:SAM:2023, Ke:SAM-HQ:2023,Wang:Painter:2023,Wang:SegGPT:2023,Zou:Segment:2023, Oquab:DINOv2:2024, ravi2024sam2} & \\cite{Wu:DiffuMask:2023, Xu:Open:2023, Gu:Diffusioninst:2024} & \\cite{Gong:Continuous:2023, Cen:Segment:2023} \\\\\n {\\bf  Extraction}\\bf  Extraction & Recognition &  \\cite{Carion:DERT:2020, Dosovitskiy:image:2021,Zhu:Deformable:2021, Liu:Swin:2021,  Neimark:video:2021, Liu:Swinv2:2022, Huang:MonoDTR:2022, Oquab:DINOv2:2024, zhao:videoprism:2024} & \\cite{Li:Your:2023,Chen:DiffusionDet:2023}\\\\\n {\\bf  and}\\bf  and  & Tracking &  \\cite{Meinhardt:TrackFormer:2022,zeng:motr:2022,cui:mixformer:2022,Mayer:Transforming:2022, yang:track:2023,Chen:SeqTrack:2023,Zhang:MOTRv2:2023, Yi:Comprehensive:2024, kang2025exploring} & \\cite{Luo:DiffusionTrack:2024, Xie:DiffusionTrack:2024, Zhang:DiffusionTracker:2024} & \\cite{Jung:AnyFlow:2023}\\\\\n {\\bf  Understanding}\\bf  Understanding  & 3D Reconstruction  & \\cite{Wang:multi:2021, Zhang:Lite:2023, Chen:Vision:2023,Yang:depthanything:2024,Oquab:DINOv2:2024,Yang:depthanythingv2:2024} & \\cite{Barron:Mip-NeRF360:2022, Ji:DDP:2023, wynn:diffusionerf:2023, Ke:Repurposing:2024} & \\cite{Mildenhall:NeRF:2020,pumarola:DNeRF:2020, mueller:instant:2022,Barron:Mip-NeRF360:2022,Mildenhall:NeRFDark:2022,Fang:Fast:2022, Guo:neural:2022, azzarelli:waveplanes:2023},\\cite{ Fridovich:kplanes:2023,  kerbl:3Dgaussians:2023, wu:4dgaussians:2024, Yu:CoGS:2024, Huang:SCGS:2024, Wang2025}$^\\dag$^\\dag  \\\\\n\\midrule\n\n{\\bf Compression}\\bf Compression & Image$^\\ast$^\\ast & \\cite{zhu2022transformer,zou2022devil,liu2023learned}& \\cite{careil2023towards,yang2024lossy,hoogeboom2023high,ghouse2023residual} & \\cite{sitzmann2020implicit,dupont2021coin,dupontcoin++,strumpler2022implicit}\\\\\n& Video& \\cite{xiang2022mimt,mentzer2022vct}& \\cite{li2024extreme}&\\cite{chen2021nerv,bai2023ps,kwan2024hinerv,kim2024c3,leguay2024cool,kwan2024nvrc,gao2024pnvc,ruan2024point,kwan2024immersive}\\\\\n&Audio$^\\ast$^\\ast & & &\\\\\n\\midrule\n{\\bf Quailty}\\bf Quailty  &  Image$^\\ast$^\\ast & \\cite{cheon2021perceptual,golestaneh2022no, shi2024transformer}& &\\\\\n\\textbf{Assessment}& Video$^\\ast$^\\ast & \\cite{wu2022fast,feng2024rankdvqa,wu2023exploringvideo,he2024cover,peng2024rmt}& & \\\\\n\\bottomrule\n\\multicolumn{5}5{l}l{$^1$ Trans./Attn. include both transformers and CNN-based architectures that use attention module.}$^1$^1 Trans./Attn. include both transformers and CNN-based architectures that use attention module. \\\\\n\\multicolumn{5}5{l}l{$^2$ Some diffusion models employ the transformer in their denoising autoencoders.}$^2$^2 Some diffusion models employ the transformer in their denoising autoencoders.\n \n \\footnotesize\n $^\\dag$^\\dag \nThese methods are based on explicit neural representations. \\\\\n$^*$^* It is noted that for some compression and quality assessment tasks, there are other dominant network architectures in existing works. For example, LLMs have been used for image and audio compression, and visual quality assessment. Many neural audio codecs are also based on VQ-VAE models.\n \n\\label{tab:gather}\n\n ", "appendix": false}, "Advanced AI for the creative industries": {"content": "\n\\label{sec:existing}\n\nSimilar to our previous paper reviewing AI for the creative industries, we categorize the applications and the corresponding AI-based solutions as shown in Table \\ref{tab:gather}. However, the subsections may differ as generative AI affects each application differently.\n\n\\subsection{Content creation}\n\nContent creation stands as a fundamental activity of artists and designers and the term `\\textit{AI art}' refers to either created with the assistance of AI algorithms or entirely by AI systems. It is in various digital forms, including images, texts, audio, and videos. The roots of AI art can be traced back to before the 2000s, exemplified by AARON, a computer program initiated in 1972 to autonomously produce paintings and drawings \\cite{encyclopedia_ai_v1}. The practicality of AI art has actually begun with advancements in deep learning, particularly GAN from 2014 and, more recently, transformers, DMs and INRs. Note that when referring to real camera content acquisition using AI, commonly known as intelligent cinematography, we direct the reader to the review in \\cite{Azzarelli:Reviewing:2024}.\n\n\\subsubsection{Text generation, script and journalism}\n\nIn the era of LLMs, AI writing tools have been widely used to help with various writing tasks, including generate written content of articles, blog posts, essays, and reports. These tools go beyond mere grammar and spelling checks; they boast advancements enabling them to analyze the style and tone of written material, adding images, videos and tables, offering suggestions to enhance clarity, coherence, and overall readability \\cite{ippolito:creative:2022}. Moreover, AI tools extend their utility beyond content generation by automating tasks like keywords, meta tags, and descriptions, thereby increasing search rankings using search engine optimization (SEO). Additionally, they support the process of publishing across multiple online platforms. Transformers are used to generate image captions by combining information of the images with a word prefix or questions \\cite{Wang:SIMVLM:2022}.\n\nAI script generators serve as beneficial aids for writers, filmmakers, and game developers, offering inspiration, idea generation, and assistance in crafting entire scripts \\cite{Jeary2024, Azzarelli:Reviewing:2024}. Human-AI brainstorming is helpful and saves time \\cite{guo:exploring:2024}. Presently, there are numerous software and websites providing both free and paid script generation services. However, many of these tools are still constrained when it comes to longform creative writing. Dramatron, developed by Google \\cite{Mirowski:cowriting:2023}, introduces hierarchical language generation, enabling the creation of cohesive scripts and screenplays spanning long ranges. This includes elements such as titles, characters, story beats, location descriptions, and dialogue.\n\nAs discussed earlier in this paper, chatbots are now powered by LLMs, effectively simulating human conversation. These fundamental LLMs are specialized for specific tasks, for instance, journalist AI and blog AI writers\\footnote{For example, see \\url{https://tryjournalist.com/}}, which generate content with layouts suitable for print or online publication. Additionally, there are AI tools designed to detect AI-generated content (e.g., for checking for copyrights), AI-writing styles and content originality to ensure the natural sound of articles. Undoubtedly, generative AI is reshaping the way artists and journalists operate. For an in-depth exploration of the impact and implications of these technological advancements on news organizations, refer to the survey conducted by Beckett et al. \\cite{Beckett:Generating:2023}.\n\nGenerating text and scripts automatically can also be done through image and video inputs without text prompts (e.g., image captioning \\cite{Stefanini:From:2023}) and with text prompts. The technical term of these approaches is called Vision Language Models (VLMs), which are  multimodal models that learn from images and text. The most common and prominent models often consist of an image encoder, an embedding projector to align image and text representation, often via a dense neural network, and a text decoder stacked in this order. The most well-known technique is Contrastive Language-Image Pre-training (CLIP) \\cite{radford2021learning}. More recent work in \\cite{wei2024vary} scales up the vision vocabulary by incorporating new image features into the existing CLIP model, resulting in improved content understanding. The comprehensive survey of VLMs for vision tasks can be found in \\cite{Zhang:vision:2024}.\n\n\\subsubsection{Audio and Music generation}\n\\label{sssec:musicgen}\n\nSimilar to language models, AI-based music generation has rapidly advanced due to unsupervised learning on large datasets and the use of transformers (see Section \\ref{ssec:llms}). Examples of such software include MuseNet\\footnote{\\url{https://openai.com/research/musenet}}, Magenta Studio\\footnote{\\url{https://magenta.tensorflow.org/studio}}, and Musicfy\\footnote{\\url{https://musicfy.lol/}}. These tools can assist in composing music by learning complex musical patterns, predicting the next word or music note in a sequence, and mixing specified instruments. Moreover, the AI tools can convert one type of sound into another, such as from whistling to violin or from flute to saxophone\\footnote{See an example by Ummet Ozcan at \\url{https://www.youtube.com/watch?v=lI1LCfTx2lI}}. This capability is invaluable for artists who may not be proficient in playing all the instruments they wish to incorporate, saving both time and costs. March 2024, Suno has released their first model capable of producing radio-quality music that can be created in 2 minutes\\footnote{\\url{https://www.suno.ai/blog/v3}}. Later, Udio \\footnote{\\url{https://www.udio.com/}}, was launched. It offers a prompt to create lyrics and music with a maximum duration of 90 seconds, and also appears to have at least some awareness of copyright.\n\nAI voice software changes voice from one person to another and enable users to train the model to convert other people's voices into their own, e.g. lalals\\footnote{\\url{https://lalals.com/}}, Kits\\footnote{\\url{https://www.kits.ai/}}, Media.io\\footnote{\\url{https://www.media.io/online-voice-changer.html}}, etc. Certain software, such as Voice.ai\\footnote{\\url{https://voice.ai/}}, even offer real-time voice changing capabilities. The technologies behind this use the transformer to learn voice features and patterns in mel-spectrogram. For example, the framework proposed in \\cite{Yang:Diffsound:2023} uses a method based on DM with transformer backbone to turn text input into a mel-spectrogram using the vector quantized variational autoencoder (VQ-VAE) \\cite{Oord:Neural:2017}. Next, this mel-spectrogram is transformed into a sound wave.  Unlike a regular spectrogram, the mel-spectrogram is based on the mel-frequency scale, which offers higher resolution for lower frequencies. Voice style transfer often use zero-shot learning (a model is trained to recognize classes or categories it has never seen during training) \\cite{Huang:GenerSpeech:2022} or few-shot learning (a model is trained with only one or a few examples per class) \\cite{Wang:One:2022}.\n\nAnother emerging AI technology is in the field of spatial audio. In 2022, Apple Music revealed that, in just over a year, more than 80\\% of its worldwide subscribers were enjoying the spatial audio experience, with monthly plays in spatial audio increasing by over 1,000\\%\\footnote{\\url{https://www.apple.com/uk/newsroom/2023/01/apple-celebrates-a-groundbreaking-year-in-entertainment/}}. With head tracking, this technology significantly enhances the immersive experience. Masterchannel has launched SpatialAI\\footnote{\\url{https://platform.masterchannel.ai/spatial}}, claiming it to be the world's first spatial mastering AI. It processes audio files and returns an optimized track for streaming platforms, along with an individually optimized stereo version for traditional distribution. All these advancements leverage transformer-based technologies.\n\n\\subsubsection{Image generation}\n\nAs described in Section \\ref{ssec:DMs}, recent advances in AI technologies for image generation are based on Diffusion Models (DMs). Well-known and highly competitive text-to-image models include Stable Diffusion\\footnote{\\url{https://stability.ai/stable-image}}, Midjourney\\footnote{\\url{https://www.midjourney.com/home}}, DALL\u00b7E\\footnote{\\url{https://openai.com/dall-e-3}}, and Ideogram\\footnote{\\url{https://ideogram.ai/}}. Released in March 2024, the latest version, Stable Diffusion 3 (SD3) \\cite{esser:scaling:2024}, reports outperforming state-of-the-art text-to-image generation systems such as DALL\u00b7E 3 (released August 2023), Midjourney v6 (released December 2023), and Ideogram v1 (released February 2024) in terms of typography and prompt adherence, based on human preference evaluations. These open-source tools are built with the Multimodal Diffusion Transformer (MM-DiT) architecture, which integrates attention from both text and image. Examples of text-to-image generation are shown in Fig. \\ref{fig:LLMGround} (a) comparing the performance of four models, i.e. Ideogram v1, DALL\u00b7E 3, Photoshop 2024, and sdxy-turbo by Nvidia. It is obvious that hands are the most difficult to generate, e.g., one hand has six fingers.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/LLMGround.png}\n    \\caption{Text-to-image generation (generated on 27 November 2024). (a) text-to-image generation by Ideogram v1, DALL\u00b7E 3, Photoshop 2024, and sdxy-turbo by Nvidia. (b) The top-row images were generated by DALL\u00b7E in ChatGPT 4. The bottom-row images are generated by LLM-grounded Diffusion \\cite{Lian:LLMG:2024}. }\n    \\label{fig:LLMGround}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/LLMGround.png}\n    \\caption{Text-to-image generation (generated on 27 November 2024). (a) text-to-image generation by Ideogram v1, DALL\u00b7E 3, Photoshop 2024, and sdxy-turbo by Nvidia. (b) The top-row images were generated by DALL\u00b7E in ChatGPT 4. The bottom-row images are generated by LLM-grounded Diffusion \\cite{Lian:LLMG:2024}. }\n    \\label{fig:LLMGround}\n\n\n\nDALL\u00b7E 3, available on ChatGPT 4, also provides an inpainting tool, allowing the user to manually select the area to edit. However, as of April 2024, the performance is still limited. As illustrated in Fig. \\ref{fig:LLMGround} (b), the selected area is the white car, and with the follow-up request to change the white car to the red car, DALL\u00b7E 3 generates correctly. However, if asked to replace it with a bicycle, it does not work. Recently, LLM-grounded Diffusion \\cite{Lian:LLMG:2024} enables instruction-based scene specification with multiple rounds of user requests without a need of manual selection on the image. This is achieved by generating layout-grounded images first using stable diffusion, then masking the latent variables as priors for the next round of generation\\footnote{Images in Fig. \\ref{fig:LLMGround} (b) were generated using their demo: https://huggingface.co/spaces/longlian/llm-grounded-diffusion.}.  \n\nSimilar to DALL\u00b7E 3, Photoshop features a Generative Fill tool\\footnote{\\url{https://www.adobe.com/th_en/products/photoshop/generative-fill.html}} designed to generate new images or assist with photo editing. It accepts a text prompt and provides several generation choices. After defining the editing area, users can remove and add new objects (more inpainting tasks are discussed in Section \\ref{ssec:inpaiting}), transfer to new styles, and expand content within images. Recently, InstructPix2Pix \\cite{Brooks:InstructPix2Pix:2023} proposed a conditional diffusion model to generate image editing examples without predefined editing areas. It combines GPT-3 and Stable Diffusion. This clearly demonstrates that the model effectively captures and matches the semantic meaning of the content in both text and image. Sometimes, style and context are not easy to describe in words. Textual Inversion \\cite{gal:Image:2023} personalizes large pre-trained text-to-image diffusion models based on specific objects and styles, using 3-5 images of a user-provided concept. Recently, ByteDance announces Hyper-SD \\cite{ren:hypersd:2024} proposed trajectory segmented consistency distillation that provides real-time high-resolution image generation from drawing with a control text prompt. \n\n\n\n\\subsubsection{Video generation and animation} \n\\label{sssec:videogen}\n\nDespite the success of text-to-image generation, text-to-video generation has just started to grow rapidly in 2024, largely due to its computational expense and content complexity. However, several major companies and private platforms have released their progress in this area, including Gemini 1.5 by Google, Make-A-Video by Meta, and Sora by OpenAI. Make-A-Video \\cite{singer:Make:2023}, through a spatiotemporally factorized diffusion model, leverages joint text-image priors and super-resolution in space and time. Some results however contain  flickering artifacts\\footnote{\\url{https://makeavideo.studio/}}. Gen-2 by Runway\\footnote{\\url{https://research.runwayml.com/gen2}} offering both text- and image-to-video can generate a smooth 4-sec video. In April 2024, Adobe Premier Pro has announced their integration of generative AI tools for video extension with third-party models by OpenAI, Runway and Pika Labs\\footnote{\\url{https://www.adobe.com/products/premiere/ai-video-editing.html}}. Its new update also includes contextual-selection tool, inpainting for object removal, and object addition to the defined areas in the videos with a text prompt. \n\nText-to-video technologies, combined with AI voice, have been tested not only by artists or producers but also by a wider audience. Results from these tests, such as automatically turning scripts into movie trailers and music videos, have been widely shared on public online platforms\\footnote{\\url{https://twitter.com/minchoi/status/1775907105813217398}}. However, scene composition and transitions still require further editing to align with producers' needs\\footnote{See an example by Curious Refuge at \\url{https://www.youtube.com/watch?v=fJQbP34GoHQ}}.\nIn April 2024, Microsoft introduced VASA-1 \\cite{xu:VASA-1:2024}, which turns a single static image and a speech audio clip into a video clip of realistic talking faces mimicking human facial expressions and head movements, shown in Fig. \\ref{fig:Deepmotion_Vasa} (right). The overall quality of the generated videos is better than VLOGGER by Google \\cite{corona:vlogger:2024}, which is based on similar technology -- diffusion models. However, VLOGGER also offers movement of the upper body and hand gestures. Recently, ByteDance introduced an audio-driven interactive head generation \\cite{Zhu:INFP:2024} that offers listening and speaking states during multi-turn conversations. This framework is based on a conditional diffusion transformer\n.\nThe main technologies of text-to-video and image-to-video tasks are based on DMs with a combination of 3D convolutions (or separately spatial and temporal convolutions), and spatial and temporal attention modules \\cite{wang:modelscope:2023}. Tune-A-Video \\cite{wu:tune:2023} modifies the style of an input video using a text prompt. The method leverages pretrained text-to-image models and introduces attention tuning to ensure temporal consistency. Early video generation methods often exhibit flickering, as observed in the CVPR2023 competition on text-guided video editing, where all results suffered from temporal inconsistency. Dreamix \\cite{molad:dreamix:2023} videos do not have this issue, but they are very blurry. As an example of a transformer-based approach, CogVideo \\cite{hong:cogvideo:2023} employs VQ-VAE to convert input frames to tokens, which are then fused with text tokens to produce a new video. Phenaki \\cite{villegas:phenaki:2023} exploits transformers to generate variable length videos, but the quality is lower than those based on DMs. Evaluations of these methods can be found in \\cite{Liu:FETV:2023}. The recent work applies spatiotemporal layers to model temporal dynamics \\cite{Gupta:Photorealistic:2024}. The transformer blocks have been redesigned for latent video diffusion modeling with window-restricted spatial and spatiotemporal attention. By the end of 2024, there will be many more free and commercial tools for video generation. These include Veo 2 by Google DeepMind, Kling AI\\footnote{https://www.klingai.com/}, Pika 2.0\\footnote{https://pikartai.com/pika-2-0/}, Hailuo AI\\footnote{https://hailuoai.video/}, etc. Though not perfect, the generated videos are close to reality (visit their websites for showcase examples).\n\nGenerating characters with human posture and motion from text prompts has also become popular. Make-An-Animation \\cite{Azadi:Make:2023} trains on image-text datasets and fine-tunes on motion capture data, adding additional layers to model the temporal dimension. Animate Anyone by Alibaba Group \\cite{Hu_2024_CVPR} inputs a real photo or anime of a person with a sequence of guided poses. The results are significantly better than existing techniques, including Disco \\cite{wang:disco:2024} and Bidirectionally Deformable Motion Modulation (BDMM) \\cite{Yu:Bidirectionally:2023}. They also suggest using Animate Anyone  with Outfit Anyone\\footnote{\\url{https://humanaigc.github.io/outfit-anyone/}} to produce a character with a reference outfit.\n\nViggle\\footnote{\\url{https://viggle.ai/}} claims to be the first video-3D foundation model with an actual understanding of physics. It combines a character and a text prompt about motion to generate character animation. Available AI tools on the market include DeepMotion\\footnote{\\url{https://www.deepmotion.com/}} that offers text-to-3D post animation and video-to-3D post animation, shown in Fig. \\ref{fig:Deepmotion_Vasa} (left). The later function can track multiple people from real video and generates replicated characters with the same motions.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/Deepmotion_Vasa.png}\n    \\caption{(Left) Video-to-3D post animation by DeepMotion. (Right) Image and audio to video by VASA-1 \\cite{xu:VASA-1:2024}}\n    \\label{fig:Deepmotion_Vasa}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/Deepmotion_Vasa.png}\n    \\caption{(Left) Video-to-3D post animation by DeepMotion. (Right) Image and audio to video by VASA-1 \\cite{xu:VASA-1:2024}}\n    \\label{fig:Deepmotion_Vasa}\n\n\n\\subsubsection{Augmented, virtual and mixed reality, and 3D content}\n\nWhile the benefits of LLMs in Augmented Reality (AR) directly target educational purposes, enhance cognitive support, and facilitate communication \\cite{XU2025103402}, mixed reality (MR) has once again become exciting since the release of the Apple Vision Pro in February 2024. It has demonstrated the potential of MR experiences by merging real-world environments with computer-generated ones. Thanks to the rapid growth of AI-based 3D representation (see Section \\ref{ssec:3Dreconstruct}), the generation of AR/VR/MR content has advanced significantly. Real-time rendering with immersive interaction has improved, and real scenes can now be generated avoiding uncanny valley effects. There has also been an attempt to use autoregressive and generative models to estimate lighting, achieving a visually coherent environment between virtual and physical spaces in AR \\cite{zhao2024clear}.\n\nSimilar to other content generations, LLMs have been influenced on immersive technologies, including text-to-3D and image-to-3D. Exciting examples include\nHolodeck \\cite{yang:Holodeck:2024}, which automatically generates 3D embodied environments via text prompt conversation of a large language model (GPT-4). 3D objects are gathered from Objaverse \\cite{deitke:Objaverse:2023}, a dataset with 800K+ annotated 3D objects. RealFusion \\cite{Melas:RealFusion:2023}, a single image to 3D object generator, merges 2D diffusion models with NeRF, improving Instant-NGP \\cite{mueller:instant:2022}, which provide API for VR controls. NeuralLift-360 \\cite{Xu:NeuralLift:2023} also uses diffusion models to generate prior for novel view synthesis. Magic123 \\cite{Qian:Magic123:2024} is the latest image-to-3D tool that use  2D and 3D priors simultaneously  to produce high-quality high-resolution 3D geometry and textures. DreamGaussian \\cite{tang:dreamgaussian:2024} offers text-to-3D and image-to-3D by adapting 3D Gaussian splatting (more in Section \\ref{sssec:3DGS}) into generative settings using diffusion prior. This generates photo-realistic 3D assets with explicit mesh and texture maps within only 2 minutes. DreamGaussian4D \\cite{ren:dreamgaussian4d:2023} employs image-to-video diffusion and a 4D Gaussian Splatting representation to generate an image-to-4D model. The results are not very sharp, but they can be further edited with Blender. \nIn July 2024, Shutterstock launched its Generative 3D service in commercial beta, powered by NVIDIA Edify, a multimodal generative AI architecture. This service enables creators to rapidly prototype 3D assets and generate 360-degree HDRi backgrounds to light scenes using text or image prompts. In conjunction with OpenUSD, the created scenes can be rendered into 2D images and used as input for AI-powered image generators, allowing for the production of precise, brand-accurate visuals.\n\n\n\n\n\n\n\n\\subsection{Information analysis}\n\n\\subsubsection{Text categorization}\n\nApplications of text categorization include detecting spam emails, automating customer support, monitoring social media for harmful content, etc. At its core, text categorization involves assigning predefined labels to text documents, which can be anything from a tweet to a lengthy article. LLMs are particularly well-suited for this task due to their ability to comprehend complex and nuanced language. One of the main advantages of using LLMs in text categorization is their transfer learning capability. Models can be pre-trained on a large amount of text and then fine-tuned on a smaller, task-specific dataset, with or without further post-processing technique. For example, CARP \\cite{sun:text:2023} applies kNN to integrate diagnostic reasoning process for final decision. ChatGraph, proposed by Shi et al. \\cite{shi:chatgraph:2023}, utilizes ChatGPT to refine text documents. It uses a knowledge graph, extracted using another specific defined prompt, and finally, a linear model is trained on the text graph for classification. Multiple learners are also used to enhance the performances \\cite{Hou:promptboosting:2023}.\n\n\\subsubsection{Advertisements and film analysis}\n\nNot only does AI assist in generating ideas and content, but it can also aid creators in effectively matching content to their audiences, particularly on an individual level \\cite{feizi:Online:2023}. This effectively helps in advertising personalization\u2014eMarketer\\footnote{\\url{https://www.emarketer.com/content/spotlight-marketing-personalization}} reported that nearly nine out of ten consumers are comfortable with their browsing history being utilized to create personalized ads. In contrast to outdated syntax-style searches, advanced LLM tools can comprehensively grasp user intent behind each search through conversation prompts, providing advertisers with a high level of granularity.\n\nCurrent advances in generative AI would greatly benefit sentiment analysis, also known as opinion mining, where opinions are gather from social media, articles, customer feedback, and corporate communication and are analysed to understand emotion of the owners. This is a potential tool for filmmakers and studios, enabling the creation of effective and targeted marketing campaigns. By analyzing viewer emotions and opinions, AI can provide valuable insights into audience preferences, aiding in the optimization of film marketing strategies. Sentiment analysis with modern generative AI produce more accurate results. Technically, LLMs learn complex patterns and relationships in text data for sentiment classification \\cite{Mao:Biases:2023, krugmann:sentiment:2024}. SiEBERT \\cite{Hartmann:More:2023} provides pre-trained model with open-source scripts to be fine-tuned to further improve accuracy for novel applications.\n\n\\subsubsection{Content retrieval and recommendation services}\n\nGenerative retrieval (GR) was pioneered by Metzler et al. \\cite{Metzler:Rethinking:2021}. Unlike traditional retrieval, which adheres to the ``index-retrieve-then-rank\" paradigm, the GR paradigm employs a single model to obtain results from query input. The model generally involve transformers, generating output token-by-token. More recent work in \\cite{li2024learning} introduces learning-to-rank training to enhance the performance system up typ 30\\%.\nGR has several advantages including substituting the bulky external index with an internal index (i.e., model parameters), significantly reducing memory usage, and enabling optimization during end-to-end model training towards a universal objective for information retrieval tasks. Conversational question answering techniques have been integrated to enhance the document retrieval \\cite{li:unigen:2024}. \n\nWhen retrieving visual content, recent work exploits generative models to enhance content-based model search \\cite{Lu:content:2023}. These models decode the text, image, or video query into samples of possible outputs, which are then used to learn statistics for better matching between the query and output candidates. DMs are also employed for visual retrieval tasks, where they learn joint data distributions between text queries and video candidates \\cite{Jin:DiffusionRet:2023}.\n\n\n\nWhile the retrieval task involves users directly defining a specific query input, recommendation services operate by retrieving content based on previous usage patterns. Essentially, a recommendation engine is a system that suggests products, services, or information to users through data analysis. Research in \\cite{CHUA:AI:2023} has reported a positive association between buyers' attitudes toward AI and their behavioral intention to accept AI-based recommendations, with potential for further growth. Notable examples include the recommendation framework developed by Google \\cite{Rajput:recommender:2023}, which utilizes GR. This framework assigns Semantic IDs to each item and trains a retrieval model to predict the Semantic ID of an item that a given user may engage with.\n\n\\subsubsection{Intelligent assistants}\n\nIntelligent assistants refer to software programs or applications that use AI and NLP to interact with users and provide helpful responses or perform tasks. These assistants can range from simple chatbots to sophisticated virtual agents capable of understanding and responding to complex queries. They're designed to assist users in various tasks, from answering questions and providing information to scheduling appointments and controlling smart home devices.\n\nCurrent LLMs obviously enhance the performance of intelligent assistants, designed to understand complex inquiries and generate more natural conversational responses, such as Sasha \\cite{King:Sasha:2024}. Generative AI can also be used to enhance the performance of human customer support agents, aiding in search and summarization, as discussed in the previous section. Brynjolfsson et al. \\cite{brynjolfsson:generative:2023} examined the implementation of a generative AI tool designed to offer conversational guidance to customer support agents. Their research revealed that AI assistance significantly enhances problem resolution and customer satisfaction. Furthermore, they observed that AI recommendations prompt low-skill workers to adopt communication styles akin to those of high-skill workers. AI-based intelligent assistants may currently be more focused on educational purposes, but they can clearly help artists write more efficiently \\cite{Lee:design:2024} or assist in customizing personal requirements \\cite{sajja2024ai}.\n \n\\subsection{Content enhancement and post production workflows}\n\n\\subsubsection{Enhancement}\n\nIn our previous review paper \\cite{Anantrasirichai:AI:2022}, we discussed AI technologies for contrast enhancement and colorization as separate topics, as methods were developed specifically for each task. However, in recent years, there has been a shift towards addressing more complex issues, such as those encountered in low-light environments and underwater scenarios. These real-world situations often involve a combination of challenges, including low contrast, color imbalance, and noise.\n\nIn low-light conditions, scenes often exhibit low contrast, leading to focusing difficulties or the need for long exposures, which can result in blurred images and videos. To address this, LEDNet \\cite{Zhou:LEDNet:2022} has introduced a synthetic dataset for such scenarios and incorporated a learnable non-linear activation function within the network to enhance feature intensities. Meanwhile, SNR-Aware \\cite{Xu:SNR:2022} estimates spatial-varying Signal-to-Noise Ratio (SNR) maps and proposes local and global learning branches using ResNet and transformer architectures, respectively. NeRCo \\cite{Yang:Implicit:2023} address low-light problem with INR, which unifies the diverse degradation factors of real-world scenes with a controllable fitting function.  Diffusion models (DMs) have also become popular choices for low-light image enhancement \\cite{HOU:Global:2023,Yi:Diff:2023,Jiang:Low:2023}. Diff-Retinex \\cite{Yi:Diff:2023} formulates the low-light image enhancement problem into Retinex decomposition, and employs multi-path generative diffusion networks to reconstruct the normal-light Retinex probability distribution. A recent state-of-the-art approach presented in \\cite{Jiang:Low:2023} decomposes images into high and low frequencies using wavelet transform. High frequencies are enhanced using a transformer-based pipeline, while the low frequencies undergo a diffusion process. This method achieves nearly 2.8dB improvement over the state-of-the-art transformer-based approach, e.g. LLFormer \\cite{Wang:Ultra:2023}, and significantly better than  INR-based method, NeRCo \\cite{Yang:Implicit:2023}, on a real low-light image benchmarking dataset. The technique has been extended for video enhancement in \\cite{lin2024lowlight}. The output of the enhancement typically depends on user preferences. This has been viewed as a one-to-many inverse problem, with attempts to solve it using Bayesian approaches. For example, a Bayesian Enhancement Model (BEM) \\cite{huang2025bayesian} incorporates Bayesian Neural Networks (BNNs) to capture data uncertainty and produce diverse outputs. The method can be used with Transformers or Mamba as the architecture backbone. CLE Diffusion \\cite{Yin:CLE:2023} offers lighting that is editable with user-friendly region controllability.\n\nRegarding video enhancement, transformer and DMs are still in their early stages. STA-SUNet \\cite{Lin:SPATIO:2024} has demonstrated that using transformers for low-light video enhancement outperforms CNN-based methods \\cite{anantrasirichai:BVI:2024}. It is important to note that low-light enhancement is subjective. While most training datasets use normal lighting conditions as ground truth \\cite{Lin:BVI-RLV:2024}, the enhanced images and videos may alter the mood and tone of the content. Therefore, the tools for creative industries should be adjustable, not only for entire images and videos but also adaptive to specific areas and content.\n\n\\subsubsection{Style transfer}\n\nStyle transfer in AI art refers to a technique where the artistic style of one image (or video) is applied to another image (or video) while preserving the content of the latter. Style transfer has numerous applications in art, design, and image editing, allowing artists and designers to create unique and visually appealing compositions by blending different artistic styles with existing images (or videos). The applications also include image-to-image and sequence-to-sequence translations.\n\nStyTr2 \\cite{Deng:StyTr2:2022} is the first transformer-based method for style transfer, applying content as a query and style as a key of attention. InST \\cite{Zhang:Inversion:2023} utilizes Stable Diffusion Models as the generative backbone and introduces an attention-based textual inversion module to learn the description of the content. StableVideo \\cite{Chai:StableVideo:2023} uses a text prompt to describe the desired appearance of the output, transforming the input video to have a new look based on a diffusion model. For instance, a video of a white car driving in summer can be altered to show a red car driving in winter. A large pre-trained DM is employed in \\cite{Chung_2024_CVPR}, where the style is injected to manipulate the self-attention of the decoder. To deal with the disharmonious color, they propose an adaptive instance normalization. Implicit Neural Representation (INR) is combined with vision transformers in \\cite{Moon:generalizable:2023} for generalizable style transfer; however, the results are not very impressive. The method proposed in \\cite{Kim:Controllable:2024} employs MLPs to map image coordinates to colors of the new style image, controlled by features from the content input and style input.\n\n\n\\subsubsection{Upscaling imagery: super-resolution (SR)}\n\nImpressive super-resolution (SR) results from transformer and diffusion models have been published extensively in the past few years. Originally, SR methods were developed using multiple low-resolution (LS) images, as different features in each image are combined to construct an enhanced one. However, these methods are not practical, as in most cases only one LS image is available. Hence, more methods have been developed for single image super-resolution (SISR).\n\nThe first use of a transformer, called ESRT, was for capturing long-term dependencies, such as repeating patterns in buildings. This was done in the feature domain extracted by a lightweight CNN module \\cite{Lu:Transformer:2022}, outperforming those that use only CNNs. Since then, most SISR methods have been based on transformers. The Hybrid Attention Transformer (HAT) \\cite{Chen:Activating:2023} was introduced, which improves the SR quality over ESRT by more than 2dB when upscaling 2$\\times$\\times-4$\\times$\\times. However, the NTIRE 2023 Real-Time Super-Resolution Challenge \\cite{Conde:Efficient:2023} showed that the winner, Bicubic++ \\cite{Bilecen:Bicubic:2023}, uses only convolutional layers and achieves the fastest speed at 1.17ms in upscaling 720p to 4K images.\n\nFor DMs, SR3 by Google \\cite{Saharia:image:2023} has produced truly impressive results. It operates by learning to transform a standard normal distribution into an empirical data distribution through a sequence of refinement steps, interpolating in a cascaded manner\u2014upscaling 4$\\times$\\times at a time. Later, IDM \\cite{Gao:Implicit:2023} combines INR with a U-Net denoising model in the reverse process of the DM. It is crucial to emphasize again that DMs are generative models. The SR results are generated based on the statistics we provide to the model during training (LR training samples). This is not for a restoration task, but rather for synthetic generation. A survey in SISR using DMs can be found in \\cite{moser:diffusion:2023}.\n\nFor video SR, numerous methods have emerged as part of a unified enhancement framework, as discussed in the previous section. One of the pioneering works to incorporate transformers specifically for video SR tasks is the Trajectory-aware Transformer for Video Super-Resolution (TTVSR) \\cite{Liu:Learning:2022}. Although the results are slightly inferior to those of BasicVSR++ \\cite{Chan:BasicVSR:2022}, which employs CNN and was introduced around the same time, both methods significantly enhance detail and sharpness compared to previous approaches, albeit not in real time. To address this limitation, the Deformable Attention Pyramid \\cite{Fuoli:Fast:2023} has been introduced, offering slightly lower quality but a speed-up of over 3$\\times$\\times. Recently, Adobe announced their VideoGigaGAN \\cite{xu:videogigagan:2024}, which can perform 8$\\times$\\times upsampling. This is achieved by adding flow estimation and temporal self-attention to the GigaGAN upsampler \\cite{kang:gigagan:2023}, which is primarily used for image SR, and text-to-image synthesis.\n\nCompared to traditional upscaling methods, generative AI can add details that did not exist in the original input image. These methods excel at generating high-quality natural images and structures, such as buildings, which are commonly included in training datasets. However, the process can be slow and may produce unpredictable results if the input image has very low resolution or contains content rarely seen in natural images. As shown in Fig. \\ref{fig:SR} (left), generative AI fails to upscale the knitting texture areas, instead generating lines more commonly found in typical images. While AI methods produce sharper edges, they perform less effectively on text.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/SR_results.png}\n    \\caption{(Left) Examples of SR ($\\times$4) using generative model. (Right) Real-time portrait editing with FacePoke.} \n    \\label{fig:SR}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/SR_results.png}\n    \\caption{(Left) Examples of SR ($\\times$4) using generative model. (Right) Real-time portrait editing with FacePoke.} \n    \\label{fig:SR}\n\n\n\\subsubsection{Restoration}\n\nIn our previous review paper \\cite{Anantrasirichai:AI:2022}, we categorized the work on restoration into several different types of distortions, including deblurring, denoising, dehazing, and mitigating atmospheric turbulence. Recent work however uses a unified network architecture to address these as inverse problems $y = hx + n$y = hx + n, where $x$x and $y$y are the ideal and observed data, respectively. $h$h is a degradation function, such as blur, and $n$n is additive noise. Often the super-resolution task is also considered as an inverse problem, meaning $h$h includes downsampling process. Note that although designed as a single network, the model is trained with each distorted dataset separately. \n\nThe pioneering transformer-based method for image restoration, SwinIR \\cite{Liang:SwinIR:2021}, employs several concatenated Swin Transformer blocks \\cite{Liu:Swin:2021}. SwinIR surpasses state-of-the-art CNN-based methods proposed up to the year 2021 in super-resolution and denoising tasks. The model is smaller and reconstructs fine details more effectively.\nOther two popular approaches that emerged in the same timeframe are Uformer \\cite{Wang:Uformer:2022} and Restormer \\cite{Zamir:Restormer:2022}. Both incorporate Transformer blocks into hierarchical encoder-decoder networks, employing skip connections similar to those in U-Net. Their objective was to restore noisy images, sharpen blurry images, and remove rain. The networks focused on predicting the residual $R$R and obtaining the restored image $\\hat{x}$\\hat{x} through $\\hat{x} = y + R$\\hat{x} = y + R. While their performance is very similar, Restormer has half the parameters of Uformer. More recent, GRL by Li et al. \\cite{li:GRL:2023} exploits a hierarchy of features in a global, regional, and local range using different ways to compute self-attentions as an image often show similarity within itself in different scales and areas. GRL outperforms SwinIR and Restormer. Additionally, Fei et al. introduced the Generative Diffusion Prior \\cite{Fei:Generative:2023} for unsupervised learning, aiming to model posterior distributions for image restoration and enhancement.\n\nFor video restoration, the general framework comprises frame alignment, feature fusion and reconstruction. The process could be similar to image restoration but input multiple frames and run through the sequences in sliding window manner to exploit temporal information of a number of consecutive frames. \nRecently, Video Restoration Transformer (VRT) \\cite{Liang:VRT:2024} and its improved version with recurrent process (RVRT) \\cite{liang:RVRT:2022}, have emerged as the state of the arts for video super-resolution, deblurring, denoising, and frame interpolation. This method introduces temporal reciprocal self-attention in the transformer architecture and parallel warping using MLP. These innovations enable parallel computation and outperform the previous state-of-the-art methods by up to 2.16dB on benchmark datasets. FMA-Net \\cite{Youk:FMA:2024} proposed multi-attention for joint Video super-resolution and deblurring, achieving fast runtime with nearly 40\\% improvement over RVRT, and the restored quality was reported better by up to 3\\%. \n\nFor audio restoration, most software discussed in Section \\ref{sssec:musicgen} offers tools for enhancing audio quality, such as eliminating background noise, echo, microphone rumble, and occasionally room reverberation, which have been well-established even before the advent of deep learning. There have been efforts to utilize AI for learning global contextual information to aid in the removal of unwanted sounds, leading to better final quality \\cite{Yu:DBT:2022}. The latest advancements in this domain are primarily focused on addressing issues where significant portions of the audio data are missing. For instance, Moliner et al. \\cite{Moliner:solving:2023} tackle problems such as audio bandwidth extension, inpainting, and declipping by treating them as inverse problems using a diffusion model.\n\nThe following methods have been proposed for specific problems, but ideally, they should be adaptable for other tasks, even though they may not perform as well as they do for the original task.\n \ni) \\textbf{Deblurring}: A lightweight deep CNN model was recently proposed in \\cite{Pan:Deep:2023}, where a new discriminative temporal feature fusion has been introduced to select the most useful spatial and temporal features from adjacent frames. Feature propagation along the video is done in the wavelet domain. The deblurring performance is comparable to RVRT \\cite{liang:RVRT:2022}, but it is 5 times faster. DaBiT \\cite{Morris:DaBiT:2024} mitigates focal blur content with depth information and applies SR for further enhancing fine details.\nNote that not only in software, but AI technologies have also been integrated into hardware. This includes autofocus, which is crucial for capturing sharp images of subjects, especially in dynamic environments where manual adjustments are impractical due to rapid movement. AI-driven autofocus methods have emerged, often tailored for specific camera hardware. For instance, Choi et al. proposed an autofocus model optimized for dual-pixel Canon cameras \\cite{Choi:Exploring:2023}. Additionally, Yang et al. investigated the correlation between language input and blur map estimation, utilizing semantic cues to enhance autofocus performance \\cite{yang:ldp:2023}. Remarkably, their model achieves comparable results to previous state-of-the-art methods while being more lightweight \\cite{Yang:K3DN:2023}. Autofocus could be used in conjunction with real-time object tracking (see Section \\ref{sssec:tracking}) to produce desirable sharpness for moving objects in the video.\n\nii) \\textbf{Denoising}: SUNet \\cite{Fan:SUNet:2022} applies Swin transformer blocks combined in a UNet-like architecture. Denoising with diffusion models (DMs) \\cite{yang:realworld:2023} has been proposed by diffusing with estimated noise that is closer to real-world noise rather than Gaussian noise, achieving better performance than SwinIR \\cite{Liang:SwinIR:2021} and Uformer \\cite{Wang:Uformer:2022}. INR with complex Gabor wavelets as activation functions show promising denoising results \\cite{Saragadam:wire:2023}.\n\niii) \\textbf{Dehazing}: Vision transformers for single image dehazing was proposed in DehazeFormer \\cite{Song:vision:2023}. Similar to SUNet, it is UNet-like architecture, but introduced Rescale Layer Normalization for better suit on improving contrast. For video dehazing, Xu et al. \\cite{Xu:Video:2023} introduced a recurrent multi-range scene radiance recovery module with the space-time deformable attention. They also employs physics prior to inform haze attenuation. This method outperforms DehazeFormer by approximately 1dB.\n\niv) \\textbf{Mitigating atmospheric turbulence}: Similar to dehazing, physics-inspired models have been widely developed to remove turbulence distortion \\cite{Jaiswal:Physics:2023,Jiang:NeRT:2023}, while complex-valued CNNs have been proposed to exploit phase information \\cite{Atmospheric:2023}. There was also an attempt to use instance normalization (INR) to solve this problem, offering tile and blur correction \\cite{Jiang:NeRT:2023}. However, diffusion models outperform on a single image \\cite{Nair:AT-DDPM:2023}, and transformer-based methods remain state-of-the-art for restoring videos \\cite{Zhang:Image:2024, zou2024deturb}. A recent review can be found in \\cite{Hill2025}.\n\n\\subsubsection{Inpainting}\n\\label{ssec:inpaiting}\n\nVisual inpainting is the process of filling in lost or damaged parts of an image or video. CNNs and GANs have already achieved impressive results (see our previous review paper \\cite{Anantrasirichai:AI:2022}). Recent work has focused more on editing rather than simply filling in the missing areas. This means users can now mask large areas of an image, and AI tools generate multiple results for users to choose from, a technique known as pluralistic inpainting \\cite{zheng:pluralistic:2019}. Some notable methods include the following: Mask-Aware Transformer (MAT) \\cite{Li:MAT:2022} offers several outputs to fill a large missing area, consisting of a convolutional head, a transformer body, and a convolutional tail for reconstruction, along with a Conv-U-Net for refinement. PUT \\cite{Liu:Reduce:2022} proposes a patch-based vector VQ-VAE and unquantized Transformer to minimize information loss. Spa-former \\cite{HUANG:Sparse:2024} employs a UNet-like architecture, where each level performs transformer with sparse self-attention to remove coefficients with low or no correlation, leading to memory reduction, while improving result quality by up to 5\\% compared to PUT.\n\nVideo inpainting presents greater complexity compared to image inpainting, despite the abundance of information available in an image sequence. The process typically involves tracking masks across frames, estimating optical flow, and ensuring temporal consistency.  The current state-of-the-art methods include DLFormer \\cite{Ren:DLFormer:2022} and  ProPainter \\cite{Zhou:ProPainter:2023}. DLFormer conducts inpainting in latent space and utilizes discrete codes for video representation. On the other hand, ProPainter employs flow-based deformable alignment to enhance robustness to occlusion and inaccurate flow completion. The method excels in filling complete and rich textures, achieving a speed of 12 fps for full HD video. Video inpainting is also used for dubbing. DINet \\cite{Zhang:DINet:2023} replaces the mouth area to synchronize with a new language being spoken.\n\nA survey of learning-based image and video inpainting,  including CNN, VAE, GAN, transformers and diffusion models can be found in \\cite{quan:deep:2024}.\n\n\\subsubsection{Image Fusion}\n\\label{sssec:fusion}\n\nImage fusion involves merging multiple images from either the same source (such as varying focal points or exposures) or different modalities (like visible and infrared cameras) into a single image. This process integrates complementary information from the various images to enhance overall quality, improve interpretation, and increase the usability of the final image.\n\nTransformers and CNNs have been combined to extract global and local information, respectively. Most methods use CNNs for feature extraction, with transformers operating in the latent space \\cite{Ma:SwinFusion:2022, Rao:TGFuse:2023}. Notable methods include SwinFusion \\cite{Ma:SwinFusion:2022}, which utilizes a self-attention-based intra-domain fusion unit and a cross-attention-based inter-domain fusion unit to achieve multi-modal and digital photography image fusion. Transformer-based image fusion has also been applied to downstream tasks like segmentation \\cite{Liu:Multi:2023}, achieving superior results by leveraging the additional information. \n\nDDFM, the first diffusion model-based image fusion method, has been introduced, using loss functions to estimate noise in the reverse process by combining multiple inputs \\cite{Zhao:DDFM:2023}. The expectation-maximization (EM) algorithm is integrated to estimate the noise distribution parameters, resulting in sharper images compared to traditional DDPM. For an in-depth review, refer to recent works in \\cite{Karim:Current:2023, Zhang:Visible:2023}.\n\n\\subsubsection{Editing and Visual Special Effects (VFX)}\n\nEditing or modifying specific areas of an image is much easier with DM technologies, particularly for headshot photos, such as targeting the eyes and mouth on the face \\cite{guo2024liveportrait}. This capability has been extended to video generation (see Section \\ref{sssec:videogen}). Fig. \\ref{fig:SR} shows an example of the online tool, FacePoke\\footnote{\\url{https://huggingface.co/spaces/jbilcke-hf/FacePoke}}, which allows users to move the head and modify the shapes of the eyes and mouth in real time. Motion-I2V \\cite{Shi:Motion-I2V:2024} provides motion blur and motion drag tools to control specific areas of an image to add motion. The method is based on a diffusion-based motion field predictor and motion-augmented temporal attention.\n\nVFX aims to create and/or manipulate imagery outside the context of a live-action shot in filmmaking and video production. When adding objects, scenes, and effects into traditional photographic videos, generative AI has obviously become a great tool, but some manual operations are still required. For example, in After Effects (EA)\\footnote{\\url{https://www.adobe.com/uk/products/aftereffects.html}}, the user selects the area where the object will be added and uses text prompts to describe such object. Subsequently, with the current EA version, the user will need to apply motion tracking so the generated object is moved accordingly.\n\nAI technologies can upscale, enhance, and restore low-quality or old footage. For example, standard definition videos can be converted to high definition or even 4K quality without traditional manual remastering processes. This is particularly useful for remastering old movies or enhancing visual details in scenes. Generative AI has also simplified and accelerated automated processes, such as rotoscoping \\cite{Tous:Lester:2024}, an animation technique where animators trace over motion picture footage frame by frame to create realistic action. AI models can accurately detect and segment objects and characters in video frames, significantly speeding up the post-production process. Additionally, AI assists in the rapid creation of 3D models from 2D images and generates realistic animations from minimal input data, facilitating complex human motions and synchronized facial expressions to voiceovers. One restriction is that current technologies still cannot generate full 4K accurate visual effects.\n\n\n\n\n\n\\subsection{Information Extraction and Understanding}\n\\label{ssec:infoextract}\n\nAI plays a crucial role in automating and optimizing the process of information extraction and understanding, enabling organizations to derive actionable insights from large and diverse data. Yan et al. \\cite{Yan:Universal:2023} have categorized information extraction tasks based on the Format-Time-Reference space, as illustrated in Fig. \\ref{fig:segmentation} (a), where object detection and video object segmentation (VOS) are considered to be the simplest and the most complex tasks, respectively. Recent advancements in this field draw significant inspiration from LLMs. These advancements include the utilization of prompts as conditional inputs for acquiring information. Moreover, following the pipeline approach used in LLMs, there is a growing trend towards leveraging very large datasets to pre-train models before fine-tuning them for specific downstream tasks. For instance, Meta AI \\cite{Oquab:DINOv2:2024} has introduced DINOv2, aimed at enriching information about visual content through self-supervised learning. This model was trained with 142 million carefully selected images, employing the ViT architecture. Google introduced VideoPrism \\cite{zhao:videoprism:2024}, a  solution for scene understanding tasks, including classification, localization, retrieval, captioning, and question answering (QA). The model was trained on an extensive and diverse dataset consisting of 36 million high-quality video-text pairs and 582 million video clips accompanied by noisy or machine-generated parallel text.\n\n\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation.png}\n    \\caption{(a) Tasks in Object-centric understanding defined by Yan et al. \\cite{Yan:Universal:2023} (REC:,  Referring Expression Comprehension, RES: Referring Expression Segmentation, VOS:  Video Object Segmentation, RVOS: Referring Video Object Segmentation, MOT: Multiple Object Tracking, MOTS: Multi-Object Tracking and Segmentation, VIS: Video Instance Segmentation, SOT: Single Object Tracking. (b) Current high-quality segmentation \\cite{Ke:SAM-HQ:2023}.}\n    \\label{fig:segmentation}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/segmentation.png}\n    \\caption{(a) Tasks in Object-centric understanding defined by Yan et al. \\cite{Yan:Universal:2023} (REC:,  Referring Expression Comprehension, RES: Referring Expression Segmentation, VOS:  Video Object Segmentation, RVOS: Referring Video Object Segmentation, MOT: Multiple Object Tracking, MOTS: Multi-Object Tracking and Segmentation, VIS: Video Instance Segmentation, SOT: Single Object Tracking. (b) Current high-quality segmentation \\cite{Ke:SAM-HQ:2023}.}\n    \\label{fig:segmentation}\n\n\n\\subsubsection{Segmentation}\n\\label{sssec:seg}\n\nSegmentation has grown dramatically in the past few years, given its central role in visual perception. Many segmentation methods now integrate an input prompt for users to define their preferred output appearances, such as pixel-wise segmentation, bounding boxes around objects, or segmented areas of interest. Most of these methods utilize transformer architectures \\cite{Bowen:Marked:2022}. Among them, Segment Anything (SAM) by Meta AI \\cite{Kirillov:SAM:2023} stands out as a pioneer in promptable segmentation approaches. This method computes masks in real-time and has been trained with over 1 billion masks across 11 million images, facilitating transferability from zero-shot to new image distributions and tasks. HQ-SAM \\cite{Ke:SAM-HQ:2023} enhances SAM by incorporating global-local feature fusion, leading to high-quality mask predictions. SegGPT \\cite{Wang:SegGPT:2023} proposed context ensemble strategies and allows users to tune a prompt for a specific dataset, scene, or even a person, while SEEM \\cite{Zou:Segment:2023} provides a completely promptable and interactive segmentation interface. More recently, SAM2 \\cite{ravi2024sam2} introduced support for real-time video segmentation. It is a unified model trained on a larger dataset than SAM. Interactive tools enable users to mark areas of interest and specify regions to exclude from the segmentation map.\n\nWith DMs tehchnologies, Baranchuk et al. \\cite{Baranchuk:label:2022} have investigated semantic representation, and found DMs outperform other few-shot learning approaches. DiffuMask \\cite{Wu:DiffuMask:2023} automatically generate image and pixel-level semantic annotation using pre-trained Stable Diffusion with input as a text prompt. It has been proven that using these synthetic data improve segmentation accuracy. Currently, the state-of-the-art panoptic segmentation is the method developed by Nvdia, which is based on text-to-image DMs \\cite{Xu:Open:2023}, outperforming the previous methods by up to 7.6\\%.\n\nApplying INRs to segmentation is more popular in the medical domain, as the specific signals used, such as computed tomography (CT) and magnetic resonance imaging (MRI), can be formulated as continuous functions. In creative technologies, unsupervised domain adaptation (UDA) and INR are used for continuous rectification function modeling in \\cite{Gong:Continuous:2023}, achieving superior segmentation results in night vision. Recently, the work integrated with a non-local means block in \\cite{Lin:Feature:2024} has shown a significant improvement for instant segmentation in low-light scenes.\n\n3D segmentation is also crucial for scene manipulation. In radiance fields, earlier segmentation methods required additional modules to separate objects from the background, such as using k-means clustering as demonstrated in \\cite{Goel:Interactive:2023}. However, the recent SA3D approach \\cite{Cen:Segment:2023} segments 3D objects using NeRFs as the structural prior. SA3D operates by taking a trained NeRF and a set of prompts from a single view, then performing an iterative procedure. This involves rendering novel 2D views, self-prompting SAM for 2D segmentation, and projecting the segmentation back onto 3D mask grids.\n\n\\subsubsection{Detection and recognition}\n\\label{sssec:recog}\n\nBack in 2020, DETR by Facebook AI \\cite{Carion:DERT:2020} is one of the first to adopt the transformer architecture for object detection. The approach achieves comparable results to an optimized Faster R-CNN \\cite{Ren:Faster:2027}, introduced in 2015. Deformable convolution has been used, Deformable DETR \\cite {Zhu:Deformable:2021}, resulting in training faster and approximately 5\\% accuracy improvement.\nA survey until 2022 reported in \\cite{Zou:object:2023} show that Deformable DETR and Swin Transformer \\cite{Liu:Swin:2021} outperform pure CNN-based YOLOv4 \\cite{bochkovskiy2020yolov4}. \nSwinV2 improves the first version by replacing original dot product attention with scaled cosine attention, and the accuracy is improved approximately 5\\%. Later, RT-DETR \\cite{lv2:detrs:2024} improves inference speed by decoupling the intra-scale interaction and cross-scale fusion of features with different scales. RT-DETR is 25\\% faster than YOLOv8\\footnote{\\url{https://github.com/ultralytics/ultralytics}} with 6\\% improvement on MS COCO Object Detection dataset. Recently, YOLOv10 \\cite{wang:yolov10:2024} has been released. YOLOv10 further improve the speed of detection approximately 30\\% over RT-DETR with the same accuracy. A review of transformer-based methods for object detection can be found in \\cite{Li:Transformer:2023}.\nAs estimating depth from a 2D image has been well established \\cite{Yang:depthanything:2024}, MonoDTR \\cite{Huang:MonoDTR:2022} integrates such module to estimate 3D box of the detected object from one image. 3D object detection is also employed transformers.\n\nWhile DMs are primarily used to generate synthetic datasets \\cite{Wu:datasetDM:2023, Fang:Data:2024}, they have also been demonstrated to function as zero-shot classifiers by Li et al. \\cite{Li:Your:2023}. DMs are of interest for detection tasks as well, though feature extractors are still predominantly based on CNNs, such as ResNet, or Transformers, like Swin. DiffusionDet \\cite{Chen:DiffusionDet:2023} formulates object detection as a denoising diffusion process from noisy boxes to object boxes, reporting performance that surpasses DETR.\n\n\n\\subsubsection{Tracking}\n\\label{sssec:tracking}\n\nObject tracking stands out as one of the tasks that greatly benefits from transformers as \\textit{attention} is needed in both space and time. An experimental survey cited in \\cite{Kugarajeevan:Transformers:2023} reveals that transformer-based methods consistently rank at the top of the leaderboard across various datasets. In the Visual Object Tracking (VOT) challenges of 2023\\footnote{https://eu.aihub.ml/competitions/201\\#results}, all top-10 leaders employed transformer-based methodologies. The highest-performing approach achieved a 10\\% improvement in tracking quality compared to the winner in 2020. The current state-of-the-art for single-object tracking\\footnote{https://paperswithcode.com/sota/visual-object-tracking-on-lasot}, however, is based on cross-attention and Mamba \\cite{kang2025exploring}.\n\nThe first three tracking-by-attention approaches are TrackFormer \\cite{Meinhardt:TrackFormer:2022}, MixFormer \\cite{cui:mixformer:2022}, and ToMP \\cite{Mayer:Transforming:2022}. TrackFormer extracts visual features using a CNN-based encoder, which are then tracked using a vanilla transformer \\cite{Vaswani:attention:2017} in a frame sequence, while MixFormer introduces cross-attention between the target and search regions. ToMP tracks the objects using prediction aspects. Many more methods have been proposed, including popular ones like SeqTrack \\cite{Chen:SeqTrack:2023} and Track Anything Model (TAM) \\cite{yang:track:2023}. SeqTrack extracts visual features with a bidirectional transformer, while the decoder generates a sequence of bounding box values autoregressively with a causal transformer. TAM combines SAM \\cite{Kirillov:SAM:2023} and XMem \\cite{cheng:xmem:2022}, offering tracking and segmentation performance on the human-selected target. However, the masked area is still not very sharp, and there is a subtle degree of temporal inconsistency. MOTRv2 \\cite{Zhang:MOTRv2:2023} combines YOLOX \\cite{ge:yolox:2021} for object recognition and MOTR \\cite{zeng:motr:2022} for tracking, outperforming TrackFormer by 20\\%. Additionally, some methods have been specifically proposed for challenging environments, such as low light \\cite{Yi:Comprehensive:2024} and small objects, as seen in AnyFlow \\cite{Jung:AnyFlow:2023}. The later case exploits INR to upsample a continuous coordinate-based flow map, similar to SISR technique proposed in \\cite{Chen:Learning:2021}.\n\nSimilar to detection tasks, DMs for tracking tasks are used as downstream tasks by concatenating the diffusion head to the feature extraction backbone. However, a spatial-temporal fusion module has been added to the diffusion head to exploit temporal features of the videos \\cite{Luo:DiffusionTrack:2024}. DiffusionTrack \\cite{Xie:DiffusionTrack:2024} localizes the target in a progressive diffusion manner, which is claimed to better handle challenging scenarios. The method in \\cite{Zhang:DiffusionTracker:2024} exploits spatial-temporal weighting to suppress the probability of the tracker changing the target to the distractors. It, however, reports slightly underperforming MixFormer.\n\n\n\n\\subsection{3D Reconstruction and Rendering}\n\\label{ssec:3Dreconstruct}\n\nBridging the gap between digital and physical realms, 3D reconstruction and rendering are integral to various creative technologies.  In film and animation, they enable the creation of detailed digital models that blend seamlessly with live-action footage. Video games leverage these technologies for dynamic environmental rendering. VR and AR use 3D reconstruction to create immersive and interactive experiences, with AR integrating digital content into real-world contexts. With recent AI technologies, 3D reconstruction and rendering have become faster and closer to reality. In particular, neural radiance fields and Gaussian Splatting enable artists and film producers to create shots that cannot be one in the real shooting environments.\n\n\\subsubsection{Depth Estimation}\n\\label{sssec:depth}\n\nAccurate depth information is typically required to construct 3D models. Depth sensors, such as lidar (Light Detection and Ranging) and structured-light 3D scanners, can be used for this purpose, but their applications are often limited by distance and cost. Consequently, vision-based sensors have become widely used. These sensors utilize two or more cameras to simulate human binocular vision or employ a single camera to capture images from different locations.\n\nAs deep learning can capture monocular cues such as object size, texture gradients, and perspective, depth estimation from a single image can produce accurate results. There have been attempts to use transformers, such as \\cite{Zhang:Lite:2023} and \\cite{Chen:Vision:2023}, and diffusion models, such as \\cite{Ji:DDP:2023} and \\cite{Ke:Repurposing:2024}. Amongst these, Depth Anything v2 \\cite{Yang:depthanythingv2:2024} has become a state-of-the-art monocular depth estimation method. It is built on the previous version \\cite{Yang:depthanything:2024}, jointly trained on large-scale labeled and unlabeled images and uses semantic priors from pretrained encoders. Depth Anything v2 significantly outperforms V1 in fine-grained details and robustness by using synthetic images and pseudo-labeled real images, as well as by extracting intermediate features from DINOv2 \\cite{Oquab:DINOv2:2024}, which is trained with vision transformers. One of the notable capabilities of Depth Anything v2 is its ability to predict depth of transparent and reflective surfaces.\n\n\\subsubsection{Neural Radiance Fields}\n\\label{sssec:nerf}\n\nNeural Radiance Fields (NeRFs), introduced in \\cite{Mildenhall:NeRF:2020}, have demonstrated the ability to learn a 3D scene from a smaller number of images captured from various viewpoints, as opposed to photogrammetry. They excel in neural rendering, particularly in view-dependent novel view synthesis, and have effectively tackled several challenges associated with automated 3D capture \\cite{xie2022neural}, such as accurately representing the reflectance properties of the scene. NeRFs offer high-resolution photo-realistic novel views and flexibility on postprocessing. They have hence gained lots of attention in cinematography \\cite{Azzarelli2024}, as it potentially safe time and cost particularly for outdoor shooting.\n\nIn the NeRF process (see Fig. \\ref{fig:3Drepresentation} (a)), the camera positions and orientations are typically estimated from a series of 2D images using techniques like feature-mapping and Structure-from-Motion (SfM), as demonstrated in \\cite{schoenberger:sfm:2016}. Leveraging INR, each image (or camera pose) is mapped into camera rays that traverse the scene, generating 3D points with directional radiance (towards the camera). These points are then processed by an MLP to predict volume density and emitted radiance. Subsequently, the volume rendering techniques are employed to generate an image, which is compared with the original for loss calculation. The MLP iteratively refines the model by minimizing this loss.\n\nSince then, there have been many variants of NeRFs aimed at improving their performance. Mip-NeRF360 \\cite{Barron:Mip-NeRF360:2022} proposed unbounded anti-aliased technique achieving full 360 degree content. Google Research\n\\cite{Mildenhall:NeRFDark:2022} trains NeRF from noisy RAW images captured in the dark scene, allowing changing viewpoint, focus, exposure, and tone mapping simultaneously. With segmentation techniques significantly advanced (see Section \\ref{sssec:seg}), there have been integrations utilizing semantic segmentation to enhance 3D representation \\cite{Guo:neural:2022}.\n\nWhile the rendering quality of NeRF is outstanding, training and rendering times remain extremely high. The Instant-NGP tool developed by Nvidia \\cite{mueller:instant:2022} enables real-time training of NeRFs by bypassing sampling in empty spaces and dense areas, and by incorporating multi-resolution hash encoding techniques. These advancements substantially reduce the computational burden associated with representing high-resolution image features -- training times have been reduced from hours to just a few seconds. Moreover, it offers VR controls for immersive 3D rendering experiences using OpenXR\\footnote{An open-source, royalty-free standard for access to virtual reality and augmented reality platforms and devices. \\url{https://www.khronos.org/openxr/}}. This allows users to navigate scenes, manipulate objects, and interact with the environment directly through VR headsets. Diffusion models are integrated to regularize NeRF reconstructions \\cite{wynn:diffusionerf:2023}, resulting in smoother depth continuity and clearer edges where depth discontinuities occur.\n\nThe initial attempt to apply NeRF to dynamic scenes was undertaken by Pumarola et al. \\cite{pumarola:DNeRF:2020}, known as D-NeRF.\nHowever, the current leading method for generating high-quality novel views of real dynamic scenes is TiNeuVox \\cite{Fang:Fast:2022}.  It enhances temporal information by interpolating voxel features before feeding them into the radiance network to estimate density and color, similar to ordinary NeRF. The main drawback of these methods is the large model size and/or long training time. Therefore, $K$K-planes \\cite{Fridovich:kplanes:2023} propose a simple planar factorization for volumetric rendering, achieving low memory usage (1000$\\times$\\times compression over a full 4D grid). Wavelet transform is employed in \\cite{azzarelli:waveplanes:2023} to further reduce model size.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/3Drepresentation.png}\n    \\caption{3D representation. (a) Neural Radiance Fields (NeRFs) \\cite{Mildenhall:NeRF:2020}. (b) Gaussian Splatting \\cite{kerbl:3Dgaussians:2023}. (c) Example scenes of VR-GS system for 3D content interaction in VR \\cite{jiang:vrgs:2024}. }\n    \\label{fig:3Drepresentation}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\textwidth]{figures/3Drepresentation.png}\n    \\caption{3D representation. (a) Neural Radiance Fields (NeRFs) \\cite{Mildenhall:NeRF:2020}. (b) Gaussian Splatting \\cite{kerbl:3Dgaussians:2023}. (c) Example scenes of VR-GS system for 3D content interaction in VR \\cite{jiang:vrgs:2024}. }\n    \\label{fig:3Drepresentation}\n\n\n\\subsubsection{Gaussian Splatting}\n\\label{sssec:3DGS}\n\nThe main issue with modeling NeRFs to generate high-quality novel views is the training time, which can exceed a day. 3D Gaussian Splatting (3D-GS) \\cite{kerbl:3Dgaussians:2023} has been introduced using anisotropic 3D Gaussians for a high-quality, unstructured representation of radiance fields. The process estimates a sparse point cloud through SfM. Each point possesses 3D Gaussian properties, such as position, covariance matrix, opacity, and spherical harmonics coefficients representing colors. The optimization of these parameters is interleaved with steps that control the density of the Gaussians to better represent the scene, as shown in Fig. \\ref{fig:3Drepresentation} (b). A survey of 3D-GS can be found in \\cite{10521791}.\n\nIn contrast to traditional NeRFs based on implicit scene representations, 3D-GS provides an explicit representation that can be seamlessly integrated with post-processing manipulations, such as animating and editing. VR-GS \\cite{jiang:vrgs:2024} offers intuitive and interactive physics-based game-play with deformable virtual objects and realistic environments represented with 3D-GS. The example scenes are shown in Fig. \\ref{fig:3Drepresentation} (c). Physics-inspired approaches are also integrated to improve 3D modeling in different media, such as 3D underwater scenes \\cite{Wang2025}.\n\nFor dynamic scenes, 4D Gaussian Splatting (4D-GS) \\cite{wu:4dgaussians:2024} proposes a Gaussian deformation field for motion and shape. It exploits a multi-resolution encoding method, achieving real-time rendering of up to 82 fps at a resolution of 800$\\times$\\times800 pixels. Instead of developing in 4D, CoGS \\cite{Yu:CoGS:2024} exploits 3D-GS by integrating control mechanisms in separate regions to learn individual temporal dimensions. SC-GS \\cite{Huang:SCGS:2024} extracts sparse control points and uses an MLP to predict time-varying 6 DoF transformations. While the results show better visual quality than 4D-GS and CoGS, the performance heavily relies on camera pose estimation. \n\nLUMA AI\\footnote{\\url{https://lumalabs.ai/interactive-scenes}} and \nPolycam\\footnote{\\url{https://poly.cam/captures}} offer free tools for Gaussian splat and photogrammetry creators. The 3D objects created can be experienced with VR headsets for more immersive 3D and further used or developed in other applications. However, these tools have limitations in handling dynamic scenes, which are challenging for both NeRF and GS to achieve good results.\n\n\n\\subsection{Data Compression}\n\\label{ssec:compression}\n\nData compression plays an important role in various applications of creative technologies, which effectively reduces memory space and bandwidth requirements during signal storage and transmission \\cite{Bull:intelligent:2021}. Although standard coding methods based on conventional signal processing theories are still widely employed in most application scenarios, learning-based solutions have emerged in research, showing great potential to achieve competitive performance in recent years. This subsection provides a brief overview on the recent advances in image, video and audio compression, in particular focusing on the approaches proposed after 2021.\n\n\\subsubsection{Image Compression}\n\nSince the first neural image codec \\cite{balle2016density} was proposed in 2016, numerous learning-based image compression methods have been developed, with significant performance improvements reported \\cite{balle2018variational,cheng2020learned}. In recent years, inspired by the latest advances in neural network architectures, many neural image codecs have been proposed, which outperform the latest standard image codecs. Instead of using CNNs as the basic network structures, transformer-based architectures have become popular, with potential to achieve better compression efficiency.  Notable examples include SwinT-ChARM \\cite{zhu2022transformer}, STF \\cite{zou2022devil} and LIC-TCM \\cite{liu2023learned}. SwinT-ChARM \\cite{zhu2022transformer} employs Swin transformers for non-linear transforms and outperforms the latest standard image codec, the Versatile Video Coding (VVC) Test Model (VTM, All Intra). STF \\cite{zou2022devil} is based on a symmetrical transformer framework containing absolute transformer blocks in both the down-sampling encoder and the up-sampling decoder, which also shows improved rate quality performance over VTM. LIC-TCM \\cite{liu2023learned} exploits the local modeling ability of CNN and the non-local modeling performance of transformers, and proposes a parallel transformer-CNN mixture block. This new network structure, together with a channel-wise entropy model based on attention modules using Swin transformers, contributes to the superior performance of STF, with a more than 10\\% bitrate saving over VTM. \n\nThere is another group of learned image coding methods, which are based on advanced generative models. Early works \\cite{agustsson2019generative,mentzer2020high} employ GANs to generate more photo realistic results with improved visual quality. Although these models fail to outperform conventional, CNN-based or transformer-based approaches, when distortion-based quality metrics, e.g., PSNR, are used for performance evaluation, they have been reported to perform well when perceptual quality models, such as MS-SSIM \\cite{Bovik_MSSSIM} and VMAF \\cite{VMAFblog}, or subjective tests are employed to measure perceived video quality. More recently, diffusion models have been applied in image compression to allow realistic reconstruction at ultra-low bitrates \\cite{careil2023towards} and achieve more competitive performance over GAN-based models \\cite{yang2024lossy}. However, it should be noted that some of these generative models aim to generate (or synthesize) images with ``perfect realism'' rather than reconstruct results which are similar to the original content. Notable works in this category also include the image codec using score-based generative models \\cite{hoogeboom2023high} and the diffusion-based residual augmentation codec (DIRAC) \\cite{ghouse2023residual}. Moreover, as another type of generative models, INR has been employed for image compression, which learns a mapping between the spatial coordinates and the respective pixel values for the input image. The learned INR model is then compressed through parameter quantization and model compression to minimize the required bitrate. Notable INR-based image codecs including COIN/COIN++ \\cite{dupont2021coin,dupontcoin++} and \\cite{strumpler2022implicit} that combines SIREN networks \\cite{sitzmann2020implicit} with positional encoding,   \n\nIn order to evaluate and compare neural image codecs under fair test conditions, public grand challenges have been held by different organizers, which are typically associated with international conferences. One of the most well-known ones is the Challenge on Learned Image Compression (CLIC) \\cite{clic}, and in its latest competition, the best performing learned image codec offers up to 0.6dB PSNR gain over VTM (version 22.2, All Intra) at similar bitrates, which is based on an autoencoder architecture with latent refinement and perceptual losses. \n\nTo support the deployment of neural image codecs, the International Organization for Standardization (ISO)/International Electrotechnical Commission(IEC) have jointly started to develop a royalty-free learned image coding standard, denoted as JPEG AI \\cite{ascenso2023jpeg}, which aims to offer significant performance improvement over existing standards for both human and machine vision tasks. The Call for Proposals of JPEG AI was published in 2022, while the Working Draft and the Committee Draft outlining its core coding system were released in 2023 \\cite{JPEGAIN100634}, with its first version published in October 2024 \\cite{JPEGAIN100634}. JPEG AI follows the same framework (the auto-encoder structure) as most existing neural image codecs, and its test model JPEG AI VM (version 4.3) has been reported to achieve up to 28.5\\% coding gains over VVC VTM (All Intra mode) \\cite{JPEGAIM101081}.\n\n\\subsubsection{Video Compression}\n\nCompared to images, the compression of video content is a much more challenging task, in particular with various immersive video formats and diverse content. Although video coding standards including H.264/AVC (Advanced Video Coding), H.265/HEVC (High Efficiency Video Coding) and H.266/VVC (Versatile Video Coding) are still predominant in real-world applications, learning-based video coding has been extensively investigated in the past five years, with deep learning enhanced conventional coding tools and end-to-end optimized neural video coding frameworks proposed.    \n\nThe enhancement of conventional coding tools focuses on employing deep learning techniques to improve the performance of a (or multiple) coding modules in a standard-applicant codec, including intra prediction \\cite{li2021deepqtmt}, inter prediction \\cite{jin2021deep}, in-loop filtering \\cite{feng2024low}, post filtering \\cite{zhang2023wcdann} and resolution re-sampling \\cite{wang2023compression}. To facilitate efficient integration, the MPEG Joint Video Experts Team (JVET)  has built a test model in 2022 based on VTM 11, named Neural Network-based Video Coding (NNVC) \\cite{li2023designs}, with its latest version NNVC-7.1 containing two major learning-based coding tools, neural-network based intra prediction and in-loop filtering, which has achieved an up to 13\\% coding gain over VTM 11 (Random Access mode) \\cite{JVET-AG0014}. However, this learning-based codec requires much higher computational complexity (up to 477 kMACs/pixel) and high-spec GPU support compared to conventional codecs. Meanwhile, members of the Alliance of Open Media (AOM) have also developed multiple CNN-based coding tools for the next generation of video coding standard beyond AV1. The latest proposals focus on the trade-off between performance and complexity, with one of them based on inloop filtering and super-resolution, which achieves an average BD-rate saving of 3.9\\% (in PSNR) over AVM, the test model of AV2, but only requires a much lower computational complexity (below 1.5kMACs/pixel) \\cite{joshi2023switchable}. More recently, research has been conducted to further improve the performance of these learning-based coding tools utilizing more advanced network architectures, including ViTs \\cite{kathariya2023joint}, and diffusion models \\cite{li2024extreme}. There are also investigations on applying preprocessing before compression \\cite{chadha2021deep,tan2024joint}, where the training of the deep preprocessors is based on proxy video codecs and/or rate-distortion loss functions to simulate the behavior of conventional video coding algorithms.\n\nAlongside enhancing coding tools in conventional video codecs, more research activities investigate using neural networks to implement the whole coding workflow, which enables data-driven end-to-end optimization. The performance of these neural video codecs has improved significantly in the last five years, since the first attempt, DVC \\cite{lu2019dvc}, was published, which only matches the performance of a fast implementation of H.264 (x264). When this paper was written, some latest well-performing learned video coding algorithms (e.g., DCVC-FM \\cite{li2024neural} and DCVC-LCG \\cite{Qi2024longterm}) have already been able to compete or even outperform the state-of-the-art standard codecs, such as VVC VTM under certain coding configurations. These learning-based methods often focus on enhancement from different perspectives, including feature space conditional coding (e.g., FVC~\\cite{hu2021fvc} and DCVC \\cite{li2021deep}), instance adaptation ~\\cite{khani2021efficient,yang2024parameter}, and motion estimation (e.g., DCVC-DC~\\cite{li2023neural}). New architectures have also been proposed such as CANF-VC~\\cite{ho2022canf} based on a video generative model, MTMT \\cite{xiang2022mimt} using a masked image modeling transformer based entropy model and VCT \\cite{mentzer2022vct} based on a video compression transformer. It is noted that although promising coding performance has been achieved in the aforementioned works, these neural video codecs (in particular those based on autoencoder backbones) are typically associated with high computational complexity (especially in the decoder), which constrains their deployment for practical applications. To address this issue, researchers attempted to achieve complexity reduction while maintaining the coding performance through model pruning and knowledge distillation~\\cite{guo2023evc,peng2024accelerating}. \n\nIt should be noted that the neural codecs mentioned above are typically trained offline with diverse video content \\cite{nawala2024bvi}, and deployed online for inference. In this case, model generalization becomes important, and this is why these codecs often have a large model capacity, resulting in large model sizes and slow inference runtime. Inspired by recent advances in implicit neural representations (INR), a new type of video codec has emerged that employs INR models to ``represent'' the video to be encoded by learning a coordinate-based mapping and compressing the network parameters for transmission. This approach converts a video coding problem into a model compression task, which allows the use of a much smaller network to ``overfit'' the input video, with the real potential for fast decoding. Existing implicit neural video representation (NeRV) models can be classified into index-based and content-based methods. The former takes frame~\\cite{chen2021nerv}, patch~\\cite{bai2023ps} or disentangled spatial/grid coordinates~\\cite{li2022nerv} as model input, while content-based approaches~\\cite{kwan2024hinerv,kim2024c3,leguay2024cool} have content-specific embedding as inputs. Currently, one of the best INR-based video codecs \\cite{kwan2024nvrc} has already achieved a performance similar to that of VVC VTM (RA), but with a much lower decoding complexity compared to autoencoder-based neural codecs. Some of these models have also been applied to volumetric video content~\\cite{ruan2024point,kwan2024immersive}, demonstrating their great potential to compete with standard and other learning-based methods. However, it should be noted that the training of most NeRV models is based on entire video sequences or even datasets, which results in a high system delay and does not meet the requirement of many video streaming applications. To address this limitation, efforts have been made~\\cite{gao2024pnvc} towards more practical INR-based video compression (such as the Low Delay and Random Access modes in VVC VTM \\cite{bossen2023vtmctc}) by combining pre-training and online model overfitting.\n\nSimilarly to image compression, there have been grand challenges organized in recent years for neural video compression, with notable venues including the NN-based Video Coding Grand Challenge associated with The IEEE International Symposium on Circuits and Systems (ISCAS) and the Challenge on Learned Image Compression (CLIC, video coding track) with IEEE/CVF CVPR and Data Compression Conference (in 2024). The best performer in ISCAS 2024 NN-based Video Coding Grand Challenge Challenge offers an overall 55\\% BD-rate saving over HEVC Test Model HM, while the winner of the CLIC (video coding track) in 2024, a neural-network enhanced ECM codec, shows a more than 2dB (in PSNR) gain compared to VTM (RA) at the same bitrates. \n\n\\subsubsection{Audio Compression}\n\nSimilarly to images and videos, learning-based solutions have also been researched to compress audio signals, and most neural audio codecs are based on VQ-VAE \\cite{Oord:Neural:2017}. SoundStream \\cite{zeghidour2021soundstream} is one of such models, which can encode audio content at various bitrate. It is based on a residual vector quantizer (RVQ) which trades off between rate, distortion, and complexity. This work has been further enhanced with a multi-scale spectrogram adversary and a loss balancer mechanism, resulting in improved rate-distortion performance. A more advanced universal model has been further developed \\cite{kumar2024high} based on improved adversarial and reconstruction losses, which can compress different types of audio. RVQ has also been extended from a single scale to multiple scales \\cite{siuzdak2024snac}, which performs hierarchical quantization at variable frame rates. \n\nMore recently, researchers have started to exploit the use of LLMs for audio compression, leveraging the audio generation/synthesis abilities of generative models. UniAudio 1.5 \\cite{yang2024uniaudio} is one of such attempts, which converts an audio into the textural space, which can be represented by a pre-trained LLM that shares a similar backbone of UniAudio \\cite{yang2023uniaudio}, an universal audio foundation model. LFSC is another neural audio codec based on LLMs, which achieved fast LLM training and inference through finite scalar quantization and adversarial training. \n\n\\subsection{Visual Quality Assessment}\n\\label{ssec:asssessment}\n\nAssessing the quality of visual signals remains an important and challenging task for many image and video processing applications. While subjective tests involving human participants offer a golden rule, objective quality models are designed and used in many cases because of their time and cost efficiency. These quality assessment methods are typically used to effectively evaluate the performance of different visual processing approaches, and they can also be converted to loss functions, which are employed for optimizing learning-based processing models.  \n\nIn recent years, quality assessment methods have also been enhanced by deep learning techniques. The resulting learning-based quality models can quickly adapt to a specific type of content, leading to better performance compared to conventional, hand-crafted quality metrics. This section provides a brief summary of existing works in this research area, and highlights the main challenges which should be addressed in the near future. A more comprehensive overview of the image and video quality assessment literature can be found in \\cite{zhai2020perceptual,zheng2024video,zhang2024quality}.\n\n\\subsubsection{Quality assessment models}\n\nImage and video quality assessment methods can be classified into two primary categories according to the availability of the corresponding reference content to the distorted test version: full-reference and no-reference models\\footnote{Reduced-reference quality metrics do existing in the literature, but the research in this field is less active in recent years.}. Prior to the AI era, conventional visual quality methods often exploit different characteristics of the human vision system and capture relevant information related to structural similarity (such as in SSIM and its variants \\cite{Bovik_SSIM,wang2003multiscale,rehman2015display}), distortion \\cite{chandler2007vsnr,larson2010most,STMAD}, and artifacts \\cite{ou2010perceptual,zhu2014no,zhang2015perception}. In many cases, the extract features are further processed by models that simulate texture masking \\cite{helmholtz1896handbook}, contract sensitivity \\cite{kelly1977visual}, and saliency \\cite{itti2001computational}. These hand-crafted quality models have also been combined with features within a regression-based framework in order to achieve more accurate prediction performance - VMAF is one of such examples \\cite{VMAFblog}. When neural networks are involved for feature extraction, they are trained to capture information which can directly contribute to quality prediction through an end-to-end optimization strategy. Initially, convolution network networks were typically used, with notable examples such as DeepQA \\cite{kim2017deep}, LPIPS \\cite{zhang2018unreasonable} and CONTRIQUE \\cite{madhusudana2022image} for image quality assessment, and TLVQA \\cite{korhonen2019two}, C3DVQA \\cite{xu2020c3dvqa} and DeepVQA \\cite{kim2018deep} for video quality assessment. Recent works have been reported to achieve better performance when Vision Transformers (ViTs) (or similar variants) are employed due to the effectiveness of their self-attention mechanism. Important works in this class including IQT \\cite{cheon2021perceptual}, TRes \\cite{golestaneh2022no}, SaTQA \\cite{shi2024transformer}, FastVQA \\cite{wu2022fast} and RankDVQA \\cite{feng2024rankdvqa}. The former has been further extended as DOVER \\cite{wu2023exploringvideo} and COVER \\cite{he2024cover} when aesthetic and/or semantic aspects in the content are taken into account. \n\nMore recently, inspired by the success of large language models (LLMs) \\cite{openai:gpt4:2023,touvron2023llama} in other machine learning tasks, they have also been utilized in image and video quality assessment and demonstrate great potential to achieve better model generalization. Q-Bench \\cite{wu2024qbench} is one of the first attempts that employs multimodality large language models to predict the perceptual quality of images based on prompt-driven evaluation. It inquiries the LLMs to provide information related to the final quality rating of the input image and the quality description. This has been further extended for video quality assessment tasks, named Q-Align \\cite{wu2024qalign}. Other notable works of this kind include X-iqe \\cite{chen2023x} that performs the quality prompt in a multi-iteration manner focusing on both image fidelity and aesthetics. Prompt-based approaches have also been proposed for differentiating the quality difference between multiple images, such as 2AFC-LMMs \\cite{zhu20242afc} based on a two-alternative forced choice prompt and MAP (maximum a posteriori) estimation. Moreover, recent research works also focus on using pre-trained vision-language models, such as CLIP \\cite{radford2021learning}, which align better image and text modalities. Important examples in this class for image quality assessment including ZEN-IQA \\cite{miyata2024zen}, QA-CLIP \\cite{pan2023quality} and PromptIQA \\cite{chen2025promptiqa}. Similar works have also been proposed for video quality assessment, such as BVQI \\cite{wu2023exploring,wu2023towards} and COVER \\cite{he2024cover}.\n\n\n\nTo support the training and validation of learning-based quality assessment models, image or video databases containing ground-truth subjective quality scores are typically employed. Commonly used image quality databases include LIVE \\cite{sheikh2006astatistical}, CSIQ \\cite{larson2010most}, TID2013 \\cite{ponomarenko2013color}, PieAPP and PIPAL, while video quality databases such as LIVE-VQA~\\cite{seshadrinathan2010study}, KoNViD-1K~\\cite{hosu2017konstanz}, YouTube UGC~\\cite{wang2019youtube} and LIVE-VQC~\\cite{sinno2018large} are typically employed for benchmarking in the literature. There are also databases developed that investigate the impact of specific video formats and/or artifacts, such as LIVE-YT-HFR  \\cite{madhusudana2021subjective} focusing on frame rates, VSR-QAD \\cite{zhou2024database} on spatial resolution (or super-resolution artifacts), BAND-2k \\cite{chen2024band2k} on banding artifacts and Maxwell \\cite{wu2023towards}/BVI-Artifact \\cite{feng2024bvi} containing multiple artifacts commonly produced in video streaming. Based on these databases, many learning-based quality assessment models are trained to minimize the difference (L1 or L2 norm) between predicted quality indices and subjective scores. However, due to the limited number of ground-truth quality labels associated with these databases and the resourcing costing nature for collecting subjective data through human participants involved psychophysical experiments, this type of training methodology cannot offer satisfactory performance, in particular when the model capacity is large. Moreover, since the experimental settings and conditions used for quality labeling are different in these databases, intra-database cross-validation is always required due to the limited model generalization and potential overfitting problems. \n\nTo address these issues, various proxy quality metrics have been used to label images and videos, which avoid expensive subjective tests and enable the generation of a large amount of training material with pseudo-ground-truth quality annotations. To further improve the reliability of quality labels, instead of learning the absolute values of the quality labels, ranking-inspired training strategies have been developed, which focus on improving the monotonicity characteristics of quality. Important examples based on these weakly supervised training methodologies include RankIQA \\cite{liu2017rankiqa} and UNIQUE \\cite{zhang2021uncertainty} for the image quality assessment task, and VFIPS \\cite{hou2022perceptual} and RankDVQA \\cite{feng2024rankdvqa} for video quality assessment. Moreover, different self-supervised learning approaches have also been employed, which transform quality labeling to an auxiliary task. For example, CONTRIQUE \\cite{madhusudana2022image} learns relevant features from an unannotated image database based on the prediction of distortion types and degrees through contrastive learning. This method has been further applied to video quality assessment, resulting in a contrastive video quality estimator, CONVIQT \\cite{madhusudana2023conviqt}. More recently, quality-aware contrastive loss has been designed in \\cite{zhao2023quality,peng2024rmt} to stabilize the learning process.\n\n\\subsubsection{Performance and main challenges}\n\nIt should be noted that due to the lack of standard test conditions and limited model generalization within many existing image and video quality assessment models, these methods are typically trained and benchmarked with different databases and intra-database cross-validation is often performed. This results in different evaluation results and conclusions. To enable a fair and meaningful comparison, various challenges and contests have been held for visual quality assessment. The Sixth Challenge on Learned Image Compression (CLIC) \\cite{clic} associated with the Data Compression Conference 2024 is one of the latest examples which includes two quality assessment tracks for image and video compression. The best performer in the video quality assessment track achieves a Spearman Ranking Correlation Coefficient value of 0.825 \\cite{feng2024rankdvqa}. Other notable challenges include the IEEE/CVF WACV 2023 HDR VQA Grand Challenge and the Video Super-Resolution Quality Assessment Challenge in ECCV 2024, which focus on high dynamic rage and super-resolved content, respectively. \n\nAlthough significant progress has been made in the past few years in visual quality assessment, including new models and training methodology, challenges still exist, including limited model generalization and high computational complexity. Another important application of quality metrics is to be used as a loss function for different image and video processing methods, which is expected to produce results with better visual quality. This requires further capability and robustness for these quality models. All of these issues need to be addressed in future work. \n\n", "appendix": false}, "Future Challenges for AI in the Creative Sector": {"content": "\n\\label{sec:discussion}\n\nLLMs enable artists to articulate their ideas through text prompts, facilitating a top-down creative process. This approach starts with a high-level conceptualization and planning, allowing artists to frame an overarching idea, theme, or goal that guides the creative process before diving into specific details or implementation tasks. This method contrasts with the bottom-up approach, where creativity begins with detailed elements and builds up to a broader structure or idea. This means the effectiveness of current AI technologies may vary among artists, and achieving desirable outcomes from AI heavily depends on the users' ability to provide accurate text inputs. Moreover, recent technologies generate digital content from a `random' seed, meaning that with the same text prompts, the output might not be the same. This makes editing those outputs to better fit the precise thoughts or imagination of artists still challenging.\n\nOriginally not directly designed for creative industries, coding and debugging tools \\cite{zhong:LDB:2024} help software developers expedite their work, including tasks within creative sectors. Speaking at the World Government Summit in Dubai in 2024\\footnote{\\url{https://blogs.nvidia.com/blog/world-governments-summit/}}, Nvidia CEO Jensen Huang argued that with rapid advancements in AI, learning to code might become less critical for those entering the tech sector. He foresees a future where advancements in computing might make traditional programming obsolete, thus automating tasks and boosting productivity, especially for artists who lack programming expertise. While this view isn't universally accepted, it clearly offers potential benefits for artists navigating an evolving tech landscape. However, AI coding for creative artists may need specific requirements to serve unstructural creativity.\n\nFurthermore, leading AI organizations like OpenAI, DeepMind, and Anthropic are proponents of the potential for artificial general intelligence (AGI), an advanced form of AI that could achieve human or even superhuman levels of intelligence. Such AI systems could eventually exceed human intelligence, potentially benefiting humanity as a whole. However, solving such complex problems requires not only advanced computation and innovative algorithms but also a deep understanding of how the human brain functions.\n\n\\subsection{Ethical Issues, Fakes and Bias}\n\n\n\nAdvancements in AI, exemplified by models like Sora and Gemini 1.5 Pro, provoke ethical concerns and societal implications. These models, capable of generating realistic content, escalate the risk of misuse, including deepfakes and misinformation. Rapid AI development raises questions about job displacement and the delicate balance between automation and human involvement. Ensuring AI augments rather than undermines human efforts poses a significant challenge for developers and policymakers. In a related context, the artist Miles Astray demonstrated that even authentic photographs could be mistaken for AI-generated images. His real photograph titled `F L A M I N G O N E' won both the jury\u2019s award and the people\u2019s choice award in the AI category of the 1839 Awards. He aimed to highlight the ethical dilemmas inherent in AI, suggesting that the benefits of discussing AI's ethical implications could surpass the ethical concerns related to viewer deception\\footnote{\\url{https://www.milesastray.com/news/newsflash-reclaiming-the-brain}}.\n\nThe uncanny valley effect further complicates matters, challenging perceptions of authenticity. Democratizing AI tools presents opportunities but necessitates robust regulatory frameworks to safeguard privacy. Deepfake technology, while having potential positive applications like in filmmaking, also raises significant concerns about spreading misinformation and other malicious uses. Efforts to detect and combat deepfakes include developing algorithms and tools to identify manipulated media, promoting media literacy, and implementing policies to address the ethical and legal implications.\n\nUnified Concept Editing \\cite{Gandikota:Unified:2024} has been proposed for image generation in digital mediums. This method aims to ensure diverse representation, reducing gender and racial biases, and producing safe content. Additionally, hallucination in generative AI refers to models generating outputs that are not faithful representations of reality but instead contain imagined or unrealistic elements, which may be due to limitations in the training data, biases in the model architecture, or imperfections in the optimization process. The hallucinations is one of the issues that the UK government's viewpoint on LLMs highlights ongoing issues \\cite{UK:Large:2024}. Other issues include biases, regurgitation of private data, difficulties with multi-step tasks, and challenges in interpreting black-box processes.\n\nGovernments have increasingly expressed concerns about the challenges and key uncertainties that generative AI technologies pose to rightsholders and human creativity \\cite{Jeary2024}. Generative AI presents substantial legal challenges, including the copyright status of AI-generated works and the ownership of intellectual property and copyrights for the datasets used in training AI models, which typically originate from various sources. There are different views from artists on using AI and copyrights. For example, the track ``Heart on My Sleeve,\" penned by an as yet unidentified human author, featured AI-generated vocals that replicated the voices of Drake and The Weeknd. Released independently on April 4, 2023, it was accessible on various streaming platforms including Apple Music, Spotify, and YouTube. The song quickly became viral, accumulating over 20 million views across all platforms\\footnote{\\url{https://www.nbcnews.com/pop-culture/viral-ai-powered-drake-weeknd-song-removed-streaming-services-rcna80098}}, before its removal by Universal Music Group, Drake's recording label. In contrast, Canadian artist Grimes has extended an invitation to musicians to emulate her voice via AI for the creation of new musical pieces, stipulating that the lyrics should not be harmful. She advocates for the democratization of art and the abolition of copyright\\footnote{\\url{https://www.bbc.com/news/entertainment-arts-65385382}}. Additionally, Grimes has employed AI to design visual content for her LED backdrop at Coachella in 2024.\n\n\n\\subsection{The future of AI technologies}\n\nA substantial amount of data is essential for training AI models to achieve high performance. Major companies such as Google, Meta, and NVIDIA, with their respective models\u2014BERT, Segment Anything, and Canvas\u2014dominate this space, leveraging their resources to gather data and train sophisticated models. However, Bloomberg released an article in November 2024 stating that OpenAI, Anthropic, and Google are experiencing slow growth in the performance of their AI models, with one of the key challenges being training data\\footnote{\\url{https://www.bloomberg.com/news/articles/2024-11-13/openai-google-and-anthropic-are-struggling-to-build-more-advanced-ai}}.\nWithin the creative industries, a popular practice is to fine-tune these pre-trained models using techniques such as supervised learning with specific datasets, zero/few-shot learning, or domain adaptation.\n\nLLMs excel in applications involving complex tasks, advanced reasoning, data analysis, and understanding context. \nHowever, these models typically require high computational resources or cloud computing for operation and fine-tuning. A new trend emerging in parallel with LLMs is the development of Small language models (SLMs), such as Phi-3 by Microsoft\\footnote{\\url{https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/}}. SLMs offer promising solutions for regulated industries and sectors encountering scenarios where high-quality results are essential while keeping data on-premises. This potential is particularly evident in deploying more capable SLMs on smartphones and other mobile devices, allowing them to operate `at the edge' without relying on cloud connectivity. The recently highly successful platforms, such as  DeepSeek-V3 \\cite{deepseekv3} and Qwen2.5-Max \\cite{qwen25}, are based on Mixture-of-Experts (MoE) models, which tackle complex problems by dividing them into simpler sub-tasks, each handled by a specialized ``expert.\"\n\nDespite the advancements in AI, the current models still struggle with tasks requiring planning ability and are prone to errors when encountering unexpected data. They may learn through reinforcement learning, but this process often identifies the best outcome as an anomaly rather than the norm. Yann LeCun, Professor at NYU and Chief AI Scientist at Meta, notes that while LLMs show a degree of comprehension in processing and generating text, their understanding lacks depth, leading to errors that defy common sense\\footnote{\\url{https://twitter.com/ylecun/status/1728496457601183865}}. He advocates for self-supervised learning as a pivotal future direction for AI, emphasizing its potential to derive insights from unlabeled data. Concurrently, Andrew Ng, Adjunct Professor at Stanford University and Founder of DeepLearning.AI, sees iterative AI agentic workflows\\footnote{\\url{https://www.youtube.com/watch?v=sal78ACtGTc}} as a key advancement for enhancing AI tool capabilities through an interactive approach by AI agents. These workflows utilize advanced algorithms and data processing techniques to automate complex tasks with simple data entry and validation to intricate decision-making processes.\n\nThe increasing openness of code and datasets is seen as a catalyst for accelerating AI advancements, with major firms like Microsoft, Google, and Meta supporting open access technologies. However, the openness also introduces significant security risks, necessitating new regulatory measures to monitor models post-release, standardize documentation, and assess the safety of disclosing software code and training data.\n\nFinally, as mentioned in \\cite{Jeary2024}, the rapid advancement of these technologies has revolutionized cultural experiences, often referred to as `CreaTech'\u2014the convergence of the creative and digital sectors \\cite{CreativeIndustriesCouncil2021}. These innovations not only reshape how people engage with art and creative works (e.g., through AR/VR) but also drive the evolution of the technologies themselves.\n\n\n\n", "appendix": false}, "Concluding Remarks": {"content": "\n\\label{sec:conclusion}\n\nThis paper presents a comprehensive review of current AI technologies and their applications that have emerged in recent years. With generative AI, there is a rapid growth in AI usage in the creative sector, and these technologies have significantly advanced the state of the art across various creative applications, including content creation, information analysis, content enhancement, information extraction, and data compression.\n\nThrough these applications, Generative AI not only broadens creative possibilities but also reduces the manual labor and time traditionally associated with production pipeline, allowing for greater creative experimentation and quicker production cycles. As this technology advances, it promises to unlock even more sophisticated capabilities in the creative industry. However, artists and users need to adapt and learn to use these tools efficiently and effectively.\n\n\\bibliographystyle{IEEEbib}IEEEbib\n\\bibliography{literature_review}\n\n", "appendix": false}}, "categories": ["cs.AI"], "published": "2025-01-06 02:46:33+00:00", "primary_category": "cs.AI", "summary": "The rapid advancements in artificial intelligence (AI), particularly in\ngenerative AI and large language models (LLMs), have profoundly impacted the\ncreative industries by enabling innovative content creation, enhancing\nworkflows, and democratizing access to creative tools. This paper explores the\nsignificant technological shifts since our previous review in 2022,\nhighlighting how these developments have expanded creative opportunities and\nefficiency. These technological advancements have enhanced the capabilities of\ntext-to-image, text-to-video, and multimodal generation technologies. In\nparticular, key breakthroughs in LLMs have established new benchmarks in\nconversational AI, while advancements in image generators have revolutionized\ncontent creation. We also discuss AI integration into post-production\nworkflows, which has significantly accelerated and refined traditional\nprocesses. Despite these innovations, challenges remain, particularly for the\nmedia industry, due to the demands on communication traffic from creative\ncontent. We therefore include data compression and quality assessment in this\npaper. Furthermore, we highlight the trend toward unified AI frameworks capable\nof addressing multiple creative tasks and underscore the importance of human\noversight to mitigate AI-generated inaccuracies. Finally, we explore AI's\nfuture potential in the creative sector, stressing the need to navigate\nemerging challenges to maximize its benefits while addressing associated risks."}