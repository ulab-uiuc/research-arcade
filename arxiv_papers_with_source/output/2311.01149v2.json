{"title": "ChineseWebText: Large-scale High-quality Chinese Web Text Extracted with Effective Evaluation Model", "author": "Jianghao Chen", "abstract": "\\begin{abstract}\n\n    During the development of large language models (LLMs), the scale and quality of the pre-training data play a crucial role in shaping LLMs' capabilities. To accelerate the research of LLMs, several large-scale datasets, such as C4 \\cite{2020T5C4}, Pile \\cite{2020_pile}, RefinedWeb \\cite{2023refinedweb} and WanJuan \\cite{2023wanjuan}, have been released to the public. However, most of the released corpus focus mainly on English, and there is still lack of complete tool-chain for extracting clean texts from web data. Furthermore, fine-grained information of the corpus, e.g. the quality of each text, is missing. To address these challenges, we propose in this paper a new complete tool-chain \\textbf{EvalWeb} to extract Chinese clean texts from noisy web data. First, similar to previous work, manually crafted rules are employed to discard explicit noisy texts from the raw crawled web contents. Second, a well-designed evaluation model is leveraged to assess the remaining relatively clean data, and each text is assigned a specific quality score. Finally, we can easily utilize an appropriate threshold to select the high-quality pre-training data for Chinese. Using our proposed approach, we release the largest and latest large-scale high-quality Chinese web text \\textbf{ChineseWebText}, which consists of 1.42 TB and each text is associated with a quality score, facilitating the LLM researchers to choose the data according to the desired quality thresholds. We also release a much cleaner subset of 600 GB Chinese data with the quality exceeding 90\\%. The data, codes and the tool-chain are available in this website \\footnote{\\url{https://github.com/CASIA-LM/ChineseWebText}}.\n    \n    %High-quality and extensive pre-training data could enable the models to acquire a wide range of knowledge and abilities. While obtaining high-quality data, especially Chinese high-quality pre-training data, still have significant challenges, web data contains a vast array of diverse Chinese pretraining data plagued by a substantial amount of noise. In order to extract high-quality pre-training data from web for Chinese, in this paper we propose a new automatic pipline approach, \\textbf{EvalWeb}. To ensure data quality, this approach first employs manually crafted rules to clean the raw crawled web data. Subsequently, a evaluaiton model is used to further process the cleaned data, thereby generating a quality score for each of them. Finally, we can use a appropriate threshold to filter out the high-quality pre-training data. With this approach, we process some crawled web data and release a 537 GB Chinese dataset \\textbf{ChineseWebText1.0}, which is the largest public Chinese dataset. The results demonstrate that the accuracy of our cleaned data can exceed 95\\%, which could effectively meet the requirements of pre-training. Notably, in the released dataset, we provide not only the final high-quality dataset but also intermediate data with quality scores, allowing users to re-filter the data according to their specific needs. Codes, data and models are available\\footnote{\\url{}}.\n   \n\\end{abstract}", "citations": {"2020T5C4": {"bib_key": "2020T5C4", "bib_title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer", "bib_author ": "Colin Raffel", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": null, "subsection": null, "subsubsection": null, "prev_context": "To accelerate the research of LLMs, several large-scale datasets, such as C4\\cite{2020T5C4}", "next_context": ", Pile\\cite{2020_pile}, RefinedWeb\\cite{2023refinedweb}and WanJuan\\cite{2023wanjuan}, have been released to the public."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "In order to expedite the research on LLMs, several large-scale datasets have been made publicly available in recent years, such as C4\\cite{2020T5C4}", "next_context": ", Pile\\cite{2020_pile}, RefinedWeb\\cite{2023refinedweb}and WanJuan\\cite{2023wanjuan}."}, {"section": "Data Construction", "subsection": "Quality Control", "subsubsection": null, "prev_context": "\\midruleC4\\cite{2020T5C4}", "next_context": ""}, {"section": "Data Construction", "subsection": "Dataset Statistics and Comparison", "subsubsection": null, "prev_context": "Specifically, C4\\cite{2020T5C4}", "next_context": ", The Pile\\cite{2020_pile}and REFINEDWEB\\cite{2023refinedweb}are three public English datasets, while WuDaoCorpora\\cite{2021WuDaoCorpora}, ROOTS-zh\\cite{2023roots}and WanJuan1.0-zh\\cite{2023wanjuan}are three corpora for Chinese."}], "importance_score": 4.0}, "2020_pile": {"bib_key": "2020_pile", "bib_title": "The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling", "bib_author ": "Gao, Leo", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": null, "subsection": null, "subsubsection": null, "prev_context": "To accelerate the research of LLMs, several large-scale datasets, such as C4\\cite{2020T5C4}, Pile\\cite{2020_pile}", "next_context": ", RefinedWeb\\cite{2023refinedweb}and WanJuan\\cite{2023wanjuan}, have been released to the public."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "In order to expedite the research on LLMs, several large-scale datasets have been made publicly available in recent years, such as C4\\cite{2020T5C4}, Pile\\cite{2020_pile}", "next_context": ", RefinedWeb\\cite{2023refinedweb}and WanJuan\\cite{2023wanjuan}."}, {"section": "Data Construction", "subsection": "Quality Control", "subsubsection": null, "prev_context": "The Pile\\cite{2020_pile}", "next_context": ""}, {"section": "Data Construction", "subsection": "Dataset Statistics and Comparison", "subsubsection": null, "prev_context": "Specifically, C4\\cite{2020T5C4}, The Pile\\cite{2020_pile}", "next_context": "and REFINEDWEB\\cite{2023refinedweb}are three public English datasets, while WuDaoCorpora\\cite{2021WuDaoCorpora}, ROOTS-zh\\cite{2023roots}and WanJuan1.0-zh\\cite{2023wanjuan}are three corpora for Chinese."}], "importance_score": 4.0}, "2023refinedweb": {"bib_key": "2023refinedweb", "bib_title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only", "bib_author ": "Guilherme Penedo", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": null, "subsection": null, "subsubsection": null, "prev_context": "To accelerate the research of LLMs, several large-scale datasets, such as C4\\cite{2020T5C4}, Pile\\cite{2020_pile}, RefinedWeb\\cite{2023refinedweb}", "next_context": "and WanJuan\\cite{2023wanjuan}, have been released to the public."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The models, such as GPT-3\\cite{brown2020language}, BLOOM\\cite{scao2022bloom}, LLaMA\\cite{llama}, Falcon\\cite{2023refinedweb}", "next_context": ", PaLM\\cite{chowdhery2022palm}and GPT-4\\cite{GPT4}, become more and more powerful, even performing better than humans in some natural language understanding and generation tasks."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "In order to expedite the research on LLMs, several large-scale datasets have been made publicly available in recent years, such as C4\\cite{2020T5C4}, Pile\\cite{2020_pile}, RefinedWeb\\cite{2023refinedweb}", "next_context": "and WanJuan\\cite{2023wanjuan}."}, {"section": "Data Construction", "subsection": "Quality Control", "subsubsection": null, "prev_context": "REFINEDWEB\\cite{2023refinedweb}", "next_context": ""}, {"section": "Data Construction", "subsection": "Dataset Statistics and Comparison", "subsubsection": null, "prev_context": "Specifically, C4\\cite{2020T5C4}, The Pile\\cite{2020_pile}and REFINEDWEB\\cite{2023refinedweb}", "next_context": "are three public English datasets, while WuDaoCorpora\\cite{2021WuDaoCorpora}, ROOTS-zh\\cite{2023roots}and WanJuan1.0-zh\\cite{2023wanjuan}are three corpora for Chinese."}], "importance_score": 5.0}, "2023wanjuan": {"bib_key": "2023wanjuan", "bib_title": "WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models", "bib_author ": "Conghui He", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": null, "subsection": null, "subsubsection": null, "prev_context": "To accelerate the research of LLMs, several large-scale datasets, such as C4\\cite{2020T5C4}, Pile\\cite{2020_pile}, RefinedWeb\\cite{2023refinedweb}and WanJuan\\cite{2023wanjuan}", "next_context": ", have been released to the public."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "In order to expedite the research on LLMs, several large-scale datasets have been made publicly available in recent years, such as C4\\cite{2020T5C4}, Pile\\cite{2020_pile}, RefinedWeb\\cite{2023refinedweb}and WanJuan\\cite{2023wanjuan}", "next_context": "."}, {"section": "Data Construction", "subsection": "Quality Control", "subsubsection": null, "prev_context": "WanJuan1.0-zh\\cite{2023wanjuan}", "next_context": ""}, {"section": "Data Construction", "subsection": "Dataset Statistics and Comparison", "subsubsection": null, "prev_context": "Specifically, C4\\cite{2020T5C4}, The Pile\\cite{2020_pile}and REFINEDWEB\\cite{2023refinedweb}are three public English datasets, while WuDaoCorpora\\cite{2021WuDaoCorpora}, ROOTS-zh\\cite{2023roots}and WanJuan1.0-zh\\cite{2023wanjuan}", "next_context": "are three corpora for Chinese."}], "importance_score": 4.0}, "brown2020language": {"bib_key": "brown2020language", "bib_title": "Language models are few-shot learners", "bib_author ": "Brown, Tom", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The models, such as GPT-3\\cite{brown2020language}", "next_context": ", BLOOM\\cite{scao2022bloom}, LLaMA\\cite{llama}, Falcon\\cite{2023refinedweb}, PaLM\\cite{chowdhery2022palm}and GPT-4\\cite{GPT4}, become more and more powerful, even performing better than humans in some natural language understanding and generation tasks."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "The simplest text classification model is logistic regression\\cite{brown2020language}", "next_context": ", which uses the logistic function to calculate the probability values for each text, and then classifies them into positive or negative with a designed threshold."}], "importance_score": 2.0}, "scao2022bloom": {"bib_key": "scao2022bloom", "bib_title": "Bloom: A 176b-parameter open-access multilingual language model", "bib_author ": "Scao, Teven Le", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The models, such as GPT-3\\cite{brown2020language}, BLOOM\\cite{scao2022bloom}", "next_context": ", LLaMA\\cite{llama}, Falcon\\cite{2023refinedweb}, PaLM\\cite{chowdhery2022palm}and GPT-4\\cite{GPT4}, become more and more powerful, even performing better than humans in some natural language understanding and generation tasks."}], "importance_score": 1.0}, "llama": {"bib_key": "llama", "bib_title": "LLaMA: Open and Efficient Foundation Language Models", "bib_author ": "Hugo Touvron", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The models, such as GPT-3\\cite{brown2020language}, BLOOM\\cite{scao2022bloom}, LLaMA\\cite{llama}", "next_context": ", Falcon\\cite{2023refinedweb}, PaLM\\cite{chowdhery2022palm}and GPT-4\\cite{GPT4}, become more and more powerful, even performing better than humans in some natural language understanding and generation tasks."}], "importance_score": 1.0}, "chowdhery2022palm": {"bib_key": "chowdhery2022palm", "bib_title": "Palm: Scaling language modeling with pathways", "bib_author ": "Chowdhery, Aakanksha", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The models, such as GPT-3\\cite{brown2020language}, BLOOM\\cite{scao2022bloom}, LLaMA\\cite{llama}, Falcon\\cite{2023refinedweb}, PaLM\\cite{chowdhery2022palm}", "next_context": "and GPT-4\\cite{GPT4}, become more and more powerful, even performing better than humans in some natural language understanding and generation tasks."}], "importance_score": 1.0}, "GPT4": {"bib_key": "GPT4", "bib_title": "{GPT-4} Technical Report", "bib_author ": "OpenAI", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "The models, such as GPT-3\\cite{brown2020language}, BLOOM\\cite{scao2022bloom}, LLaMA\\cite{llama}, Falcon\\cite{2023refinedweb}, PaLM\\cite{chowdhery2022palm}and GPT-4\\cite{GPT4}", "next_context": ", become more and more powerful, even performing better than humans in some natural language understanding and generation tasks."}], "importance_score": 1.0}, "lee_deduplicating_2022": {"bib_key": "lee_deduplicating_2022", "bib_title": "Deduplicating {Training} {Data} {Makes} {Language} {Models} {Better}", "bib_author ": "Lee, Katherine", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "\\cite{lee_deduplicating_2022}", "next_context": "methods are employed to remove duplicate text from the data, while some handcrafted rules\\cite{raffel_exploring_2020,luccioni_whats_2021}are adopted to filter out violence, pornographic, advertisement and other explicit noisy data."}], "importance_score": 1.0}, "raffel_exploring_2020": {"bib_key": "raffel_exploring_2020", "bib_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "bib_author ": "Raffel, Colin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "\\cite{lee_deduplicating_2022}methods are employed to remove duplicate text from the data, while some handcrafted rules\\cite{raffel_exploring_2020,luccioni_whats_2021}", "next_context": "are adopted to filter out violence, pornographic, advertisement and other explicit noisy data."}], "importance_score": 0.5}, "luccioni_whats_2021": {"bib_key": "luccioni_whats_2021", "bib_title": "What's in the {Box}? {An} {Analysis} of {Undesirable} {Content} in the {Common} {Crawl} {Corpus}", "bib_author ": "Luccioni, Alex", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "\\cite{lee_deduplicating_2022}methods are employed to remove duplicate text from the data, while some handcrafted rules\\cite{raffel_exploring_2020,luccioni_whats_2021}", "next_context": "are adopted to filter out violence, pornographic, advertisement and other explicit noisy data."}], "importance_score": 0.5}, "wenzek_ccnet_2020": {"bib_key": "wenzek_ccnet_2020", "bib_title": "{CCNet}: {Extracting} {High} {Quality} {Monolingual} {Datasets} from {Web} {Crawl} {Data}", "bib_author ": "Wenzek, Guillaume", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Besides, perplexity\\cite{wenzek_ccnet_2020}", "next_context": "is also usually used to evaluate the fluency of the texts."}, {"section": "Data Construction", "subsection": "Data Collection and Preparation", "subsubsection": null, "prev_context": "Following the work of CCNet\\cite{wenzek_ccnet_2020}", "next_context": ", in this module a Hash-based inter-string deduplication method is employed to remove duplicate text from different snapshots."}, {"section": "Data Construction", "subsection": "Quality Evaluation", "subsubsection": "Evaluation Model Comparison", "prev_context": "\\cite{wenzek_ccnet_2020}", "next_context": ", we utilize a well-trained language model to calculate the perplexity of the texts and classify them with a threshold based on perplexity values."}], "importance_score": 3.0}, "devlin_bert_2018": {"bib_key": "devlin_bert_2018", "bib_title": "Bert: {Pre}-training of deep bidirectional transformers for language understanding", "bib_author ": "Devlin, Jacob", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Currently, BERT\\cite{devlin_bert_2018}", "next_context": "and FastText\\cite{joulin_bag_2017}are both commonly used text classification models."}], "importance_score": 1.0}, "joulin_bag_2017": {"bib_key": "joulin_bag_2017", "bib_title": "Bag of {Tricks} for {Efficient} {Text} {Classification}", "bib_author ": "Joulin, Arm", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Currently, BERT\\cite{devlin_bert_2018}and FastText\\cite{joulin_bag_2017}", "next_context": "are both commonly used text classification models."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "FastText\\cite{joulin_bag_2017}", "next_context": "is also a neural network based approach which is similar to CBOW\\cite{word2vec}."}], "importance_score": 2.0}, "transformer": {"bib_key": "transformer", "bib_title": "Attention is All you Need", "bib_author ": "Ashish Vaswani", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "BERT is a transformer-based\\cite{transformer}", "next_context": "pre-training language model that has achieved remarkable performance in various text classification and understanding tasks."}], "importance_score": 1.0}, "word2vec": {"bib_key": "word2vec", "bib_title": "Efficient Estimation of Word Representations in Vector Space", "bib_author ": "Tom{\\'{a}}s Mikolov", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "FastText\\cite{joulin_bag_2017}is also a neural network based approach which is similar to CBOW\\cite{word2vec}", "next_context": "."}], "importance_score": 1.0}, "raffel2020exploring": {"bib_key": "raffel2020exploring", "bib_title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "bib_author ": "Raffel, Colin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Due to the convenience of acquisition and cost-efficiency associated with web-scraped data, it has progressively emerged as a pivotal source for pre-training datasets\\cite{raffel2020exploring}", "next_context": "."}], "importance_score": 1.0}, "gao2020pile": {"bib_key": "gao2020pile", "bib_title": "The pile: An 800gb dataset of diverse text for language modeling", "bib_author ": "Gao, Leo", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "(2020)\\cite{gao2020pile}", "next_context": "build a 825 GB English corpus by mixing established natural language processing datasets and several newly introduced ones."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "(2020)\\cite{gao2020pile}", "next_context": ", Penedo et al."}], "importance_score": 2.0}, "presser2020books3": {"bib_key": "presser2020books3", "bib_title": "Books3", "bib_author ": "Presser, Shawn", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "This dataset covers 22 diverse high-quality subsets which derive from academic or professional sources, including PubMed Central, the FreeLaw Project, Stack Exchange, Books3\\cite{presser2020books3}", "next_context": ", OpenSubtitles\\cite{tiedemann2016finding}and so on."}], "importance_score": 1.0}, "tiedemann2016finding": {"bib_key": "tiedemann2016finding", "bib_title": "Finding alternative translations in a large corpus of movie subtitle", "bib_author ": "Tiedemann, J{\\\"o}rg", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "This dataset covers 22 diverse high-quality subsets which derive from academic or professional sources, including PubMed Central, the FreeLaw Project, Stack Exchange, Books3\\cite{presser2020books3}, OpenSubtitles\\cite{tiedemann2016finding}", "next_context": "and so on."}], "importance_score": 1.0}, "penedo2023refinedweb": {"bib_key": "penedo2023refinedweb", "bib_title": "The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only", "bib_author ": "Penedo, Guilherme", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "(2023)\\cite{penedo2023refinedweb}", "next_context": "demonstrate that properly filtered and deduplicated web data alone can also train a powerful model, even outperforming the LLMs trained on curated corpora."}], "importance_score": 1.0}, "gunasekar2023textbooks": {"bib_key": "gunasekar2023textbooks", "bib_title": "Textbooks Are All You Need", "bib_author ": "Gunasekar, Suriya", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "(2023)\\cite{gunasekar2023textbooks}", "next_context": "build a code dataset which has 7B tokens."}], "importance_score": 1.0}, "he2023wanjuan": {"bib_key": "he2023wanjuan", "bib_title": "WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models", "bib_author ": "Conghui He", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "(2023)\\cite{he2023wanjuan}", "next_context": "release a comprehensive multimodal dataset, which also contains texts in both Chinese and English, and is collected from a wide range of web sources."}], "importance_score": 1.0}, "stanton2021does": {"bib_key": "stanton2021does", "bib_title": "Does knowledge distillation really work?", "bib_author ": "Stanton, Samuel", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Data Construction", "subsection": null, "subsubsection": null, "prev_context": "considering computational cost and efficiency, we further propose to leverage knowledge distillation\\cite{stanton2021does}", "next_context": "techniques to train a FastText classifier, which can achieve similar performance with faster efficiency and lower computational costs."}], "importance_score": 1.0}, "grave2018learning": {"bib_key": "grave2018learning", "bib_title": "Learning Word Vectors for 157 Languages", "bib_author ": "Grave, {\\'E}douard", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Data Construction", "subsection": "Data Collection and Preparation", "subsubsection": null, "prev_context": "Additionally, a well-trained language identification model\\cite{grave2018learning}", "next_context": ", which could support 157 languages, is applied to select Chinese data."}], "importance_score": 1.0}, "wang_use_2022": {"bib_key": "wang_use_2022", "bib_title": "On the Use of Bert for Automated Essay Scoring: Joint Learning of Multi-Scale Essay Representation", "bib_author ": "Wang, Yongjie", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Data Construction", "subsection": "Quality Evaluation", "subsubsection": "BERTEval", "prev_context": "\\bottomrule\\textbf{BERTEval Architecture}We utilized Tran-BERT-MS-ML-R\\cite{wang_use_2022}", "next_context": ", an effective AES model based on the BERT-base architecture, to evaluate the quality of the text obtained from web crawling."}, {"section": "Data Construction", "subsection": "Quality Evaluation", "subsubsection": "BERTEval", "prev_context": "\\textbf{Loss Function}In addition to the MSE loss\\cite{mesgar_neural_2018}, we used the following two loss functions: Margin Ranking (MR) loss\\cite{liu2021temp}and Cosine Similarity (CS) loss\\cite{wang_use_2022}", "next_context": "."}], "importance_score": 2.0}, "mesgar_neural_2018": {"bib_key": "mesgar_neural_2018", "bib_title": "A neural local coherence model for text quality assessment", "bib_author ": "Mesgar, Mohsen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Data Construction", "subsection": "Quality Evaluation", "subsubsection": "BERTEval", "prev_context": "\\textbf{Loss Function}In addition to the MSE loss\\cite{mesgar_neural_2018}", "next_context": ", we used the following two loss functions: Margin Ranking (MR) loss\\cite{liu2021temp}and Cosine Similarity (CS) loss\\cite{wang_use_2022}."}], "importance_score": 1.0}, "liu2021temp": {"bib_key": "liu2021temp", "bib_title": "TEMP: taxonomy expansion with dynamic margin loss through taxonomy-paths", "bib_author ": "Liu, Zichen", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Data Construction", "subsection": "Quality Evaluation", "subsubsection": "BERTEval", "prev_context": "\\textbf{Loss Function}In addition to the MSE loss\\cite{mesgar_neural_2018}, we used the following two loss functions: Margin Ranking (MR) loss\\cite{liu2021temp}", "next_context": "and Cosine Similarity (CS) loss\\cite{wang_use_2022}."}], "importance_score": 1.0}, "scudder1965probability": {"bib_key": "scudder1965probability", "bib_title": "Probability of error of some adaptive pattern-recognition machines", "bib_author ": "Scudder, Henry", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Data Construction", "subsection": "Quality Evaluation", "subsubsection": "BERTEval", "prev_context": "To ameliorate this problem, we adopt a self-training approach\\cite{scudder1965probability}", "next_context": "."}], "importance_score": 1.0}, "mukherjee_uncertainty-aware_2020": {"bib_key": "mukherjee_uncertainty-aware_2020", "bib_title": "Uncertainty-aware {Self}-training for {Few}-shot {Text} {Classification}", "bib_author ": "Mukherjee, Subhabrata", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Data Construction", "subsection": "Quality Evaluation", "subsubsection": "BERTEval", "prev_context": "W^t + 1\\cite{mukherjee_uncertainty-aware_2020}", "next_context": "."}], "importance_score": 1.0}, "gururangan_whose_2022": {"bib_key": "gururangan_whose_2022", "bib_title": "Whose {Language} {Counts} as {High} {Quality}? {Measuring} {Language} {Ideologies} in {Text} {Data} {Selection}", "bib_author ": "Gururangan, Suchin", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Data Construction", "subsection": "Quality Evaluation", "subsubsection": "Evaluation Model Comparison", "prev_context": "(2022)\\cite{gururangan_whose_2022}", "next_context": ", we combine logistic regression with a word frequency-based vertorizaiton method to conduct text classification on the testset."}], "importance_score": 1.0}, "2021WuDaoCorpora": {"bib_key": "2021WuDaoCorpora", "bib_title": "WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models", "bib_author ": "Sha Yuan", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Data Construction", "subsection": "Quality Control", "subsubsection": null, "prev_context": "WuDaoCorpora\\cite{2021WuDaoCorpora}", "next_context": ""}, {"section": "Data Construction", "subsection": "Dataset Statistics and Comparison", "subsubsection": null, "prev_context": "Specifically, C4\\cite{2020T5C4}, The Pile\\cite{2020_pile}and REFINEDWEB\\cite{2023refinedweb}are three public English datasets, while WuDaoCorpora\\cite{2021WuDaoCorpora}", "next_context": ", ROOTS-zh\\cite{2023roots}and WanJuan1.0-zh\\cite{2023wanjuan}are three corpora for Chinese."}], "importance_score": 2.0}, "2023roots": {"bib_key": "2023roots", "bib_title": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset", "bib_author ": "Hugo Lauren\u00e7on", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Data Construction", "subsection": "Quality Control", "subsubsection": null, "prev_context": "ROOTS-zh\\cite{2023roots}", "next_context": ""}, {"section": "Data Construction", "subsection": "Dataset Statistics and Comparison", "subsubsection": null, "prev_context": "Specifically, C4\\cite{2020T5C4}, The Pile\\cite{2020_pile}and REFINEDWEB\\cite{2023refinedweb}are three public English datasets, while WuDaoCorpora\\cite{2021WuDaoCorpora}, ROOTS-zh\\cite{2023roots}", "next_context": "and WanJuan1.0-zh\\cite{2023wanjuan}are three corpora for Chinese."}], "importance_score": 2.0}}, "refs": [], "table": [{"original": "\\begin{table}[htbp]\n    \\caption{ Examples for different filtering rules.}\\label{Data Filtering Statistics}\n    \\centering\n    \\begin{tabular}{p{4cm} | p{9cm} }\n    \\toprule\n     Filtering Operation & Example \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Text Extraction}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{gbsn} \\{\"url\": \"http://sarahokane.com/ywly/index.aspx\",\\newline \"date\\_download\": \"2022-05-28T10:17:39Z\", \\newline \"length\": 854, \\newline \"nlines\": 17, \\newline \"source\\_domain\": \"sarahokane.com\", \\newline \"title\": \"\u5408\u80a5\u5e02\u5efa\u8bbe\u6295\u8d44\u63a7\u80a1(\u96c6\u56e2)\u6709\u9650\u516c\u53f8\", \\newline \\textcolor{red}{\"raw\\_content\": \" \u4e61\u6751\u632f\u5174\u548c\u73b0\u4ee3\u519c\u4e1a\u677f\u5757\\textbackslash n\u4e61\u6751\u632f\u5174\u4e0e\u73b0\u4ee3\u519c\u4e1a...\u6ce8\u518c\u8d44\u672c4.39\u4ebf\u5143\u3002\",} \\newline ...\\newline \"language\": \"zh\", \\newline \"bucket\": \"head\"\\} \\end{CJK}}  \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Length less than 200}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{gbsn} \"\u5fb7\u56fd\u9ed1\u68ee\u5dde\u6cd5\u5170\u514b\u798fAMazon\u6570\u636e\u4e2d\u5fc3\" \\end{CJK}}  \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Average line length less than 10}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{gbsn} \"\u6c7d\u8f66\u8d44\u8baf\\textbackslash n\u6c7d\u8f66\u5236\u9020\u5546\\textbackslash n\u5b66\u8f66\u79df\u8f66\\textbackslash n\u4ff1\u4e50\u90e8,\u6c7d\u8f66\u7f51,\u6c7d\u8f66\u62a5\u4ef7,\u65b0\u8f66,\u6c7d\u8f66\u56fe\u7247\\textbackslash n\u6c7d\u8f66\u7f51,\u6c7d\u8f66\u62a5\u4ef7,\u65b0\u80fd\u6e90,\u65b0\u8f66,\u6c7d\u8f66\u56fe\u7247\\textbackslash n\u5357\u65b9\u7f51\uff1a\u6c7d\u8f66\u9891\u9053\\textbackslash n\u7ef4\u4fee,\u6539\u88c5,\u8f66\u6a21,\u65b0\u8f66,\u7528\u8f66\\textbackslash n ......\" \\end{CJK}}  \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Traditional Chinese characters}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{bsmi} \"\u6709\u770b\u904e\u88dd\u6f62\u4e2d\u7684\u4fbf\u5229\u5546\u5e97\u55ce\uff1f\u5b83\u53ef\u80fd\u5167\u90e8\u9084\u6c92\u6574\u5099\u597d\uff0c\u751a\u81f3\u6839\u672c\u662f\u7a7a\u76ea\u76ea\u7684\u4e00\u7247\u3002\u4f46\u5e97\u9580\u4e00\u5b9a\u6703\u4e0a\u7d05\u5e03\u689d\uff0c\u5beb\u8457\u300cXXX\u4fbf\u5229\u5546\u5e97\u5728\u6b64\u70ba\u60a8\u670d\u52d9\u300d\u3002\u53bb\u904e\u5916\u7e23\u5e02\u65c5\u904a\u55ce\uff1f\u7576\u4f60\u958b\u8457\u8eca\u8981\u627e\u67d0\u5bb6\u5e97\u6642\uff0c\u4f60\u5927\u6982\u4e0d\u6703\u958b\u5230\u6b63\u9580\u53e3\u624d\u770b\u662f\u4e0d\u662f\u4f60\u8981\u627e\u7684\u5e97\u3002\" \\end{CJK}}  \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Proportion of Chinese characters fewer than 30\\%}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{gbsn} \"\u7ebf\u4e0a\u4e70\u7403\u5e73\u53f0\\textbackslash u0006\\textbackslash u0007\u3001\u7cbe\u51c6\\textbackslash u0007\\textbackslash u0005\u3001\u53ef\u9760\u7684\u4f20\u611f\u6280\u672f\u89e3\u51b3\u65b9\u6848\u53ca\u4ea7\u54c1\\textbackslash n\\textbackslash nXSENS MTi 600\u7cfb\u5217 ...\\textbackslash n\\textbackslash n7 GNSS/INS\\textbackslash n\u7ebf\u4e0a\u4e70\u7403\u5e73\u53f0\\textbackslash u0006\uff0c\u5177\u6709\u591a\u4e2aGN......</p>\\textbackslash nMTi-G-710 GNSS/I...\\textbackslash n<p><span style=\"font-size: 12px;\">......\" \\end{CJK}}  \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Occurrence of sensitive words more than 0.5 per line}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{gbsn} \"\u5b9d\u535a\u4f53\u80b2\u5f3a\u5316\u521b\u65b0\u5f15\u9886\uff0c\u575a\u6301\u201c\u79d1\u6280\u5b9d\u535a\u4f53\u80b2\u201d\u6218\u7565\uff0c\u6784\u5efa\u201c\u4ee5\u4f01\u4e1a\u4e3a\u4e3b\u4f53\u3001\u5e02\u573a\u4e3a\u5bfc\u5411\u3001\u4ea7\u5b66\u7814\u76f8\u7ed3\u5408\u201d\u7684\u79d1\u6280\u521b\u65b0\u4f53\u7cfb\u3002\\textbackslash n2019\u5e74\u5ea6\u5927\u4e8b\u8bb0\\textbackslash n\u53cb\u60c5\u94fe\u63a5\uff1a\u73a9\u7403\u76f4\u64adnba \u771f\u94b1\u6eda\u7403\u771f\u4eba \u91d1\u82b1\u4e09\u5f20\u724c\u8d62\u94b1 55\u76f4\u64adnba \u4e70\u7403 \u624b\u673a\u8f6e\u76d8app \u4e9a\u535aapp\u5728\u7ebf\u767b\u5f55 \u4e9a\u535aapp\u767b\u5f55......\" \\end{CJK}}  \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Internal duplication ratio greater than 50\\%}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{gbsn} \"\\textcolor{red}{\u201c\u5c71\u4e1c\u7701\u6c11\u95f4\u878d\u8d44\u673a\u6784\u5ba3\u4f20\u6708\u6d3b\u52a8\u201d......\u6d3b\u52a8\u671f\u95f4\u5438\u5f15\u4e86\u5f53\u5730\u5e02\u6c11\\textbackslash{}n}2018\u5e7411\u67089\u65e5\uff0c\\textcolor{red}{\u201c\u5c71\u4e1c\u7701\u6c11\u95f4\u878d\u8d44\u673a\u6784\u5ba3\u4f20\u6708\u6d3b\u52a8\u201d......\u6d3b\u52a8\u671f\u95f4\u5438\u5f15\u4e86\u5f53\u5730\u5e02\u6c11}\u7684\u5e7f\u6cdb\u5173\u6ce8\uff0c\u73b0\u573a\u53d1\u653e\u5ba3\u4f20\u8d44\u6599600\u4f59\u4efd\uff0c\u89e3\u7b54\u5e02\u6c11\u7591\u95ee100\u4f59\u4eba\u6b21\uff0c\u5fb7\u5dde\u5e7f\u64ad\u7535\u89c6\u53f0\uff0c\u5fb7\u5dde\u65e5\u62a5\uff0c\u5fb7\u5dde\u665a\u62a5\u5168\u7a0b\u91c7\u8bbf\u62a5\u9053\u3002\" \\end{CJK}} \\\\\n     \\bottomrule\n    \\end{tabular}\n\\end{table}", "caption": "\\caption{ Examples for different filtering rules.}", "label": "\\label{Data Filtering Statistics}", "tabular": "\\begin{tabular}{p{4cm} | p{9cm} }\n    \\toprule\n     Filtering Operation & Example \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Text Extraction}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{gbsn} \\{\"url\": \"http://sarahokane.com/ywly/index.aspx\",\\newline \"date\\_download\": \"2022-05-28T10:17:39Z\", \\newline \"length\": 854, \\newline \"nlines\": 17, \\newline \"source\\_domain\": \"sarahokane.com\", \\newline \"title\": \"\u5408\u80a5\u5e02\u5efa\u8bbe\u6295\u8d44\u63a7\u80a1(\u96c6\u56e2)\u6709\u9650\u516c\u53f8\", \\newline \\textcolor{red}{\"raw\\_content\": \" \u4e61\u6751\u632f\u5174\u548c\u73b0\u4ee3\u519c\u4e1a\u677f\u5757\\textbackslash n\u4e61\u6751\u632f\u5174\u4e0e\u73b0\u4ee3\u519c\u4e1a...\u6ce8\u518c\u8d44\u672c4.39\u4ebf\u5143\u3002\",} \\newline ...\\newline \"language\": \"zh\", \\newline \"bucket\": \"head\"\\} \\end{CJK}}  \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Length less than 200}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{gbsn} \"\u5fb7\u56fd\u9ed1\u68ee\u5dde\u6cd5\u5170\u514b\u798fAMazon\u6570\u636e\u4e2d\u5fc3\" \\end{CJK}}  \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Average line length less than 10}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{gbsn} \"\u6c7d\u8f66\u8d44\u8baf\\textbackslash n\u6c7d\u8f66\u5236\u9020\u5546\\textbackslash n\u5b66\u8f66\u79df\u8f66\\textbackslash n\u4ff1\u4e50\u90e8,\u6c7d\u8f66\u7f51,\u6c7d\u8f66\u62a5\u4ef7,\u65b0\u8f66,\u6c7d\u8f66\u56fe\u7247\\textbackslash n\u6c7d\u8f66\u7f51,\u6c7d\u8f66\u62a5\u4ef7,\u65b0\u80fd\u6e90,\u65b0\u8f66,\u6c7d\u8f66\u56fe\u7247\\textbackslash n\u5357\u65b9\u7f51\uff1a\u6c7d\u8f66\u9891\u9053\\textbackslash n\u7ef4\u4fee,\u6539\u88c5,\u8f66\u6a21,\u65b0\u8f66,\u7528\u8f66\\textbackslash n ......\" \\end{CJK}}  \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Traditional Chinese characters}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{bsmi} \"\u6709\u770b\u904e\u88dd\u6f62\u4e2d\u7684\u4fbf\u5229\u5546\u5e97\u55ce\uff1f\u5b83\u53ef\u80fd\u5167\u90e8\u9084\u6c92\u6574\u5099\u597d\uff0c\u751a\u81f3\u6839\u672c\u662f\u7a7a\u76ea\u76ea\u7684\u4e00\u7247\u3002\u4f46\u5e97\u9580\u4e00\u5b9a\u6703\u4e0a\u7d05\u5e03\u689d\uff0c\u5beb\u8457\u300cXXX\u4fbf\u5229\u5546\u5e97\u5728\u6b64\u70ba\u60a8\u670d\u52d9\u300d\u3002\u53bb\u904e\u5916\u7e23\u5e02\u65c5\u904a\u55ce\uff1f\u7576\u4f60\u958b\u8457\u8eca\u8981\u627e\u67d0\u5bb6\u5e97\u6642\uff0c\u4f60\u5927\u6982\u4e0d\u6703\u958b\u5230\u6b63\u9580\u53e3\u624d\u770b\u662f\u4e0d\u662f\u4f60\u8981\u627e\u7684\u5e97\u3002\" \\end{CJK}}  \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Proportion of Chinese characters fewer than 30\\%}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{gbsn} \"\u7ebf\u4e0a\u4e70\u7403\u5e73\u53f0\\textbackslash u0006\\textbackslash u0007\u3001\u7cbe\u51c6\\textbackslash u0007\\textbackslash u0005\u3001\u53ef\u9760\u7684\u4f20\u611f\u6280\u672f\u89e3\u51b3\u65b9\u6848\u53ca\u4ea7\u54c1\\textbackslash n\\textbackslash nXSENS MTi 600\u7cfb\u5217 ...\\textbackslash n\\textbackslash n7 GNSS/INS\\textbackslash n\u7ebf\u4e0a\u4e70\u7403\u5e73\u53f0\\textbackslash u0006\uff0c\u5177\u6709\u591a\u4e2aGN......</p>\\textbackslash nMTi-G-710 GNSS/I...\\textbackslash n<p><span style=\"font-size: 12px;\">......\" \\end{CJK}}  \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Occurrence of sensitive words more than 0.5 per line}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{gbsn} \"\u5b9d\u535a\u4f53\u80b2\u5f3a\u5316\u521b\u65b0\u5f15\u9886\uff0c\u575a\u6301\u201c\u79d1\u6280\u5b9d\u535a\u4f53\u80b2\u201d\u6218\u7565\uff0c\u6784\u5efa\u201c\u4ee5\u4f01\u4e1a\u4e3a\u4e3b\u4f53\u3001\u5e02\u573a\u4e3a\u5bfc\u5411\u3001\u4ea7\u5b66\u7814\u76f8\u7ed3\u5408\u201d\u7684\u79d1\u6280\u521b\u65b0\u4f53\u7cfb\u3002\\textbackslash n2019\u5e74\u5ea6\u5927\u4e8b\u8bb0\\textbackslash n\u53cb\u60c5\u94fe\u63a5\uff1a\u73a9\u7403\u76f4\u64adnba \u771f\u94b1\u6eda\u7403\u771f\u4eba \u91d1\u82b1\u4e09\u5f20\u724c\u8d62\u94b1 55\u76f4\u64adnba \u4e70\u7403 \u624b\u673a\u8f6e\u76d8app \u4e9a\u535aapp\u5728\u7ebf\u767b\u5f55 \u4e9a\u535aapp\u767b\u5f55......\" \\end{CJK}}  \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Internal duplication ratio greater than 50\\%}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{gbsn} \"\\textcolor{red}{\u201c\u5c71\u4e1c\u7701\u6c11\u95f4\u878d\u8d44\u673a\u6784\u5ba3\u4f20\u6708\u6d3b\u52a8\u201d......\u6d3b\u52a8\u671f\u95f4\u5438\u5f15\u4e86\u5f53\u5730\u5e02\u6c11\\textbackslash{}n}2018\u5e7411\u67089\u65e5\uff0c\\textcolor{red}{\u201c\u5c71\u4e1c\u7701\u6c11\u95f4\u878d\u8d44\u673a\u6784\u5ba3\u4f20\u6708\u6d3b\u52a8\u201d......\u6d3b\u52a8\u671f\u95f4\u5438\u5f15\u4e86\u5f53\u5730\u5e02\u6c11}\u7684\u5e7f\u6cdb\u5173\u6ce8\uff0c\u73b0\u573a\u53d1\u653e\u5ba3\u4f20\u8d44\u6599600\u4f59\u4efd\uff0c\u89e3\u7b54\u5e02\u6c11\u7591\u95ee100\u4f59\u4eba\u6b21\uff0c\u5fb7\u5dde\u5e7f\u64ad\u7535\u89c6\u53f0\uff0c\u5fb7\u5dde\u65e5\u62a5\uff0c\u5fb7\u5dde\u665a\u62a5\u5168\u7a0b\u91c7\u8bbf\u62a5\u9053\u3002\" \\end{CJK}} \\\\\n     \\bottomrule\n    \\end{tabular}", "subtables": []}, {"original": "\\begin{table}[htbp]\n\t\\caption{Composition of BERTEval Training Data.}\\label{bert_data}\n\t\\centering\n\t\\begin{tabular}{llc}\n\t\t\\toprule\n\t\tType     & Source     & Size ($\\times 10^4 $) \\\\\n\t\t\\midrule\n\t\t\\multirow{8}{*}{Positive samples}   & Wikipedia & 12.50 \\\\ \n                                            & Sina News & 12.50 \\\\\n                                    \t\t& Cbooks & 12.50  \\\\\n\t                                        & Zhihu & 12.40 \\\\\n\t                                        & WikiQA & 0.90 \\\\\n\t                                        & Law & 0.40 \\\\\n\t                                        & Poetry & 0.20 \\\\\n\t                                        & GovReport & 0.13 \\\\\n\t    \\midrule\n            \\xrowht[()]{10pt}\n\t\tNegative sample & CC-sampling & 55.00 \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}", "caption": "\\caption{Composition of BERTEval Training Data.}", "label": "\\label{bert_data}", "tabular": "\\begin{tabular}{llc}\n\t\t\\toprule\n\t\tType     & Source     & Size ($\\times 10^4 $) \\\\\n\t\t\\midrule\n\t\t\\multirow{8}{*}{Positive samples}   & Wikipedia & 12.50 \\\\ \n                                            & Sina News & 12.50 \\\\\n                                    \t\t& Cbooks & 12.50  \\\\\n\t                                        & Zhihu & 12.40 \\\\\n\t                                        & WikiQA & 0.90 \\\\\n\t                                        & Law & 0.40 \\\\\n\t                                        & Poetry & 0.20 \\\\\n\t                                        & GovReport & 0.13 \\\\\n\t    \\midrule\n            \\xrowht[()]{10pt}\n\t\tNegative sample & CC-sampling & 55.00 \\\\\n\t\t\\bottomrule\n\t\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[htbp]\n\t\\caption{Composition of FastText Training Data.}\\label{fasttext_data}\n\t\\centering\n\t\\begin{tabular}{llc}\n\t\t\\toprule\n\t\tType     & Source     & Size ($\\times 10^4 $) \\\\\n\t\t\\midrule\n\t\t\\multirow{8}{*}{Positive samples}   & Baike & 20 \\\\ \n                                            & Cbook & 20 \\\\\n                                    \t\t& Zhidao& 20  \\\\\n\t\t                                    & China News & 20 \\\\\n\t                                        & Zhihu & 20 \\\\\n\t                                        & WikiQA & 10 \\\\\n\t                                        & other news & 10 \\\\\n\t                                        & BERT-positive & 40 \\\\\n\t    \\midrule\n            \\xrowht[()]{10pt}\n\t\tNegative sample & BERT-negative & 160 \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}", "caption": "\\caption{Composition of FastText Training Data.}", "label": "\\label{fasttext_data}", "tabular": "\\begin{tabular}{llc}\n\t\t\\toprule\n\t\tType     & Source     & Size ($\\times 10^4 $) \\\\\n\t\t\\midrule\n\t\t\\multirow{8}{*}{Positive samples}   & Baike & 20 \\\\ \n                                            & Cbook & 20 \\\\\n                                    \t\t& Zhidao& 20  \\\\\n\t\t                                    & China News & 20 \\\\\n\t                                        & Zhihu & 20 \\\\\n\t                                        & WikiQA & 10 \\\\\n\t                                        & other news & 10 \\\\\n\t                                        & BERT-positive & 40 \\\\\n\t    \\midrule\n            \\xrowht[()]{10pt}\n\t\tNegative sample & BERT-negative & 160 \\\\\n\t\t\\bottomrule\n\t\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[htbp]\n \t\\caption{Classification results of different evaluation models.}\\label{results}\n \t\\centering\n \t\\begin{tabular}{lccc}\n \t\t\\toprule\n \t\tModel      & Precision(\\%) & TP+FP & TP \\\\\n \t\t\\midrule\\xrowht[()]{10pt}\n \t\tRegression & 49.57 & 234 & 116 \\\\\n \t\t\\xrowht[()]{10pt}\n \t\tPerplexity &  63.27 & 245 & 155 \\\\\n \t\t \\xrowht[()]{10pt}\n          BERTEval & 73.79 & 103 & 76\\\\\n \t\t\\xrowht[()]{10pt}\n        FastText&  \\textbf{81.58} & 76 & 62 \\\\\t  \n \t\t\\bottomrule\n \t\\end{tabular}\n \\end{table}", "caption": "\\caption{Classification results of different evaluation models.}", "label": "\\label{results}", "tabular": "\\begin{tabular}{lccc}\n \t\t\\toprule\n \t\tModel      & Precision(\\%) & TP+FP & TP \\\\\n \t\t\\midrule\\xrowht[()]{10pt}\n \t\tRegression & 49.57 & 234 & 116 \\\\\n \t\t\\xrowht[()]{10pt}\n \t\tPerplexity &  63.27 & 245 & 155 \\\\\n \t\t \\xrowht[()]{10pt}\n          BERTEval & 73.79 & 103 & 76\\\\\n \t\t\\xrowht[()]{10pt}\n        FastText&  \\textbf{81.58} & 76 & 62 \\\\\t  \n \t\t\\bottomrule\n \t\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[htbp]\n    \\caption{Data samples in json format. The higher score of Sample 1 versus the lower score of Sample 2 demonstrates their differing text qualities, with Sample 1 having better quality than Sample 2. }\n    \\label{Data Example Format}\n    \\centering\n    \\begin{tabular}{p{2.5cm} p{12cm} }\n    \\toprule\n      \\textbf{Key} & \\textbf{Value}\\\\\n     \\midrule\n        <\\textbf{title}> &\\begin{CJK}{UTF8}{gbsn}\"\u6f4d\u574a\u94f6\u884c2021\u5e74\u4e0a\u534a\u5e74\u51c0\u5229\u6da6\u540c\u6bd4\u589e\u957f29.57\\% \u4e0d\u826f\u7387\u964d\u81f31.10\\%\\_\u8d22\u7ecf\\_\u4e2d\u56fd\u7f51\"\\end{CJK}\\\\\n        <\\textbf{score}>& 0.95 \\\\\n        <\\textbf{text}> & \\multicolumn{1}{m{12cm}}{\\begin{CJK}{UTF8}{gbsn} \"\u6f4d\u574a\u94f6\u884c2021\u5e74\u4e0a\u534a\u5e74\u51c0\u5229\u6da6\u540c\u6bd4\u589e\u957f29.57\\% \u4e0d\u826f\u7387\u964d\u81f31.10\\%\\textbackslash n\u4e2d\u56fd\u7f51\u8d22\u7ecf8\u670824\u65e5\u8baf \u6f4d\u574a\u94f6\u884c\u6628\u65e5\u62ab\u97322021\u5e74\u4e8c\u5b63\u5ea6\u4fe1\u606f\u62a5\u544a\u663e\u793a\uff0c\u622a\u81f32021\u5e746\u6708\u672b\uff0c\u6f4d\u574a\u94f6\u884c\u8d44\u4ea7\u603b\u989d1920.44\u4ebf\u5143\uff0c\u8f83\u4e0a\u5e74\u672b\u589e\u957f9.34\\%\uff1b\u8d1f\u503a\u603b\u989d1789.16\u4ebf\u5143\uff0c\u8f83\u4e0a\u5e74\u672b\u589e\u957f10.54\\%\u30022021\u5e74\u4e0a\u534a\u5e74\uff0c\u6f4d\u574a\u94f6\u884c\u5b9e\u73b0\u51c0\u5229\u6da66.09\u4ebf\u5143\uff0c\u540c\u6bd4\u589e\u957f29.57\\%\u3002\\textbackslash n\u8d44\u4ea7\u8d28\u91cf\u65b9\u9762\uff0c\u622a\u81f32021\u5e746\u6708\u672b\uff0c\u6f4d\u574a\u94f6\u884c\u4e0d\u826f\u8d37\u6b3e\u73871.10\\%\uff0c\u8f83\u4e0a\u5e74\u672b\u4e0b\u964d0.13\u4e2a\u767e\u5206\u70b9\u3002\\textbackslash n\u8d44\u672c\u91d1\u65b9\u9762\uff0c\u622a\u81f32021\u5e746\u6708\u672b\uff0c\u6f4d\u574a\u94f6\u884c\u8d44\u672c\u5145\u8db3\u7387\u3001\u6838\u5fc3\u4e00\u7ea7\u8d44\u672c\u5145\u8db3\u7387\u3001\u4e00\u7ea7\u8d44\u672c\u5145\u8db3\u7387\u5206\u522b\u4e3a11.66\\%\u30017.89\\%\u300110.13\\%\uff0c\u5206\u522b\u8f83\u4e0a\u5e74\u672b\u4e0b\u964d1.89\u30010.89\u30011.15\u4e2a\u767e\u5206\u70b9\u3002\" \\end{CJK}}  \\\\\n        <\\textbf{url}> & \\url{http://finance.china.com.cn/news/special/2021bnb/20210824/5638343.shtml}\\\\\n        <\\textbf{source\\_domain}> & \\url{finance.china.com.cn}\\\\\n         \\midrule\n        <\\textbf{title}> &\\begin{CJK}{UTF8}{gbsn}\"\u4e0a\u6d77\u5de8\u4e5f\u4eea\u5668\u8bbe\u5907\u6709\u9650\u516c\u53f8\"\\end{CJK}\\\\\n        <\\textbf{score}>& 0.19 \\\\\n        <\\textbf{text}> & \\multicolumn{1}{m{12cm}}{\\begin{CJK}{UTF8}{gbsn} \"\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u4eea\\textbackslash n\u73b0\u8d27\u63d0\u4f9b\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\\textbackslash n\u73b0\u8d27\u63d0\u4f9b\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\u7b26\u5408SAE\u3001ASTM\u3001VDA\u3001GM\u3001Ford\u3001Mazda\u3001JIS\u3001Nissan\u3001\u53caToyota\u7b49\u7684\u6d4b\u8bd5\u8981\u6c42\u3002\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\uff08\u6b22\u8fce\u5b9e\u5730\u8003\u5bdf\uff09\u6ee1\u8db3\uff1a\u5927\u4f17\u3001\u795e\u9f99\u3001\u901a\u7528\u3001\u65e5\u4ea7\u3001\u9a6c\u81ea\u8fbe\u3001\u4e30\u7530\u3001\u672c\u7530\u3001\u798f\u7279\u7b49\u6c7d\u8f66\u5382\u5bb6\u8bd5\u9a8c\u3002\\textbackslash nNS\u8010\u77f3\u5b50\u51b2\u51fb\u6027\u80fd\u8bd5\u9a8c\u673a\\textbackslash n\u8010\u77f3\u5b50\u51b2\u51fb\u6027\u80fd\u8bd5\u9a8c\u673a\u4e3b\u8981\u7528\u4e8e\u3001\u5fb7\u7cfb\u65e5\u7cfb\u3001\u7f8e\u7cfb\u6c7d\u8f66\u5382\u5bb6\u8bd5\u9a8c\u65b9\u6cd5\uff0c\u5b83\u80fd\u51c6\u786e\u518d\u73b0\u7531\u98de\u6e85\u7684\u7802\u783e\u9020\u6210\u7684\u7834\u5316\u73b0\u8c61, \u9002\u7528\u4e8e\u5916\u6d82\u5c42\u7c98\u805a\u6027\u7834\u574f\u8bd5\u9a8c\u3001\u6d82\u5c42\u7cfb\u7edf\u4e2d\u4e0d\u540c\u5c42\u95f4\u7c98\u5408\u6027\u7834\u574f\u8bd5\u9a8c\u3001\u6297\u5265\u843d\u7684*\u6d82\u819c\u539a\u5ea6\u3001\u5851\u6599\u53ca\u73bb\u7483\u7684\u6297\u5265\u843d\u3001\u6297\u78b0\u649e\u3001\u6297\u78e8\u635f\u6d4b\u8bd5\u7b49\u76f8\u5173\u8bd5\u9a8c\u3002\\textbackslash n\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\uff08\u6b22\u8fce\u5b9e\u5730\u8003\u5bdf\uff09\\textbackslash n\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\uff08\u6b22\u8fce\u5b9e\u5730\u8003\u5bdf\uff09\u7b26\u5408SAE\u3001ASTM\u3001VDA\u3001GM\u3001Ford\u3001Mazda\u3001JIS\u3001Nissan\u3001\u53caToyota\u7b49\u7684\u6d4b\u8bd5\u8981\u6c42\u3002\u6ee1\u8db3\uff1a\u5927\u4f17\u3001\u795e\u9f99\u3001\u901a\u7528\u3001\u65e5\u4ea7\u3001\u9a6c\u81ea\u8fbe\u3001\u4e30\u7530\u3001\u672c\u7530\u3001\u798f\u7279\u7b49\u6c7d\u8f66\u5382\u5bb6\u8bd5\u9a8c\u3002\\textbackslash n\u6f06\u819c\u6297\u77f3\u51fb\u8bd5\u9a8c\u4eea\\textbackslash n\u6f06\u819c\u6297\u77f3\u51fb\u8bd5\u9a8c\u4eea\u7b26\u5408SAE\u3001ASTM\u3001VDA\u3001GM\u3001Ford\u3001Mazda\u3001JIS\u3001Nissan\u3001\u53caToyota\u7b49\u7684\u6d4b\u8bd5\u8981\u6c42\u3002\\textbackslash n\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a/\u6297\u77f3\u5b50\u51b2\u51fb\u4eea\\textbackslash n\u5de8\u4e5f\u4eea\u5668\uff01\u6709\u5927\u91cf\u73b0\u8d27\u63d0\u4f9b\uff0c\u6b22\u8fce\u5ba2\u6237\u968f\u65f6\u6765\u5382\u53c2\u89c2\u4e0e\u6307\u5bfc\uff01\" \\end{CJK}}  \\\\\n        <\\textbf{url}> & \\url{http://www.juyesh.com/SonList-1094890.html}\\\\\n        <\\textbf{source\\_domain}> & \\url{www.juyesh.com}\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}", "caption": "\\caption{Data samples in json format. The higher score of Sample 1 versus the lower score of Sample 2 demonstrates their differing text qualities, with Sample 1 having better quality than Sample 2. }", "label": "\\label{Data Example Format}", "tabular": "\\begin{tabular}{p{2.5cm} p{12cm} }\n    \\toprule\n      \\textbf{Key} & \\textbf{Value}\\\\\n     \\midrule\n        <\\textbf{title}> &\\begin{CJK}{UTF8}{gbsn}\"\u6f4d\u574a\u94f6\u884c2021\u5e74\u4e0a\u534a\u5e74\u51c0\u5229\u6da6\u540c\u6bd4\u589e\u957f29.57\\% \u4e0d\u826f\u7387\u964d\u81f31.10\\%\\_\u8d22\u7ecf\\_\u4e2d\u56fd\u7f51\"\\end{CJK}\\\\\n        <\\textbf{score}>& 0.95 \\\\\n        <\\textbf{text}> & \\multicolumn{1}{m{12cm}}{\\begin{CJK}{UTF8}{gbsn} \"\u6f4d\u574a\u94f6\u884c2021\u5e74\u4e0a\u534a\u5e74\u51c0\u5229\u6da6\u540c\u6bd4\u589e\u957f29.57\\% \u4e0d\u826f\u7387\u964d\u81f31.10\\%\\textbackslash n\u4e2d\u56fd\u7f51\u8d22\u7ecf8\u670824\u65e5\u8baf \u6f4d\u574a\u94f6\u884c\u6628\u65e5\u62ab\u97322021\u5e74\u4e8c\u5b63\u5ea6\u4fe1\u606f\u62a5\u544a\u663e\u793a\uff0c\u622a\u81f32021\u5e746\u6708\u672b\uff0c\u6f4d\u574a\u94f6\u884c\u8d44\u4ea7\u603b\u989d1920.44\u4ebf\u5143\uff0c\u8f83\u4e0a\u5e74\u672b\u589e\u957f9.34\\%\uff1b\u8d1f\u503a\u603b\u989d1789.16\u4ebf\u5143\uff0c\u8f83\u4e0a\u5e74\u672b\u589e\u957f10.54\\%\u30022021\u5e74\u4e0a\u534a\u5e74\uff0c\u6f4d\u574a\u94f6\u884c\u5b9e\u73b0\u51c0\u5229\u6da66.09\u4ebf\u5143\uff0c\u540c\u6bd4\u589e\u957f29.57\\%\u3002\\textbackslash n\u8d44\u4ea7\u8d28\u91cf\u65b9\u9762\uff0c\u622a\u81f32021\u5e746\u6708\u672b\uff0c\u6f4d\u574a\u94f6\u884c\u4e0d\u826f\u8d37\u6b3e\u73871.10\\%\uff0c\u8f83\u4e0a\u5e74\u672b\u4e0b\u964d0.13\u4e2a\u767e\u5206\u70b9\u3002\\textbackslash n\u8d44\u672c\u91d1\u65b9\u9762\uff0c\u622a\u81f32021\u5e746\u6708\u672b\uff0c\u6f4d\u574a\u94f6\u884c\u8d44\u672c\u5145\u8db3\u7387\u3001\u6838\u5fc3\u4e00\u7ea7\u8d44\u672c\u5145\u8db3\u7387\u3001\u4e00\u7ea7\u8d44\u672c\u5145\u8db3\u7387\u5206\u522b\u4e3a11.66\\%\u30017.89\\%\u300110.13\\%\uff0c\u5206\u522b\u8f83\u4e0a\u5e74\u672b\u4e0b\u964d1.89\u30010.89\u30011.15\u4e2a\u767e\u5206\u70b9\u3002\" \\end{CJK}}  \\\\\n        <\\textbf{url}> & \\url{http://finance.china.com.cn/news/special/2021bnb/20210824/5638343.shtml}\\\\\n        <\\textbf{source\\_domain}> & \\url{finance.china.com.cn}\\\\\n         \\midrule\n        <\\textbf{title}> &\\begin{CJK}{UTF8}{gbsn}\"\u4e0a\u6d77\u5de8\u4e5f\u4eea\u5668\u8bbe\u5907\u6709\u9650\u516c\u53f8\"\\end{CJK}\\\\\n        <\\textbf{score}>& 0.19 \\\\\n        <\\textbf{text}> & \\multicolumn{1}{m{12cm}}{\\begin{CJK}{UTF8}{gbsn} \"\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u4eea\\textbackslash n\u73b0\u8d27\u63d0\u4f9b\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\\textbackslash n\u73b0\u8d27\u63d0\u4f9b\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\u7b26\u5408SAE\u3001ASTM\u3001VDA\u3001GM\u3001Ford\u3001Mazda\u3001JIS\u3001Nissan\u3001\u53caToyota\u7b49\u7684\u6d4b\u8bd5\u8981\u6c42\u3002\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\uff08\u6b22\u8fce\u5b9e\u5730\u8003\u5bdf\uff09\u6ee1\u8db3\uff1a\u5927\u4f17\u3001\u795e\u9f99\u3001\u901a\u7528\u3001\u65e5\u4ea7\u3001\u9a6c\u81ea\u8fbe\u3001\u4e30\u7530\u3001\u672c\u7530\u3001\u798f\u7279\u7b49\u6c7d\u8f66\u5382\u5bb6\u8bd5\u9a8c\u3002\\textbackslash nNS\u8010\u77f3\u5b50\u51b2\u51fb\u6027\u80fd\u8bd5\u9a8c\u673a\\textbackslash n\u8010\u77f3\u5b50\u51b2\u51fb\u6027\u80fd\u8bd5\u9a8c\u673a\u4e3b\u8981\u7528\u4e8e\u3001\u5fb7\u7cfb\u65e5\u7cfb\u3001\u7f8e\u7cfb\u6c7d\u8f66\u5382\u5bb6\u8bd5\u9a8c\u65b9\u6cd5\uff0c\u5b83\u80fd\u51c6\u786e\u518d\u73b0\u7531\u98de\u6e85\u7684\u7802\u783e\u9020\u6210\u7684\u7834\u5316\u73b0\u8c61, \u9002\u7528\u4e8e\u5916\u6d82\u5c42\u7c98\u805a\u6027\u7834\u574f\u8bd5\u9a8c\u3001\u6d82\u5c42\u7cfb\u7edf\u4e2d\u4e0d\u540c\u5c42\u95f4\u7c98\u5408\u6027\u7834\u574f\u8bd5\u9a8c\u3001\u6297\u5265\u843d\u7684*\u6d82\u819c\u539a\u5ea6\u3001\u5851\u6599\u53ca\u73bb\u7483\u7684\u6297\u5265\u843d\u3001\u6297\u78b0\u649e\u3001\u6297\u78e8\u635f\u6d4b\u8bd5\u7b49\u76f8\u5173\u8bd5\u9a8c\u3002\\textbackslash n\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\uff08\u6b22\u8fce\u5b9e\u5730\u8003\u5bdf\uff09\\textbackslash n\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\uff08\u6b22\u8fce\u5b9e\u5730\u8003\u5bdf\uff09\u7b26\u5408SAE\u3001ASTM\u3001VDA\u3001GM\u3001Ford\u3001Mazda\u3001JIS\u3001Nissan\u3001\u53caToyota\u7b49\u7684\u6d4b\u8bd5\u8981\u6c42\u3002\u6ee1\u8db3\uff1a\u5927\u4f17\u3001\u795e\u9f99\u3001\u901a\u7528\u3001\u65e5\u4ea7\u3001\u9a6c\u81ea\u8fbe\u3001\u4e30\u7530\u3001\u672c\u7530\u3001\u798f\u7279\u7b49\u6c7d\u8f66\u5382\u5bb6\u8bd5\u9a8c\u3002\\textbackslash n\u6f06\u819c\u6297\u77f3\u51fb\u8bd5\u9a8c\u4eea\\textbackslash n\u6f06\u819c\u6297\u77f3\u51fb\u8bd5\u9a8c\u4eea\u7b26\u5408SAE\u3001ASTM\u3001VDA\u3001GM\u3001Ford\u3001Mazda\u3001JIS\u3001Nissan\u3001\u53caToyota\u7b49\u7684\u6d4b\u8bd5\u8981\u6c42\u3002\\textbackslash n\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a/\u6297\u77f3\u5b50\u51b2\u51fb\u4eea\\textbackslash n\u5de8\u4e5f\u4eea\u5668\uff01\u6709\u5927\u91cf\u73b0\u8d27\u63d0\u4f9b\uff0c\u6b22\u8fce\u5ba2\u6237\u968f\u65f6\u6765\u5382\u53c2\u89c2\u4e0e\u6307\u5bfc\uff01\" \\end{CJK}}  \\\\\n        <\\textbf{url}> & \\url{http://www.juyesh.com/SonList-1094890.html}\\\\\n        <\\textbf{source\\_domain}> & \\url{www.juyesh.com}\\\\\n        \\bottomrule\n    \\end{tabular}", "subtables": []}, {"original": "\\begin{table}[htbp]\n    \\caption{Overview of output datasets.}\n    \\label{total_remain_size}\n    \\centering\n    \\begin{tabular}{cp{2cm}p{2cm}p{2cm}}\n    \\toprule\n    \\multirow{3.5}{*}{Snapshot}   & \\multicolumn{3}{c}{Data Size(GB)} \\\\\n                                \\cmidrule{2-4}\n                                & \\centering Monolingual Chinese Data & \\centering ChineseWebText Dataset & \\centering Cleaner Subset \\arraybackslash \\\\\n    \\midrule \n    \\xrowht[()]{5pt}\n    \\textbf{2021-43} & \\centering 505.92 & \\centering 187.57 & \\centering 78.95 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2022-05} & \\centering 442.47 & \\centering 164.96 & \\centering 69.44 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2022-21} & \\centering 443.57 & \\centering 166.75 & \\centering 70.19 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2022-27} & \\centering 417.95 & \\centering 149.41 & \\centering 62.70 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2022-33} & \\centering 369.56 & \\centering 123.70 & \\centering 51.98 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2022-49} & \\centering 445.29 & \\centering 160.87 & \\centering 67.76 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2023-06} & \\centering 396.40 & \\centering 173.47 & \\centering 74.19 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2023-14} & \\centering 441.46 & \\centering 150.04 & \\centering 63.33 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2023-23} & \\centering 371.96 & \\centering 143.93 & \\centering 61.28 \\arraybackslash\\\\\n    \\midrule\\xrowht[()]{5pt}\n    \\textbf{Total} & \\centering 3834.58 & \\centering 1420.70 & \\centering 599.82 \\arraybackslash\\\\\n    % \\midrule\n    \\bottomrule\n    \\end{tabular}\n\\end{table}", "caption": "\\caption{Overview of output datasets.}", "label": "\\label{total_remain_size}", "tabular": "\\begin{tabular}{cp{2cm}p{2cm}p{2cm}}\n    \\toprule\n    \\multirow{3.5}{*}{Snapshot}   & \\multicolumn{3}{c}{Data Size(GB)} \\\\\n                                \\cmidrule{2-4}\n                                & \\centering Monolingual Chinese Data & \\centering ChineseWebText Dataset & \\centering Cleaner Subset \\arraybackslash \\\\\n    \\midrule \n    \\xrowht[()]{5pt}\n    \\textbf{2021-43} & \\centering 505.92 & \\centering 187.57 & \\centering 78.95 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2022-05} & \\centering 442.47 & \\centering 164.96 & \\centering 69.44 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2022-21} & \\centering 443.57 & \\centering 166.75 & \\centering 70.19 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2022-27} & \\centering 417.95 & \\centering 149.41 & \\centering 62.70 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2022-33} & \\centering 369.56 & \\centering 123.70 & \\centering 51.98 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2022-49} & \\centering 445.29 & \\centering 160.87 & \\centering 67.76 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2023-06} & \\centering 396.40 & \\centering 173.47 & \\centering 74.19 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2023-14} & \\centering 441.46 & \\centering 150.04 & \\centering 63.33 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2023-23} & \\centering 371.96 & \\centering 143.93 & \\centering 61.28 \\arraybackslash\\\\\n    \\midrule\\xrowht[()]{5pt}\n    \\textbf{Total} & \\centering 3834.58 & \\centering 1420.70 & \\centering 599.82 \\arraybackslash\\\\\n    % \\midrule\n    \\bottomrule\n    \\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[htb]\n\\centering\n    %\\small\n     \\caption{ The comparison of different pre-training datasets.}\n    \\label{Dataset Compare}\n    \\begin{tabular}{ccccc}\n    \\toprule\n    \\textbf{Dataset} & \\textbf{Lang.} &\\textbf{Availability}& \\textbf{Pubilc Size} & \\textbf{Scoring} \\\\\n    \\midrule\n    C4\\cite{2020T5C4}   & EN & Public &  807GB  & NO   \\\\  \n    The Pile\\cite{2020_pile} & EN & Public & 825GB  &   NO \\\\ \n    REFINEDWEB\\cite{2023refinedweb} & EN & Public& 2.8TB    &  NO  \\\\\n    WuDaoCorpora\\cite{2021WuDaoCorpora}  & ZH & Pratly Public & 200GB  &  NO \\\\\n    ROOTS-zh\\cite{2023roots} & ZH & Public &  265GB   &  NO\\\\\n    WanJuan1.0-zh\\cite{2023wanjuan}   & ZH & Public & 550GB  & NO\\\\\n    \\textbf{ChineseWebText} (Ours)        & ZH & Public & 1.4 TB &  YES \\\\\n    \\textbf{Cleaner Subset} (Ours)        & ZH & Public & 600 GB &  YES \\\\\n    \\bottomrule\n    \\end{tabular}\n   \n\\end{table*}", "caption": "\\caption{ The comparison of different pre-training datasets.}", "label": "\\label{Dataset Compare}", "tabular": "\\begin{tabular}{ccccc}\n    \\toprule\n    \\textbf{Dataset} & \\textbf{Lang.} &\\textbf{Availability}& \\textbf{Pubilc Size} & \\textbf{Scoring} \\\\\n    \\midrule\n    C4\\cite{2020T5C4}   & EN & Public &  807GB  & NO   \\\\  \n    The Pile\\cite{2020_pile} & EN & Public & 825GB  &   NO \\\\ \n    REFINEDWEB\\cite{2023refinedweb} & EN & Public& 2.8TB    &  NO  \\\\\n    WuDaoCorpora\\cite{2021WuDaoCorpora}  & ZH & Pratly Public & 200GB  &  NO \\\\\n    ROOTS-zh\\cite{2023roots} & ZH & Public &  265GB   &  NO\\\\\n    WanJuan1.0-zh\\cite{2023wanjuan}   & ZH & Public & 550GB  & NO\\\\\n    \\textbf{ChineseWebText} (Ours)        & ZH & Public & 1.4 TB &  YES \\\\\n    \\textbf{Cleaner Subset} (Ours)        & ZH & Public & 600 GB &  YES \\\\\n    \\bottomrule\n    \\end{tabular}", "subtables": []}, {"original": "\\begin{table}[htbp]\n    \\small\n    \\caption{The remaining data size and filtering ratio for each preprocessing step and quality evaluation module.}\n    \\label{remain_size}\n    \\centering\n    \\begin{tabular}{cp{2cm}p{1.5cm}p{1.2cm}p{2cm}p{1.5cm}p{1.5cm}p{1.5cm}}\n    \\toprule\n    \\multirow{3.2}{*}{Snapshot} & \\multicolumn{7}{c}{Size After filtering operation(GB)} \\\\\n                                \\cmidrule{2-8}\n                                & \\centering Monolingual Chinese Data & \\centering Text Extraction & \\centering Data Length & \\centering Proportion of Characters & \\centering Sensitive Words & \\centering Internal Duplication  & \\centering Quality Evaluation \\arraybackslash \\\\\n    \\midrule\n    \\textbf{2021-43} & \\centering 505.92 & \\centering 424.43 & \\centering 409.68 & \\centering 217.52 & \\centering 192.84 & \\centering 187.57 & \\centering 78.95 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-16.11\\%}} & \\centering \\textcolor{gray}{\\textit{-3.48\\%}} & \\centering \\textcolor{gray}{\\textit{-46.90\\%}} & \\centering \\textcolor{gray}{\\textit{-11.35\\%}} & \\centering \\textcolor{gray}{\\textit{-2.73\\%}} & \\centering \\textcolor{gray}{\\textit{-57.91\\%}} \\arraybackslash \\\\\n    \\textbf{2022-05} & \\centering 442.47 & \\centering 375.64 & \\centering 362.34 & \\centering 182.88 & \\centering 169.01 & \\centering 164.96 & \\centering 69.44 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-15.10\\%}} & \\centering \\textcolor{gray}{\\textit{-3.54\\%}} & \\centering \\textcolor{gray}{\\textit{-49.53\\%}} & \\centering \\textcolor{gray}{\\textit{-7.58\\%}} & \\centering \\textcolor{gray}{\\textit{-2.40\\%}} & \\centering \\textcolor{gray}{\\textit{-57.90\\%}} \\arraybackslash \\\\\n    \\textbf{2022-21} & \\centering 443.57 & \\centering 363.33 & \\centering 348.51 & \\centering 178.16 & \\centering 170.09 & \\centering 166.75 & \\centering 70.19 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-18.09\\%}} & \\centering \\textcolor{gray}{\\textit{-4.08\\%}} & \\centering \\textcolor{gray}{\\textit{-48.88\\%}} & \\centering \\textcolor{gray}{\\textit{-4.53\\%}} & \\centering \\textcolor{gray}{\\textit{-1.96\\%}} & \\centering \\textcolor{gray}{\\textit{-57.91\\%}} \\arraybackslash \\\\\n    \\textbf{2022-27} & \\centering 417.95 & \\centering 340.65 & \\centering 326.52 & \\centering 158.83 & \\centering 152.33 & \\centering 149.41 & \\centering 62.7 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-18.50\\%}} & \\centering \\textcolor{gray}{\\textit{-4.15\\%}} & \\centering \\textcolor{gray}{\\textit{-51.36\\%}} & \\centering \\textcolor{gray}{\\textit{-4.09\\%}} & \\centering \\textcolor{gray}{\\textit{-1.92\\%}} & \\centering \\textcolor{gray}{\\textit{-58.03\\%}} \\arraybackslash \\\\\n    \\textbf{2022-33} & \\centering 369.56 & \\centering 293.07 & \\centering 280.58 & \\centering 131.39 & \\centering 125.84 & \\centering 123.70 & \\centering 51.98 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-20.70\\%}} & \\centering \\textcolor{gray}{\\textit{-4.26\\%}} & \\centering \\textcolor{gray}{\\textit{-53.17\\%}} & \\centering \\textcolor{gray}{\\textit{-4.22\\%}} & \\centering \\textcolor{gray}{\\textit{-1.70\\%}} & \\centering \\textcolor{gray}{\\textit{-57.98\\%}} \\arraybackslash \\\\\n    \\textbf{2022-49} & \\centering 445.29 & \\centering 367.73 & \\centering 352.59 & \\centering 173.86 & \\centering 164.34 & \\centering 160.87 & \\centering 67.76 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-17.42\\%}} & \\centering \\textcolor{gray}{\\textit{-4.12\\%}} & \\centering \\textcolor{gray}{\\textit{-50.69\\%}} & \\centering \\textcolor{gray}{\\textit{-5.48\\%}} & \\centering \\textcolor{gray}{\\textit{-2.11\\%}} & \\centering \\textcolor{gray}{\\textit{-57.88\\%}} \\arraybackslash \\\\\n    \\textbf{2023-06} & \\centering 396.40 & \\centering 275.04 & \\centering 263.59 & \\centering 211.10 & \\centering 177.44 & \\centering 173.47 & \\centering 74.19 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-30.62\\%}} & \\centering \\textcolor{gray}{\\textit{-4.16\\%}} & \\centering \\textcolor{gray}{\\textit{-19.91\\%}} & \\centering \\textcolor{gray}{\\textit{-15.95\\%}} & \\centering \\textcolor{gray}{\\textit{-2.24\\%}} & \\centering \\textcolor{gray}{\\textit{-57.23\\%}} \\arraybackslash \\\\\n    \\textbf{2023-14} & \\centering 441.46 & \\centering 368.40 & \\centering 354.18 & \\centering 161.54 & \\centering 153.27 & \\centering 150.04 & \\centering 63.33 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-16.55\\%}} & \\centering \\textcolor{gray}{\\textit{-3.86\\%}} & \\centering \\textcolor{gray}{\\textit{-54.39\\%}} & \\centering \\textcolor{gray}{\\textit{-5.12\\%}} & \\centering \\textcolor{gray}{\\textit{-2.11\\%}} & \\centering \\textcolor{gray}{\\textit{-57.79\\%}} \\arraybackslash \\\\\n    \\textbf{2023-23} & \\centering 371.96 & \\centering 305.10 & \\centering 292.58 & \\centering 152.20 & \\centering 146.90 & \\centering 143.93 & \\centering 61.28 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-17.98\\%}} & \\centering \\textcolor{gray}{\\textit{-4.10\\%}} & \\centering \\textcolor{gray}{\\textit{-47.98\\%}} & \\centering \\textcolor{gray}{\\textit{-3.48\\%}} & \\centering \\textcolor{gray}{\\textit{-2.02\\%}} & \\centering \\textcolor{gray}{\\textit{-57.42\\%}} \\arraybackslash \\\\\n    \\midrule\n    % Total   & 3835.32 & 1514.33(-2320.99) & 1507.54(-6.79) & 1466.25(-41.29) & 1450.90(-15.35) & 1420.70(-30.20) \\\\\n    \\textbf{Total} & \\centering 3834.58 & \\centering 3113.39 & \\centering 2990.57 & \\centering 1567.48 & \\centering 1452.06 & \\centering 1420.70 & \\centering 599.82 \\arraybackslash \\\\\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-18.81\\%}} & \\centering \\textcolor{gray}{\\textit{-3.94\\%}} & \\centering \\textcolor{gray}{\\textit{-47.59\\%}} & \\centering \\textcolor{gray}{\\textit{-7.36\\%}} & \\centering \\textcolor{gray}{\\textit{-2.16\\%}} & \\centering \\textcolor{gray}{\\textit{-57.78\\%}} \\arraybackslash \\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table}", "caption": "\\caption{The remaining data size and filtering ratio for each preprocessing step and quality evaluation module.}", "label": "\\label{remain_size}", "tabular": "\\begin{tabular}{cp{2cm}p{1.5cm}p{1.2cm}p{2cm}p{1.5cm}p{1.5cm}p{1.5cm}}\n    \\toprule\n    \\multirow{3.2}{*}{Snapshot} & \\multicolumn{7}{c}{Size After filtering operation(GB)} \\\\\n                                \\cmidrule{2-8}\n                                & \\centering Monolingual Chinese Data & \\centering Text Extraction & \\centering Data Length & \\centering Proportion of Characters & \\centering Sensitive Words & \\centering Internal Duplication  & \\centering Quality Evaluation \\arraybackslash \\\\\n    \\midrule\n    \\textbf{2021-43} & \\centering 505.92 & \\centering 424.43 & \\centering 409.68 & \\centering 217.52 & \\centering 192.84 & \\centering 187.57 & \\centering 78.95 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-16.11\\%}} & \\centering \\textcolor{gray}{\\textit{-3.48\\%}} & \\centering \\textcolor{gray}{\\textit{-46.90\\%}} & \\centering \\textcolor{gray}{\\textit{-11.35\\%}} & \\centering \\textcolor{gray}{\\textit{-2.73\\%}} & \\centering \\textcolor{gray}{\\textit{-57.91\\%}} \\arraybackslash \\\\\n    \\textbf{2022-05} & \\centering 442.47 & \\centering 375.64 & \\centering 362.34 & \\centering 182.88 & \\centering 169.01 & \\centering 164.96 & \\centering 69.44 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-15.10\\%}} & \\centering \\textcolor{gray}{\\textit{-3.54\\%}} & \\centering \\textcolor{gray}{\\textit{-49.53\\%}} & \\centering \\textcolor{gray}{\\textit{-7.58\\%}} & \\centering \\textcolor{gray}{\\textit{-2.40\\%}} & \\centering \\textcolor{gray}{\\textit{-57.90\\%}} \\arraybackslash \\\\\n    \\textbf{2022-21} & \\centering 443.57 & \\centering 363.33 & \\centering 348.51 & \\centering 178.16 & \\centering 170.09 & \\centering 166.75 & \\centering 70.19 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-18.09\\%}} & \\centering \\textcolor{gray}{\\textit{-4.08\\%}} & \\centering \\textcolor{gray}{\\textit{-48.88\\%}} & \\centering \\textcolor{gray}{\\textit{-4.53\\%}} & \\centering \\textcolor{gray}{\\textit{-1.96\\%}} & \\centering \\textcolor{gray}{\\textit{-57.91\\%}} \\arraybackslash \\\\\n    \\textbf{2022-27} & \\centering 417.95 & \\centering 340.65 & \\centering 326.52 & \\centering 158.83 & \\centering 152.33 & \\centering 149.41 & \\centering 62.7 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-18.50\\%}} & \\centering \\textcolor{gray}{\\textit{-4.15\\%}} & \\centering \\textcolor{gray}{\\textit{-51.36\\%}} & \\centering \\textcolor{gray}{\\textit{-4.09\\%}} & \\centering \\textcolor{gray}{\\textit{-1.92\\%}} & \\centering \\textcolor{gray}{\\textit{-58.03\\%}} \\arraybackslash \\\\\n    \\textbf{2022-33} & \\centering 369.56 & \\centering 293.07 & \\centering 280.58 & \\centering 131.39 & \\centering 125.84 & \\centering 123.70 & \\centering 51.98 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-20.70\\%}} & \\centering \\textcolor{gray}{\\textit{-4.26\\%}} & \\centering \\textcolor{gray}{\\textit{-53.17\\%}} & \\centering \\textcolor{gray}{\\textit{-4.22\\%}} & \\centering \\textcolor{gray}{\\textit{-1.70\\%}} & \\centering \\textcolor{gray}{\\textit{-57.98\\%}} \\arraybackslash \\\\\n    \\textbf{2022-49} & \\centering 445.29 & \\centering 367.73 & \\centering 352.59 & \\centering 173.86 & \\centering 164.34 & \\centering 160.87 & \\centering 67.76 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-17.42\\%}} & \\centering \\textcolor{gray}{\\textit{-4.12\\%}} & \\centering \\textcolor{gray}{\\textit{-50.69\\%}} & \\centering \\textcolor{gray}{\\textit{-5.48\\%}} & \\centering \\textcolor{gray}{\\textit{-2.11\\%}} & \\centering \\textcolor{gray}{\\textit{-57.88\\%}} \\arraybackslash \\\\\n    \\textbf{2023-06} & \\centering 396.40 & \\centering 275.04 & \\centering 263.59 & \\centering 211.10 & \\centering 177.44 & \\centering 173.47 & \\centering 74.19 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-30.62\\%}} & \\centering \\textcolor{gray}{\\textit{-4.16\\%}} & \\centering \\textcolor{gray}{\\textit{-19.91\\%}} & \\centering \\textcolor{gray}{\\textit{-15.95\\%}} & \\centering \\textcolor{gray}{\\textit{-2.24\\%}} & \\centering \\textcolor{gray}{\\textit{-57.23\\%}} \\arraybackslash \\\\\n    \\textbf{2023-14} & \\centering 441.46 & \\centering 368.40 & \\centering 354.18 & \\centering 161.54 & \\centering 153.27 & \\centering 150.04 & \\centering 63.33 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-16.55\\%}} & \\centering \\textcolor{gray}{\\textit{-3.86\\%}} & \\centering \\textcolor{gray}{\\textit{-54.39\\%}} & \\centering \\textcolor{gray}{\\textit{-5.12\\%}} & \\centering \\textcolor{gray}{\\textit{-2.11\\%}} & \\centering \\textcolor{gray}{\\textit{-57.79\\%}} \\arraybackslash \\\\\n    \\textbf{2023-23} & \\centering 371.96 & \\centering 305.10 & \\centering 292.58 & \\centering 152.20 & \\centering 146.90 & \\centering 143.93 & \\centering 61.28 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-17.98\\%}} & \\centering \\textcolor{gray}{\\textit{-4.10\\%}} & \\centering \\textcolor{gray}{\\textit{-47.98\\%}} & \\centering \\textcolor{gray}{\\textit{-3.48\\%}} & \\centering \\textcolor{gray}{\\textit{-2.02\\%}} & \\centering \\textcolor{gray}{\\textit{-57.42\\%}} \\arraybackslash \\\\\n    \\midrule\n    % Total   & 3835.32 & 1514.33(-2320.99) & 1507.54(-6.79) & 1466.25(-41.29) & 1450.90(-15.35) & 1420.70(-30.20) \\\\\n    \\textbf{Total} & \\centering 3834.58 & \\centering 3113.39 & \\centering 2990.57 & \\centering 1567.48 & \\centering 1452.06 & \\centering 1420.70 & \\centering 599.82 \\arraybackslash \\\\\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-18.81\\%}} & \\centering \\textcolor{gray}{\\textit{-3.94\\%}} & \\centering \\textcolor{gray}{\\textit{-47.59\\%}} & \\centering \\textcolor{gray}{\\textit{-7.36\\%}} & \\centering \\textcolor{gray}{\\textit{-2.16\\%}} & \\centering \\textcolor{gray}{\\textit{-57.78\\%}} \\arraybackslash \\\\\n    \\bottomrule\n    \\end{tabular}", "subtables": []}, {"original": "\\begin{table*}[htb]\n\\centering\n    %\\small\n     \\caption{The data quality distribution with different quality threshold.}\n    \\label{data quality distribution}\n    \\begin{tabular}{cccccc}\n    \\toprule\n    \\multirow{2.5}{*}{\\textbf{Threshold}} & \\multirow{2.5}{*}{\\textbf{High Quality Data Size}}&         \\multicolumn{4}{c}{\\textbf{Accuracy}}\\\\\n    \\cmidrule{3-6}\n    & & \\#1 &  \\#2 & \\#3 &{Average} \\\\\n    \\midrule \n    25\\% & 376.90 GB & 92.80\\% & 94.80\\% & 94.90\\% & 94.17\\%  \\\\  \n    35\\% & 525.59 GB & 93.60\\% & 93.60\\% & 93.10\\% & 93.43\\% \\\\ \n    40\\% & 599.82 GB & 90.30\\% & 91.90\\% & 89.50\\% & 90.57\\%  \\\\\n    45\\% & 672.68 GB & 84.60\\% & 85.50\\% & 85.59\\% & 85.33\\% \\\\\n    \\bottomrule\n    \\end{tabular}\n   \n\\end{table*}", "caption": "\\caption{The data quality distribution with different quality threshold.}", "label": "\\label{data quality distribution}", "tabular": "\\begin{tabular}{cccccc}\n    \\toprule\n    \\multirow{2.5}{*}{\\textbf{Threshold}} & \\multirow{2.5}{*}{\\textbf{High Quality Data Size}}&         \\multicolumn{4}{c}{\\textbf{Accuracy}}\\\\\n    \\cmidrule{3-6}\n    & & \\#1 &  \\#2 & \\#3 &{Average} \\\\\n    \\midrule \n    25\\% & 376.90 GB & 92.80\\% & 94.80\\% & 94.90\\% & 94.17\\%  \\\\  \n    35\\% & 525.59 GB & 93.60\\% & 93.60\\% & 93.10\\% & 93.43\\% \\\\ \n    40\\% & 599.82 GB & 90.30\\% & 91.90\\% & 89.50\\% & 90.57\\%  \\\\\n    45\\% & 672.68 GB & 84.60\\% & 85.50\\% & 85.59\\% & 85.33\\% \\\\\n    \\bottomrule\n    \\end{tabular}", "subtables": []}], "figure": [{"original": "\\begin{figure}[htbp]\n  \\centering\n  \\includegraphics[width=1\\textwidth]{picture/BERTRater_final.pdf}\n  \\caption{The architecture of our EvalWeb approach.}\n  \\label{fig1}\n\\end{figure}", "caption": "\\caption{The architecture of our EvalWeb approach.}", "label": "\\label{fig1}", "subfigures": [], "figure_paths": ["picture/BERTRater_final.pdf"]}, {"original": "\\begin{figure}[htbp]\n  \\centering\n  \\includegraphics[width=0.9\\textwidth]{picture/human_quality_control_final.pdf}\n  \\caption{Quality Control.}\n  \\label{fig2}\n \\end{figure}", "caption": "\\caption{Quality Control.}", "label": "\\label{fig2}", "subfigures": [], "figure_paths": ["picture/human_quality_control_final.pdf"]}, {"original": "\\begin{figure}[htbp]\n  \\centering\n  \\includegraphics[width=0.95\\textwidth]{picture/Ratio.pdf}\n  \\caption{Removal rate for different stages. Grey represents the removal rate with respect to each previous step, while other colors represent the kept rate of all data.}\n  \\label{fig4removal-rate}\n\\end{figure}", "caption": "\\caption{Removal rate for different stages. Grey represents the removal rate with respect to each previous step, while other colors represent the kept rate of all data.}", "label": "\\label{fig4removal-rate}", "subfigures": [], "figure_paths": ["picture/Ratio.pdf"]}, {"original": "\\begin{figure}[htbp]\n  \\centering\n  \\includegraphics[width=0.6\\textwidth]{picture/Length_analyze_bar.pdf}\n  \\caption{Length distribution of Data.}\n  \\label{fig4length}\n\\end{figure}", "caption": "\\caption{Length distribution of Data.}", "label": "\\label{fig4length}", "subfigures": [], "figure_paths": ["picture/Length_analyze_bar.pdf"]}], "equations": ["\\begin{equation}\n    \\mathcal{L}(\\boldsymbol{Y}, f(\\boldsymbol{X }| W)) =\\alpha MSE(\\boldsymbol{Y}, f(\\boldsymbol{X} | W))+\\beta MR(\\boldsymbol{Y}, f(\\boldsymbol{X} | W))+ \\gamma CS(\\boldsymbol{Y}, f(\\boldsymbol{X} | W)),\n\\end{equation}", "\\begin{equation}\nW^{t+1} = \\mathop{\\arg\\min} \\limits_{W} \\mathbb{E}_{\\boldsymbol{X}_p^l \\subset D_p} \\mathbb{E}_{S_n \\subset D_n} \\mathbb{E}_{\\boldsymbol{X}_n^l \\sim p(x_n| W^{t}), \\boldsymbol{X}_n^l \\subset S_n}\\{\\mathcal{L}(\\boldsymbol{Y}_p^l \\oplus \\boldsymbol{Y}_n^l, f(\\boldsymbol{X}_p^l \\oplus \\boldsymbol{X}_n^l | W))\\}.\n\\end{equation}", "\\begin{equation}\np(x_n|W^t) = \\frac{p(y_n|x_n; W)}{\\sum_{x \\in S_n} p(y_n|x; W)},\n\\end{equation}"], "algorithm": [], "sections": {"Introduction": {"content": "\n\nRecent years have witnessed the rapid progress of large language models (LLMs). The models, such as GPT-3\\cite{brown2020language}, BLOOM\\cite{scao2022bloom}, LLaMA\\cite{llama}, Falcon\\cite{2023refinedweb}, PaLM\\cite{chowdhery2022palm} and GPT-4\\cite{GPT4}, become more and more powerful, even performing better than humans in some natural language understanding and generation tasks.  \nDuring the development, it is evident that the scale and quality of pre-training data play a crucial role on LLM's capability. A large-scale and high-quality dataset is the foundation of LLMs and is the source of all the LLM's amazing capabilities.\n\nIn order to expedite the research on LLMs, several large-scale datasets have been made publicly available in recent years, such as C4 \\cite{2020T5C4}, Pile \\cite{2020_pile}, RefinedWeb \\cite{2023refinedweb} and WanJuan \\cite{2023wanjuan}. Previous studies usually collect the raw texts at first from various sources, such as Wikipedia, GitHub, ArXiv, Stack Exchange, and CommonCrawl, in which CommonCrawl data often accounts for the vast majority. Then, handcrafted rules are designed to filter out the raw data in three steps: extracting the data in the language of interested, filtering out the noisy texts with language-specific rules and data deduplication. It should be noted that, most of the previous studies mainly focus on the collection of English-centered texts, and there is lack of a complete tool-chain for extracting clean data centered in other languages, e.g. Chinese. Furthermore, previous work usually directly release the final data, without giving the fine-grained information of the text, such as the quality of each text, limiting the potential that assists LLM researchers to re-filter the data according to their desired quality threshold.\n\n\n\n\n\n\nTo address these problems, in this paper we introduce a new complete tool-chain \\textbf{EvalWeb}, which could extract high-quality Chinese texts from raw web data. The whole process can be divided into two parts. The first part is similar to previous studies and mainly utilizes manually designed rules to filter out explicit noisy data, generating the initial Chinese clean data. This part processes the web texts with two modules: preparation module and processing module. The preparation module first employs a language identification model to extract Chinese data, and then adopts a hash-based deduplication algorithm to remove duplicate texts. The preprocessing module then handles the resulting data with well-designed rules, including length filtering, sensitive words filtering and filtering with Chinese character ratio. Previous studies usually stop after the usage of these rules. In contrast, we introduce the second part which is a quality evaluation module. Due to the diversity of web texts, the remaining dataset after the usage of filtering rules still contains a large number of low-quality text, which cannot be cleared using manually crafted rules. Consequently, we propose to design a BERT-based quality evaluation model for assessing all the remaining data, thereby generating a quality score for each text. Finally, we can select the high-quality Chinese data in the dataset according to the quality threshold. Using our complete tool-chain \\textbf{EvalWeb}, we release the latest and largest Chinese dataset \\textbf{ChineseWebText}, which consists of 1.42 TB data and each text is assigned a quality score, facilitating LLM researchers to select data according to a new quality threshold. We also release a much cleaner subset of 600 GB Chinese texts with quality exceeding 90\\%.\n\n\n\n\n\n\nOur contributions can be summarized as follows:\n\n(1) In this paper, we propose a new complete tool-chain \\textbf{EvalWeb}, which could extract high-quality Chinese pre-training data from noisy web texts.\n\n(2) In this paper, we release the latest and largest Chinese dataset consisting of 1.42 TB, and each text in this dataset is assigned a quality score according to our quality evaluation module. We further release a much cleaner subset of 600 GB Chinese texts with quality exceeding 90\\%.\n\n\n\n\n", "appendix": false}, "Related Work": {"content": "\n\n\\textbf{Rule-based Text Filtering.} Rule-based text filtering methods are the dominant paradigm to identify content-rich and semantically coherent data from collected raw datasets with handcrafted rules. During the collection of pre-training data, there are a large number of text data on the web. However, these data include a lot of noise, such as violence, pornographic, advertisement and error characters. Consequently, in order to extract high-quality data, several rule-based methods have been proposed to explore how to automatically filter undesired content from noisy web data. In these work, deduplication\\cite{lee_deduplicating_2022} methods are employed to remove duplicate text from the data, while some handcrafted rules \\cite{raffel_exploring_2020,luccioni_whats_2021} are adopted to filter out violence, pornographic, advertisement and other explicit noisy data. Besides, perplexity \\cite{wenzek_ccnet_2020} is also usually used to evaluate the fluency of the texts. However, these work mainly focus on English and lack a complete tool-chain for Chinese.\n\n\\textbf{Text Classification Model}. Different from rule-based text filtering methods, text classification model is an alternative approach to identify high-quality data with a well-designed classifier. The simplest text classification model is logistic regression\\cite{brown2020language}, which uses the logistic function to calculate the probability values for each text, and then classifies them into positive or negative with a designed threshold. Currently, BERT\\cite{devlin_bert_2018} and FastText\\cite{joulin_bag_2017} are both commonly used text classification models. BERT is a transformer-based\\cite{transformer} pre-training language model that has achieved remarkable performance in various text classification and understanding tasks. Through pre-training on masked language model and next sentence prediction tasks with a large dataset, this model learns powerful language understanding and representation abilities, which makes it perform well on text classification tasks. FastText\\cite{joulin_bag_2017} is also a neural network based approach which is similar to CBOW\\cite{word2vec}. It is characterized by its ability to train efficiently and quickly on large-scale data, while achieving competitive classification performance. In this paper, both of these two approaches will be employed to evaluate the qualities of the Chinese texts.  \n\n\\textbf{Datasets for Pre-training.} In recent years as the scale of pre-trained language models expands, there is a concomitant increase in the demand for large-scale pre-training datasets. Due to the convenience of acquisition and cost-efficiency associated with web-scraped data, it has progressively emerged as a pivotal source for pre-training datasets\\cite{raffel2020exploring}. In these work, Gao et al. (2020) \\cite{gao2020pile} build a 825 GB English corpus by mixing established natural language processing datasets and several newly introduced ones. This dataset covers 22 diverse high-quality subsets which derive from academic or professional sources, including PubMed Central, the FreeLaw Project, Stack Exchange, Books3\\cite{presser2020books3}, OpenSubtitles\\cite{tiedemann2016finding} and so on. Different from the work of Gao et al. (2020) \\cite{gao2020pile}, Penedo et al. (2023)\\cite{penedo2023refinedweb} demonstrate that properly filtered and deduplicated web data alone can also train a powerful model, even outperforming the LLMs trained on curated corpora. They use a pipeline approach to filter and deduplicate web data from CommonCrawl at very large scale and then release an English dataset which have 600 billion tokens. In addition, with data from web and synthetically generated textbooks and exercises with GPT-3.5, Gunasekar et al. (2023)\\cite{gunasekar2023textbooks} build a code dataset which has 7B tokens. They mainly focus on the coding functions of LLMs, esecpically the writing Python functions. Moreover, He et al. (2023) \\cite{he2023wanjuan} release a comprehensive multimodal dataset, which also contains texts in both Chinese and English, and is collected from a wide range of web sources. These public datasets mainly focus on English, and lack a complete tool-chain for extracting Chinese clean data from web sources. Furthermore, they only contain the cleaned texts, while missing the corresponding fine-grained information (e.g. the quality of each text) which could help LLM researchers to re-filter texts with new desired quality thresholds.\n\n\n", "appendix": false}, "Data Construction": {"content": "\n\nDue to the presence of substantial noise and irrelevant information on the web, extracting high-quality Chinese data from the web poses a significant challenge. In order to extract high-quality Chinese text from web effectively, in this paper we propose a pipeline system \\textbf{EvalWeb}, which integrates mannual crafted rules and evaluation models. With this approach, we can effectively filter undesirable content such as offensive speech, advertisements and idle chatter, and then extract high-quality Chinese texts. As in Figure \\ref{fig1}, it illustrates the overview of our proposed approach. For the crawled data from web, we first use a preparation module to process them, and then extract the monolingual Chinese data. After that, a preprocessing module will be used to further filter them with mannual crafted rules, including data length, sensitive words, proportion of Chinese characters and so on. Finally, a BERT-based evaluation model will be employed to assess the qualities of filtered data. By this way, we can generate a quality score for each of the text, and then use an appropriate threshold to extract the high-quality data as we required. Furthermore, \nconsidering computational cost and efficiency, we further propose to leverage knowledge distillation\\cite{stanton2021does} techniques to train a FastText classifier, which can achieve similar performance with faster efficiency and lower computational costs.\n\n\\begin{figure}[htbp]\n  \\centering\n  \\includegraphics[width=1\\textwidth]{picture/BERTRater_final.pdf}\n  \\caption{The architecture of our EvalWeb approach.}\n  \\label{fig1}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=1\\textwidth]{picture/BERTRater_final.pdf}\n  \\caption{The architecture of our EvalWeb approach.}\n  \\label{fig1}\n\n\n\\subsection{Data Collection and Preparation}\n\nAs a publicly accessible web scrape dataset, CommonCrawl has been running for 12 years and has accumulated petabytes of web data. Consequently, we regard it as the source of our web data. In this paper, we collect nine latest CommonCrawl snapshots from the internet, including \"2021-43\", \"2022-05\", \"2022-21\", \"2022-27\",\"2022-33\",\"2022-49\",\"2023-06\",\"2023-14\" and \"2023-23\". These obtained snapshots are compressed plain text, each of which is 8-10 TB in size (approximately 3 billion web pages). Each snapshot is regrouped into JSON format shards of 5 GB, where each item corresponds to a web page. \n\nSince the original CommonCrawl file is typically quite diverse and contains texts in many languages, to efficiently extract Chinese text data from it, we first employ a deduplication and language identification (LID) module to perform preliminary cleaning on the collected datasets. Following the work of CCNet\\cite{wenzek_ccnet_2020}, in this module a Hash-based inter-string deduplication method is employed to remove duplicate text from different snapshots. Additionally, a well-trained language identification model\\cite{grave2018learning}, which could support 157 languages, is applied to select Chinese data. By this way, we can obtain all the monolingual Chinese text data we required. \n\n\\subsection{Preprocessing}\n\n\\begin{table}[htbp]\n    \\caption{ Examples for different filtering rules.}\\label{Data Filtering Statistics}\n    \\centering\n    \\begin{tabular}{p{4cm} | p{9cm} }\n    \\toprule\n     Filtering Operation & Example \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Text Extraction}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{gbsn} \\{\"url\": \"http://sarahokane.com/ywly/index.aspx\",\\newline \"date\\_download\": \"2022-05-28T10:17:39Z\", \\newline \"length\": 854, \\newline \"nlines\": 17, \\newline \"source\\_domain\": \"sarahokane.com\", \\newline \"title\": \"\u5408\u80a5\u5e02\u5efa\u8bbe\u6295\u8d44\u63a7\u80a1(\u96c6\u56e2)\u6709\u9650\u516c\u53f8\", \\newline \\textcolor{red}{\"raw\\_content\": \" \u4e61\u6751\u632f\u5174\u548c\u73b0\u4ee3\u519c\u4e1a\u677f\u5757\\textbackslash n\u4e61\u6751\u632f\u5174\u4e0e\u73b0\u4ee3\u519c\u4e1a...\u6ce8\u518c\u8d44\u672c4.39\u4ebf\u5143\u3002\",} \\newline ...\\newline \"language\": \"zh\", \\newline \"bucket\": \"head\"\\} \\end{CJK}}  \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Length less than 200}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{gbsn} \"\u5fb7\u56fd\u9ed1\u68ee\u5dde\u6cd5\u5170\u514b\u798fAMazon\u6570\u636e\u4e2d\u5fc3\" \\end{CJK}}  \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Average line length less than 10}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{gbsn} \"\u6c7d\u8f66\u8d44\u8baf\\textbackslash n\u6c7d\u8f66\u5236\u9020\u5546\\textbackslash n\u5b66\u8f66\u79df\u8f66\\textbackslash n\u4ff1\u4e50\u90e8,\u6c7d\u8f66\u7f51,\u6c7d\u8f66\u62a5\u4ef7,\u65b0\u8f66,\u6c7d\u8f66\u56fe\u7247\\textbackslash n\u6c7d\u8f66\u7f51,\u6c7d\u8f66\u62a5\u4ef7,\u65b0\u80fd\u6e90,\u65b0\u8f66,\u6c7d\u8f66\u56fe\u7247\\textbackslash n\u5357\u65b9\u7f51\uff1a\u6c7d\u8f66\u9891\u9053\\textbackslash n\u7ef4\u4fee,\u6539\u88c5,\u8f66\u6a21,\u65b0\u8f66,\u7528\u8f66\\textbackslash n ......\" \\end{CJK}}  \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Traditional Chinese characters}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{bsmi} \"\u6709\u770b\u904e\u88dd\u6f62\u4e2d\u7684\u4fbf\u5229\u5546\u5e97\u55ce\uff1f\u5b83\u53ef\u80fd\u5167\u90e8\u9084\u6c92\u6574\u5099\u597d\uff0c\u751a\u81f3\u6839\u672c\u662f\u7a7a\u76ea\u76ea\u7684\u4e00\u7247\u3002\u4f46\u5e97\u9580\u4e00\u5b9a\u6703\u4e0a\u7d05\u5e03\u689d\uff0c\u5beb\u8457\u300cXXX\u4fbf\u5229\u5546\u5e97\u5728\u6b64\u70ba\u60a8\u670d\u52d9\u300d\u3002\u53bb\u904e\u5916\u7e23\u5e02\u65c5\u904a\u55ce\uff1f\u7576\u4f60\u958b\u8457\u8eca\u8981\u627e\u67d0\u5bb6\u5e97\u6642\uff0c\u4f60\u5927\u6982\u4e0d\u6703\u958b\u5230\u6b63\u9580\u53e3\u624d\u770b\u662f\u4e0d\u662f\u4f60\u8981\u627e\u7684\u5e97\u3002\" \\end{CJK}}  \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Proportion of Chinese characters fewer than 30\\%}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{gbsn} \"\u7ebf\u4e0a\u4e70\u7403\u5e73\u53f0\\textbackslash u0006\\textbackslash u0007\u3001\u7cbe\u51c6\\textbackslash u0007\\textbackslash u0005\u3001\u53ef\u9760\u7684\u4f20\u611f\u6280\u672f\u89e3\u51b3\u65b9\u6848\u53ca\u4ea7\u54c1\\textbackslash n\\textbackslash nXSENS MTi 600\u7cfb\u5217 ...\\textbackslash n\\textbackslash n7 GNSS/INS\\textbackslash n\u7ebf\u4e0a\u4e70\u7403\u5e73\u53f0\\textbackslash u0006\uff0c\u5177\u6709\u591a\u4e2aGN......</p>\\textbackslash nMTi-G-710 GNSS/I...\\textbackslash n<p><span style=\"font-size: 12px;\">......\" \\end{CJK}}  \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Occurrence of sensitive words more than 0.5 per line}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{gbsn} \"\u5b9d\u535a\u4f53\u80b2\u5f3a\u5316\u521b\u65b0\u5f15\u9886\uff0c\u575a\u6301\u201c\u79d1\u6280\u5b9d\u535a\u4f53\u80b2\u201d\u6218\u7565\uff0c\u6784\u5efa\u201c\u4ee5\u4f01\u4e1a\u4e3a\u4e3b\u4f53\u3001\u5e02\u573a\u4e3a\u5bfc\u5411\u3001\u4ea7\u5b66\u7814\u76f8\u7ed3\u5408\u201d\u7684\u79d1\u6280\u521b\u65b0\u4f53\u7cfb\u3002\\textbackslash n2019\u5e74\u5ea6\u5927\u4e8b\u8bb0\\textbackslash n\u53cb\u60c5\u94fe\u63a5\uff1a\u73a9\u7403\u76f4\u64adnba \u771f\u94b1\u6eda\u7403\u771f\u4eba \u91d1\u82b1\u4e09\u5f20\u724c\u8d62\u94b1 55\u76f4\u64adnba \u4e70\u7403 \u624b\u673a\u8f6e\u76d8app \u4e9a\u535aapp\u5728\u7ebf\u767b\u5f55 \u4e9a\u535aapp\u767b\u5f55......\" \\end{CJK}}  \\\\\n     \\midrule\n     \\multicolumn{1}{m{4cm}|}{Internal duplication ratio greater than 50\\%}  & \\multicolumn{1}{m{9cm}}{\\begin{CJK}{UTF8}{gbsn} \"\\textcolor{red}{\u201c\u5c71\u4e1c\u7701\u6c11\u95f4\u878d\u8d44\u673a\u6784\u5ba3\u4f20\u6708\u6d3b\u52a8\u201d......\u6d3b\u52a8\u671f\u95f4\u5438\u5f15\u4e86\u5f53\u5730\u5e02\u6c11\\textbackslash{}n}2018\u5e7411\u67089\u65e5\uff0c\\textcolor{red}{\u201c\u5c71\u4e1c\u7701\u6c11\u95f4\u878d\u8d44\u673a\u6784\u5ba3\u4f20\u6708\u6d3b\u52a8\u201d......\u6d3b\u52a8\u671f\u95f4\u5438\u5f15\u4e86\u5f53\u5730\u5e02\u6c11}\u7684\u5e7f\u6cdb\u5173\u6ce8\uff0c\u73b0\u573a\u53d1\u653e\u5ba3\u4f20\u8d44\u6599600\u4f59\u4efd\uff0c\u89e3\u7b54\u5e02\u6c11\u7591\u95ee100\u4f59\u4eba\u6b21\uff0c\u5fb7\u5dde\u5e7f\u64ad\u7535\u89c6\u53f0\uff0c\u5fb7\u5dde\u65e5\u62a5\uff0c\u5fb7\u5dde\u665a\u62a5\u5168\u7a0b\u91c7\u8bbf\u62a5\u9053\u3002\" \\end{CJK}} \\\\\n     \\bottomrule\n    \\end{tabular}\n\\end{table}\n    \\caption{ Examples for different filtering rules.}\\label{Data Filtering Statistics}\n    \\centering\n    \n    \\toprule\n     Filtering Operation & Example \\\\\n     \\midrule\n     \\multicolumn{1}1{m{4cm}|}m{4cm}4cm|{Text Extraction}Text Extraction  & \\multicolumn{1}1{m{9cm}}m{9cm}9cm{\\begin{CJK}{UTF8}{gbsn} \\{\"url\": \"http://sarahokane.com/ywly/index.aspx\",\\newline \"date\\_download\": \"2022-05-28T10:17:39Z\", \\newline \"length\": 854, \\newline \"nlines\": 17, \\newline \"source\\_domain\": \"sarahokane.com\", \\newline \"title\": \"\u5408\u80a5\u5e02\u5efa\u8bbe\u6295\u8d44\u63a7\u80a1(\u96c6\u56e2)\u6709\u9650\u516c\u53f8\", \\newline \\textcolor{red}{\"raw\\_content\": \" \u4e61\u6751\u632f\u5174\u548c\u73b0\u4ee3\u519c\u4e1a\u677f\u5757\\textbackslash n\u4e61\u6751\u632f\u5174\u4e0e\u73b0\u4ee3\u519c\u4e1a...\u6ce8\u518c\u8d44\u672c4.39\u4ebf\u5143\u3002\",} \\newline ...\\newline \"language\": \"zh\", \\newline \"bucket\": \"head\"\\} \\end{CJK}}{UTF8}UTF8{gbsn}gbsn \\{\"url\": \"http://sarahokane.com/ywly/index.aspx\",\\newline \"date\\_download\": \"2022-05-28T10:17:39Z\", \\newline \"length\": 854, \\newline \"nlines\": 17, \\newline \"source\\_domain\": \"sarahokane.com\", \\newline \"title\": \"\u5408\u80a5\u5e02\u5efa\u8bbe\u6295\u8d44\u63a7\u80a1(\u96c6\u56e2)\u6709\u9650\u516c\u53f8\", \\newline \\textcolor{red}{\"raw\\_content\": \" \u4e61\u6751\u632f\u5174\u548c\u73b0\u4ee3\u519c\u4e1a\u677f\u5757\\textbackslash n\u4e61\u6751\u632f\u5174\u4e0e\u73b0\u4ee3\u519c\u4e1a...\u6ce8\u518c\u8d44\u672c4.39\u4ebf\u5143\u3002\",} \\newline ...\\newline \"language\": \"zh\", \\newline \"bucket\": \"head\"\\}   \\\\\n     \\midrule\n     \\multicolumn{1}1{m{4cm}|}m{4cm}4cm|{Length less than 200}Length less than 200  & \\multicolumn{1}1{m{9cm}}m{9cm}9cm{\\begin{CJK}{UTF8}{gbsn} \"\u5fb7\u56fd\u9ed1\u68ee\u5dde\u6cd5\u5170\u514b\u798fAMazon\u6570\u636e\u4e2d\u5fc3\" \\end{CJK}}{UTF8}UTF8{gbsn}gbsn \"\u5fb7\u56fd\u9ed1\u68ee\u5dde\u6cd5\u5170\u514b\u798fAMazon\u6570\u636e\u4e2d\u5fc3\"   \\\\\n     \\midrule\n     \\multicolumn{1}1{m{4cm}|}m{4cm}4cm|{Average line length less than 10}Average line length less than 10  & \\multicolumn{1}1{m{9cm}}m{9cm}9cm{\\begin{CJK}{UTF8}{gbsn} \"\u6c7d\u8f66\u8d44\u8baf\\textbackslash n\u6c7d\u8f66\u5236\u9020\u5546\\textbackslash n\u5b66\u8f66\u79df\u8f66\\textbackslash n\u4ff1\u4e50\u90e8,\u6c7d\u8f66\u7f51,\u6c7d\u8f66\u62a5\u4ef7,\u65b0\u8f66,\u6c7d\u8f66\u56fe\u7247\\textbackslash n\u6c7d\u8f66\u7f51,\u6c7d\u8f66\u62a5\u4ef7,\u65b0\u80fd\u6e90,\u65b0\u8f66,\u6c7d\u8f66\u56fe\u7247\\textbackslash n\u5357\u65b9\u7f51\uff1a\u6c7d\u8f66\u9891\u9053\\textbackslash n\u7ef4\u4fee,\u6539\u88c5,\u8f66\u6a21,\u65b0\u8f66,\u7528\u8f66\\textbackslash n ......\" \\end{CJK}}{UTF8}UTF8{gbsn}gbsn \"\u6c7d\u8f66\u8d44\u8baf\\textbackslash n\u6c7d\u8f66\u5236\u9020\u5546\\textbackslash n\u5b66\u8f66\u79df\u8f66\\textbackslash n\u4ff1\u4e50\u90e8,\u6c7d\u8f66\u7f51,\u6c7d\u8f66\u62a5\u4ef7,\u65b0\u8f66,\u6c7d\u8f66\u56fe\u7247\\textbackslash n\u6c7d\u8f66\u7f51,\u6c7d\u8f66\u62a5\u4ef7,\u65b0\u80fd\u6e90,\u65b0\u8f66,\u6c7d\u8f66\u56fe\u7247\\textbackslash n\u5357\u65b9\u7f51\uff1a\u6c7d\u8f66\u9891\u9053\\textbackslash n\u7ef4\u4fee,\u6539\u88c5,\u8f66\u6a21,\u65b0\u8f66,\u7528\u8f66\\textbackslash n ......\"   \\\\\n     \\midrule\n     \\multicolumn{1}1{m{4cm}|}m{4cm}4cm|{Traditional Chinese characters}Traditional Chinese characters  & \\multicolumn{1}1{m{9cm}}m{9cm}9cm{\\begin{CJK}{UTF8}{bsmi} \"\u6709\u770b\u904e\u88dd\u6f62\u4e2d\u7684\u4fbf\u5229\u5546\u5e97\u55ce\uff1f\u5b83\u53ef\u80fd\u5167\u90e8\u9084\u6c92\u6574\u5099\u597d\uff0c\u751a\u81f3\u6839\u672c\u662f\u7a7a\u76ea\u76ea\u7684\u4e00\u7247\u3002\u4f46\u5e97\u9580\u4e00\u5b9a\u6703\u4e0a\u7d05\u5e03\u689d\uff0c\u5beb\u8457\u300cXXX\u4fbf\u5229\u5546\u5e97\u5728\u6b64\u70ba\u60a8\u670d\u52d9\u300d\u3002\u53bb\u904e\u5916\u7e23\u5e02\u65c5\u904a\u55ce\uff1f\u7576\u4f60\u958b\u8457\u8eca\u8981\u627e\u67d0\u5bb6\u5e97\u6642\uff0c\u4f60\u5927\u6982\u4e0d\u6703\u958b\u5230\u6b63\u9580\u53e3\u624d\u770b\u662f\u4e0d\u662f\u4f60\u8981\u627e\u7684\u5e97\u3002\" \\end{CJK}}{UTF8}UTF8{bsmi}bsmi \"\u6709\u770b\u904e\u88dd\u6f62\u4e2d\u7684\u4fbf\u5229\u5546\u5e97\u55ce\uff1f\u5b83\u53ef\u80fd\u5167\u90e8\u9084\u6c92\u6574\u5099\u597d\uff0c\u751a\u81f3\u6839\u672c\u662f\u7a7a\u76ea\u76ea\u7684\u4e00\u7247\u3002\u4f46\u5e97\u9580\u4e00\u5b9a\u6703\u4e0a\u7d05\u5e03\u689d\uff0c\u5beb\u8457\u300cXXX\u4fbf\u5229\u5546\u5e97\u5728\u6b64\u70ba\u60a8\u670d\u52d9\u300d\u3002\u53bb\u904e\u5916\u7e23\u5e02\u65c5\u904a\u55ce\uff1f\u7576\u4f60\u958b\u8457\u8eca\u8981\u627e\u67d0\u5bb6\u5e97\u6642\uff0c\u4f60\u5927\u6982\u4e0d\u6703\u958b\u5230\u6b63\u9580\u53e3\u624d\u770b\u662f\u4e0d\u662f\u4f60\u8981\u627e\u7684\u5e97\u3002\"   \\\\\n     \\midrule\n     \\multicolumn{1}1{m{4cm}|}m{4cm}4cm|{Proportion of Chinese characters fewer than 30\\%}Proportion of Chinese characters fewer than 30\\%  & \\multicolumn{1}1{m{9cm}}m{9cm}9cm{\\begin{CJK}{UTF8}{gbsn} \"\u7ebf\u4e0a\u4e70\u7403\u5e73\u53f0\\textbackslash u0006\\textbackslash u0007\u3001\u7cbe\u51c6\\textbackslash u0007\\textbackslash u0005\u3001\u53ef\u9760\u7684\u4f20\u611f\u6280\u672f\u89e3\u51b3\u65b9\u6848\u53ca\u4ea7\u54c1\\textbackslash n\\textbackslash nXSENS MTi 600\u7cfb\u5217 ...\\textbackslash n\\textbackslash n7 GNSS/INS\\textbackslash n\u7ebf\u4e0a\u4e70\u7403\u5e73\u53f0\\textbackslash u0006\uff0c\u5177\u6709\u591a\u4e2aGN......</p>\\textbackslash nMTi-G-710 GNSS/I...\\textbackslash n<p><span style=\"font-size: 12px;\">......\" \\end{CJK}}{UTF8}UTF8{gbsn}gbsn \"\u7ebf\u4e0a\u4e70\u7403\u5e73\u53f0\\textbackslash u0006\\textbackslash u0007\u3001\u7cbe\u51c6\\textbackslash u0007\\textbackslash u0005\u3001\u53ef\u9760\u7684\u4f20\u611f\u6280\u672f\u89e3\u51b3\u65b9\u6848\u53ca\u4ea7\u54c1\\textbackslash n\\textbackslash nXSENS MTi 600\u7cfb\u5217 ...\\textbackslash n\\textbackslash n7 GNSS/INS\\textbackslash n\u7ebf\u4e0a\u4e70\u7403\u5e73\u53f0\\textbackslash u0006\uff0c\u5177\u6709\u591a\u4e2aGN......</p>\\textbackslash nMTi-G-710 GNSS/I...\\textbackslash n<p><span style=\"font-size: 12px;\">......\"   \\\\\n     \\midrule\n     \\multicolumn{1}1{m{4cm}|}m{4cm}4cm|{Occurrence of sensitive words more than 0.5 per line}Occurrence of sensitive words more than 0.5 per line  & \\multicolumn{1}1{m{9cm}}m{9cm}9cm{\\begin{CJK}{UTF8}{gbsn} \"\u5b9d\u535a\u4f53\u80b2\u5f3a\u5316\u521b\u65b0\u5f15\u9886\uff0c\u575a\u6301\u201c\u79d1\u6280\u5b9d\u535a\u4f53\u80b2\u201d\u6218\u7565\uff0c\u6784\u5efa\u201c\u4ee5\u4f01\u4e1a\u4e3a\u4e3b\u4f53\u3001\u5e02\u573a\u4e3a\u5bfc\u5411\u3001\u4ea7\u5b66\u7814\u76f8\u7ed3\u5408\u201d\u7684\u79d1\u6280\u521b\u65b0\u4f53\u7cfb\u3002\\textbackslash n2019\u5e74\u5ea6\u5927\u4e8b\u8bb0\\textbackslash n\u53cb\u60c5\u94fe\u63a5\uff1a\u73a9\u7403\u76f4\u64adnba \u771f\u94b1\u6eda\u7403\u771f\u4eba \u91d1\u82b1\u4e09\u5f20\u724c\u8d62\u94b1 55\u76f4\u64adnba \u4e70\u7403 \u624b\u673a\u8f6e\u76d8app \u4e9a\u535aapp\u5728\u7ebf\u767b\u5f55 \u4e9a\u535aapp\u767b\u5f55......\" \\end{CJK}}{UTF8}UTF8{gbsn}gbsn \"\u5b9d\u535a\u4f53\u80b2\u5f3a\u5316\u521b\u65b0\u5f15\u9886\uff0c\u575a\u6301\u201c\u79d1\u6280\u5b9d\u535a\u4f53\u80b2\u201d\u6218\u7565\uff0c\u6784\u5efa\u201c\u4ee5\u4f01\u4e1a\u4e3a\u4e3b\u4f53\u3001\u5e02\u573a\u4e3a\u5bfc\u5411\u3001\u4ea7\u5b66\u7814\u76f8\u7ed3\u5408\u201d\u7684\u79d1\u6280\u521b\u65b0\u4f53\u7cfb\u3002\\textbackslash n2019\u5e74\u5ea6\u5927\u4e8b\u8bb0\\textbackslash n\u53cb\u60c5\u94fe\u63a5\uff1a\u73a9\u7403\u76f4\u64adnba \u771f\u94b1\u6eda\u7403\u771f\u4eba \u91d1\u82b1\u4e09\u5f20\u724c\u8d62\u94b1 55\u76f4\u64adnba \u4e70\u7403 \u624b\u673a\u8f6e\u76d8app \u4e9a\u535aapp\u5728\u7ebf\u767b\u5f55 \u4e9a\u535aapp\u767b\u5f55......\"   \\\\\n     \\midrule\n     \\multicolumn{1}1{m{4cm}|}m{4cm}4cm|{Internal duplication ratio greater than 50\\%}Internal duplication ratio greater than 50\\%  & \\multicolumn{1}1{m{9cm}}m{9cm}9cm{\\begin{CJK}{UTF8}{gbsn} \"\\textcolor{red}{\u201c\u5c71\u4e1c\u7701\u6c11\u95f4\u878d\u8d44\u673a\u6784\u5ba3\u4f20\u6708\u6d3b\u52a8\u201d......\u6d3b\u52a8\u671f\u95f4\u5438\u5f15\u4e86\u5f53\u5730\u5e02\u6c11\\textbackslash{}n}2018\u5e7411\u67089\u65e5\uff0c\\textcolor{red}{\u201c\u5c71\u4e1c\u7701\u6c11\u95f4\u878d\u8d44\u673a\u6784\u5ba3\u4f20\u6708\u6d3b\u52a8\u201d......\u6d3b\u52a8\u671f\u95f4\u5438\u5f15\u4e86\u5f53\u5730\u5e02\u6c11}\u7684\u5e7f\u6cdb\u5173\u6ce8\uff0c\u73b0\u573a\u53d1\u653e\u5ba3\u4f20\u8d44\u6599600\u4f59\u4efd\uff0c\u89e3\u7b54\u5e02\u6c11\u7591\u95ee100\u4f59\u4eba\u6b21\uff0c\u5fb7\u5dde\u5e7f\u64ad\u7535\u89c6\u53f0\uff0c\u5fb7\u5dde\u65e5\u62a5\uff0c\u5fb7\u5dde\u665a\u62a5\u5168\u7a0b\u91c7\u8bbf\u62a5\u9053\u3002\" \\end{CJK}}{UTF8}UTF8{gbsn}gbsn \"\\textcolor{red}{\u201c\u5c71\u4e1c\u7701\u6c11\u95f4\u878d\u8d44\u673a\u6784\u5ba3\u4f20\u6708\u6d3b\u52a8\u201d......\u6d3b\u52a8\u671f\u95f4\u5438\u5f15\u4e86\u5f53\u5730\u5e02\u6c11\\textbackslash{}n}2018\u5e7411\u67089\u65e5\uff0c\\textcolor{red}{\u201c\u5c71\u4e1c\u7701\u6c11\u95f4\u878d\u8d44\u673a\u6784\u5ba3\u4f20\u6708\u6d3b\u52a8\u201d......\u6d3b\u52a8\u671f\u95f4\u5438\u5f15\u4e86\u5f53\u5730\u5e02\u6c11}\u7684\u5e7f\u6cdb\u5173\u6ce8\uff0c\u73b0\u573a\u53d1\u653e\u5ba3\u4f20\u8d44\u6599600\u4f59\u4efd\uff0c\u89e3\u7b54\u5e02\u6c11\u7591\u95ee100\u4f59\u4eba\u6b21\uff0c\u5fb7\u5dde\u5e7f\u64ad\u7535\u89c6\u53f0\uff0c\u5fb7\u5dde\u65e5\u62a5\uff0c\u5fb7\u5dde\u665a\u62a5\u5168\u7a0b\u91c7\u8bbf\u62a5\u9053\u3002\"  \\\\\n     \\bottomrule\n    \n\n\n\nAfter getting the monolingual Chinese web data, in this section we will focus on how to extract high-quality Chinese texts from them. Given the prevalence of violent, pornographic, advertising, and error characters in web data, we will first employ some manually crafted rules to filter out these noisy data. The details of these crafted rules are presented in the following.\n\n \\textbf{Text Extraction} After the data preparation stage, there exists a substantial amount of redundant content, which holds little substantive value for subsequent analysis, such as irrelevant key-value pairs. To ensure the accuracy and efficiency of data analysis, we initially undertake the task of extracting all textual content from the entire dataset.\n\n\n\n\\textbf{Data Length} In web texts, a substantial portion of data consists of documents with short text lines which are separated by '\\textbackslash n'. And the texts in different lines do not have significant semantic relevance to each other, which results in that these data are not useful for the training of language models. To eliminate excessively documents with short text lines, we will calculate the average text line length for each document and then remove documents with an average line length of fewer than 10 characters. Besides, during pre-training procedure, short text data usually contains limited information, making it ineffective in providing context and contextual information. Consequently, we will remove texts whose length is less than 200.\n\n\n\n\n\n\n\n\\textbf{Proportion of Characters} The objective of this paper is to create a high-quality simplified Chinese dataset sourced from web data. Therefore, we firstly eliminate text data composed of traditional Chinese characters. Additionally, we found that some data exhibit a notably low percentage of Chinese characters. And the rest of these data are filled with some other language characters, non-essential characters, special symbols, irrelevant markers, and so on. These data are unhelpful for the training of large language models. Consequently, we will remove the texts with fewer than 30\\% Chinese characters.\n\n\\textbf{Sensitive Words} In web data, there is a large amount of harmful texts, including sex, gambling, violence, discrimination, drugs, religion, and so on.  These texts can make large language models generate toxic contents, which have a big negative influence on society, nations and individuals. To avoid these issues, it is necessary to filter out harmful content from the web texts. Firstly, we collect a lot of harmful words and build a sensitive word list. After that, we count the sensitive words in each line of the texts. For one text, if the occurrence of sensitive words exceeds 0.5 per lines, we will regard it as a toxic text and will remove it from our dataset.\n\n\n\n\n\n\\textbf{Internal duplication}\nIn the training of large language models, duplicate texts can significantly impact training efficiency and model performance. Although we have conducted deduplication in the first stage, subsequent analysis revealed that some duplicate information still exists in the texts. Therefore, we adopt a granularity of 13-gram for analysis, quantifying the proportion of repetitive 13-gram character sequences across all data. When the proportion of repeated 13-gram characters in a data sample exceeds 50\\%, we opt to filter it out.\n\n\n\n\n\n\nThrough the aforementioned rigorous data preprocessing steps, a substantial amount of low-quality data is filtered out. After that, a quality evaluation model will be employed to evaluate the quality scores of the remaining data and then extract the high-quality data with a desired threshold. \n\n\\subsection{Quality Evaluation}\n\\label{quality_evaluation}\n\\subsubsection{BERTEval}\n\nIn preprocessing procedure, we have used some handcrafted rules to remove the explicit noisy texts from our dataset. However, within the remaining data, there is still a considerable amount of low-quality text data, which cannot be filtered out with handcrafted rules. In order to extract the data of higher quality from them, in this section we further propose to design an evaluation model. In this approach, we will develop a BERT-based classification model to generate a quality score for each text, and then filter the high-quality data with a threshold. The details of the classification model are presented in the following.\n\n\n\n\n\\textbf{Training Data Composition} While the evaluation in our current experiment targets CommonCrawl data, we believe the positive training samples should encompass a variety of text types, such as Wikipedia, e-books, poetry, news, and Q\\&A data, to prevent the model from exhibiting bias toward deeming any specific text type as high quality. Since CommonCrawl data has a relatively high noise level overall, we directly sampled from CommonCrawl and used the sampling results as negative examples. Table \\ref{bert_data} presents the detailed composition and quantity of the training data.\n\\begin{table}[htbp]\n\t\\caption{Composition of BERTEval Training Data.}\\label{bert_data}\n\t\\centering\n\t\\begin{tabular}{llc}\n\t\t\\toprule\n\t\tType     & Source     & Size ($\\times 10^4 $) \\\\\n\t\t\\midrule\n\t\t\\multirow{8}{*}{Positive samples}   & Wikipedia & 12.50 \\\\ \n                                            & Sina News & 12.50 \\\\\n                                    \t\t& Cbooks & 12.50  \\\\\n\t                                        & Zhihu & 12.40 \\\\\n\t                                        & WikiQA & 0.90 \\\\\n\t                                        & Law & 0.40 \\\\\n\t                                        & Poetry & 0.20 \\\\\n\t                                        & GovReport & 0.13 \\\\\n\t    \\midrule\n            \\xrowht[()]{10pt}\n\t\tNegative sample & CC-sampling & 55.00 \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}\n\t\\caption{Composition of BERTEval Training Data.}\\label{bert_data}\n\t\\centering\n\t\n\t\t\\toprule\n\t\tType     & Source     & Size ($\\times 10^4 $\\times 10^4 ) \\\\\n\t\t\\midrule\n\t\t\\multirow{8}8{*}*{Positive samples}Positive samples   & Wikipedia & 12.50 \\\\ \n                                            & Sina News & 12.50 \\\\\n                                    \t\t& Cbooks & 12.50  \\\\\n\t                                        & Zhihu & 12.40 \\\\\n\t                                        & WikiQA & 0.90 \\\\\n\t                                        & Law & 0.40 \\\\\n\t                                        & Poetry & 0.20 \\\\\n\t                                        & GovReport & 0.13 \\\\\n\t    \\midrule\n            \\xrowht[()]{10pt}10pt\n\t\tNegative sample & CC-sampling & 55.00 \\\\\n\t\t\\bottomrule\n\t\n\n\n\n\\textbf{BERTEval Architecture} We utilized Tran-BERT-MS-ML-R\\cite{wang_use_2022}, an effective AES model based on the BERT-base architecture, to evaluate the quality of the text obtained from web crawling. To reduce computational complexity, we opted to exclude the sub-document scale representation in Tran-BERT-MS-ML-R, which employs text segmentation at various scales as input. Instead, we focused solely on the text-scale representation, utilizing the $[CLS]$[CLS] embedding to extract pertinent information and structural features from the broadest perspective of the text. Simultaneously, the token-scale representation was derived from the sequence outputs of BERT. We anticipate that this token-scale representation will be instrumental in identifying and filtering out texts containing offensive language, sexually explicit terms, and frequent nonsensical vocabulary, typically absent in high-quality corpora. Let $x$x represent the text input. After the application of Max Pooling, the token-scale representation is concatenated with the text-scale representation and passed through a Dense Layer with Sigmoid activation, producing a text quality score $f(x|W)$f(x|W) that falls within the $(0, 1)$(0, 1) range.\n\n\n\n \n\n\n\\textbf{Loss Function} In addition to the MSE loss\\cite{mesgar_neural_2018}, we used the following two loss functions: Margin Ranking ($MR$MR) loss \\cite{liu2021temp} and Cosine Similarity ($CS$CS) loss \\cite{wang_use_2022}. Let $D$D denote the CommonCrawl corpus. Each negative sample $x_n$x_n is sampled from CommonCrawl associated with a fixed low score label $y_n$y_n, which formed $D_n$D_n. The positive sample, $x_p$x_p, denotes curated corpora with ideal quality with a constant score $y_p$y_p, which formed $D_p$D_p. Throughout the training process, the labels for positive and negative samples are persistently $y_p$y_p and $y_n$y_n, respectively. Moreover, it's noteworthy that a significant portion of high-quality texts is present within the CommonCrawl corpus. Given that the supervision employed is coarse-grained even somewhat inaccurate, it would be imprudent to solely rely on MSE loss to rigidly compel the model to fit these labels. For the $ML$ML loss, losses only emerge when the ranking of quality scores for samples within each batch doesn't align with their respective labels. The $CS$CS loss evaluates the correlation between quality scores and their supervision, rather than their absolute differences. Therefore, the combined loss is \n\\begin{equation}\n    \\mathcal{L}(\\boldsymbol{Y}, f(\\boldsymbol{X }| W)) =\\alpha MSE(\\boldsymbol{Y}, f(\\boldsymbol{X} | W))+\\beta MR(\\boldsymbol{Y}, f(\\boldsymbol{X} | W))+ \\gamma CS(\\boldsymbol{Y}, f(\\boldsymbol{X} | W)),\n\\end{equation}\\begin{equation}\n    \\mathcal{L}(\\boldsymbol{Y}, f(\\boldsymbol{X }| W)) =\\alpha MSE(\\boldsymbol{Y}, f(\\boldsymbol{X} | W))+\\beta MR(\\boldsymbol{Y}, f(\\boldsymbol{X} | W))+ \\gamma CS(\\boldsymbol{Y}, f(\\boldsymbol{X} | W)),\n\\end{equation}\n    \\mathcal{L}(\\boldsymbol{Y}Y, f(\\boldsymbol{X }X | W)) =\\alpha MSE(\\boldsymbol{Y}Y, f(\\boldsymbol{X}X | W))+\\beta MR(\\boldsymbol{Y}Y, f(\\boldsymbol{X}X | W))+ \\gamma CS(\\boldsymbol{Y}Y, f(\\boldsymbol{X}X | W)),\n\n\nwhere $\\boldsymbol{Y}$\\boldsymbol{Y}Y and $f(\\boldsymbol{X} | W)$f(\\boldsymbol{X}X | W) are the quality score labels and predictions of a batch, respectively. Given the supervision is coarse-grained, we contend that the combined loss function offers valuable insights for enhancing our capability of BERTEval to assess the relative quality of texts. The BERTEval training process consists of the following two stages, which are shown in Fig 2.\n\n\\textbf{Pre-training Stage} At this stage, we extracted positive and negative samples at a 1:1 ratio from the CommonCrawl corpus and the curated corpora. We trained BERTEval based on the loss functions previously described. After this training stage, BERTEval acquired a preliminary ability to discern the quality of web-scraped texts, which will be elaborated in detail in the subsequent experiments section. \n\n\\textbf{Self-training Stage} As mentioned before, there is a considerable proportion of texts with desired quality in the Common corpus $D_n$D_n, which might introduce inaccurate supervision, resulting in neither increasing the epoch nor scaling up the training set leading to a detectable improvement in BERTEval. To ameliorate this problem, we adopt a self-training approach \\cite{scudder1965probability}. Let $S_n$S_n denote a randomly sampled subset of $D_n$D_n. In each self-training iteration, the parameters of BERTEval from the previous iteration, $W^t$W^t, are used to generate the pseudo labels of $S_n$S_n, and then BERTEval is retrained on sampled data in $S_n$S_n with pseudo labels to learn the new parameters $W^{t + 1}$W^{t + 1}t + 1\\cite{mukherjee_uncertainty-aware_2020}. It's worth noting that since the positive samples originate from large-scale, high-reliability, curated corpora that do not require pseudo-labeling, we exclusively sample instances with pseudo-labels being $y_n$y_n. The self-training stage can be formulated as:\n\\begin{equation}\nW^{t+1} = \\mathop{\\arg\\min} \\limits_{W} \\mathbb{E}_{\\boldsymbol{X}_p^l \\subset D_p} \\mathbb{E}_{S_n \\subset D_n} \\mathbb{E}_{\\boldsymbol{X}_n^l \\sim p(x_n| W^{t}), \\boldsymbol{X}_n^l \\subset S_n}\\{\\mathcal{L}(\\boldsymbol{Y}_p^l \\oplus \\boldsymbol{Y}_n^l, f(\\boldsymbol{X}_p^l \\oplus \\boldsymbol{X}_n^l | W))\\}.\n\\end{equation}\\begin{equation}\nW^{t+1} = \\mathop{\\arg\\min} \\limits_{W} \\mathbb{E}_{\\boldsymbol{X}_p^l \\subset D_p} \\mathbb{E}_{S_n \\subset D_n} \\mathbb{E}_{\\boldsymbol{X}_n^l \\sim p(x_n| W^{t}), \\boldsymbol{X}_n^l \\subset S_n}\\{\\mathcal{L}(\\boldsymbol{Y}_p^l \\oplus \\boldsymbol{Y}_n^l, f(\\boldsymbol{X}_p^l \\oplus \\boldsymbol{X}_n^l | W))\\}.\n\\end{equation}\nW^{t+1}t+1 = \\mathop{\\arg\\min}\\arg\\min \\limits_{W}W \\mathbb{E}_{\\boldsymbol{X}_p^l \\subset D_p}\\boldsymbol{X}X_p^l \\subset D_p \\mathbb{E}_{S_n \\subset D_n}S_n \\subset D_n \\mathbb{E}_{\\boldsymbol{X}_n^l \\sim p(x_n| W^{t}), \\boldsymbol{X}_n^l \\subset S_n}\\boldsymbol{X}X_n^l \\sim p(x_n| W^{t}t), \\boldsymbol{X}X_n^l \\subset S_n\\{\\mathcal{L}(\\boldsymbol{Y}Y_p^l \\oplus \\boldsymbol{Y}Y_n^l, f(\\boldsymbol{X}X_p^l \\oplus \\boldsymbol{X}X_n^l | W))\\}.\n\nwhere $\\boldsymbol{X}_p^l$\\boldsymbol{X}X_p^l and $\\boldsymbol{X}_n^l$\\boldsymbol{X}X_n^l denote the vectors consisting of $l$l randomly sampled samples from $D_n$D_n and $S_n$S_n, respectively, and $\\boldsymbol{Y}_p^l$\\boldsymbol{Y}Y_p^l and $\\boldsymbol{Y}_n^l$\\boldsymbol{Y}Y_n^l are the corresponding constant vector labels. Since the output layer is activated by Sigmoid, the quality score $f(x | W)$f(x | W) can also be regarded as a probability of positive samples $p(y_p | x; W)$p(y_p | x; W). Based on the idea of preferring pseudo-labels with high confidence, in iteration $t$t, the probability of selecting each sample $x_n \\in S_n$x_n \\in S_n is denoted by \n\\begin{equation}\np(x_n|W^t) = \\frac{p(y_n|x_n; W)}{\\sum_{x \\in S_n} p(y_n|x; W)},\n\\end{equation}\\begin{equation}\np(x_n|W^t) = \\frac{p(y_n|x_n; W)}{\\sum_{x \\in S_n} p(y_n|x; W)},\n\\end{equation}\np(x_n|W^t) = \\frac{p(y_n|x_n; W)}{\\sum_{x \\in S_n} p(y_n|x; W)},\n\nwhich is the normalization of $p(y_n|x_n; W)$p(y_n|x_n; W). Heuristically, we avoid sampling the web-crawled texts where $p(y_n|x_n; W)$p(y_n|x_n; W) falls within the last $K$K proportions. The value of $K$K is informed by our sampling observations from the CommonCrawl dataset. \n\n\n\n\\subsubsection{FastText-based Evaluation Model}\n\n\\begin{table}[htbp]\n\t\\caption{Composition of FastText Training Data.}\\label{fasttext_data}\n\t\\centering\n\t\\begin{tabular}{llc}\n\t\t\\toprule\n\t\tType     & Source     & Size ($\\times 10^4 $) \\\\\n\t\t\\midrule\n\t\t\\multirow{8}{*}{Positive samples}   & Baike & 20 \\\\ \n                                            & Cbook & 20 \\\\\n                                    \t\t& Zhidao& 20  \\\\\n\t\t                                    & China News & 20 \\\\\n\t                                        & Zhihu & 20 \\\\\n\t                                        & WikiQA & 10 \\\\\n\t                                        & other news & 10 \\\\\n\t                                        & BERT-positive & 40 \\\\\n\t    \\midrule\n            \\xrowht[()]{10pt}\n\t\tNegative sample & BERT-negative & 160 \\\\\n\t\t\\bottomrule\n\t\\end{tabular}\n\\end{table}\n\t\\caption{Composition of FastText Training Data.}\\label{fasttext_data}\n\t\\centering\n\t\n\t\t\\toprule\n\t\tType     & Source     & Size ($\\times 10^4 $\\times 10^4 ) \\\\\n\t\t\\midrule\n\t\t\\multirow{8}8{*}*{Positive samples}Positive samples   & Baike & 20 \\\\ \n                                            & Cbook & 20 \\\\\n                                    \t\t& Zhidao& 20  \\\\\n\t\t                                    & China News & 20 \\\\\n\t                                        & Zhihu & 20 \\\\\n\t                                        & WikiQA & 10 \\\\\n\t                                        & other news & 10 \\\\\n\t                                        & BERT-positive & 40 \\\\\n\t    \\midrule\n            \\xrowht[()]{10pt}10pt\n\t\tNegative sample & BERT-negative & 160 \\\\\n\t\t\\bottomrule\n\t\n\n\nTo further enhance data processing efficiency and reduce hardware resource requirements, in this paper we also develop a text evaluation model based on FastText\\footnote{https://github.com/facebookresearch/fastText} in addition to BERTEval. FastText is libiary for efficient learning of word representations and text classificaiton. Compared to other classification models, such as SVM, logistic regression, and BERT, FastText could significantly reduce training and inference time while maintaining classification performance. \n\nIn the last section, we have built a BERT-based evaluation model BERTEval which performs a good performance on the quality evaluation of Chinese texts. With this model, we can classify the preprocessed web data into high-quality texts (positive) and low-quality texts (negative). Inspired by the idea of knowledge distillation, we will use these classified texts to guide the training of our FastText model. In our approach, we select 400,000 high-quality texts classified by BERTEval as our positive data, while choosing 1,600,000 low-quality texts as our negative data. In order to increase the diversity of training data, our positive data also include some high-quality Chinese data from some other websites and books, such as Baidubaike, Zhihu, Cbook, ChinaNews and so on. These data have been manually proofread and processed. In this way, we can build a good training dataset with 3200K samples. As shown in Table \\ref{fasttext_data}, it presents the composition of our training data.\n\nAfter collecting these training data, we will use a word segmentation tool to process all the texts, and then input the processed data into FastText to train the model. Through this approach, we can obtain a more efficient quality evaluation model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \\subsubsection{Evaluation Model Comparison}\n \n In order to compare the performance of different evaluation models, in this section we evaluate them on a test set which includes 300 samples. Here we first list other two baseline quality evaluation models: regression-based approach and perplexity-based approach.\n \n \\textbf{Regression-based Evaluator}\n \n Following the work of Gururangan et al. (2022) \\cite{gururangan_whose_2022}, we combine logistic regression with a word frequency-based vertorizaiton method to conduct text classification on the testset. In this approach, logistic regression is used to calculate a probability value for each sample, and then a threshold is adopted to determine whether the data point should be classified into positive or negative. \n \n\n \\textbf{Perplexity-based Evaluator}\n \n Perplexity could effectively measure the difficulty of a language model in predicting tokens and reflect the fluency of the input texts. Following the work of Wenzek et al. (2020) \\cite{wenzek_ccnet_2020}, we utilize a well-trained language model to calculate the perplexity of the texts and classify them with a threshold based on perplexity values. The samples with lower perplexity values will be classified into positive.\n \n \\textbf{Comparison Results}\n \n During testing procedure, we will classify the samples of testset with Regression, Perplexity, BERTEval and FastText models repectively, and then compute the precisions of them on positive data. As in Table \\ref{results}, it shows the precisions of different evaluaiton models on the testset. TP represents the number of \"True Positive\" samples, while FP represents the number of \"False Positive\" samples. From this table, we can see that our BERTEval evaluaiton model gets a much better performance than the regression and perplexity approaches. Besides, benefiting from the good classified results of our BERTEval model, the FastText-based model could further improve the classification precision. This result indicates that using BERTEval to guide the construction of the FastText-based evaluation model is effective. And with this FastText-based evaluation model, our EvalWeb tool-chain could achieve a better performance while effectively improving processing efficiency and resource utilization.\n\n\n\n\n\n \\begin{table}[htbp]\n \t\\caption{Classification results of different evaluation models.}\\label{results}\n \t\\centering\n \t\\begin{tabular}{lccc}\n \t\t\\toprule\n \t\tModel      & Precision(\\%) & TP+FP & TP \\\\\n \t\t\\midrule\\xrowht[()]{10pt}\n \t\tRegression & 49.57 & 234 & 116 \\\\\n \t\t\\xrowht[()]{10pt}\n \t\tPerplexity &  63.27 & 245 & 155 \\\\\n \t\t \\xrowht[()]{10pt}\n          BERTEval & 73.79 & 103 & 76\\\\\n \t\t\\xrowht[()]{10pt}\n        FastText&  \\textbf{81.58} & 76 & 62 \\\\\t  \n \t\t\\bottomrule\n \t\\end{tabular}\n \\end{table}\n \t\\caption{Classification results of different evaluation models.}\\label{results}\n \t\\centering\n \t\n \t\t\\toprule\n \t\tModel      & Precision(\\%) & TP+FP & TP \\\\\n \t\t\\midrule\\xrowht[()]{10pt}10pt\n \t\tRegression & 49.57 & 234 & 116 \\\\\n \t\t\\xrowht[()]{10pt}10pt\n \t\tPerplexity &  63.27 & 245 & 155 \\\\\n \t\t \\xrowht[()]{10pt}10pt\n          BERTEval & 73.79 & 103 & 76\\\\\n \t\t\\xrowht[()]{10pt}10pt\n        FastText&  \\textbf{81.58} & 76 & 62 \\\\\t  \n \t\t\\bottomrule\n \t\n \n\n\n\\subsection{Quality Control}\\label{quality_control}\n\n \\begin{figure}[htbp]\n  \\centering\n  \\includegraphics[width=0.9\\textwidth]{picture/human_quality_control_final.pdf}\n  \\caption{Quality Control.}\n  \\label{fig2}\n \\end{figure}\n  \\centering\n  \\includegraphics[width=0.9\\textwidth]{picture/human_quality_control_final.pdf}\n  \\caption{Quality Control.}\n  \\label{fig2}\n \n\nIn section 3.3, after filtering data with a desired quality threshold, we can obtain a Chinese text dataset. In order to ensure the quality of this dataset, We will hire some human evaluators to evaluate its quality. In this method, we will randomly sample 1000 examples from the dataset for three times. After that, three human evaluators will be hired to assess the quality of these data respectively, and the quality of these data is required to be evaluated from the following four aspects: \n\n\n\n\\begin{itemize}\n\\item \\textbf{Informativeness}: Whether the text contains enough knowledge and information,  or is just meaningless crap.\n\\item \\textbf{Fluency}: Whether the text has formatting issues, capitalization mistakes, or evident grammatical errors that impair readability.\n\\item \\textbf{Coherence}: Whether the text progressively forms a coherent body of information on a topic through its successive sentences. \n\\item \\textbf{Toxicity}: Texts used for pre-training should endeavor to exclude offensive remarks, sexually explicit content, and politically sensitive statements to mitigate potential generative risks.\n \\end{itemize}\\begin{itemize}\n\\item \\textbf{Informativeness}: Whether the text contains enough knowledge and information,  or is just meaningless crap.\n\\item \\textbf{Fluency}: Whether the text has formatting issues, capitalization mistakes, or evident grammatical errors that impair readability.\n\\item \\textbf{Coherence}: Whether the text progressively forms a coherent body of information on a topic through its successive sentences. \n\\item \\textbf{Toxicity}: Texts used for pre-training should endeavor to exclude offensive remarks, sexually explicit content, and politically sensitive statements to mitigate potential generative risks.\n \\end{itemize}\n\\item \\textbf{Informativeness}: Whether the text contains enough knowledge and information,  or is just meaningless crap.\n\\item \\textbf{Fluency}: Whether the text has formatting issues, capitalization mistakes, or evident grammatical errors that impair readability.\n\\item \\textbf{Coherence}: Whether the text progressively forms a coherent body of information on a topic through its successive sentences. \n\\item \\textbf{Toxicity}: Texts used for pre-training should endeavor to exclude offensive remarks, sexually explicit content, and politically sensitive statements to mitigate potential generative risks.\n \n \n \n \\begin{table}[htbp]\n    \\caption{Data samples in json format. The higher score of Sample 1 versus the lower score of Sample 2 demonstrates their differing text qualities, with Sample 1 having better quality than Sample 2. }\n    \\label{Data Example Format}\n    \\centering\n    \\begin{tabular}{p{2.5cm} p{12cm} }\n    \\toprule\n      \\textbf{Key} & \\textbf{Value}\\\\\n     \\midrule\n        <\\textbf{title}> &\\begin{CJK}{UTF8}{gbsn}\"\u6f4d\u574a\u94f6\u884c2021\u5e74\u4e0a\u534a\u5e74\u51c0\u5229\u6da6\u540c\u6bd4\u589e\u957f29.57\\% \u4e0d\u826f\u7387\u964d\u81f31.10\\%\\_\u8d22\u7ecf\\_\u4e2d\u56fd\u7f51\"\\end{CJK}\\\\\n        <\\textbf{score}>& 0.95 \\\\\n        <\\textbf{text}> & \\multicolumn{1}{m{12cm}}{\\begin{CJK}{UTF8}{gbsn} \"\u6f4d\u574a\u94f6\u884c2021\u5e74\u4e0a\u534a\u5e74\u51c0\u5229\u6da6\u540c\u6bd4\u589e\u957f29.57\\% \u4e0d\u826f\u7387\u964d\u81f31.10\\%\\textbackslash n\u4e2d\u56fd\u7f51\u8d22\u7ecf8\u670824\u65e5\u8baf \u6f4d\u574a\u94f6\u884c\u6628\u65e5\u62ab\u97322021\u5e74\u4e8c\u5b63\u5ea6\u4fe1\u606f\u62a5\u544a\u663e\u793a\uff0c\u622a\u81f32021\u5e746\u6708\u672b\uff0c\u6f4d\u574a\u94f6\u884c\u8d44\u4ea7\u603b\u989d1920.44\u4ebf\u5143\uff0c\u8f83\u4e0a\u5e74\u672b\u589e\u957f9.34\\%\uff1b\u8d1f\u503a\u603b\u989d1789.16\u4ebf\u5143\uff0c\u8f83\u4e0a\u5e74\u672b\u589e\u957f10.54\\%\u30022021\u5e74\u4e0a\u534a\u5e74\uff0c\u6f4d\u574a\u94f6\u884c\u5b9e\u73b0\u51c0\u5229\u6da66.09\u4ebf\u5143\uff0c\u540c\u6bd4\u589e\u957f29.57\\%\u3002\\textbackslash n\u8d44\u4ea7\u8d28\u91cf\u65b9\u9762\uff0c\u622a\u81f32021\u5e746\u6708\u672b\uff0c\u6f4d\u574a\u94f6\u884c\u4e0d\u826f\u8d37\u6b3e\u73871.10\\%\uff0c\u8f83\u4e0a\u5e74\u672b\u4e0b\u964d0.13\u4e2a\u767e\u5206\u70b9\u3002\\textbackslash n\u8d44\u672c\u91d1\u65b9\u9762\uff0c\u622a\u81f32021\u5e746\u6708\u672b\uff0c\u6f4d\u574a\u94f6\u884c\u8d44\u672c\u5145\u8db3\u7387\u3001\u6838\u5fc3\u4e00\u7ea7\u8d44\u672c\u5145\u8db3\u7387\u3001\u4e00\u7ea7\u8d44\u672c\u5145\u8db3\u7387\u5206\u522b\u4e3a11.66\\%\u30017.89\\%\u300110.13\\%\uff0c\u5206\u522b\u8f83\u4e0a\u5e74\u672b\u4e0b\u964d1.89\u30010.89\u30011.15\u4e2a\u767e\u5206\u70b9\u3002\" \\end{CJK}}  \\\\\n        <\\textbf{url}> & \\url{http://finance.china.com.cn/news/special/2021bnb/20210824/5638343.shtml}\\\\\n        <\\textbf{source\\_domain}> & \\url{finance.china.com.cn}\\\\\n         \\midrule\n        <\\textbf{title}> &\\begin{CJK}{UTF8}{gbsn}\"\u4e0a\u6d77\u5de8\u4e5f\u4eea\u5668\u8bbe\u5907\u6709\u9650\u516c\u53f8\"\\end{CJK}\\\\\n        <\\textbf{score}>& 0.19 \\\\\n        <\\textbf{text}> & \\multicolumn{1}{m{12cm}}{\\begin{CJK}{UTF8}{gbsn} \"\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u4eea\\textbackslash n\u73b0\u8d27\u63d0\u4f9b\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\\textbackslash n\u73b0\u8d27\u63d0\u4f9b\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\u7b26\u5408SAE\u3001ASTM\u3001VDA\u3001GM\u3001Ford\u3001Mazda\u3001JIS\u3001Nissan\u3001\u53caToyota\u7b49\u7684\u6d4b\u8bd5\u8981\u6c42\u3002\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\uff08\u6b22\u8fce\u5b9e\u5730\u8003\u5bdf\uff09\u6ee1\u8db3\uff1a\u5927\u4f17\u3001\u795e\u9f99\u3001\u901a\u7528\u3001\u65e5\u4ea7\u3001\u9a6c\u81ea\u8fbe\u3001\u4e30\u7530\u3001\u672c\u7530\u3001\u798f\u7279\u7b49\u6c7d\u8f66\u5382\u5bb6\u8bd5\u9a8c\u3002\\textbackslash nNS\u8010\u77f3\u5b50\u51b2\u51fb\u6027\u80fd\u8bd5\u9a8c\u673a\\textbackslash n\u8010\u77f3\u5b50\u51b2\u51fb\u6027\u80fd\u8bd5\u9a8c\u673a\u4e3b\u8981\u7528\u4e8e\u3001\u5fb7\u7cfb\u65e5\u7cfb\u3001\u7f8e\u7cfb\u6c7d\u8f66\u5382\u5bb6\u8bd5\u9a8c\u65b9\u6cd5\uff0c\u5b83\u80fd\u51c6\u786e\u518d\u73b0\u7531\u98de\u6e85\u7684\u7802\u783e\u9020\u6210\u7684\u7834\u5316\u73b0\u8c61, \u9002\u7528\u4e8e\u5916\u6d82\u5c42\u7c98\u805a\u6027\u7834\u574f\u8bd5\u9a8c\u3001\u6d82\u5c42\u7cfb\u7edf\u4e2d\u4e0d\u540c\u5c42\u95f4\u7c98\u5408\u6027\u7834\u574f\u8bd5\u9a8c\u3001\u6297\u5265\u843d\u7684*\u6d82\u819c\u539a\u5ea6\u3001\u5851\u6599\u53ca\u73bb\u7483\u7684\u6297\u5265\u843d\u3001\u6297\u78b0\u649e\u3001\u6297\u78e8\u635f\u6d4b\u8bd5\u7b49\u76f8\u5173\u8bd5\u9a8c\u3002\\textbackslash n\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\uff08\u6b22\u8fce\u5b9e\u5730\u8003\u5bdf\uff09\\textbackslash n\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\uff08\u6b22\u8fce\u5b9e\u5730\u8003\u5bdf\uff09\u7b26\u5408SAE\u3001ASTM\u3001VDA\u3001GM\u3001Ford\u3001Mazda\u3001JIS\u3001Nissan\u3001\u53caToyota\u7b49\u7684\u6d4b\u8bd5\u8981\u6c42\u3002\u6ee1\u8db3\uff1a\u5927\u4f17\u3001\u795e\u9f99\u3001\u901a\u7528\u3001\u65e5\u4ea7\u3001\u9a6c\u81ea\u8fbe\u3001\u4e30\u7530\u3001\u672c\u7530\u3001\u798f\u7279\u7b49\u6c7d\u8f66\u5382\u5bb6\u8bd5\u9a8c\u3002\\textbackslash n\u6f06\u819c\u6297\u77f3\u51fb\u8bd5\u9a8c\u4eea\\textbackslash n\u6f06\u819c\u6297\u77f3\u51fb\u8bd5\u9a8c\u4eea\u7b26\u5408SAE\u3001ASTM\u3001VDA\u3001GM\u3001Ford\u3001Mazda\u3001JIS\u3001Nissan\u3001\u53caToyota\u7b49\u7684\u6d4b\u8bd5\u8981\u6c42\u3002\\textbackslash n\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a/\u6297\u77f3\u5b50\u51b2\u51fb\u4eea\\textbackslash n\u5de8\u4e5f\u4eea\u5668\uff01\u6709\u5927\u91cf\u73b0\u8d27\u63d0\u4f9b\uff0c\u6b22\u8fce\u5ba2\u6237\u968f\u65f6\u6765\u5382\u53c2\u89c2\u4e0e\u6307\u5bfc\uff01\" \\end{CJK}}  \\\\\n        <\\textbf{url}> & \\url{http://www.juyesh.com/SonList-1094890.html}\\\\\n        <\\textbf{source\\_domain}> & \\url{www.juyesh.com}\\\\\n        \\bottomrule\n    \\end{tabular}\n\\end{table}\n    \\caption{Data samples in json format. The higher score of Sample 1 versus the lower score of Sample 2 demonstrates their differing text qualities, with Sample 1 having better quality than Sample 2. }\n    \\label{Data Example Format}\n    \\centering\n    \n    \\toprule\n      \\textbf{Key} & \\textbf{Value}\\\\\n     \\midrule\n        <\\textbf{title}> &{UTF8}UTF8{gbsn}gbsn\"\u6f4d\u574a\u94f6\u884c2021\u5e74\u4e0a\u534a\u5e74\u51c0\u5229\u6da6\u540c\u6bd4\u589e\u957f29.57\\% \u4e0d\u826f\u7387\u964d\u81f31.10\\%\\_\u8d22\u7ecf\\_\u4e2d\u56fd\u7f51\"\\\\\n        <\\textbf{score}>& 0.95 \\\\\n        <\\textbf{text}> & \\multicolumn{1}1{m{12cm}}m{12cm}12cm{\\begin{CJK}{UTF8}{gbsn} \"\u6f4d\u574a\u94f6\u884c2021\u5e74\u4e0a\u534a\u5e74\u51c0\u5229\u6da6\u540c\u6bd4\u589e\u957f29.57\\% \u4e0d\u826f\u7387\u964d\u81f31.10\\%\\textbackslash n\u4e2d\u56fd\u7f51\u8d22\u7ecf8\u670824\u65e5\u8baf \u6f4d\u574a\u94f6\u884c\u6628\u65e5\u62ab\u97322021\u5e74\u4e8c\u5b63\u5ea6\u4fe1\u606f\u62a5\u544a\u663e\u793a\uff0c\u622a\u81f32021\u5e746\u6708\u672b\uff0c\u6f4d\u574a\u94f6\u884c\u8d44\u4ea7\u603b\u989d1920.44\u4ebf\u5143\uff0c\u8f83\u4e0a\u5e74\u672b\u589e\u957f9.34\\%\uff1b\u8d1f\u503a\u603b\u989d1789.16\u4ebf\u5143\uff0c\u8f83\u4e0a\u5e74\u672b\u589e\u957f10.54\\%\u30022021\u5e74\u4e0a\u534a\u5e74\uff0c\u6f4d\u574a\u94f6\u884c\u5b9e\u73b0\u51c0\u5229\u6da66.09\u4ebf\u5143\uff0c\u540c\u6bd4\u589e\u957f29.57\\%\u3002\\textbackslash n\u8d44\u4ea7\u8d28\u91cf\u65b9\u9762\uff0c\u622a\u81f32021\u5e746\u6708\u672b\uff0c\u6f4d\u574a\u94f6\u884c\u4e0d\u826f\u8d37\u6b3e\u73871.10\\%\uff0c\u8f83\u4e0a\u5e74\u672b\u4e0b\u964d0.13\u4e2a\u767e\u5206\u70b9\u3002\\textbackslash n\u8d44\u672c\u91d1\u65b9\u9762\uff0c\u622a\u81f32021\u5e746\u6708\u672b\uff0c\u6f4d\u574a\u94f6\u884c\u8d44\u672c\u5145\u8db3\u7387\u3001\u6838\u5fc3\u4e00\u7ea7\u8d44\u672c\u5145\u8db3\u7387\u3001\u4e00\u7ea7\u8d44\u672c\u5145\u8db3\u7387\u5206\u522b\u4e3a11.66\\%\u30017.89\\%\u300110.13\\%\uff0c\u5206\u522b\u8f83\u4e0a\u5e74\u672b\u4e0b\u964d1.89\u30010.89\u30011.15\u4e2a\u767e\u5206\u70b9\u3002\" \\end{CJK}}{UTF8}UTF8{gbsn}gbsn \"\u6f4d\u574a\u94f6\u884c2021\u5e74\u4e0a\u534a\u5e74\u51c0\u5229\u6da6\u540c\u6bd4\u589e\u957f29.57\\% \u4e0d\u826f\u7387\u964d\u81f31.10\\%\\textbackslash n\u4e2d\u56fd\u7f51\u8d22\u7ecf8\u670824\u65e5\u8baf \u6f4d\u574a\u94f6\u884c\u6628\u65e5\u62ab\u97322021\u5e74\u4e8c\u5b63\u5ea6\u4fe1\u606f\u62a5\u544a\u663e\u793a\uff0c\u622a\u81f32021\u5e746\u6708\u672b\uff0c\u6f4d\u574a\u94f6\u884c\u8d44\u4ea7\u603b\u989d1920.44\u4ebf\u5143\uff0c\u8f83\u4e0a\u5e74\u672b\u589e\u957f9.34\\%\uff1b\u8d1f\u503a\u603b\u989d1789.16\u4ebf\u5143\uff0c\u8f83\u4e0a\u5e74\u672b\u589e\u957f10.54\\%\u30022021\u5e74\u4e0a\u534a\u5e74\uff0c\u6f4d\u574a\u94f6\u884c\u5b9e\u73b0\u51c0\u5229\u6da66.09\u4ebf\u5143\uff0c\u540c\u6bd4\u589e\u957f29.57\\%\u3002\\textbackslash n\u8d44\u4ea7\u8d28\u91cf\u65b9\u9762\uff0c\u622a\u81f32021\u5e746\u6708\u672b\uff0c\u6f4d\u574a\u94f6\u884c\u4e0d\u826f\u8d37\u6b3e\u73871.10\\%\uff0c\u8f83\u4e0a\u5e74\u672b\u4e0b\u964d0.13\u4e2a\u767e\u5206\u70b9\u3002\\textbackslash n\u8d44\u672c\u91d1\u65b9\u9762\uff0c\u622a\u81f32021\u5e746\u6708\u672b\uff0c\u6f4d\u574a\u94f6\u884c\u8d44\u672c\u5145\u8db3\u7387\u3001\u6838\u5fc3\u4e00\u7ea7\u8d44\u672c\u5145\u8db3\u7387\u3001\u4e00\u7ea7\u8d44\u672c\u5145\u8db3\u7387\u5206\u522b\u4e3a11.66\\%\u30017.89\\%\u300110.13\\%\uff0c\u5206\u522b\u8f83\u4e0a\u5e74\u672b\u4e0b\u964d1.89\u30010.89\u30011.15\u4e2a\u767e\u5206\u70b9\u3002\"   \\\\\n        <\\textbf{url}> & \\url{http://finance.china.com.cn/news/special/2021bnb/20210824/5638343.shtml}\\\\\n        <\\textbf{source\\_domain}> & \\url{finance.china.com.cn}\\\\\n         \\midrule\n        <\\textbf{title}> &{UTF8}UTF8{gbsn}gbsn\"\u4e0a\u6d77\u5de8\u4e5f\u4eea\u5668\u8bbe\u5907\u6709\u9650\u516c\u53f8\"\\\\\n        <\\textbf{score}>& 0.19 \\\\\n        <\\textbf{text}> & \\multicolumn{1}1{m{12cm}}m{12cm}12cm{\\begin{CJK}{UTF8}{gbsn} \"\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u4eea\\textbackslash n\u73b0\u8d27\u63d0\u4f9b\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\\textbackslash n\u73b0\u8d27\u63d0\u4f9b\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\u7b26\u5408SAE\u3001ASTM\u3001VDA\u3001GM\u3001Ford\u3001Mazda\u3001JIS\u3001Nissan\u3001\u53caToyota\u7b49\u7684\u6d4b\u8bd5\u8981\u6c42\u3002\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\uff08\u6b22\u8fce\u5b9e\u5730\u8003\u5bdf\uff09\u6ee1\u8db3\uff1a\u5927\u4f17\u3001\u795e\u9f99\u3001\u901a\u7528\u3001\u65e5\u4ea7\u3001\u9a6c\u81ea\u8fbe\u3001\u4e30\u7530\u3001\u672c\u7530\u3001\u798f\u7279\u7b49\u6c7d\u8f66\u5382\u5bb6\u8bd5\u9a8c\u3002\\textbackslash nNS\u8010\u77f3\u5b50\u51b2\u51fb\u6027\u80fd\u8bd5\u9a8c\u673a\\textbackslash n\u8010\u77f3\u5b50\u51b2\u51fb\u6027\u80fd\u8bd5\u9a8c\u673a\u4e3b\u8981\u7528\u4e8e\u3001\u5fb7\u7cfb\u65e5\u7cfb\u3001\u7f8e\u7cfb\u6c7d\u8f66\u5382\u5bb6\u8bd5\u9a8c\u65b9\u6cd5\uff0c\u5b83\u80fd\u51c6\u786e\u518d\u73b0\u7531\u98de\u6e85\u7684\u7802\u783e\u9020\u6210\u7684\u7834\u5316\u73b0\u8c61, \u9002\u7528\u4e8e\u5916\u6d82\u5c42\u7c98\u805a\u6027\u7834\u574f\u8bd5\u9a8c\u3001\u6d82\u5c42\u7cfb\u7edf\u4e2d\u4e0d\u540c\u5c42\u95f4\u7c98\u5408\u6027\u7834\u574f\u8bd5\u9a8c\u3001\u6297\u5265\u843d\u7684*\u6d82\u819c\u539a\u5ea6\u3001\u5851\u6599\u53ca\u73bb\u7483\u7684\u6297\u5265\u843d\u3001\u6297\u78b0\u649e\u3001\u6297\u78e8\u635f\u6d4b\u8bd5\u7b49\u76f8\u5173\u8bd5\u9a8c\u3002\\textbackslash n\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\uff08\u6b22\u8fce\u5b9e\u5730\u8003\u5bdf\uff09\\textbackslash n\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\uff08\u6b22\u8fce\u5b9e\u5730\u8003\u5bdf\uff09\u7b26\u5408SAE\u3001ASTM\u3001VDA\u3001GM\u3001Ford\u3001Mazda\u3001JIS\u3001Nissan\u3001\u53caToyota\u7b49\u7684\u6d4b\u8bd5\u8981\u6c42\u3002\u6ee1\u8db3\uff1a\u5927\u4f17\u3001\u795e\u9f99\u3001\u901a\u7528\u3001\u65e5\u4ea7\u3001\u9a6c\u81ea\u8fbe\u3001\u4e30\u7530\u3001\u672c\u7530\u3001\u798f\u7279\u7b49\u6c7d\u8f66\u5382\u5bb6\u8bd5\u9a8c\u3002\\textbackslash n\u6f06\u819c\u6297\u77f3\u51fb\u8bd5\u9a8c\u4eea\\textbackslash n\u6f06\u819c\u6297\u77f3\u51fb\u8bd5\u9a8c\u4eea\u7b26\u5408SAE\u3001ASTM\u3001VDA\u3001GM\u3001Ford\u3001Mazda\u3001JIS\u3001Nissan\u3001\u53caToyota\u7b49\u7684\u6d4b\u8bd5\u8981\u6c42\u3002\\textbackslash n\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a/\u6297\u77f3\u5b50\u51b2\u51fb\u4eea\\textbackslash n\u5de8\u4e5f\u4eea\u5668\uff01\u6709\u5927\u91cf\u73b0\u8d27\u63d0\u4f9b\uff0c\u6b22\u8fce\u5ba2\u6237\u968f\u65f6\u6765\u5382\u53c2\u89c2\u4e0e\u6307\u5bfc\uff01\" \\end{CJK}}{UTF8}UTF8{gbsn}gbsn \"\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u4eea\\textbackslash n\u73b0\u8d27\u63d0\u4f9b\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\\textbackslash n\u73b0\u8d27\u63d0\u4f9b\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\u7b26\u5408SAE\u3001ASTM\u3001VDA\u3001GM\u3001Ford\u3001Mazda\u3001JIS\u3001Nissan\u3001\u53caToyota\u7b49\u7684\u6d4b\u8bd5\u8981\u6c42\u3002\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\uff08\u6b22\u8fce\u5b9e\u5730\u8003\u5bdf\uff09\u6ee1\u8db3\uff1a\u5927\u4f17\u3001\u795e\u9f99\u3001\u901a\u7528\u3001\u65e5\u4ea7\u3001\u9a6c\u81ea\u8fbe\u3001\u4e30\u7530\u3001\u672c\u7530\u3001\u798f\u7279\u7b49\u6c7d\u8f66\u5382\u5bb6\u8bd5\u9a8c\u3002\\textbackslash nNS\u8010\u77f3\u5b50\u51b2\u51fb\u6027\u80fd\u8bd5\u9a8c\u673a\\textbackslash n\u8010\u77f3\u5b50\u51b2\u51fb\u6027\u80fd\u8bd5\u9a8c\u673a\u4e3b\u8981\u7528\u4e8e\u3001\u5fb7\u7cfb\u65e5\u7cfb\u3001\u7f8e\u7cfb\u6c7d\u8f66\u5382\u5bb6\u8bd5\u9a8c\u65b9\u6cd5\uff0c\u5b83\u80fd\u51c6\u786e\u518d\u73b0\u7531\u98de\u6e85\u7684\u7802\u783e\u9020\u6210\u7684\u7834\u5316\u73b0\u8c61, \u9002\u7528\u4e8e\u5916\u6d82\u5c42\u7c98\u805a\u6027\u7834\u574f\u8bd5\u9a8c\u3001\u6d82\u5c42\u7cfb\u7edf\u4e2d\u4e0d\u540c\u5c42\u95f4\u7c98\u5408\u6027\u7834\u574f\u8bd5\u9a8c\u3001\u6297\u5265\u843d\u7684*\u6d82\u819c\u539a\u5ea6\u3001\u5851\u6599\u53ca\u73bb\u7483\u7684\u6297\u5265\u843d\u3001\u6297\u78b0\u649e\u3001\u6297\u78e8\u635f\u6d4b\u8bd5\u7b49\u76f8\u5173\u8bd5\u9a8c\u3002\\textbackslash n\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\uff08\u6b22\u8fce\u5b9e\u5730\u8003\u5bdf\uff09\\textbackslash n\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a\uff08\u6b22\u8fce\u5b9e\u5730\u8003\u5bdf\uff09\u7b26\u5408SAE\u3001ASTM\u3001VDA\u3001GM\u3001Ford\u3001Mazda\u3001JIS\u3001Nissan\u3001\u53caToyota\u7b49\u7684\u6d4b\u8bd5\u8981\u6c42\u3002\u6ee1\u8db3\uff1a\u5927\u4f17\u3001\u795e\u9f99\u3001\u901a\u7528\u3001\u65e5\u4ea7\u3001\u9a6c\u81ea\u8fbe\u3001\u4e30\u7530\u3001\u672c\u7530\u3001\u798f\u7279\u7b49\u6c7d\u8f66\u5382\u5bb6\u8bd5\u9a8c\u3002\\textbackslash n\u6f06\u819c\u6297\u77f3\u51fb\u8bd5\u9a8c\u4eea\\textbackslash n\u6f06\u819c\u6297\u77f3\u51fb\u8bd5\u9a8c\u4eea\u7b26\u5408SAE\u3001ASTM\u3001VDA\u3001GM\u3001Ford\u3001Mazda\u3001JIS\u3001Nissan\u3001\u53caToyota\u7b49\u7684\u6d4b\u8bd5\u8981\u6c42\u3002\\textbackslash n\u77f3\u5b50\u51b2\u51fb\u8bd5\u9a8c\u673a/\u6297\u77f3\u5b50\u51b2\u51fb\u4eea\\textbackslash n\u5de8\u4e5f\u4eea\u5668\uff01\u6709\u5927\u91cf\u73b0\u8d27\u63d0\u4f9b\uff0c\u6b22\u8fce\u5ba2\u6237\u968f\u65f6\u6765\u5382\u53c2\u89c2\u4e0e\u6307\u5bfc\uff01\"   \\\\\n        <\\textbf{url}> & \\url{http://www.juyesh.com/SonList-1094890.html}\\\\\n        <\\textbf{source\\_domain}> & \\url{www.juyesh.com}\\\\\n        \\bottomrule\n    \n \n \n \n During evaluation procedure, each text is assigned a label of either \"True\" or \"False.\" \"True\" indicates that the data meets the quality requirement of pre-training in all four aspects, while \"False\" signifies that the text is noisy to some extent. After completing all the evaluations, we will calculate the average accuracy of these three evaluators. If the average accuracy could exceed 0.9, the filtered Chinese dataset is considered to be a high-quality dataset. Otherwise, we believe that there is still some noisy text in the dataset, and we need to optimize the preprocessing and evaluation modules again, and reprocess the dataset until it meets the quality requirements. The architecture of quality control process is illustrated in Figure \\ref{fig2}. \n \n \n \\begin{table}[htbp]\n    \\caption{Overview of output datasets.}\n    \\label{total_remain_size}\n    \\centering\n    \\begin{tabular}{cp{2cm}p{2cm}p{2cm}}\n    \\toprule\n    \\multirow{3.5}{*}{Snapshot}   & \\multicolumn{3}{c}{Data Size(GB)} \\\\\n                                \\cmidrule{2-4}\n                                & \\centering Monolingual Chinese Data & \\centering ChineseWebText Dataset & \\centering Cleaner Subset \\arraybackslash \\\\\n    \\midrule \n    \\xrowht[()]{5pt}\n    \\textbf{2021-43} & \\centering 505.92 & \\centering 187.57 & \\centering 78.95 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2022-05} & \\centering 442.47 & \\centering 164.96 & \\centering 69.44 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2022-21} & \\centering 443.57 & \\centering 166.75 & \\centering 70.19 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2022-27} & \\centering 417.95 & \\centering 149.41 & \\centering 62.70 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2022-33} & \\centering 369.56 & \\centering 123.70 & \\centering 51.98 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2022-49} & \\centering 445.29 & \\centering 160.87 & \\centering 67.76 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2023-06} & \\centering 396.40 & \\centering 173.47 & \\centering 74.19 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2023-14} & \\centering 441.46 & \\centering 150.04 & \\centering 63.33 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}\n    \\textbf{2023-23} & \\centering 371.96 & \\centering 143.93 & \\centering 61.28 \\arraybackslash\\\\\n    \\midrule\\xrowht[()]{5pt}\n    \\textbf{Total} & \\centering 3834.58 & \\centering 1420.70 & \\centering 599.82 \\arraybackslash\\\\\n    % \\midrule\n    \\bottomrule\n    \\end{tabular}\n\\end{table}\n    \\caption{Overview of output datasets.}\n    \\label{total_remain_size}\n    \\centering\n    \n    \\toprule\n    \\multirow{3.5}3.5{*}*{Snapshot}Snapshot   & \\multicolumn{3}3{c}c{Data Size(GB)}Data Size(GB) \\\\\n                                \\cmidrule{2-4}2-4\n                                & \\centering Monolingual Chinese Data & \\centering ChineseWebText Dataset & \\centering Cleaner Subset \\arraybackslash \\\\\n    \\midrule \n    \\xrowht[()]{5pt}5pt\n    \\textbf{2021-43} & \\centering 505.92 & \\centering 187.57 & \\centering 78.95 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}5pt\n    \\textbf{2022-05} & \\centering 442.47 & \\centering 164.96 & \\centering 69.44 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}5pt\n    \\textbf{2022-21} & \\centering 443.57 & \\centering 166.75 & \\centering 70.19 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}5pt\n    \\textbf{2022-27} & \\centering 417.95 & \\centering 149.41 & \\centering 62.70 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}5pt\n    \\textbf{2022-33} & \\centering 369.56 & \\centering 123.70 & \\centering 51.98 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}5pt\n    \\textbf{2022-49} & \\centering 445.29 & \\centering 160.87 & \\centering 67.76 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}5pt\n    \\textbf{2023-06} & \\centering 396.40 & \\centering 173.47 & \\centering 74.19 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}5pt\n    \\textbf{2023-14} & \\centering 441.46 & \\centering 150.04 & \\centering 63.33 \\arraybackslash\\\\\n     \\xrowht[()]{5pt}5pt\n    \\textbf{2023-23} & \\centering 371.96 & \\centering 143.93 & \\centering 61.28 \\arraybackslash\\\\\n    \\midrule\\xrowht[()]{5pt}5pt\n    \\textbf{Total} & \\centering 3834.58 & \\centering 1420.70 & \\centering 599.82 \\arraybackslash\\\\\n    \\bottomrule\n    \n\n \n \\begin{table*}[htb]\n\\centering\n    %\\small\n     \\caption{ The comparison of different pre-training datasets.}\n    \\label{Dataset Compare}\n    \\begin{tabular}{ccccc}\n    \\toprule\n    \\textbf{Dataset} & \\textbf{Lang.} &\\textbf{Availability}& \\textbf{Pubilc Size} & \\textbf{Scoring} \\\\\n    \\midrule\n    C4\\cite{2020T5C4}   & EN & Public &  807GB  & NO   \\\\  \n    The Pile\\cite{2020_pile} & EN & Public & 825GB  &   NO \\\\ \n    REFINEDWEB\\cite{2023refinedweb} & EN & Public& 2.8TB    &  NO  \\\\\n    WuDaoCorpora\\cite{2021WuDaoCorpora}  & ZH & Pratly Public & 200GB  &  NO \\\\\n    ROOTS-zh\\cite{2023roots} & ZH & Public &  265GB   &  NO\\\\\n    WanJuan1.0-zh\\cite{2023wanjuan}   & ZH & Public & 550GB  & NO\\\\\n    \\textbf{ChineseWebText} (Ours)        & ZH & Public & 1.4 TB &  YES \\\\\n    \\textbf{Cleaner Subset} (Ours)        & ZH & Public & 600 GB &  YES \\\\\n    \\bottomrule\n    \\end{tabular}\n   \n\\end{table*}\n\\centering\n    \\caption{ The comparison of different pre-training datasets.}\n    \\label{Dataset Compare}\n    \n    \\toprule\n    \\textbf{Dataset} & \\textbf{Lang.} &\\textbf{Availability}& \\textbf{Pubilc Size} & \\textbf{Scoring} \\\\\n    \\midrule\n    C4\\cite{2020T5C4}   & EN & Public &  807GB  & NO   \\\\  \n    The Pile\\cite{2020_pile} & EN & Public & 825GB  &   NO \\\\ \n    REFINEDWEB\\cite{2023refinedweb} & EN & Public& 2.8TB    &  NO  \\\\\n    WuDaoCorpora\\cite{2021WuDaoCorpora}  & ZH & Pratly Public & 200GB  &  NO \\\\\n    ROOTS-zh\\cite{2023roots} & ZH & Public &  265GB   &  NO\\\\\n    WanJuan1.0-zh\\cite{2023wanjuan}   & ZH & Public & 550GB  & NO\\\\\n    \\textbf{ChineseWebText} (Ours)        & ZH & Public & 1.4 TB &  YES \\\\\n    \\textbf{Cleaner Subset} (Ours)        & ZH & Public & 600 GB &  YES \\\\\n    \\bottomrule\n    \n   \n\n \n \n\\subsection{Dataset Statistics and Comparison}\\label{dataset_statistics_comparison}\n\n\nAfter processing the collected CommonCrawl data with preprocessing and quality evaluation modules, this paper constructs a clean Chinese dataset ChineseWebText, which consists of 1.42 TB data. As shown in Table \\ref{Data Example Format}, each text in this dataset is assigned a quality score which is generated by the quality evaluation model BERTEval. In this table, a larger quality score signifies a higher text quality. With these quality scores, LLM researchers could further select data according to a desired quality threshold. In addition to ChineseWebText, this paper also release a much cleaner subset of nearly 600 GB Chinese texts, which is built by choosing data from ChineseWebText with quality scores in the top 40\\%. Through manual evaluations with three evaluators, the accuracy of this cleaner subset reaches 90\\%. Table \\ref{total_remain_size} shows the details of our datasets, which are extracted from nine CommonCrawl snapshots. \n\n\n\n \n\n\n\n\n\n \n\n\n\n\nIn Table \\ref{Dataset Compare}, we compare our datasets with some other public pre-training corpora. In these work, the researchers first collect raw data from different sources, such as BookCorpus, Github, Arxiv, PubMed Central, CommonCrawl and so on. And then they clean them with some well-designed rules and algorithms. Specifically, C4\\cite{2020T5C4}, The Pile\\cite{2020_pile} and REFINEDWEB\\cite{2023refinedweb} are three public English datasets, while WuDaoCorpora\\cite{2021WuDaoCorpora}, ROOTS-zh\\cite{2023roots} and WanJuan1.0-zh\\cite{2023wanjuan} are three corpora for Chinese. From this table, we can see that our datasets are the latest and largest Chinese datasets. Besides, different with these previous datasets, each text in our datasets is also assigned a quality score, which could allow LLM researchers to choose data according to a new quality threshold.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "appendix": false}, "Data Analysis": {"content": "\n\n\n\\subsection{Removal Rate for Different Stages}\n\n\n\n\nTo more precisely introduce our data processing workflow, we show Table \\ref{remain_size}, which details the remaining data size and its corresponding filtering ratio for each preprocessing step and quality evaluation module. In addition, we further depict the processing workflow and the removal rate of each step in Figure \\ref{fig4removal-rate}, thereby providing a high-level overview of the entire process. In each step, we show the removal ratio of data from the previous step and the absolute percentage of the remaining data from the original CommonCrawl. This facilitates readers in conveniently tracking the various processing stages from the raw data to the final data. \n\nSpecifically, since the proportion of Chinese data is relatively low in the original CommonCrawl dataset, a large amount of data is filtered out during the preparation stage, retaining only about 4.65\\% of the original data. In preprocessing stage, data is filtered in several steps. In the step of text extraction, we aim to extract all the text content and remove  redundant content generated in the preparation stage, such as useless key-value pairs. Based on this, a variety of manually defined criteria are employed to further refine the dataset, targeting the elimination of texts that either possess limited informative value, contain sensitive or inappropriate content, exhibit a low percentage of Chinese characters, or display redundant characteristics. Due to the presence of numerous entries containing traditional Chinese characters, the step of filtering based on character proportion results in a large proportion of data being cleaned up. After the preprocessing stage, we score each text of the remaining data using our evaluation model, and then construct the ChineseWebText dataset of 1.4 TB. Finally, We select the top 40\\% of data based on the quality scores to construct a higher-quality subset of 600GB, which accounts for only 0.73\\% of the original CommonCrawl data.\n\n\n\n\\begin{table}[htbp]\n    \\small\n    \\caption{The remaining data size and filtering ratio for each preprocessing step and quality evaluation module.}\n    \\label{remain_size}\n    \\centering\n    \\begin{tabular}{cp{2cm}p{1.5cm}p{1.2cm}p{2cm}p{1.5cm}p{1.5cm}p{1.5cm}}\n    \\toprule\n    \\multirow{3.2}{*}{Snapshot} & \\multicolumn{7}{c}{Size After filtering operation(GB)} \\\\\n                                \\cmidrule{2-8}\n                                & \\centering Monolingual Chinese Data & \\centering Text Extraction & \\centering Data Length & \\centering Proportion of Characters & \\centering Sensitive Words & \\centering Internal Duplication  & \\centering Quality Evaluation \\arraybackslash \\\\\n    \\midrule\n    \\textbf{2021-43} & \\centering 505.92 & \\centering 424.43 & \\centering 409.68 & \\centering 217.52 & \\centering 192.84 & \\centering 187.57 & \\centering 78.95 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-16.11\\%}} & \\centering \\textcolor{gray}{\\textit{-3.48\\%}} & \\centering \\textcolor{gray}{\\textit{-46.90\\%}} & \\centering \\textcolor{gray}{\\textit{-11.35\\%}} & \\centering \\textcolor{gray}{\\textit{-2.73\\%}} & \\centering \\textcolor{gray}{\\textit{-57.91\\%}} \\arraybackslash \\\\\n    \\textbf{2022-05} & \\centering 442.47 & \\centering 375.64 & \\centering 362.34 & \\centering 182.88 & \\centering 169.01 & \\centering 164.96 & \\centering 69.44 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-15.10\\%}} & \\centering \\textcolor{gray}{\\textit{-3.54\\%}} & \\centering \\textcolor{gray}{\\textit{-49.53\\%}} & \\centering \\textcolor{gray}{\\textit{-7.58\\%}} & \\centering \\textcolor{gray}{\\textit{-2.40\\%}} & \\centering \\textcolor{gray}{\\textit{-57.90\\%}} \\arraybackslash \\\\\n    \\textbf{2022-21} & \\centering 443.57 & \\centering 363.33 & \\centering 348.51 & \\centering 178.16 & \\centering 170.09 & \\centering 166.75 & \\centering 70.19 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-18.09\\%}} & \\centering \\textcolor{gray}{\\textit{-4.08\\%}} & \\centering \\textcolor{gray}{\\textit{-48.88\\%}} & \\centering \\textcolor{gray}{\\textit{-4.53\\%}} & \\centering \\textcolor{gray}{\\textit{-1.96\\%}} & \\centering \\textcolor{gray}{\\textit{-57.91\\%}} \\arraybackslash \\\\\n    \\textbf{2022-27} & \\centering 417.95 & \\centering 340.65 & \\centering 326.52 & \\centering 158.83 & \\centering 152.33 & \\centering 149.41 & \\centering 62.7 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-18.50\\%}} & \\centering \\textcolor{gray}{\\textit{-4.15\\%}} & \\centering \\textcolor{gray}{\\textit{-51.36\\%}} & \\centering \\textcolor{gray}{\\textit{-4.09\\%}} & \\centering \\textcolor{gray}{\\textit{-1.92\\%}} & \\centering \\textcolor{gray}{\\textit{-58.03\\%}} \\arraybackslash \\\\\n    \\textbf{2022-33} & \\centering 369.56 & \\centering 293.07 & \\centering 280.58 & \\centering 131.39 & \\centering 125.84 & \\centering 123.70 & \\centering 51.98 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-20.70\\%}} & \\centering \\textcolor{gray}{\\textit{-4.26\\%}} & \\centering \\textcolor{gray}{\\textit{-53.17\\%}} & \\centering \\textcolor{gray}{\\textit{-4.22\\%}} & \\centering \\textcolor{gray}{\\textit{-1.70\\%}} & \\centering \\textcolor{gray}{\\textit{-57.98\\%}} \\arraybackslash \\\\\n    \\textbf{2022-49} & \\centering 445.29 & \\centering 367.73 & \\centering 352.59 & \\centering 173.86 & \\centering 164.34 & \\centering 160.87 & \\centering 67.76 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-17.42\\%}} & \\centering \\textcolor{gray}{\\textit{-4.12\\%}} & \\centering \\textcolor{gray}{\\textit{-50.69\\%}} & \\centering \\textcolor{gray}{\\textit{-5.48\\%}} & \\centering \\textcolor{gray}{\\textit{-2.11\\%}} & \\centering \\textcolor{gray}{\\textit{-57.88\\%}} \\arraybackslash \\\\\n    \\textbf{2023-06} & \\centering 396.40 & \\centering 275.04 & \\centering 263.59 & \\centering 211.10 & \\centering 177.44 & \\centering 173.47 & \\centering 74.19 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-30.62\\%}} & \\centering \\textcolor{gray}{\\textit{-4.16\\%}} & \\centering \\textcolor{gray}{\\textit{-19.91\\%}} & \\centering \\textcolor{gray}{\\textit{-15.95\\%}} & \\centering \\textcolor{gray}{\\textit{-2.24\\%}} & \\centering \\textcolor{gray}{\\textit{-57.23\\%}} \\arraybackslash \\\\\n    \\textbf{2023-14} & \\centering 441.46 & \\centering 368.40 & \\centering 354.18 & \\centering 161.54 & \\centering 153.27 & \\centering 150.04 & \\centering 63.33 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-16.55\\%}} & \\centering \\textcolor{gray}{\\textit{-3.86\\%}} & \\centering \\textcolor{gray}{\\textit{-54.39\\%}} & \\centering \\textcolor{gray}{\\textit{-5.12\\%}} & \\centering \\textcolor{gray}{\\textit{-2.11\\%}} & \\centering \\textcolor{gray}{\\textit{-57.79\\%}} \\arraybackslash \\\\\n    \\textbf{2023-23} & \\centering 371.96 & \\centering 305.10 & \\centering 292.58 & \\centering 152.20 & \\centering 146.90 & \\centering 143.93 & \\centering 61.28 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-17.98\\%}} & \\centering \\textcolor{gray}{\\textit{-4.10\\%}} & \\centering \\textcolor{gray}{\\textit{-47.98\\%}} & \\centering \\textcolor{gray}{\\textit{-3.48\\%}} & \\centering \\textcolor{gray}{\\textit{-2.02\\%}} & \\centering \\textcolor{gray}{\\textit{-57.42\\%}} \\arraybackslash \\\\\n    \\midrule\n    % Total   & 3835.32 & 1514.33(-2320.99) & 1507.54(-6.79) & 1466.25(-41.29) & 1450.90(-15.35) & 1420.70(-30.20) \\\\\n    \\textbf{Total} & \\centering 3834.58 & \\centering 3113.39 & \\centering 2990.57 & \\centering 1567.48 & \\centering 1452.06 & \\centering 1420.70 & \\centering 599.82 \\arraybackslash \\\\\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-18.81\\%}} & \\centering \\textcolor{gray}{\\textit{-3.94\\%}} & \\centering \\textcolor{gray}{\\textit{-47.59\\%}} & \\centering \\textcolor{gray}{\\textit{-7.36\\%}} & \\centering \\textcolor{gray}{\\textit{-2.16\\%}} & \\centering \\textcolor{gray}{\\textit{-57.78\\%}} \\arraybackslash \\\\\n    \\bottomrule\n    \\end{tabular}\n\\end{table}\n    \\small\n    \\caption{The remaining data size and filtering ratio for each preprocessing step and quality evaluation module.}\n    \\label{remain_size}\n    \\centering\n    \n    \\toprule\n    \\multirow{3.2}3.2{*}*{Snapshot}Snapshot & \\multicolumn{7}7{c}c{Size After filtering operation(GB)}Size After filtering operation(GB) \\\\\n                                \\cmidrule{2-8}2-8\n                                & \\centering Monolingual Chinese Data & \\centering Text Extraction & \\centering Data Length & \\centering Proportion of Characters & \\centering Sensitive Words & \\centering Internal Duplication  & \\centering Quality Evaluation \\arraybackslash \\\\\n    \\midrule\n    \\textbf{2021-43} & \\centering 505.92 & \\centering 424.43 & \\centering 409.68 & \\centering 217.52 & \\centering 192.84 & \\centering 187.57 & \\centering 78.95 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}7pt\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-16.11\\%}} & \\centering \\textcolor{gray}{\\textit{-3.48\\%}} & \\centering \\textcolor{gray}{\\textit{-46.90\\%}} & \\centering \\textcolor{gray}{\\textit{-11.35\\%}} & \\centering \\textcolor{gray}{\\textit{-2.73\\%}} & \\centering \\textcolor{gray}{\\textit{-57.91\\%}} \\arraybackslash \\\\\n    \\textbf{2022-05} & \\centering 442.47 & \\centering 375.64 & \\centering 362.34 & \\centering 182.88 & \\centering 169.01 & \\centering 164.96 & \\centering 69.44 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}7pt\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-15.10\\%}} & \\centering \\textcolor{gray}{\\textit{-3.54\\%}} & \\centering \\textcolor{gray}{\\textit{-49.53\\%}} & \\centering \\textcolor{gray}{\\textit{-7.58\\%}} & \\centering \\textcolor{gray}{\\textit{-2.40\\%}} & \\centering \\textcolor{gray}{\\textit{-57.90\\%}} \\arraybackslash \\\\\n    \\textbf{2022-21} & \\centering 443.57 & \\centering 363.33 & \\centering 348.51 & \\centering 178.16 & \\centering 170.09 & \\centering 166.75 & \\centering 70.19 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}7pt\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-18.09\\%}} & \\centering \\textcolor{gray}{\\textit{-4.08\\%}} & \\centering \\textcolor{gray}{\\textit{-48.88\\%}} & \\centering \\textcolor{gray}{\\textit{-4.53\\%}} & \\centering \\textcolor{gray}{\\textit{-1.96\\%}} & \\centering \\textcolor{gray}{\\textit{-57.91\\%}} \\arraybackslash \\\\\n    \\textbf{2022-27} & \\centering 417.95 & \\centering 340.65 & \\centering 326.52 & \\centering 158.83 & \\centering 152.33 & \\centering 149.41 & \\centering 62.7 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}7pt\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-18.50\\%}} & \\centering \\textcolor{gray}{\\textit{-4.15\\%}} & \\centering \\textcolor{gray}{\\textit{-51.36\\%}} & \\centering \\textcolor{gray}{\\textit{-4.09\\%}} & \\centering \\textcolor{gray}{\\textit{-1.92\\%}} & \\centering \\textcolor{gray}{\\textit{-58.03\\%}} \\arraybackslash \\\\\n    \\textbf{2022-33} & \\centering 369.56 & \\centering 293.07 & \\centering 280.58 & \\centering 131.39 & \\centering 125.84 & \\centering 123.70 & \\centering 51.98 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}7pt\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-20.70\\%}} & \\centering \\textcolor{gray}{\\textit{-4.26\\%}} & \\centering \\textcolor{gray}{\\textit{-53.17\\%}} & \\centering \\textcolor{gray}{\\textit{-4.22\\%}} & \\centering \\textcolor{gray}{\\textit{-1.70\\%}} & \\centering \\textcolor{gray}{\\textit{-57.98\\%}} \\arraybackslash \\\\\n    \\textbf{2022-49} & \\centering 445.29 & \\centering 367.73 & \\centering 352.59 & \\centering 173.86 & \\centering 164.34 & \\centering 160.87 & \\centering 67.76 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}7pt\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-17.42\\%}} & \\centering \\textcolor{gray}{\\textit{-4.12\\%}} & \\centering \\textcolor{gray}{\\textit{-50.69\\%}} & \\centering \\textcolor{gray}{\\textit{-5.48\\%}} & \\centering \\textcolor{gray}{\\textit{-2.11\\%}} & \\centering \\textcolor{gray}{\\textit{-57.88\\%}} \\arraybackslash \\\\\n    \\textbf{2023-06} & \\centering 396.40 & \\centering 275.04 & \\centering 263.59 & \\centering 211.10 & \\centering 177.44 & \\centering 173.47 & \\centering 74.19 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}7pt\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-30.62\\%}} & \\centering \\textcolor{gray}{\\textit{-4.16\\%}} & \\centering \\textcolor{gray}{\\textit{-19.91\\%}} & \\centering \\textcolor{gray}{\\textit{-15.95\\%}} & \\centering \\textcolor{gray}{\\textit{-2.24\\%}} & \\centering \\textcolor{gray}{\\textit{-57.23\\%}} \\arraybackslash \\\\\n    \\textbf{2023-14} & \\centering 441.46 & \\centering 368.40 & \\centering 354.18 & \\centering 161.54 & \\centering 153.27 & \\centering 150.04 & \\centering 63.33 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}7pt\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-16.55\\%}} & \\centering \\textcolor{gray}{\\textit{-3.86\\%}} & \\centering \\textcolor{gray}{\\textit{-54.39\\%}} & \\centering \\textcolor{gray}{\\textit{-5.12\\%}} & \\centering \\textcolor{gray}{\\textit{-2.11\\%}} & \\centering \\textcolor{gray}{\\textit{-57.79\\%}} \\arraybackslash \\\\\n    \\textbf{2023-23} & \\centering 371.96 & \\centering 305.10 & \\centering 292.58 & \\centering 152.20 & \\centering 146.90 & \\centering 143.93 & \\centering 61.28 \\arraybackslash \\\\\n    \\xrowht[()]{7pt}7pt\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-17.98\\%}} & \\centering \\textcolor{gray}{\\textit{-4.10\\%}} & \\centering \\textcolor{gray}{\\textit{-47.98\\%}} & \\centering \\textcolor{gray}{\\textit{-3.48\\%}} & \\centering \\textcolor{gray}{\\textit{-2.02\\%}} & \\centering \\textcolor{gray}{\\textit{-57.42\\%}} \\arraybackslash \\\\\n    \\midrule\n    \\textbf{Total} & \\centering 3834.58 & \\centering 3113.39 & \\centering 2990.57 & \\centering 1567.48 & \\centering 1452.06 & \\centering 1420.70 & \\centering 599.82 \\arraybackslash \\\\\n    \\textit{removal rate} & \\centering - & \\centering \\textcolor{gray}{\\textit{-18.81\\%}} & \\centering \\textcolor{gray}{\\textit{-3.94\\%}} & \\centering \\textcolor{gray}{\\textit{-47.59\\%}} & \\centering \\textcolor{gray}{\\textit{-7.36\\%}} & \\centering \\textcolor{gray}{\\textit{-2.16\\%}} & \\centering \\textcolor{gray}{\\textit{-57.78\\%}} \\arraybackslash \\\\\n    \\bottomrule\n    \n\n\n\n\\begin{figure}[htbp]\n  \\centering\n  \\includegraphics[width=0.95\\textwidth]{picture/Ratio.pdf}\n  \\caption{Removal rate for different stages. Grey represents the removal rate with respect to each previous step, while other colors represent the kept rate of all data.}\n  \\label{fig4removal-rate}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.95\\textwidth]{picture/Ratio.pdf}\n  \\caption{Removal rate for different stages. Grey represents the removal rate with respect to each previous step, while other colors represent the kept rate of all data.}\n  \\label{fig4removal-rate}\n\n\n\n\n\n\\subsection{Data Quality Distribution}\n\nTo investigate the relationship between data quality and data quantity, in this section, we adopt different quality thresholds to select data from our ChineseWebText dataset. As shown in Table \\ref{data quality distribution}, it presents the high-quality data size for different threshold values. In this table, the value of threshold represents the proportion of selected data in the overall dataset. Then, we hire three human evaluators to assess the quality of the selected data for each threshold. The evaluation criteria has been outlined in section 3.4. \n\n\n\\begin{table*}[htb]\n\\centering\n    %\\small\n     \\caption{The data quality distribution with different quality threshold.}\n    \\label{data quality distribution}\n    \\begin{tabular}{cccccc}\n    \\toprule\n    \\multirow{2.5}{*}{\\textbf{Threshold}} & \\multirow{2.5}{*}{\\textbf{High Quality Data Size}}&         \\multicolumn{4}{c}{\\textbf{Accuracy}}\\\\\n    \\cmidrule{3-6}\n    & & \\#1 &  \\#2 & \\#3 &{Average} \\\\\n    \\midrule \n    25\\% & 376.90 GB & 92.80\\% & 94.80\\% & 94.90\\% & 94.17\\%  \\\\  \n    35\\% & 525.59 GB & 93.60\\% & 93.60\\% & 93.10\\% & 93.43\\% \\\\ \n    40\\% & 599.82 GB & 90.30\\% & 91.90\\% & 89.50\\% & 90.57\\%  \\\\\n    45\\% & 672.68 GB & 84.60\\% & 85.50\\% & 85.59\\% & 85.33\\% \\\\\n    \\bottomrule\n    \\end{tabular}\n   \n\\end{table*}\n\\centering\n    \\caption{The data quality distribution with different quality threshold.}\n    \\label{data quality distribution}\n    \n    \\toprule\n    \\multirow{2.5}2.5{*}*{\\textbf{Threshold}}\\textbf{Threshold} & \\multirow{2.5}2.5{*}*{\\textbf{High Quality Data Size}}\\textbf{High Quality Data Size}&         \\multicolumn{4}4{c}c{\\textbf{Accuracy}}\\textbf{Accuracy}\\\\\n    \\cmidrule{3-6}3-6\n    & & \\#1 &  \\#2 & \\#3 &{Average}Average \\\\\n    \\midrule \n    25\\% & 376.90 GB & 92.80\\% & 94.80\\% & 94.90\\% & 94.17\\%  \\\\  \n    35\\% & 525.59 GB & 93.60\\% & 93.60\\% & 93.10\\% & 93.43\\% \\\\ \n    40\\% & 599.82 GB & 90.30\\% & 91.90\\% & 89.50\\% & 90.57\\%  \\\\\n    45\\% & 672.68 GB & 84.60\\% & 85.50\\% & 85.59\\% & 85.33\\% \\\\\n    \\bottomrule\n    \n   \n\n\n\n\nFrom Table \\ref{data quality distribution},\nwe can observe that a lower threshold leads to higher data quality, but results in smaller data size. For example, when we keep top 25\\% of our ChineseWebText, the quality can be higher than 94\\%, but the remaining data only accounts for 376.9 GB. The threshold of 40\\% seems to be a good choice, and it can balance the data scale and data quality. The data size can reaches about 600 GB and the average data quality with human evaluation can be higher than 90\\%. Therefore, this threshold is selected to construct the cleaner subset which is released along with our ChineseWebText. Anyway, our ChineseWebText could facilitate the LLMs researchers to choose their own high-quality dataset with their desired threshold.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\subsection{Data Length Distribution}\n\nDuring the training procedure of LLMs, longer texts can provide more abundant knowledge and information, making it easier for the models to understand complex relationships in the text, and learn more knowledge. In this section, we will analyze the length distribution of the texts in our cleaner dataset. As shown in Figure  \\ref{fig4length}, it illustrates the distribution of text lengths within our cleaner Chinese subset.  From this figure, we can observe that the majority of text lengths are mainly distributed within 1000 characters or less. Among them, the most significant proportion is observed within the length interval of 300 to 500 characters. Texts exceeding 1000 characters account for a relatively small portion, and there is a long tail of very long texts. After analysis, we found that the maximum text length in this dataset can reach 300,000 characters. However, they are considered to be outliers and excluded from this figure. The text length distribution in this figure could provide valuable insights into the structure and characteristics of our cleaner subset, thereby help researchers in understanding the composition of the processed dataset and facilitating the utilization of the dataset.\n\n\n\n\n\n\\begin{figure}[htbp]\n  \\centering\n  \\includegraphics[width=0.6\\textwidth]{picture/Length_analyze_bar.pdf}\n  \\caption{Length distribution of Data.}\n  \\label{fig4length}\n\\end{figure}\n  \\centering\n  \\includegraphics[width=0.6\\textwidth]{picture/Length_analyze_bar.pdf}\n  \\caption{Length distribution of Data.}\n  \\label{fig4length}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "appendix": false}, "Conclusions and Future Work": {"content": "\nIn order to extract large-scale and high-quality Chinese pre-training data from the web, we have proposed a new pipeline approach which filter the raw crawled web data with both handcrafted rules and well-designed quality evaluation model. The rules are employed to first extract the Chinese texts and remove duplicate documents, and then filter out the explicit noisy contents such as toxic texts and advertisements. The quality evaluation model is designed based on BERT and can assign each text with a quality score. With the proposed approach, we release the latest and largest Chinese dataset of 1.4 TB, each of which is associated with a quality score, facilitating the LLMs researchers to re-filter the data with desired quality thresholds. We further release a much cleaner subset of 600 GB Chinese data with the quality exceeding 90\\% by human evaluation. We also release the complete tool-chain that processes the raw data into the clean texts.\n\nIn the future, we will continue to enlarge the Chinese dataset with newly incoming web data. Meanwhile, we are going to explore better algorithms and strategies for data filtering. For example, we can design quality evaluation models for each kind of data noise.\n\n\\bibliographystyle{unsrt}unsrt\n\n\\bibliography{references}  \n\n\n\n\n\n\n\n\n\n\n\n\n\n", "appendix": false}}, "categories": ["cs.CL"], "published": "2023-11-02 11:13:51+00:00", "primary_category": "cs.CL", "summary": "During the development of large language models (LLMs), the scale and quality\nof the pre-training data play a crucial role in shaping LLMs' capabilities. To\naccelerate the research of LLMs, several large-scale datasets, such as C4 [1],\nPile [2], RefinedWeb [3] and WanJuan [4], have been released to the public.\nHowever, most of the released corpus focus mainly on English, and there is\nstill lack of complete tool-chain for extracting clean texts from web data.\nFurthermore, fine-grained information of the corpus, e.g. the quality of each\ntext, is missing. To address these challenges, we propose in this paper a new\ncomplete tool-chain EvalWeb to extract Chinese clean texts from noisy web data.\nFirst, similar to previous work, manually crafted rules are employed to discard\nexplicit noisy texts from the raw crawled web contents. Second, a well-designed\nevaluation model is leveraged to assess the remaining relatively clean data,\nand each text is assigned a specific quality score. Finally, we can easily\nutilize an appropriate threshold to select the high-quality pre-training data\nfor Chinese. Using our proposed approach, we release the largest and latest\nlarge-scale high-quality Chinese web text ChineseWebText, which consists of\n1.42 TB and each text is associated with a quality score, facilitating the LLM\nresearchers to choose the data according to the desired quality thresholds. We\nalso release a much cleaner subset of 600 GB Chinese data with the quality\nexceeding 90%."}