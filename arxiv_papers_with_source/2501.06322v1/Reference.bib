
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries

@Article{Abril07,
  author        = "Patricia S. Abril and Robert Plant",
  title         = "The patent holder's dilemma: Buy, sell, or troll?",
  journal       = "Communications of the ACM",
  volume        = "50",
  number        = "1",
  month         = jan,
  year          = "2007",
  pages         = "36--44",
  note          = "",
}

@Article{Cohen07,
  author        = "Sarah Cohen and Werner Nutt and Yehoshua Sagic",
  title         = "Deciding equivalances among conjunctive aggregate queries",
  journal       = JACM,
  articleno     = 5,
  numpages      = 50,
  volume        = 54,
  number        = 2,
  month         = apr,
  year          = 2007,
  acmid         = 1219093,
}


@periodical{JCohen96,
  key =          "Cohen",
  editor =       "Jacques Cohen",
  title =        "Special issue: Digital Libraries",
  journal =      CACM,
  volume =       "39",
  number =       "11",
  month =        nov,
  year =         "1996",
}


@Book{Kosiur01,
  author =       "David Kosiur",
  title =        "Understanding Policy-Based Networking",
  publisher =    "Wiley",
  year =         "2001",
  address =      "New York, NY",
  edition =      "2nd.",
  editor =       "",
  volume =       "",
  number =       "",
  series =       "",
  month =        "",
  note =         "",
}


@Book{Harel79,
  author =       "David Harel",
  year =         "1979",
  title =        "First-Order Dynamic Logic",
  series =       "Lecture Notes in Computer Science",
  volume =       "68",
  address =      "New York, NY",
  publisher =    "Springer-Verlag",
  editor =       "",
  number =       "",
  month =        "",
  note =         "",
}


@Inbook{Editor00,
  author =       "",
  editor =       "Ian Editor",
  title =        "The title of book one",
  subtitle =     "The book subtitle",
  series =       "The name of the series one",
  year =         "2007",
  volume =       "9",
  address =      "Chicago",
  edition =      "1st.",
  publisher =    "University of Chicago Press",
  chapter =      "",
  pages =        "",
  number =       "",
  type =         "",
  month =        "",
  note =         "",
}

%
@InBook{Editor00a,
  author =       "",
  editor =       "Ian Editor",
  title =        "The title of book two",
  subtitle =     "The book subtitle",
  series =       "The name of the series two",
  year =         "2008",
  address =      "Chicago",
  edition =      "2nd.",
  publisher =    "University of Chicago Press",
  doi =          "10.1007/3-540-09237-4",
  url =          "http://dx.doi.org/10.1007/3-540-09456-9",
  volume =       "",
  chapter =      "100",
  pages =        "",
  number =       "",
  type =         "",
  month =        "",
  note =         "",
}


% incollection (has an editor, title, and possibly a booktitle)
@Incollection{Spector90,
  author =       "Asad Z. Spector",
  title =        "Achieving application requirements",
  booktitle =    "Distributed Systems",
  publisher =    "ACM Press",
  address =      "New York, NY",
  year =         "1990",
  edition =      "2nd.",
  chapter =      "",
  editor =       "Sape Mullender",
  pages =        "19--33",
  doi =          "10.1145/90417.90738",
  url =          "http://doi.acm.org/10.1145/90417.90738",
  volume =       "",
  number =       "",
  series =       "",
  type =         "",
  month =        "",
  note =         "",
}


% incollection (has an editor, title, and possibly a booktitle)
@Incollection{Douglass98,
  author =       "Bruce P. Douglass and David Harel and Mark B. Trakhtenbrot",
  title =        "Statecarts in use: structured analysis and object-orientation",
  series =       "Lecture Notes in Computer Science",
  booktitle =    "Lectures on Embedded Systems",
  publisher =    "Springer-Verlag",
  address =      "London",
  volume =       "1494",
  year =         "1998",
  chapter =      "",
  editor =       "Grzegorz Rozenberg and Frits W. Vaandrager",
  pages =        "368--394",
  doi =          "10.1007/3-540-65193-4_29",
  url =          "http://dx.doi.org/10.1007/3-540-65193-4_29",
  edition =      "",
  number =       "",
  type =         "",
  month =        "",
  note =         "",
}


@Book{Knuth97,
  author =       "Donald E. Knuth",
  title =        "The Art of Computer Programming, Vol. 1: Fundamental Algorithms (3rd. ed.)",
  publisher =    "Addison Wesley Longman Publishing Co., Inc.",
  year =         "1997",
  address =      "",
  edition =      "",
  editor =       "",
  volume =       "",
  number =       "",
  series =       "",
  month =        "",
  note =         "",
}


@Book{Knuth98,
  author =       "Donald E. Knuth",
  year =         "1998",
  title =        "The Art of Computer Programming",
  series =       "Fundamental Algorithms",
  volume =       "1",
  edition =      "3rd",
  address =      "",
  publisher =    "Addison Wesley Longman Publishing Co., Inc.",
  doi =          "",
  url =          "",
  editor =       "",
  number =       "",
  month =        "",
  note =         "(book)",
}

%Inbook{Knuth97,
%  author =       "Donald E. Knuth",
%  title =        "The Art of Computer Programming",
%  booktitle =    "the booktitle",
%  edition =      "3",
%  volume =       "1",
%  year =         "1997",
%  publisher =    "Addison Wesley Longman Publishing Co., Inc.",
%  editor =       "",
%  number =       "",
%  series =       "Fundamental Algorithms",
%  type =         "",
%  chapter =      "",
%  pages =        "",
%  address =      "",
%  month =        "",
%  note =         "(inbook)",
%}

%INBOOK{DK:73-inbook-full,
%   author = "Donald E. Knuth",
%   title = "Fundamental Algorithms (inbook w series)",
%   volume = 1,
%   series = "The Art of Computer Programming",
%   publisher = "Addison-Wesley",
%   address = "Reading, Massachusetts",
%   edition = "Second",
%   month = "10~" # jan,
%   year = "1973",
%   type = "Section",
%   chapter = "1.2",
%   pages = "10--119",
%   note = "Full INBOOK entry (w series)",
%}

%INcollection{DK:74-incoll,
%   author = "Donald E. Knuth",
%   title = "Fundamental Algorithms (incoll)",
%   volume = 1,
%   booktitle = "The Art of Computer Programming",
%   publisher = "Addison-Wesley",
%   address = "Reading, Massachusetts",
%   month = "10~" # jan,
%   year = "1974",
%   pages = "10--119",
%   editor = "Bernard Rous",
%   note = "This is a full incoll entry with an editor",
%}

%INcollection{DK:75-incollws,
%   author = "Donald E. Knuth",
%   title = "Fundamental Algorithms (incoll w series)",
%   volume = 1,
%   booktitle = "The Art of Computer Programming",
%   series = "The Art of Computer Programming",
%   publisher = "Addison-Wesley",
%   address = "Reading, Massachusetts",
%   month = "10~" # jan,
%   year = "1975",
%   pages = "10--119",
%   editor = "Bernard Rous",
%   note = "This is a full incoll entry with an editor and series",
%}


@incollection{GM05,
Author= "Dan Geiger and Christopher Meek",
Title= "Structured Variational Inference Procedures and their Realizations (as incol)",
Year= 2005,
Booktitle="Proceedings of Tenth International Workshop on Artificial Intelligence and Statistics, {\rm The Barbados}",
Publisher="The Society for Artificial Intelligence and Statistics",
Month= jan,
Editors= "Z. Ghahramani and R. Cowell"
}

@Inproceedings{Smith10,
  author =       "Stan W. Smith",
  title =        "An experiment in bibliographic mark-up: Parsing metadata for XML export",
  booktitle =    "Proceedings of the 3rd. annual workshop on Librarians and Computers",
  series =       "LAC '10",
  editor =       "Reginald N. Smythe and Alexander Noble",
  volume =       "3",
  year =         "2010",
  publisher =    "Paparazzi Press",
  address =      "Milan Italy",
  pages =        "422--431",
  doi =          "99.9999/woot07-S422",
  url =          "http://dx.doi.org/99.0000/woot07-S422",
  number =       "",
  month =        "",
  organization = "",
  note =         "",
}

@Inproceedings{VanGundy07,
  author =       "Matthew Van Gundy and Davide Balzarotti and Giovanni Vigna",
  year =         2007,
  title =        "Catch me, if you can: Evading network signatures with web-based polymorphic worms",
  booktitle =    "Proceedings of the first USENIX workshop on Offensive Technologies",
  series =       "WOOT '07",
  publisher =    "USENIX Association",
  address =      "Berkley, CA",
  articleno =    {Paper 7},
  numpages =     9,
}

@Inproceedings{VanGundy08,
  author =       "Matthew Van Gundy and Davide Balzarotti and Giovanni Vigna",
  year =         2008,
  title =        "Catch me, if you can: Evading network signatures with web-based polymorphic worms",
  booktitle =    "Proceedings of the first USENIX workshop on Offensive Technologies",
  series =       "WOOT '08",
  publisher =    "USENIX Association",
  address =      "Berkley, CA",
  articleno =    7,
  numpages =     2,
  pages =        "99-100",
}

@Inproceedings{VanGundy09,
  author =       "Matthew Van Gundy and Davide Balzarotti and Giovanni Vigna",
  year =         2009,
  title =        "Catch me, if you can: Evading network signatures with web-based polymorphic worms",
  booktitle =    "Proceedings of the first USENIX workshop on Offensive Technologies",
  series =       "WOOT '09",
  publisher =    "USENIX Association",
  address =      "Berkley, CA",
  pages =        "90--100",
}

@Inproceedings{Andler79,
  author =       "Sten Andler",
  title =        "Predicate Path expressions",
  booktitle =    "Proceedings of the 6th. ACM SIGACT-SIGPLAN symposium on Principles of Programming Languages",
  series =       "POPL '79",
  year =         "1979",
  publisher =    "ACM Press",
  address =      "New York, NY",
  pages =        "226--236",
  doi =          "10.1145/567752.567774",
  url =          "http://doi.acm.org/10.1145/567752.567774",
  editor =       "",
  volume =       "",
  number =       "",
  month =        "",
  organization = "",
  note =         "",
}

@Techreport{Harel78,
  author =       "David Harel",
  year =         "1978",
  title =        "LOGICS of Programs: AXIOMATICS and DESCRIPTIVE POWER",
  institution =  "Massachusetts Institute of Technology",
  type =         "MIT Research Lab Technical Report",
  number =       "TR-200",
  address =      "Cambridge, MA",
  month =        "",
  note =         "",
}

@MASTERSTHESIS{anisi03,
author = {David A. Anisi},
title = {Optimal Motion Control of a Ground Vehicle},
school = {Royal Institute of Technology (KTH), Stockholm, Sweden},
intitution = {FOI-R-0961-SE, Swedish Defence Research Agency (FOI)},
year = {2003},
}


@Phdthesis{Clarkson85,
  author =       "Kenneth L. Clarkson",
  year =         "1985",
  title =        "Algorithms for Closest-Point Problems (Computational Geometry)",
  school =       "Stanford University",
  address =      "Palo Alto, CA",
  note =         "UMI Order Number: AAT 8506171",
  type =         "",
  month =        "",
}


@online{Thornburg01,
  author =       "Harry Thornburg",
  year =         "2001",
  title =        "Introduction to Bayesian Statistics",
  url =          "http://ccrma.stanford.edu/~jos/bayes/bayes.html",
  month =        mar,
  lastaccessed = "March 2, 2005",
}


@online{Ablamowicz07,
  author =       "Rafal Ablamowicz and Bertfried Fauser",
  year =         "2007",
  title =        "CLIFFORD: a Maple 11 Package for Clifford Algebra Computations, version 11",
  url =          "http://math.tntech.edu/rafal/cliff11/index.html",
  lastaccessed = "February 28, 2008",
}


@misc{Poker06,
  author =       "Poker-Edge.Com",
  year =         "2006",
  month =        mar,
  title =        "Stats and Analysis",
  lastaccessed = "June 7, 2006",
  url =          "http://www.poker-edge.com/stats.php",
}

@misc{Obama08,
  author        = "Barack Obama",
  year          = "2008",
  title         = "A more perfect union",
  howpublished  = "Video",
  day           = "5",
  url           = "http://video.google.com/videoplay?docid=6528042696351994555",
  month         = mar,
  lastaccessed  = "March 21, 2008",
  note          =  "",
}

@misc{JoeScientist001,
  author =       "Joseph Scientist",
  year =         "2009",
  title =        "The fountain of youth",
  note =         "Patent No. 12345, Filed July 1st., 2008, Issued Aug. 9th., 2009",
  url =          "",
  howpublished = "",
  month =        aug,
  lastaccessed = "",
}


@Inproceedings{Novak03,
  author =       "Dave Novak",
  title =        "Solder man",
  booktitle =    "ACM SIGGRAPH 2003 Video Review on Animation theater Program: Part I - Vol. 145 (July 27--27, 2003)",
  year =         "2003",
  publisher =    "ACM Press",
  address =      "New York, NY",
  pages =        "4",
  month =        "March 21, 2008",
  doi =          "99.9999/woot07-S422",
  url =          "http://video.google.com/videoplay?docid=6528042696351994555",
  note =         "",
  howpublished = "Video",
  editor =       "",
  volume =       "",
  number =       "",
  series =       "",
  organization = "",
  distinctURL = 1
}


@article{Lee05,
  author =       "Newton Lee",
  year =         "2005",
  title =        "Interview with Bill Kinder: January 13, 2005",
  journal =      "Comput. Entertain.",
  eid =          "4",
  volume =       "3",
  number =       "1",
  month =        "Jan.-March",
  doi =          "10.1145/1057270.1057278",
  url =          "http://doi.acm.org/10.1145/1057270.1057278",
  howpublished = "Video",
  note =         "",
}

@article{rous08,
  author =       "Bernard Rous",
  year =         "2008",
  title =        "The Enabling of Digital Libraries",
  journal =      "Digital Libraries",
  volume =       "12",
  number =       "3",
  month =        jul,
  articleno =    "Article~5",
  doi =          "",
  url =          "",
  howpublished = "",
  note =         "To appear",
}

@article{384253,
 author = {Werneck,, Renato and Setubal,, Jo\~{a}o and da Conceic\~{a}o,, Arlindo},
 title = {(old) Finding minimum congestion spanning trees},
 journal = {J. Exp. Algorithmics},
 volume = {5},
 year = {2000},
 issn = {1084-6654},
 pages = {11},
 doi = {http://doi.acm.org/10.1145/351827.384253},
 publisher = {ACM},
 address = {New York, NY, USA},
 }


@article{Werneck:2000:FMC:351827.384253,
 author = {Werneck, Renato and Setubal, Jo\~{a}o and da Conceic\~{a}o, Arlindo},
 title = {(new) Finding minimum congestion spanning trees},
 journal = {J. Exp. Algorithmics},
 volume = 5,
 month = dec,
 year = 2000,
 issn = {1084-6654},
 articleno = 11,
 url = {http://portal.acm.org/citation.cfm?id=351827.384253},
 doi = {10.1145/351827.384253},
 acmid = 384253,
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{1555162,
 author = {Conti, Mauro and Di Pietro, Roberto and Mancini, Luigi V. and Mei, Alessandro},
 title = {(old) Distributed data source verification in wireless sensor networks},
 journal = {Inf. Fusion},
 volume = {10},
 number = {4},
 year = {2009},
 issn = {1566-2535},
 pages = {342--353},
 doi = {http://dx.doi.org/10.1016/j.inffus.2009.01.002},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 }

@article{Conti:2009:DDS:1555009.1555162,
 author = {Conti, Mauro and Di Pietro, Roberto and Mancini, Luigi V. and Mei, Alessandro},
 title = {(new) Distributed data source verification in wireless sensor networks},
 journal = {Inf. Fusion},
 volume = {10},
 number = {4},
 month = oct,
 year = {2009},
 issn = {1566-2535},
 pages = {342--353},
 numpages = {12},
 url = {http://portal.acm.org/citation.cfm?id=1555009.1555162},
 doi = {10.1016/j.inffus.2009.01.002},
 acmid = {1555162},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {Clone detection, Distributed protocol, Securing data fusion, Wireless sensor networks},
}

@inproceedings{Li:2008:PUC:1358628.1358946,
 author = {Li, Cheng-Lun and Buyuktur, Ayse G. and Hutchful, David K. and Sant, Natasha B. and Nainwal, Satyendra K.},
 title = {Portalis: using competitive online interactions to support aid initiatives for the homeless},
 booktitle = {CHI '08 extended abstracts on Human factors in computing systems},
 year = {2008},
 isbn = {978-1-60558-012-X},
 location = {Florence, Italy},
 pages = {3873--3878},
 numpages = {6},
 url = {http://portal.acm.org/citation.cfm?id=1358628.1358946},
 doi = {10.1145/1358628.1358946},
 acmid = {1358946},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cscw, distributed knowledge acquisition, incentive design, online games, recommender systems, reputation systems, user studies, virtual community},
}

@book{Hollis:1999:VBD:519964,
 author = {Hollis, Billy S.},
 title = {Visual Basic 6: Design, Specification, and Objects with Other},
 year = {1999},
 isbn = {0130850845},
 edition = {1st},
 publisher = {Prentice Hall PTR},
 address = {Upper Saddle River, NJ, USA},
 }


@book{Goossens:1999:LWC:553897,
 author = {Goossens, Michel and Rahtz, S. P. and Moore, Ross and Sutor, Robert S.},
 title = {The  Latex Web Companion: Integrating TEX, HTML, and XML},
 year = {1999},
 isbn = {0201433117},
 edition = {1st},
 publisher = {Addison-Wesley Longman Publishing Co., Inc.},
 address = {Boston, MA, USA},
 }

% need to test genres for errant isbn output

% techreport
@techreport{897367,
 author = {Buss, Jonathan F. and Rosenberg, Arnold L. and Knott, Judson D.},
 title = {Vertex Types in Book-Embeddings},
 year = {1987},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Aumass_cs%3Ancstrl.umassa_cs%2F%2FUM-CS-1987-018},
 publisher = {University of Massachusetts},
 address = {Amherst, MA, USA},
 }

@techreport{Buss:1987:VTB:897367,
 author = {Buss, Jonathan F. and Rosenberg, Arnold L. and Knott, Judson D.},
 title = {Vertex Types in Book-Embeddings},
 year = {1987},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Aumass_cs%3Ancstrl.umassa_cs%2F%2FUM-CS-1987-018},
 publisher = {University of Massachusetts},
 address = {Amherst, MA, USA},
 }

% whole proceedings

@proceedings{Czerwinski:2008:1358628,
 author = {},
 note = {General Chair-Czerwinski, Mary and General Chair-Lund, Arnie and Program Chair-Tan, Desney},
 title = {CHI '08: CHI '08 extended abstracts on Human factors in computing systems},
 year = {2008},
 isbn = {978-1-60558-012-X},
 location = {Florence, Italy},
 order_no = {608085},
 publisher = {ACM},
 address = {New York, NY, USA},
 }

% phdthesis

@phdthesis{Clarkson:1985:ACP:911891,
 author = {Clarkson, Kenneth Lee},
 advisor = {Yao, Andrew C.},
 title = {Algorithms for Closest-Point Problems (Computational Geometry)},
 year = {1985},
 note = {AAT 8506171},
 school = {Stanford University},
 address = {Stanford, CA, USA},
 }
% school is being picked up -- but not publisher (which is OK)
% Also -- the title is NOT being output in italics !!! Arrrrgh! - I fixed it. :-)


%%% compare with 'old'
%%% atsign-Phdthesis{Clarkson85,
%%%  author =       "Kenneth L. Clarkson",
%%%  year =         "1985",
%%%  title =        "Algorithms for Closest-Point Problems (Computational Geometry)",
%%%  school =       "Stanford University",
%%%  address =      "Palo Alto, CA",
%%%  note =         "UMI Order Number: AAT 8506171",
%%%  type =         "",
%%%  month =        "",
%%%}

% A bibliography
@Article{1984:1040142,
 key = {{$\!\!$}},
 journal = {SIGCOMM Comput. Commun. Rev.},
 year = {1984},
 issn = {0146-4833},
 volume = {13-14},
 number = {5-1},
 issue_date = {January/April 1984},
 publisher = {ACM},
 address = {New York, NY, USA},
 }


% grinder
@inproceedings{2004:ITE:1009386.1010128,
 key = {IEEE},
 title = {IEEE TCSC Executive Committee},
 booktitle = {Proceedings of the IEEE International Conference on Web Services},
 series = {ICWS '04},
 year = {2004},
 isbn = {0-7695-2167-3},
 pages = {21--22},
 url = {http://dx.doi.org/10.1109/ICWS.2004.64},
 doi = {http://dx.doi.org/10.1109/ICWS.2004.64},
 acmid = {1010128},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}

% div book
@book{Mullender:1993:DS:302430,
 editor = {Mullender, Sape},
 title = {Distributed systems (2nd Ed.)},
 year = {1993},
 isbn = {0-201-62427-3},
 publisher = {ACM Press/Addison-Wesley Publishing Co.},
 address = {New York, NY, USA},
 }

% master thesis (as techreport and thesis)

@techreport{Petrie:1986:NAD:899644,
 author = {Petrie, Charles J.},
 title = {New Algorithms for Dependency-Directed Backtracking (Master's thesis)},
 year = {1986},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Autexas_cs%3AUTEXAS_CS%2F%2FAI86-33},
 publisher = {University of Texas at Austin},
 address = {Austin, TX, USA},
 }

@MASTERSTHESIS{Petrie:1986:NAD:12345,
 author = {Petrie, Charles J.},
 title = {New Algorithms for Dependency-Directed Backtracking (Master's thesis)},
 year = {1986},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Autexas_cs%3AUTEXAS_CS%2F%2FAI86-33},
 school = {University of Texas at Austin},
 address = {Austin, TX, USA},
 }




@BOOK{book-minimal,
   author = "Donald E. Knuth",
   title = "Seminumerical Algorithms",
   publisher = "Addison-Wesley",
   year = "1981",
}

% incollection (has an editor, title, and possibly a booktitle)
@INcollection{KA:2001,
 author = {Kong, Wei-Chang},
 Title = {The implementation of electronic commerce in SMEs in Singapore (as Incoll)},
 booktitle = {E-commerce and cultural values},
 year = {2001},
 isbn = {1-59140-056-2},
 pages = {51--74},
 numpages = {24},
 url = {http://portal.acm.org/citation.cfm?id=887006.887010},
 acmid = {887010},
 publisher = {IGI Publishing},
 address = {Hershey, PA, USA},
}


% with bibfield 'type' before chapter (note no editor)
@INBOOK{KAGM:2001,
 author = {Kong, Wei-Chang},
 type = {Name of Chapter:},
 chapter = {The implementation of electronic commerce in SMEs in Singapore (Inbook-w-chap-w-type)},
 title = {E-commerce and cultural values},
 year = {2001},
 isbn = {1-59140-056-2},
 pages = {51--74},
 numpages = {24},
 url = {http://portal.acm.org/citation.cfm?id=887006.887010},
 acmid = {887010},
 publisher = {IGI Publishing},
 address = {Hershey, PA, USA},
}

%%% Notes! This is because the atsign-INBOOK citation type specifies EITHER
%%% editor or author, but not both. In my experiments with the harvard/dcu
%%% bibtex style (and presumably this applies to other styles too), bibtex
%%% ignores the editor information if author information exists in an
%%% atsign-INBOOK entry. atsign-INCOLLECTION is far more commonly used in my references,
%%% and in the absence of an editor I believe most bibtex styles will just
%%% ommit the editor from the reference - the chapter information will not
%%% end up in the in-text citation as you suggest it should be but at least
%%% there is a place to put the editor if necessary.



% was 'Inbook' -- changed to incollection - (editor is different to author) - need to tell Asad to codify as such.
@incollection{Kong:2002:IEC:887006.887010,
  author =      {Kong, Wei-Chang},
  editor =      {Theerasak Thanasankit},
  title =       {Chapter 9},
  booktitle =   {E-commerce and cultural values (Incoll-w-text (chap 9) 'title')},
  year =        {2002},
  address =     {Hershey, PA, USA},
  publisher =   {IGI Publishing},
  url =         {http://portal.acm.org/citation.cfm?id=887006.887010},
  pages =       {51--74},
  numpages =    {24},
  acmid =       {887010},
  isbn =        {1-59140-056-2},
  number =      "",
  type =        "",
  month =       "",
  note =        "",
}

% incol when the chapter is 'text' - due to presence of editor (different to author)
@incollection{Kong:2003:IEC:887006.887011,
 author = {Kong, Wei-Chang},
 title = {The implementation of electronic commerce in SMEs in Singapore (Incoll)},
 booktitle = {E-commerce and cultural values},
 editor = {Thanasankit, Theerasak},
 year = {2003},
 isbn = {1-59140-056-2},
 pages = {51--74},
 numpages = {24},
 url = {http://portal.acm.org/citation.cfm?id=887006.887010},
 acmid = {887010},
 publisher = {IGI Publishing},
 address = {Hershey, PA, USA},
}

% ------ test
%incollection{Kong:2003:IEC:887006.887010,
% author = {Kong, Wei-Chang},
% chapter = {The implementation of electronic commerce in SMEs in Singapore (Incoll-text-in-chap)},
% booktitle = {booktitle E-commerce and cultural values},
% title =   {The title},
% editor = {Thanasankit, Theerasak},
% year = {2003},
% isbn = {1-59140-056-2},
% pages = {51--74},
% numpages = {24},
% url = {http://portal.acm.org/citation.cfm?id=887006.887010},
% acmid = {887010},
% publisher = {IGI Publishing},
% address = {Hershey, PA, USA},
%}


% ---------





% Need inbook with num in chapter

% and inbook with number in chapter
@InBook{Kong:2004:IEC:123456.887010,
  author =      {Kong, Wei-Chang},
  editor =      {Theerasak Thanasankit},
  title =       {E-commerce and cultural values - (InBook-num-in-chap)},
  chapter =     {9},
  year =        {2004},
  address =     {Hershey, PA, USA},
  publisher =   {IGI Publishing},
  url =         {http://portal.acm.org/citation.cfm?id=887006.887010},
  pages =       {51--74},
  numpages =    {24},
  acmid =       {887010},
  isbn =        {1-59140-056-2},
  number =      "",
  type =        "",
  month =       "",
  note =        "",
}


% and inbook with text in chapter
@Inbook{Kong:2005:IEC:887006.887010,
  author =      {Kong, Wei-Chang},
  editor =      {Theerasak Thanasankit},
  title =       {E-commerce and cultural values (Inbook-text-in-chap)},
  chapter =     {The implementation of electronic commerce in SMEs in Singapore},
  year =        {2005},
  address =     {Hershey, PA, USA},
  publisher =   {IGI Publishing},
  url =         {http://portal.acm.org/citation.cfm?id=887006.887010},
  type =        {Chapter:},
  pages =       {51--74},
  numpages =    {24},
  acmid =       {887010},
  isbn =        {1-59140-056-2},
  number =      "",
  month =       "",
  note =        "",
}


% and inbook with a num and type field
@Inbook{Kong:2006:IEC:887006.887010,
  author =      {Kong, Wei-Chang},
  editor =      {Theerasak Thanasankit},
  title =       {E-commerce and cultural values (Inbook-num chap)},
  chapter =     {22},
  year =        {2006},
  address =     {Hershey, PA, USA},
  publisher =   {IGI Publishing},
  url =         {http://portal.acm.org/citation.cfm?id=887006.887010},
  type =        {Chapter (in type field)},
  pages =       {51--74},
  numpages =    {24},
  acmid =       {887010},
  isbn =        {1-59140-056-2},
  number =      "",
  month =       "",
  note =        "",
}


% and incol coz we have a BLANK chapter - due to presence of editor
%atIncollection{Kong:2006:IEC:887006.887011,
%  author =     {Kong, Wei-Chang},
%  editor =     {Theerasak Thanasankit},
%  title =      "The title"
%  booktitle =  {E-commerce and cultural values (Incol-coz-blank-chap)},
%  year =       {2006},
%  address =    {Hershey, PA, USA},
%  publisher =  {IGI Publishing},
%  url =        {http://portal.acm.org/citation.cfm?id=887006.887010},
%  type =       {Type!},
%  chapter =    {},
%  pages =      {51--74},
%  numpages =   {24},
%  acmid =      {887010},
%  isbn =       {1-59140-056-2},
%  number =     "",
%  month =      "",
%  note =       "",
%}

@article{SaeediMEJ10,
            author = {Mehdi Saeedi and Morteza Saheb Zamani and Mehdi Sedighi},
            title = {A library-based synthesis methodology for reversible logic},
            journal = {Microelectron. J.},
            volume = {41},
            number = {4},
            month = apr,
            year = {2010},
            pages = {185--194},
}

@ARTICLE{SaeediJETC10,
            author = {Mehdi Saeedi and Morteza Saheb Zamani and Mehdi Sedighi and Zahra Sasanian},
            title = {Synthesis of Reversible Circuit Using Cycle-Based Approach},
            journal = {J. Emerg. Technol. Comput. Syst.},
            volume = {6},
            number = {4},
            month = dec,
            year = {2010}
            }

% Asad's new version
@article{Kirschmer:2010:AEI:1958016.1958018,
 author = {Kirschmer, Markus and Voight, John},
 title = {Algorithmic Enumeration of Ideal Classes for Quaternion Orders},
 journal = {SIAM J. Comput.},
 issue_date = {January 2010},
 volume = {39},
 number = {5},
 month = jan,
 year = {2010},
 issn = {0097-5397},
 pages = {1714--1747},
 numpages = {34},
 url = {http://dx.doi.org/10.1137/080734467},
 doi = {https://doi.org/10.1137/080734467},
 acmid = {1958018},
 publisher = {Society for Industrial and Applied Mathematics},
 address = {Philadelphia, PA, USA},
 keywords = {ideal classes, maximal orders, number theory, quaternion algebras},
}


% incol due to presence of booktitle
@incollection{Hoare:1972:CIN:1243380.1243382,
 author = {Hoare, C. A. R.},
 title = {Chapter II: Notes on data structuring},
 booktitle = {Structured programming (incoll)},
 editor = {Dahl, O. J. and Dijkstra, E. W. and Hoare, C. A. R.},
 year = {1972},
 isbn = {0-12-200550-3},
 pages = {83--174},
 numpages = {92},
 url = {http://portal.acm.org/citation.cfm?id=1243380.1243382},
 acmid = {1243382},
 publisher = {Academic Press Ltd.},
 address = {London, UK, UK},
}

% incol due to presence of booktitle
@incollection{Lee:1978:TQA:800025.1198348,
 author = {Lee, Jan},
 title = {Transcript of question and answer session},
 booktitle = {History of programming languages I (incoll)},
 editor = {Wexelblat, Richard L.},
 year = {1981},
 isbn = {0-12-745040-8},
 pages = {68--71},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/800025.1198348},
 doi = {http://doi.acm.org/10.1145/800025.1198348},
 acmid = {1198348},
 publisher = {ACM},
 address = {New York, NY, USA},
}

% incol due to booktitle
@incollection{Dijkstra:1979:GSC:1241515.1241518,
 author = {Dijkstra, E.},
 title = {Go to statement considered harmful},
 booktitle = {Classics in software engineering (incoll)},
 year = {1979},
 isbn = {0-917072-14-6},
 pages = {27--33},
 numpages = {7},
 url = {http://portal.acm.org/citation.cfm?id=1241515.1241518},
 acmid = {1241518},
 publisher = {Yourdon Press},
 address = {Upper Saddle River, NJ, USA},
}

% incol due to booktitle
@incollection{Wenzel:1992:TVA:146022.146089,
 author = {Wenzel, Elizabeth M.},
 title = {Three-dimensional virtual acoustic displays},
 booktitle = {Multimedia interface design (incoll)},
 year = {1992},
 isbn = {0-201-54981-6},
 pages = {257--288},
 numpages = {32},
 url = {http://portal.acm.org/citation.cfm?id=146022.146089},
 doi = {10.1145/146022.146089},
 acmid = {146089},
 publisher = {ACM},
 address = {New York, NY, USA},
}

% incol due to booktitle
@incollection{Mumford:1987:MES:54905.54911,
 author = {Mumford, E.},
 title = {Managerial expert systems and organizational change: some critical research issues},
 booktitle = {Critical issues in information systems research (incoll)},
 year = {1987},
 isbn = {0-471-91281-6},
 pages = {135--155},
 numpages = {21},
 url = {http://portal.acm.org/citation.cfm?id=54905.54911},
 acmid = {54911},
 publisher = {John Wiley \& Sons, Inc.},
 address = {New York, NY, USA},
}

@book{McCracken:1990:SSC:575315,
 author = {McCracken, Daniel D. and Golden, Donald G.},
 title = {Simplified Structured COBOL with Microsoft/MicroFocus COBOL},
 year = {1990},
 isbn = {0471514071},
 publisher = {John Wiley \& Sons, Inc.},
 address = {New York, NY, USA},
}

% Let's include Boris / BBeeton entries  (multi-volume works)

@book {MR781537,
    AUTHOR = {H{\"o}rmander, Lars},
     TITLE = {The analysis of linear partial differential operators. {III}},
    SERIES = {Grundlehren der Mathematischen Wissenschaften [Fundamental
              Principles of Mathematical Sciences]},
    VOLUME = {275},
      NOTE = {Pseudodifferential operators},
PUBLISHER = {Springer-Verlag},
   ADDRESS = {Berlin, Germany},
      YEAR = {1985},
     PAGES = {viii+525},
      ISBN = {3-540-13828-5},
   MRCLASS = {35-02 (35Sxx 47G05 58G15)},
  MRNUMBER = {781536 (87d:35002a)},
MRREVIEWER = {Min You Qi},
}

@book {MR781536,
    AUTHOR = {H{\"o}rmander, Lars},
     TITLE = {The analysis of linear partial differential operators. {IV}},
    SERIES = {Grundlehren der Mathematischen Wissenschaften [Fundamental
              Principles of Mathematical Sciences]},
    VOLUME = {275},
      NOTE = {Fourier integral operators},
PUBLISHER = {Springer-Verlag},
   ADDRESS = {Berlin, Germany},
      YEAR = {1985},
     PAGES = {vii+352},
      ISBN = {3-540-13829-3},
   MRCLASS = {35-02 (35Sxx 47G05 58G15)},
  MRNUMBER = {781537 (87d:35002b)},
MRREVIEWER = {Min You Qi},
}

%%%%%%%%%%%%%%%%%%%%%% Start of Aptara sample bib entries

% acmsmall-sam.bib
@InProceedings{Adya-01,
  author        = {A. Adya and P. Bahl and J. Padhye and A.Wolman and L. Zhou},
  title         = {A multi-radio unification protocol for {IEEE} 802.11 wireless networks},
  booktitle     = {Proceedings of the IEEE 1st International Conference on Broadnets Networks (BroadNets'04)},
  publisher     = "IEEE",
  address       = "Los Alamitos, CA",
  year          = {2004},
  pages         = "210--217"
}

@article{Akyildiz-01,
  author        = {I. F. Akyildiz and W. Su and Y. Sankarasubramaniam and E. Cayirci},
  title         = {Wireless Sensor Networks: A Survey},
  journal       = {Comm. ACM},
  volume        = 38,
  number        = "4",
  year          = {2002},
  pages         = "393--422"
}

@article{Akyildiz-02,
  author        = {I. F. Akyildiz and T. Melodia and K. R. Chowdhury},
  title         = {A Survey on Wireless Multimedia Sensor Networks},
  journal       = {Computer Netw.},
  volume        = 51,
  number        = "4",
  year          = {2007},
  pages         = "921--960"
}

@InProceedings{Bahl-02,
  author        = {P. Bahl and R. Chancre and J. Dungeon},
  title         = {{SSCH}: Slotted Seeded Channel Hopping for Capacity Improvement in {IEEE} 802.11 Ad-Hoc Wireless Networks},
  booktitle     = {Proceeding of the 10th International Conference on Mobile Computing and Networking (MobiCom'04)},
  publisher     = "ACM",
  address       = "New York, NY",
  year          = {2004},
  pages         = "112--117"
}

@misc{CROSSBOW,
  key       = {CROSSBOW},
  title     = {{XBOW} Sensor Motes Specifications},
  note      = {http://www.xbow.com},
  year      = 2008
}

@article{Culler-01,
  author        = {D. Culler and D. Estrin and M. Srivastava},
  title         = {Overview of Sensor Networks},
  journal       = {IEEE Comput.},
  volume        = 37,
  number        = "8 (Special Issue on Sensor Networks)",
  publisher     = "IEEE",
  address       = "Los Alamitos, CA",
  year          = {2004},
  pages         = "41--49"
}

@misc{Harvard-01,
    key         = {Harvard CodeBlue},
    title       = {{CodeBlue}: Sensor Networks for Medical Care},
    note        = {http://www.eecs.harvard.edu/mdw/ proj/codeblue/},
    year        = 2008
}

@InProceedings{Natarajan-01,
    author      = {A. Natarajan and M. Motani and B. de Silva and K. Yap and K. C. Chua},
    title       = {Investigating Network Architectures for Body Sensor Networks},
    booktitle   = {Network Architectures},
    editor      = {G. Whitcomb and P. Neece},
    publisher   = "Keleuven Press",
    address     = "Dayton, OH",
    year        = {2007},
    pages       = "322--328",
    eprint      = "960935712",
    primaryclass = "cs",
}

@techreport{Tzamaloukas-01,
  author        = {A. Tzamaloukas and J. J. Garcia-Luna-Aceves},
  title         = {Channel-Hopping Multiple Access},
  number =        {I-CA2301},
  institution =   {Department of Computer Science, University of California},
  address =       {Berkeley, CA},
  year          = {2000}
}

@BOOK{Zhou-06,
  author        = {G. Zhou and J. Lu and C.-Y. Wan and M. D. Yarvis and J. A. Stankovic},
  title         = {Body Sensor Networks},
  publisher     = "MIT Press",
  address       = "Cambridge, MA",
  year          = {2008}
}

@mastersthesis{ko94,
author = "Jacob Kornerup",
title = "Mapping Powerlists onto Hypercubes",
school = "The University of Texas at Austin",
note = "(In preparation)",
year = "1994"}
%month = "dec",}

@PhdThesis{gerndt:89,
  author =       "Michael Gerndt",
  title =        "Automatic Parallelization for Distributed-Memory
                  Multiprocessing Systems",
  school =       "University of Bonn",
  year =         1989,
  address =      "Bonn, Germany",
  month =        dec
}

@article{6:1:1,
author = "J. E. {Archer, Jr.} and R. Conway and F. B. Schneider",
title = "User recovery and reversal in interactive systems",
journal = "ACM Trans. Program. Lang. Syst.",
volume =  "6",
number = "1",
month = jan,
year = 1984,
pages = "1--19"}

@article{7:1:137,
author = "D. D. Dunlop and V. R. Basili",
title = "Generalizing specifications for uniformly implemented loops",
journal = "ACM Trans. Program. Lang. Syst.",
volume =  "7",
number = "1",
month = jan,
year = 1985,
pages = "137--158"}

@article{7:2:183,
author = "J. Heering and P. Klint",
title = "Towards monolingual programming environments",
journal = "ACM Trans. Program. Lang. Syst.",
volume =  "7",
number = "2",
month = apr,
year = 1985,
pages = "183--213"}

@book{knuth:texbook,
author = "Donald E. Knuth",
title = "The {\TeX{}book}",
publisher = "Addison-Wesley",
address = "Reading, MA.",
year = 1984}

@article{6:3:380,
author = "E. Korach and D.  Rotem and N. Santoro",
title = "Distributed algorithms for finding centers and medians in networks",
journal = "ACM Trans. Program. Lang. Syst.",
volume =  "6",
number = "3",
month = jul,
year = 1984,
pages = "380--401"}

@book{Lamport:LaTeX,
author = "Leslie Lamport",
title = "\it {\LaTeX}: A Document Preparation System",
publisher = "Addison-Wesley",
address = "Reading, MA.",
year = 1986}

@article{7:3:359,
author = "F. Nielson",
title = "Program transformations in a denotational setting",
journal = "ACM Trans. Program. Lang. Syst.",
volume =  "7",
number = "3",
month = jul,
year = 1985,
pages = "359--379"}

%testing
@BOOK{test,
   author = "Donald E. Knuth",
   title = "Seminumerical Algorithms",
   volume = 2,
   series = "The Art of Computer Programming",
   publisher = "Addison-Wesley",
   address = "Reading, MA",
   edition = "2nd",
   month = "10~" # jan,
   year = "1981",
}

@inproceedings{reid:scribe,
author = "Brian K. Reid",
title = "A high-level approach to computer document formatting",
booktitle = "Proceedings of the 7th Annual Symposium on Principles of
  Programming Languages",
month = jan,
year = 1980,
publisher = "ACM",
address = "New York",
pages = "24--31"}

@article{Zhou:2010:MMS:1721695.1721705,
 author = {Zhou, Gang and Wu, Yafeng and Yan, Ting and He, Tian and Huang, Chengdu and Stankovic, John A. and Abdelzaher, Tarek F.},
 title = {A multifrequency MAC specially designed for wireless sensor network applications},
 journal = {ACM Trans. Embed. Comput. Syst.},
 issue_date = {March 2010},
 volume = 9,
 number = 4,
 month = {April},
 year = 2010,
 issn = {1539-9087},
 pages = {39:1--39:41},
 articleno = 39,
 numpages = 41,
 url = {http://doi.acm.org/10.1145/1721695.1721705},
 doi = {10.1145/1721695.1721705},
 acmid = 1721705,
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Wireless sensor networks, media access control, multi-channel, radio interference, time synchronization},
}


@online{TUGInstmem,
  key =          {TUG},
  year  =        2017,
  title =        "Institutional members of the {\TeX} Users Group",
  url =          "http://wwtug.org/instmem.html",
  lastaccessed = "May 27, 2017",
}

@online{CTANacmart,
  author =    {Boris Veytsman},
  title =  {acmart---{C}lass for typesetting publications of {ACM}},
  year = 2017,
  url =    {http://www.ctan.org/pkg/acmart},
  lastaccessed = {May 27, 2017}
  }

@ARTICLE{bowman:reasoning,
    author = {Bowman, Mic and Debray, Saumya K. and Peterson, Larry L.},
    title = {Reasoning About Naming Systems},
    journal = {ACM Trans. Program. Lang. Syst.},
    volume = {15},
    number = {5},
    pages = {795-825},
    month = {November},
    year = {1993},
    doi = {10.1145/161468.161471},
}

@ARTICLE{braams:babel,
    author = {Braams, Johannes},
    title = {Babel, a Multilingual Style-Option System for Use with LaTeX's Standard Document Styles},
    journal = {TUGboat},
    volume = {12},
    number = {2},
    pages = {291-301},
    month = {June},
    year = {1991},
}

@INPROCEEDINGS{clark:pct,
  AUTHOR = "Malcolm Clark",
  TITLE = "Post Congress Tristesse",
  BOOKTITLE = "TeX90 Conference Proceedings",
  PAGES = "84-89",
  ORGANIZATION = "TeX Users Group",
  MONTH = "March",
  YEAR = {1991}
}

@ARTICLE{herlihy:methodology,
    author = {Herlihy, Maurice},
    title = {A Methodology for Implementing Highly Concurrent Data Objects},
    journal = {ACM Trans. Program. Lang. Syst.},
    volume = {15},
    number = {5},
    pages = {745-770},
    month = {November},
    year = {1993},
    doi = {10.1145/161468.161469},
}

@BOOK{salas:calculus,
  AUTHOR = "S.L. Salas and Einar Hille",
  TITLE = "Calculus: One and Several Variable",
  PUBLISHER = "John Wiley and Sons",
  ADDRESS = "New York",
  YEAR = "1978"
}

@MANUAL{Fear05,
  title =        {Publication quality tables in {\LaTeX}},
  author =       {Simon Fear},
  month =        {April},
  year =         2005,
  note =         {\url{http://www.ctan.org/pkg/booktabs}}
}

@Manual{Amsthm15,
  title =        {Using the amsthm Package},
  organization = {American Mathematical Society},
  month =        {April},
  year =         2015,
  note =         {\url{http://www.ctan.org/pkg/amsthm}}
}

@ArtifactSoftware{R,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2019},
    url = {https://www.R-project.org/},
}

@ArtifactDataset{UMassCitations,
 author    =  {Sam Anzaroot and Andrew McCallum},
 title     =  {{UMass} Citation Field Extraction Dataset},
 year      = 2013,
 url       =
    {http://www.iesl.cs.umass.edu/data/data-umasscitationfield},
 lastaccessed = {May 27, 2019}
}

@Eprint{Bornmann2019,
       author = {Bornmann, Lutz and Wray, K. Brad and Haunschild,
                  Robin},
        title = {Citation concept analysis {(CCA)}---A new form of
                  citation analysis revealing the usefulness of
                  concepts for other researchers illustrated by two
                  exemplary case studies including classic books by
                  {Thomas S.~Kuhn} and {Karl R.~Popper}},
     keywords = {Computer Science - Digital Libraries},
         year = 2019,
        month = "May",
          eid = {arXiv:1905.12410},
archivePrefix = {arXiv},
       eprint = {1905.12410},
 primaryClass = {cs.DL},
}

@Eprint{AnzarootPBM14,
  author    = {Sam Anzaroot and
               others},
  title     = {Learning Soft Linear Constraints with Application to
                  Citation Field Extraction},
  year      = {2014},
  archivePrefix = {arXiv},
  eprint    = {1403.1349},
}

@inproceedings{Hagerup1993,
title        = {Maintaining Discrete Probability Distributions Optimally},
author       = {Hagerup, Torben and Mehlhorn, Kurt and Munro, J. Ian},
booktitle    = {Proceedings of the 20th International Colloquium on Automata, Languages and Programming},
series       = {Lecture Notes in Computer Science},
volume       = {700},
pages        = {253--264},
year         = {1993},
publisher    = {Springer-Verlag},
address      = {Berlin},
}

@article{Zhao2023A,title={A Survey of Large Language Models},author={Wayne Xin Zhao and others},journal={ArXiv},year={2023},volume={abs/2303.18223}
}

@article{Naveed2023A,title={A Comprehensive Overview of Large Language Models},author={Humza Naveed and others},journal={ArXiv},year={2023},volume={abs/2307.06435}}

@article{Douglas2023Large,title={Large Language Models},author={Michael R Douglas},journal={Communications of the ACM},year={2023},volume={66},pages={7 - 7}
}

@article{Zhang2023How,title={How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances},author={Zihan Zhang and others},year={2023},pages={8289-8311}}

@article{2023Large,title={Large language models: fast proliferation and budding international competition},author={},journal={Strategic Comments},year={2023},volume={29},pages={iv - vi}
}

@article{Hong2023MetaGPT,title={MetaGPT: Meta Programming for Multi-Agent Collaborative Framework},author={Sirui Hong and others},journal={ArXiv},year={2023},volume={abs/2308.00352}
}


@article{Agashe2023Evaluating,title={Evaluating Multi-Agent Coordination Abilities in Large Language Models},author={Saaket Agashe and Yue Fan and Xin Eric Wang},journal={ArXiv},year={2023},volume={abs/2310.03903}
}

@article{Chen2023MultiAgent,title={Multi-Agent Consensus Seeking via Large Language Models},author={Huaben Chen and Wenkang Ji and Lufeng Xu and Shiyu Zhao},journal={ArXiv},year={2023},volume={abs/2310.20151}
}

@ARTICLE{9918176,
  author={Crowley, James L. and others},
  journal={IEEE Pervasive Computing}, 
  title={A Hierarchical Framework for Collaborative Artificial Intelligence}, 
  year={2023},
  volume={22},
  number={1},
  pages={9-18},
  keywords={Collaboration;Behavioral sciences;Task analysis;Robots;Intelligent systems;Robot sensing systems;Protocols},
  doi={10.1109/MPRV.2022.3208321}}


@misc{dafoe2020openproblemscooperativeai,
      title={Open Problems in Cooperative AI}, 
      author={Allan Dafoe and others},
      year={2020},
      eprint={2012.08630},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@article{Conitzer_Oesterheld_2024, title={Foundations of Cooperative AI}, volume={37}, abstractNote={AI systems can interact in unexpected ways, sometimes with disastrous consequences. As AI gets to control more of our world, these interactions will become more common and have higher stakes. As AI becomes more advanced, these interactions will become more sophisticated, and game theory will provide the tools for analyzing these interactions. However, AI agents are in some ways unlike the agents traditionally studied in game theory, introducing new challenges as well as opportunities. We propose a research agenda to develop the game theory of highly advanced AI agents, with a focus on achieving cooperation.}, number={13}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Conitzer, Vincent and Oesterheld, Caspar}, year={2024}, month={Jul.}, pages={15359-15367} }

@article{MenaGuacas2023,
  title = {Collaborative learning and skill development for educational growth of artificial intelligence: A systematic review},
  volume = {15},
  ISSN = {1309-517X},
  number = {3},
  journal = {Contemporary Educational Technology},
  publisher = {Bastas Publications},
  author = {Mena-Guacas,  Andres F. and others},
  year = {2023},
  month = jul,
  pages = {ep428}
}

@article{OUYANG2024100616,
title = {AI-driven learning analytics applications and tools in computer-supported collaborative learning: A systematic review},
journal = {Educational Research Review},
volume = {44},
pages = {100616},
year = {2024},
issn = {1747-938X},
author = {Fan Ouyang and Liyin Zhang},
keywords = {Computer-supported collaborative learning (CSCL), Learning analytics, AI-Driven learning analytics, Systematic review, Automatic feedback, Artificial intelligence},
abstract = {Artificial intelligence (AI) has brought new ways for implementing learning analytics in computer-supported collaborative learning (CSCL). However, there is a lack of literature reviews that focus on AI-driven learning analytics applications and tools in CSCL contexts. To fill the gap, this systematic review provides an overview of the goals, characteristics, and effects of existing AI-driven learning analytics applications and tools in CSCL. According to the screening criteria, out of the 2607 initially identified articles between 2004 and 2023, 26 articles are included for final synthesis. Our results show that existing tools primarily focus on students’ cognitive engagement. Existing tools primarily utilize communicative discourse, behavioral, and evaluation data to present results and visualizations. Despite various formats of feedback are provided in existing tools, there is a lack of design principles to guide the tool design and development process. Moreover, although AI techniques have been applied for presenting statistical information, there is a lack of providing alert or suggestive information in existing tools or applications. Compared with the positive impacts on collaborative learning, our results indicate a lack of support for instructional interventions in existing tools. This systematic review proposes the following theoretical, technological, and practical implications: (1) the integration of educational and learning theories into AI-driven learning analytics applications and tools; (2) the adoption of advanced AI technologies to collect, analyze, and interpret multi-source and multimodal data; and (3) the support for instructors with actionable suggestions and instructional interventions. Based on our findings, we provide further directions on how to design, analyze, and implement AI-driven learning analytics applications and tools within CSCL contexts.}
}

@article{Dafoe2021,
  title = {Cooperative AI: machines must learn to find common ground},
  volume = {593},
  ISSN = {1476-4687},
  number = {7857},
  journal = {Nature},
  publisher = {Springer Science and Business Media LLC},
  author = {Dafoe,  Allan and others},
  year = {2021},
  month = may,
  pages = {33–36}
}

@inproceedings{ijcai2024p890,
  title     = {Large Language Model Based Multi-agents: A Survey of Progress and Challenges},
  author    = {Guo, Taicheng and others},
  booktitle = {Proceedings of the Thirty-Third International Joint Conference on
               Artificial Intelligence, {IJCAI-24}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Kate Larson},
  pages     = {8048--8057},
  year      = {2024},
  month     = {8},
  note      = {Survey Track},
}


@ARTICLE{8352646,
  author={Dorri, Ali and Kanhere, Salil S. and Jurdak, Raja},
  journal={IEEE Access}, 
  title={Multi-Agent Systems: A Survey}, 
  year={2018},
  volume={6},
  number={},
  pages={28573-28593},
  keywords={Task analysis;Multi-agent systems;Computer science;Security;Australia;Computational modeling;Decision making;Multi-agent systems;survey;MAS applications;challenges},
}


@inbook{introMAC,
author = {Parasumanna Gokulan, Balaji and Srinivasan, D.},
year = {2010},
month = {07},
pages = {1-27},
title = {An Introduction to Multi-Agent Systems},
volume = {310},
isbn = {978-3-642-14434-9},
journal = {Studies in Computational Intelligence},
doi = {10.1007/978-3-642-14435-6_1}
}

@inproceedings{leviathan2023fast,
  title={Fast inference from transformers via speculative decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  booktitle={International Conference on Machine Learning},
  pages={19274--19286},
  year={2023},
  organization={PMLR}
}

@inproceedings{dong-etal-2024-villageragent,
    title = "{V}illager{A}gent: A Graph-Based Multi-Agent Framework for Coordinating Complex Task Dependencies in {M}inecraft",
    author = "Dong, Yubo  and
      others",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    pages = "16290--16314",
    abstract = "In this paper, we aim to evaluate multi-agent systems against complex dependencies, including spatial, causal, and temporal constraints. First, we construct a new benchmark, named VillagerBench, within the Minecraft environment. VillagerBench comprises diverse tasks crafted to test various aspects of multi-agent collaboration, from workload distribution to dynamic adaptation and synchronized task execution. Second, we introduce a Directed Acyclic Graph Multi-Agent Framework (VillagerAgent) to resolve complex inter-agent dependencies and enhance collaborative efficiency. This solution incorporates a task decomposer that creates a directed acyclic graph (DAG) for structured task management, an agent controller for task distribution, and a state manager for tracking environmental and agent data.Our empirical evaluation on VillagerBench demonstrates that VillagerAgentoutperforms the existing AgentVerse model, reducing hallucinations and improving task decomposition efficacy. The results underscore VillagerAgent{'}s potential in advancing multi-agent collaboration, offering a scalable and generalizable solution in dynamic environments. Source code is open-source on GitHub.",
}

@book{marl-book,
  author = {Stefano V. Albrecht and Filippos Christianos and Lukas Sch\"afer},
  title = {Multi-Agent Reinforcement Learning: Foundations and Modern Approaches},
  publisher = {MIT Press},
  year = {2024},
}


@inproceedings{10.1145/3626772.3657669,
author = {Wang, Zhefan and others},
title = {MACRec: A Multi-Agent Collaboration Framework for Recommendation},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {LLM-based agents have gained considerable attention for their decision-making skills and ability to handle complex tasks. Recognizing the current gap in leveraging agent capabilities for multi-agent collaboration in recommendation systems, we introduce MACRec, a novel framework designed to enhance recommendation systems through multi-agent collaboration. Unlike existing work on using agents for user/item simulation, we aim to deploy multi-agents to tackle recommendation tasks directly. In our framework, recommendation tasks are addressed through the collaborative efforts of various specialized agents, including Manager, User/Item Analyst, Reflector, Searcher, and Task Interpreter, with different working flows. Furthermore, we provide application examples of how developers can easily use MACRec on various recommendation tasks, including rating prediction, sequential recommendation, conversational recommendation, and explanation generation of recommendation results. The framework and demonstration video are publicly available at https://github.com/wzf2000/MACRec.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2760–2764},
numpages = {5},
keywords = {large language models, multi-agents, recommender systems},
location = {Washington DC, USA},
series = {SIGIR '24}
}
@inproceedings{10.1145/3491102.3517582,
author = {Wu, Tongshuang and Terry, Michael and Cai, Carrie Jun},
title = {AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Although large language models (LLMs) have demonstrated impressive potential on simple tasks, their breadth of scope, lack of transparency, and insufficient controllability can make them less effective when assisting humans on more complex tasks. In response, we introduce the concept of Chaining LLM steps together, where the output of one step becomes the input for the next, thus aggregating the gains per step. We first define a set of LLM primitive operations useful for Chain construction, then present an interactive system where users can modify these Chains, along with their intermediate results, in a modular way. In a 20-person user study, we found that Chaining not only improved the quality of task outcomes, but also significantly enhanced system transparency, controllability, and sense of collaboration. Additionally, we saw that users developed new ways of interacting with LLMs through Chains: they leveraged sub-tasks to calibrate model expectations, compared and contrasted alternative strategies by observing parallel downstream effects, and debugged unexpected model outputs by “unit-testing” sub-components of a Chain. In two case studies, we further explore how LLM Chains may be used in future applications.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {385},
numpages = {22},
keywords = {Human-AI Interaction, Large Language Models, Natural Language Processing},
location = {New Orleans, LA, USA},
series = {CHI '22}
}
@inproceedings{li-etal-2023-theory,
title = "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
author = "Li, Huao  and
others",
editor = "Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika",
booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
month = dec,
year = "2023",
address = "Singapore",
publisher = "Association for Computational Linguistics",
pages = "180--192",
abstract = "While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents{'} planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.",
}
@inproceedings{chen-etal-2024-llmarena,
title = "{LLMA}rena: Assessing Capabilities of Large Language Models in Dynamic Multi-Agent Environments",
author = "Chen, Junzhe and others",
booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
month = aug,
year = "2024",
address = "Bangkok, Thailand",
publisher = "Association for Computational Linguistics",
pages = "13055--13077",
abstract = "Recent advancements in large language models (LLMs) have revealed their potential for achieving autonomous agents possessing human-level intelligence. However, existing benchmarks for evaluating LLM Agents either use static datasets, potentially leading to data leakage or focus only on single-agent scenarios, overlooking the complexities of multi-agent interactions. There is a lack of a benchmark that evaluates the diverse capabilities of LLM agents in multi-agent, dynamic environments. To this end, we introduce LLMArena, a novel and easily extensible framework for evaluating the diverse capabilities of LLM in multi-agent dynamic environments. LLMArena encompasses seven distinct gaming environments, employing Trueskill scoring to assess crucial abilities in LLM agents, including spatial reasoning, strategic planning, numerical reasoning, risk assessment, communication, opponent modeling, and team collaboration. We conduct an extensive experiment and human evaluation among different sizes and types of LLMs, showing that LLMs still have a significant journey ahead in their development towards becoming fully autonomous agents, especially in opponent modeling and team collaboration. We hope LLMArena could guide future research towards enhancing these capabilities in LLMs, ultimately leading to more sophisticated and practical applications in dynamic, multi-agent settings.",
}
@inproceedings{islam-etal-2024-mapcoder,
title = "{M}ap{C}oder: Multi-Agent Code Generation for Competitive Problem Solving",
author = "Islam, Md. Ashraful  and
Ali, Mohammed Eunus  and
Parvez, Md Rizwan",
booktitle = "Proceedings of the Annual Meeting of the Association for Computational Linguistics",
month = "Aug.",
year = "2024"
}
@inproceedings{chen-etal-2024-comm,
title = "{C}o{MM}: Collaborative Multi-Agent, Multi-Reasoning-Path Prompting for Complex Problem Solving",
author = "Chen, Pei  and
Zhang, Shuai  and
Han, Boran",
editor = "Duh, Kevin  and
Gomez, Helena  and
Bethard, Steven",
booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
month = jun,
year = "2024",
address = "Mexico City, Mexico",
publisher = "ACL",
pages = "1720--1738",
abstract = "Large Language Models (LLMs) have shown great ability in solving traditional natural language tasks and elementary reasoning tasks with appropriate prompting techniques. However, their ability is still limited in solving complicated science problems. In this work, we aim to push the upper bound of the reasoning capability of LLMs by proposing a collaborative multi-agent, multi-reasoning-path (CoMM) prompting framework. Specifically, we prompt LLMs to play different roles in a problem-solving team, and encourage different role-play agents to collaboratively solve the target task. In particular, we discover that applying different reasoning paths for different roles is an effective strategy to implement few-shot prompting approaches in the multi-agent scenarios. Empirical results demonstrate the effectiveness of the proposed methods on two college-level science problems over competitive baselines. Our further analysis shows the necessity of prompting LLMs to play different roles or experts independently.",
}
@inproceedings{feng-lu-2023-multi,
title = "Multi-Agent Language Learning: Symbolic Mapping",
author = "Feng, Yicheng  and
Lu, Zongqing",
editor = "Rogers, Anna  and
Boyd-Graber, Jordan  and
Okazaki, Naoaki",
booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
month = jul,
year = "2023",
address = "Toronto, Canada",
publisher = "Association for Computational Linguistics",
pages = "7756--7770",
abstract = "The study of emergent communication has long been devoted to coax neural network agents to learn a language sharing similar properties with human language. In this paper, we try to find a {`}natural{'} way to help agents learn a compositional and symmetric language in complex settings like dialog games. Inspired by the theory that human language was originated from simple interactions, we hypothesize that language may evolve from simple tasks to difficult tasks. We propose a curriculum learning method called task transfer, and propose a novel architecture called symbolic mapping. We find that task transfer distinctly helps language learning in difficult tasks, and symbolic mapping promotes the effect. Further, we explore vocabulary expansion, and show that with the help of symbolic mapping, agents can easily learn to use new symbols when the environment becomes more complex. All in all, we find that a process from simplicity to complexity can serve as a natural way to help multi-agent language learning, and the proposed symbolic mapping is effective for this process.",
}
@inproceedings{he-etal-2023-lego,
title = "{LEGO}: A Multi-agent Collaborative Framework with Role-playing and Iterative Feedback for Causality Explanation Generation",
author = "He, Zhitao  and
others",
editor = "Bouamor, Houda  and
Pino, Juan  and
Bali, Kalika",
booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
month = dec,
year = "2023",
address = "Singapore",
publisher = "Association for Computational Linguistics",
pages = "9142--9163",
abstract = "Causality Explanation Generation refers to generate an explanation in natural language given an initial cause-effect pair. It demands rigorous explicit rationales to demonstrate the acquisition of implicit commonsense knowledge, which is unlikely to be easily memorized, making it challenging for large language models since they are often suffering from spurious causal associations when they encounter the content that does not exist in their memory. In this work, we introduce LEGO, a Multi-agent Collaborative Framework with Role-playing and Iterative Feedback for causality explanation generation. Specifically, we treat LLM as character malleable LEGO block and utilize role-playing to assign specific roles to five LLMs. We firstly devise a Fine-grained World Knowledge Integration Module to augment information about tasks for alleviating the phenomenon of spurious causal associations. Then, we leverage an Iterative Feedback and Refinement Module to improve the generated explanation by multi-aspect feedback. Extensive experiments on widely used WIKIWHY and e-CARE datasets show the superiority of our multi-agent framework in terms of reasoning about the causality among cause and effect.",
}
@inproceedings{10.1145/3593013.3594033,
author = {Chan, Alan and others},
title = {Harms from Increasingly Agentic Algorithmic Systems},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Research in Fairness, Accountability, Transparency, and Ethics (FATE)1 has established many sources and forms of algorithmic harm, in domains as diverse as health care, finance, policing, and recommendations. Much work remains to be done to mitigate the serious harms of these systems, particularly those disproportionately affecting marginalized communities. Despite these ongoing harms, new systems are being developed and deployed, typically without strong regulatory barriers, threatening the perpetuation of the same harms and the creation of novel ones. In response, the FATE community has emphasized the importance of anticipating harms, rather than just responding to them. Anticipation of harms is especially important given the rapid pace of developments in machine learning (ML). Our work focuses on the anticipation of harms from increasingly agentic systems. Rather than providing a definition of agency as a binary property, we identify 4 key characteristics which, particularly in combination, tend to increase the agency of a given algorithmic system: underspecification, directness of impact, goal-directedness, and long-term planning. We also discuss important harms which arise from increasing agency – notably, these include systemic and/or long-range impacts, often on marginalized or unconsidered stakeholders. We emphasize that recognizing agency of algorithmic systems does not absolve or shift the human responsibility for algorithmic harms. Rather, we use the term agency to highlight the increasingly evident fact that ML systems are not fully under human control. Our work explores increasingly agentic algorithmic systems in three parts. First, we explain the notion of an increase in agency for algorithmic systems in the context of diverse perspectives on agency across disciplines. Second, we argue for the need to anticipate harms from increasingly agentic systems. Third, we discuss important harms from increasingly agentic systems and ways forward for addressing them. We conclude by reflecting on implications of our work for anticipating algorithmic harms from emerging systems.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {651–666},
numpages = {16},
keywords = {FATE, agency, algorithmic systems, autonomy, delayed impacts, ethics, harms, negative externalities, power, safety, sociotechnical systems},
location = {Chicago, IL, USA},
series = {FAccT '23}
}

@article{10.1145/3672456,
author = {Jiang, Xue and others},
title = {Self-Planning Code Generation with Large Language Models},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {7},
issn = {1049-331X},
abstract = {Although large language models (LLMs) have demonstrated impressive ability in code generation, they are still struggling to address the complicated intent provided by humans. It is widely acknowledged that humans typically employ planning to decompose complex problems and schedule solution steps prior to implementation. To this end, we introduce planning into code generation to help the model understand complex intent and reduce the difficulty of problem-solving. This paper proposes a self-planning code generation approach with large language models, which consists of two phases, namely planning phase and implementation phase. Specifically, in the planning phase, LLM plans out concise solution steps from the intent combined with few-shot prompting. Subsequently, in the implementation phase, the model generates code step by step, guided by the preceding solution steps. We conduct extensive experiments on various code-generation benchmarks across multiple programming languages. Experimental results show that self-planning code generation achieves a relative improvement of up to 25.4\% in Pass@1 compared to direct code generation, and up to 11.9\% compared to Chain-of-Thought of code generation. Moreover, our self-planning approach also enhances the quality of the generated code with respect to correctness, readability, and robustness, as assessed by humans.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = sep,
articleno = {182},
numpages = {30},
keywords = {Code Generation, Large language models, Planning}
}

@inproceedings{puerto-etal-2023-metaqa,
title = "{M}eta{QA}: Combining Expert Agents for Multi-Skill Question Answering",
author = {Puerto, Haritz  and
{\c{S}}ahin, G{\"o}zde  and
Gurevych, Iryna},
editor = "Vlachos, Andreas  and
Augenstein, Isabelle",
booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
month = may,
year = "2023",
address = "Dubrovnik, Croatia",
publisher = "Association for Computational Linguistics",
pages = "3566--3580",
abstract = "The recent explosion of question-answering (QA) datasets and models has increased the interest in the generalization of models across multiple domains and formats by either training on multiple datasets or combining multiple models. Despite the promising results of multi-dataset models, some domains or QA formats may require specific architectures, and thus the adaptability of these models might be limited. In addition, current approaches for combining models disregard cues such as question-answer compatibility. In this work, we propose to combine expert agents with a novel, flexible, and training-efficient architecture that considers questions, answer predictions, and answer-prediction confidence scores to select the best answer among a list of answer predictions. Through quantitative and qualitative experiments, we show that our model i) creates a collaboration between agents that outperforms previous multi-agent and multi-dataset approaches, ii) is highly data-efficient to train, and iii) can be adapted to any QA format. We release our code and a dataset of answer predictions from expert agents for 16 QA datasets to foster future research of multi-agent systems.",
}

@inproceedings{wang-etal-2024-rethinking-bounds,
title = "Rethinking the Bounds of {LLM} Reasoning: Are Multi-Agent Discussions the Key?",
author = "Wang, Qineng  and
others",
booktitle = "Proceedings of the Annual Meeting of the Association for Computational Linguistics",
month = "Aug.",
year = "2024"
}
@inproceedings{pandey-etal-2024-advancing,
title = "Advancing Healthcare Automation: Multi-Agent System for Medical Necessity Justification",
author = "Pandey, Himanshu Gautam  and
Amod, Akhil  and
Kumar, Shivang",
editor = "Demner-Fushman, Dina  and
Ananiadou, Sophia  and
Miwa, Makoto  and
Roberts, Kirk  and
Tsujii, Junichi",
booktitle = "Proceedings of the 23rd Workshop on Biomedical Natural Language Processing",
month = aug,
year = "2024",
address = "Bangkok, Thailand",
publisher = "Association for Computational Linguistics",
pages = "39--49",
abstract = "Prior Authorization delivers safe, appropriate, and cost-effective care that is medically justified with evidence-based guidelines. However, the process often requires labor-intensive manual comparisons between patient medical records and clinical guidelines, that is both repetitive and time-consuming. Recent developments in Large Language Models (LLMs) have shown potential in addressing complex medical NLP tasks with minimal supervision. This paper explores the application of Multi-Agent System (MAS) that utilize specialized LLM agents to automate Prior Authorization task by breaking them down into simpler and manageable sub-tasks. Our study systematically investigates the effects of various prompting strategies on these agents and benchmarks the performance of different LLMs. We demonstrate that GPT-4 achieves an accuracy of 86.2{\%} in predicting checklist item-level judgments with evidence, and 95.6{\%} in determining overall checklist judgment. Additionally, we explore how these agents can contribute to explainability of steps taken in the process, thereby enhancing trust and transparency in the system.",
}
@inproceedings{zhang-etal-2024-psysafe,
title = "{P}sy{S}afe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety",
author = "Zhang, Zaibin  and
others",
booktitle = "Proceedings of the Annual Meeting of the Association for Computational Linguistics",
month = "Aug.",
year = "2024"
}
@inproceedings{wu-etal-2024-deciphering,
title = "Deciphering Digital Detectives: Understanding {LLM} Behaviors and Capabilities in Multi-Agent Mystery Games",
author = "Wu, Dekun  and
others",
editor = "Ku, Lun-Wei  and
Martins, Andre  and
Srikumar, Vivek",
booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
month = aug,
year = "2024",
address = "Bangkok, Thailand and virtual meeting",
publisher = "Association for Computational Linguistics",
pages = "8225--8291",
abstract = "In this study, we explore the application of Large Language Models (LLMs) in Jubensha, a Chinese detective role-playing game and a novel area in Artificial Intelligence (AI) driven gaming. We introduce the first dataset specifically for Jubensha, including character scripts and game rules, to foster AI agent development in this complex narrative environment. Our work also presents a unique multi-agent interaction framework using LLMs, allowing AI agents to autonomously engage in Jubensha games. To evaluate the gaming performance of these AI agents, we developed novel methods measuring their mastery of case information and reasoning skills. Furthermore, we incorporated the latest advancements in prompting engineering to enhance the agents{'} performance in information gathering, murderer identification, and logical reasoning. The experimental results validate the effectiveness of our proposed methods. This work aims to offer a novel perspective on understanding LLM capabilities and establish a new benchmark for evaluating large language model-based agents.",
}
@inproceedings{almutairi-etal-2024-synthetic,
title = "Synthetic {A}rabic Medical Dialogues Using Advanced Multi-Agent {LLM} Techniques",
author = "ALMutairi, Mariam  and
others",
editor = "Habash, Nizar  and
Bouamor, Houda  and
Eskander, Ramy  and
Tomeh, Nadi  and
Abu Farha, Ibrahim  and
Abdelali, Ahmed  and
Touileb, Samia  and
Hamed, Injy  and
Onaizan, Yaser  and
Alhafni, Bashar  and
Antoun, Wissam  and
Khalifa, Salam  and
Haddad, Hatem  and
Zitouni, Imed  and
AlKhamissi, Badr  and
Almatham, Rawan  and
Mrini, Khalil",
booktitle = "Proceedings of The Second Arabic Natural Language Processing Conference",
month = aug,
year = "2024",
address = "Bangkok, Thailand",
publisher = "Association for Computational Linguistics",
pages = "11--26",
abstract = "The increasing use of artificial intelligence in healthcare requires robust datasets for training and validation, particularly in the domain of medical conversations. However, the creation and accessibility of such datasets in Arabic face significant challenges, especially due to the sensitivity and privacy concerns that are associated with medical conversations. These conversations are rarely recorded or preserved, making the availability of comprehensive Arabic medical dialogue datasets scarce. This limitation slows down not only the development of effective natural language processing models but also restricts the opportunity for open comparison of algorithms and their outcomes. Recent advancements in large language models (LLMs) like ChatGPT, GPT-4, Gemini-pro, and Claude-3 show promising capabilities in generating synthetic data. To address this gap, we introduce a novel Multi-Agent LLM approach capable of generating synthetic Arabic medical dialogues from patient notes, regardless of the original language. This development presents a significant step towards overcoming the barriers in dataset availability, enhancing the potential for broader research and application in AI-driven medical dialogue systems.",
}
@misc{wu2024perhapshumantranslationharnessing,
title={(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts},
author={Minghao Wu and others},
year={2024},
eprint={2405.11804},
archivePrefix={arXiv},
primaryClass={cs.CL},
}
@misc{liu2024controllablediversedataaugmentation,
title={Controllable and Diverse Data Augmentation with Large Language Model for Low-Resource Open-Domain Dialogue Generation},
author={Zhenhua Liu and others},
year={2024},
eprint={2404.00361},
archivePrefix={arXiv},
primaryClass={cs.CL},
}
@misc{wang2024mixtureofagentsenhanceslargelanguage,
title={Mixture-of-Agents Enhances Large Language Model Capabilities},
author={Junlin Wang and others},
year={2024},
eprint={2406.04692},
archivePrefix={arXiv},
primaryClass={cs.CL},
}
@inproceedings{xia2024llm,
title={LLM experiments with simulation: Large Language Model Multi-Agent System for Simulation Model Parametrization in Digital Twins},
author={Xia, Yuchen and others},
booktitle={2024 IEEE 29th International Conference on Emerging Technologies and Factory Automation (ETFA)},
pages={1--4},
year={2024},
organization={IEEE}
}

@inproceedings{wei2024editable,
title={Editable scene simulation for autonomous driving via collaborative llm-agents},
author={Wei, Yuxi and others},
booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
pages={15077--15087},
year={2024}
}

@inproceedings{nascimento2023self,
title={Self-adaptive large language model (llm)-based multiagent systems},
author={Nascimento, Nathalia and Alencar, Paulo and Cowan, Donald},
booktitle={2023 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion (ACSOS-C)},
pages={104--109},
year={2023},
organization={IEEE}
}
@article{yao2024velo,
title={VELO: A Vector Database-Assisted Cloud-Edge Collaborative LLM QoS Optimization Framework},
author={Yao, Zhi and others},
journal={arXiv preprint arXiv:2406.13399},
year={2024}
}
@inproceedings{das2023enabling,
title={Enabling Synergistic Knowledge Sharing and Reasoning in Large Language Models with Collaborative Multi-Agents},
author={Das, Ayushman and others},
booktitle={IEEE International Conference on Collaboration and Internet Computing},
year={2023},
}

@inproceedings{barbarroxa2024benchmarking,
title={Benchmarking AutoGen with different large language models},
author={Barbarroxa, Rafael and others},
booktitle={2024 IEEE Conference on Artificial Intelligence (CAI)},
pages={263--264},
year={2024},
organization={IEEE}
}
@misc{li2024personalllmagentsinsights,
title={Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security},
author={Yuanchun Li and others},
year={2024},
eprint={2401.05459},
archivePrefix={arXiv},
primaryClass={cs.HC},
}
@inproceedings{
zhao2024competeai,
title={Compete{AI}: Understanding the Competition Dynamics of Large Language Model-based Agents},
author={Qinlin Zhao and others},
booktitle={Agentic Markets Workshop at ICML 2024},
year={2024},
}
@inproceedings{10.1145/3586183.3606763,
author = {Park, Joon Sung and others},
title = {Generative Agents: Interactive Simulacra of Human Behavior},
year = {2023},
series = {UIST '23}
}
@inproceedings{xu2023magic,
title={Magic: Investigation of large language model powered multi-agent in cognition, adaptability, rationality and collaboration},
author={Xu, Lin and others},
booktitle={ICLR 2024 Workshop on Large Language Model (LLM) Agents},
year={2023}
}

@inproceedings{
shinn2023reflexion,
title={Reflexion: language agents with verbal reinforcement learning},
author={Noah Shinn and others},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
}
@inproceedings{liang-etal-2024-encouraging,
title = "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
author = "Liang, Tian  and
others",
editor = "Al-Onaizan, Yaser  and
Bansal, Mohit  and
Chen, Yun-Nung",
booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
month = nov,
year = "2024",
address = "Miami, Florida, USA",
publisher = "Association for Computational Linguistics",
pages = "17889--17904",
abstract = "Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of {}tit for tat{''} and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of {}tit for tat{''} state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents.",
}
@article{chen2024optima,
title={Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System},
author={Chen, Weize and others},
journal={arXiv preprint arXiv:2410.08115},
year={2024}
}
@misc{
wu2024autogen,
title={AutoGen: Enabling Next-Gen {LLM} Applications via Multi-Agent Conversation},
author={Qingyun Wu and others},
year={2024},
}
@inproceedings{
anonymous2024towards,
title={Towards Efficient {LLM} Grounding for Embodied Multi-Agent Collaboration},
author={Anonymous},
booktitle={Submitted to The Thirteenth International Conference on Learning Representations},
year={2024},
note={under review}
}
@inproceedings{
zhang2023gobigger,
title={GoBigger: A Scalable Platform for Cooperative-Competitive Multi-Agent Interactive Simulation},
author={Ming Zhang and others},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
}
@inproceedings{
kwon2023reward,
title={Reward Design with Language Models},
author={Minae Kwon and Sang Michael Xie and Kalesha Bullard and Dorsa Sadigh},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
}
@inproceedings{51647,title
	= {Do As I Can, Not As I Say: Grounding Language in Robotic 
Affordances},author	= {Alex Irpan and others},year	= 
{2022},
}
@inproceedings{
gupta2021dynamic,
title={Dynamic population-based meta-learning for multi-agent communication with natural language},
author={Abhinav Gupta and Marc Lanctot and Angeliki Lazaridou},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
}
@inproceedings{
guan2023relative,
title={Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences},
author={Lin Guan and Karthik Valmeekam and Subbarao Kambhampati},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
}
@article{
doi:10.1126/science.ade9097,
author = {Meta Fundamental AI Research Diplomacy Team (FAIR)†},
title = {Human-level play in the game of <i>Diplomacy</i> by combining language models with strategic reasoning},
journal = {Science},
volume = {378},
number = {6624},
pages = {1067-1074},
year = {2022},
doi = {10.1126/science.ade9097},
}

@inproceedings{NEURIPS2021_f702defb,
author = {Li, Yiming and Ren, Shunli and Wu, Pengxiang and Chen, Siheng and Feng, Chen and Zhang, Wenjun},
booktitle = {Advances in Neural Information Processing Systems},
editor = {M. Ranzato and others},
pages = {29541--29552},
publisher = {Curran Associates, Inc.},
title = {Learning Distilled Collaboration Graph for Multi-Agent Perception},
volume = {34},
year = {2021}
}
@inproceedings{
chan2024chateval,
title={ChatEval: Towards Better {LLM}-based Evaluators through Multi-Agent Debate},
author={Chi-Min Chan and others},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}
@inproceedings{
hong2024metagpt,
title={Meta{GPT}: Meta Programming for A Multi-Agent Collaborative Framework},
author={Sirui Hong and others},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}
@inproceedings{
liu2024agentbench,
title={AgentBench: Evaluating {LLM}s as Agents},
author={Xiao Liu and others},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}
@inproceedings{
xiao2024chainofexperts,
title={Chain-of-Experts: When {LLM}s Meet Complex Operations Research Problems},
author={Ziyang Xiao and others},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}
@inproceedings{
chen2024agentverse,
title={AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors},
author={Weize Chen and others},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}


@inproceedings{
cao2024enhancing,
title={Enhancing Human-{AI} Collaboration Through Logic-Guided Reasoning},
author={Chengzhi Cao and others},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}
@inproceedings{
jin2023lending,
title={Lending Interaction Wings to Recommender Systems with Conversational Agents},
author={Jiarui Jin and others},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
}
@inproceedings{
du2023learning,
title={Learning Universal Policies via Text-Guided Video Generation},
author={Yilun Du and others},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
}
@inproceedings{
li2023camel,
title={{CAMEL}: Communicative Agents for ''Mind'' Exploration of Large Language Model Society},
author={Guohao Li and others},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
}
@inproceedings{
borzunov2023distributed,
title={Distributed Inference and Fine-tuning of Large Language Models Over The Internet},
author={Alexander Borzunov and others},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
}
@InProceedings{Wei_2024_CVPR,
    author    = {Wei, Yuxi and others},
    title     = {Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {15077-15087}
}
@InProceedings{Tran_2024_CVPR,
    author    = {Tran, Minh-Tuan and others},
    title     = {Text-Enhanced Data-free Approach for Federated Class-Incremental Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {23870-23880}
}
@InProceedings{Li_2024_CVPR,
    author    = {Li, Hongxia and others},
    title     = {Global and Local Prompts Cooperation via Optimal Transport for Federated Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {12151-12161}
}
@InProceedings{Zhang_2024_CVPR,
    author    = {Zhang, Jianqing and others},
    title     = {An Upload-Efficient Scheme for Transferring Knowledge From a Server-Side Pre-trained Generator to Clients in Heterogeneous Federated Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {12109-12119}
}
@InProceedings{Feng_2023_CVPR,
author    = {Feng, Chun-Mei and others},
title     = {Learning Federated Visual Prompt in Null Space for MRI Reconstruction},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month     = {June},
year      = {2023},
pages     = {8064-8073}
}

@InProceedings{pmlr-v162-lang22a,
title = 	 {Co-training Improves Prompt-based Learning for Large Language Models},
author =       {Lang, Hunter and others},
booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
pages = 	 {11985--12003},
year = 	 {2022},
editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
volume = 	 {162},
series = 	 {Proceedings of Machine Learning Research},
month = 	 {17--23 Jul},
publisher =    {PMLR}
}


@InProceedings{pmlr-v235-cui24c,
  title = 	 {Harmonizing Generalization and Personalization in Federated Prompt Learning},
  author =       {Cui, Tianyu and others},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {9646--9661},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  abstract = 	 {Federated Prompt Learning (FPL) incorporates large pre-trained Vision-Language models (VLM) into federated learning through prompt tuning. The transferable representations and remarkable generalization capacity of VLM make them highly compatible with the integration of federated learning. Addressing data heterogeneity in federated learning requires personalization, but excessive focus on it across clients could compromise the model’s ability to generalize effectively. To preserve the impressive generalization capability of VLM, it is crucial to strike a balance between personalization and generalization in FPL. To tackle this challenge, we proposed Federated Prompt Learning with CLIP Generalization and low-rank Personalization (FedPGP), which employs pre-trained CLIP to provide knowledge-guidance on the global prompt for improved generalization and incorporates a low-rank adaptation term to personalize the global prompt. Further, FedPGP integrates a prompt-wise contrastive loss to achieve knowledge guidance and personalized adaptation simultaneously, enabling a harmonious balance between personalization and generalization in FPL. We conduct extensive experiments on various datasets to explore base-to-novel generalization in both category-level and domain-level scenarios with heterogeneous data, showing the superiority of FedPGP in balancing generalization and personalization.}
}

@InProceedings{pmlr-v235-piao24a,
title = 	 {Federated Continual Learning via Prompt-based Dual Knowledge Transfer},
author =       {Piao, Hongming and others},
booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
pages = 	 {40725--40739},
year = 	 {2024},
editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
volume = 	 {235},
series = 	 {Proceedings of Machine Learning Research},
month = 	 {21--27 Jul},
publisher =    {PMLR},
abstract = 	 {In Federated Continual Learning (FCL), the challenge lies in effectively facilitating knowledge transfer and enhancing the performance across various tasks on different clients. Current FCL methods predominantly focus on avoiding interference between tasks, thereby overlooking the potential for positive knowledge transfer across tasks learned by different clients at separate time intervals. To address this issue, we introduce a <b>P</b>rompt-based kn<b>ow</b>le<b>d</b>ge transf<b>er</b> FCL algorithm, called <b>Powder</b>, designed to effectively foster the transfer of knowledge encapsulated in prompts between various sequentially learned tasks and clients. Furthermore, we have devised a unique approach for prompt generation and aggregation, intending to alleviate privacy protection concerns and communication overhead, while still promoting knowledge transfer. Comprehensive experimental results demonstrate the superiority of our method in terms of reduction in communication costs, and enhancement of knowledge transfer. Code is available at https://github.com/piaohongming/Powder.}
}

@InProceedings{pmlr-v235-sun24j,
title = 	 {{F}ed{BPT}: Efficient Federated Black-box Prompt Tuning for Large Language Models},
author =       {Sun, Jingwei and others},
booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
pages = 	 {47159--47173},
year = 	 {2024},
editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
volume = 	 {235},
series = 	 {Proceedings of Machine Learning Research},
month = 	 {21--27 Jul},
publisher =    {PMLR},
abstract = 	 {Pre-trained language models (PLM) have revolutionized the NLP landscape, achieving stellar performances across diverse tasks. These models, while benefiting from vast training data, often require fine-tuning on specific data to cater to distinct downstream tasks. However, this data adaptation process has inherent security and privacy concerns, primarily when leveraging user-generated, device-residing data. Federated learning (FL) provides a solution, allowing collaborative model fine-tuning without centralized data collection. However, applying FL to finetune PLMs is hampered by challenges, including restricted model parameter access due to the high encapsulation, high computational requirements, and communication overheads. This paper introduces Federated Black-box Prompt Tuning (FedBPT), a framework designed to address these challenges. FedBPT allows the clients to treat the model as a black-box inference API. By focusing on training optimal prompts and utilizing gradient-free optimization methods, FedBPT reduces the number of exchanged variables, boosts communication efficiency, and minimizes computational and storage costs. Experiments highlight the framework’s ability to drastically cut communication and memory costs while maintaining competitive performance. Ultimately, FedBPT presents a promising solution for efficient, privacy-preserving fine-tuning of PLM in the age of large language models.}
}

@InProceedings{pmlr-v235-hou24c,
title = 	 {{P}r{E}-Text: Training Language Models on Private Federated Data in the Age of {LLM}s},
author =       {Hou, Charlie and others},
booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
pages = 	 {19043--19061},
year = 	 {2024},
editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
volume = 	 {235},
series = 	 {Proceedings of Machine Learning Research},
month = 	 {21--27 Jul},
publisher =    {PMLR},
abstract = 	 {On-device training is currently the most common approach for training machine learning (ML) models on private, distributed user data. Despite this, on-device training has several drawbacks: (1) most user devices are too small to train large models on-device, (2) on-device training is communication- and computation-intensive, and (3) on-device training can be difficult to debug and deploy. To address these problems, we propose Private Evolution-Text (PrE-Text), a method for generating differentially private (DP) synthetic textual data. First, we show that across multiple datasets, training small models (models that fit on user devices) with PrE-Text synthetic data outperforms small models trained on-device under practical privacy regimes ($\epsilon=1.29$, $\epsilon=7.58$). We achieve these results while using 9$\times$ fewer rounds, 6$\times$ less client computation per round, and 100$\times$ less communication per round. Second, finetuning large models on PrE-Text’s DP synthetic data improves large language model (LLM) performance on private data across the same range of privacy budgets. Altogether, these results suggest that training on DP synthetic data can be a better option than training a model on-device on private distributed data. Code is available at https://github.com/houcharlie/PrE-Text.}
}
@ARTICLE{10666083,

  author={Wu, Panlong and others},

  journal={IEEE Transactions on Mobile Computing}, 

  title={FedFMSL: Federated Learning of Foundation Models With Sparsely Activated LoRA}, 

  year={2024},

  volume={23},

  number={12},

  pages={15167-15181},

  keywords={Frequency modulation;Training;Federated learning;Logic gates;Data models;Adaptation models;Computational modeling;Edge computing;federated learning;foundation model},

}

@ARTICLE{10058174,

  author={Xu, Liang and others},

  journal={IEEE Transactions on Computational Social Systems}, 

  title={Knowledge Graph-Based Reinforcement Federated Learning for Chinese Question and Answering}, 

  year={2024},

  volume={11},

  number={1},

  pages={1035-1045},

  keywords={Data models;Semantics;Federated learning;Knowledge graphs;Data privacy;Transformers;Task analysis;Knowledge graph;multitask semantic parsing [MSP-bidirectional and auto-regressive transformers (BART)];prompt learning;question and answering (Q&A);reinforcement federated learning (RFL)},

}

@ARTICLE{10721229,
author={Qiang Wang, Zhi and Wang, Haopeng and El Saddik, Abdulmotaleb},
journal={IEEE Access},
title={FedITD: A Federated Parameter-Efficient Tuning With Pre-Trained Large Language Models and Transfer Learning Framework for Insider Threat Detection},
year={2024},
volume={12},
number={},
pages={160396-160417},
keywords={Data models;Adaptation models;Threat assessment;Tuning;Security;Organizations;Costs;Computational modeling;Transfer learning;Deep learning;Computer security;Data augmentation;Artificial intelligence;Machine learning;Cybersecurity;insider threat;deep learning;transformer;BERT;RoBERTa;XLNet;DistilBERT;GPT;data augmentation;artificial intelligence;machine learning;pre-trained LLM;PETuning;adapter;LoRA;BitFit;LLM;NLP},
}

@ARTICLE{10384606,
  author={Shen, Yifei and others},
  journal={IEEE Communications Magazine}, 
  title={Large Language Models Empowered Autonomous Edge AI for Connected Intelligence}, 
  year={2024},
  volume={62},
  number={10},
  pages={140-146},
  keywords={Artificial intelligence;Codes;Sensors;Adaptation models;Task analysis;Servers;Computational modeling;Large language models},
}

@ARTICLE{10557150,
  author={Feng, Xiachong and others},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Adapter-Based Selective Knowledge Distillation for Federated Multi-Domain Meeting Summarization}, 
  year={2024},
  volume={32},
  number={},
  pages={3694-3708},
  keywords={Adaptation models;Servers;Federated learning;Data models;Task analysis;Training;Optimization;Meeting summarization;federated learning;knowledge distillation;parameter-efficient fine-tuning},
}

@ARTICLE{10210127,
  author={Guo, Tao and others},
  journal={IEEE Transactions on Mobile Computing}, 
  title={PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models – Federated Learning in Age of Foundation Model}, 
  year={2024},
  volume={23},
  number={5},
  pages={5179-5194},
  keywords={Training;Data models;Servers;Frequency modulation;Federated learning;Adaptation models;Computational modeling;Edge computing;prompt federated learning;vision-language model},
}

@INPROCEEDINGS{10554888,
author={Cai, Zeju and others},
booktitle={2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)},
title={F-CodeLLM: A Federated Learning Framework for Adapting Large Language Models to Practical Software Development},
year={2024},volume={},number={},pages={416-417},
keywords={Data privacy;Codes;Federated learning;Computational modeling;Organizations;Task analysis;Software development management;Code Intelligence;Federated Fine-Tuning;Large Language Model;Software Development}
}
@ARTICLE{10629177,
author={Guo, Tao and Guo, Song and Wang, Junxiao},
journal={IEEE Transactions on Mobile Computing},
title={Explore and Cure: Unveiling Sample Effectiveness With Context-Aware Federated Prompt Tuning},
year={2024},
volume={23},
number={12},
pages={14044-14054},
keywords={Vectors;Task analysis;Tuning;Training;Computational modeling;Data models;Servers;Context-aware prompt tuning;prompt federated learning;vision-language model}
}

@INPROCEEDINGS{9892457,
author={Mu, Hongzhang and Liu, Tingwen and Xu, Hongbo},
booktitle={2022 International Joint Conference on Neural Networks (IJCNN)},
title={Learning-to-Learn Agent Adaptation Policy for Abstractive Summarization},
year={2022},
volume={},
number={},
pages={1-7},
keywords={Adaptation models;Neural networks;Cognition;Task analysis;abstractive summarization;dynamic reasoning;Learning-to-Learn Agent Adaption policy}}

@INPROCEEDINGS{10651475,
author={Zhang, Shuili and Mu, Hongzhang and Liu, Tingwen},
booktitle={2024 International Joint Conference on Neural Networks (IJCNN)},
title={Improving Accuracy and Generalizability via Multi-Modal Large Language Models Collaboration},
year={2024},
volume={},
number={},
pages={1-8},
keywords={Training;Visualization;Accuracy;Federated learning;Large language models;Neurons;Collaboration;Multi-Modal Large Language Model;MultiAgent Collaboration}
}

@ARTICLE{9829327,
  author={Nguyen, Minh-Duong and others},
  journal={IEEE Transactions on Mobile Computing}, 
  title={HCFL: A High Compression Approach for Communication-Efficient Federated Learning in Very Large Scale IoT Networks}, 
  year={2023},
  volume={22},
  number={11},
  pages={6495-6507}
}

@INPROCEEDINGS{9892845,
author={Lit, Zhengyang and others},
booktitle={2022 International Joint Conference on Neural Networks (IJCNN)},
title={Federated Split BERT for Heterogeneous Text Classification},
year={2022},
volume={},
number={},
pages={1-8},
keywords={Data privacy;Costs;Quantization (signal);Federated learning;Bit error rate;Text categorization;Natural language processing;Federated Learning;BERT;Data Heterogeneity;Quantization;Text Classification}
}

@INPROCEEDINGS{10095215,
author={Li, Shuyue Stella and others},
booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={PQLM - Multilingual Decentralized Portable Quantum Language Model},
year={2023},
volume={},
number={},
pages={1-5},
keywords={Training;Sentiment analysis;Social networking (online);Computational modeling;Signal processing;Stability analysis;Servers;Quantum Machine Learning;Language Modeling;Federated Learning;Model Portability}
}

@INPROCEEDINGS{10096570,
author={Xu, Mingbin and others},
booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Training Large-Vocabulary Neural Language Models by Private Federated Learning for Resource-Constrained Devices},
year={2023},
volume={},
number={},
pages={1-5},
keywords={Training;Adaptation models;Privacy;Differential privacy;Vocabulary;Federated learning;Computational modeling}
}

@INPROCEEDINGS{9747113,
author={Dupuy, Christophe and others},
booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Learnings from Federated Learning in The Real World},
year={2022},
volume={},
number={},
pages={8767-8771},
keywords={Training;Performance evaluation;Natural languages;Distributed databases;Signal processing algorithms;Signal processing;Collaborative work;Federated Learning;NLU}
}

@INPROCEEDINGS{8852464,
author={Ji, Shaoxiong and others},
booktitle={2019 International Joint Conference on Neural Networks (IJCNN)},
title={Learning Private Neural Language Modeling with Attentive Aggregation},
year={2019},
volume={},
number={},
pages={1-8},
keywords={Servers;Optimization;Training;Adaptation models;Data models;Distributed databases;Neural networks;federated learning;language modeling;attentive aggregation}
}

@INPROCEEDINGS{10621164,
author={Su, Ningxin and others},
booktitle={IEEE INFOCOM 2024 - IEEE Conference on Computer Communications},
title={Titanic: Towards Production Federated Learning with Large Language Models},
year={2024},
volume={},
number={},
pages={611-620},
keywords={Training;Performance evaluation;Data privacy;Federated learning;Computational modeling;Large language models;Bandwidth}
}

@ARTICLE{10416403,
author={Wu, Hongrun and others},
journal={IEEE Transactions on Computational Social Systems},
title={Agent-Network-Computation-Based Evolutionary Game Model in Language Competition},
year={2024},
volume={11},
number={3},
pages={4226-4241},
keywords={Biological system modeling;Mathematical models;Games;Statistics;Sociology;Data models;Microscopy;Agent network computation (ANC);evolutionary game simulation;language competition;language endangerment;replicator dynamic equation;social dynamical systems}
}

@ARTICLE{10439991,
author={Zhong, Ningze and others},
journal={IEEE Internet of Things Journal},
title={CASIT: Collective Intelligent Agent System for Internet of Things},
year={2024},
volume={11},
number={11},
pages={19646-19656},
keywords={Internet of Things;Edge computing;Task analysis;Intelligent agents;Bandwidth;Natural languages;Multi-agent systems;Collective intelligent agent system;collective wisdom;Internet of Things (IoT);large language model (LLM)}
}

@ARTICLE{9667188,
author={Silva, Andrew and others},
journal={IEEE Robotics and Automation Letters},
title={LanCon-Learn: Learning With Language to Enable Generalization in Multi-Task Manipulation},
year={2022},
volume={7},
number={2},
pages={1635-1642},
keywords={Task analysis;Multitasking;Robots;Natural languages;Reinforcement learning;Encoding;Navigation;Deep learning methods;imitation learning;reinforcement learning}
}

@ARTICLE{10237194,
author={Soonpipatskul, Nattida and others},
journal={IEEE Access},
title={Personality Perceptions of Conversational Agents: A Task-Based Analysis Using Thai as the Conversational Language},
year={2023},
volume={11},
number={},
pages={94545-94562},
keywords={Task analysis;Virtual assistants;Taxonomy;Psychology;Internet;User experience;Systematics;Social factors;Artificial intelligence;Natural language processing;Big five;conversational agents;multi-criteria decision making;functional tasks;personality;social tasks}
}

@ARTICLE{10056996,
author={Zhang, Chen and others},
journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
title={PoE: A Panel of Experts for Generalized Automatic Dialogue Assessment},
year={2023},
volume={31},
number={},
pages={1234-1250},
keywords={Measurement;Transformers;Task analysis;Adaptation models;Training;Speech processing;Correlation;Adapters;automatic dialogue evaluation;multi-domain generalization;multitask learning}
}

@ARTICLE{9468337,
author={Zhu, Ye and others},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Saying the Unseen: Video Descriptions via Dialog Agents},
year={2022},
volume={44},
number={10},
pages={7190-7204},
keywords={Task analysis;Visualization;Artificial intelligence;Natural languages;Knowledge transfer;Semantics;Video description;Video description;dialog agents;multi-modal learning}
}

@ARTICLE{10720863,
author={Zhou, Li and others},
journal={IEEE Transactions on Cognitive Communications and Networking},
title={Semantic Information Extraction and Multi-Agent Communication Optimization Based on Generative Pre-Trained Transformer},
year={2024},
volume={},
number={},
pages={1-1},
keywords={Bandwidth;Data mining;Data models;Training;Data communication;Computational modeling;Multi-agent systems;Large language models;Generative adversarial networks;Feature extraction;generative ai;multi-agent;reinforcement learning;semantic communication}
}

@ARTICLE{10745878,
author={Jiang, Kemou and others},
journal={IEEE Transactions on Intelligent Vehicles},
title={KoMA: Knowledge-Driven Multi-Agent Framework for Autonomous Driving With Large Language Models},
year={2024},
volume={},
number={},
pages={1-15},
keywords={Cognition;Autonomous vehicles;Reflection;Safety;Memory modules;Complexity theory;Planning;Collaboration;Vehicle dynamics;Training;Autonomous driving;large language models;multi agents;shared memory;multi-step planning;chain of thought}
}

@ARTICLE{10681241,
author={Zhou, Yijie and others},
journal={IEEE Transactions on Intelligent Vehicles},
title={ALGPT: Multi-Agent Cooperative Framework for Open-Vocabulary Multi-Modal Auto-Annotating in Autonomous Driving},
year={2024},
volume={},
number={},
pages={1-15},
keywords={Annotations;Intelligent agents;Autonomous vehicles;Manuals;Cognition;Three-dimensional displays;Standards;Automatic annotation;autonomous driving;multi-agents;multi-modal data annotation}
}

@ARTICLE{10638533,
author={Jiang, Feibo and others},
journal={IEEE Wireless Communications},
title={Large Language Model Enhanced Multi-Agent Systems for 6G Communications},
year={2024},
volume={},
number={},
pages={1-8},
keywords={6G mobile communication;Task analysis;Knowledge engineering;Artificial intelligence;Multi-agent systems;Communication systems;Cognition}
}

@inproceedings{10.5555/3635637.3662979,
author = {Liu, Jijia and others},
title = {LLM-Powered Hierarchical Language Agent for Real-time Human-AI Coordination},
year = {2024},
isbn = {9798400704864},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {AI agents powered by Large Language Models (LLMs) have made significant advances, enabling them to assist humans in diverse complex tasks and leading to a revolution in human-AI coordination. LLM-powered agents typically require invoking LLM APIs and employing artificially designed complex prompts, which results in high inference latency. While this paradigm works well in scenarios with minimal interactive demands, such as code generation, it is unsuitable for highly interactive and real-time applications, such as gaming. Traditional gaming AI often employs small models or reactive policies, enabling fast inference but offering limited task completion and interaction abilities. In this work, we consider Overcooked as our testbed where players could communicate with natural language and cooperate to serve orders. We propose a Hierarchical Language Agent (HLA) for human-AI coordination that provides both strong reasoning abilities while keeping real-time execution. In particular, HLA adopts a hierarchical framework and comprises three modules: a proficient LLM, referred to as Slow Mind, for intention reasoning and language interaction, a lightweight LLM, referred to as Fast Mind, for generating macro actions, and a reactive policy, referred to as Executor, for transforming macro actions into atomic actions. Human studies show that HLA outperforms other baseline agents, including slow-mind-only agents and fast-mind-only agents, with stronger cooperation abilities, faster responses, and more consistent language communications.},
booktitle = {Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems},
pages = {1219–1228},
numpages = {10},
keywords = {hierarchical reasoning and planning, language agents, large language models, real-time human-ai coordination},
location = {Auckland, New Zealand},
series = {AAMAS '24}
}

@inproceedings{
davidson2024evaluating,
title={Evaluating Language Model Agency Through Negotiations},
author={Tim Ruben Davidson and others},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}

@inproceedings{
abdelnabi2024cooperation,
title={Cooperation, Competition, and Maliciousness: {LLM}-Stakeholders Interactive Negotiation},
author={Sahar Abdelnabi and others},
booktitle={The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2024},
}

@misc{cai2024surveymixtureexperts,
      title={A Survey on Mixture of Experts}, 
      author={Weilin Cai and others},
      year={2024},
      eprint={2407.06204},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@inproceedings{ahn2021nested,
  title={Nested mixture of experts: Cooperative and competitive learning of hybrid dynamical system},
  author={Ahn, Junhyeok and Sentis, Luis},
  booktitle={Learning for Dynamics and Control},
  pages={779--790},
  year={2021},
  organization={PMLR}
}

@inproceedings{jeyakumar2024advancing,
title={Advancing Agentic Systems: Dynamic Task Decomposition, Tool Integration and Evaluation using Novel Metrics and Dataset},
author={Shankar Kumar Jeyakumar and Alaa Alameer Ahmad and Adrian Garret Gabriel},
booktitle={NeurIPS 2024 Workshop on Open-World Agents},
year={2024},
}
@misc{lu2024mergeensemblecooperatesurvey,
title={Merge, Ensemble, and Cooperate! A Survey on Collaborative Strategies in the Era of Large Language Models},
author={Jinliang Lu and others},
year={2024},
eprint={2407.06089},
archivePrefix={arXiv},
primaryClass={cs.CL},
}
@misc{talebirad2023multiagentcollaborationharnessingpower,
title={Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents},
author={Yashar Talebirad and Amirhossein Nadiri},
year={2023},
eprint={2306.03314},
archivePrefix={arXiv},
primaryClass={cs.AI},
}

@misc{xi2023risepotentiallargelanguage,
title={The Rise and Potential of Large Language Model Based Agents: A Survey},
author={Zhiheng Xi and others},
year={2023},
eprint={2309.07864},
archivePrefix={arXiv},
primaryClass={cs.AI},
}

@article{Gao2024,
title = {Large language models empowered agent-based modeling and simulation: a survey and perspectives},
volume = {11},
ISSN = {2662-9992},
number = {1},
journal = {Humanities and Social Sciences Communications},
publisher = {Springer Science and Business Media LLC},
author = {Gao,  Chen and others},
year = {2024},
month = sep
}

@misc{han2024llmmultiagentsystemschallenges,
      title={LLM Multi-Agent Systems: Challenges and Open Problems}, 
      author={Shanshan Han and others},
      year={2024},
      eprint={2402.03578},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
}

@misc{li2023deepmodelfusionsurvey,
      title={Deep Model Fusion: A Survey}, 
      author={Weishi Li and others},
      year={2023},
      eprint={2309.15698},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@inproceedings{
    zhuang2024foundationmodelmeetsfederated,
    title={When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions}, 
    author={Weiming Zhuang and Chen Chen and Lingjuan Lyu}, 
    booktitle="NIPS FL@FM",
    year={2023},
    month={Dec.}
}

@inproceedings{
    xu-etal-2023-federated,
    title = "Federated Learning of Gboard Language Models with Differential Privacy",
    author = "Xu, Zheng and others",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)",
    month = {Jul.},
    year = {2023}
}

@article{10.1145/3494834.3500240,
    author = {Bonawitz, Kallista and others},
    title = {Federated Learning and Privacy: Building privacy-preserving systems for machine learning and data science on decentralized data},
    year = {2021},
    publisher = {Association for Computing Machinery},
    journal = {Queue},
    month = {Nov.}
}

@inproceedings{NEURIPS2020_1457c0d6,
    author = {Brown, Tom and others},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
    pages = {1877--1901},
    publisher = {Curran Associates, Inc.},
    title = {Language Models are Few-Shot Learners},
    volume = {33},
    year = {2020}
}

@ARTICLE{10398264,
    author={Huang, Xumin and others},
    journal={IEEE Network}, 
    title={Federated Learning-Empowered AI-Generated Content in Wireless Networks}, 
    year={2024},
    volume={38},
    number={5},
    pages={304-313},
}

@inproceedings{10.1145/3637528.3671897,
author = {Wu, Feijie and others},
title = {FedBiOT: LLM Local Fine-tuning in Federated Learning without Full Model},
year = {2024},
isbn = {9798400704901},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
pages = {3345–3355},
numpages = {11},
keywords = {federated learning, large language models},
location = {Barcelona, Spain},
series = {KDD '24}
}

@inproceedings{
liu2024a,
title={A Dynamic {LLM}-Powered Agent Network for Task-Oriented Agent Collaboration},
author={Zijun Liu and others},
booktitle={First Conference on Language Modeling},
year={2024},
month={Oct.},
}

% Centralised
@inproceedings{
qin2024federated,
    title={Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes},
    author={Zhen Qin and Daoyuan Chen and Bingchen Qian and Bolin Ding and Yaliang Li and Shuiguang Deng},
    booktitle={Forty-first International Conference on Machine Learning},
    year={2024},
}


@InProceedings{pmlr-v235-cui24c,
    title = 	 {Harmonizing Generalization and Personalization in Federated Prompt Learning},
    author =       {Cui, Tianyu and others},
    booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
    pages = 	 {9646--9661},
    year = 	 {2024},
    editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
    volume = 	 {235},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {21--27 Jul},
    publisher =    {PMLR},
}

@InProceedings{pmlr-v235-piao24a,
    title = 	 {Federated Continual Learning via Prompt-based Dual Knowledge Transfer},
    author =       {Piao, Hongming and others},
    booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
    pages = 	 {40725--40739},
    year = 	 {2024},
    editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
    volume = 	 {235},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {21--27 Jul},
    publisher =    {PMLR},
}

@InProceedings{pmlr-v235-sun24j,
    title = 	 {{F}ed{BPT}: Efficient Federated Black-box Prompt Tuning for Large Language Models},
    author =       {Sun, Jingwei and others},
    booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
    pages = 	 {47159--47173},
    year = 	 {2024},
    editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
    volume = 	 {235},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {21--27 Jul},
    publisher =    {PMLR},
}

@InProceedings{pmlr-v235-hou24c,
    title = 	 {{P}r{E}-Text: Training Language Models on Private Federated Data in the Age of {LLM}s},
    author =       {Hou, Charlie and others},
    booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
    pages = 	 {19043--19061},
    year = 	 {2024},
    editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
    volume = 	 {235},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {21--27 Jul},
    publisher =    {PMLR},
}

@ARTICLE{10666083,
    author={Wu, Panlong and others},
    journal={IEEE Transactions on Mobile Computing}, 
    title={FedFMSL: Federated Learning of Foundation Models With Sparsely Activated LoRA},
    year={2024},
    volume={23},
    number={12},
    pages={15167-15181},
    keywords={Frequency modulation;Training;Federated learning;Logic gates;Data models;Adaptation models;Computational modeling;Edge computing;federated learning;foundation model},
}

@ARTICLE{10058174,
    author={Xu, Liang and others},
    journal={IEEE Transactions on Computational Social Systems}, 
    title={Knowledge Graph-Based Reinforcement Federated Learning for Chinese Question and Answering}, 
    year={2024},
    volume={11},
    number={1},
    pages={1035-1045},
}

@ARTICLE{10721229,
    author={Qiang Wang, Zhi and Wang, Haopeng and El Saddik, Abdulmotaleb},
    journal={IEEE Access},
    title={FedITD: A Federated Parameter-Efficient Tuning With Pre-Trained Large Language Models and Transfer Learning Framework for Insider Threat Detection},
    year={2024},
    volume={12},
    number={},
    pages={160396-160417},
    keywords={Data models;Adaptation models;Threat assessment;Tuning;Security;Organizations;Costs;Computational modeling;Transfer learning;Deep learning;Computer security;Data augmentation;Artificial intelligence;Machine learning;Cybersecurity;insider threat;deep learning;transformer;BERT;RoBERTa;XLNet;DistilBERT;GPT;data augmentation;artificial intelligence;machine learning;pre-trained LLM;PETuning;adapter;LoRA;BitFit;LLM;NLP},
}

@ARTICLE{10384606,
    author={Shen, Yifei and others},
    journal={IEEE Communications Magazine}, 
    title={Large Language Models Empowered Autonomous Edge AI for Connected Intelligence}, 
    year={2024},
    volume={62},
    number={10},
    pages={140-146},
    keywords={Artificial intelligence;Codes;Sensors;Adaptation models;Task analysis;Servers;Computational modeling;Large language models},
}

@ARTICLE{10557150,
    author={Feng, Xiachong and others},
    journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
    title={Adapter-Based Selective Knowledge Distillation for Federated Multi-Domain Meeting Summarization}, 
    year={2024},
    volume={32},
    number={},
    pages={3694-3708},
    keywords={Adaptation models;Servers;Federated learning;Data models;Task analysis;Training;Optimization;Meeting summarization;federated learning;knowledge distillation;parameter-efficient fine-tuning},
}

@ARTICLE{10210127,
  author={Guo, Tao and others},
  journal={IEEE Transactions on Mobile Computing}, 
  title={PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models – Federated Learning in Age of Foundation Model}, 
  year={2024},
  volume={23},
  number={5},
  pages={5179-5194},
  keywords={Training;Data models;Servers;Frequency modulation;Federated learning;Adaptation models;Computational modeling;Edge computing;prompt federated learning;vision-language model},
  }

@ARTICLE{10614356,
  author={Lee, Chae-Won and Lee, Jae-Hong and Chang, Joon-Hyuk},
  journal={IEEE Signal Processing Letters}, 
  title={Language Model Personalization for Speech Recognition: A Clustered Federated Learning Approach With Adaptive Weight Average}, 
  year={2024},
  volume={31},
  number={},
  pages={2710-2714},
  keywords={Data models;Adaptation models;Mathematical models;Federated learning;Training;Speech recognition;Degradation;Automatic speech recognition;personalization;language model;federated learning;non-i.i.d;weight average;clustered federated learning},
}

@INPROCEEDINGS{10554888,author={Cai, Zeju and others},booktitle={2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)},title={F-CodeLLM: A Federated Learning Framework for Adapting Large Language Models to Practical Software Development},year={2024},volume={},number={},pages={416-417},keywords={Data privacy;Codes;Federated learning;Computational modeling;Organizations;Task analysis;Software development management;Code Intelligence;Federated Fine-Tuning;Large Language Model;Software Development},}

@ARTICLE{10629177,
author={Guo, Tao and Guo, Song and Wang, Junxiao},
journal={IEEE Transactions on Mobile Computing},
title={Explore and Cure: Unveiling Sample Effectiveness With Context-Aware Federated Prompt Tuning},
year={2024},
volume={23},
number={12},
pages={14044-14054},
keywords={Vectors;Task analysis;Tuning;Training;Computational modeling;Data models;Servers;Context-aware prompt tuning;prompt federated learning;vision-language model},
}

@misc{pan2024agentcoordvisuallyexploringcoordination,
      title={AgentCoord: Visually Exploring Coordination Strategy for LLM-based Multi-Agent Collaboration}, 
      author={Bo Pan and others},
      year={2024},
      eprint={2404.11943},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
}

% Hierarchical
@misc{ishibashi2024selforganizedagentsllmmultiagent,
      title={Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra Large-Scale Code Generation and Optimization}, 
      author={Yoichi Ishibashi and Yoshimasa Nishimura},
      year={2024},
      eprint={2404.02183},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
}

% Decentralised
@inproceedings{10.1145/3674399.3674445,
author = {Chen, Bei and others},
title = {BlockAgents: Towards Byzantine-Robust LLM-Based Multi-Agent Coordination via Blockchain},
year = {2024},
isbn = {9798400710117},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
booktitle = {Proceedings of the ACM Turing Award Celebration Conference - China 2024},
pages = {187–192},
numpages = {6},
keywords = {Blockchain, Large Language Model (LLM), Multi-Agent System (MAS)},
location = {Changsha, China},
series = {ACM-TURC '24}
}

@inproceedings{wang-etal-2024-unleashing,
title = "Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration",
author = "Wang, Zhenhailong  and
others",
booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
month = "Jun.",
year = "2024"
}

@misc{zhang2024efficientllmgroundingembodied,
      title={Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration}, 
      author={Yang Zhang and others},
      year={2024},
      eprint={2405.14314},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}


@article{hagendorff2023human,
  title={Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT},
  author={Hagendorff, Thilo and Fabi, Sarah and Kosinski, Michal},
  journal={Nature Computational Science},
  volume={3},
  number={10},
  pages={833--838},
  year={2023},
  publisher={Nature Publishing Group US New York}
}

@article{huang2023survey,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author={Huang, Lei and others},
  journal={arXiv preprint arXiv:2311.05232},
  year={2023}
}


@inproceedings{10.5555/3600270.3602446,
author = {Hoffmann, Jordan and others},
title = {Training compute-optimal large language models},
year = {2024},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We investigate the optimal model size and number of tokens for training a Transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\texttimes{} more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {2176},
numpages = {15},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and others},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{sun2024llm,
  title={LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions},
  author={Sun, Chuanneng and Huang, Songjun and Pompili, Dario},
  journal={arXiv preprint arXiv:2405.11106},
  year={2024}
}

@article{frith2005theory,
  title={Theory of mind},
  author={Frith, Chris and Frith, Uta},
  journal={Current biology},
  volume={15},
  number={17},
  pages={R644--R645},
  year={2005},
  publisher={Elsevier}
}

@book{minsky1988society,
  title={Society of mind},
  author={Minsky, Marvin},
  year={1988},
  publisher={Simon and Schuster}
}

@inproceedings{hatalis2023memory,
  title={Memory Matters: The Need to Improve Long-Term Memory in LLM-Agents},
  author={Hatalis, Kostas and others},
  booktitle={Proceedings of the AAAI Symposium Series},
  volume={2},
  number={1},
  pages={277--280},
  year={2023}
}

@article{zhang2024survey,
  title={A survey on the memory mechanism of large language model based agents},
  author={Zhang, Zeyu and others},
  journal={arXiv preprint arXiv:2404.13501},
  year={2024}
}

@article{huang2024understanding,
  title={Understanding the planning of LLM agents: A survey},
  author={Huang, Xu and others},
  journal={arXiv preprint arXiv:2402.02716},
  year={2024}
}

@misc{du2023improvingfactualityreasoninglanguage,
      title={Improving Factuality and Reasoning in Language Models through Multiagent Debate}, 
      author={Yilun Du and others},
      year={2023},
      eprint={2305.14325},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@inproceedings{xiong-etal-2023-examining,
    title = "Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate",
    author = "Xiong, Kai  and
      others",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = "Dec.",
    year = "2023"
}

@inproceedings{jiang-etal-2023-llm,
    title = "{LLM}-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion",
    author = "Jiang, Dongfu  and
      Ren, Xiang  and
      Lin, Bill Yuchen",
    booktitle = "Proceedings of the Annual Meeting of the Association for Computational Linguistics",
    month = "Jul.",
    year = "2023"
}

@inproceedings{
    ning2024skeletonofthought,
    title={Skeleton-of-Thought: Prompting {LLM}s for Efficient Parallel Generation},
    author={Xuefei Ning and others},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
}

@misc{suzgun2024metapromptingenhancinglanguagemodels,
      title={Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding}, 
      author={Mirac Suzgun and Adam Tauman Kalai},
      year={2024},
      eprint={2401.12954},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@misc{qiao2024autoactautomaticagentlearning,
      title={AutoAct: Automatic Agent Learning from Scratch for QA via Self-Planning}, 
      author={Shuofei Qiao and others},
      year={2024},
      eprint={2401.05268},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@misc{zhang2024cumulativereasoninglargelanguage,
      title={Cumulative Reasoning with Large Language Models}, 
      author={Yifan Zhang and others},
      year={2024},
      eprint={2308.04371},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@inproceedings{yin-etal-2023-exchange,
    title = "Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication",
    author = "Yin, Zhangyue  and
      others",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    pages = "15135--15153"
}

@inproceedings{zhang-etal-2024-exploring,
    title = "Exploring Collaboration Mechanisms for {LLM} Agents: A Social Psychology View",
    author = "Zhang, Jintian  and
      others",
    booktitle = "Proceedings of the Annual Meeting of the Association for Computational Linguistics",
    month = "Aug.",
    year = "2024"
}


@InProceedings{pmlr-v235-zhuge24a,
  title = 	 {{GPTS}warm: Language Agents as Optimizable Graphs},
  author =       {Zhuge, Mingchen and others},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {62743--62767},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and others},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
}

@inproceedings{tang-etal-2024-medagents,
    title = "{M}ed{A}gents: Large Language Models as Collaborators for Zero-shot Medical Reasoning",
    author = "Tang, Xiangru  and
      others",
    booktitle = "Findings of the Association for Computational Linguistics",
    month = "Aug.",
    year = "2024"
}

@inproceedings{qian-etal-2024-chatdev,
    title = "{C}hat{D}ev: Communicative Agents for Software Development",
    author = "Qian, Chen  and
      others",
    booktitle = "Proceedings of the Annual Meeting of the Association for Computational Linguistics",
    month = "Aug.",
    year = "2024"
}

@misc{darcy2024margmultiagentreviewgeneration,
      title={MARG: Multi-Agent Review Generation for Scientific Papers}, 
      author={Mike D'Arcy and others},
      year={2024},
      eprint={2401.04259},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@inproceedings{ijcai2024p3,
  title     = {AutoAgents: A Framework for Automatic Agent Generation},
  author    = {Chen, Guangyao and others},
  booktitle = {Proceedings of the Thirty-Third International Joint Conference on
               Artificial Intelligence, {IJCAI-24}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Kate Larson},
  pages     = {22--30},
  year      = {2024},
  month     = {8},
  note      = {Main Track},
}

@misc{zheng2023agentsmeetokrobject,
      title={Agents meet OKR: An Object and Key Results Driven Agent System with Hierarchical Self-Collaboration and Self-Evaluation}, 
      author={Yi Zheng and others},
      year={2023},
      eprint={2311.16542},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
}

@inproceedings{10.1145/3589334.3645537,
author = {Zhang, Junjie and others},
title = {AgentCF: Collaborative Learning with Autonomous Language Agents for Recommender Systems},
year = {2024},
booktitle = {Proceedings of the ACM Web Conference}
}

@inproceedings{
zhang2024building,
title={Building Cooperative Embodied Agents Modularly with Large Language Models},
author={Hongxin Zhang and others},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}

@misc{zhuge2023mindstormsnaturallanguagebasedsocieties,
      title={Mindstorms in Natural Language-Based Societies of Mind}, 
      author={Mingchen Zhuge and others},
      year={2023},
      eprint={2305.17066},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@misc{xie2023openagentsopenplatformlanguage,
      title={OpenAgents: An Open Platform for Language Agents in the Wild}, 
      author={Tianbao Xie and others},
      year={2023},
      eprint={2310.10634},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@article{mu2023runtime,
  title={Runtime verification of self-adaptive multi-agent system using probabilistic timed automata},
  author={Mu, Yongan and others},
  journal={Journal of Intelligent \& Fuzzy Systems},
  volume={45},
  number={6},
  pages={10305--10322},
  year={2023},
  publisher={IOS Press}
}

@article{xu2023towards,
  title={Towards reasoning in large language models via multi-agent peer review collaboration},
  author={Xu, Zhenran and others},
  journal={arXiv preprint arXiv:2311.08152},
  year={2023}
}

@article{Zhang_Yang_Hu_Wang_Li_Sun_Zhang_Zhang_Liu_Zhu_Chang_Zhang_Yin_Liang_Yang_2024, 
title={ProAgent: Building Proactive Cooperative Agents with Large Language Models}, 
volume={38}, 
number={16}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Zhang, Ceyao and others}, year={2024}, 
month={Mar.}, 
pages={17591-17599} 
}

@inproceedings{mandi2024roco,
  title={Roco: Dialectic multi-robot collaboration with large language models},
  author={Mandi, Zhao and Jain, Shreeya and Song, Shuran},
  booktitle={2024 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={286--299},
  year={2024},
  organization={IEEE}
}

@inproceedings{
    anonymous2024federated,
    title={Federated Domain Generalization with Data-free On-server Gradient Matching},
    author={Anonymous},
    booktitle={Submitted to The Thirteenth International Conference on Learning Representations},
    year={2024},
    note={under review}
}

@inproceedings{
    anonymous2024domain,
    title={{DOMAIN} {GENERALIZATION} {VIA} {PARETO} {OPTIMAL} {GRADIENT} {MATCHING}},
    author={Anonymous},
    booktitle={Submitted to The Thirteenth International Conference on Learning Representations},
    year={2024},
    note={under review}
}

@misc{nguyen2024layerwisepersonalizedfederatedlearning,
    title={Towards Layer-Wise Personalized Federated Learning: Adaptive Layer Disentanglement via Conflicting Gradients}, 
    author={Minh Duong Nguyen and others},
    year={2024},
    eprint={2410.02845},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
}

@inproceedings{
jhunjhunwala2023fedexp,
title={FedExP: Speeding Up Federated Averaging via Extrapolation},
author={Divyansh Jhunjhunwala and Shiqiang Wang and Gauri Joshi},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
}

@article{ding2023dynamic,
  title={Dynamic event-triggered consensus for discrete-time hidden Markov multi-agent systems with partially unknown probabilities},
  author={Ding, Pengcheng and others},
  journal={Transactions of the Institute of Measurement and Control},
  pages={01423312231152655},
  year={2023},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{cheng2023preassigned,
  title={Preassigned-Time Bipartite Flocking Consensus Problem in Multi-Agent Systems},
  author={Cheng, Xiejun and others},
  journal={Symmetry},
  volume={15},
  number={5},
  pages={1105},
  year={2023},
  publisher={MDPI}
}

@article{Liu2023Scale-free,title={Scale-free Non-collaborative Linear Protocol Design for A Class of Homogeneous Multi-agent Systems},author={Zhenwei Liu and A. Saberi and A. Stoorvogel},journal={ArXiv},year={2023},volume={abs/2306.01582}}

@article{Zhuang2024PoSE,title={PoSE: Suppressing Perceptual Noise in Embodied Agents for Enhanced Semantic Navigation},author={Benhui Zhuang and Chunhong Zhang and Zheng Hu},journal={IEEE Robotics and Automation Letters},year={2024},volume={9},pages={963-970}}

@ARTICLE{10433480,
  author={Raiaan, Mohaimenul Azam Khan and others},
  journal={IEEE Access}, 
  title={A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges}, 
  year={2024},
  volume={12},
  number={},
  pages={26839-26874},
  keywords={Cognition;Artificial intelligence;Transformers;Training;Taxonomy;Task analysis;Surveys;Natural language processing;Question answering (information retrieval);Information analysis;Linguistics;Large language models (LLM);natural language processing (NLP);artificial intelligence;transformer;pre-trained models;taxonomy;application},
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{schaeffer2024emergent,
  title={Are emergent abilities of large language models a mirage?},
  author={Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{fan2024bibliometric,
  title={A bibliometric review of large language models research from 2017 to 2023},
  author={Fan, Lizhou and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={5},
  pages={1--25},
  year={2024},
  publisher={ACM New York, NY, USA}
}

@article{chang2024survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={3},
  pages={1--45},
  year={2024},
  publisher={ACM New York, NY}
}

@article{10.1145/3605943,
author = {Min, Bonan and others},
title = {Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {2},
issn = {0360-0300},
abstract = {Large, pre-trained language models (PLMs) such as BERT and GPT have drastically changed the Natural Language Processing (NLP) field. For numerous NLP tasks, approaches leveraging PLMs have achieved state-of-the-art performance. The key idea is to learn a generic, latent representation of language from a generic task once, then share it across disparate NLP tasks. Language modeling serves as the generic task, one with abundant self-supervised text available for extensive training. This article presents the key fundamental concepts of PLM architectures and a comprehensive view of the shift to PLM-driven NLP techniques. It surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches. In addition, it discusses PLM limitations and suggested directions for future research.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {30},
numpages = {40},
keywords = {neural networks, generative AI, foundational models, Large language models}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A and others},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@techreport{fourney2024magenticone,
author = {Fourney, Adam and others},
title = {Magentic-One: A Generalist Multi-Agent System for Solving Complex Tasks},
institution = {Microsoft},
year = {2024},
month = {November},
number = {MSR-TR-2024-47},
}

@misc{wang2024largelanguagemodelenabled,
    title={Large Language Model Enabled Semantic Communication Systems},
    author={Zhenyi Wang and others},
    year={2024},
    eprint={2407.14112},
    archivePrefix={arXiv},
    primaryClass={eess.SP},
}

@ARTICLE{10670195,
    author={Jiang, Feibo and others},
    journal={IEEE Communications Magazine}, 
    title={Large AI Model Empowered Multimodal Semantic Communications}, 
    year={2024},
    volume={},
    number={},
    pages={1-7},
    keywords={Semantics;Data models;Channel estimation;Artificial intelligence;Receivers;Adaptation models;Accuracy},
}

@misc{yang2024rethinkinggenerativesemanticcommunication,
    title={Rethinking Generative Semantic Communication for Multi-User Systems with Multi-Modal LLM},
    author={Wanting Yang and others},
    year={2024},
    eprint={2408.08765},
    archivePrefix={arXiv},
    primaryClass={cs.NI},
}

@ARTICLE{10720863,
    author={Zhou, Li and others},
    journal={IEEE Transactions on Cognitive Communications and Networking}, 
    title={Semantic Information Extraction and Multi-Agent Communication Optimization Based on Generative Pre-Trained Transformer}, 
    year={2024},
    volume={},
    number={},
    pages={1-1},
}

@ARTICLE{10638533,
    author={Jiang, Feibo and others},
    journal={IEEE Wireless Communications}, 
    title={Large Language Model Enhanced Multi-Agent Systems for 6G Communications}, 
    year={2024},
    volume={},
    number={},
    pages={1-8},
    keywords={6G mobile communication;Task analysis;Knowledge engineering;Artificial intelligence;Multi-agent systems;Communication systems;Cognition},
}

@ARTICLE{10639525,
    author={Qin, Zhijin and others},
    journal={Proceedings of the IEEE}, 
    title={AI Empowered Wireless Communications: From Bits to Semantics}, 
    year={2024},
    volume={112},
    number={7},
    pages={621-652},
}

@misc{tang2024largelanguagemodelllmassisted,
    title={Large Language Model(LLM) assisted End-to-End Network Health Management based on Multi-Scale Semanticization},
    author={Fengxiao Tang and others},
    year={2024},
    eprint={2406.08305},
    archivePrefix={arXiv},
    primaryClass={cs.NI},
}

@ARTICLE{10731639,
    author={Sehad, Nassim and others},
    journal={IEEE Communications Magazine}, 
    title={Generative AI for Immersive Communication: The Next Frontier in Internet-of-Senses Through 6G}, 
    year={2024},
    volume={},
    number={},
    pages={1-13},
    keywords={Streaming media;Haptic interfaces;Media;Synchronization;Visualization;Internet;Three-dimensional displays;Servers;Resists;Generative AI},
}

@ARTICLE{10531769,
    author={Zhao, Yaru and others},
    journal={IEEE Transactions on Cognitive Communications and Networking}, 
    title={LaMoSC: Large Language Model-Driven Semantic Communication System for Visual Transmission}, 
    year={2024},
    volume={10},
    number={6},
    pages={2005-2018},
    keywords={Semantics;Feature extraction;Image reconstruction;Communication systems;Visualization;Signal to noise ratio;Decoding;Semantic communication;large language models;multimodal fusion;robust reconstruction},
}

@ARTICLE{10439991,
  author={Zhong, Ningze and others},
  journal={IEEE Internet of Things Journal}, 
  title={CASIT: Collective Intelligent Agent System for Internet of Things}, 
  year={2024},
  volume={11},
  number={11},
  pages={19646-19656},
  keywords={Internet of Things;Edge computing;Task analysis;Intelligent agents;Bandwidth;Natural languages;Multi-agent systems;Collective intelligent agent system;collective wisdom;Internet of Things (IoT);large language model (LLM)},
}

@article{Xiao_2024,
title={Efficient Prompting for LLM-Based Generative Internet of Things},
ISSN={2372-2541},
journal={IEEE Internet of Things Journal},
publisher={Institute of Electrical and Electronics Engineers (IEEE)},
author={Xiao, Bin and others},
year={2024},
pages={1–1} }

@ARTICLE{10729865,
  author={Rivkin, Dmitriy and others},
  journal={IEEE Internet of Things Journal}, 
  title={AIoT Smart Home via Autonomous LLM Agents}, 
  year={2024},
  volume={},
  number={},
  pages={1-1},
  keywords={Smart homes;Codes;Performance evaluation;Internet of Things;Benchmark testing;Smart devices;Logic;Process control;Monitoring;Manuals;Autonomous LLM Agents;Smart Home;IoT;Generative AI;Embodied AI;Personalized AI;AI Assistant},
}

@ARTICLE{10520918,
  author={Liu, Qiang and others},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={LLM Enhanced Reconfigurable Intelligent Surface for Energy-Efficient and Reliable 6G IoV}, 
  year={2024},
  volume={},
  number={},
  pages={1-9},
  keywords={Reconfigurable intelligent surfaces;Wireless communication;Wireless sensor networks;Quality of service;Reliability;6G mobile communication;Optimization;reconfigurable intelligent surface;large language model;6G;Internet of Vehicles},
}

@ARTICLE{10742575,
  author={Rong, Yi and others},
  journal={IEEE Internet of Things Magazine}, 
  title={Large-Scale Traffic Flow Forecast with Lightweight LLM in Edge Intelligence}, 
  year={2024},
  volume={},
  number={},
  pages={1-7},
  keywords={Roads;Cloud computing;Correlation;Training;Predictive models;Time series analysis;Forecasting;Edge computing;Large language models;Feature extraction},
}


@misc{leng2024llmagentsexhibitsocial,
title={Do LLM Agents Exhibit Social Behavior?},
author={Yan Leng and Yuan Yuan},
year={2024},
eprint={2312.15198},
archivePrefix={arXiv},
primaryClass={cs.AI},
}
@article{doi:10.1073/pnas.2314021121,author = {Christopher A. Bail },title = {Can Generative AI improve social science?},journal = {Proceedings of the National Academy of Sciences},volume = {121},number = {21},pages = {e2314021121},year = {2024},abstract = {Generative AI that can produce realistic text, images, and other human-like outputs is currently transforming many different industries. Yet it is not yet known how such tools might influence social science research. I argue Generative AI has the potential to improve survey research, online experiments, automated content analyses, agent-based models, and other techniques commonly used to study human behavior. In the second section of this article, I discuss the many limitations of Generative AI. I examine how bias in the data used to train these tools can negatively impact social science research—as well as a range of other challenges related to ethics, replication, environmental impact, and the proliferation of low-quality research. I conclude by arguing that social scientists can address many of these limitations by creating open-source infrastructure for research on human behavior. Such infrastructure is not only necessary to ensure broad access to high-quality research tools, I argue, but also because the progress of AI will require deeper understanding of the social forces that guide human behavior.}}

@article{10.1093/pnasnexus/pgae245,
author = {Abdurahman, Suhaib and others},
title = {Perils and opportunities in using large language models in psychological research},
journal = {PNAS Nexus},
year = {2024},
month = {Jul.}
}

@misc{li2023metaagentssimulatinginteractionshuman,
title={MetaAgents: Simulating Interactions of Human Behaviors for LLM-based Task-oriented Coordination via Collaborative Generative Agents},
author={Yuan Li and Yixuan Zhang and Lichao Sun},
year={2023},
eprint={2310.06500},
archivePrefix={arXiv},
primaryClass={cs.AI},
}
@misc{dai2024artificialleviathanexploringsocial,
title={Artificial Leviathan: Exploring Social Evolution of LLM Agents Through the Lens of Hobbesian Social Contract Theory},
author={Gordon Dai and others},
year={2024},
eprint={2406.14373},
archivePrefix={arXiv},
primaryClass={cs.AI},
}
@article{DILLION2023597,
title = {Can AI language models replace human participants?},
journal = {Trends in Cognitive Sciences},
volume = {27},
number = {7},
pages = {597-600},
year = {2023},
issn = {1364-6613},
author = {Danica Dillion and others},
keywords = {language models, artificial intelligence, morality, judgments, participants, research methods},
abstract = {Recent work suggests that language models such as GPT can make human-like judgments across a number of domains. We explore whether and when language models might replace human participants in psychological science. We review nascent research, provide a theoretical model, and outline caveats of using AI as a participant.}
}
@inproceedings{10.5555/3618408.3618425,
author = {Aher, Gati and others},
title = {Using large language models to simulate multiple humans and replicate human subject studies},
year = {2023},
booktitle = {Proceedings of the International Conference on Machine Learning}
}
@article{Ying2024,
title = {Inferring the Goals of Communicating Agents from Actions and Instructions},
volume = {2},
ISSN = {2994-4317},
number = {1},
journal = {Proceedings of the AAAI Symposium Series},
publisher = {Association for the Advancement of Artificial Intelligence (AAAI)},
author = {Ying,  Lance and others},
year = {2024},
month = jan,
pages = {26–33}
}
@misc{liu2024largelanguagemodelsassume,
title={Large Language Models Assume People are More Rational than We Really are},
author={Ryan Liu and others},
year={2024},
eprint={2406.17055},
archivePrefix={arXiv},
primaryClass={cs.CL},
}
@inproceedings{zhou-etal-2024-real,
title = "Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With {LLM}s",
author = "Zhou, Xuhui  and
others",
editor = "Al-Onaizan, Yaser  and
Bansal, Mohit  and
Chen, Yun-Nung",
booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
month = nov,
year = "2024",
address = "Miami, Florida, USA",
publisher = "ACL",
pages = "21692--21714",
abstract = "Recent advances in large language models (LLM) have enabled richer social simulations, allowing for the study of various social phenomena. However, most recent work has used a more omniscient perspective on these simulations (e.g., single LLM to generate all interlocutors), which is fundamentally at odds with the non-omniscient, information asymmetric interactions that involve humans and AI agents in the real world. To examine these differences, we develop an evaluation framework to simulate social interactions with LLMs in various settings (omniscient, non-omniscient). Our experiments show that LLMs perform better in unrealistic, omniscient simulation settings but struggle in ones that more accurately reflect real-world conditions with information asymmetry. Moreover, we illustrate the limitations inherent in learning from omniscient simulations. Our findings indicate that addressing information asymmetry remains a fundamental challenge for LLM-based agents.",
}
@misc{mou2024agentsensebenchmarkingsocialintelligence,
title={AgentSense: Benchmarking Social Intelligence of Language Agents through Interactive Scenarios},
author={Xinyi Mou and Jingcong Liang and Jiayu Lin and Xinnong Zhang and Xiawei Liu and Shiyue Yang and Rong Ye and Lei Chen and Haoyu Kuang and Xuanjing Huang and Zhongyu Wei},
year={2024},
eprint={2410.19346},
archivePrefix={arXiv},
primaryClass={cs.CL},
}
@inproceedings{chen-etal-2024-socialbench,
title = "{S}ocial{B}ench: Sociality Evaluation of Role-Playing Conversational Agents",
author = "Chen, Hongzhan  and
others",
editor = "Ku, Lun-Wei  and
Martins, Andre  and
Srikumar, Vivek",
booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
month = aug,
year = "2024",
address = "Bangkok, Thailand",
publisher = "Association for Computational Linguistics",
pages = "2108--2126",
abstract = "Large language models (LLMs) have advanced the development of various AI conversational agents, including role-playing agents that mimic diverse characters and human behaviors. While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge and style of these agents, there has been a noticeable gap in assessing their social intelligence. In this paper, we introduce SocialBench, the first benchmark designed to systematically evaluate the sociality of role-playing agents at both individual and group levels of social interactions. SocialBench is constructed from various sources and covers a wide range of 500 characters and over 6,000 question prompts and 30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations on this benchmark using mainstream LLMs. We find that agents excelling in individual level does not imply their proficiency in group level. Experimental results on SocialBench confirm its significance as a testbed for assessing the social interaction of role-playing agents. The benchmark is publicly accessible at https://github.com/X-PLUG/RoleInteract.",
}
@article{mitra2024agentinstruct,
title={Agentinstruct: Toward generative teaching with agentic flows},
author={Mitra, Arindam and others},
journal={arXiv preprint arXiv:2407.03502},
year={2024}
}
@misc{yang2024socialmindllmbasedproactivear,
title={SocialMind: LLM-based Proactive AR Social Assistive System with Human-like Perception for In-situ Live Interactions},
author={Bufang Yang and others},
year={2024},
eprint={2412.04036},
archivePrefix={arXiv},
primaryClass={cs.AI},
}

@mastersthesis{ansaldo2023agentspeak,
  title={AgentSpeak: A Framework for Agent-Based Modeling with Integrated Large Language Models; Case Study: Analyzing Policy Interventions in Electric Vehicle Adoption},
  author={Ansaldo, Gabriele},
  year={2023},
  school={Northeastern University}
}

@article{zeng2024exploring,
  title={Exploring the opportunities and challenges of using large language models to represent institutional agency in land system modelling},
  author={Zeng, Yongchao and others},
  journal={EGUsphere},
  volume={2024},
  pages={1--35},
  year={2024},
  publisher={Copernicus Publications G{\"o}ttingen, Germany}
}

@misc{gurcan2024llmaugmentedagentbasedmodellingsocial,
      title={LLM-Augmented Agent-Based Modelling for Social Simulations: Challenges and Opportunities}, 
      author={Onder Gurcan},
      year={2024},
      eprint={2405.06700},
      archivePrefix={arXiv},
      primaryClass={physics.soc-ph},
}

@misc{he2024normviolationdetectionmultiagent,
      title={Norm Violation Detection in Multi-Agent Systems using Large Language Models: A Pilot Study}, 
      author={Shawn He and others},
      year={2024},
      eprint={2403.16517},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
}

@misc{li2024cultureparkboostingcrossculturalunderstanding,
      title={CulturePark: Boosting Cross-cultural Understanding in Large Language Models}, 
      author={Cheng Li and others},
      year={2024},
      eprint={2405.15145},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@misc{perez2024culturalevolutionpopulationslarge,
      title={Cultural evolution in populations of Large Language Models}, 
      author={Jérémy Perez and others},
      year={2024},
      eprint={2403.08882},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
}

@inproceedings{10.1145/3627673.3679768,
author = {Nguyen, Tuan-Phong and Razniewski, Simon and Weikum, Gerhard},
title = {Cultural Commonsense Knowledge for Intercultural Dialogues},
year = {2024},
isbn = {9798400704369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Despite recent progress, large language models (LLMs) still face the challenge of appropriately reacting to the intricacies of social and cultural conventions. This paper presents Mango, a methodology for distilling high-accuracy, high-recall assertions of cultural knowledge. We judiciously and iteratively prompt LLMs for this purpose from two entry points, concepts and cultures. Outputs are consolidated via clustering and generative summarization. Running the Mango method with GPT-3.5 as underlying LLM yields 167K high-accuracy assertions for 30K concepts and 11K cultures, surpassing prior resources by a large margin in quality and size. In an extrinsic evaluation for intercultural dialogues, we explore augmenting dialogue systems with cultural knowledge assertions. Notably, despite LLMs inherently possessing cultural knowledge, we find that adding knowledge from Mango improves the overall quality, specificity, and cultural sensitivity of dialogue responses, as judged by human annotators. Data and code are available for download.},
booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
pages = {1774–1784},
numpages = {11},
keywords = {cultural commonsense knowledge, intercultural dialogues, knowledge distillation},
location = {Boise, ID, USA},
series = {CIKM '24}
}

@misc{qiu2024evaluatingculturalsocialawareness,
      title={Evaluating Cultural and Social Awareness of LLM Web Agents}, 
      author={Haoyi Qiu and others},
      year={2024},
      eprint={2410.23252},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@article{Leimeister2010,
  title = {Collective Intelligence},
  volume = {2},
  ISSN = {1867-0202},
  number = {4},
  journal = {Business and Information Systems Engineering},
  publisher = {Springer Science and Business Media LLC},
  author = {Leimeister,  Jan Marco},
  year = {2010},
  month = jun,
  pages = {245–248}
}

@article{zhuge2024agent,
  title={Agent-as-a-Judge: Evaluate Agents with Agents},
  author={Zhuge, Mingchen and others},
  journal={arXiv preprint arXiv:2410.10934},
  year={2024}
}


@article{chang2024survey,
  title={A survey on evaluation of large language models},
  author={Chang, Yupeng and others},
  journal={ACM Transactions on Intelligent Systems and Technology},
  volume={15},
  number={3},
  pages={1--45},
  year={2024},
  publisher={ACM New York, NY}
}

@article{peng2024survey,
  title={A Survey of Useful LLM Evaluation},
  author={Peng, Ji-Lun and others},
  journal={arXiv preprint arXiv:2406.00936},
  year={2024}
}

@inproceedings{
patil2024gorilla,
title={Gorilla: Large Language Model Connected with Massive {API}s},
author={Shishir G Patil and others},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
}

@misc{ji2024testingunderstandingerroneousplanning,
      title={Testing and Understanding Erroneous Planning in LLM Agents through Synthesized User Inputs}, 
      author={Zhenlan Ji and others},
      year={2024},
      eprint={2404.17833},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@misc{deshpande2023anthropomorphizationaiopportunitiesrisks,
      title={Anthropomorphization of AI: Opportunities and Risks}, 
      author={Ameet Deshpande and others},
      year={2023},
      eprint={2305.14784},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@article{Akbulut2024,
  title = {All Too Human? Mapping and Mitigating the Risk from Anthropomorphic AI},
  volume = {7},
  ISSN = {3065-8365},
  journal = {Proceedings of the AAAI/ACM Conference on AI,  Ethics,  and Society},
  publisher = {Association for the Advancement of Artificial Intelligence (AAAI)},
  author = {Akbulut,  Canfer and others},
  year = {2024},
  month = oct,
  pages = {13–26}
}

@misc{meinke2024frontiermodelscapableincontext,
      title={Frontier Models are Capable of In-context Scheming}, 
      author={Alexander Meinke and others},
      year={2024},
      eprint={2412.04984},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
}

@article{shayegani2023survey,
  title={Survey of vulnerabilities in large language models revealed by adversarial attacks},
  author={Shayegani, Erfan and others},
  journal={arXiv preprint arXiv:2310.10844},
  year={2023}
}

@article{wang2024benchmark,
  title={Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation},
  author={Wang, Siyuan and others},
  journal={arXiv preprint arXiv:2402.11443},
  year={2024}
}

@article{chiang2024chatbot,
  title={Chatbot arena: An open platform for evaluating llms by human preference},
  author={Chiang, Wei-Lin and others},
  journal={arXiv preprint arXiv:2403.04132},
  year={2024}
}

@article{mitra2024agentinstruct,
  title={Agentinstruct: Toward generative teaching with agentic flows},
  author={Mitra, Arindam and others},
  journal={arXiv preprint arXiv:2407.03502},
  year={2024}
}

@article{tran2024irish,
  title={Irish-based Large Language Model with Extreme Low-Resource Settings in Machine Translation},
  author={Tran, Khanh-Tung and O’Sullivan, Barry and Nguyen, Hoang D},
  journal={LoResMT 2024},
  pages={193},
  year={2024}
}

@inproceedings{nguyen2023vigptqa,
  title={ViGPTQA-state-of-the-art LLMs for vietnamese question answering: system overview, core models training, and evaluations},
  author={Nguyen, Minh Thuan and others},
  booktitle={Proceedings of the 2023 conference on empirical methods in natural language processing: industry track},
  pages={754--764},
  year={2023}
}

@article{tran2024uccix,
  title={UCCIX: Irish-eXcellence Large Language Model},
  author={Tran, Khanh-Tung and O'Sullivan, Barry and Nguyen, Hoang D},
  journal={ECAI 2024},
  year={2024}
}

@article{10.1145/3697350,
author = {Kalyani, Yogeswaranathan and Collier, Rem},
title = {The Role of Multi-Agents in Digital Twin Implementation: Short Survey},
year = {2024},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {3},
issn = {0360-0300},
abstract = {In recent years, Digital Twin (DT) technology has emerged as a significant technological advancement. A digital twin is a digital representation of a physical asset that mirrors its data model, behaviour, and interactions with other physical assets. Digital Twin aims at achieving adaptability, seamless data integration, modelling, simulation, automation, and real-time data management. The primary goal of this article is to explore the role of agents in DT implementations, seeking to understand their predominant usage scenarios and purposes. From our perspective, agents serving as intelligent entities play a role in realising the features of DTs. This article also discusses the gaps in DT, highlights future directions, and analyses various technologies integrated with multi-agent systems technologies in DT implementations. Finally, the article briefly discusses an overview of an architecture to implement a DT for smart agriculture with multi-agents.},
journal = {ACM Comput. Surv.},
month = nov,
articleno = {72},
numpages = {15},
keywords = {Agents, digital twin, multi-agent systems}
}

@article{10.1145/3704435,
author = {Qin, Yujia and others},
title = {Tool Learning with Foundation Models},
year = {2024},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {4},
issn = {0360-0300},
abstract = {Humans possess an extraordinary ability to create and utilize tools. With the advent of foundation models, artificial intelligence systems have the potential to be equally adept in tool use as humans. This paradigm, which is dubbed as tool learning with foundation models, combines the strengths of tools and foundation models to achieve enhanced accuracy, efficiency, and automation in problem-solving. This article presents a systematic investigation and comprehensive review of tool learning. We first introduce the background of tool learning, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models. Then we recapitulate existing tool learning research and formulate a general framework: starting from understanding the user instruction, models should learn to decompose a complex task into several subtasks, dynamically adjust their plan through reasoning, and effectively conquer each sub-task by selecting appropriate tools. We also discuss how to train models for improved tool-use capabilities and facilitate generalization in tool learning. Finally, we discuss several open problems that require further investigation, such as ensuring trustworthy tool use, enabling tool creation with foundation models, and addressing personalization challenges. Overall, we hope this article could inspire future research in integrating tools with foundation models.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {101},
keywords = {Tool use, foundation models, literature survey}
}

@article{10.1145/3649449,
author = {Li, Junyi and others},
title = {Pre-Trained Language Models for Text Generation: A Survey},
year = {2024},
issue_date = {September 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {9},
issn = {0360-0300},
abstract = {Text Generation aims to produce plausible and readable text in human language from input data. The resurgence of deep learning has greatly advanced this field, in particular, with the help of neural generation models based on pre-trained language models (PLMs). Text generation based on PLMs is viewed as a promising approach in both academia and industry. In this article, we provide a survey on the utilization of PLMs in text generation. We begin with introducing two key aspects of applying PLMs to text generation: (1) how to design an effective PLM to serve as the generation model; and (2) how to effectively optimize PLMs given the reference text and to ensure that the generated texts satisfy special text properties. Then, we show the major challenges that have arisen in these aspects, as well as possible solutions for them. We also include a summary of various useful resources and typical text generation applications based on PLMs. Finally, we highlight the future research directions which will further improve these PLMs for text generation. This comprehensive survey is intended to help researchers interested in text generation problems to learn the core concepts, the main techniques and the latest developments in this area based on PLMs.},
journal = {ACM Comput. Surv.},
month = apr,
articleno = {230},
numpages = {39},
keywords = {Pre-trained language models, natural language processing}
}

@article{10.1145/3704806,
author = {Lambiase, Stefano and others},
title = {Motivations, Challenges, Best Practices, and Benefits for Bots and Conversational Agents in Software Engineering: A Multivocal Literature Review},
year = {2024},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {4},
issn = {0360-0300},
abstract = {Bots are software systems designed to support users by automating specific processes, tasks, or activities. When these systems implement a conversational component to interact with users, they are also known as conversational agents or chatbots. Bots—particularly in their conversation-oriented version and AI-powered—have seen increased adoption over time for software development and engineering purposes. Despite their exciting potential, which has been further enhanced by the advent of Generative AI and Large Language Models, bots still face challenges in terms of development and integration into the development cycle, as practitioners report that bots can add difficulties rather than provide improvements. In this work, we aim to provide a taxonomy for characterizing bots, as well as a series of challenges for their adoption in software engineering, accompanied by potential mitigation strategies. To achieve our objectives, we conducted a multivocal literature review, examining both research and practitioner literature. Through such an approach, we hope to contribute to both researchers and practitioners by providing (i) a series of future research directions to pursue, (ii) a list of strategies to adopt for improving the use of bots for software engineering purposes, and (iii) fostering technology and knowledge transfer from the research field to practice—one of the primary goals of multivocal literature reviews.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {93},
numpages = {37},
keywords = {Bot, chatbot, software engineering, literature review}
}

@inproceedings{braccini2024swarm,
  title={Swarm Intelligence: A Novel and Unconventional Approach to Dance Choreography Creation},
  author={Braccini, Michele and others},
  year={2024}
}

@article{divband2022intelligent,
  title={From intelligent agents to trustworthy human-centred multiagent systems},
  author={Divband Soorati, Mohammad and others},
  journal={AI Communications},
  volume={35},
  number={4},
  pages={443--457},
  year={2022},
  publisher={IOS Press}
}

@article{fischer2021loop,
  title={In-the-loop or on-the-loop? Interactional arrangements to support team coordination with a planning agent},
  author={Fischer, Joel E and others},
  journal={Concurrency and Computation: Practice and Experience},
  volume={33},
  number={8},
  pages={e4082},
  year={2021},
  publisher={Wiley Online Library}
}

@article{peikos2024leveraging,
  title={Leveraging Large Language Models for Medical Information Extraction and Query Generation},
  author={Peikos, Georgios and Kasela, Pranav and Pasi, Gabriella},
  journal={arXiv preprint arXiv:2410.23851},
  year={2024}
}

@article{he2024llm,
  title={LLM-Based Multi-Agent Systems for Software Engineering: Vision and the Road Ahead},
  author={He, Junda and Treude, Christoph and Lo, David},
  journal={arXiv preprint arXiv:2404.04834},
  year={2024}
}

@inproceedings{cuconasu2024power,
  title={The power of noise: Redefining retrieval for rag systems},
  author={Cuconasu, Florin and others},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={719--729},
  year={2024}
}

@article{dong2024can,
  title={Can LLMs Serve As Time Series Anomaly Detectors?},
  author={Dong, Manqing and Huang, Hao and Cao, Longbing},
  journal={arXiv preprint arXiv:2408.03475},
  year={2024}
}

@inproceedings{abad2017autonomous,
  title={Autonomous crowdsourcing through human-machine collaborative learning},
  author={Abad, Azad and Nabi, Moin and Moschitti, Alessandro},
  booktitle={Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={873--876},
  year={2017}
}