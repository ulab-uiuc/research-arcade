@article{brown2020language,
	title={Language models are few-shot learners},
	author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	journal={Advances in neural information processing systems},
	volume={33},
	pages={1877--1901},
	year={2020}
}

@article{GPT4,
	author       = {OpenAI},
	title        = {{GPT-4} Technical Report},
	journal      = {CoRR},
	volume       = {abs/2303.08774},
	year         = {2023}
}

@article{llama,
	author       = {Hugo Touvron and
	Thibaut Lavril and
	Gautier Izacard and
	Xavier Martinet and
	Marie{-}Anne Lachaux and
	Timoth{\'{e}}e Lacroix and
	Baptiste Rozi{\`{e}}re and
	Naman Goyal and
	Eric Hambro and
	Faisal Azhar and
	Aur{\'{e}}lien Rodriguez and
	Armand Joulin and
	Edouard Grave and
	Guillaume Lample},
	title        = {LLaMA: Open and Efficient Foundation Language Models},
	journal      = {CoRR},
	volume       = {abs/2302.13971},
	year         = {2023}
}

@article{zhang2022opt,
	title={Opt: Open pre-trained transformer language models},
	author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
	journal={arXiv preprint arXiv:2205.01068},
	year={2022}
}

@article{touvron2023llama,
	title={Llama 2: Open foundation and fine-tuned chat models},
	author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
	journal={arXiv preprint arXiv:2307.09288},
	year={2023}
}

@misc{nakano2022webgpt,
	title={WebGPT: Browser-assisted question-answering with human feedback}, 
	author={Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman},
	year={2022},
	eprint={2112.09332},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{chowdhery2022palm,
	title={Palm: Scaling language modeling with pathways},
	author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
	journal={arXiv preprint arXiv:2204.02311},
	year={2022}
}

@inproceedings{kour2014real,
  title={Real-time segmentation of on-line handwritten arabic script},
  author={Kour, George and Saabne, Raid},
  booktitle={Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on},
  pages={417--422},
  year={2014},
  organization={IEEE}
}

@inproceedings{kour2014fast,
  title={Fast classification of handwritten on-line Arabic characters},
  author={Kour, George and Saabne, Raid},
  booktitle={Soft Computing and Pattern Recognition (SoCPaR), 2014 6th International Conference of},
  pages={312--318},
  year={2014},
  organization={IEEE},
  doi={10.1109/SOCPAR.2014.7008025}
}

@article{hadash2018estimate,
  title={Estimate and Replace: A Novel Approach to Integrating Deep Neural Networks with Existing Applications},
  author={Hadash, Guy and Kermany, Einat and Carmeli, Boaz and Lavi, Ofer and Kour, George and Jacovi, Alon},
  journal={arXiv preprint arXiv:1804.09028},
  year={2018}
}

@inproceedings{wang_use_2022,
    title = "On the Use of Bert for Automated Essay Scoring: Joint Learning of Multi-Scale Essay Representation",
    author = "Wang, Yongjie  and
      Wang, Chuang  and
      Li, Ruobing  and
      Lin, Hui",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.249",
    doi = "10.18653/v1/2022.naacl-main.249",
    pages = "3416--3425",
    abstract = "In recent years, pre-trained models have become dominant in most natural language processing (NLP) tasks. However, in the area of Automated Essay Scoring (AES), pre-trained models such as BERT have not been properly used to outperform other deep learning models such as LSTM. In this paper, we introduce a novel multi-scale essay representation for BERT that can be jointly learned. We also employ multiple losses and transfer learning from out-of-domain essays to further improve the performance. Experiment results show that our approach derives much benefit from joint learning of multi-scale essay representation and obtains almost the state-of-the-art result among all deep learning models in the ASAP task. Our multi-scale essay representation also generalizes well to CommonLit Readability Prize data set, which suggests that the novel text representation proposed in this paper may be a new and effective choice for long-text tasks.",
}


@inproceedings{wenzek_ccnet_2020,
	title = {{CCNet}: {Extracting} {High} {Quality} {Monolingual} {Datasets} from {Web} {Crawl} {Data}},
	url = {https://aclanthology.org/2020.lrec-1.494},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	author = {Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzmán, Francisco and Joulin, Armand and Grave, Édouard},
	year = {2020},
	keywords = {⛔ No DOI found, ��, ⭐},
	pages = {4003--4012},
	file = {全文:E\:\\Program Data\\Zotero\\storage\\YPVJU4KP\\Wenzek 等 - 2020 - CCNet Extracting High Quality Monolingual Dataset.pdf:application/pdf},
}

@inproceedings{zhao_memory-augmented_2017,
	title = {A memory-augmented neural model for automated grading},
	doi = {10.1145/3051457.3053982},
	booktitle = {Proceedings of the fourth (2017) {ACM} conference on learning@ scale},
	author = {Zhao, Siyuan and Zhang, Yaqiong and Xiong, Xiaolu and Botelho, Anthony and Heffernan, Neil},
	year = {2017},
	keywords = {��, ��, ⭐},
	pages = {189--192},
	file = {全文:E\:\\Program Data\\Zotero\\storage\\SHZ32VSS\\Zhao 等 - 2017 - A memory-augmented neural model for automated grad.pdf:application/pdf},
}

@inproceedings{alikaniotis_automatic_2016,
	title = {Automatic {Text} {Scoring} {Using} {Neural} {Networks}},
	doi = {10.18653/v1/P16-1068},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	author = {Alikaniotis, Dimitrios and Yannakoudakis, Helen and Rei, Marek},
	year = {2016},
	pages = {715--725},
	file = {全文:E\:\\Program Data\\Zotero\\storage\\IFM9MLWW\\Alikaniotis 等 - 2016 - Automatic Text Scoring Using Neural Networks.pdf:application/pdf},
}

@inproceedings{yuan_bartscore_2021,
	title = {{BARTScore}: {Evaluating} {Generated} {Text} as {Text} {Generation}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yuan, Weizhe and Neubig, Graham and Liu, Pengfei},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	keywords = {⛔ No DOI found, ✅, ⭐},
	pages = {27263--27277},
	file = {全文:E\:\\Program Data\\Zotero\\storage\\NZR44CFQ\\Yuan 等 - 2021 - BARTScore Evaluating Generated Text as Text Gener.pdf:application/pdf},
}

@inproceedings{sellam_bleurt_2020,
	address = {Online},
	title = {{BLEURT}: {Learning} {Robust} {Metrics} for {Text} {Generation}},
	url = {https://aclanthology.org/2020.acl-main.704},
	doi = {10.18653/v1/2020.acl-main.704},
	abstract = {Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgment. We propose BLEURT, a learned evaluation metric for English based on BERT. BLEURT can model human judgment with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG data set. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution.},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Sellam, Thibault and Das, Dipanjan and Parikh, Ankur},
	month = jul,
	year = {2020},
	keywords = {��, /unread},
	pages = {7881--7892},
}

@inproceedings{dodge_documenting_2021,
	title = {Documenting {Large} {Webtext} {Corpora}: {A} {Case} {Study} on the {Colossal} {Clean} {Crawled} {Corpus}},
	doi = {10.18653/v1/2021.emnlp-main.98},
	abstract = {大型语言模型在许多 NLP 任务上取得了显着进展，研究人员正在转向更大的文本语料库来训练它们。一些最大的可用语料库是通过抓取互联网的重要部分而创建的，并且通常只用最少的文档进行介绍。在这项工作中，我们提供了 Colossal Clean Crawled Corpus（C4；Raffel 等人，2020）的一些第一个文档，该数据集是通过将一组过滤器应用于 Common Crawl 的单个快照而创建的。我们首先调查数据的来源，并从专利和美国军事网站等意想不到的来源找到大量文本。然后，我们探索文本本身的内容，并找到机器生成的文本（例如，来自机器翻译系统）和来自其他基准 NLP 数据集的评估示例。为了了解创建此数据集所应用的过滤器的影响，我们评估了被删除的文本，并表明黑名单过滤不成比例地删除了来自少数群体的文本以及有关少数群体的文本。最后，我们对如何从互联网上创建和记录网络规模的数据集提出了一些建议。},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Dodge, Jesse and Sap, Maarten and Marasović, Ana and Agnew, William and Ilharco, Gabriel and Groeneveld, Dirk and Mitchell, Margaret and Gardner, Matt},
	year = {2021},
	keywords = {✅},
	pages = {1286--1305},
	file = {全文:E\:\\Program Data\\Zotero\\storage\\NJATSTH5\\Dodge 等 - 2021 - Documenting Large Webtext Corpora A Case Study on.pdf:application/pdf;Documenting Large Webtext Corpora.md:D\:\\学习\\CIP\\NLP数据预处理\\文献阅读\\Documenting Large Webtext Corpora.md:text/plain},
}

@article{kreutzer_quality_2022,
	title = {Quality at a glance: {An} audit of web-crawled multilingual datasets},
	volume = {10},
	doi = {10.1162/tacl_a_00447},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Kreutzer, Julia and Caswell, Isaac and Wang, Lisa and Wahab, Ahsan and van Esch, Daan and Ulzii-Orshikh, Nasanbayar and Tapo, Allahsera and Subramani, Nishant and Sokolov, Artem and Sikasote, Claytone},
	year = {2022},
	note = {ISBN: 2307-387X
Publisher: MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info …},
	pages = {50--72},
	file = {全文:E\:\\Program Data\\Zotero\\storage\\JTCVM455\\Kreutzer 等 - 2022 - Quality at a glance An audit of web-crawled multi.pdf:application/pdf},
}

@inproceedings{luccioni_whats_2021,
	address = {Online},
	title = {What's in the {Box}? {An} {Analysis} of {Undesirable} {Content} in the {Common} {Crawl} {Corpus}},
	url = {https://aclanthology.org/2021.acl-short.24},
	doi = {10.18653/v1/2021.acl-short.24},
	abstract = {数据集中不良内容的检测},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Luccioni, Alexandra and Viviano, Joseph},
	month = aug,
	year = {2021},
	keywords = {✅},
	pages = {182--189},
	file = {全文:E\:\\Program Data\\Zotero\\storage\\GFMQU2JB\\Luccioni 和 Viviano - 2021 - What's in the Box An Analysis of Undesirable Cont.pdf:application/pdf},
}

@article{petukhova_textcl_2022,
	title = {{TextCL}: {A} {Python} package for {NLP} preprocessing tasks},
	volume = {19},
	doi = {10.1016/j.softx.2022.101122},
	journal = {SoftwareX},
	author = {Petukhova, Alina and Fachada, Nuno},
	year = {2022},
	note = {ISBN: 2352-7110
Publisher: Elsevier},
	keywords = {��, /unread},
	pages = {101122},
	file = {全文:E\:\\Program Data\\Zotero\\storage\\MYVAIBGS\\Petukhova 和 Fachada - 2022 - TextCL A Python package for NLP preprocessing tas.pdf:application/pdf},
}

@inproceedings{mekala_contextualized_2020,
	address = {Online},
	title = {Contextualized {Weak} {Supervision} for {Text} {Classification}},
	url = {https://aclanthology.org/2020.acl-main.30},
	doi = {10.18653/v1/2020.acl-main.30},
	abstract = {Weakly supervised text classification based on a few user-provided seed words has recently attracted much attention from researchers. Existing methods mainly generate pseudo-labels in a context-free manner (e.g., string matching), therefore, the ambiguous, context-dependent nature of human language has been long overlooked. In this paper, we propose a novel framework ConWea, providing contextualized weak supervision for text classification. Specifically, we leverage contextualized representations of word occurrences and seed word information to automatically differentiate multiple interpretations of the same word, and thus create a contextualized corpus. This contextualized corpus is further utilized to train the classifier and expand seed words in an iterative manner. This process not only adds new contextualized, highly label-indicative keywords but also disambiguates initial seed words, making our weak supervision fully contextualized. Extensive experiments and case studies on real-world datasets demonstrate the necessity and significant advantages of using contextualized weak supervision, especially when the class labels are fine-grained.},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Mekala, Dheeraj and Shang, Jingbo},
	month = jul,
	year = {2020},
	pages = {323--333},
	file = {全文:E\:\\Program Data\\Zotero\\storage\\K4U3JGZM\\Mekala 和 Shang - 2020 - Contextualized Weak Supervision for Text Classific.pdf:application/pdf},
}

@inproceedings{mukherjee_uncertainty-aware_2020,
	title = {Uncertainty-aware {Self}-training for {Few}-shot {Text} {Classification}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f23d125da1e29e34c552f448610ff25f-Paper.pdf},
	abstract = {利用自训练机制，从无标注池中随机抽样实例，生成伪标注并扩充标注数据。1) 利用蒙特-卡洛 Dropout 从未标明池中选择实例的获取函数。2) 利用模型置信度进行自我训练的学习机制。用于文本分类任务有明显提升。},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Mukherjee, Subhabrata and Awadallah, Ahmed},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	keywords = {⛔ No DOI found, ✅, ⭐},
	pages = {21199--21212},
	file = {Mukherjee 和 Awadallah - 2020 - Uncertainty-aware Self-training for Few-shot Text .pdf:E\:\\Program Data\\Zotero\\storage\\KRA32JGH\\Mukherjee 和 Awadallah - 2020 - Uncertainty-aware Self-training for Few-shot Text .pdf:application/pdf;Uncertainty-aware Self-training for Few-shot Text Classification.md:D\:\\学习&工作\\NLP数据预处理\\文献阅读\\Uncertainty-aware Self-training for Few-shot Text Classification.md:text/plain},
}

@article{yu_regen_2023,
	title = {{ReGen}: {Zero}-{Shot} {Text} {Classification} via {Training} {Data} {Generation} with {Progressive} {Dense} {Retrieval}},
	journal = {arXiv preprint arXiv:2305.10703},
	author = {Yu, Yue and Zhuang, Yuchen and Zhang, Rongzhi and Meng, Yu and Shen, Jiaming and Zhang, Chao},
	year = {2023},
	keywords = {⛔ No DOI found},
	file = {Yu 等 - 2023 - ReGen Zero-Shot Text Classification via Training .pdf:E\:\\Program Data\\Zotero\\storage\\PVMBHQEC\\Yu 等 - 2023 - ReGen Zero-Shot Text Classification via Training .pdf:application/pdf},
}

@inproceedings{ye_zero-shot_2020,
	address = {Online},
	title = {Zero-shot {Text} {Classification} via {Reinforced} {Self}-training},
	url = {https://aclanthology.org/2020.acl-main.272},
	doi = {10.18653/v1/2020.acl-main.272},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Ye, Zhiquan and Geng, Yuxia and Chen, Jiaoyan and Chen, Jingmin and Xu, Xiaoxiao and Zheng, SuHang and Wang, Feng and Zhang, Jun and Chen, Huajun},
	month = jul,
	year = {2020},
	keywords = {��, ✅, ⭐},
	pages = {3014--3024},
	file = {全文:E\:\\Program Data\\Zotero\\storage\\MRKRZ2BH\\Ye 等 - 2020 - Zero-shot Text Classification via Reinforced Self-.pdf:application/pdf;Zero-shot Text Classification via Reinforced Self-training.md:D\:\\学习&工作\\NLP数据预处理\\文献阅读\\Zero-shot Text Classification via Reinforced Self-training.md:text/plain},
}

@inproceedings{chu_unsupervised_2021,
	address = {Online},
	title = {Unsupervised {Label} {Refinement} {Improves} {Dataless} {Text} {Classification}},
	url = {https://aclanthology.org/2021.findings-acl.365},
	doi = {10.18653/v1/2021.findings-acl.365},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL}-{IJCNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Chu, Zewei and Stratos, Karl and Gimpel, Kevin},
	month = aug,
	year = {2021},
	keywords = {/unread},
	pages = {4165--4178},
	file = {全文:E\:\\Program Data\\Zotero\\storage\\9JJTQHUF\\Chu 等 - 2021 - Unsupervised Label Refinement Improves Dataless Te.pdf:application/pdf},
}

@inproceedings{liu_few-shot_2022,
	title = {Few-{Shot} {Parameter}-{Efficient} {Fine}-{Tuning} is {Better} and {Cheaper} than {In}-{Context} {Learning}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/0cde695b83bd186c1fd456302888454c-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin A},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	keywords = {⛔ No DOI found, /unread},
	pages = {1950--1965},
	file = {全文:E\:\\Program Data\\Zotero\\storage\\KA5NVMK9\\Liu 等 - 2022 - Few-Shot Parameter-Efficient Fine-Tuning is Better.pdf:application/pdf},
}

@inproceedings{gururangan_whose_2022,
	title = {Whose {Language} {Counts} as {High} {Quality}? {Measuring} {Language} {Ideologies} in {Text} {Data} {Selection}},
	doi = {10.18653/v1/2022.emnlp-main.165},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Gururangan, Suchin and Card, Dallas and Dreier, Sarah and Gade, Emily and Wang, Leroy and Wang, Zeyu and Zettlemoyer, Luke and Smith, Noah A.},
	year = {2022},
	pages = {2562--2580},
	file = {全文:E\:\\Program Data\\Zotero\\storage\\LC28YE9N\\Gururangan 等 - 2022 - Whose Language Counts as High Quality Measuring L.pdf:application/pdf},
}

@inproceedings{mesgar_neural_2018,
	title = {A neural local coherence model for text quality assessment},
	doi = {10.18653/v1/D18-1464},
	booktitle = {Proceedings of the 2018 conference on empirical methods in natural language processing},
	author = {Mesgar, Mohsen and Strube, Michael},
	year = {2018},
	pages = {4328--4339},
	file = {全文:E\:\\Program Data\\Zotero\\storage\\MS9GAIDK\\Mesgar 和 Strube - 2018 - A neural local coherence model for text quality as.pdf:application/pdf},
}

@inproceedings{ousidhoum_probing_2021,
	title = {Probing toxic content in large pre-trained language models},
	doi = {10.18653/v1/2021.acl-long.329},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	author = {Ousidhoum, Nedjma and Zhao, Xinran and Fang, Tianqing and Song, Yangqiu and Yeung, Dit-Yan},
	year = {2021},
	pages = {4262--4274},
	file = {全文:E\:\\Program Data\\Zotero\\storage\\WJ52STBE\\Ousidhoum 等 - 2021 - Probing toxic content in large pre-trained languag.pdf:application/pdf},
}

@inproceedings{zhang_when_2021,
	title = {When {Do} {You} {Need} {Billions} of {Words} of {Pretraining} {Data}?},
	doi = {10.18653/v1/2021.acl-long.90},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	author = {Zhang, Yian and Warstadt, Alex and Li, Xiaocheng and Bowman, Samuel},
	year = {2021},
	keywords = {/unread},
	pages = {1112--1125},
	file = {Zhang et al_2021_When Do You Need Billions of Words of Pretraining Data.pdf:E\:\\Program Data\\Zotero_linked_files\\NLP\\文本数据预处理\\Zhang et al_2021_When Do You Need Billions of Words of Pretraining Data.pdf:application/pdf},
}

@inproceedings{lee_deduplicating_2022,
	title = {Deduplicating {Training} {Data} {Makes} {Language} {Models} {Better}},
	doi = {10.18653/v1/2022.acl-long.577},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	author = {Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
	year = {2022},
	keywords = {✅, ⭐},
	pages = {8424--8445},
	file = {全文:E\:\\Program Data\\Zotero\\storage\\DAS6PZHX\\Lee 等 - 2022 - Deduplicating Training Data Makes Language Models .pdf:application/pdf;Deduplicating Training DataMakes Language Models Better.md:D\:\\学习\\CIP\\NLP数据预处理\\文献阅读\\Deduplicating Training DataMakes Language Models Better.md:text/plain},
}

@inproceedings{song_multi-stage_2020,
	address = {Online},
	title = {Multi-{Stage} {Pre}-training for {Automated} {Chinese} {Essay} {Scoring}},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.546},
	doi = {10.18653/v1/2020.emnlp-main.546},
	abstract = {本文提出了一种基于预训练的中文作文自动评分方法。该方法包括三个阶段：弱监督预训练、监督交叉提示微调和监督目标提示微调。首先在一个大型作文数据集上对作文评分器进行预训练，该数据集涵盖各种主题，并有粗略的评分，即好和差，作为一种弱监督。},
	language = {en},
	urldate = {2023-08-01},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Song, Wei and Zhang, Kai and Fu, Ruiji and Liu, Lizhen and Liu, Ting and Cheng, Miaomiao},
	year = {2020},
	keywords = {��, ��},
	pages = {6723--6733},
	file = {Song 等 - 2020 - Multi-Stage Pre-training for Automated Chinese Ess.pdf:E\:\\Program Data\\Zotero\\storage\\WV6IX5RC\\Song 等 - 2020 - Multi-Stage Pre-training for Automated Chinese Ess.pdf:application/pdf},
}

@inproceedings{deng_cold_2022,
	title = {{COLD}: {A} {Benchmark} for {Chinese} {Offensive} {Language} {Detection}},
	doi = {10.18653/v1/2022.emnlp-main.796},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Deng, Jiawen and Zhou, Jingyan and Sun, Hao and Zheng, Chujie and Mi, Fei and Meng, Helen and Huang, Minlie},
	year = {2022},
	pages = {11580--11599},
	file = {Deng et al_2022_COLD.pdf:E\:\\Program Data\\Zotero_linked_files\\NLP\\文本数据预处理\\Deng et al_2022_COLD.pdf:application/pdf},
}


@article{devlin_bert_2018,
	title = {Bert: {Pre}-training of deep bidirectional transformers for language understanding},
	url = {https://arxiv.org/pdf/1810.04805.pdf},
	journal = {arXiv preprint arXiv:1810.04805},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	year = {2018},
	keywords = {⛔ No DOI found, ��, ⭐},
	file = {全文:E\:\\Program Data\\Zotero\\storage\\TP74BYT5\\Devlin 等 - 2018 - Bert Pre-training of deep bidirectional transform.pdf:application/pdf},
}

@article{raffel_exploring_2020,
	title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
	volume = {21},
	doi = {abs/10.5555/3455716.3455856},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	year = {2020},
	note = {ISBN: 1532-4435
Publisher: JMLRORG},
	keywords = {⛔ No DOI found, /unread, ⚠️ Invalid DOI},
	pages = {5485--5551},
	file = {Raffel 等 - 2020 - Exploring the limits of transfer learning with a u.pdf:E\:\\Program Data\\Zotero\\storage\\QCVSCKPV\\Raffel 等 - 2020 - Exploring the limits of transfer learning with a u.pdf:application/pdf},
}

@inproceedings{li_blip_2022,
	title = {Blip: {Bootstrapping} language-image pre-training for unified vision-language understanding and generation},
	isbn = {2640-3498},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
	year = {2022},
	pages = {12888--12900},
	file = {Li et al_2022_Blip.pdf:E\:\\Program Data\\Zotero_linked_files\\NLP\\Pretrain\\多模态预训练\\Li et al_2022_Blip.pdf:application/pdf},
}

@article{wang_blsp_2023,
	title = {{BLSP}: {Bootstrapping} {Language}-{Speech} {Pre}-training via {Behavior} {Alignment} of {Continuation} {Writing}},
	journal = {arXiv preprint arXiv:2309.00916},
	author = {Wang, Chen and Liao, Minpeng and Huang, Zhongqiang and Lu, Jinliang and Wu, Junhong and Liu, Yuchen and Zong, Chengqing and Zhang, Jiajun},
	year = {2023},
	keywords = {⛔ No DOI found},
	file = {Wang et al_2023_BLSP.pdf:E\:\\Program Data\\Zotero_linked_files\\NLP\\Pretrain\\多模态预训练\\Wang et al_2023_BLSP.pdf:application/pdf},
}

@inproceedings{ao_speecht5_2022,
	title = {{SpeechT5}: {Unified}-{Modal} {Encoder}-{Decoder} {Pre}-{Training} for {Spoken} {Language} {Processing}},
	doi = {10.18653/v1/2022.acl-long.393},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	author = {Ao, Junyi and Wang, Rui and Zhou, Long and Wang, Chengyi and Ren, Shuo and Wu, Yu and Liu, Shujie and Ko, Tom and Li, Qing and Zhang, Yu},
	year = {2022},
	keywords = {��},
	pages = {5723--5738},
	file = {Ao et al_2022_SpeechT5.pdf:E\:\\Program Data\\Zotero_linked_files\\NLP\\Pretrain\\多模态预训练\\Ao et al_2022_SpeechT5.pdf:application/pdf},
}

@inproceedings{li_align_2021,
	title = {Align before {Fuse}: {Vision} and {Language} {Representation} {Learning} with {Momentum} {Distillation}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/505259756244493872b7709a8a01b536-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P. S. and Vaughan, J. Wortman},
	year = {2021},
	keywords = {⛔ No DOI found, ��},
	pages = {9694--9705},
	file = {Li et al_2021_Align before Fuse.pdf:E\:\\Program Data\\Zotero_linked_files\\NLP\\Pretrain\\多模态预训练\\Li et al_2021_Align before Fuse.pdf:application/pdf},
}

@article{hoffmann_training_2022,
	title = {Training compute-optimal large language models},
	journal = {arXiv preprint arXiv:2203.15556},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan},
	year = {2022},
	keywords = {⛔ No DOI found},
	file = {Hoffmann et al_2022_Training compute-optimal large language models.pdf:E\:\\Program Data\\Zotero_linked_files\\NLP\\Pretrain\\Hoffmann et al_2022_Training compute-optimal large language models.pdf:application/pdf},
}

@inproceedings{grave2018learning,
  title={Learning Word Vectors for 157 Languages},
  author={Grave, {\'E}douard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tom{\'a}{\v{s}}},
  booktitle={Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  year={2018}
}



@article{bojanowski_enriching_2017,
	title = {Enriching word vectors with subword information},
	volume = {5},
	doi = {10.1162/tacl_a_00051},
	journal = {Transactions of the association for computational linguistics},
	author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	year = {2017},
	note = {ISBN: 2307-387X
Publisher: MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info …},
	keywords = {��},
	pages = {135--146},
	file = {Bojanowski et al_2017_Enriching word vectors with subword information.pdf:E\:\\Program Data\\Zotero_linked_files\\NLP\\文本数据预处理\\Bojanowski et al_2017_Enriching word vectors with subword information.pdf:application/pdf},
}
@inproceedings{word2vec,
  author       = {Tom{\'{a}}s Mikolov and
                  Kai Chen and
                  Greg Corrado and
                  Jeffrey Dean},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Efficient Estimation of Word Representations in Vector Space},
  booktitle    = {1st International Conference on Learning Representations, {ICLR} 2013,
                  Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings},
  year         = {2013},
  url          = {http://arxiv.org/abs/1301.3781},
  timestamp    = {Mon, 28 Dec 2020 11:31:01 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1301-3781.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{transformer,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  editor       = {Isabelle Guyon and
                  Ulrike von Luxburg and
                  Samy Bengio and
                  Hanna M. Wallach and
                  Rob Fergus and
                  S. V. N. Vishwanathan and
                  Roman Garnett},
  title        = {Attention is All you Need},
  booktitle    = {Advances in Neural Information Processing Systems 30: Annual Conference
                  on Neural Information Processing Systems 2017, December 4-9, 2017,
                  Long Beach, CA, {USA}},
  pages        = {5998--6008},
  year         = {2017},
  url          = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  timestamp    = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{joulin_bag_2017,
	title = {Bag of {Tricks} for {Efficient} {Text} {Classification}},
	doi = {10.18653/v1/E17-2068},
	booktitle = {Proceedings of the 15th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Volume} 2, {Short} {Papers}},
	author = {Joulin, Armand and Grave, Édouard and Bojanowski, Piotr and Mikolov, Tomáš},
	year = {2017},
	keywords = {��},
	pages = {427--431},
}


@article{stanton2021does,
  title={Does knowledge distillation really work?},
  author={Stanton, Samuel and Izmailov, Pavel and Kirichenko, Polina and Alemi, Alexander A and Wilson, Andrew G},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={6906--6919},
  year={2021}
}

@inproceedings{liu2021temp,
  title={TEMP: taxonomy expansion with dynamic margin loss through taxonomy-paths},
  author={Liu, Zichen and Xu, Hongyuan and Wen, Yanlong and Jiang, Ning and Wu, Haiying and Yuan, Xiaojie},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={3854--3863},
  year={2021}
}

@article{scudder1965probability,
  title={Probability of error of some adaptive pattern-recognition machines},
  author={Scudder, Henry},
  journal={IEEE Transactions on Information Theory},
  volume={11},
  number={3},
  pages={363--371},
  year={1965},
  publisher={IEEE}
}

@inproceedings{sun2019patient,
  title={Patient Knowledge Distillation for BERT Model Compression},
  author={Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={4323--4332},
  year={2019}
}
@article{2015kd,
  author       = {Geoffrey E. Hinton and
                  Oriol Vinyals and
                  Jeffrey Dean},
  title        = {Distilling the Knowledge in a Neural Network},
  journal      = {CoRR},
  volume       = {abs/1503.02531},
  year         = {2015},
  url          = {http://arxiv.org/abs/1503.02531},
  eprinttype    = {arXiv},
  eprint       = {1503.02531},
  timestamp    = {Mon, 13 Aug 2018 16:48:36 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/HintonVD15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{2019kdmc,
  author       = {Siqi Sun and
                  Yu Cheng and
                  Zhe Gan and
                  Jingjing Liu},
  editor       = {Kentaro Inui and
                  Jing Jiang and
                  Vincent Ng and
                  Xiaojun Wan},
  title        = {Patient Knowledge Distillation for {BERT} Model Compression},
  booktitle    = {Proceedings of the 2019 Conference on Empirical Methods in Natural
                  Language Processing and the 9th International Joint Conference on
                  Natural Language Processing, {EMNLP-IJCNLP} 2019, Hong Kong, China,
                  November 3-7, 2019},
  pages        = {4322--4331},
  publisher    = {Association for Computational Linguistics},
  year         = {2019},
  url          = {https://doi.org/10.18653/v1/D19-1441},
  doi          = {10.18653/v1/D19-1441},
  timestamp    = {Sat, 30 Sep 2023 09:40:16 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/SunCGL19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{2020_pile,
  title={The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{gao2020pile,
	title={The pile: An 800gb dataset of diverse text for language modeling},
	author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
	journal={arXiv preprint arXiv:2101.00027},
	year={2020}
}


@article{penedo2023refinedweb,
	title={The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only},
	author={Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
	journal={arXiv preprint arXiv:2306.01116},
	year={2023}
}

@article{scao2022bloom,
	title={Bloom: A 176b-parameter open-access multilingual language model},
	author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
	journal={arXiv preprint arXiv:2211.05100},
	year={2022}
}

@article{gunasekar2023textbooks,
	title={Textbooks Are All You Need},
	author={Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
	journal={arXiv preprint arXiv:2306.11644},
	year={2023}
}
@misc{2023refinedweb,
      title={The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only}, 
      author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},
      year={2023},
      eprint={2306.01116},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{2020T5C4,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1--67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@misc{shen2023slimpajamadc,
      title={SlimPajama-DC: Understanding Data Combinations for LLM Training}, 
      author={Zhiqiang Shen and Tianhua Tao and Liqun Ma and Willie Neiswanger and Zhengzhong Liu and Hongyi Wang and Bowen Tan and Joel Hestness and Natalia Vassilieva and Daria Soboleva and Eric Xing},
      year={2023},
      eprint={2309.10818},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{2019OSCAR,
  author    = {Pedro Javier {Ortiz Su{\'a}rez} and Beno{\^i}t Sagot and Laurent Romary},
  title     = {Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures},
  series = {Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019},
  editor    = {Piotr Bański and Adrien Barbaresi and Hanno Biber and Evelyn Breiteneder and Simon Clematide and Marc Kupietz and Harald L{\"u}ngen and Caroline Iliadi},
  publisher = {Leibniz-Institut f{\"u}r Deutsche Sprache},
  address   = {Mannheim},
  doi       = {10.14618/ids-pub-9021},
  url       = {http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215},
  pages     = {9 -- 16},
  year      = {2019},
  abstract  = {Common Crawl is a considerably large, heterogeneous multilingual corpus comprised of crawled documents from the internet, surpassing 20TB of data and distributed as a set of more than 50 thousand plain text files where each contains many documents written in a wide variety of languages. Even though each document has a metadata block associated to it, this data lacks any information about the language in which each document is written, making it extremely difficult to use Common Crawl for monolingual applications. We propose a general, highly parallel, multithreaded pipeline to clean and classify Common Crawl by language; we specifically design it so that it runs efficiently on medium to low resource infrastructures where I/O speeds are the main constraint. We develop the pipeline so that it can be easily reapplied to any kind of heterogeneous corpus and so that it can be parameterised to a wide range of infrastructures. We also distribute a 6.3TB version of Common Crawl, filtered, classified by language, shuffled at line level in order to avoid copyright issues, and ready to be used for NLP applications.},
  language  = {en}
}
@misc{2023wanjuan,
      title={WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models}, 
      author={Conghui He and Zhenjiang Jin and Chao Xu and Jiantao Qiu and Bin Wang and Wei Li and Hang Yan and Jiaqi Wang and Dahua Lin},
      year={2023},
      eprint={2308.10755},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{2021WuDaoCorpora,
  title={WuDaoCorpora: A super large-scale Chinese corpora for pre-training language models},
  author={Sha Yuan and Hanyu Zhao and Zhengxiao Du and Ming Ding and Xiao Liu and Yukuo Cen and Xu Zou and Zhilin Yang and Jie Tang},
  journal={AI Open},
  year={2021},
  volume={2},
  pages={65-68},
  url={https://api.semanticscholar.org/CorpusID:236712622}
}
@misc{2023roots,
      title={The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset}, 
      author={Hugo Laurençon and Lucile Saulnier and Thomas Wang and Christopher Akiki and Albert Villanova del Moral and Teven Le Scao and Leandro Von Werra and Chenghao Mou and Eduardo González Ponferrada and Huu Nguyen and Jörg Frohberg and Mario Šaško and Quentin Lhoest and Angelina McMillan-Major and Gerard Dupont and Stella Biderman and Anna Rogers and Loubna Ben allal and Francesco De Toni and Giada Pistilli and Olivier Nguyen and Somaieh Nikpoor and Maraim Masoud and Pierre Colombo and Javier de la Rosa and Paulo Villegas and Tristan Thrush and Shayne Longpre and Sebastian Nagel and Leon Weber and Manuel Muñoz and Jian Zhu and Daniel Van Strien and Zaid Alyafeai and Khalid Almubarak and Minh Chien Vu and Itziar Gonzalez-Dios and Aitor Soroa and Kyle Lo and Manan Dey and Pedro Ortiz Suarez and Aaron Gokaslan and Shamik Bose and David Adelani and Long Phan and Hieu Tran and Ian Yu and Suhas Pai and Jenny Chim and Violette Lepercq and Suzana Ilic and Margaret Mitchell and Sasha Alexandra Luccioni and Yacine Jernite},
      year={2023},
      eprint={2303.03915},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{presser2020books3,
	title={Books3},
	author={Presser, Shawn},
	year={2020}
}

@inproceedings{tiedemann2016finding,
	title={Finding alternative translations in a large corpus of movie subtitle},
	author={Tiedemann, J{\"o}rg},
	booktitle={Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)},
	pages={3518--3522},
	year={2016}
}


@misc{he2023wanjuan,
	title={WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models}, 
	author={Conghui He and Zhenjiang Jin and Chao Xu and Jiantao Qiu and Bin Wang and Wei Li and Hang Yan and Jiaqi Wang and Dahua Lin},
	year={2023},
	eprint={2308.10755},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

