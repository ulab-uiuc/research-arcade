Fetched figures: [{'id': 7852, 'label': '\\label{fig:framework}', 'caption': '\\caption{\n The \\textit{Sel3DCraft} pipeline begins by expanding user text input into multiple candidates via LLM. A dual-branch pipeline then generates/retrieves corresponding multi-view images, displayed in an interactive satellite view for 3D perception. Each candidate is automatically evaluated using eight-level semantic scores with heatmap visualization. For refinement, a score-guided recommendation mechanism suggests optimal keywords, enabling iterative optimization until user satisfaction is achieved.\n}', 'path': 'figs/Method.pdf'}, {'id': 60981, 'label': '\\label{fig:sd_latents}', 'caption': "\\caption{The generated results at each timestep of the diffusion sampling process from $T$ to $1$. For example, given one prompt case ``A man with a beard wearing glasses in blue shirt'', the noise in the image is gradually reduced from timestep $T$ to $1$, and the image is eventually generated with clarity (from left to right, top to bottom).}", 'path': 'app_imgs/sd_latents_compressed.pdf'}, {'id': 108193, 'label': None, 'caption': '\\caption{Llama2-7b}', 'path': 'figure/enumerate_answer/enumerate_answer_llama.pdf'}, {'id': 155405, 'label': None, 'caption': '\\caption{Simply supported beam with distributed loading.}', 'path': './figures/midCantileverBeam.png'}, {'id': 202617, 'label': None, 'caption': "\\caption{Additional qualitative results for Edges$\\rightarrow$Shoes$\\times64$,\nwhere each pair of consecutive rows displaying the input image in\nthe ``Edges'' domain and its translation in the ``Shoes'' domain,\nrespectively.\\label{fig:e2s_appdx}}", 'path': 'asset/Result/qualitative_result/more_viz/e2s_appdx_v2'}, {'id': 249829, 'label': '\\label{fig:juxtaposition}', 'caption': '\\caption{Performance on a commonsense benchmark (HellaSwag), a linguistic understanding benchmark (SuperGLUE), and the massive multitask test. On previous benchmarks, smaller models start well above random chance levels and exhibit more continuous improvements with model size increases, but on our test, GPT-3 moves beyond random chance with the largest model.}', 'path': 'figures/test_juxaposition.pdf'}, {'id': 297041, 'label': '\\label{fig:sfig2}', 'caption': '\\caption{}', 'path': 'figs/key_sample_vis_3_all.eps'}, {'id': 344253, 'label': '\\label{fig:rarl_curves}', 'caption': '\\caption{}', 'path': 'Figures/cartpole_baseline_curves.png'}, {'id': 391465, 'label': '\\label{fig:combined_benchmarkgss-26-s100.cnf}', 'caption': '\\caption{gss-26-s100}', 'path': 'figs/cnc/combined_benchmarkgss-26-s100.cnf.png'}, {'id': 438677, 'label': '\\label{fig:diversity_stats_places}', 'caption': '\\caption{Statistics over locations}', 'path': 'figures/schemes/location.pdf'}]
[{'id': 7858, 'label': '\\label{tab:results}', 'text': ''}, {'id': 60987, 'label': None, 'text': ''}, {'id': 108199, 'label': '\\label{table:example_str}', 'text': "\\begin{tabular}{p{2.5cm}|p{12cm}@{}}\n    \\toprule\n    Examples &\n\nHelp me answer question regarding spatial relationship in a 2D plane:\n\nGiven Information:\n\n    You will receive a series of object trajectory and the corresponding timestamps of the coordinates in the trajectory. You can treat the trajectory as linestring.\n\n    Sensor A: $[(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)]$\n\n    Timestamp: $[t1, t2, ..., tn]$\n\n    You will be provided with geometric information involving three types of 2D geometries—Point, LineString, and Polygon—all defined using the ESRI (Environmental Systems Research Institute) geometric format. These geometries are expressed as lists of coordinates in a Cartesian plane.\n\n    Point: A single coordinate location in space, defined as a tuple: \n    \n    $[(x, y)]$.\n\n    LineString: A sequence of points that forms a continuous line. It is represented as an ordered list of coordinate pairs: \n    \n    $[(x_1, y_1), (x_2, y2), ... (x_n, y_n)]$.\n\n   Polygon: A closed shape formed by a sequence of coordinate pairs where the first and last points are the same to close the loop:\n    \n    $[(x_1, y_1), (x_2, y2), ... (x_n, y_n), (x_1, y_1)]$.\n\n    Spatial Relationships (based on ArcGIS model) are as follows. During computation, you should account for floating-point precision with a numerical tolerance. For instance, 'Equals' should return True not just for exact mathematical identity, but if two geometries are identical within this tolerance.\n\n    ArcGIS defines spatial relationships as logical conditions between geometric objects:\n\n...\n\n    Temporal Relationships (based on Allen's interval algebra):\n\n...\n\nObjective:\n\nDetermine whether the time interval during which the EVENT holds has the temporal relationship **is\\_equal\\_to** with the reference interval (6.9569, 9.5423)?\nEVENT: the following object trajectory has the spatial relationship **intersects** with Polygon [(8.9995, 9.6963), (9.4793, 9.4669), (9.9179, 9.7677), (9.8767, 10.2979), (9.3969, 10.5273), (8.9583, 10.2265), (8.9995, 9.6963)]\n\nFor any interaction between a trajectory (you can view it as a LineString) and a fixed geometry—whether that geometry is another LineString, a Point, or a Polygon—define the “event interval” as follows:\n\n1. Pick a trajectory segment and a predicate.\n2. Project the segment endpoints back to their timestamps.  For each contiguous satisfying segment, let $t_1$ be the time at the segment’s first vertex and $t_2$ be the time at its last vertex.\n3. Define the event interval as the union of all **$[t_1, t_2]$** intervals in which the predicate is true.\n4. Do not include portions of the trajectory before the relationship begins or after it ends. Do not interpolate.\n\nAnswer 1 if answer is Yes. Otherwise, answer 0.\n\nObject trajectory: [(10.0000, 9.6138), (9.8171, 9.8075), (9.6491, 9.9374), (9.4950, 10.0000), (9.4381, 9.9971), (9.2219, 9.9145), (9.0990, 9.7653), (8.9822, 9.5463), (8.8692, 9.2601), (8.7575, 8.9104)]\nTimestamp: [1.9871, 3.0075, 4.0279, 5.0483, 6.0687, 7.0891, 8.1095, 9.1299, 10.1503, 11.1707]\n\n\nOutput Format:\n    Your response **must** include the answer (0 or 1) in the following format:\n\n    [RESULTS\\_START] [p] [RESULTS\\_END]\n\n    You may include explanatory text elsewhere in your response. However, do not include any text or additional formatting between [RESULTS\\_START] and [RESULTS\\_END].\n\nExample:\n\n    [RESULTS\\_START] [1] [RESULTS\\_END] \\\\\n\\midrule\nAnswer & [0] \\\\\n\\bottomrule\n  \\end{tabular}"}, {'id': 155411, 'label': '\\label{tab:ED.compare}', 'text': '\\begin{tabular}{l c c c}\n         \\toprule\n         \\textbf{Method} & ($g_1^*$, $g_2^*$, $g_3^*$) \\textbf{(MW/h)} & \\textbf{Nominal cost (\\$/h)}  \\\\\n         \\midrule\n         ED & (144.3, 170.7, 0) & 926 \\\\\n         \\PSCED{} & (110, 160, 45) & 1192 \\\\\n         \\CSCED{} & (119, 181, 15) & 962  \\\\\n         \\bottomrule \\\\\n    \\end{tabular}'}]
Fetched tables: [{'id': 7858, 'label': '\\label{tab:results}', 'text': ''}, {'id': 60987, 'label': None, 'text': ''}, {'id': 108199, 'label': '\\label{table:example_str}', 'text': "\\begin{tabular}{p{2.5cm}|p{12cm}@{}}\n    \\toprule\n    Examples &\n\nHelp me answer question regarding spatial relationship in a 2D plane:\n\nGiven Information:\n\n    You will receive a series of object trajectory and the corresponding timestamps of the coordinates in the trajectory. You can treat the trajectory as linestring.\n\n    Sensor A: $[(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)]$\n\n    Timestamp: $[t1, t2, ..., tn]$\n\n    You will be provided with geometric information involving three types of 2D geometries—Point, LineString, and Polygon—all defined using the ESRI (Environmental Systems Research Institute) geometric format. These geometries are expressed as lists of coordinates in a Cartesian plane.\n\n    Point: A single coordinate location in space, defined as a tuple: \n    \n    $[(x, y)]$.\n\n    LineString: A sequence of points that forms a continuous line. It is represented as an ordered list of coordinate pairs: \n    \n    $[(x_1, y_1), (x_2, y2), ... (x_n, y_n)]$.\n\n   Polygon: A closed shape formed by a sequence of coordinate pairs where the first and last points are the same to close the loop:\n    \n    $[(x_1, y_1), (x_2, y2), ... (x_n, y_n), (x_1, y_1)]$.\n\n    Spatial Relationships (based on ArcGIS model) are as follows. During computation, you should account for floating-point precision with a numerical tolerance. For instance, 'Equals' should return True not just for exact mathematical identity, but if two geometries are identical within this tolerance.\n\n    ArcGIS defines spatial relationships as logical conditions between geometric objects:\n\n...\n\n    Temporal Relationships (based on Allen's interval algebra):\n\n...\n\nObjective:\n\nDetermine whether the time interval during which the EVENT holds has the temporal relationship **is\\_equal\\_to** with the reference interval (6.9569, 9.5423)?\nEVENT: the following object trajectory has the spatial relationship **intersects** with Polygon [(8.9995, 9.6963), (9.4793, 9.4669), (9.9179, 9.7677), (9.8767, 10.2979), (9.3969, 10.5273), (8.9583, 10.2265), (8.9995, 9.6963)]\n\nFor any interaction between a trajectory (you can view it as a LineString) and a fixed geometry—whether that geometry is another LineString, a Point, or a Polygon—define the “event interval” as follows:\n\n1. Pick a trajectory segment and a predicate.\n2. Project the segment endpoints back to their timestamps.  For each contiguous satisfying segment, let $t_1$ be the time at the segment’s first vertex and $t_2$ be the time at its last vertex.\n3. Define the event interval as the union of all **$[t_1, t_2]$** intervals in which the predicate is true.\n4. Do not include portions of the trajectory before the relationship begins or after it ends. Do not interpolate.\n\nAnswer 1 if answer is Yes. Otherwise, answer 0.\n\nObject trajectory: [(10.0000, 9.6138), (9.8171, 9.8075), (9.6491, 9.9374), (9.4950, 10.0000), (9.4381, 9.9971), (9.2219, 9.9145), (9.0990, 9.7653), (8.9822, 9.5463), (8.8692, 9.2601), (8.7575, 8.9104)]\nTimestamp: [1.9871, 3.0075, 4.0279, 5.0483, 6.0687, 7.0891, 8.1095, 9.1299, 10.1503, 11.1707]\n\n\nOutput Format:\n    Your response **must** include the answer (0 or 1) in the following format:\n\n    [RESULTS\\_START] [p] [RESULTS\\_END]\n\n    You may include explanatory text elsewhere in your response. However, do not include any text or additional formatting between [RESULTS\\_START] and [RESULTS\\_END].\n\nExample:\n\n    [RESULTS\\_START] [1] [RESULTS\\_END] \\\\\n\\midrule\nAnswer & [0] \\\\\n\\bottomrule\n  \\end{tabular}"}, {'id': 155411, 'label': '\\label{tab:ED.compare}', 'text': '\\begin{tabular}{l c c c}\n         \\toprule\n         \\textbf{Method} & ($g_1^*$, $g_2^*$, $g_3^*$) \\textbf{(MW/h)} & \\textbf{Nominal cost (\\$/h)}  \\\\\n         \\midrule\n         ED & (144.3, 170.7, 0) & 926 \\\\\n         \\PSCED{} & (110, 160, 45) & 1192 \\\\\n         \\CSCED{} & (119, 181, 15) & 962  \\\\\n         \\bottomrule \\\\\n    \\end{tabular}'}]
Previous paragraphs: ["\\subsection{Relevant Details of the Novelty LLM}\n\\label{Appendix:novelty}\nFor \\textit{Novelty LLM}, it is trained through supervised fine-tuning based on the Qwen2.5-7B-Instruct-1M model. Its prompt template is shown in Figure \\ref{prompt_22}. Its fine-tuning prompt and inference prompt are \\(\\mathcal{P}^{(t-2:t-1)}_{\\text{ft-nov}}\\)\\mathcal{P}^{(t-2:t-1)}(t-2:t-1)_{\\text{ft-nov}}\\text{ft-nov} and \\(\\mathcal{P}_{\\text{infer-nov}}\\)\\mathcal{P}_{\\text{infer-nov}}\\text{infer-nov} respectively, and they share the same prompt template, with differences only in the specific content to be filled during inference: for fine-tuning, the content corresponding to \\#Categories in the template is \\(\\mathcal{S}^{(t-2:t-1)}_{\\text{short}}\\)\\mathcal{S}^{(t-2:t-1)}(t-2:t-1)_{\\text{short}}\\text{short} from \\(\\mathcal{P}^{(t-2:t-1)}_{\\text{ft-nov}}\\)\\mathcal{P}^{(t-2:t-1)}(t-2:t-1)_{\\text{ft-nov}}\\text{ft-nov}; for inference, the content corresponding to \\#Categories in the template is \\(\\mathcal{C}'_{\\text{short}}\\)\\mathcal{C}'_{\\text{short}}\\text{short} from \\(\\mathcal{P}_{\\text{infer-nov}}\\)\\mathcal{P}_{\\text{infer-nov}}\\text{infer-nov}. If their lengths are different, the last two categories of \\(\\mathcal{C}'_{\\text{short}}\\)\\mathcal{C}'_{\\text{short}}\\text{short} are selected to ensure consistent length."]
Next paragraphs: ['In the incremental fine-tuning of closed-loop optimization, we perform LoRA fine-tuning on \\textit{Novelty LLM} using the DPO (Direct Preference Optimization) method, with specific configurations as follows: LoRA rank is 8, covering all layers, the preference value is set to 0.1, and the sigmoid loss function is adopted. Regarding training parameters, the single-device batch size is 2, the number of gradient accumulations is 8, and the learning rate is 5.0e-6.', "\\subsection{Relevant Details of the Relevance LLM}\n\\label{Appendix:relevance}\nFor \\textit{Revelance LLM}, it is also based on the Qwen2.5-7B-Instruct-1M model, with fine-tuning configurations consistent with those of \\textit{Novelty LLM}.\nIts prompt template is shown in Figure \\ref{prompt_33}. The fine-tuning prompt \\(\\mathcal{P}^{(t-2:t-1)}_{\\text{ft-rel}}\\)\\mathcal{P}^{(t-2:t-1)}(t-2:t-1)_{\\text{ft-rel}}\\text{ft-rel} and inference prompt \\(\\mathcal{P}_{\\text{infer-rel}}\\)\\mathcal{P}_{\\text{infer-rel}}\\text{infer-rel} share the same template, with differences only in the specific content to be filled during inference: during fine-tuning, \\#Category1 in the template corresponds to \\(\\mathcal{C}'^{(t-2:t-1)}_{\\text{short}}\\)\\mathcal{C}'^{(t-2:t-1)}(t-2:t-1)_{\\text{short}}\\text{short} from \\(\\mathcal{P}^{(t-2:t-1)}_{\\text{ft-rel}}\\)\\mathcal{P}^{(t-2:t-1)}(t-2:t-1)_{\\text{ft-rel}}\\text{ft-rel}, and \\#Category2 corresponds to \\(c_{\\text{pos}}\\)c_{\\text{pos}}\\text{pos} or \\(c_{\\text{neg}}\\)c_{\\text{neg}}\\text{neg} from it; during inference, \\#Category1 in the template corresponds to \\(\\mathcal{C}'_{\\text{short}}\\)\\mathcal{C}'_{\\text{short}}\\text{short} from \\(\\mathcal{P}_{\\text{infer-rel}}\\)\\mathcal{P}_{\\text{infer-rel}}\\text{infer-rel}, and \\#Category2 corresponds to \\(C_{n_i}\\)C_{n_i}n_i from it. If \\(\\mathcal{C}'^{(t-2:t-1)}_{\\text{short}}\\)\\mathcal{C}'^{(t-2:t-1)}(t-2:t-1)_{\\text{short}}\\text{short} and \\(\\mathcal{C}'_{\\text{short}}\\)\\mathcal{C}'_{\\text{short}}\\text{short} have different lengths, the last two categories of \\(\\mathcal{C}'_{\\text{short}}\\)\\mathcal{C}'_{\\text{short}}\\text{short} should be selected to ensure consistent length."]
Prompt: You are given the following inputs for reconstructing a missing paragraph in a research paper.

Figure (optional):
Figure (optional):
- label: \label{fig:framework}; caption: \caption{  The \textit{Sel3DCraft} pipeline begins by expanding user text input into multiple candidates via LLM. A dual-branch pipeline then generates/retrieves corresponding multi-view images, displayed in an interactive satellite view fo
- label: \label{fig:sd_latents}; caption: \caption{The generated results at each timestep of the diffusion sampling process from $T$ to $1$. For example, given one prompt case ``A man with a beard wearing glasses in blue shirt'', the noise in the image is gradually reduced from tim
- label: None; caption: \caption{Llama2-7b}
- label: None; caption: \caption{Simply supported beam with distributed loading.}
- label: None; caption: \caption{Additional qualitative results for Edges$\rightarrow$Shoes$\times64$, where each pair of consecutive rows displaying the input image in the ``Edges'' domain and its translation in the ``Shoes'' domain, respectively.\label{fig:e2s_a
- label: \label{fig:juxtaposition}; caption: \caption{Performance on a commonsense benchmark (HellaSwag), a linguistic understanding benchmark (SuperGLUE), and the massive multitask test. On previous benchmarks, smaller models start well above random chance levels and exhibit more con
- label: \label{fig:sfig2}; caption: \caption{}
- label: \label{fig:rarl_curves}; caption: \caption{}
- label: \label{fig:combined_benchmarkgss-26-s100.cnf}; caption: \caption{gss-26-s100}
- label: \label{fig:diversity_stats_places}; caption: \caption{Statistics over locations}

Table (optional):
Table (optional):
- label: \label{tab:results}; text: 
- label: None; text: 
- label: \label{table:example_str}; text: \begin{tabular}{p{2.5cm}|p{12cm}@{}}     \toprule     Examples &  Help me answer question regarding spatial relationship in a 2D plane:  Given Information:      You will receive a series of object trajectory and the corresponding timestamps
- label: \label{tab:ED.compare}; text: \begin{tabular}{l c c c}          \toprule          \textbf{Method} & ($g_1^*$, $g_2^*$, $g_3^*$) \textbf{(MW/h)} & \textbf{Nominal cost (\$/h)}  \\          \midrule          ED & (144.3, 170.7, 0) & 926 \\          \PSCED{} & (110, 160, 4

Abstract(s) of cited paper(s):
(none)

2-Most Adjacent Paragraphs (context):
Previous:
1. \subsection{Relevant Details of the Novelty LLM}
\label{Appendix:novelty}
For \textit{Novelty LLM}, it is trained through supervised fine-tuning based on the Qwen2.5-7B-Instruct-1M model. Its prompt template is shown in Figure \ref{prompt_22}. Its fine-tuning prompt and inference prompt are \(\mathcal{P}^{(t-2:t-1)}_{\text{ft-nov}}\)\mathcal{P}^{(t-2:t-1)}(t-2:t-1)_{\text{ft-nov}}\text{ft-nov} and \(\mathcal{P}_{\text{infer-nov}}\)\mathcal{P}_{\text{infer-nov}}\text{infer-nov} respectively, and they share the same prompt template, with differences only in the specific content to be filled during inference: for fine-tuning, the content corresponding to \#Categories in the template is \(\mathcal{S}^{(t-2:t-1)}_{\text{short}}\)\mathcal{S}^{(t-2:t-1)}(t-2:t-1)_{\text{short}}\text{short} from \(\mathcal{P}^{(t-2:t-1)}_{\text{ft-nov}}\)\mathcal{P}^{(t-2:t-1)}(t-2:t-1)_{\text{ft-nov}}\text{ft-nov}; for inference, the content corresponding to \#Categories in the template is \(\mathcal{C}'_{\text{short}}\)\mathcal{C}'_{\text{short}}\text{short} from \(\mathcal{P}_{\text{infer-nov}}\)\mathcal{P}_{\text{infer-nov}}\text{infer-nov}. If their lengths are different, the last two categories of \(\mathcal{C}'_{\text{short}}\)\mathcal{C}'_{\text{short}}\text{short} are selected to ensure consistent length.
Next:
1. In the incremental fine-tuning of closed-loop optimization, we perform LoRA fine-tuning on \textit{Novelty LLM} using the DPO (Direct Preference Optimization) method, with specific configurations as follows: LoRA rank is 8, covering all layers, the preference value is set to 0.1, and the sigmoid loss function is adopted. Regarding training parameters, the single-device batch size is 2, the number of gradient accumulations is 8, and the learning rate is 5.0e-6.
2. \subsection{Relevant Details of the Relevance LLM}
\label{Appendix:relevance}
For \textit{Revelance LLM}, it is also based on the Qwen2.5-7B-Instruct-1M model, with fine-tuning configurations consistent with those of \textit{Novelty LLM}.
Its prompt template is shown in Figure \ref{prompt_33}. The fine-tuning prompt \(\mathcal{P}^{(t-2:t-1)}_{\text{ft-rel}}\)\mathcal{P}^{(t-2:t-1)}(t-2:t-1)_{\text{ft-rel}}\text{ft-rel} and inference prompt \(\mathcal{P}_{\text{infer-rel}}\)\mathcal{P}_{\text{infer-rel}}\text{infer-rel} share the same template, with differences only in the specific content to be filled during inference: during fine-tuning, \#Category1 in the template corresponds to \(\mathcal{C}'^{(t-2:t-1)}_{\text{short}}\)\mathcal{C}'^{(t-2:t-1)}(t-2:t-1)_{\text{short}}\text{short} from \(\mathcal{P}^{(t-2:t-1)}_{\text{ft-rel}}\)\mathcal{P}^{(t-2:t-1)}(t-2:t-1)_{\text{ft-rel}}\text{ft-rel}, and \#Category2 corresponds to \(c_{\text{pos}}\)c_{\text{pos}}\text{pos} or \(c_{\text{neg}}\)c_{\text{neg}}\text{neg} from it; during inference, \#Category1 in the template corresponds to \(\mathcal{C}'_{\text{short}}\)\mathcal{C}'_{\text{short}}\text{short} from \(\mathcal{P}_{\text{infer-rel}}\)\mathcal{P}_{\text{infer-rel}}\text{infer-rel}, and \#Category2 corresponds to \(C_{n_i}\)C_{n_i}n_i from it. If \(\mathcal{C}'^{(t-2:t-1)}_{\text{short}}\)\mathcal{C}'^{(t-2:t-1)}(t-2:t-1)_{\text{short}}\text{short} and \(\mathcal{C}'_{\text{short}}\)\mathcal{C}'_{\text{short}}\text{short} have different lengths, the last two categories of \(\mathcal{C}'_{\text{short}}\)\mathcal{C}'_{\text{short}}\text{short} should be selected to ensure consistent length.

# Task
Write exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.

# Requirements
- If a figure is provided, explicitly reference it with: Figure~\ref{{{figure_label} }}, and incorporate at least one concrete detail from the figure’s content or caption.
- If a table is provided, explicitly reference it with: Table~\ref{{{table_label} }}, and incorporate at least one concrete detail from the table’s content.
- Incorporate at least one core claim or finding from the abstract(s) and cite it with \citep{{{bib_key} }}. Use the provided BibTeX key(s) if present; otherwise, use a stable placeholder key derived from title (e.g., {derived_bib_key}).
- Ensure the paragraph logically continues from and sets up the surrounding 2 adjacent paragraph(s).
- Style: objective, concise, academic tone; ~120–180 words.
- Formatting: produce a single LaTeX paragraph only (no section headers, lists, environments; math only if essential).
- Constraints: do not include \label{...}; do not write “Figure X”/“Table Y”; do not copy raw table/figure content verbatim; summarize/interpret key points.

# Output
Return only the LaTeX paragraph text, nothing else.
Fetched figures: [{'id': 7852, 'label': '\\label{fig:framework}', 'caption': '\\caption{\n The \\textit{Sel3DCraft} pipeline begins by expanding user text input into multiple candidates via LLM. A dual-branch pipeline then generates/retrieves corresponding multi-view images, displayed in an interactive satellite view for 3D perception. Each candidate is automatically evaluated using eight-level semantic scores with heatmap visualization. For refinement, a score-guided recommendation mechanism suggests optimal keywords, enabling iterative optimization until user satisfaction is achieved.\n}', 'path': 'figs/Method.pdf'}, {'id': 60981, 'label': '\\label{fig:sd_latents}', 'caption': "\\caption{The generated results at each timestep of the diffusion sampling process from $T$ to $1$. For example, given one prompt case ``A man with a beard wearing glasses in blue shirt'', the noise in the image is gradually reduced from timestep $T$ to $1$, and the image is eventually generated with clarity (from left to right, top to bottom).}", 'path': 'app_imgs/sd_latents_compressed.pdf'}, {'id': 108193, 'label': None, 'caption': '\\caption{Llama2-7b}', 'path': 'figure/enumerate_answer/enumerate_answer_llama.pdf'}, {'id': 155405, 'label': None, 'caption': '\\caption{Simply supported beam with distributed loading.}', 'path': './figures/midCantileverBeam.png'}, {'id': 202617, 'label': None, 'caption': "\\caption{Additional qualitative results for Edges$\\rightarrow$Shoes$\\times64$,\nwhere each pair of consecutive rows displaying the input image in\nthe ``Edges'' domain and its translation in the ``Shoes'' domain,\nrespectively.\\label{fig:e2s_appdx}}", 'path': 'asset/Result/qualitative_result/more_viz/e2s_appdx_v2'}, {'id': 249829, 'label': '\\label{fig:juxtaposition}', 'caption': '\\caption{Performance on a commonsense benchmark (HellaSwag), a linguistic understanding benchmark (SuperGLUE), and the massive multitask test. On previous benchmarks, smaller models start well above random chance levels and exhibit more continuous improvements with model size increases, but on our test, GPT-3 moves beyond random chance with the largest model.}', 'path': 'figures/test_juxaposition.pdf'}, {'id': 297041, 'label': '\\label{fig:sfig2}', 'caption': '\\caption{}', 'path': 'figs/key_sample_vis_3_all.eps'}, {'id': 344253, 'label': '\\label{fig:rarl_curves}', 'caption': '\\caption{}', 'path': 'Figures/cartpole_baseline_curves.png'}, {'id': 391465, 'label': '\\label{fig:combined_benchmarkgss-26-s100.cnf}', 'caption': '\\caption{gss-26-s100}', 'path': 'figs/cnc/combined_benchmarkgss-26-s100.cnf.png'}, {'id': 438677, 'label': '\\label{fig:diversity_stats_places}', 'caption': '\\caption{Statistics over locations}', 'path': 'figures/schemes/location.pdf'}]
[{'id': 7858, 'label': '\\label{tab:results}', 'text': ''}, {'id': 60987, 'label': None, 'text': ''}, {'id': 108199, 'label': '\\label{table:example_str}', 'text': "\\begin{tabular}{p{2.5cm}|p{12cm}@{}}\n    \\toprule\n    Examples &\n\nHelp me answer question regarding spatial relationship in a 2D plane:\n\nGiven Information:\n\n    You will receive a series of object trajectory and the corresponding timestamps of the coordinates in the trajectory. You can treat the trajectory as linestring.\n\n    Sensor A: $[(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)]$\n\n    Timestamp: $[t1, t2, ..., tn]$\n\n    You will be provided with geometric information involving three types of 2D geometries—Point, LineString, and Polygon—all defined using the ESRI (Environmental Systems Research Institute) geometric format. These geometries are expressed as lists of coordinates in a Cartesian plane.\n\n    Point: A single coordinate location in space, defined as a tuple: \n    \n    $[(x, y)]$.\n\n    LineString: A sequence of points that forms a continuous line. It is represented as an ordered list of coordinate pairs: \n    \n    $[(x_1, y_1), (x_2, y2), ... (x_n, y_n)]$.\n\n   Polygon: A closed shape formed by a sequence of coordinate pairs where the first and last points are the same to close the loop:\n    \n    $[(x_1, y_1), (x_2, y2), ... (x_n, y_n), (x_1, y_1)]$.\n\n    Spatial Relationships (based on ArcGIS model) are as follows. During computation, you should account for floating-point precision with a numerical tolerance. For instance, 'Equals' should return True not just for exact mathematical identity, but if two geometries are identical within this tolerance.\n\n    ArcGIS defines spatial relationships as logical conditions between geometric objects:\n\n...\n\n    Temporal Relationships (based on Allen's interval algebra):\n\n...\n\nObjective:\n\nDetermine whether the time interval during which the EVENT holds has the temporal relationship **is\\_equal\\_to** with the reference interval (6.9569, 9.5423)?\nEVENT: the following object trajectory has the spatial relationship **intersects** with Polygon [(8.9995, 9.6963), (9.4793, 9.4669), (9.9179, 9.7677), (9.8767, 10.2979), (9.3969, 10.5273), (8.9583, 10.2265), (8.9995, 9.6963)]\n\nFor any interaction between a trajectory (you can view it as a LineString) and a fixed geometry—whether that geometry is another LineString, a Point, or a Polygon—define the “event interval” as follows:\n\n1. Pick a trajectory segment and a predicate.\n2. Project the segment endpoints back to their timestamps.  For each contiguous satisfying segment, let $t_1$ be the time at the segment’s first vertex and $t_2$ be the time at its last vertex.\n3. Define the event interval as the union of all **$[t_1, t_2]$** intervals in which the predicate is true.\n4. Do not include portions of the trajectory before the relationship begins or after it ends. Do not interpolate.\n\nAnswer 1 if answer is Yes. Otherwise, answer 0.\n\nObject trajectory: [(10.0000, 9.6138), (9.8171, 9.8075), (9.6491, 9.9374), (9.4950, 10.0000), (9.4381, 9.9971), (9.2219, 9.9145), (9.0990, 9.7653), (8.9822, 9.5463), (8.8692, 9.2601), (8.7575, 8.9104)]\nTimestamp: [1.9871, 3.0075, 4.0279, 5.0483, 6.0687, 7.0891, 8.1095, 9.1299, 10.1503, 11.1707]\n\n\nOutput Format:\n    Your response **must** include the answer (0 or 1) in the following format:\n\n    [RESULTS\\_START] [p] [RESULTS\\_END]\n\n    You may include explanatory text elsewhere in your response. However, do not include any text or additional formatting between [RESULTS\\_START] and [RESULTS\\_END].\n\nExample:\n\n    [RESULTS\\_START] [1] [RESULTS\\_END] \\\\\n\\midrule\nAnswer & [0] \\\\\n\\bottomrule\n  \\end{tabular}"}, {'id': 155411, 'label': '\\label{tab:ED.compare}', 'text': '\\begin{tabular}{l c c c}\n         \\toprule\n         \\textbf{Method} & ($g_1^*$, $g_2^*$, $g_3^*$) \\textbf{(MW/h)} & \\textbf{Nominal cost (\\$/h)}  \\\\\n         \\midrule\n         ED & (144.3, 170.7, 0) & 926 \\\\\n         \\PSCED{} & (110, 160, 45) & 1192 \\\\\n         \\CSCED{} & (119, 181, 15) & 962  \\\\\n         \\bottomrule \\\\\n    \\end{tabular}'}]
Fetched tables: [{'id': 7858, 'label': '\\label{tab:results}', 'text': ''}, {'id': 60987, 'label': None, 'text': ''}, {'id': 108199, 'label': '\\label{table:example_str}', 'text': "\\begin{tabular}{p{2.5cm}|p{12cm}@{}}\n    \\toprule\n    Examples &\n\nHelp me answer question regarding spatial relationship in a 2D plane:\n\nGiven Information:\n\n    You will receive a series of object trajectory and the corresponding timestamps of the coordinates in the trajectory. You can treat the trajectory as linestring.\n\n    Sensor A: $[(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)]$\n\n    Timestamp: $[t1, t2, ..., tn]$\n\n    You will be provided with geometric information involving three types of 2D geometries—Point, LineString, and Polygon—all defined using the ESRI (Environmental Systems Research Institute) geometric format. These geometries are expressed as lists of coordinates in a Cartesian plane.\n\n    Point: A single coordinate location in space, defined as a tuple: \n    \n    $[(x, y)]$.\n\n    LineString: A sequence of points that forms a continuous line. It is represented as an ordered list of coordinate pairs: \n    \n    $[(x_1, y_1), (x_2, y2), ... (x_n, y_n)]$.\n\n   Polygon: A closed shape formed by a sequence of coordinate pairs where the first and last points are the same to close the loop:\n    \n    $[(x_1, y_1), (x_2, y2), ... (x_n, y_n), (x_1, y_1)]$.\n\n    Spatial Relationships (based on ArcGIS model) are as follows. During computation, you should account for floating-point precision with a numerical tolerance. For instance, 'Equals' should return True not just for exact mathematical identity, but if two geometries are identical within this tolerance.\n\n    ArcGIS defines spatial relationships as logical conditions between geometric objects:\n\n...\n\n    Temporal Relationships (based on Allen's interval algebra):\n\n...\n\nObjective:\n\nDetermine whether the time interval during which the EVENT holds has the temporal relationship **is\\_equal\\_to** with the reference interval (6.9569, 9.5423)?\nEVENT: the following object trajectory has the spatial relationship **intersects** with Polygon [(8.9995, 9.6963), (9.4793, 9.4669), (9.9179, 9.7677), (9.8767, 10.2979), (9.3969, 10.5273), (8.9583, 10.2265), (8.9995, 9.6963)]\n\nFor any interaction between a trajectory (you can view it as a LineString) and a fixed geometry—whether that geometry is another LineString, a Point, or a Polygon—define the “event interval” as follows:\n\n1. Pick a trajectory segment and a predicate.\n2. Project the segment endpoints back to their timestamps.  For each contiguous satisfying segment, let $t_1$ be the time at the segment’s first vertex and $t_2$ be the time at its last vertex.\n3. Define the event interval as the union of all **$[t_1, t_2]$** intervals in which the predicate is true.\n4. Do not include portions of the trajectory before the relationship begins or after it ends. Do not interpolate.\n\nAnswer 1 if answer is Yes. Otherwise, answer 0.\n\nObject trajectory: [(10.0000, 9.6138), (9.8171, 9.8075), (9.6491, 9.9374), (9.4950, 10.0000), (9.4381, 9.9971), (9.2219, 9.9145), (9.0990, 9.7653), (8.9822, 9.5463), (8.8692, 9.2601), (8.7575, 8.9104)]\nTimestamp: [1.9871, 3.0075, 4.0279, 5.0483, 6.0687, 7.0891, 8.1095, 9.1299, 10.1503, 11.1707]\n\n\nOutput Format:\n    Your response **must** include the answer (0 or 1) in the following format:\n\n    [RESULTS\\_START] [p] [RESULTS\\_END]\n\n    You may include explanatory text elsewhere in your response. However, do not include any text or additional formatting between [RESULTS\\_START] and [RESULTS\\_END].\n\nExample:\n\n    [RESULTS\\_START] [1] [RESULTS\\_END] \\\\\n\\midrule\nAnswer & [0] \\\\\n\\bottomrule\n  \\end{tabular}"}, {'id': 155411, 'label': '\\label{tab:ED.compare}', 'text': '\\begin{tabular}{l c c c}\n         \\toprule\n         \\textbf{Method} & ($g_1^*$, $g_2^*$, $g_3^*$) \\textbf{(MW/h)} & \\textbf{Nominal cost (\\$/h)}  \\\\\n         \\midrule\n         ED & (144.3, 170.7, 0) & 926 \\\\\n         \\PSCED{} & (110, 160, 45) & 1192 \\\\\n         \\CSCED{} & (119, 181, 15) & 962  \\\\\n         \\bottomrule \\\\\n    \\end{tabular}'}]
Previous paragraphs: ["\\subsection{Relevant Details of the Novelty LLM}\n\\label{Appendix:novelty}\nFor \\textit{Novelty LLM}, it is trained through supervised fine-tuning based on the Qwen2.5-7B-Instruct-1M model. Its prompt template is shown in Figure \\ref{prompt_22}. Its fine-tuning prompt and inference prompt are \\(\\mathcal{P}^{(t-2:t-1)}_{\\text{ft-nov}}\\)\\mathcal{P}^{(t-2:t-1)}(t-2:t-1)_{\\text{ft-nov}}\\text{ft-nov} and \\(\\mathcal{P}_{\\text{infer-nov}}\\)\\mathcal{P}_{\\text{infer-nov}}\\text{infer-nov} respectively, and they share the same prompt template, with differences only in the specific content to be filled during inference: for fine-tuning, the content corresponding to \\#Categories in the template is \\(\\mathcal{S}^{(t-2:t-1)}_{\\text{short}}\\)\\mathcal{S}^{(t-2:t-1)}(t-2:t-1)_{\\text{short}}\\text{short} from \\(\\mathcal{P}^{(t-2:t-1)}_{\\text{ft-nov}}\\)\\mathcal{P}^{(t-2:t-1)}(t-2:t-1)_{\\text{ft-nov}}\\text{ft-nov}; for inference, the content corresponding to \\#Categories in the template is \\(\\mathcal{C}'_{\\text{short}}\\)\\mathcal{C}'_{\\text{short}}\\text{short} from \\(\\mathcal{P}_{\\text{infer-nov}}\\)\\mathcal{P}_{\\text{infer-nov}}\\text{infer-nov}. If their lengths are different, the last two categories of \\(\\mathcal{C}'_{\\text{short}}\\)\\mathcal{C}'_{\\text{short}}\\text{short} are selected to ensure consistent length."]
Next paragraphs: ['In the incremental fine-tuning of closed-loop optimization, we perform LoRA fine-tuning on \\textit{Novelty LLM} using the DPO (Direct Preference Optimization) method, with specific configurations as follows: LoRA rank is 8, covering all layers, the preference value is set to 0.1, and the sigmoid loss function is adopted. Regarding training parameters, the single-device batch size is 2, the number of gradient accumulations is 8, and the learning rate is 5.0e-6.', "\\subsection{Relevant Details of the Relevance LLM}\n\\label{Appendix:relevance}\nFor \\textit{Revelance LLM}, it is also based on the Qwen2.5-7B-Instruct-1M model, with fine-tuning configurations consistent with those of \\textit{Novelty LLM}.\nIts prompt template is shown in Figure \\ref{prompt_33}. The fine-tuning prompt \\(\\mathcal{P}^{(t-2:t-1)}_{\\text{ft-rel}}\\)\\mathcal{P}^{(t-2:t-1)}(t-2:t-1)_{\\text{ft-rel}}\\text{ft-rel} and inference prompt \\(\\mathcal{P}_{\\text{infer-rel}}\\)\\mathcal{P}_{\\text{infer-rel}}\\text{infer-rel} share the same template, with differences only in the specific content to be filled during inference: during fine-tuning, \\#Category1 in the template corresponds to \\(\\mathcal{C}'^{(t-2:t-1)}_{\\text{short}}\\)\\mathcal{C}'^{(t-2:t-1)}(t-2:t-1)_{\\text{short}}\\text{short} from \\(\\mathcal{P}^{(t-2:t-1)}_{\\text{ft-rel}}\\)\\mathcal{P}^{(t-2:t-1)}(t-2:t-1)_{\\text{ft-rel}}\\text{ft-rel}, and \\#Category2 corresponds to \\(c_{\\text{pos}}\\)c_{\\text{pos}}\\text{pos} or \\(c_{\\text{neg}}\\)c_{\\text{neg}}\\text{neg} from it; during inference, \\#Category1 in the template corresponds to \\(\\mathcal{C}'_{\\text{short}}\\)\\mathcal{C}'_{\\text{short}}\\text{short} from \\(\\mathcal{P}_{\\text{infer-rel}}\\)\\mathcal{P}_{\\text{infer-rel}}\\text{infer-rel}, and \\#Category2 corresponds to \\(C_{n_i}\\)C_{n_i}n_i from it. If \\(\\mathcal{C}'^{(t-2:t-1)}_{\\text{short}}\\)\\mathcal{C}'^{(t-2:t-1)}(t-2:t-1)_{\\text{short}}\\text{short} and \\(\\mathcal{C}'_{\\text{short}}\\)\\mathcal{C}'_{\\text{short}}\\text{short} have different lengths, the last two categories of \\(\\mathcal{C}'_{\\text{short}}\\)\\mathcal{C}'_{\\text{short}}\\text{short} should be selected to ensure consistent length."]
Prompt: You are given the following inputs for reconstructing a missing paragraph in a research paper.

Figure (optional):
Figure (optional):
- label: \label{fig:framework}; caption: \caption{  The \textit{Sel3DCraft} pipeline begins by expanding user text input into multiple candidates via LLM. A dual-branch pipeline then generates/retrieves corresponding multi-view images, displayed in an interactive satellite view fo
- label: \label{fig:sd_latents}; caption: \caption{The generated results at each timestep of the diffusion sampling process from $T$ to $1$. For example, given one prompt case ``A man with a beard wearing glasses in blue shirt'', the noise in the image is gradually reduced from tim
- label: None; caption: \caption{Llama2-7b}
- label: None; caption: \caption{Simply supported beam with distributed loading.}
- label: None; caption: \caption{Additional qualitative results for Edges$\rightarrow$Shoes$\times64$, where each pair of consecutive rows displaying the input image in the ``Edges'' domain and its translation in the ``Shoes'' domain, respectively.\label{fig:e2s_a
- label: \label{fig:juxtaposition}; caption: \caption{Performance on a commonsense benchmark (HellaSwag), a linguistic understanding benchmark (SuperGLUE), and the massive multitask test. On previous benchmarks, smaller models start well above random chance levels and exhibit more con
- label: \label{fig:sfig2}; caption: \caption{}
- label: \label{fig:rarl_curves}; caption: \caption{}
- label: \label{fig:combined_benchmarkgss-26-s100.cnf}; caption: \caption{gss-26-s100}
- label: \label{fig:diversity_stats_places}; caption: \caption{Statistics over locations}

Table (optional):
Table (optional):
- label: \label{tab:results}; text: 
- label: None; text: 
- label: \label{table:example_str}; text: \begin{tabular}{p{2.5cm}|p{12cm}@{}}     \toprule     Examples &  Help me answer question regarding spatial relationship in a 2D plane:  Given Information:      You will receive a series of object trajectory and the corresponding timestamps
- label: \label{tab:ED.compare}; text: \begin{tabular}{l c c c}          \toprule          \textbf{Method} & ($g_1^*$, $g_2^*$, $g_3^*$) \textbf{(MW/h)} & \textbf{Nominal cost (\$/h)}  \\          \midrule          ED & (144.3, 170.7, 0) & 926 \\          \PSCED{} & (110, 160, 4

Abstract(s) of cited paper(s):
(none)

2-Most Adjacent Paragraphs (context):
Previous:
1. \subsection{Relevant Details of the Novelty LLM}
\label{Appendix:novelty}
For \textit{Novelty LLM}, it is trained through supervised fine-tuning based on the Qwen2.5-7B-Instruct-1M model. Its prompt template is shown in Figure \ref{prompt_22}. Its fine-tuning prompt and inference prompt are \(\mathcal{P}^{(t-2:t-1)}_{\text{ft-nov}}\)\mathcal{P}^{(t-2:t-1)}(t-2:t-1)_{\text{ft-nov}}\text{ft-nov} and \(\mathcal{P}_{\text{infer-nov}}\)\mathcal{P}_{\text{infer-nov}}\text{infer-nov} respectively, and they share the same prompt template, with differences only in the specific content to be filled during inference: for fine-tuning, the content corresponding to \#Categories in the template is \(\mathcal{S}^{(t-2:t-1)}_{\text{short}}\)\mathcal{S}^{(t-2:t-1)}(t-2:t-1)_{\text{short}}\text{short} from \(\mathcal{P}^{(t-2:t-1)}_{\text{ft-nov}}\)\mathcal{P}^{(t-2:t-1)}(t-2:t-1)_{\text{ft-nov}}\text{ft-nov}; for inference, the content corresponding to \#Categories in the template is \(\mathcal{C}'_{\text{short}}\)\mathcal{C}'_{\text{short}}\text{short} from \(\mathcal{P}_{\text{infer-nov}}\)\mathcal{P}_{\text{infer-nov}}\text{infer-nov}. If their lengths are different, the last two categories of \(\mathcal{C}'_{\text{short}}\)\mathcal{C}'_{\text{short}}\text{short} are selected to ensure consistent length.
Next:
1. In the incremental fine-tuning of closed-loop optimization, we perform LoRA fine-tuning on \textit{Novelty LLM} using the DPO (Direct Preference Optimization) method, with specific configurations as follows: LoRA rank is 8, covering all layers, the preference value is set to 0.1, and the sigmoid loss function is adopted. Regarding training parameters, the single-device batch size is 2, the number of gradient accumulations is 8, and the learning rate is 5.0e-6.
2. \subsection{Relevant Details of the Relevance LLM}
\label{Appendix:relevance}
For \textit{Revelance LLM}, it is also based on the Qwen2.5-7B-Instruct-1M model, with fine-tuning configurations consistent with those of \textit{Novelty LLM}.
Its prompt template is shown in Figure \ref{prompt_33}. The fine-tuning prompt \(\mathcal{P}^{(t-2:t-1)}_{\text{ft-rel}}\)\mathcal{P}^{(t-2:t-1)}(t-2:t-1)_{\text{ft-rel}}\text{ft-rel} and inference prompt \(\mathcal{P}_{\text{infer-rel}}\)\mathcal{P}_{\text{infer-rel}}\text{infer-rel} share the same template, with differences only in the specific content to be filled during inference: during fine-tuning, \#Category1 in the template corresponds to \(\mathcal{C}'^{(t-2:t-1)}_{\text{short}}\)\mathcal{C}'^{(t-2:t-1)}(t-2:t-1)_{\text{short}}\text{short} from \(\mathcal{P}^{(t-2:t-1)}_{\text{ft-rel}}\)\mathcal{P}^{(t-2:t-1)}(t-2:t-1)_{\text{ft-rel}}\text{ft-rel}, and \#Category2 corresponds to \(c_{\text{pos}}\)c_{\text{pos}}\text{pos} or \(c_{\text{neg}}\)c_{\text{neg}}\text{neg} from it; during inference, \#Category1 in the template corresponds to \(\mathcal{C}'_{\text{short}}\)\mathcal{C}'_{\text{short}}\text{short} from \(\mathcal{P}_{\text{infer-rel}}\)\mathcal{P}_{\text{infer-rel}}\text{infer-rel}, and \#Category2 corresponds to \(C_{n_i}\)C_{n_i}n_i from it. If \(\mathcal{C}'^{(t-2:t-1)}_{\text{short}}\)\mathcal{C}'^{(t-2:t-1)}(t-2:t-1)_{\text{short}}\text{short} and \(\mathcal{C}'_{\text{short}}\)\mathcal{C}'_{\text{short}}\text{short} have different lengths, the last two categories of \(\mathcal{C}'_{\text{short}}\)\mathcal{C}'_{\text{short}}\text{short} should be selected to ensure consistent length.

# Task
Write exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.

# Requirements
- If a figure is provided, explicitly reference it with: Figure~\ref{{{figure_label} }}, and incorporate at least one concrete detail from the figure’s content or caption.
- If a table is provided, explicitly reference it with: Table~\ref{{{table_label} }}, and incorporate at least one concrete detail from the table’s content.
- Incorporate at least one core claim or finding from the abstract(s) and cite it with \citep{{{bib_key} }}. Use the provided BibTeX key(s) if present; otherwise, use a stable placeholder key derived from title (e.g., {derived_bib_key}).
- Ensure the paragraph logically continues from and sets up the surrounding 2 adjacent paragraph(s).
- Style: objective, concise, academic tone; ~120–180 words.
- Formatting: produce a single LaTeX paragraph only (no section headers, lists, environments; math only if essential).
- Constraints: do not include \label{...}; do not write “Figure X”/“Table Y”; do not copy raw table/figure content verbatim; summarize/interpret key points.

# Output
Return only the LaTeX paragraph text, nothing else.
