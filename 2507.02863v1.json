{"title": "Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory", "author": "Yuqi Wu", "abstract": "\\begin{abstract}\nDense 3D scene reconstruction from an ordered sequence or unordered image collections is a critical step when bringing research in computer vision into practical scenarios. \nFollowing the paradigm introduced by DUSt3R, which unifies an image pair densely into a shared coordinate system, subsequent methods maintain an implicit memory to achieve dense 3D reconstruction from more images. \nHowever, such implicit memory is limited in capacity and may suffer from information loss of earlier frames.\nWe propose Point3R, an online framework targeting dense streaming 3D reconstruction. \nTo be specific, we maintain an explicit spatial pointer memory directly associated with the 3D structure of the current scene. \nEach pointer in this memory is assigned a specific 3D position and aggregates scene information nearby in the global coordinate system into a changing spatial feature.\nInformation extracted from the latest frame interacts explicitly with this pointer memory, enabling dense integration of the current observation into the global coordinate system.\nWe design a 3D hierarchical position embedding to promote this interaction and design a simple yet effective fusion mechanism to ensure that our pointer memory is uniform and efficient.\nOur method achieves competitive or state-of-the-art performance on various tasks with low training costs.\nCode is available at: \\url{https://github.com/YkiWu/Point3R}.\n\\end{abstract}", "citations": {"huang2023tri": {"bib_key": "huang2023tri", "bib_title": "Tri-perspective view for vision-based 3d semantic occupancy prediction.", "bib_author ": "Yuanhui Huang, Wenzhao Zheng, Yunpeng Zhang, Jie Zhou, and Jiwen Lu.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "\\vspace{-3mm}Dense 3D reconstruction from image collections has long been a fundamental task in computer vision, with broad applications in fields such as autonomous driving~\\cite{huang2023tri, zheng2024occworld}", "next_context": ", medical modeling, and cultural heritage preservation."}], "importance_score": 0.5}, "zheng2024occworld": {"bib_key": "zheng2024occworld", "bib_title": "Occworld: Learning a 3d occupancy world model for autonomous driving.", "bib_author ": "Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Borui Zhang, Yueqi Duan, and Jiwen Lu.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "\\vspace{-3mm}Dense 3D reconstruction from image collections has long been a fundamental task in computer vision, with broad applications in fields such as autonomous driving~\\cite{huang2023tri, zheng2024occworld}", "next_context": ", medical modeling, and cultural heritage preservation."}], "importance_score": 0.5}, "sweeney2015optimizing": {"bib_key": "sweeney2015optimizing", "bib_title": "Optimizing the viewing graph for structure-from-motion.", "bib_author ": "Chris Sweeney, Torsten Sattler, Tobias Hollerer, Matthew Turk, and Marc Pollefeys.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Conventional approaches~\\cite{sweeney2015optimizing, agarwal2011building, crandall2012sfm, wu2013towards, schoenberger2016sfm, wilson2014robust}", "next_context": "first conduct a sequence of sub-tasks, including feature extraction and matching~\\cite{lowe1999object, oliensis2000critique}, triangulation, and global alignment to get sparse geometry and camera poses."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Structure-from-motion (SfM)~\\cite{agarwal2011building, crandall2012sfm, wu2013towards, wilson2014robust, schoenberger2016sfm, sweeney2015optimizing}", "next_context": "follows a pipeline consisting of feature extraction~\\cite{lowe2004distinctive, rublee2011orb, dusmanu2019d2}, image matching~\\cite{wu2013towards, lindenberger2023lightglue, shi2022clustergnn, chen2021learning}, triangulation to 3D, and bundle adjustment~\\cite{agarwal2010bundle, triggs1999bundle}to obtain sparse geometry and estimated camera poses."}], "importance_score": 0.3333333333333333}, "agarwal2011building": {"bib_key": "agarwal2011building", "bib_title": "Building rome in a day.", "bib_author ": "Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven~M Seitz, and Richard Szeliski.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Conventional approaches~\\cite{sweeney2015optimizing, agarwal2011building, crandall2012sfm, wu2013towards, schoenberger2016sfm, wilson2014robust}", "next_context": "first conduct a sequence of sub-tasks, including feature extraction and matching~\\cite{lowe1999object, oliensis2000critique}, triangulation, and global alignment to get sparse geometry and camera poses."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Structure-from-motion (SfM)~\\cite{agarwal2011building, crandall2012sfm, wu2013towards, wilson2014robust, schoenberger2016sfm, sweeney2015optimizing}", "next_context": "follows a pipeline consisting of feature extraction~\\cite{lowe2004distinctive, rublee2011orb, dusmanu2019d2}, image matching~\\cite{wu2013towards, lindenberger2023lightglue, shi2022clustergnn, chen2021learning}, triangulation to 3D, and bundle adjustment~\\cite{agarwal2010bundle, triggs1999bundle}to obtain sparse geometry and estimated camera poses."}], "importance_score": 0.3333333333333333}, "crandall2012sfm": {"bib_key": "crandall2012sfm", "bib_title": "Sfm with mrfs: Discrete-continuous optimization for large-scale structure from motion.", "bib_author ": "David~J Crandall, Andrew Owens, Noah Snavely, and Daniel~P Huttenlocher.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Conventional approaches~\\cite{sweeney2015optimizing, agarwal2011building, crandall2012sfm, wu2013towards, schoenberger2016sfm, wilson2014robust}", "next_context": "first conduct a sequence of sub-tasks, including feature extraction and matching~\\cite{lowe1999object, oliensis2000critique}, triangulation, and global alignment to get sparse geometry and camera poses."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Structure-from-motion (SfM)~\\cite{agarwal2011building, crandall2012sfm, wu2013towards, wilson2014robust, schoenberger2016sfm, sweeney2015optimizing}", "next_context": "follows a pipeline consisting of feature extraction~\\cite{lowe2004distinctive, rublee2011orb, dusmanu2019d2}, image matching~\\cite{wu2013towards, lindenberger2023lightglue, shi2022clustergnn, chen2021learning}, triangulation to 3D, and bundle adjustment~\\cite{agarwal2010bundle, triggs1999bundle}to obtain sparse geometry and estimated camera poses."}], "importance_score": 0.3333333333333333}, "wu2013towards": {"bib_key": "wu2013towards", "bib_title": "Towards linear-time incremental structure from motion.", "bib_author ": "Changchang Wu.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Conventional approaches~\\cite{sweeney2015optimizing, agarwal2011building, crandall2012sfm, wu2013towards, schoenberger2016sfm, wilson2014robust}", "next_context": "first conduct a sequence of sub-tasks, including feature extraction and matching~\\cite{lowe1999object, oliensis2000critique}, triangulation, and global alignment to get sparse geometry and camera poses."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Structure-from-motion (SfM)~\\cite{agarwal2011building, crandall2012sfm, wu2013towards, wilson2014robust, schoenberger2016sfm, sweeney2015optimizing}", "next_context": "follows a pipeline consisting of feature extraction~\\cite{lowe2004distinctive, rublee2011orb, dusmanu2019d2}, image matching~\\cite{wu2013towards, lindenberger2023lightglue, shi2022clustergnn, chen2021learning}, triangulation to 3D, and bundle adjustment~\\cite{agarwal2010bundle, triggs1999bundle}to obtain sparse geometry and estimated camera poses."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Structure-from-motion (SfM)~\\cite{agarwal2011building, crandall2012sfm, wu2013towards, wilson2014robust, schoenberger2016sfm, sweeney2015optimizing}follows a pipeline consisting of feature extraction~\\cite{lowe2004distinctive, rublee2011orb, dusmanu2019d2}, image matching~\\cite{wu2013towards, lindenberger2023lightglue, shi2022clustergnn, chen2021learning}", "next_context": ", triangulation to 3D, and bundle adjustment~\\cite{agarwal2010bundle, triggs1999bundle}to obtain sparse geometry and estimated camera poses."}], "importance_score": 0.5833333333333333}, "schoenberger2016sfm": {"bib_key": "schoenberger2016sfm", "bib_title": "o", "bib_author ": "Johannes~Lutz Sch\\\"", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Conventional approaches~\\cite{sweeney2015optimizing, agarwal2011building, crandall2012sfm, wu2013towards, schoenberger2016sfm, wilson2014robust}", "next_context": "first conduct a sequence of sub-tasks, including feature extraction and matching~\\cite{lowe1999object, oliensis2000critique}, triangulation, and global alignment to get sparse geometry and camera poses."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Structure-from-motion (SfM)~\\cite{agarwal2011building, crandall2012sfm, wu2013towards, wilson2014robust, schoenberger2016sfm, sweeney2015optimizing}", "next_context": "follows a pipeline consisting of feature extraction~\\cite{lowe2004distinctive, rublee2011orb, dusmanu2019d2}, image matching~\\cite{wu2013towards, lindenberger2023lightglue, shi2022clustergnn, chen2021learning}, triangulation to 3D, and bundle adjustment~\\cite{agarwal2010bundle, triggs1999bundle}to obtain sparse geometry and estimated camera poses."}], "importance_score": 0.3333333333333333}, "wilson2014robust": {"bib_key": "wilson2014robust", "bib_title": "Robust global translations with 1dsfm.", "bib_author ": "Kyle Wilson and Noah Snavely.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Conventional approaches~\\cite{sweeney2015optimizing, agarwal2011building, crandall2012sfm, wu2013towards, schoenberger2016sfm, wilson2014robust}", "next_context": "first conduct a sequence of sub-tasks, including feature extraction and matching~\\cite{lowe1999object, oliensis2000critique}, triangulation, and global alignment to get sparse geometry and camera poses."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Structure-from-motion (SfM)~\\cite{agarwal2011building, crandall2012sfm, wu2013towards, wilson2014robust, schoenberger2016sfm, sweeney2015optimizing}", "next_context": "follows a pipeline consisting of feature extraction~\\cite{lowe2004distinctive, rublee2011orb, dusmanu2019d2}, image matching~\\cite{wu2013towards, lindenberger2023lightglue, shi2022clustergnn, chen2021learning}, triangulation to 3D, and bundle adjustment~\\cite{agarwal2010bundle, triggs1999bundle}to obtain sparse geometry and estimated camera poses."}], "importance_score": 0.3333333333333333}, "lowe1999object": {"bib_key": "lowe1999object", "bib_title": "Object recognition from local scale-invariant features.", "bib_author ": "David~G Lowe.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Conventional approaches~\\cite{sweeney2015optimizing, agarwal2011building, crandall2012sfm, wu2013towards, schoenberger2016sfm, wilson2014robust}first conduct a sequence of sub-tasks, including feature extraction and matching~\\cite{lowe1999object, oliensis2000critique}", "next_context": ", triangulation, and global alignment to get sparse geometry and camera poses."}], "importance_score": 0.5}, "oliensis2000critique": {"bib_key": "oliensis2000critique", "bib_title": "A critique of structure-from-motion algorithms.", "bib_author ": "John Oliensis.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Conventional approaches~\\cite{sweeney2015optimizing, agarwal2011building, crandall2012sfm, wu2013towards, schoenberger2016sfm, wilson2014robust}first conduct a sequence of sub-tasks, including feature extraction and matching~\\cite{lowe1999object, oliensis2000critique}", "next_context": ", triangulation, and global alignment to get sparse geometry and camera poses."}], "importance_score": 0.5}, "furukawa2009accurate": {"bib_key": "furukawa2009accurate", "bib_title": "Accurate, dense, and robust multiview stereopsis.", "bib_author ": "Yasutaka Furukawa and Jean Ponce.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Multi-view stereo~\\cite{furukawa2009accurate, colmapmvs, gu2020cascade}", "next_context": "is then used to obtain dense geometry."}], "importance_score": 0.3333333333333333}, "colmapmvs": {"bib_key": "colmapmvs", "bib_title": "o", "bib_author ": "Johannes~Lutz Sch\\\"", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Multi-view stereo~\\cite{furukawa2009accurate, colmapmvs, gu2020cascade}", "next_context": "is then used to obtain dense geometry."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Building upon this, subsequent methods such as Multi-view Stereo (MVS)~\\cite{furukawa2015multi, colmapmvs, fu2022geo, Wei_2021_ICCV}", "next_context": ", Neural Radiance Fields (NeRF)~\\cite{mildenhall2021nerf, chen2022tensorf, muller2022instant, wang2021neus, barron2022mip}, and 3D Gaussian Splatting (3DGS)~\\cite{kerbl3Dgaussians}leverage known camera parameters to recover dense geometry or high-fidelity scene representation."}], "importance_score": 0.5833333333333333}, "gu2020cascade": {"bib_key": "gu2020cascade", "bib_title": "Cascade cost volume for high-resolution multi-view stereo and stereo matching.", "bib_author ": "Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "Multi-view stereo~\\cite{furukawa2009accurate, colmapmvs, gu2020cascade}", "next_context": "is then used to obtain dense geometry."}], "importance_score": 0.3333333333333333}, "wang2024dust3r": {"bib_key": "wang2024dust3r", "bib_title": "Dust3r: Geometric 3d vision made easy.", "bib_author ": "Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "To address these challenges, DUSt3R~\\cite{wang2024dust3r}", "next_context": "proposes a novel data-driven paradigm that directly aligns the input image pair and reconstructs it as point maps within a unified coordinate system."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "DUSt3R~\\cite{wang2024dust3r}", "next_context": "introduces a pointmap representation and directly learns to integrate an image pair into the same coordinate system, which unifies all sub-tasks we have mentioned above."}, {"section": "Proposed Approach", "subsection": "Memory-Based Streaming 3D Reconstruction", "subsubsection": null, "prev_context": "(1) pair-wise reconstruction~\\cite{wang2024dust3r, zhang2024monst3r, mast3r}", "next_context": "with an optimization-based global alignment, "}, {"section": "Proposed Approach", "subsection": "Pointer-Image Interaction", "subsubsection": null, "prev_context": "Then we use two intertwined decoders~\\cite{weinzaepfel2022croco, wang2024dust3r}", "next_context": "to enable interaction between the current image tokens and the memory:F'_0,z'_0"}, {"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "\\textbf{Implementation details.}We initialize our ViT-Large~\\cite{wang2024dust3r, dosovitskiy2021an}", "next_context": "image encoder, ViT-Base interaction decoders~\\cite{wang2024dust3r, weinzaepfel2022croco}, and DPT~\\cite{ranftl21dpt}heads with pre-trained weights from DUSt3R~\\cite{wang2024dust3r}."}, {"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "\\textbf{Implementation details.}We initialize our ViT-Large~\\cite{wang2024dust3r, dosovitskiy2021an}image encoder, ViT-Base interaction decoders~\\cite{wang2024dust3r, weinzaepfel2022croco}", "next_context": ", and DPT~\\cite{ranftl21dpt}heads with pre-trained weights from DUSt3R~\\cite{wang2024dust3r}."}, {"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "\\textbf{Implementation details.}We initialize our ViT-Large~\\cite{wang2024dust3r, dosovitskiy2021an}image encoder, ViT-Base interaction decoders~\\cite{wang2024dust3r, weinzaepfel2022croco}, and DPT~\\cite{ranftl21dpt}heads with pre-trained weights from DUSt3R~\\cite{wang2024dust3r}", "next_context": "."}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We choose DUSt3R~\\cite{wang2024dust3r}", "next_context": ", MASt3R~\\cite{mast3r}, MonST3R~\\cite{zhang2024monst3r}, Spann3R~\\cite{wang20243d}, and CUT3R~\\cite{cut3r}as our primary baselines."}, {"section": "Experiments", "subsection": "3D Reconstruction", "subsubsection": null, "prev_context": "~\\cite{wang2024dust3r, wang20243d, azinovic2022neural}", "next_context": "."}, {"section": "Experiments", "subsection": "3D Reconstruction", "subsubsection": null, "prev_context": "~\\ref{tab:3d_recon}, our method achieves comparable or better results than other memory-based online approaches or even DUSt3R-GA~\\cite{wang2024dust3r}", "next_context": "."}, {"section": "Experiments", "subsection": "3D Reconstruction", "subsubsection": null, "prev_context": "\\midruleDUSt3R-GA~\\cite{wang2024dust3r}", "next_context": ""}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "We adopt per-frame median scaling following DUSt3R~\\cite{wang2024dust3r}", "next_context": ", and the evaluation metrics we used include absolute relative error (Abs Rel) and percentage of inlier points\\delta<1.25."}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "\\midruleDUSt3R~\\cite{wang2024dust3r}", "next_context": ""}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "As shown in Table~\\ref{tab:videodepth}, with the per-sequence scale alignment, our method outperforms DUSt3R~\\cite{wang2024dust3r}", "next_context": ", MASt3R~\\cite{mast3r}, and Spann3R~\\cite{wang20243d}by a large margin."}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "DUSt3R-GA~\\cite{wang2024dust3r}", "next_context": ""}, {"section": "Experiments", "subsection": "Camera Pose Estimation", "subsubsection": null, "prev_context": "DUSt3R-GA~\\cite{wang2024dust3r}", "next_context": ""}], "importance_score": 13.166666666666668}, "yang2025fast3r": {"bib_key": "yang2025fast3r", "bib_title": "Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass.", "bib_author ": "Jianing Yang, Alexander Sax, Kevin~J Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "~\\cite{yang2025fast3r, wang2025vggtvisualgeometrygrounded, wang20243d, cut3r}", "next_context": "can be categorized into two main paradigms."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "One category of these works~\\cite{yang2025fast3r, wang2025vggtvisualgeometrygrounded}", "next_context": "takes all input images simultaneously and employs global attention to reconstruct them into a unified coordinate system, requiring substantial computational resources and is misaligned with the incremental nature of real-world reconstruction scenarios."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Subsequent works~\\cite{wang20243d, cut3r, yang2025fast3r, wang2025vggtvisualgeometrygrounded}", "next_context": "are exploring how to further replace the global optimization step with an end-to-end learning framework."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "The former~\\cite{yang2025fast3r, wang2025vggtvisualgeometrygrounded}", "next_context": "feeds all images simultaneously and leverages global attention to reconstruct the scene within a unified coordinate system; the latter~\\cite{wang20243d, cut3r}proposes a streaming paradigm, in which a memory module stores information from past frames, thereby enabling online incremental reconstruction from sequential inputs."}, {"section": "Proposed Approach", "subsection": "Memory-Based Streaming 3D Reconstruction", "subsubsection": null, "prev_context": "(2) one-shot global reconstruction~\\cite{yang2025fast3r, wang2025vggtvisualgeometrygrounded}", "next_context": "with all inputs, "}], "importance_score": 2.0}, "wang2025vggtvisualgeometrygrounded": {"bib_key": "wang2025vggtvisualgeometrygrounded", "bib_title": "Vggt: Visual geometry grounded transformer, 2025", "bib_author ": "Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "~\\cite{yang2025fast3r, wang2025vggtvisualgeometrygrounded, wang20243d, cut3r}", "next_context": "can be categorized into two main paradigms."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "One category of these works~\\cite{yang2025fast3r, wang2025vggtvisualgeometrygrounded}", "next_context": "takes all input images simultaneously and employs global attention to reconstruct them into a unified coordinate system, requiring substantial computational resources and is misaligned with the incremental nature of real-world reconstruction scenarios."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Subsequent works~\\cite{wang20243d, cut3r, yang2025fast3r, wang2025vggtvisualgeometrygrounded}", "next_context": "are exploring how to further replace the global optimization step with an end-to-end learning framework."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "The former~\\cite{yang2025fast3r, wang2025vggtvisualgeometrygrounded}", "next_context": "feeds all images simultaneously and leverages global attention to reconstruct the scene within a unified coordinate system; the latter~\\cite{wang20243d, cut3r}proposes a streaming paradigm, in which a memory module stores information from past frames, thereby enabling online incremental reconstruction from sequential inputs."}, {"section": "Proposed Approach", "subsection": "Memory-Based Streaming 3D Reconstruction", "subsubsection": null, "prev_context": "(2) one-shot global reconstruction~\\cite{yang2025fast3r, wang2025vggtvisualgeometrygrounded}", "next_context": "with all inputs, "}], "importance_score": 2.0}, "wang20243d": {"bib_key": "wang20243d", "bib_title": "3d reconstruction with spatial memory.", "bib_author ": "Hengyi Wang and Lourdes Agapito.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "~\\cite{yang2025fast3r, wang2025vggtvisualgeometrygrounded, wang20243d, cut3r}", "next_context": "can be categorized into two main paradigms."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "~\\cite{wang20243d, cut3r}", "next_context": "introduces an external memory mechanism to retain information from past frames, enabling each new input to be directly integrated into the global coordinate system."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "~\\cite{wang20243d}", "next_context": "maintains a memory that essentially caches implicit features of processed frames."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Subsequent works~\\cite{wang20243d, cut3r, yang2025fast3r, wang2025vggtvisualgeometrygrounded}", "next_context": "are exploring how to further replace the global optimization step with an end-to-end learning framework."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "The former~\\cite{yang2025fast3r, wang2025vggtvisualgeometrygrounded}feeds all images simultaneously and leverages global attention to reconstruct the scene within a unified coordinate system; the latter~\\cite{wang20243d, cut3r}", "next_context": "proposes a streaming paradigm, in which a memory module stores information from past frames, thereby enabling online incremental reconstruction from sequential inputs."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Existing streaming reconstruction methods~\\cite{engel2014lsd, choy20163d, zhang2022nerfusion, wang20243d, cut3r}", "next_context": "universally incorporate a certain form of memory to store information from past frames."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "This memory can take on various forms, such as explicit scene representation (the most direct form of memory), recurrent neural network architectures~\\cite{choy20163d}, and encoded or learnable token features~\\cite{wang20243d, cut3r}", "next_context": "."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Spann3R~\\cite{wang20243d}", "next_context": "stores implicit features from previous frames in the memory, which may lead to redundancy."}, {"section": "Proposed Approach", "subsection": "Memory-Based Streaming 3D Reconstruction", "subsubsection": null, "prev_context": "~\\cite{cut3r, wang20243d}", "next_context": "based on a memory mechanism."}, {"section": "Proposed Approach", "subsection": "Memory-Based Streaming 3D Reconstruction", "subsubsection": null, "prev_context": "To elaborate, in Spann3R~\\cite{wang20243d}", "next_context": ",\\mathcal{M}is a growing set of key-value-pair features, where the features are encoded from the output of each previous frame."}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We choose DUSt3R~\\cite{wang2024dust3r}, MASt3R~\\cite{mast3r}, MonST3R~\\cite{zhang2024monst3r}, Spann3R~\\cite{wang20243d}", "next_context": ", and CUT3R~\\cite{cut3r}as our primary baselines."}, {"section": "Experiments", "subsection": "3D Reconstruction", "subsubsection": null, "prev_context": "~\\cite{wang2024dust3r, wang20243d, azinovic2022neural}", "next_context": "."}, {"section": "Experiments", "subsection": "3D Reconstruction", "subsubsection": null, "prev_context": "We compare the reconstruction quality of our method with other memory-based online approaches, Spann3R~\\cite{wang20243d}", "next_context": "and CUT3R~\\cite{cut3r}in Figure~\\ref{fig:vis}, and our method achieves state-of-the-art reconstruction performance with sparse inputs from the 7-scenes and NRGBD datasets."}, {"section": "Experiments", "subsection": "3D Reconstruction", "subsubsection": null, "prev_context": "Spann3R~\\cite{wang20243d}", "next_context": ""}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "Spann3R~\\cite{wang20243d}", "next_context": ""}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "As shown in Table~\\ref{tab:videodepth}, with the per-sequence scale alignment, our method outperforms DUSt3R~\\cite{wang2024dust3r}, MASt3R~\\cite{mast3r}, and Spann3R~\\cite{wang20243d}", "next_context": "by a large margin."}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "Spann3R~\\cite{wang20243d}", "next_context": ""}, {"section": "Experiments", "subsection": "Camera Pose Estimation", "subsubsection": null, "prev_context": "Spann3R~\\cite{wang20243d}", "next_context": ""}], "importance_score": 13.033333333333333}, "cut3r": {"bib_key": "cut3r", "bib_title": "Continuous 3d perception model with persistent state, 2025", "bib_author ": "Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei~A. Efros, and Angjoo Kanazawa.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "~\\cite{yang2025fast3r, wang2025vggtvisualgeometrygrounded, wang20243d, cut3r}", "next_context": "can be categorized into two main paradigms."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "~\\cite{wang20243d, cut3r}", "next_context": "introduces an external memory mechanism to retain information from past frames, enabling each new input to be directly integrated into the global coordinate system."}, {"section": "Introduction", "subsection": null, "subsubsection": null, "prev_context": "CUT3R~\\cite{cut3r}", "next_context": "uses a fixed-length token-based memory mechanism and directly updates the memory through interactions with image features."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Subsequent works~\\cite{wang20243d, cut3r, yang2025fast3r, wang2025vggtvisualgeometrygrounded}", "next_context": "are exploring how to further replace the global optimization step with an end-to-end learning framework."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "The former~\\cite{yang2025fast3r, wang2025vggtvisualgeometrygrounded}feeds all images simultaneously and leverages global attention to reconstruct the scene within a unified coordinate system; the latter~\\cite{wang20243d, cut3r}", "next_context": "proposes a streaming paradigm, in which a memory module stores information from past frames, thereby enabling online incremental reconstruction from sequential inputs."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Existing streaming reconstruction methods~\\cite{engel2014lsd, choy20163d, zhang2022nerfusion, wang20243d, cut3r}", "next_context": "universally incorporate a certain form of memory to store information from past frames."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "This memory can take on various forms, such as explicit scene representation (the most direct form of memory), recurrent neural network architectures~\\cite{choy20163d}, and encoded or learnable token features~\\cite{wang20243d, cut3r}", "next_context": "."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "CUT3R~\\cite{cut3r}", "next_context": "employs a fixed-length set of learnable token features as its memory module, which is continuously updated during sequential processing."}, {"section": "Proposed Approach", "subsection": "Memory-Based Streaming 3D Reconstruction", "subsubsection": null, "prev_context": "~\\cite{cut3r, wang20243d}", "next_context": "based on a memory mechanism."}, {"section": "Proposed Approach", "subsection": "Memory-Based Streaming 3D Reconstruction", "subsubsection": null, "prev_context": "In CUT3R~\\cite{cut3r}", "next_context": ",\\mathcal{M}takes the form of a fixed-length feature sequence that is iteratively updated as new frame comes."}, {"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "\\textbf{Training objective.}Following MASt3R~\\cite{mast3r}and CUT3R~\\cite{cut3r}", "next_context": ", we use the L2 norm loss for the poses and a confidence-aware loss for the pointmaps."}, {"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "~\\cite{cut3r}", "next_context": "."}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We choose DUSt3R~\\cite{wang2024dust3r}, MASt3R~\\cite{mast3r}, MonST3R~\\cite{zhang2024monst3r}, Spann3R~\\cite{wang20243d}, and CUT3R~\\cite{cut3r}", "next_context": "as our primary baselines."}, {"section": "Experiments", "subsection": "3D Reconstruction", "subsubsection": null, "prev_context": "~\\cite{cut3r}", "next_context": ": 3 to 5 frames per scene for the 7-scenes datasets and 2 to 4 frames per scene for the NRGBD dataset."}, {"section": "Experiments", "subsection": "3D Reconstruction", "subsubsection": null, "prev_context": "We compare the reconstruction quality of our method with other memory-based online approaches, Spann3R~\\cite{wang20243d}and CUT3R~\\cite{cut3r}", "next_context": "in Figure~\\ref{fig:vis}, and our method achieves state-of-the-art reconstruction performance with sparse inputs from the 7-scenes and NRGBD datasets."}, {"section": "Experiments", "subsection": "3D Reconstruction", "subsubsection": null, "prev_context": "CUT3R~\\cite{cut3r}", "next_context": ""}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "\\textbf{Monocular Depth Estimation.}We evaluate zero-shot monocular depth estimation~\\cite{zhang2024monst3r, cut3r}", "next_context": "performance on NYU-v2~\\cite{silberman2012indoor}(static), Sintel~\\cite{butler2012naturalistic}, Bonn~\\cite{palazzolo2019refusion}, and KITTI~\\cite{geiger2013vision}datasets."}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "CUT3R~\\cite{cut3r}", "next_context": ""}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "We also compare results without alignment with other metric pointmap methods like MASt3R~\\cite{mast3r}and CUT3R~\\cite{cut3r}", "next_context": "(Metric-scale alignment)."}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "Our method performs comparably, or even better than MonST3R-GA~\\cite{zhang2024monst3r}and CUT3R~\\cite{cut3r}", "next_context": "(methods trained on dynamic datasets)."}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "Besides, in the metric-scale setting, our method outperforms MASt3R-GA~\\cite{mast3r}and performs comparably with CUT3R~\\cite{cut3r}", "next_context": ", leading on Sintel and Bonn."}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "CUT3R~\\cite{cut3r}", "next_context": ""}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "CUT3R~\\cite{cut3r}", "next_context": ""}, {"section": "Experiments", "subsection": "Camera Pose Estimation", "subsubsection": null, "prev_context": "We evaluate the camera pose estimation performance on ScanNet~\\cite{dai2017scannet}(static), Sintel~\\cite{butler2012naturalistic}, and TUM-dynamics~\\cite{sturm2012benchmark}datasets following MonST3R~\\cite{zhang2024monst3r}and CUT3R~\\cite{cut3r}", "next_context": "."}, {"section": "Experiments", "subsection": "Camera Pose Estimation", "subsubsection": null, "prev_context": "CUT3R~\\cite{cut3r}", "next_context": ""}, {"section": "More Implementation and Training Details", "subsection": null, "subsubsection": null, "prev_context": "In the forward pass of\\mathrm{Head}_global, we first generate the pose-modulated tokens using an additional modulation function in CUT3R~\\cite{cut3r}", "next_context": ","}], "importance_score": 21.2}, "lowe2004distinctive": {"bib_key": "lowe2004distinctive", "bib_title": "Distinctive image features from scale-invariant keypoints.", "bib_author ": "David~G Lowe.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Structure-from-motion (SfM)~\\cite{agarwal2011building, crandall2012sfm, wu2013towards, wilson2014robust, schoenberger2016sfm, sweeney2015optimizing}follows a pipeline consisting of feature extraction~\\cite{lowe2004distinctive, rublee2011orb, dusmanu2019d2}", "next_context": ", image matching~\\cite{wu2013towards, lindenberger2023lightglue, shi2022clustergnn, chen2021learning}, triangulation to 3D, and bundle adjustment~\\cite{agarwal2010bundle, triggs1999bundle}to obtain sparse geometry and estimated camera poses."}], "importance_score": 0.3333333333333333}, "rublee2011orb": {"bib_key": "rublee2011orb", "bib_title": "Orb: An efficient alternative to sift or surf.", "bib_author ": "Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Structure-from-motion (SfM)~\\cite{agarwal2011building, crandall2012sfm, wu2013towards, wilson2014robust, schoenberger2016sfm, sweeney2015optimizing}follows a pipeline consisting of feature extraction~\\cite{lowe2004distinctive, rublee2011orb, dusmanu2019d2}", "next_context": ", image matching~\\cite{wu2013towards, lindenberger2023lightglue, shi2022clustergnn, chen2021learning}, triangulation to 3D, and bundle adjustment~\\cite{agarwal2010bundle, triggs1999bundle}to obtain sparse geometry and estimated camera poses."}], "importance_score": 0.3333333333333333}, "dusmanu2019d2": {"bib_key": "dusmanu2019d2", "bib_title": "D2-net: A trainable cnn for joint description and detection of local features.", "bib_author ": "Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Pollefeys, Josef Sivic, Akihiko Torii, and Torsten Sattler.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Structure-from-motion (SfM)~\\cite{agarwal2011building, crandall2012sfm, wu2013towards, wilson2014robust, schoenberger2016sfm, sweeney2015optimizing}follows a pipeline consisting of feature extraction~\\cite{lowe2004distinctive, rublee2011orb, dusmanu2019d2}", "next_context": ", image matching~\\cite{wu2013towards, lindenberger2023lightglue, shi2022clustergnn, chen2021learning}, triangulation to 3D, and bundle adjustment~\\cite{agarwal2010bundle, triggs1999bundle}to obtain sparse geometry and estimated camera poses."}], "importance_score": 0.3333333333333333}, "lindenberger2023lightglue": {"bib_key": "lindenberger2023lightglue", "bib_title": "Lightglue: Local feature matching at light speed.", "bib_author ": "Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Pollefeys.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Structure-from-motion (SfM)~\\cite{agarwal2011building, crandall2012sfm, wu2013towards, wilson2014robust, schoenberger2016sfm, sweeney2015optimizing}follows a pipeline consisting of feature extraction~\\cite{lowe2004distinctive, rublee2011orb, dusmanu2019d2}, image matching~\\cite{wu2013towards, lindenberger2023lightglue, shi2022clustergnn, chen2021learning}", "next_context": ", triangulation to 3D, and bundle adjustment~\\cite{agarwal2010bundle, triggs1999bundle}to obtain sparse geometry and estimated camera poses."}], "importance_score": 0.25}, "shi2022clustergnn": {"bib_key": "shi2022clustergnn", "bib_title": "Clustergnn: Cluster-based coarse-to-fine graph neural network for efficient feature matching.", "bib_author ": "Yan Shi, Jun-Xiong Cai, Yoli Shavit, Tai-Jiang Mu, Wensen Feng, and Kai Zhang.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Structure-from-motion (SfM)~\\cite{agarwal2011building, crandall2012sfm, wu2013towards, wilson2014robust, schoenberger2016sfm, sweeney2015optimizing}follows a pipeline consisting of feature extraction~\\cite{lowe2004distinctive, rublee2011orb, dusmanu2019d2}, image matching~\\cite{wu2013towards, lindenberger2023lightglue, shi2022clustergnn, chen2021learning}", "next_context": ", triangulation to 3D, and bundle adjustment~\\cite{agarwal2010bundle, triggs1999bundle}to obtain sparse geometry and estimated camera poses."}], "importance_score": 0.25}, "chen2021learning": {"bib_key": "chen2021learning", "bib_title": "Learning to match features with seeded graph matching network.", "bib_author ": "Hongkai Chen, Zixin Luo, Jiahui Zhang, Lei Zhou, Xuyang Bai, Zeyu Hu, Chiew-Lan Tai, and Long Quan.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Structure-from-motion (SfM)~\\cite{agarwal2011building, crandall2012sfm, wu2013towards, wilson2014robust, schoenberger2016sfm, sweeney2015optimizing}follows a pipeline consisting of feature extraction~\\cite{lowe2004distinctive, rublee2011orb, dusmanu2019d2}, image matching~\\cite{wu2013towards, lindenberger2023lightglue, shi2022clustergnn, chen2021learning}", "next_context": ", triangulation to 3D, and bundle adjustment~\\cite{agarwal2010bundle, triggs1999bundle}to obtain sparse geometry and estimated camera poses."}], "importance_score": 0.25}, "agarwal2010bundle": {"bib_key": "agarwal2010bundle", "bib_title": "Bundle adjustment in the large.", "bib_author ": "Sameer Agarwal, Noah Snavely, Steven~M Seitz, and Richard Szeliski.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Structure-from-motion (SfM)~\\cite{agarwal2011building, crandall2012sfm, wu2013towards, wilson2014robust, schoenberger2016sfm, sweeney2015optimizing}follows a pipeline consisting of feature extraction~\\cite{lowe2004distinctive, rublee2011orb, dusmanu2019d2}, image matching~\\cite{wu2013towards, lindenberger2023lightglue, shi2022clustergnn, chen2021learning}, triangulation to 3D, and bundle adjustment~\\cite{agarwal2010bundle, triggs1999bundle}", "next_context": "to obtain sparse geometry and estimated camera poses."}], "importance_score": 0.5}, "triggs1999bundle": {"bib_key": "triggs1999bundle", "bib_title": "Bundle adjustment\u2014a modern synthesis.", "bib_author ": "Bill Triggs, Philip~F McLauchlan, Richard~I Hartley, and Andrew~W Fitzgibbon.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Structure-from-motion (SfM)~\\cite{agarwal2011building, crandall2012sfm, wu2013towards, wilson2014robust, schoenberger2016sfm, sweeney2015optimizing}follows a pipeline consisting of feature extraction~\\cite{lowe2004distinctive, rublee2011orb, dusmanu2019d2}, image matching~\\cite{wu2013towards, lindenberger2023lightglue, shi2022clustergnn, chen2021learning}, triangulation to 3D, and bundle adjustment~\\cite{agarwal2010bundle, triggs1999bundle}", "next_context": "to obtain sparse geometry and estimated camera poses."}], "importance_score": 0.5}, "furukawa2015multi": {"bib_key": "furukawa2015multi", "bib_title": "\\'a", "bib_author ": "Yasutaka Furukawa, Carlos Hern", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Building upon this, subsequent methods such as Multi-view Stereo (MVS)~\\cite{furukawa2015multi, colmapmvs, fu2022geo, Wei_2021_ICCV}", "next_context": ", Neural Radiance Fields (NeRF)~\\cite{mildenhall2021nerf, chen2022tensorf, muller2022instant, wang2021neus, barron2022mip}, and 3D Gaussian Splatting (3DGS)~\\cite{kerbl3Dgaussians}leverage known camera parameters to recover dense geometry or high-fidelity scene representation."}], "importance_score": 0.25}, "fu2022geo": {"bib_key": "fu2022geo", "bib_title": "Geo-neus: Geometry-consistent neural implicit surfaces learning for multi-view reconstruction.", "bib_author ": "Qiancheng Fu, Qingshan Xu, Yew~Soon Ong, and Wenbing Tao.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Building upon this, subsequent methods such as Multi-view Stereo (MVS)~\\cite{furukawa2015multi, colmapmvs, fu2022geo, Wei_2021_ICCV}", "next_context": ", Neural Radiance Fields (NeRF)~\\cite{mildenhall2021nerf, chen2022tensorf, muller2022instant, wang2021neus, barron2022mip}, and 3D Gaussian Splatting (3DGS)~\\cite{kerbl3Dgaussians}leverage known camera parameters to recover dense geometry or high-fidelity scene representation."}], "importance_score": 0.25}, "Wei_2021_ICCV": {"bib_key": "Wei_2021_ICCV", "bib_title": "Nerfingmvs: Guided optimization of neural radiance fields for indoor multi-view stereo.", "bib_author ": "Yi~Wei, Shaohui Liu, Yongming Rao, Wang Zhao, Jiwen Lu, and Jie Zhou.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Building upon this, subsequent methods such as Multi-view Stereo (MVS)~\\cite{furukawa2015multi, colmapmvs, fu2022geo, Wei_2021_ICCV}", "next_context": ", Neural Radiance Fields (NeRF)~\\cite{mildenhall2021nerf, chen2022tensorf, muller2022instant, wang2021neus, barron2022mip}, and 3D Gaussian Splatting (3DGS)~\\cite{kerbl3Dgaussians}leverage known camera parameters to recover dense geometry or high-fidelity scene representation."}], "importance_score": 0.25}, "mildenhall2021nerf": {"bib_key": "mildenhall2021nerf", "bib_title": "Nerf: Representing scenes as neural radiance fields for view synthesis.", "bib_author ": "Ben Mildenhall, Pratul~P Srinivasan, Matthew Tancik, Jonathan~T Barron, Ravi Ramamoorthi, and Ren Ng.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Building upon this, subsequent methods such as Multi-view Stereo (MVS)~\\cite{furukawa2015multi, colmapmvs, fu2022geo, Wei_2021_ICCV}, Neural Radiance Fields (NeRF)~\\cite{mildenhall2021nerf, chen2022tensorf, muller2022instant, wang2021neus, barron2022mip}", "next_context": ", and 3D Gaussian Splatting (3DGS)~\\cite{kerbl3Dgaussians}leverage known camera parameters to recover dense geometry or high-fidelity scene representation."}], "importance_score": 0.2}, "chen2022tensorf": {"bib_key": "chen2022tensorf", "bib_title": "Tensorf: Tensorial radiance fields.", "bib_author ": "Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Building upon this, subsequent methods such as Multi-view Stereo (MVS)~\\cite{furukawa2015multi, colmapmvs, fu2022geo, Wei_2021_ICCV}, Neural Radiance Fields (NeRF)~\\cite{mildenhall2021nerf, chen2022tensorf, muller2022instant, wang2021neus, barron2022mip}", "next_context": ", and 3D Gaussian Splatting (3DGS)~\\cite{kerbl3Dgaussians}leverage known camera parameters to recover dense geometry or high-fidelity scene representation."}], "importance_score": 0.2}, "muller2022instant": {"bib_key": "muller2022instant", "bib_title": "\\\"u", "bib_author ": "Thomas M", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Building upon this, subsequent methods such as Multi-view Stereo (MVS)~\\cite{furukawa2015multi, colmapmvs, fu2022geo, Wei_2021_ICCV}, Neural Radiance Fields (NeRF)~\\cite{mildenhall2021nerf, chen2022tensorf, muller2022instant, wang2021neus, barron2022mip}", "next_context": ", and 3D Gaussian Splatting (3DGS)~\\cite{kerbl3Dgaussians}leverage known camera parameters to recover dense geometry or high-fidelity scene representation."}], "importance_score": 0.2}, "wang2021neus": {"bib_key": "wang2021neus", "bib_title": "Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction.", "bib_author ": "Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Building upon this, subsequent methods such as Multi-view Stereo (MVS)~\\cite{furukawa2015multi, colmapmvs, fu2022geo, Wei_2021_ICCV}, Neural Radiance Fields (NeRF)~\\cite{mildenhall2021nerf, chen2022tensorf, muller2022instant, wang2021neus, barron2022mip}", "next_context": ", and 3D Gaussian Splatting (3DGS)~\\cite{kerbl3Dgaussians}leverage known camera parameters to recover dense geometry or high-fidelity scene representation."}], "importance_score": 0.2}, "barron2022mip": {"bib_key": "barron2022mip", "bib_title": "Mip-nerf 360: Unbounded anti-aliased neural radiance fields.", "bib_author ": "Jonathan~T Barron, Ben Mildenhall, Dor Verbin, Pratul~P Srinivasan, and Peter Hedman.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Building upon this, subsequent methods such as Multi-view Stereo (MVS)~\\cite{furukawa2015multi, colmapmvs, fu2022geo, Wei_2021_ICCV}, Neural Radiance Fields (NeRF)~\\cite{mildenhall2021nerf, chen2022tensorf, muller2022instant, wang2021neus, barron2022mip}", "next_context": ", and 3D Gaussian Splatting (3DGS)~\\cite{kerbl3Dgaussians}leverage known camera parameters to recover dense geometry or high-fidelity scene representation."}], "importance_score": 0.2}, "kerbl3Dgaussians": {"bib_key": "kerbl3Dgaussians", "bib_title": "\\\"u", "bib_author ": "Bernhard Kerbl, Georgios Kopanas, Thomas Leimk", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Building upon this, subsequent methods such as Multi-view Stereo (MVS)~\\cite{furukawa2015multi, colmapmvs, fu2022geo, Wei_2021_ICCV}, Neural Radiance Fields (NeRF)~\\cite{mildenhall2021nerf, chen2022tensorf, muller2022instant, wang2021neus, barron2022mip}, and 3D Gaussian Splatting (3DGS)~\\cite{kerbl3Dgaussians}", "next_context": "leverage known camera parameters to recover dense geometry or high-fidelity scene representation."}], "importance_score": 1.0}, "davison2007monoslam": {"bib_key": "davison2007monoslam", "bib_title": "Monoslam: Real-time single camera slam.", "bib_author ": "Andrew~J Davison, Ian~D Reid, Nicholas~D Molton, and Olivier Stasse.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "It is worth noting that Simultaneous Localization and Mapping (SLAM)~\\cite{davison2007monoslam, engel2014lsd, newcombe2011dtam, klein2007parallel}", "next_context": "can perform localization and reconstruction in an online manner."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "However, they often rely on specific camera motion assumptions~\\cite{newcombe2011dtam, davison2007monoslam}", "next_context": "(sometimes these motion assumptions may be misleading or restrictive) or require additional depth/LiDAR sensors~\\cite{newcombe2011kinectfusion}for better performance."}], "importance_score": 0.75}, "engel2014lsd": {"bib_key": "engel2014lsd", "bib_title": "\\\"o", "bib_author ": "Jakob Engel, Thomas Sch", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "It is worth noting that Simultaneous Localization and Mapping (SLAM)~\\cite{davison2007monoslam, engel2014lsd, newcombe2011dtam, klein2007parallel}", "next_context": "can perform localization and reconstruction in an online manner."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Existing streaming reconstruction methods~\\cite{engel2014lsd, choy20163d, zhang2022nerfusion, wang20243d, cut3r}", "next_context": "universally incorporate a certain form of memory to store information from past frames."}], "importance_score": 0.45}, "newcombe2011dtam": {"bib_key": "newcombe2011dtam", "bib_title": "Dtam: Dense tracking and mapping in real-time.", "bib_author ": "Richard~A Newcombe, Steven~J Lovegrove, and Andrew~J Davison.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "It is worth noting that Simultaneous Localization and Mapping (SLAM)~\\cite{davison2007monoslam, engel2014lsd, newcombe2011dtam, klein2007parallel}", "next_context": "can perform localization and reconstruction in an online manner."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "However, they often rely on specific camera motion assumptions~\\cite{newcombe2011dtam, davison2007monoslam}", "next_context": "(sometimes these motion assumptions may be misleading or restrictive) or require additional depth/LiDAR sensors~\\cite{newcombe2011kinectfusion}for better performance."}], "importance_score": 0.75}, "klein2007parallel": {"bib_key": "klein2007parallel", "bib_title": "Parallel tracking and mapping for small ar workspaces.", "bib_author ": "Georg Klein and David Murray.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "It is worth noting that Simultaneous Localization and Mapping (SLAM)~\\cite{davison2007monoslam, engel2014lsd, newcombe2011dtam, klein2007parallel}", "next_context": "can perform localization and reconstruction in an online manner."}], "importance_score": 0.25}, "newcombe2011kinectfusion": {"bib_key": "newcombe2011kinectfusion", "bib_title": "Kinectfusion: Real-time dense surface mapping and tracking.", "bib_author ": "Richard~A Newcombe, Shahram Izadi, Otmar Hilliges, David Molyneaux, David Kim, Andrew~J Davison, Pushmeet Kohi, Jamie Shotton, Steve Hodges, and Andrew Fitzgibbon.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "However, they often rely on specific camera motion assumptions~\\cite{newcombe2011dtam, davison2007monoslam}(sometimes these motion assumptions may be misleading or restrictive) or require additional depth/LiDAR sensors~\\cite{newcombe2011kinectfusion}", "next_context": "for better performance."}], "importance_score": 1.0}, "detone2018self": {"bib_key": "detone2018self", "bib_title": "Superpoint: Self-supervised interest point detection and description.", "bib_author ": "Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Some approaches utilize learnable modules to replace hand-crafted components~\\cite{detone2018self, sarlin2020superglue}", "next_context": "during the traditional reconstruction pipeline."}], "importance_score": 0.5}, "sarlin2020superglue": {"bib_key": "sarlin2020superglue", "bib_title": "Superglue: Learning feature matching with graph neural networks.", "bib_author ": "Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Some approaches utilize learnable modules to replace hand-crafted components~\\cite{detone2018self, sarlin2020superglue}", "next_context": "during the traditional reconstruction pipeline."}], "importance_score": 0.5}, "tang2018ba": {"bib_key": "tang2018ba", "bib_title": "Ba-net: Dense bundle adjustment network.", "bib_author ": "Chengzhou Tang and Ping Tan.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Some others try to optimize the overall pipeline in an end-to-end manner~\\cite{tang2018ba, wang2024vggsfm, yao2018mvsnet}", "next_context": "."}], "importance_score": 0.3333333333333333}, "wang2024vggsfm": {"bib_key": "wang2024vggsfm", "bib_title": "Vggsfm: Visual geometry grounded deep structure from motion.", "bib_author ": "Jianyuan Wang, Nikita Karaev, Christian Rupprecht, and David Novotny.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Some others try to optimize the overall pipeline in an end-to-end manner~\\cite{tang2018ba, wang2024vggsfm, yao2018mvsnet}", "next_context": "."}], "importance_score": 0.3333333333333333}, "yao2018mvsnet": {"bib_key": "yao2018mvsnet", "bib_title": "Mvsnet: Depth inference for unstructured multi-view stereo.", "bib_author ": "Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Some others try to optimize the overall pipeline in an end-to-end manner~\\cite{tang2018ba, wang2024vggsfm, yao2018mvsnet}", "next_context": "."}], "importance_score": 0.3333333333333333}, "zhang2024monst3r": {"bib_key": "zhang2024monst3r", "bib_title": "Monst3r: A simple approach for estimating geometry in the presence of motion.", "bib_author ": "Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "MonST3R~\\cite{zhang2024monst3r}", "next_context": "extends this paradigm to dynamic scenes by fine-tuning it on dynamic datasets."}, {"section": "Proposed Approach", "subsection": "Memory-Based Streaming 3D Reconstruction", "subsubsection": null, "prev_context": "(1) pair-wise reconstruction~\\cite{wang2024dust3r, zhang2024monst3r, mast3r}", "next_context": "with an optimization-based global alignment, "}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We choose DUSt3R~\\cite{wang2024dust3r}, MASt3R~\\cite{mast3r}, MonST3R~\\cite{zhang2024monst3r}", "next_context": ", Spann3R~\\cite{wang20243d}, and CUT3R~\\cite{cut3r}as our primary baselines."}, {"section": "Experiments", "subsection": "3D Reconstruction", "subsubsection": null, "prev_context": "MonST3R-GA~\\cite{zhang2024monst3r}", "next_context": ""}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "\\textbf{Monocular Depth Estimation.}We evaluate zero-shot monocular depth estimation~\\cite{zhang2024monst3r, cut3r}", "next_context": "performance on NYU-v2~\\cite{silberman2012indoor}(static), Sintel~\\cite{butler2012naturalistic}, Bonn~\\cite{palazzolo2019refusion}, and KITTI~\\cite{geiger2013vision}datasets."}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "MonST3R~\\cite{zhang2024monst3r}", "next_context": ""}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "Our method performs comparably, or even better than MonST3R-GA~\\cite{zhang2024monst3r}", "next_context": "and CUT3R~\\cite{cut3r}(methods trained on dynamic datasets)."}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "MonST3R-GA~\\cite{zhang2024monst3r}", "next_context": ""}, {"section": "Experiments", "subsection": "Camera Pose Estimation", "subsubsection": null, "prev_context": "We evaluate the camera pose estimation performance on ScanNet~\\cite{dai2017scannet}(static), Sintel~\\cite{butler2012naturalistic}, and TUM-dynamics~\\cite{sturm2012benchmark}datasets following MonST3R~\\cite{zhang2024monst3r}", "next_context": "and CUT3R~\\cite{cut3r}."}, {"section": "Experiments", "subsection": "Camera Pose Estimation", "subsubsection": null, "prev_context": "We report Absolute Translation Error (ATE), Relative Translation Error (RPE trans), and Relative Rotation Error (RPE rot) after applying a Sim(3) Umeyama alignment with the ground truth~\\cite{zhang2024monst3r}", "next_context": "."}, {"section": "Experiments", "subsection": "Camera Pose Estimation", "subsubsection": null, "prev_context": "MonST3R-GA~\\cite{zhang2024monst3r}", "next_context": ""}], "importance_score": 9.833333333333332}, "choy20163d": {"bib_key": "choy20163d", "bib_title": "3d-r2n2: A unified approach for single and multi-view 3d object reconstruction.", "bib_author ": "Christopher~B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Existing streaming reconstruction methods~\\cite{engel2014lsd, choy20163d, zhang2022nerfusion, wang20243d, cut3r}", "next_context": "universally incorporate a certain form of memory to store information from past frames."}, {"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "This memory can take on various forms, such as explicit scene representation (the most direct form of memory), recurrent neural network architectures~\\cite{choy20163d}", "next_context": ", and encoded or learnable token features~\\cite{wang20243d, cut3r}."}], "importance_score": 1.2}, "zhang2022nerfusion": {"bib_key": "zhang2022nerfusion", "bib_title": "Nerfusion: Fusing radiance fields for large-scale scene reconstruction.", "bib_author ": "Xiaoshuai Zhang, Sai Bi, Kalyan Sunkavalli, Hao Su, and Zexiang Xu.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Related Work", "subsection": null, "subsubsection": null, "prev_context": "Existing streaming reconstruction methods~\\cite{engel2014lsd, choy20163d, zhang2022nerfusion, wang20243d, cut3r}", "next_context": "universally incorporate a certain form of memory to store information from past frames."}], "importance_score": 0.2}, "mast3r": {"bib_key": "mast3r", "bib_title": "\\'e", "bib_author ": "Vincent Leroy, Yohann Cabon, and J", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Proposed Approach", "subsection": "Memory-Based Streaming 3D Reconstruction", "subsubsection": null, "prev_context": "(1) pair-wise reconstruction~\\cite{wang2024dust3r, zhang2024monst3r, mast3r}", "next_context": "with an optimization-based global alignment, "}, {"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "\\textbf{Training objective.}Following MASt3R~\\cite{mast3r}", "next_context": "and CUT3R~\\cite{cut3r}, we use the L2 norm loss for the poses and a confidence-aware loss for the pointmaps."}, {"section": "Experiments", "subsection": null, "subsubsection": null, "prev_context": "We choose DUSt3R~\\cite{wang2024dust3r}, MASt3R~\\cite{mast3r}", "next_context": ", MonST3R~\\cite{zhang2024monst3r}, Spann3R~\\cite{wang20243d}, and CUT3R~\\cite{cut3r}as our primary baselines."}, {"section": "Experiments", "subsection": "3D Reconstruction", "subsubsection": null, "prev_context": "MASt3R-GA~\\cite{mast3r}", "next_context": ""}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "MASt3R~\\cite{mast3r}", "next_context": ""}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "We also compare results without alignment with other metric pointmap methods like MASt3R~\\cite{mast3r}", "next_context": "and CUT3R~\\cite{cut3r}(Metric-scale alignment)."}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "As shown in Table~\\ref{tab:videodepth}, with the per-sequence scale alignment, our method outperforms DUSt3R~\\cite{wang2024dust3r}, MASt3R~\\cite{mast3r}", "next_context": ", and Spann3R~\\cite{wang20243d}by a large margin."}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "Besides, in the metric-scale setting, our method outperforms MASt3R-GA~\\cite{mast3r}", "next_context": "and performs comparably with CUT3R~\\cite{cut3r}, leading on Sintel and Bonn."}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "MASt3R-GA~\\cite{mast3r}", "next_context": ""}, {"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "MASt3R-GA~\\cite{mast3r}", "next_context": ""}, {"section": "Experiments", "subsection": "Camera Pose Estimation", "subsubsection": null, "prev_context": "MASt3R-GA~\\cite{mast3r}", "next_context": ""}], "importance_score": 10.333333333333332}, "dosovitskiy2021an": {"bib_key": "dosovitskiy2021an", "bib_title": "An image is worth 16x16 words: Transformers for image recognition at scale.", "bib_author ": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Proposed Approach", "subsection": "Pointer-Image Interaction", "subsubsection": null, "prev_context": "\\textbf{Image encoder.}For each frame, we use a ViT~\\cite{dosovitskiy2021an}", "next_context": "to encode the current inputI_tinto image tokensF_t:F_t=\\mathrm{Encoder}(I_t)."}, {"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "\\textbf{Implementation details.}We initialize our ViT-Large~\\cite{wang2024dust3r, dosovitskiy2021an}", "next_context": "image encoder, ViT-Base interaction decoders~\\cite{wang2024dust3r, weinzaepfel2022croco}, and DPT~\\cite{ranftl21dpt}heads with pre-trained weights from DUSt3R~\\cite{wang2024dust3r}."}], "importance_score": 1.5}, "weinzaepfel2022croco": {"bib_key": "weinzaepfel2022croco", "bib_title": "\\'e", "bib_author ": "Philippe Weinzaepfel, Vincent Leroy, Thomas Lucas, Romain Br", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Proposed Approach", "subsection": "Pointer-Image Interaction", "subsubsection": null, "prev_context": "Then we use two intertwined decoders~\\cite{weinzaepfel2022croco, wang2024dust3r}", "next_context": "to enable interaction between the current image tokens and the memory:F'_0,z'_0"}, {"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "\\textbf{Implementation details.}We initialize our ViT-Large~\\cite{wang2024dust3r, dosovitskiy2021an}image encoder, ViT-Base interaction decoders~\\cite{wang2024dust3r, weinzaepfel2022croco}", "next_context": ", and DPT~\\cite{ranftl21dpt}heads with pre-trained weights from DUSt3R~\\cite{wang2024dust3r}."}], "importance_score": 1.0}, "ranftl21dpt": {"bib_key": "ranftl21dpt", "bib_title": "\\'e", "bib_author ": "Ren", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Proposed Approach", "subsection": "Pointer-Image Interaction", "subsubsection": null, "prev_context": "=\\mathrm{Head}_global(F'_t,z'_t),where\\mathrm{Head}_poseis an MLP network,\\mathrm{Head}_selfand\\mathrm{Head}_globalare DPT~\\cite{ranftl21dpt}", "next_context": "heads."}, {"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "\\textbf{Implementation details.}We initialize our ViT-Large~\\cite{wang2024dust3r, dosovitskiy2021an}image encoder, ViT-Base interaction decoders~\\cite{wang2024dust3r, weinzaepfel2022croco}, and DPT~\\cite{ranftl21dpt}", "next_context": "heads with pre-trained weights from DUSt3R~\\cite{wang2024dust3r}."}], "importance_score": 2.0}, "su2024roformer": {"bib_key": "su2024roformer", "bib_title": "Roformer: Enhanced transformer with rotary position embedding.", "bib_author ": "Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Proposed Approach", "subsection": "Pointer-Image Interaction", "subsubsection": null, "prev_context": "\\textbf{3D hierarchical position embedding.}We expand the rotary position embedding (RoPE~\\cite{su2024roformer, heo2024rotary}", "next_context": ", a method of relative position embedding usually used in transformers) to a 3D hierarchical position embedding and use this to conduct position embedding in continuous 3D space."}], "importance_score": 0.5}, "heo2024rotary": {"bib_key": "heo2024rotary", "bib_title": "Rotary position embedding for vision transformer.", "bib_author ": "Byeongho Heo, Song Park, Dongyoon Han, and Sangdoo Yun.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Proposed Approach", "subsection": "Pointer-Image Interaction", "subsubsection": null, "prev_context": "\\textbf{3D hierarchical position embedding.}We expand the rotary position embedding (RoPE~\\cite{su2024roformer, heo2024rotary}", "next_context": ", a method of relative position embedding usually used in transformers) to a 3D hierarchical position embedding and use this to conduct position embedding in continuous 3D space."}], "importance_score": 0.5}, "baruch2021arkitscenes": {"bib_key": "baruch2021arkitscenes", "bib_title": "Arkitscenes: A diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data.", "bib_author ": "Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et~al.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "\\textbf{Training datasets.}During training, we use a combination of 14 datasets, including ARKitScenes~\\cite{baruch2021arkitscenes}", "next_context": ", ScanNet~\\cite{dai2017scannet}, ScanNet++~\\cite{yeshwanth2023scannet++}, CO3Dv2~\\cite{reizenstein2021common}, WildRGBD~\\cite{xia2024rgbd}, OmniObject3D~\\cite{wu2023omniobject3d}, HyperSim (a subset of it)~\\cite{hypersim}, BlendedMVS~\\cite{yao2020blendedmvs}, MegaDepth~\\cite{li2018megadepth}, Waymo~\\cite{sun2020scalability}, VirtualKITTI2~\\cite{cabon2020virtual}, PointOdyssey~\\cite{zheng2023pointodyssey}, Spring~\\cite{mehl2023spring}, and MVS-Synth~\\cite{mvssynth}."}, {"section": "More Method Details", "subsection": "3D Hierarchical Position Embedding", "subsubsection": null, "prev_context": "\\midruleARKitScenes~\\cite{baruch2021arkitscenes}", "next_context": ""}], "importance_score": 2.0}, "reizenstein2021common": {"bib_key": "reizenstein2021common", "bib_title": "Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction.", "bib_author ": "Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "\\textbf{Training datasets.}During training, we use a combination of 14 datasets, including ARKitScenes~\\cite{baruch2021arkitscenes}, ScanNet~\\cite{dai2017scannet}, ScanNet++~\\cite{yeshwanth2023scannet++}, CO3Dv2~\\cite{reizenstein2021common}", "next_context": ", WildRGBD~\\cite{xia2024rgbd}, OmniObject3D~\\cite{wu2023omniobject3d}, HyperSim (a subset of it)~\\cite{hypersim}, BlendedMVS~\\cite{yao2020blendedmvs}, MegaDepth~\\cite{li2018megadepth}, Waymo~\\cite{sun2020scalability}, VirtualKITTI2~\\cite{cabon2020virtual}, PointOdyssey~\\cite{zheng2023pointodyssey}, Spring~\\cite{mehl2023spring}, and MVS-Synth~\\cite{mvssynth}."}, {"section": "More Method Details", "subsection": "3D Hierarchical Position Embedding", "subsubsection": null, "prev_context": "CO3Dv2~\\cite{reizenstein2021common}", "next_context": ""}], "importance_score": 2.0}, "xia2024rgbd": {"bib_key": "xia2024rgbd", "bib_title": "Rgbd objects in the wild: scaling real-world 3d object learning from rgb-d videos.", "bib_author ": "Hongchi Xia, Yang Fu, Sifei Liu, and Xiaolong Wang.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "\\textbf{Training datasets.}During training, we use a combination of 14 datasets, including ARKitScenes~\\cite{baruch2021arkitscenes}, ScanNet~\\cite{dai2017scannet}, ScanNet++~\\cite{yeshwanth2023scannet++}, CO3Dv2~\\cite{reizenstein2021common}, WildRGBD~\\cite{xia2024rgbd}", "next_context": ", OmniObject3D~\\cite{wu2023omniobject3d}, HyperSim (a subset of it)~\\cite{hypersim}, BlendedMVS~\\cite{yao2020blendedmvs}, MegaDepth~\\cite{li2018megadepth}, Waymo~\\cite{sun2020scalability}, VirtualKITTI2~\\cite{cabon2020virtual}, PointOdyssey~\\cite{zheng2023pointodyssey}, Spring~\\cite{mehl2023spring}, and MVS-Synth~\\cite{mvssynth}."}, {"section": "More Method Details", "subsection": "3D Hierarchical Position Embedding", "subsubsection": null, "prev_context": "WildRGBD~\\cite{xia2024rgbd}", "next_context": ""}], "importance_score": 2.0}, "wu2023omniobject3d": {"bib_key": "wu2023omniobject3d", "bib_title": "Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation.", "bib_author ": "Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, et~al.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "\\textbf{Training datasets.}During training, we use a combination of 14 datasets, including ARKitScenes~\\cite{baruch2021arkitscenes}, ScanNet~\\cite{dai2017scannet}, ScanNet++~\\cite{yeshwanth2023scannet++}, CO3Dv2~\\cite{reizenstein2021common}, WildRGBD~\\cite{xia2024rgbd}, OmniObject3D~\\cite{wu2023omniobject3d}", "next_context": ", HyperSim (a subset of it)~\\cite{hypersim}, BlendedMVS~\\cite{yao2020blendedmvs}, MegaDepth~\\cite{li2018megadepth}, Waymo~\\cite{sun2020scalability}, VirtualKITTI2~\\cite{cabon2020virtual}, PointOdyssey~\\cite{zheng2023pointodyssey}, Spring~\\cite{mehl2023spring}, and MVS-Synth~\\cite{mvssynth}."}, {"section": "More Method Details", "subsection": "3D Hierarchical Position Embedding", "subsubsection": null, "prev_context": "OmniObject3D~\\cite{wu2023omniobject3d}", "next_context": ""}], "importance_score": 2.0}, "hypersim": {"bib_key": "hypersim", "bib_title": "Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding.", "bib_author ": "Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel~Angel Bautista, Nathan Paczan, Russ Webb, and Joshua~M. Susskind.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "\\textbf{Training datasets.}During training, we use a combination of 14 datasets, including ARKitScenes~\\cite{baruch2021arkitscenes}, ScanNet~\\cite{dai2017scannet}, ScanNet++~\\cite{yeshwanth2023scannet++}, CO3Dv2~\\cite{reizenstein2021common}, WildRGBD~\\cite{xia2024rgbd}, OmniObject3D~\\cite{wu2023omniobject3d}, HyperSim (a subset of it)~\\cite{hypersim}", "next_context": ", BlendedMVS~\\cite{yao2020blendedmvs}, MegaDepth~\\cite{li2018megadepth}, Waymo~\\cite{sun2020scalability}, VirtualKITTI2~\\cite{cabon2020virtual}, PointOdyssey~\\cite{zheng2023pointodyssey}, Spring~\\cite{mehl2023spring}, and MVS-Synth~\\cite{mvssynth}."}, {"section": "More Method Details", "subsection": "3D Hierarchical Position Embedding", "subsubsection": null, "prev_context": "HyperSim~\\cite{hypersim}", "next_context": ""}], "importance_score": 2.0}, "yao2020blendedmvs": {"bib_key": "yao2020blendedmvs", "bib_title": "Blendedmvs: A large-scale dataset for generalized multi-view stereo networks.", "bib_author ": "Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "\\textbf{Training datasets.}During training, we use a combination of 14 datasets, including ARKitScenes~\\cite{baruch2021arkitscenes}, ScanNet~\\cite{dai2017scannet}, ScanNet++~\\cite{yeshwanth2023scannet++}, CO3Dv2~\\cite{reizenstein2021common}, WildRGBD~\\cite{xia2024rgbd}, OmniObject3D~\\cite{wu2023omniobject3d}, HyperSim (a subset of it)~\\cite{hypersim}, BlendedMVS~\\cite{yao2020blendedmvs}", "next_context": ", MegaDepth~\\cite{li2018megadepth}, Waymo~\\cite{sun2020scalability}, VirtualKITTI2~\\cite{cabon2020virtual}, PointOdyssey~\\cite{zheng2023pointodyssey}, Spring~\\cite{mehl2023spring}, and MVS-Synth~\\cite{mvssynth}."}, {"section": "More Method Details", "subsection": "3D Hierarchical Position Embedding", "subsubsection": null, "prev_context": "BlendedMVS~\\cite{yao2020blendedmvs}", "next_context": ""}], "importance_score": 2.0}, "li2018megadepth": {"bib_key": "li2018megadepth", "bib_title": "Megadepth: Learning single-view depth prediction from internet photos.", "bib_author ": "Zhengqi Li and Noah Snavely.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "\\textbf{Training datasets.}During training, we use a combination of 14 datasets, including ARKitScenes~\\cite{baruch2021arkitscenes}, ScanNet~\\cite{dai2017scannet}, ScanNet++~\\cite{yeshwanth2023scannet++}, CO3Dv2~\\cite{reizenstein2021common}, WildRGBD~\\cite{xia2024rgbd}, OmniObject3D~\\cite{wu2023omniobject3d}, HyperSim (a subset of it)~\\cite{hypersim}, BlendedMVS~\\cite{yao2020blendedmvs}, MegaDepth~\\cite{li2018megadepth}", "next_context": ", Waymo~\\cite{sun2020scalability}, VirtualKITTI2~\\cite{cabon2020virtual}, PointOdyssey~\\cite{zheng2023pointodyssey}, Spring~\\cite{mehl2023spring}, and MVS-Synth~\\cite{mvssynth}."}, {"section": "More Method Details", "subsection": "3D Hierarchical Position Embedding", "subsubsection": null, "prev_context": "MegaDepth~\\cite{li2018megadepth}", "next_context": ""}], "importance_score": 2.0}, "sun2020scalability": {"bib_key": "sun2020scalability", "bib_title": "Scalability in perception for autonomous driving: Waymo open dataset.", "bib_author ": "Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et~al.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "\\textbf{Training datasets.}During training, we use a combination of 14 datasets, including ARKitScenes~\\cite{baruch2021arkitscenes}, ScanNet~\\cite{dai2017scannet}, ScanNet++~\\cite{yeshwanth2023scannet++}, CO3Dv2~\\cite{reizenstein2021common}, WildRGBD~\\cite{xia2024rgbd}, OmniObject3D~\\cite{wu2023omniobject3d}, HyperSim (a subset of it)~\\cite{hypersim}, BlendedMVS~\\cite{yao2020blendedmvs}, MegaDepth~\\cite{li2018megadepth}, Waymo~\\cite{sun2020scalability}", "next_context": ", VirtualKITTI2~\\cite{cabon2020virtual}, PointOdyssey~\\cite{zheng2023pointodyssey}, Spring~\\cite{mehl2023spring}, and MVS-Synth~\\cite{mvssynth}."}, {"section": "More Method Details", "subsection": "3D Hierarchical Position Embedding", "subsubsection": null, "prev_context": "Waymo~\\cite{sun2020scalability}", "next_context": ""}], "importance_score": 2.0}, "cabon2020virtual": {"bib_key": "cabon2020virtual", "bib_title": "Virtual kitti 2.", "bib_author ": "Yohann Cabon, Naila Murray, and Martin Humenberger.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "\\textbf{Training datasets.}During training, we use a combination of 14 datasets, including ARKitScenes~\\cite{baruch2021arkitscenes}, ScanNet~\\cite{dai2017scannet}, ScanNet++~\\cite{yeshwanth2023scannet++}, CO3Dv2~\\cite{reizenstein2021common}, WildRGBD~\\cite{xia2024rgbd}, OmniObject3D~\\cite{wu2023omniobject3d}, HyperSim (a subset of it)~\\cite{hypersim}, BlendedMVS~\\cite{yao2020blendedmvs}, MegaDepth~\\cite{li2018megadepth}, Waymo~\\cite{sun2020scalability}, VirtualKITTI2~\\cite{cabon2020virtual}", "next_context": ", PointOdyssey~\\cite{zheng2023pointodyssey}, Spring~\\cite{mehl2023spring}, and MVS-Synth~\\cite{mvssynth}."}, {"section": "More Method Details", "subsection": "3D Hierarchical Position Embedding", "subsubsection": null, "prev_context": "VirtualKITTI2~\\cite{cabon2020virtual}", "next_context": ""}], "importance_score": 2.0}, "zheng2023pointodyssey": {"bib_key": "zheng2023pointodyssey", "bib_title": "Pointodyssey: A large-scale synthetic dataset for long-term point tracking.", "bib_author ": "Yang Zheng, Adam~W Harley, Bokui Shen, Gordon Wetzstein, and Leonidas~J Guibas.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "\\textbf{Training datasets.}During training, we use a combination of 14 datasets, including ARKitScenes~\\cite{baruch2021arkitscenes}, ScanNet~\\cite{dai2017scannet}, ScanNet++~\\cite{yeshwanth2023scannet++}, CO3Dv2~\\cite{reizenstein2021common}, WildRGBD~\\cite{xia2024rgbd}, OmniObject3D~\\cite{wu2023omniobject3d}, HyperSim (a subset of it)~\\cite{hypersim}, BlendedMVS~\\cite{yao2020blendedmvs}, MegaDepth~\\cite{li2018megadepth}, Waymo~\\cite{sun2020scalability}, VirtualKITTI2~\\cite{cabon2020virtual}, PointOdyssey~\\cite{zheng2023pointodyssey}", "next_context": ", Spring~\\cite{mehl2023spring}, and MVS-Synth~\\cite{mvssynth}."}, {"section": "More Method Details", "subsection": "3D Hierarchical Position Embedding", "subsubsection": null, "prev_context": "PointOdyssey~\\cite{zheng2023pointodyssey}", "next_context": ""}], "importance_score": 2.0}, "mehl2023spring": {"bib_key": "mehl2023spring", "bib_title": "\\'e", "bib_author ": "Lukas Mehl, Jenny Schmalfuss, Azin Jahedi, Yaroslava Nalivayko, and Andr", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "\\textbf{Training datasets.}During training, we use a combination of 14 datasets, including ARKitScenes~\\cite{baruch2021arkitscenes}, ScanNet~\\cite{dai2017scannet}, ScanNet++~\\cite{yeshwanth2023scannet++}, CO3Dv2~\\cite{reizenstein2021common}, WildRGBD~\\cite{xia2024rgbd}, OmniObject3D~\\cite{wu2023omniobject3d}, HyperSim (a subset of it)~\\cite{hypersim}, BlendedMVS~\\cite{yao2020blendedmvs}, MegaDepth~\\cite{li2018megadepth}, Waymo~\\cite{sun2020scalability}, VirtualKITTI2~\\cite{cabon2020virtual}, PointOdyssey~\\cite{zheng2023pointodyssey}, Spring~\\cite{mehl2023spring}", "next_context": ", and MVS-Synth~\\cite{mvssynth}."}, {"section": "More Method Details", "subsection": "3D Hierarchical Position Embedding", "subsubsection": null, "prev_context": "Spring~\\cite{mehl2023spring}", "next_context": ""}], "importance_score": 2.0}, "mvssynth": {"bib_key": "mvssynth", "bib_title": "Deepmvs: Learning multi-view stereopsis.", "bib_author ": "Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "\\textbf{Training datasets.}During training, we use a combination of 14 datasets, including ARKitScenes~\\cite{baruch2021arkitscenes}, ScanNet~\\cite{dai2017scannet}, ScanNet++~\\cite{yeshwanth2023scannet++}, CO3Dv2~\\cite{reizenstein2021common}, WildRGBD~\\cite{xia2024rgbd}, OmniObject3D~\\cite{wu2023omniobject3d}, HyperSim (a subset of it)~\\cite{hypersim}, BlendedMVS~\\cite{yao2020blendedmvs}, MegaDepth~\\cite{li2018megadepth}, Waymo~\\cite{sun2020scalability}, VirtualKITTI2~\\cite{cabon2020virtual}, PointOdyssey~\\cite{zheng2023pointodyssey}, Spring~\\cite{mehl2023spring}, and MVS-Synth~\\cite{mvssynth}", "next_context": "."}, {"section": "More Method Details", "subsection": "3D Hierarchical Position Embedding", "subsubsection": null, "prev_context": "MVS-Synth~\\cite{mvssynth}", "next_context": ""}], "importance_score": 2.0}, "loshchilov2017decoupled": {"bib_key": "loshchilov2017decoupled", "bib_title": "Decoupled weight decay regularization.", "bib_author ": "Ilya Loshchilov and Frank Hutter.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Proposed Approach", "subsection": "Training Strategy", "subsubsection": null, "prev_context": "We use the AdamW optimizer~\\cite{loshchilov2017decoupled}", "next_context": "and the learning rate warms up to a maximum value of 5e-5 and decreases according to a cosine schedule."}], "importance_score": 1.0}, "shotton2013scene": {"bib_key": "shotton2013scene", "bib_title": "Scene coordinate regression forests for camera relocalization in rgb-d images.", "bib_author ": "Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "3D Reconstruction", "subsubsection": null, "prev_context": "We evaluate the 3D reconstruction performance on the 7-scenes~\\cite{shotton2013scene}", "next_context": "and NRGBD~\\cite{azinovic2022neural}datasets in Table~\\ref{tab:3d_recon}, and the metrics we used include accuracy (Acc), completion (Comp), and normal consistency (NC), following previous works~\\cite{wang2024dust3r, wang20243d, azinovic2022neural}."}], "importance_score": 1.0}, "azinovic2022neural": {"bib_key": "azinovic2022neural", "bib_title": "\\'c", "bib_author ": "Dejan Azinovi", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "3D Reconstruction", "subsubsection": null, "prev_context": "We evaluate the 3D reconstruction performance on the 7-scenes~\\cite{shotton2013scene}and NRGBD~\\cite{azinovic2022neural}", "next_context": "datasets in Table~\\ref{tab:3d_recon}, and the metrics we used include accuracy (Acc), completion (Comp), and normal consistency (NC), following previous works~\\cite{wang2024dust3r, wang20243d, azinovic2022neural}."}, {"section": "Experiments", "subsection": "3D Reconstruction", "subsubsection": null, "prev_context": "~\\cite{wang2024dust3r, wang20243d, azinovic2022neural}", "next_context": "."}], "importance_score": 1.3333333333333333}, "silberman2012indoor": {"bib_key": "silberman2012indoor", "bib_title": "Indoor segmentation and support inference from rgbd images.", "bib_author ": "Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "\\textbf{Monocular Depth Estimation.}We evaluate zero-shot monocular depth estimation~\\cite{zhang2024monst3r, cut3r}performance on NYU-v2~\\cite{silberman2012indoor}", "next_context": "(static), Sintel~\\cite{butler2012naturalistic}, Bonn~\\cite{palazzolo2019refusion}, and KITTI~\\cite{geiger2013vision}datasets."}], "importance_score": 1.0}, "butler2012naturalistic": {"bib_key": "butler2012naturalistic", "bib_title": "A naturalistic open source movie for optical flow evaluation.", "bib_author ": "Daniel~J Butler, Jonas Wulff, Garrett~B Stanley, and Michael~J Black.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "\\textbf{Monocular Depth Estimation.}We evaluate zero-shot monocular depth estimation~\\cite{zhang2024monst3r, cut3r}performance on NYU-v2~\\cite{silberman2012indoor}(static), Sintel~\\cite{butler2012naturalistic}", "next_context": ", Bonn~\\cite{palazzolo2019refusion}, and KITTI~\\cite{geiger2013vision}datasets."}, {"section": "Experiments", "subsection": "Camera Pose Estimation", "subsubsection": null, "prev_context": "We evaluate the camera pose estimation performance on ScanNet~\\cite{dai2017scannet}(static), Sintel~\\cite{butler2012naturalistic}", "next_context": ", and TUM-dynamics~\\cite{sturm2012benchmark}datasets following MonST3R~\\cite{zhang2024monst3r}and CUT3R~\\cite{cut3r}."}], "importance_score": 2.0}, "palazzolo2019refusion": {"bib_key": "palazzolo2019refusion", "bib_title": "Refusion: 3d reconstruction in dynamic environments for rgb-d cameras exploiting residuals.", "bib_author ": "Emanuele Palazzolo, Jens Behley, Philipp Lottes, Philippe Giguere, and Cyrill Stachniss.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "\\textbf{Monocular Depth Estimation.}We evaluate zero-shot monocular depth estimation~\\cite{zhang2024monst3r, cut3r}performance on NYU-v2~\\cite{silberman2012indoor}(static), Sintel~\\cite{butler2012naturalistic}, Bonn~\\cite{palazzolo2019refusion}", "next_context": ", and KITTI~\\cite{geiger2013vision}datasets."}], "importance_score": 1.0}, "geiger2013vision": {"bib_key": "geiger2013vision", "bib_title": "Vision meets robotics: The kitti dataset.", "bib_author ": "Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Monocular and Video Depth Estimation", "subsubsection": null, "prev_context": "\\textbf{Monocular Depth Estimation.}We evaluate zero-shot monocular depth estimation~\\cite{zhang2024monst3r, cut3r}performance on NYU-v2~\\cite{silberman2012indoor}(static), Sintel~\\cite{butler2012naturalistic}, Bonn~\\cite{palazzolo2019refusion}, and KITTI~\\cite{geiger2013vision}", "next_context": "datasets."}], "importance_score": 1.0}, "sturm2012benchmark": {"bib_key": "sturm2012benchmark", "bib_title": "\\\"u", "bib_author ": "J", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Camera Pose Estimation", "subsubsection": null, "prev_context": "We evaluate the camera pose estimation performance on ScanNet~\\cite{dai2017scannet}(static), Sintel~\\cite{butler2012naturalistic}, and TUM-dynamics~\\cite{sturm2012benchmark}", "next_context": "datasets following MonST3R~\\cite{zhang2024monst3r}and CUT3R~\\cite{cut3r}."}], "importance_score": 1.0}, "kopf2021robust": {"bib_key": "kopf2021robust", "bib_title": "Robust consistent video depth estimation.", "bib_author ": "Johannes Kopf, Xuejian Rong, and Jia-Bin Huang.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Camera Pose Estimation", "subsubsection": null, "prev_context": "\\midruleRobust-CVD~\\cite{kopf2021robust}", "next_context": ""}], "importance_score": 1.0}, "zhang2022structure": {"bib_key": "zhang2022structure", "bib_title": "Structure and motion from casual videos.", "bib_author ": "Zhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Rubinstein, Noah Snavely, and William~T Freeman.", "arxiv_id": null, "short_id": null, "title": null, "author": null, "published": null, "similar_score": null, "context": [{"section": "Experiments", "subsection": "Camera Pose Estimation", "subsubsection": null, "prev_context": "CasualSAM~\\cite{zhang2022structure}", "next_context": ""}], "importance_score": 1.0}}, "refs": [], "table": [{"original": "\\begin{table}[t]\n  \\centering\n  \\caption{\\textbf{Quantitative 3D reconstruction results on 7-scenes and NRGBD datasets.} We use ``GA'' to mark methods with global alignment, and use ``Optim.'' and ``Onl.'' to distinguish between optimization-based and online methods~\\cite{cut3r}. Our method achieves competitive or better performance than those optimization-based methods and current online methods.} \n  \\small\n  \\setlength{\\tabcolsep}{0.1em}\n    \\begin{tabularx}{\\textwidth}{c c c >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >\n    {\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >\n    {\\centering\\arraybackslash}X >\n    {\\centering\\arraybackslash}X}\n      \\toprule\n\n           & &  & \\multicolumn{6}{c}{\\textbf{7-scenes }} & \\multicolumn{6}{c}{\\textbf{NRGBD}}\\\\\n\n      \\cmidrule(lr){4-9} \\cmidrule(lr){10-15}\n         & & & \\multicolumn{2}{c}{{Acc}$\\downarrow$} & \\multicolumn{2}{c}{{Comp}$\\downarrow$} & \\multicolumn{2}{c}{{NC}$\\uparrow$} & \\multicolumn{2}{c}{{Acc}$\\downarrow$} & \\multicolumn{2}{c}{{Comp}$\\downarrow$} & \\multicolumn{2}{c}{{NC}$\\uparrow$}\\\\\n      \\cmidrule(lr){4-5} \\cmidrule(lr){6-7} \\cmidrule(lr){8-9} \\cmidrule(lr){10-11} \\cmidrule(lr){12-13} \\cmidrule(lr){14-15}\n         \\textbf{Method} & \\textbf{Optim.} & \\textbf{Onl.} & {Mean} & {Med.} & {Mean} & {Med.} & {Mean} & {Med.} & {Mean} & {Med.} & {Mean} & {Med.} & {Mean} & {Med.}\\\\\n      \\midrule\n        DUSt3R-GA~\\cite{wang2024dust3r} &\\checkmark &\n        & 0.146\t& 0.077\t& {0.181}\t& 0.067\t& \\bf{0.736}\t& \\bf{0.839}\t&  0.144\t& \\bf{0.019}\t& 0.154\t& \\bf{0.018}\t& \\bf{0.870}\t& \\bf{0.982}\\\\\n        MASt3R-GA~\\cite{mast3r} &\\checkmark &\n        & 0.185 & 0.081 & 0.180 & 0.069 & 0.701\t& 0.792\t& \\underline{0.085} & 0.033\t& \\bf 0.063\t& 0.028\t& 0.794 & 0.928\\\\\n        MonST3R-GA~\\cite{zhang2024monst3r} &\\checkmark &\n        & 0.248 & 0.185 & 0.266 & 0.167 & 0.672 & 0.759 & 0.272 & 0.114 & 0.287 & 0.110 & 0.758 & 0.843\\\\\n        Spann3R~\\cite{wang20243d} & & \\checkmark\n        & 0.298 & 0.226 & 0.205 &  0.112 & {0.650} & {0.730}   & {0.416}  & 0.323  &  {0.417}  & {0.285}  & 0.684  & 0.789\\\\\n        CUT3R~\\cite{cut3r} & & \\checkmark & \\underline{0.126} & \\bf{0.047} & \\underline{0.154} & \\bf{0.031} & \\underline{0.727} & \\underline{0.834} & 0.099 &  \\underline{0.031} & 0.076 & \\underline{0.026} & \\underline{0.837} & \\underline{0.971}\\\\\n        \\textbf{Ours} & & \\checkmark & \\bf 0.124 & \\underline{0.058} & \\bf{0.139} & \\underline{0.054} & 0.725 & \\underline{0.834} & \\textbf{0.079} &  \\underline{0.031} & \\underline{0.073} & 0.027 & 0.824 & 0.965 \\\\\n      \\bottomrule\n    \\end{tabularx}\n    \\label{tab:3d_recon}\n    \\vspace{-3mm}\n\\end{table}", "caption": "\\caption{\\textbf{Quantitative 3D reconstruction results on 7-scenes and NRGBD datasets.} We use ``GA'' to mark methods with global alignment, and use ``Optim.'' and ``Onl.'' to distinguish between optimization-based and online methods~\\cite{cut3r}. Our method achieves competitive or better performance than those optimization-based methods and current online methods.}", "label": "\\label{tab:3d_recon}", "tabular": "\\begin{tabularx}{\\textwidth}{c c c >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >\n    {\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >\n    {\\centering\\arraybackslash}X >\n    {\\centering\\arraybackslash}X}\n      \\toprule\n\n           & &  & \\multicolumn{6}{c}{\\textbf{7-scenes }} & \\multicolumn{6}{c}{\\textbf{NRGBD}}\\\\\n\n      \\cmidrule(lr){4-9} \\cmidrule(lr){10-15}\n         & & & \\multicolumn{2}{c}{{Acc}$\\downarrow$} & \\multicolumn{2}{c}{{Comp}$\\downarrow$} & \\multicolumn{2}{c}{{NC}$\\uparrow$} & \\multicolumn{2}{c}{{Acc}$\\downarrow$} & \\multicolumn{2}{c}{{Comp}$\\downarrow$} & \\multicolumn{2}{c}{{NC}$\\uparrow$}\\\\\n      \\cmidrule(lr){4-5} \\cmidrule(lr){6-7} \\cmidrule(lr){8-9} \\cmidrule(lr){10-11} \\cmidrule(lr){12-13} \\cmidrule(lr){14-15}\n         \\textbf{Method} & \\textbf{Optim.} & \\textbf{Onl.} & {Mean} & {Med.} & {Mean} & {Med.} & {Mean} & {Med.} & {Mean} & {Med.} & {Mean} & {Med.} & {Mean} & {Med.}\\\\\n      \\midrule\n        DUSt3R-GA~\\cite{wang2024dust3r} &\\checkmark &\n        & 0.146\t& 0.077\t& {0.181}\t& 0.067\t& \\bf{0.736}\t& \\bf{0.839}\t&  0.144\t& \\bf{0.019}\t& 0.154\t& \\bf{0.018}\t& \\bf{0.870}\t& \\bf{0.982}\\\\\n        MASt3R-GA~\\cite{mast3r} &\\checkmark &\n        & 0.185 & 0.081 & 0.180 & 0.069 & 0.701\t& 0.792\t& \\underline{0.085} & 0.033\t& \\bf 0.063\t& 0.028\t& 0.794 & 0.928\\\\\n        MonST3R-GA~\\cite{zhang2024monst3r} &\\checkmark &\n        & 0.248 & 0.185 & 0.266 & 0.167 & 0.672 & 0.759 & 0.272 & 0.114 & 0.287 & 0.110 & 0.758 & 0.843\\\\\n        Spann3R~\\cite{wang20243d} & & \\checkmark\n        & 0.298 & 0.226 & 0.205 &  0.112 & {0.650} & {0.730}   & {0.416}  & 0.323  &  {0.417}  & {0.285}  & 0.684  & 0.789\\\\\n        CUT3R~\\cite{cut3r} & & \\checkmark & \\underline{0.126} & \\bf{0.047} & \\underline{0.154} & \\bf{0.031} & \\underline{0.727} & \\underline{0.834} & 0.099 &  \\underline{0.031} & 0.076 & \\underline{0.026} & \\underline{0.837} & \\underline{0.971}\\\\\n        \\textbf{Ours} & & \\checkmark & \\bf 0.124 & \\underline{0.058} & \\bf{0.139} & \\underline{0.054} & 0.725 & \\underline{0.834} & \\textbf{0.079} &  \\underline{0.031} & \\underline{0.073} & 0.027 & 0.824 & 0.965 \\\\\n      \\bottomrule\n    \\end{tabularx}", "subtables": []}, {"original": "\\begin{table}[t]\n\\centering\n\\caption{\\textbf{Monocular Depth Evaluation} on NYU-v2 (static), Sintel, Bonn, and KITTI datasets.}\n\\small\n\\renewcommand{\\arraystretch}{0.95}\n\\renewcommand{\\tabcolsep}{2.pt}\n\\begin{tabular}{ccc|cc|cc|cc}\n\\toprule\n  & \\multicolumn{2}{c}{\\textbf{NYU-v2 (Static)}}&\\multicolumn{2}{c}{\\textbf{Sintel}} & \\multicolumn{2}{c}{\\textbf{Bonn}} & \\multicolumn{2}{c}{\\textbf{KITTI}}\\\\ \n\\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-7} \\cmidrule(lr){8-9}\n {\\textbf{Method}} & {Abs Rel $\\downarrow$} & {$\\delta$\\textless{}$1.25\\uparrow$} & {Abs Rel $\\downarrow$} & { $\\delta$\\textless{}$1.25\\uparrow$} & {Abs Rel $\\downarrow$} & {$\\delta$\\textless{}$1.25\\uparrow$} & {Abs Rel $\\downarrow$} & {$\\delta$\\textless{}$1.25\\uparrow$} \\\\ \n\\midrule\nDUSt3R~\\cite{wang2024dust3r} & \\underline{0.080} & \\underline{90.7} & 0.424 & \\underline{58.7} & 0.141 & 82.5 & 0.112 & 86.3 \\\\ \nMASt3R~\\cite{mast3r} & {0.129} & 84.9 & \\textbf{0.340} & \\textbf{60.4} & {0.142} & {82.0} & \\textbf{0.079} & \\textbf{94.7}\\\\ \nMonST3R~\\cite{zhang2024monst3r} & 0.102 & 88.0 & \\underline{0.358} & 54.8 & 0.076 & 93.9 & {0.100} & {89.3} \\\\ \nSpann3R~\\cite{wang20243d}  & 0.122 & 84.9 & {0.470} & 53.9 & {0.118} & {85.9} & {0.128} & {84.6} \\\\ \nCUT3R~\\cite{cut3r} & 0.086 & 90.9 & {0.428} & {55.4} & \\underline{0.063} & \\bf{96.2} & 0.092 & 91.3 \\\\ \n\\textbf{Ours} & \\bf{0.079} & \\textbf{92.0} & {0.395} & {56.8} & \\bf{0.061} & \\underline{95.4} & \\underline{0.087} & \\underline{93.7} \\\\ \n\\bottomrule\n\\end{tabular}\n\\label{tab:monodepth}\n    \\vspace{-3mm}\n\\end{table}", "caption": "\\caption{\\textbf{Monocular Depth Evaluation} on NYU-v2 (static), Sintel, Bonn, and KITTI datasets.}", "label": "\\label{tab:monodepth}", "tabular": "\\begin{tabular}{ccc|cc|cc|cc}\n\\toprule\n  & \\multicolumn{2}{c}{\\textbf{NYU-v2 (Static)}}&\\multicolumn{2}{c}{\\textbf{Sintel}} & \\multicolumn{2}{c}{\\textbf{Bonn}} & \\multicolumn{2}{c}{\\textbf{KITTI}}\\\\ \n\\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-7} \\cmidrule(lr){8-9}\n {\\textbf{Method}} & {Abs Rel $\\downarrow$} & {$\\delta$\\textless{}$1.25\\uparrow$} & {Abs Rel $\\downarrow$} & { $\\delta$\\textless{}$1.25\\uparrow$} & {Abs Rel $\\downarrow$} & {$\\delta$\\textless{}$1.25\\uparrow$} & {Abs Rel $\\downarrow$} & {$\\delta$\\textless{}$1.25\\uparrow$} \\\\ \n\\midrule\nDUSt3R~\\cite{wang2024dust3r} & \\underline{0.080} & \\underline{90.7} & 0.424 & \\underline{58.7} & 0.141 & 82.5 & 0.112 & 86.3 \\\\ \nMASt3R~\\cite{mast3r} & {0.129} & 84.9 & \\textbf{0.340} & \\textbf{60.4} & {0.142} & {82.0} & \\textbf{0.079} & \\textbf{94.7}\\\\ \nMonST3R~\\cite{zhang2024monst3r} & 0.102 & 88.0 & \\underline{0.358} & 54.8 & 0.076 & 93.9 & {0.100} & {89.3} \\\\ \nSpann3R~\\cite{wang20243d}  & 0.122 & 84.9 & {0.470} & 53.9 & {0.118} & {85.9} & {0.128} & {84.6} \\\\ \nCUT3R~\\cite{cut3r} & 0.086 & 90.9 & {0.428} & {55.4} & \\underline{0.063} & \\bf{96.2} & 0.092 & 91.3 \\\\ \n\\textbf{Ours} & \\bf{0.079} & \\textbf{92.0} & {0.395} & {56.8} & \\bf{0.061} & \\underline{95.4} & \\underline{0.087} & \\underline{93.7} \\\\ \n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[t]\n\\centering\n\\caption{\\textbf{Video Depth Evaluation.} \nWe compare scale-invariant depth (per-sequence alignment) and metric depth (no alignment) results on Sintel, Bonn, and KITTI datasets.}\n\\small\n\\renewcommand{\\arraystretch}{1.0}\n\\renewcommand{\\tabcolsep}{1.4pt}\n\\begin{tabular}\n{@{}cccccccccc@{}}\n\\toprule\n &  &  &  & \\multicolumn{2}{c}{\\textbf{Sintel}} & \\multicolumn{2}{c}{\\textbf{BONN}} & \\multicolumn{2}{c}{\\textbf{KITTI}}\\\\ \n\\cmidrule(lr){5-6} \\cmidrule(lr){7-8} \\cmidrule(lr){9-10}\n\\textbf{Alignment} & \\textbf{Method} & \\textbf{Optim.} & \\textbf{Onl.\\ } & {Abs Rel $\\downarrow$} & {$\\delta$\\textless $1.25\\uparrow$} & {Abs Rel $\\downarrow$} & {$\\delta$\\textless $1.25\\uparrow$} & {Abs Rel $\\downarrow$} & {$\\delta$ \\textless $1.25\\uparrow$} \\\\ \n\\midrule\n\\multirow{6}{*}{\\begin{minipage}[c]{1.5cm}Per-sequence\\end{minipage}} \n& DUSt3R-GA~\\cite{wang2024dust3r} &  \\checkmark & & 0.656 & {45.2} & {0.155} & {83.3} & 0.144 & 81.3 \\\\\n& MASt3R-GA~\\cite{mast3r} &   \\checkmark & & 0.641 & {43.9} & {0.252} & {70.1} & {0.183} & {74.5} \\\\\n& MonST3R-GA~\\cite{zhang2024monst3r} &  \\checkmark &  & \\textbf{0.378} & \\textbf{55.8} & \\underline{0.067} & \\textbf{96.3} & {0.168} & {74.4} \\\\\n& Spann3R~\\cite{wang20243d}&  & \\checkmark & 0.622 & {42.6} & {0.144} & {81.3} & {0.198} & {73.7} \\\\\n& CUT3R~\\cite{cut3r} &  & \\checkmark & \\underline{0.421}  & 47.9 & 0.078 & 93.7 & \\textbf{0.118} & \\textbf{88.1} \\\\\n& \\textbf{Ours} &  & \\checkmark & 0.452  & \\underline{48.9} & \\bf{0.060} & \\underline{96.0} & \\underline{0.136} & \\underline{84.2} \\\\\n\\midrule\n\\multirow{3}{*}{\\begin{minipage}[c]{1.5cm}Metric-scale\\end{minipage}} \n& MASt3R-GA~\\cite{mast3r} & \\checkmark &  & 1.022  & 14.3 & 0.272 & 70.6 & 0.467 & 15.2 \\\\  \n& CUT3R~\\cite{cut3r} &  & \\checkmark & 1.029 & \\textbf{23.8} & \\textbf{0.103} & 88.5& \\textbf{0.122} & \\textbf{85.5} \\\\\n& \\textbf{Ours} &  & \\checkmark & \\textbf{0.777} & 17.1 & 0.137 & \\textbf{94.7}& 0.191 & 73.8 \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:videodepth}\n    \\vspace{-5mm}\n\\end{table}", "caption": "\\caption{\\textbf{Video Depth Evaluation.} \nWe compare scale-invariant depth (per-sequence alignment) and metric depth (no alignment) results on Sintel, Bonn, and KITTI datasets.}", "label": "\\label{tab:videodepth}", "tabular": "\\begin{tabular}\n{@{}cccccccccc@{}}\n\\toprule\n &  &  &  & \\multicolumn{2}{c}{\\textbf{Sintel}} & \\multicolumn{2}{c}{\\textbf{BONN}} & \\multicolumn{2}{c}{\\textbf{KITTI}}\\\\ \n\\cmidrule(lr){5-6} \\cmidrule(lr){7-8} \\cmidrule(lr){9-10}\n\\textbf{Alignment} & \\textbf{Method} & \\textbf{Optim.} & \\textbf{Onl.\\ } & {Abs Rel $\\downarrow$} & {$\\delta$\\textless $1.25\\uparrow$} & {Abs Rel $\\downarrow$} & {$\\delta$\\textless $1.25\\uparrow$} & {Abs Rel $\\downarrow$} & {$\\delta$ \\textless $1.25\\uparrow$} \\\\ \n\\midrule\n\\multirow{6}{*}{\\begin{minipage}[c]{1.5cm}Per-sequence\\end{minipage}} \n& DUSt3R-GA~\\cite{wang2024dust3r} &  \\checkmark & & 0.656 & {45.2} & {0.155} & {83.3} & 0.144 & 81.3 \\\\\n& MASt3R-GA~\\cite{mast3r} &   \\checkmark & & 0.641 & {43.9} & {0.252} & {70.1} & {0.183} & {74.5} \\\\\n& MonST3R-GA~\\cite{zhang2024monst3r} &  \\checkmark &  & \\textbf{0.378} & \\textbf{55.8} & \\underline{0.067} & \\textbf{96.3} & {0.168} & {74.4} \\\\\n& Spann3R~\\cite{wang20243d}&  & \\checkmark & 0.622 & {42.6} & {0.144} & {81.3} & {0.198} & {73.7} \\\\\n& CUT3R~\\cite{cut3r} &  & \\checkmark & \\underline{0.421}  & 47.9 & 0.078 & 93.7 & \\textbf{0.118} & \\textbf{88.1} \\\\\n& \\textbf{Ours} &  & \\checkmark & 0.452  & \\underline{48.9} & \\bf{0.060} & \\underline{96.0} & \\underline{0.136} & \\underline{84.2} \\\\\n\\midrule\n\\multirow{3}{*}{\\begin{minipage}[c]{1.5cm}Metric-scale\\end{minipage}} \n& MASt3R-GA~\\cite{mast3r} & \\checkmark &  & 1.022  & 14.3 & 0.272 & 70.6 & 0.467 & 15.2 \\\\  \n& CUT3R~\\cite{cut3r} &  & \\checkmark & 1.029 & \\textbf{23.8} & \\textbf{0.103} & 88.5& \\textbf{0.122} & \\textbf{85.5} \\\\\n& \\textbf{Ours} &  & \\checkmark & \\textbf{0.777} & 17.1 & 0.137 & \\textbf{94.7}& 0.191 & 73.8 \\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[t]\n\\centering\n\\caption{\\textbf{Camera Pose Estimation Evaluation} on ScanNet, Sintel, and TUM-dynamics datasets.} \n    \\vspace{-1mm}\n\\small\n\\renewcommand{\\arraystretch}{1.}\n\\renewcommand{\\tabcolsep}{1pt}\n \\resizebox{1\\textwidth}{!}{\n\\begin{tabular}{cccccc|ccc|ccc}\n\\toprule\n& & & \\multicolumn{3}{c}{\\textbf{ScanNet (Static)}} & \\multicolumn{3}{c}{\\textbf{Sintel}} & \\multicolumn{3}{c}{\\textbf{TUM-dynamics}} \\\\ \n\\cmidrule(lr){4-6} \\cmidrule(lr){7-9} \\cmidrule(lr){10-12}\n{\\textbf{Method}}  &  \\textbf{Optim.} & \\textbf{Onl.} & {ATE $\\downarrow$} & {RPE trans $\\downarrow$} & {RPE rot $\\downarrow$} & {ATE $\\downarrow$} & {RPE trans $\\downarrow$} & {RPE rot $\\downarrow$} & {ATE $\\downarrow$} & {RPE trans $\\downarrow$} & {RPE rot $\\downarrow$} \\\\ \n\\midrule\nRobust-CVD~\\cite{kopf2021robust} & \\checkmark & & 0.227 & 0.064 & 7.374 & 0.360 & 0.154 & 3.443 & 0.153 & 0.026 & 3.528\\\\\nCasualSAM~\\cite{zhang2022structure} & \\checkmark & & 0.158 & 0.034 & 1.618 & 0.141 & \\bf 0.035 & \\bf 0.615 & 0.071 & \\bf 0.010 & 1.712\\\\\nDUSt3R-GA~\\cite{wang2024dust3r} & \\checkmark & & {0.081} & 0.028 & 0.784 & 0.417 & 0.250 & 5.796 & 0.083 & 0.017 & 3.567 \\\\  \nMASt3R-GA~\\cite{mast3r} & \\checkmark & & {\\underline{0.078}} & {\\underline{0.020}} & {\\bf {0.475}} & \\underline{0.185} & {0.060} & {1.496} & {\\bf{0.038}} & {\\underline{0.012}} & {\\bf{0.448}} \\\\ \nMonST3R-GA~\\cite{zhang2024monst3r}  & \\checkmark & & {\\bf{0.077}} & {\\bf{0.018}} & {\\underline{0.529}}& \\bf {{0.111}} & \\underline{0.044} & {0.869} & {{0.098}} & {{0.019}} & 0.935 \\\\\nSpann3R~\\cite{wang20243d}& &\\checkmark & 0.096 & 0.023 & 0.661 & {{0.329}} & 0.110 & 4.471 & 0.056 & 0.021 & 0.591 \\\\ \nCUT3R~\\cite{cut3r} &  & \\checkmark & 0.099 & 0.022 & 0.600 & 0.213 & 0.066 & \\underline{0.621} & \\underline{0.046} & 0.015 & \\underline{0.473}\\\\ \n\\textbf{Ours} &  & \\checkmark & 0.106 & 0.035 & 1.946 & 0.351 & 0.128 & 1.822 & 0.075 & 0.029 & 0.642\\\\ \n\\bottomrule\n\\end{tabular}\n}\n\\label{tab:pose}\n    \\vspace{-3mm}\n\\end{table}", "caption": "\\caption{\\textbf{Camera Pose Estimation Evaluation} on ScanNet, Sintel, and TUM-dynamics datasets.}", "label": "\\label{tab:pose}", "tabular": "\\begin{tabular}{cccccc|ccc|ccc}\n\\toprule\n& & & \\multicolumn{3}{c}{\\textbf{ScanNet (Static)}} & \\multicolumn{3}{c}{\\textbf{Sintel}} & \\multicolumn{3}{c}{\\textbf{TUM-dynamics}} \\\\ \n\\cmidrule(lr){4-6} \\cmidrule(lr){7-9} \\cmidrule(lr){10-12}\n{\\textbf{Method}}  &  \\textbf{Optim.} & \\textbf{Onl.} & {ATE $\\downarrow$} & {RPE trans $\\downarrow$} & {RPE rot $\\downarrow$} & {ATE $\\downarrow$} & {RPE trans $\\downarrow$} & {RPE rot $\\downarrow$} & {ATE $\\downarrow$} & {RPE trans $\\downarrow$} & {RPE rot $\\downarrow$} \\\\ \n\\midrule\nRobust-CVD~\\cite{kopf2021robust} & \\checkmark & & 0.227 & 0.064 & 7.374 & 0.360 & 0.154 & 3.443 & 0.153 & 0.026 & 3.528\\\\\nCasualSAM~\\cite{zhang2022structure} & \\checkmark & & 0.158 & 0.034 & 1.618 & 0.141 & \\bf 0.035 & \\bf 0.615 & 0.071 & \\bf 0.010 & 1.712\\\\\nDUSt3R-GA~\\cite{wang2024dust3r} & \\checkmark & & {0.081} & 0.028 & 0.784 & 0.417 & 0.250 & 5.796 & 0.083 & 0.017 & 3.567 \\\\  \nMASt3R-GA~\\cite{mast3r} & \\checkmark & & {\\underline{0.078}} & {\\underline{0.020}} & {\\bf {0.475}} & \\underline{0.185} & {0.060} & {1.496} & {\\bf{0.038}} & {\\underline{0.012}} & {\\bf{0.448}} \\\\ \nMonST3R-GA~\\cite{zhang2024monst3r}  & \\checkmark & & {\\bf{0.077}} & {\\bf{0.018}} & {\\underline{0.529}}& \\bf {{0.111}} & \\underline{0.044} & {0.869} & {{0.098}} & {{0.019}} & 0.935 \\\\\nSpann3R~\\cite{wang20243d}& &\\checkmark & 0.096 & 0.023 & 0.661 & {{0.329}} & 0.110 & 4.471 & 0.056 & 0.021 & 0.591 \\\\ \nCUT3R~\\cite{cut3r} &  & \\checkmark & 0.099 & 0.022 & 0.600 & 0.213 & 0.066 & \\underline{0.621} & \\underline{0.046} & 0.015 & \\underline{0.473}\\\\ \n\\textbf{Ours} &  & \\checkmark & 0.106 & 0.035 & 1.946 & 0.351 & 0.128 & 1.822 & 0.075 & 0.029 & 0.642\\\\ \n\\bottomrule\n\\end{tabular}", "subtables": []}, {"original": "\\begin{table}[t]\n  \\centering\n  \\caption{\\textbf{Effects of the 3D hierarchical position embedding and the memory fusion mechanism.}} \n    \\vspace{-1mm}\n  \\small\n    \\begin{tabularx}{\\textwidth}{c >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X}\n      \\toprule\n\n          & \\multicolumn{3}{c}{\\textbf{7-scenes }} & \\multicolumn{3}{c}{\\textbf{NRGBD}}\\\\\n\n      \\cmidrule(lr){2-4} \\cmidrule(lr){5-7}\n         & \\multicolumn{1}{c}{{Acc}$\\downarrow$} & \\multicolumn{1}{c}{{Comp}$\\downarrow$} & \\multicolumn{1}{c}{{NC}$\\uparrow$} & \\multicolumn{1}{c}{{Acc}$\\downarrow$} & \\multicolumn{1}{c}{{Comp}$\\downarrow$} & \\multicolumn{1}{c}{{NC}$\\uparrow$}\\\\\n      \\midrule\n      Ours (w/o 3DHPE) & 0.180 & 0.180 & 0.683 & 0.145 &  0.123 & 0.770 \\\\\n      Ours (w/o Mem-Fusion) & \\bf 0.118 & 0.148 & 0.721 & \\bf 0.079 &  0.074 & \\bf 0.824 \\\\\n        Ours & 0.124 & \\bf{0.139} & \\bf 0.725 & \\bf 0.079 &  \\bf 0.073 & \\bf 0.824 \\\\\n      \\bottomrule\n    \\end{tabularx}\n    \\label{tab:ablation}\n    \\vspace{-5mm}\n\\end{table}", "caption": "\\caption{\\textbf{Effects of the 3D hierarchical position embedding and the memory fusion mechanism.}}", "label": "\\label{tab:ablation}", "tabular": "\\begin{tabularx}{\\textwidth}{c >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X}\n      \\toprule\n\n          & \\multicolumn{3}{c}{\\textbf{7-scenes }} & \\multicolumn{3}{c}{\\textbf{NRGBD}}\\\\\n\n      \\cmidrule(lr){2-4} \\cmidrule(lr){5-7}\n         & \\multicolumn{1}{c}{{Acc}$\\downarrow$} & \\multicolumn{1}{c}{{Comp}$\\downarrow$} & \\multicolumn{1}{c}{{NC}$\\uparrow$} & \\multicolumn{1}{c}{{Acc}$\\downarrow$} & \\multicolumn{1}{c}{{Comp}$\\downarrow$} & \\multicolumn{1}{c}{{NC}$\\uparrow$}\\\\\n      \\midrule\n      Ours (w/o 3DHPE) & 0.180 & 0.180 & 0.683 & 0.145 &  0.123 & 0.770 \\\\\n      Ours (w/o Mem-Fusion) & \\bf 0.118 & 0.148 & 0.721 & \\bf 0.079 &  0.074 & \\bf 0.824 \\\\\n        Ours & 0.124 & \\bf{0.139} & \\bf 0.725 & \\bf 0.079 &  \\bf 0.073 & \\bf 0.824 \\\\\n      \\bottomrule\n    \\end{tabularx}", "subtables": []}, {"original": "\\begin{table}[h]\n\\centering\n\\tiny\n\\vspace{1mm}\n\\setlength{\\tabcolsep}{10pt}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{l c c c c}\n\\toprule\n\\textbf{Dataset} & \\textbf{Scene Type} & \\textbf{Dynamic} & \\textbf{Real} & \\textbf{Metric} \\\\\n\\midrule\nARKitScenes~\\cite{baruch2021arkitscenes} & Indoor & Static & Real & Yes\\\\\nBlendedMVS~\\cite{yao2020blendedmvs} & Mixed & Static & Synthetic & No\\\\\nCO3Dv2~\\cite{reizenstein2021common} & Object-Centric & Static & Real & No\\\\\nHyperSim~\\cite{hypersim} & Indoor & Static & Synthetic & Yes\\\\\nMegaDepth~\\cite{li2018megadepth} & Outdoor & Static & Real & No\\\\\nOmniObject3D~\\cite{wu2023omniobject3d} & Object-Centric & Static & Synthetic & Yes\\\\\nScanNet~\\cite{dai2017scannet} & Indoor & Static & Real & Yes\\\\\nScanNet++~\\cite{yeshwanth2023scannet++} & Indoor & Static & Real & Yes\\\\\nWildRGBD~\\cite{xia2024rgbd} & Object-Centric & Static & Real & Yes\\\\\nMVS-Synth~\\cite{mvssynth} & Outdoor & Dynamic & Synthetic & Yes\\\\\nPointOdyssey~\\cite{zheng2023pointodyssey} & Mixed & Dynamic & Synthetic & Yes\\\\\nSpring~\\cite{mehl2023spring} & Mixed & Dynamic & Synthetic & Yes\\\\\nVirtualKITTI2~\\cite{cabon2020virtual} & Outdoor & Dynamic & Synthetic & Yes\\\\\nWaymo~\\cite{sun2020scalability} & Outdoor & Dynamic & Real & Yes\\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\vspace{2mm}\n\\caption{\\textbf{Training Datasets.}}\n\\label{tab:dataset_tab}\n\\end{table}", "caption": "\\caption{\\textbf{Training Datasets.}}", "label": "\\label{tab:dataset_tab}", "tabular": "\\begin{tabular}{l c c c c}\n\\toprule\n\\textbf{Dataset} & \\textbf{Scene Type} & \\textbf{Dynamic} & \\textbf{Real} & \\textbf{Metric} \\\\\n\\midrule\nARKitScenes~\\cite{baruch2021arkitscenes} & Indoor & Static & Real & Yes\\\\\nBlendedMVS~\\cite{yao2020blendedmvs} & Mixed & Static & Synthetic & No\\\\\nCO3Dv2~\\cite{reizenstein2021common} & Object-Centric & Static & Real & No\\\\\nHyperSim~\\cite{hypersim} & Indoor & Static & Synthetic & Yes\\\\\nMegaDepth~\\cite{li2018megadepth} & Outdoor & Static & Real & No\\\\\nOmniObject3D~\\cite{wu2023omniobject3d} & Object-Centric & Static & Synthetic & Yes\\\\\nScanNet~\\cite{dai2017scannet} & Indoor & Static & Real & Yes\\\\\nScanNet++~\\cite{yeshwanth2023scannet++} & Indoor & Static & Real & Yes\\\\\nWildRGBD~\\cite{xia2024rgbd} & Object-Centric & Static & Real & Yes\\\\\nMVS-Synth~\\cite{mvssynth} & Outdoor & Dynamic & Synthetic & Yes\\\\\nPointOdyssey~\\cite{zheng2023pointodyssey} & Mixed & Dynamic & Synthetic & Yes\\\\\nSpring~\\cite{mehl2023spring} & Mixed & Dynamic & Synthetic & Yes\\\\\nVirtualKITTI2~\\cite{cabon2020virtual} & Outdoor & Dynamic & Synthetic & Yes\\\\\nWaymo~\\cite{sun2020scalability} & Outdoor & Dynamic & Real & Yes\\\\\n\\bottomrule\n\\end{tabular}", "subtables": []}], "figure": [{"original": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/teaser.pdf}\n    \\vspace{-8mm}\n    \\caption{Comparison between our explicit spatial pointer memory and other paradigms in dense 3D reconstruction.\n    Methods that conduct all-to-all interaction among all inputs simultaneously~\\cite{yang2025fast3r, wang2025vggtvisualgeometrygrounded} can be considered as using \\textbf{other frames as memory} (for one of the inputs).\n    Methods that cache encoded features of processed frames and conduct token-image interaction~\\cite{wang20243d} can be considered as using \\textbf{past frames as memory}.\n    Methods maintaining a fixed-length state memory and conducting state-image interaction~\\cite{cut3r} can be considered as using \\textbf{implicit state memory}.\n    We propose an \\textbf{explicit spatial pointer memory} in which each pointer is assigned a 3D position and points to a changing spatial feature. We conduct a pointer-image interaction to integrate new observations into the global coordinate system and update our spatial pointer memory accordingly.\n}\n    \\label{fig:motivation}\n    \\vspace{-3mm}\n\\end{figure}", "caption": "\\caption{Comparison between our explicit spatial pointer memory and other paradigms in dense 3D reconstruction.\n    Methods that conduct all-to-all interaction among all inputs simultaneously~\\cite{yang2025fast3r, wang2025vggtvisualgeometrygrounded} can be considered as using \\textbf{other frames as memory} (for one of the inputs).\n    Methods that cache encoded features of processed frames and conduct token-image interaction~\\cite{wang20243d} can be considered as using \\textbf{past frames as memory}.\n    Methods maintaining a fixed-length state memory and conducting state-image interaction~\\cite{cut3r} can be considered as using \\textbf{implicit state memory}.\n    We propose an \\textbf{explicit spatial pointer memory} in which each pointer is assigned a 3D position and points to a changing spatial feature. We conduct a pointer-image interaction to integrate new observations into the global coordinate system and update our spatial pointer memory accordingly.\n}", "label": "\\label{fig:motivation}", "subfigures": [], "figure_paths": ["figures/teaser.pdf"]}, {"original": "\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/Main.pdf}\n    \\caption{\\textbf{Overview of Point3R.}\n    Given streaming image inputs, our method maintains an explicit spatial pointer memory to store the observed information of the current scene.\n    We use a ViT~\\cite{dosovitskiy2021an, wang2024dust3r} encoder to encode the current input into image tokens and use ViT-based decoders to conduct interaction between image tokens and spatial features in the memory.\n    We use two DPT~\\cite{ranftl21dpt} heads to decode local and global pointmaps from the output image tokens.\n    Besides, a learnable pose token is added during this stage so we can directly decode the camera parameters of the current frame.\n    Then we use a simple memory encoder to encode the current input and its integrated output into new pointers, and use a memory fusion mechanism to enrich and update our spatial pointer memory.}\n    \\label{fig:main}\n    \\vspace{-3mm}\n\\end{figure}", "caption": "\\caption{\\textbf{Overview of Point3R.}\n    Given streaming image inputs, our method maintains an explicit spatial pointer memory to store the observed information of the current scene.\n    We use a ViT~\\cite{dosovitskiy2021an, wang2024dust3r} encoder to encode the current input into image tokens and use ViT-based decoders to conduct interaction between image tokens and spatial features in the memory.\n    We use two DPT~\\cite{ranftl21dpt} heads to decode local and global pointmaps from the output image tokens.\n    Besides, a learnable pose token is added during this stage so we can directly decode the camera parameters of the current frame.\n    Then we use a simple memory encoder to encode the current input and its integrated output into new pointers, and use a memory fusion mechanism to enrich and update our spatial pointer memory.}", "label": "\\label{fig:main}", "subfigures": [], "figure_paths": ["figures/Main.pdf"]}, {"original": "\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/vis.pdf}\n    \\vspace{-3mm}\n    \\caption{\\textbf{Qualitative results on sparse inputs from the 7-scenes and NRGBD datasets.} Our method achieves the best qualitative results among memory-based methods.}\n    \\label{fig:vis}\n    \\vspace{-2mm}\n\\end{figure}", "caption": "\\caption{\\textbf{Qualitative results on sparse inputs from the 7-scenes and NRGBD datasets.} Our method achieves the best qualitative results among memory-based methods.}", "label": "\\label{fig:vis}", "subfigures": [], "figure_paths": ["figures/vis.pdf"]}, {"original": "\\begin{wrapfigure}{r}{0.5\\textwidth}\n\\vspace{-12mm}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/Ablation.pdf}\n    \\vspace{-7mm}\n    \\caption{Changes on \\textbf{the total number of pointers} and \\textbf{per-frame runtime} with memory fusion.}\n    \\label{fig:ablation_fig}\n\n\\end{wrapfigure}", "caption": "\\caption{Changes on \\textbf{the total number of pointers} and \\textbf{per-frame runtime} with memory fusion.}", "label": "\\label{fig:ablation_fig}", "subfigures": [], "figure_paths": ["figures/Ablation.pdf"]}, {"original": "\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/staticvis.pdf}\n    \\caption{\\textbf{Qualitative results on dense inputs from static scenes.}}\n    \\label{fig:staticvis}\n\\end{figure}", "caption": "\\caption{\\textbf{Qualitative results on dense inputs from static scenes.}}", "label": "\\label{fig:staticvis}", "subfigures": [], "figure_paths": ["figures/staticvis.pdf"]}, {"original": "\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/dynamicvis.pdf}\n    \\caption{\\textbf{Qualitative results on dense inputs from dynamic scenes.}}\n    \\label{fig:dynamicvis}\n\\end{figure}", "caption": "\\caption{\\textbf{Qualitative results on dense inputs from dynamic scenes.}}", "label": "\\label{fig:dynamicvis}", "subfigures": [], "figure_paths": ["figures/dynamicvis.pdf"]}], "equations": ["\\begin{equation}\n     {X_{t}}=\\mathcal{F}(\\mathcal{M}_{t-1}, {I_{t}}),\n\\end{equation}", "\\begin{equation}\n    {F}_{t}=\\mathrm{Encoder}({I}_{t}).\n\\end{equation}", "\\begin{align}\n    {F'_{0}}, {z'_{0}} &= \\mathrm{Decoders}(({F_{0}}, {z_{0}}), {M_{0}}), \\\\\n    {F'_{t}}, {z'_{t}} &= \\mathrm{Decoders}(({F_{t}}, {z_{t}}), {M_{t-1}}), \n\\end{align}", "\\begin{align}\n    {\\hat{T}_{t}}&=\\mathrm{Head}_{pose}({z'_{t}}), \\\\\n    {\\hat{X}_{t}}^{self}, {C_{t}}^{self}&=\\mathrm{Head}_{self}({F'_{t}}), \\\\\n    {\\hat{X}_{t}}^{global}, {C_{t}}^{global}&=\\mathrm{Head}_{global}({F'_{t}}, {z'_{t}}),\n\\end{align}", "\\begin{align}\n    {P}_{new}(u,v)&=\\frac{1}{\\left | {R}_{u,v} \\right |} \\sum_{(i,j)\\in {R}_{u,v}}{\\hat{X}_{t}}^{global}(i,j), \\\\\n    {M}_{new}&=\\mathrm{Encoder}_{f}({F_{t}}, {F'_{t}}) + \\mathrm{Encoder}_{geo}({\\hat{X}_{t}}^{global}),\n\\end{align}", "\\begin{equation}\n    p'=\\frac{1}{K}\\sum_{i=1}^{K} p_{i}^{new}, m'= \\frac{1}{K}\\sum_{i=1}^{K} m_{i}^{new},\n\\end{equation}", "\\begin{equation}\n\\label{rope_theta}\n    \\theta_{t}=10000^{-t/(d_{head}/2)}, \\mathrm{where}\\ t\\in \\{0,1,...,d_{head}/2\\}.\n\\end{equation}", "\\begin{equation}\n\\label{eq_rotation}\n    \\mathbf{R}(n,t)=e^{i\\theta_{t}n},\n\\end{equation}", "\\begin{equation}\n    \\bar{\\mathbf{q}}' = \\bar{\\mathbf{q}} \\circ\\mathbf{R}, \\ \\bar{\\mathbf{k}}' = \\bar{\\mathbf{k}} \\circ\\mathbf{R},\\ \\mathbf{A}'=\\mathrm{Re}[\\bar{\\mathbf{q}}'\\bar{\\mathbf{k}}'^{*}].\n\\end{equation}", "\\begin{equation}\n\\label{eq_theta}\n    \\mathbf{R}(n,3t)=e^{i\\theta_{t}p_{n}^{x}}, \\ \\mathbf{R}(n,3t+1)=e^{i\\theta_{t}p_{n}^{y}}, \\ \\mathbf{R}(n,3t+2)=e^{i\\theta_{t}p_{n}^{z}}.\n\\end{equation}", "\\begin{equation}\n\\label{h_rot}\n    \\bar{\\mathbf{q}}' = \\frac{1}{h}\\sum_{i=1}^{h}(\\bar{\\mathbf{q}} \\circ\\mathbf{R}_{i}), \\ \\bar{\\mathbf{k}}' =\\frac{1}{h}\\sum_{i=1}^{h}(\\bar{\\mathbf{k}} \\circ\\mathbf{R}_{i}),\\ \\mathbf{A}'=\\mathrm{Re}[\\bar{\\mathbf{q}}'\\bar{\\mathbf{k}}'^{*}]. \n\\end{equation}", "\\begin{equation}\n    {p}(u,v)=\\frac{1}{\\left | {R}_{u,v} \\right |} \\sum_{(i,j)\\in {R}_{u,v}}{\\hat{X}}_{t-1}^{global}(i,j),\n\\end{equation}", "\\begin{align}\n    \\mathcal{L}_{pose}&=\\sum_{t=1}^{N}\\left ( \\left \\| {\\hat{q}_{t}}-{{q}_{t}} \\right \\|_{2}+ \\left \\| {\\frac{\\hat{\\tau}_{t}}{\\hat{s}}-\\frac{{\\tau}_{t}}{{s}} } \\right \\|_{2}  \\right ), \\\\\n    \\mathcal{L}_{conf}&=\\sum_{(\\hat{x},c)\\in(\\hat{\\mathcal{X}}, \\mathcal{C})}\\left ( c\\ \\cdot\\left \\| \\frac{\\hat{x}}{\\hat{s}}-\\frac{{x}}{{s}}  \\right \\|_{2}-\\alpha \\log c     \\right ),\n\\end{align}", "\\begin{equation}\n    \\delta = \\sqrt{(\\frac{\\max P_{t-1}^{x}-\\min P_{t-1}^{x}}{l_{x}} )^{2} + (\\frac{\\max P_{t-1}^{y}-\\min P_{t-1}^{y}}{l_{y}} )^{2}+(\\frac{\\max P_{t-1}^{z}-\\min P_{t-1}^{z}}{l_{z}} )^{2}},\n\\end{equation}", "\\begin{equation}\n    \\theta_{t}=b^{-t/(d_{head}/6)}, \\mathrm{where}\\ t\\in \\{0,1,...,d_{head}/6\\}, b \\in \\{10,100,1000,10000\\}.\n\\end{equation}", "\\begin{equation}\n    \\bar{\\mathbf{q}}' = \\frac{1}{4}\\sum_{i=1}^{4}(\\bar{\\mathbf{q}} \\circ\\mathbf{R}_{i}), \\ \\bar{\\mathbf{k}}' =\\frac{1}{4}\\sum_{i=1}^{4}(\\bar{\\mathbf{k}} \\circ\\mathbf{R}_{i}),\\ \\mathbf{A}'=\\mathrm{Re}[\\bar{\\mathbf{q}}'\\bar{\\mathbf{k}}'^{*}].\n\\end{equation}"], "algorithm": [], "sections": {"Introduction": {"content": "\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/teaser.pdf}\n    \\vspace{-8mm}\n    \\caption{Comparison between our explicit spatial pointer memory and other paradigms in dense 3D reconstruction.\n    Methods that conduct all-to-all interaction among all inputs simultaneously~\\cite{yang2025fast3r, wang2025vggtvisualgeometrygrounded} can be considered as using \\textbf{other frames as memory} (for one of the inputs).\n    Methods that cache encoded features of processed frames and conduct token-image interaction~\\cite{wang20243d} can be considered as using \\textbf{past frames as memory}.\n    Methods maintaining a fixed-length state memory and conducting state-image interaction~\\cite{cut3r} can be considered as using \\textbf{implicit state memory}.\n    We propose an \\textbf{explicit spatial pointer memory} in which each pointer is assigned a 3D position and points to a changing spatial feature. We conduct a pointer-image interaction to integrate new observations into the global coordinate system and update our spatial pointer memory accordingly.\n}\n    \\label{fig:motivation}\n    \\vspace{-3mm}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/teaser.pdf}\n    \\vspace{-8mm}\n    \\caption{Comparison between our explicit spatial pointer memory and other paradigms in dense 3D reconstruction.\n    Methods that conduct all-to-all interaction among all inputs simultaneously~\\cite{yang2025fast3r, wang2025vggtvisualgeometrygrounded} can be considered as using \\textbf{other frames as memory} (for one of the inputs).\n    Methods that cache encoded features of processed frames and conduct token-image interaction~\\cite{wang20243d} can be considered as using \\textbf{past frames as memory}.\n    Methods maintaining a fixed-length state memory and conducting state-image interaction~\\cite{cut3r} can be considered as using \\textbf{implicit state memory}.\n    We propose an \\textbf{explicit spatial pointer memory} in which each pointer is assigned a 3D position and points to a changing spatial feature. We conduct a pointer-image interaction to integrate new observations into the global coordinate system and update our spatial pointer memory accordingly.\n}\n    \\label{fig:motivation}\n    \\vspace{-3mm}\n\n\nDense 3D reconstruction from image collections has long been a fundamental task in computer vision, with broad applications in fields such as autonomous driving~\\cite{huang2023tri, zheng2024occworld}, medical modeling, and cultural heritage preservation. \nConventional approaches~\\cite{sweeney2015optimizing, agarwal2011building, crandall2012sfm, wu2013towards, schoenberger2016sfm, wilson2014robust} first conduct a sequence of sub-tasks, including feature extraction and matching~\\cite{lowe1999object, oliensis2000critique}, triangulation, and global alignment to get sparse geometry and camera poses.\nMulti-view stereo~\\cite{furukawa2009accurate, colmapmvs, gu2020cascade} is then used to obtain dense geometry.\nHowever, this tightly coupled pipeline is inefficient and tends to be vulnerable to noise.\nTo address these challenges, DUSt3R~\\cite{wang2024dust3r} proposes a novel data-driven paradigm that directly aligns the input image pair and reconstructs it as point maps within a unified coordinate system.\n\nDue to the constraint of pair-wise input, DUSt3R requires an additional global alignment step when performing dense reconstruction from multiple images, which is inefficient in multi-view settings. \nTo improve this, subsequent works~\\cite{yang2025fast3r, wang2025vggtvisualgeometrygrounded, wang20243d, cut3r} can be categorized into two main paradigms.\nOne category of these works~\\cite{yang2025fast3r, wang2025vggtvisualgeometrygrounded} takes all input images simultaneously and employs global attention to reconstruct them into a unified coordinate system, requiring substantial computational resources and is misaligned with the incremental nature of real-world reconstruction scenarios. \nThe second category~\\cite{wang20243d, cut3r} introduces an external memory mechanism to retain information from past frames, enabling each new input to be directly integrated into the global coordinate system. \nFor instance, Spann3R~\\cite{wang20243d} maintains a memory that essentially caches implicit features of processed frames. \nHowever, this implicit memory often contains redundant information, and a discard strategy must be employed once memory capacity is saturated. \nCUT3R~\\cite{cut3r} uses a fixed-length token-based memory mechanism and directly updates the memory through interactions with image features.\nNonetheless, as the number of processed frames increases, this fixed-capacity memory inevitably leads to the loss of earlier information.\n\nInspired by the human memory mechanism, we propose \\textbf{Point3R}, an online framework equipped with a spatial pointer memory. \nHuman memory of previously encountered environments is inherently related to spatial locations. \nFor example, when we talk about a caf\u00e9 or our bedroom, the images we recall are distinct. \nSimilarly, each 3D pointer in our spatial pointer memory is assigned a 3D position in the global coordinate system. \nEach 3D pointer is directly linked to an explored spatial location and points to a dynamically updated spatial feature. \nLeveraging this 3D-aware structure, we introduce a 3D hierarchical position embedding, which is integrated into the interaction module between current image features and stored spatial features, enabling more efficient and structured memory querying. \nFurthermore, since the spatial pointer memory expands as the scene exploration progresses, we design a simple yet effective fusion mechanism to ensure that the memory remains spatially uniform and efficient. \nOur spatial pointer memory evolves in sync with the current scene, allowing our model to handle both static and dynamic scenes. \nWe use Figure~\\ref{fig:motivation} to compare our spatial pointer memory with other paradigms mentioned before.\nOur method achieves competitive or state-of-the-art performance across various tasks: dense 3D reconstruction, monocular and video depth estimation, and camera pose estimation. \nIt is worth mentioning that although trained on a variety of datasets, the training of our method has a low cost in time and computational resources.\n", "appendix": false}, "Related Work": {"content": "\n\n\\textbf{Conventional 3D Reconstruction:}\nClassic approaches to 3D reconstruction from image collections are typically optimization-based and tailored to specific scenes. \nStructure-from-motion (SfM)~\\cite{agarwal2011building, crandall2012sfm, wu2013towards, wilson2014robust, schoenberger2016sfm, sweeney2015optimizing} follows a pipeline consisting of feature extraction~\\cite{lowe2004distinctive, rublee2011orb, dusmanu2019d2}, image matching~\\cite{wu2013towards, lindenberger2023lightglue, shi2022clustergnn, chen2021learning}, triangulation to 3D, and bundle adjustment~\\cite{agarwal2010bundle, triggs1999bundle} to obtain sparse geometry and estimated camera poses. \nBuilding upon this, subsequent methods such as Multi-view Stereo (MVS)~\\cite{furukawa2015multi, colmapmvs, fu2022geo, Wei_2021_ICCV}, Neural Radiance Fields (NeRF)~\\cite{mildenhall2021nerf, chen2022tensorf, muller2022instant, wang2021neus, barron2022mip}, and 3D Gaussian Splatting (3DGS)~\\cite{kerbl3Dgaussians} leverage known camera parameters to recover dense geometry or high-fidelity scene representation. \nThese approaches rely on a sequential combination of multiple modules, which not only require considerable optimization time but are also vulnerable to noise. \nIt is worth noting that Simultaneous Localization and Mapping (SLAM)~\\cite{davison2007monoslam, engel2014lsd, newcombe2011dtam, klein2007parallel} can perform localization and reconstruction in an online manner. \nHowever, they often rely on specific camera motion assumptions~\\cite{newcombe2011dtam, davison2007monoslam} (sometimes these motion assumptions may be misleading or restrictive) or require additional depth/LiDAR sensors~\\cite{newcombe2011kinectfusion} for better performance. \n\n\\textbf{Learning-Based 3D Reconstruction:}\nTo enhance accuracy and efficiency, the field of 3D reconstruction is gradually shifting toward learning-based and end-to-end paradigms. \nSome approaches utilize learnable modules to replace hand-crafted components~\\cite{detone2018self, sarlin2020superglue} during the traditional reconstruction pipeline.\nSome others try to optimize the overall pipeline in an end-to-end manner~\\cite{tang2018ba, wang2024vggsfm, yao2018mvsnet}.\nDUSt3R~\\cite{wang2024dust3r} introduces a pointmap representation and directly learns to integrate an image pair into the same coordinate system, which unifies all sub-tasks we have mentioned above.\nMonST3R~\\cite{zhang2024monst3r} extends this paradigm to dynamic scenes by fine-tuning it on dynamic datasets.\nThe pair-wise formulation needs a global alignment to process more views.\nSubsequent works~\\cite{wang20243d, cut3r, yang2025fast3r, wang2025vggtvisualgeometrygrounded} are exploring how to further replace the global optimization step with an end-to-end learning framework. \nThese efforts can be broadly categorized into two main branches.\nThe former~\\cite{yang2025fast3r, wang2025vggtvisualgeometrygrounded} feeds all images simultaneously and leverages global attention to reconstruct the scene within a unified coordinate system; the latter~\\cite{wang20243d, cut3r} proposes a streaming paradigm, in which a memory module stores information from past frames, thereby enabling online incremental reconstruction from sequential inputs.\nThe streaming paradigm aligns more closely with real-world applications, offering improved scalability without imposing excessive computational demands.\n\n\\textbf{Streaming Reconstruction and Memory Mechanism:}\nThe streaming reconstruction paradigm and the memory mechanism are inherently aligned with each other. \nExisting streaming reconstruction methods~\\cite{engel2014lsd, choy20163d, zhang2022nerfusion, wang20243d, cut3r} universally incorporate a certain form of memory to store information from past frames. \nThis memory can take on various forms, such as explicit scene representation (the most direct form of memory), recurrent neural network architectures~\\cite{choy20163d}, and encoded or learnable token features~\\cite{wang20243d, cut3r}.\nExplicit 3D scene representation is compact and efficient but tailored to a specific scene, limiting its generalizability. \nSpann3R~\\cite{wang20243d} stores implicit features from previous frames in the memory, which may lead to redundancy. \nCUT3R~\\cite{cut3r} employs a fixed-length set of learnable token features as its memory module, which is continuously updated during sequential processing. \nHowever, its limited capacity can lead to information loss.\nIn contrast, we propose a spatial pointer memory, in which each pointer is dynamically assigned a 3D position. \nThis ensures that the total amount of stored information scales naturally with the extent of the explored scene. \nFurthermore, each pointer has a spatial feature that captures aggregated scene information nearby.\n\n\n", "appendix": false}, "Proposed Approach": {"content": "\n\\label{all_method}\n\n\\subsection{Memory-Based Streaming 3D Reconstruction}\n\nTo densely reconstruct image collections \\(\\mathbf{I} \\in  \\mathbb{R}^{N\\times H\\times W\\times 3} \\)\\mathbf{I} \\in  \\mathbb{R}^{N\\times H\\times W\\times 3}N\\times H\\times W\\times 3  into a unified global coordinate system as per-frame pointmaps \\(\\mathbf{X} \\in  \\mathbb{R}^{N\\times H\\times W\\times 3} \\)\\mathbf{X} \\in  \\mathbb{R}^{N\\times H\\times W\\times 3}N\\times H\\times W\\times 3 , existing methods can be categorized into three paradigms: \n(1) pair-wise reconstruction~\\cite{wang2024dust3r, zhang2024monst3r, mast3r} with an optimization-based global alignment, \n(2) one-shot global reconstruction~\\cite{yang2025fast3r, wang2025vggtvisualgeometrygrounded} with all inputs, \nand (3) frame-by-frame input and reconstruction~\\cite{cut3r, wang20243d} based on a memory mechanism. \nPair-wise methods take only one image pair each time as input and reconstruct it into a local coordinate system. \nA global alignment is then conducted to merge all local outputs into a global coordinate system. \nThis approach suffers from inefficiency due to the need for repeated pair-wise processing and a post-processing stage.\nThe second category feeds all images into the model simultaneously and employs a global attention to reconstruct them directly in a shared coordinate system. \nHowever, this approach is computationally intensive and inherently mismatched with the core demands of embodied agents, which typically perform streaming perception of their surroundings and respond correspondingly in practical scenarios.\n\nIn pursuit of a balance between practicality and efficiency, the key idea of memory-based methods is maintaining a memory that stores observed information and interacts with each incoming frame to enable streaming dense reconstruction. \nWe formulate the memory-based pipeline as follows:\n\\begin{equation}\n     {X_{t}}=\\mathcal{F}(\\mathcal{M}_{t-1}, {I_{t}}),\n\\end{equation}\\begin{equation}\n     {X_{t}}=\\mathcal{F}(\\mathcal{M}_{t-1}, {I_{t}}),\n\\end{equation}\n     {X_{t}}X_{t}t=\\mathcal{F}(\\mathcal{M}_{t-1}t-1, {I_{t}}I_{t}t),\n\nwhere \\({I_{t}} \\in \\mathbb{R} ^{H\\times W\\times 3}\\){I_{t}}I_{t}t \\in \\mathbb{R} ^{H\\times W\\times 3}H\\times W\\times 3 and \\({X_{t}} \\in \\mathbb{R} ^{H\\times W\\times 3}\\){X_{t}}X_{t}t \\in \\mathbb{R} ^{H\\times W\\times 3}H\\times W\\times 3 are the current image input and pointmap output, \\(\\mathcal{F}\\)\\mathcal{F} is a certain approach and \\(\\mathcal{M}_{t-1}\\)\\mathcal{M}_{t-1}t-1 represents a memory with integrated information from the past frames.\n\nTo elaborate, in Spann3R~\\cite{wang20243d}, \\(\\mathcal{M}\\)\\mathcal{M} is a growing set of key-value-pair features, where the features are encoded from the output of each previous frame.\nIn CUT3R~\\cite{cut3r}, \\(\\mathcal{M}\\)\\mathcal{M} takes the form of a fixed-length feature sequence that is iteratively updated as new frame comes.\nIn this work, we propose an explicit spatial pointer memory \\(\\mathcal{M}\\)\\mathcal{M} that stores a set of 3D pointers corresponding to the explored regions of the current scene, along with their associated spatial features.\nWe argue that this explicit memory enables compact and structured integration of information from past observations. \nEach pointer is indexed by a 3D position in the global coordinate system, rather than implicit features, making the interaction between the memory and the current frame more direct and efficient.\n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/Main.pdf}\n    \\caption{\\textbf{Overview of Point3R.}\n    Given streaming image inputs, our method maintains an explicit spatial pointer memory to store the observed information of the current scene.\n    We use a ViT~\\cite{dosovitskiy2021an, wang2024dust3r} encoder to encode the current input into image tokens and use ViT-based decoders to conduct interaction between image tokens and spatial features in the memory.\n    We use two DPT~\\cite{ranftl21dpt} heads to decode local and global pointmaps from the output image tokens.\n    Besides, a learnable pose token is added during this stage so we can directly decode the camera parameters of the current frame.\n    Then we use a simple memory encoder to encode the current input and its integrated output into new pointers, and use a memory fusion mechanism to enrich and update our spatial pointer memory.}\n    \\label{fig:main}\n    \\vspace{-3mm}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/Main.pdf}\n    \\caption{\\textbf{Overview of Point3R.}\n    Given streaming image inputs, our method maintains an explicit spatial pointer memory to store the observed information of the current scene.\n    We use a ViT~\\cite{dosovitskiy2021an, wang2024dust3r} encoder to encode the current input into image tokens and use ViT-based decoders to conduct interaction between image tokens and spatial features in the memory.\n    We use two DPT~\\cite{ranftl21dpt} heads to decode local and global pointmaps from the output image tokens.\n    Besides, a learnable pose token is added during this stage so we can directly decode the camera parameters of the current frame.\n    Then we use a simple memory encoder to encode the current input and its integrated output into new pointers, and use a memory fusion mechanism to enrich and update our spatial pointer memory.}\n    \\label{fig:main}\n    \\vspace{-3mm}\n\n\n\\subsection{Pointer-Image Interaction}\nThe core of our method is making the interaction between the current input and our spatial pointer memory more effective and efficient.\nWe will elaborate on the overall model architecture, which is composed of an image encoder, interaction decoders, a memory encoder, and a memory fusion mechanism.\nWe also incorporate a 3D hierarchical position embedding into the interaction decoders to promote our pointer-image interaction.\nFigure~\\ref{fig:main} shows the overview of our method.\n\n\\textbf{Image encoder.}\nFor each frame, we use a ViT~\\cite{dosovitskiy2021an} to encode the current input \\({I_{t}}\\){I_{t}}I_{t}t into image tokens \\({F_{t}}\\){F_{t}}F_{t}t:\n\\begin{equation}\n    {F}_{t}=\\mathrm{Encoder}({I}_{t}).\n\\end{equation}\\begin{equation}\n    {F}_{t}=\\mathrm{Encoder}({I}_{t}).\n\\end{equation}\n    {F}F_{t}t=\\mathrm{Encoder}({I}I_{t}t).\n\n\n\\textbf{Interaction decoders.}\nOur spatial pointer memory \\(\\mathcal{M}\\)\\mathcal{M} consists of a set of 3D pointers (3D positions \\({P}\\){P}P and spatial features \\({M}\\){M}M).\nBefore processing the first frame, the memory has not yet stored any global spatial features of the current scene.\nTherefore, we use a simple layer to embed the image tokens \\({F}_0\\){F}F_0 of the first frame, and use the output \\({M}_0\\){M}M_0 to initialize the features of our memory.\nIt is worth noting that, since the precise spatial positions represented by these features are not yet known at this time, each feature has not been assigned a specific 3D position.\nThen we use two intertwined decoders~\\cite{weinzaepfel2022croco, wang2024dust3r} to enable interaction between the current image tokens and the memory:\n\\begin{align}\n    {F'_{0}}, {z'_{0}} &= \\mathrm{Decoders}(({F_{0}}, {z_{0}}), {M_{0}}), \\\\\n    {F'_{t}}, {z'_{t}} &= \\mathrm{Decoders}(({F_{t}}, {z_{t}}), {M_{t-1}}), \n\\end{align}\\begin{align}\n    {F'_{0}}, {z'_{0}} &= \\mathrm{Decoders}(({F_{0}}, {z_{0}}), {M_{0}}), \\\\\n    {F'_{t}}, {z'_{t}} &= \\mathrm{Decoders}(({F_{t}}, {z_{t}}), {M_{t-1}}), \n\\end{align}\n    {F'_{0}}F'_{0}0, {z'_{0}}z'_{0}0 &= \\mathrm{Decoders}(({F_{0}}F_{0}0, {z_{0}}z_{0}0), {M_{0}}M_{0}0), \\\\\n    {F'_{t}}F'_{t}t, {z'_{t}}z'_{t}t &= \\mathrm{Decoders}(({F_{t}}F_{t}t, {z_{t}}z_{t}t), {M_{t-1}}M_{t-1}t-1), \n\nwhere we use a learnable pose token \\({z}\\){z}z as a bridge between the current frame and the global coordinate system. \nAfter this interaction, we get the updated image tokens \\({F'_{t}}\\){F'_{t}}F'_{t}t and pose token \\({z'_{t}}\\){z'_{t}}z'_{t}t.\nThen we use \\({F'_{t}}\\){F'_{t}}F'_{t}t and \\({z'_{t}}\\){z'_{t}}z'_{t}t to predict two pointmaps (\\({\\hat{X}_{t}}^{self}\\){\\hat{X}_{t}}\\hat{X}_{t}t^{self}self in the local coordinate system of the current input and \\({\\hat{X}_{t}}^{global}\\){\\hat{X}_{t}}\\hat{X}_{t}t^{global}global in the global coordinate system) with their own confidence maps (\\({C_{t}}^{self}\\){C_{t}}C_{t}t^{self}selfand \\({C_{t}}^{global}\\){C_{t}}C_{t}t^{global}global), and a camera pose \\({\\hat{T}_{t}}\\){\\hat{T}_{t}}\\hat{T}_{t}t representing the rigid transformation between this two coordinate system:\n\\begin{align}\n    {\\hat{T}_{t}}&=\\mathrm{Head}_{pose}({z'_{t}}), \\\\\n    {\\hat{X}_{t}}^{self}, {C_{t}}^{self}&=\\mathrm{Head}_{self}({F'_{t}}), \\\\\n    {\\hat{X}_{t}}^{global}, {C_{t}}^{global}&=\\mathrm{Head}_{global}({F'_{t}}, {z'_{t}}),\n\\end{align}\\begin{align}\n    {\\hat{T}_{t}}&=\\mathrm{Head}_{pose}({z'_{t}}), \\\\\n    {\\hat{X}_{t}}^{self}, {C_{t}}^{self}&=\\mathrm{Head}_{self}({F'_{t}}), \\\\\n    {\\hat{X}_{t}}^{global}, {C_{t}}^{global}&=\\mathrm{Head}_{global}({F'_{t}}, {z'_{t}}),\n\\end{align}\n    {\\hat{T}_{t}}\\hat{T}_{t}t&=\\mathrm{Head}_{pose}pose({z'_{t}}z'_{t}t), \\\\\n    {\\hat{X}_{t}}\\hat{X}_{t}t^{self}self, {C_{t}}C_{t}t^{self}self&=\\mathrm{Head}_{self}self({F'_{t}}F'_{t}t), \\\\\n    {\\hat{X}_{t}}\\hat{X}_{t}t^{global}global, {C_{t}}C_{t}t^{global}global&=\\mathrm{Head}_{global}global({F'_{t}}F'_{t}t, {z'_{t}}z'_{t}t),\n\nwhere \\(\\mathrm{Head}_{pose}\\)\\mathrm{Head}_{pose}pose is an MLP network, \\(\\mathrm{Head}_{self}\\)\\mathrm{Head}_{self}self and \\(\\mathrm{Head}_{global}\\)\\mathrm{Head}_{global}global are DPT~\\cite{ranftl21dpt} heads. The global coordinate system is actually the first input's own coordinate system.\n\n\\textbf{Memory encoder.}\nAfter processing \\({I}_{t}\\){I}I_{t}t, we use the current features \\({F_{t}}, {F'_{t}}\\){F_{t}}F_{t}t, {F'_{t}}F'_{t}tand the predicted pointmap \\({\\hat{X}_{t}}^{global}\\){\\hat{X}_{t}}\\hat{X}_{t}t^{global}global to obtain the new pointers:\n\\begin{align}\n    {P}_{new}(u,v)&=\\frac{1}{\\left | {R}_{u,v} \\right |} \\sum_{(i,j)\\in {R}_{u,v}}{\\hat{X}_{t}}^{global}(i,j), \\\\\n    {M}_{new}&=\\mathrm{Encoder}_{f}({F_{t}}, {F'_{t}}) + \\mathrm{Encoder}_{geo}({\\hat{X}_{t}}^{global}),\n\\end{align}\\begin{align}\n    {P}_{new}(u,v)&=\\frac{1}{\\left | {R}_{u,v} \\right |} \\sum_{(i,j)\\in {R}_{u,v}}{\\hat{X}_{t}}^{global}(i,j), \\\\\n    {M}_{new}&=\\mathrm{Encoder}_{f}({F_{t}}, {F'_{t}}) + \\mathrm{Encoder}_{geo}({\\hat{X}_{t}}^{global}),\n\\end{align}\n    {P}P_{new}new(u,v)&=\\frac{1}{\\left | {R}_{u,v} \\right |} \\sum_{(i,j)\\in {R}_{u,v}}(i,j)\\in {R}R_{u,v}u,v{\\hat{X}_{t}}\\hat{X}_{t}t^{global}global(i,j), \\\\\n    {M}M_{new}new&=\\mathrm{Encoder}_{f}f({F_{t}}F_{t}t, {F'_{t}}F'_{t}t) + \\mathrm{Encoder}_{geo}geo({\\hat{X}_{t}}\\hat{X}_{t}t^{global}global),\n\nwhere \\({M}_{new}\\){M}M_{new}new is the set of new spatial features and we compute the 3D location \\({P}_{new}(u,v)\\){P}P_{new}new(u,v) for each feature in the global coordinate system as its 3D position by averaging all 3D coordinates within the corresponding patch \\({R}_{u,v}\\){R}R_{u,v}u,v.\nBesides, \\(\\mathrm{Encoder}_{f}\\)\\mathrm{Encoder}_{f}f is a MLP and \\(\\mathrm{Encoder}_{geo}\\)\\mathrm{Encoder}_{geo}geo is a lightweight ViT.\n\n\\textbf{Memory fusion mechanism.}\nApart from the first frame when we simply add all the obtained pointers into the memory, new pointers extracted from each subsequent frame \\({I}_{t}\\){I}I_{t}t are integrated into the existing memory \\(\\mathcal{M}_{t-1}\\)\\mathcal{M}_{t-1}t-1 through a fusion mechanism.\nTo elaborate, we compute the Euclidean distance between each new pointer and all existing pointers from the memory to identify its nearest neighbor. \nIf the distance to its nearest neighbor in the memory is below a changing threshold \\(\\delta \\)\\delta  (we change \\(\\delta \\)\\delta  accordingly to make the distribution of memory units more uniform), we treat the neighbors as corresponding and perform the fusion. \nOtherwise, the new pointer is directly added to the memory. \nNotably, if a memory pointer is identified as the nearest neighbor by one or a few new pointers, we update the position \\(p\\)p and spatial feature \\(m\\)m of this pointer as follows:\n\\begin{equation}\n    p'=\\frac{1}{K}\\sum_{i=1}^{K} p_{i}^{new}, m'= \\frac{1}{K}\\sum_{i=1}^{K} m_{i}^{new},\n\\end{equation}\\begin{equation}\n    p'=\\frac{1}{K}\\sum_{i=1}^{K} p_{i}^{new}, m'= \\frac{1}{K}\\sum_{i=1}^{K} m_{i}^{new},\n\\end{equation}\n    p'=\\frac{1}{K}\\sum_{i=1}i=1^{K}K p_{i}i^{new}new, m'= \\frac{1}{K}\\sum_{i=1}i=1^{K}K m_{i}i^{new}new,\n\nwhere \\(K\\)K is the total number of new neighbors of the target pointer.\nThis fusion mechanism ensures that each pointer always stores the current spatial information at its location, thereby enabling the memory to deal with dynamic scenes.\nIn this way, we obtain an enriched and updated memory \\(\\mathcal{M}_{t}\\)\\mathcal{M}_{t}t.\n\n\\textbf{3D hierarchical position embedding.}\nWe expand the rotary position embedding (RoPE~\\cite{su2024roformer, heo2024rotary}, a method of relative position embedding usually used in transformers) to a 3D hierarchical position embedding and use this to conduct position embedding in continuous 3D space.\nIn practical implementation, RoPE utilizes multiple frequencies \\(\\theta_{t}\\)\\theta_{t}t using channel dimensions \\(d_{head}\\)d_{head}head of key and query as\n\\begin{equation}\n\\label{rope_theta}\n    \\theta_{t}=10000^{-t/(d_{head}/2)}, \\mathrm{where}\\ t\\in \\{0,1,...,d_{head}/2\\}.\n\\end{equation}\\begin{equation}\n\\label{rope_theta}\n    \\theta_{t}=10000^{-t/(d_{head}/2)}, \\mathrm{where}\\ t\\in \\{0,1,...,d_{head}/2\\}.\n\\end{equation}\n\\label{rope_theta}\n    \\theta_{t}t=10000^{-t/(d_{head}/2)}-t/(d_{head}head/2), \\mathrm{where}\\ t\\in \\{0,1,...,d_{head}head/2\\}.\n\nThen a rotation matrix \\(\\mathbf{R}\\in \\mathbb{C}^{N\\times (d_{head}/2)}\\)\\mathbf{R}\\in \\mathbb{C}^{N\\times (d_{head}/2)}N\\times (d_{head}head/2) is defined as \n\\begin{equation}\n\\label{eq_rotation}\n    \\mathbf{R}(n,t)=e^{i\\theta_{t}n},\n\\end{equation}\\begin{equation}\n\\label{eq_rotation}\n    \\mathbf{R}(n,t)=e^{i\\theta_{t}n},\n\\end{equation}\n\\label{eq_rotation}\n    \\mathbf{R}(n,t)=e^{i\\theta_{t}n}i\\theta_{t}tn,\n\nand applied to query and key with the Hadamard product \\(\\circ\\)\\circ as\n\\begin{equation}\n    \\bar{\\mathbf{q}}' = \\bar{\\mathbf{q}} \\circ\\mathbf{R}, \\ \\bar{\\mathbf{k}}' = \\bar{\\mathbf{k}} \\circ\\mathbf{R},\\ \\mathbf{A}'=\\mathrm{Re}[\\bar{\\mathbf{q}}'\\bar{\\mathbf{k}}'^{*}].\n\\end{equation}\\begin{equation}\n    \\bar{\\mathbf{q}}' = \\bar{\\mathbf{q}} \\circ\\mathbf{R}, \\ \\bar{\\mathbf{k}}' = \\bar{\\mathbf{k}} \\circ\\mathbf{R},\\ \\mathbf{A}'=\\mathrm{Re}[\\bar{\\mathbf{q}}'\\bar{\\mathbf{k}}'^{*}].\n\\end{equation}\n    \\bar{\\mathbf{q}}' = \\bar{\\mathbf{q}} \\circ\\mathbf{R}, \\ \\bar{\\mathbf{k}}' = \\bar{\\mathbf{k}} \\circ\\mathbf{R},\\ \\mathbf{A}'=\\mathrm{Re}[\\bar{\\mathbf{q}}'\\bar{\\mathbf{k}}'^{*}*].\n\nHere, the attention matrix with RoPE \\(\\mathbf{A}'\\)\\mathbf{A}' implies relative position in rotation form \\(e^{i(n-m)\\theta_{t}}\\)e^{i(n-m)\\theta_{t}}i(n-m)\\theta_{t}t.\nInspired by this, we formulate a 3D hierarchical position embedding.\nWe need to change the 1D token index \\(n\\)n in RoPE to a 3D token position \\({p}_{n}=({p}_{n}^{x}, {p}_{n}^{y}, {p}_{n}^{z})\\){p}p_{n}n=({p}p_{n}n^{x}x, {p}p_{n}n^{y}y, {p}p_{n}n^{z}z) where \\({p}_{n}^{x}\\){p}p_{n}n^{x}x, \\({p}_{n}^{y}\\){p}p_{n}n^{y}y and \\({p}_{n}^{z}\\){p}p_{n}n^{z}z correspond to the coordinates on three axes in a continuous 3D space.\nThus, the rotation matrix \\(\\mathbf{R}\\in \\mathbb{C}^{N\\times (d_{head}/2)}\\)\\mathbf{R}\\in \\mathbb{C}^{N\\times (d_{head}/2)}N\\times (d_{head}head/2) in Eq.~\\ref{eq_rotation} is changed accordingly as\n\\begin{equation}\n\\label{eq_theta}\n    \\mathbf{R}(n,3t)=e^{i\\theta_{t}p_{n}^{x}}, \\ \\mathbf{R}(n,3t+1)=e^{i\\theta_{t}p_{n}^{y}}, \\ \\mathbf{R}(n,3t+2)=e^{i\\theta_{t}p_{n}^{z}}.\n\\end{equation}\\begin{equation}\n\\label{eq_theta}\n    \\mathbf{R}(n,3t)=e^{i\\theta_{t}p_{n}^{x}}, \\ \\mathbf{R}(n,3t+1)=e^{i\\theta_{t}p_{n}^{y}}, \\ \\mathbf{R}(n,3t+2)=e^{i\\theta_{t}p_{n}^{z}}.\n\\end{equation}\n\\label{eq_theta}\n    \\mathbf{R}(n,3t)=e^{i\\theta_{t}p_{n}^{x}}i\\theta_{t}tp_{n}n^{x}x, \\ \\mathbf{R}(n,3t+1)=e^{i\\theta_{t}p_{n}^{y}}i\\theta_{t}tp_{n}n^{y}y, \\ \\mathbf{R}(n,3t+2)=e^{i\\theta_{t}p_{n}^{z}}i\\theta_{t}tp_{n}n^{z}z.\n\nTo accommodate spatial position inputs of varying scales, we use \\(h\\)h different frequency bases (10000 in Eq.~\\ref{rope_theta}) to derive hierarchical rotation matrices and apply them to query and key as\n\\begin{equation}\n\\label{h_rot}\n    \\bar{\\mathbf{q}}' = \\frac{1}{h}\\sum_{i=1}^{h}(\\bar{\\mathbf{q}} \\circ\\mathbf{R}_{i}), \\ \\bar{\\mathbf{k}}' =\\frac{1}{h}\\sum_{i=1}^{h}(\\bar{\\mathbf{k}} \\circ\\mathbf{R}_{i}),\\ \\mathbf{A}'=\\mathrm{Re}[\\bar{\\mathbf{q}}'\\bar{\\mathbf{k}}'^{*}]. \n\\end{equation}\\begin{equation}\n\\label{h_rot}\n    \\bar{\\mathbf{q}}' = \\frac{1}{h}\\sum_{i=1}^{h}(\\bar{\\mathbf{q}} \\circ\\mathbf{R}_{i}), \\ \\bar{\\mathbf{k}}' =\\frac{1}{h}\\sum_{i=1}^{h}(\\bar{\\mathbf{k}} \\circ\\mathbf{R}_{i}),\\ \\mathbf{A}'=\\mathrm{Re}[\\bar{\\mathbf{q}}'\\bar{\\mathbf{k}}'^{*}]. \n\\end{equation}\n\\label{h_rot}\n    \\bar{\\mathbf{q}}' = \\frac{1}{h}\\sum_{i=1}i=1^{h}h(\\bar{\\mathbf{q}} \\circ\\mathbf{R}_{i}i), \\ \\bar{\\mathbf{k}}' =\\frac{1}{h}\\sum_{i=1}i=1^{h}h(\\bar{\\mathbf{k}} \\circ\\mathbf{R}_{i}i),\\ \\mathbf{A}'=\\mathrm{Re}[\\bar{\\mathbf{q}}'\\bar{\\mathbf{k}}'^{*}*]. \n\nWe use this hierarchical position embedding in our interaction decoders to inject relative position information into image tokens and memory features.\nWhen applying the rotation matrices in Eq.~\\ref{h_rot}, the 3D token position of each memory feature is its 3D position.\nFor image tokens from \\({I}_{t}\\){I}I_{t}t, the 3D token position assigned to each of them is as follows:\n\\begin{equation}\n    {p}(u,v)=\\frac{1}{\\left | {R}_{u,v} \\right |} \\sum_{(i,j)\\in {R}_{u,v}}{\\hat{X}}_{t-1}^{global}(i,j),\n\\end{equation}\\begin{equation}\n    {p}(u,v)=\\frac{1}{\\left | {R}_{u,v} \\right |} \\sum_{(i,j)\\in {R}_{u,v}}{\\hat{X}}_{t-1}^{global}(i,j),\n\\end{equation}\n    {p}p(u,v)=\\frac{1}{\\left | {R}_{u,v} \\right |} \\sum_{(i,j)\\in {R}_{u,v}}(i,j)\\in {R}R_{u,v}u,v{\\hat{X}}\\hat{X}_{t-1}t-1^{global}global(i,j),\n\nwhere \\({R}_{u,v}\\){R}R_{u,v}u,v is the corresponding image patch and we use \\({\\hat{X}}_{t-1}^{global}\\){\\hat{X}}\\hat{X}_{t-1}t-1^{global}global from \\(t-1\\)t-1 because we assume that the image tokens of the current frame are more likely to be spatially close to those of the previous frame. \nOf course, even in cases of significant pose shifts or unordered inputs, this assumption will not introduce any adverse effects. \nThis is because the image tokens of the current frame interact with all memory features in our interaction decoders, and our hierarchical position embedding merely provides a potential prior.\nDue to the space limit, we will add more details about the design and implementation of our 3D hierarchical position embedding in the supplementary material.\n\n\\subsection{Training Strategy}\n\n\\textbf{Training objective.}\nFollowing MASt3R~\\cite{mast3r} and CUT3R~\\cite{cut3r}, we use the L2 norm loss for the poses and a confidence-aware loss for the pointmaps.\nIn practical implementation, we parameterize the predicted pose \\({\\hat{T}_{t}}\\){\\hat{T}_{t}}\\hat{T}_{t}t as quaternion \\({\\hat{q}_{t}}\\){\\hat{q}_{t}}\\hat{q}_{t}t and translation \\({\\hat{\\tau}_{t}}\\){\\hat{\\tau}_{t}}\\hat{\\tau}_{t}t. \nWe use \\(\\hat{\\mathcal{X}}= \\{\\hat{\\mathcal{X}}^{self},\\hat{\\mathcal{X}}^{global}\\}\\)\\hat{\\mathcal{X}}= \\{\\hat{\\mathcal{X}}^{self}self,\\hat{\\mathcal{X}}^{global}global\\} to denote the predicted pointmaps, where \\(\\hat{\\mathcal{X}}^{self}=\\{{\\hat{X}}_{t}^{self}\\}_{t=1}^{N} \\)\\hat{\\mathcal{X}}^{self}self=\\{{\\hat{X}}\\hat{X}_{t}t^{self}self\\}_{t=1}t=1^{N}N , \\(\\hat{\\mathcal{X}}^{global}=\\{{\\hat{X}}_{t}^{global}\\}_{t=1}^{N} \\)\\hat{\\mathcal{X}}^{global}global=\\{{\\hat{X}}\\hat{X}_{t}t^{global}global\\}_{t=1}t=1^{N}N  and \\(N\\)N is the number of images per sequence.\nBesides, \\(\\mathcal{C}\\)\\mathcal{C} is used to denote the set of confidence scores correspondingly.\nSo the final expression of the loss we used is:\n\\begin{align}\n    \\mathcal{L}_{pose}&=\\sum_{t=1}^{N}\\left ( \\left \\| {\\hat{q}_{t}}-{{q}_{t}} \\right \\|_{2}+ \\left \\| {\\frac{\\hat{\\tau}_{t}}{\\hat{s}}-\\frac{{\\tau}_{t}}{{s}} } \\right \\|_{2}  \\right ), \\\\\n    \\mathcal{L}_{conf}&=\\sum_{(\\hat{x},c)\\in(\\hat{\\mathcal{X}}, \\mathcal{C})}\\left ( c\\ \\cdot\\left \\| \\frac{\\hat{x}}{\\hat{s}}-\\frac{{x}}{{s}}  \\right \\|_{2}-\\alpha \\log c     \\right ),\n\\end{align}\\begin{align}\n    \\mathcal{L}_{pose}&=\\sum_{t=1}^{N}\\left ( \\left \\| {\\hat{q}_{t}}-{{q}_{t}} \\right \\|_{2}+ \\left \\| {\\frac{\\hat{\\tau}_{t}}{\\hat{s}}-\\frac{{\\tau}_{t}}{{s}} } \\right \\|_{2}  \\right ), \\\\\n    \\mathcal{L}_{conf}&=\\sum_{(\\hat{x},c)\\in(\\hat{\\mathcal{X}}, \\mathcal{C})}\\left ( c\\ \\cdot\\left \\| \\frac{\\hat{x}}{\\hat{s}}-\\frac{{x}}{{s}}  \\right \\|_{2}-\\alpha \\log c     \\right ),\n\\end{align}\n    \\mathcal{L}_{pose}pose&=\\sum_{t=1}t=1^{N}N\\left ( \\left \\| {\\hat{q}_{t}}\\hat{q}_{t}t-{{q}_{t}}{q}q_{t}t \\right \\|_{2}2+ \\left \\| {\\frac{\\hat{\\tau}_{t}}{\\hat{s}}-\\frac{{\\tau}_{t}}{{s}} }\\frac{\\hat{\\tau}_{t}}{\\hat{s}}-\\frac{{\\tau}_{t}}{{s}}  \\right \\|_{2}2  \\right ), \\\\\n    \\mathcal{L}_{conf}conf&=\\sum_{(\\hat{x},c)\\in(\\hat{\\mathcal{X}}, \\mathcal{C})}(\\hat{x},c)\\in(\\hat{\\mathcal{X}}, \\mathcal{C})\\left ( c\\ \\cdot\\left \\| \\frac{\\hat{x}}{\\hat{s}}-\\frac{{x}}{{s}}  \\right \\|_{2}2-\\alpha \\log c     \\right ),\n\nwhere \\(\\hat{s}\\)\\hat{s} and \\(s\\)s are scale normalization factors for \\(\\hat{\\mathcal{X}}\\)\\hat{\\mathcal{X}} and \\(\\mathcal{X}\\)\\mathcal{X}. When the groundtruth pointmaps are metric, we set \\(\\hat{s}:=s \\)\\hat{s}:=s  to force the model to learn metric-scale results.\n\n\\textbf{Training datasets.}\nDuring training, we use a combination of 14 datasets, including ARKitScenes~\\cite{baruch2021arkitscenes}, ScanNet~\\cite{dai2017scannet}, ScanNet++~\\cite{yeshwanth2023scannet++}, CO3Dv2~\\cite{reizenstein2021common}, WildRGBD~\\cite{xia2024rgbd}, OmniObject3D~\\cite{wu2023omniobject3d}, HyperSim (a subset of it)~\\cite{hypersim}, BlendedMVS~\\cite{yao2020blendedmvs}, MegaDepth~\\cite{li2018megadepth}, Waymo~\\cite{sun2020scalability}, VirtualKITTI2~\\cite{cabon2020virtual}, PointOdyssey~\\cite{zheng2023pointodyssey}, Spring~\\cite{mehl2023spring}, and MVS-Synth~\\cite{mvssynth}.\nThese datasets exhibit highly diverse characteristics, encompassing both indoor and outdoor, static and dynamic, as well as real-world and synthetic scenes.\nSee the supplementary material for more details.\n\n\\textbf{Training stages.}\nOur model is trained in three stages.\nWe train the model by sampling 5 frames per sequence in the first stage. \nThe input here is 224$\\times$\\times224 resolution.\nThen we use input with different aspect ratios (set the maximum side to 512) in the second stage, following CUT3R~\\cite{cut3r}.\nAnd finally, we freeze the encoder and fine-tune other parts on 8-frame sequences.\n\n\\textbf{Implementation details.}\nWe initialize our ViT-Large~\\cite{wang2024dust3r, dosovitskiy2021an} image encoder, ViT-Base interaction decoders~\\cite{wang2024dust3r, weinzaepfel2022croco}, and DPT~\\cite{ranftl21dpt} heads with pre-trained weights from DUSt3R~\\cite{wang2024dust3r}.\nOur memory encoder is composed of a light-weight ViT (6 blocks) and a 2-layer MLP.\nEach memory feature has a dimensionality of 768.\nWe use the AdamW optimizer~\\cite{loshchilov2017decoupled} and the learning rate warms up to a maximum value of 5e-5 and decreases according to a cosine schedule.\nWe train our model on 8 H100 NVIDIA GPUs for 7 days, which is a relatively low cost.\n", "appendix": false}, "Experiments": {"content": "\n\\label{all_exp}\n\n\\textbf{Tasks and Baselines.}\nWe use various 3D/4D tasks (dense 3D reconstruction, monocular depth estimation, video depth estimation, and camera pose estimation) to evaluate our method.\nWe choose DUSt3R~\\cite{wang2024dust3r}, MASt3R~\\cite{mast3r}, MonST3R~\\cite{zhang2024monst3r}, Spann3R~\\cite{wang20243d}, and CUT3R~\\cite{cut3r} as our primary baselines.\nAmong these methods, DUSt3R, MASt3R, and MonST3R take an image pair as input, so an optimization-based global alignment (GA) stage is conducted when dealing with streaming inputs.\nBoth Spann3R and CUT3R have a memory module so they can process an image sequence in an online manner, similar to our method.\n\n\\subsection{3D Reconstruction}\nWe evaluate the 3D reconstruction performance on the 7-scenes~\\cite{shotton2013scene} and NRGBD~\\cite{azinovic2022neural} datasets in Table~\\ref{tab:3d_recon}, and the metrics we used include accuracy (Acc), completion (Comp), and normal consistency (NC), following previous works~\\cite{wang2024dust3r, wang20243d, azinovic2022neural}.\nWe use inputs with minimal overlap~\\cite{cut3r}: 3 to 5 frames per scene for the 7-scenes datasets and 2 to 4 frames per scene for the NRGBD dataset.\nSuch sparsely sampled inputs can directly demonstrate the effectiveness of our proposed pointer memory, which is explicitly associated with 3D spatial locations and does not rely on similarity or continuity between input frames.\nAs shown in Table~\\ref{tab:3d_recon}, our method achieves comparable or better results than other memory-based online approaches or even DUSt3R-GA~\\cite{wang2024dust3r}.\nWe compare the reconstruction quality of our method with other memory-based online approaches, Spann3R~\\cite{wang20243d} and CUT3R~\\cite{cut3r} in Figure~\\ref{fig:vis}, and our method achieves state-of-the-art reconstruction performance with sparse inputs from the 7-scenes and NRGBD datasets.\n\n\\begin{table}[t]\n  \\centering\n  \\caption{\\textbf{Quantitative 3D reconstruction results on 7-scenes and NRGBD datasets.} We use ``GA'' to mark methods with global alignment, and use ``Optim.'' and ``Onl.'' to distinguish between optimization-based and online methods~\\cite{cut3r}. Our method achieves competitive or better performance than those optimization-based methods and current online methods.} \n  \\small\n  \\setlength{\\tabcolsep}{0.1em}\n    \\begin{tabularx}{\\textwidth}{c c c >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >\n    {\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >\n    {\\centering\\arraybackslash}X >\n    {\\centering\\arraybackslash}X}\n      \\toprule\n\n           & &  & \\multicolumn{6}{c}{\\textbf{7-scenes }} & \\multicolumn{6}{c}{\\textbf{NRGBD}}\\\\\n\n      \\cmidrule(lr){4-9} \\cmidrule(lr){10-15}\n         & & & \\multicolumn{2}{c}{{Acc}$\\downarrow$} & \\multicolumn{2}{c}{{Comp}$\\downarrow$} & \\multicolumn{2}{c}{{NC}$\\uparrow$} & \\multicolumn{2}{c}{{Acc}$\\downarrow$} & \\multicolumn{2}{c}{{Comp}$\\downarrow$} & \\multicolumn{2}{c}{{NC}$\\uparrow$}\\\\\n      \\cmidrule(lr){4-5} \\cmidrule(lr){6-7} \\cmidrule(lr){8-9} \\cmidrule(lr){10-11} \\cmidrule(lr){12-13} \\cmidrule(lr){14-15}\n         \\textbf{Method} & \\textbf{Optim.} & \\textbf{Onl.} & {Mean} & {Med.} & {Mean} & {Med.} & {Mean} & {Med.} & {Mean} & {Med.} & {Mean} & {Med.} & {Mean} & {Med.}\\\\\n      \\midrule\n        DUSt3R-GA~\\cite{wang2024dust3r} &\\checkmark &\n        & 0.146\t& 0.077\t& {0.181}\t& 0.067\t& \\bf{0.736}\t& \\bf{0.839}\t&  0.144\t& \\bf{0.019}\t& 0.154\t& \\bf{0.018}\t& \\bf{0.870}\t& \\bf{0.982}\\\\\n        MASt3R-GA~\\cite{mast3r} &\\checkmark &\n        & 0.185 & 0.081 & 0.180 & 0.069 & 0.701\t& 0.792\t& \\underline{0.085} & 0.033\t& \\bf 0.063\t& 0.028\t& 0.794 & 0.928\\\\\n        MonST3R-GA~\\cite{zhang2024monst3r} &\\checkmark &\n        & 0.248 & 0.185 & 0.266 & 0.167 & 0.672 & 0.759 & 0.272 & 0.114 & 0.287 & 0.110 & 0.758 & 0.843\\\\\n        Spann3R~\\cite{wang20243d} & & \\checkmark\n        & 0.298 & 0.226 & 0.205 &  0.112 & {0.650} & {0.730}   & {0.416}  & 0.323  &  {0.417}  & {0.285}  & 0.684  & 0.789\\\\\n        CUT3R~\\cite{cut3r} & & \\checkmark & \\underline{0.126} & \\bf{0.047} & \\underline{0.154} & \\bf{0.031} & \\underline{0.727} & \\underline{0.834} & 0.099 &  \\underline{0.031} & 0.076 & \\underline{0.026} & \\underline{0.837} & \\underline{0.971}\\\\\n        \\textbf{Ours} & & \\checkmark & \\bf 0.124 & \\underline{0.058} & \\bf{0.139} & \\underline{0.054} & 0.725 & \\underline{0.834} & \\textbf{0.079} &  \\underline{0.031} & \\underline{0.073} & 0.027 & 0.824 & 0.965 \\\\\n      \\bottomrule\n    \\end{tabularx}\n    \\label{tab:3d_recon}\n    \\vspace{-3mm}\n\\end{table}\n  \\centering\n  \\caption{\\textbf{Quantitative 3D reconstruction results on 7-scenes and NRGBD datasets.} We use ``GA'' to mark methods with global alignment, and use ``Optim.'' and ``Onl.'' to distinguish between optimization-based and online methods~\\cite{cut3r}. Our method achieves competitive or better performance than those optimization-based methods and current online methods.} \n  \\small\n  \\setlength{\\tabcolsep}{0.1em}\n    \n      \\toprule\n\n           & &  & \\multicolumn{6}6{c}c{\\textbf{7-scenes }}\\textbf{7-scenes } & \\multicolumn{6}6{c}c{\\textbf{NRGBD}}\\textbf{NRGBD}\\\\\n\n      \\cmidrule(lr){4-9}4-9 \\cmidrule(lr){10-15}10-15\n         & & & \\multicolumn{2}2{c}c{{Acc}$\\downarrow$}{Acc}Acc$\\downarrow$\\downarrow & \\multicolumn{2}2{c}c{{Comp}$\\downarrow$}{Comp}Comp$\\downarrow$\\downarrow & \\multicolumn{2}2{c}c{{NC}$\\uparrow$}{NC}NC$\\uparrow$\\uparrow & \\multicolumn{2}2{c}c{{Acc}$\\downarrow$}{Acc}Acc$\\downarrow$\\downarrow & \\multicolumn{2}2{c}c{{Comp}$\\downarrow$}{Comp}Comp$\\downarrow$\\downarrow & \\multicolumn{2}2{c}c{{NC}$\\uparrow$}{NC}NC$\\uparrow$\\uparrow\\\\\n      \\cmidrule(lr){4-5}4-5 \\cmidrule(lr){6-7}6-7 \\cmidrule(lr){8-9}8-9 \\cmidrule(lr){10-11}10-11 \\cmidrule(lr){12-13}12-13 \\cmidrule(lr){14-15}14-15\n         \\textbf{Method} & \\textbf{Optim.} & \\textbf{Onl.} & {Mean}Mean & {Med.}Med. & {Mean}Mean & {Med.}Med. & {Mean}Mean & {Med.}Med. & {Mean}Mean & {Med.}Med. & {Mean}Mean & {Med.}Med. & {Mean}Mean & {Med.}Med.\\\\\n      \\midrule\n        DUSt3R-GA~\\cite{wang2024dust3r} &\\checkmark &\n        & 0.146\t& 0.077\t& {0.181}0.181\t& 0.067\t& \\bf{0.736}0.736\t& \\bf{0.839}0.839\t&  0.144\t& \\bf{0.019}0.019\t& 0.154\t& \\bf{0.018}0.018\t& \\bf{0.870}0.870\t& \\bf{0.982}0.982\\\\\n        MASt3R-GA~\\cite{mast3r} &\\checkmark &\n        & 0.185 & 0.081 & 0.180 & 0.069 & 0.701\t& 0.792\t& \\underline{0.085} & 0.033\t& \\bf 0.063\t& 0.028\t& 0.794 & 0.928\\\\\n        MonST3R-GA~\\cite{zhang2024monst3r} &\\checkmark &\n        & 0.248 & 0.185 & 0.266 & 0.167 & 0.672 & 0.759 & 0.272 & 0.114 & 0.287 & 0.110 & 0.758 & 0.843\\\\\n        Spann3R~\\cite{wang20243d} & & \\checkmark\n        & 0.298 & 0.226 & 0.205 &  0.112 & {0.650}0.650 & {0.730}0.730   & {0.416}0.416  & 0.323  &  {0.417}0.417  & {0.285}0.285  & 0.684  & 0.789\\\\\n        CUT3R~\\cite{cut3r} & & \\checkmark & \\underline{0.126} & \\bf{0.047}0.047 & \\underline{0.154} & \\bf{0.031}0.031 & \\underline{0.727} & \\underline{0.834} & 0.099 &  \\underline{0.031} & 0.076 & \\underline{0.026} & \\underline{0.837} & \\underline{0.971}\\\\\n        \\textbf{Ours} & & \\checkmark & \\bf 0.124 & \\underline{0.058} & \\bf{0.139}0.139 & \\underline{0.054} & 0.725 & \\underline{0.834} & \\textbf{0.079} &  \\underline{0.031} & \\underline{0.073} & 0.027 & 0.824 & 0.965 \\\\\n      \\bottomrule\n    \n    \\label{tab:3d_recon}\n    \\vspace{-3mm}\n\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/vis.pdf}\n    \\vspace{-3mm}\n    \\caption{\\textbf{Qualitative results on sparse inputs from the 7-scenes and NRGBD datasets.} Our method achieves the best qualitative results among memory-based methods.}\n    \\label{fig:vis}\n    \\vspace{-2mm}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/vis.pdf}\n    \\vspace{-3mm}\n    \\caption{\\textbf{Qualitative results on sparse inputs from the 7-scenes and NRGBD datasets.} Our method achieves the best qualitative results among memory-based methods.}\n    \\label{fig:vis}\n    \\vspace{-2mm}\n\n\n\\subsection{Monocular and Video Depth Estimation}\n\n\\textbf{Monocular Depth Estimation.}\nWe evaluate zero-shot monocular depth estimation~\\cite{zhang2024monst3r, cut3r} performance on NYU-v2~\\cite{silberman2012indoor} (static), Sintel~\\cite{butler2012naturalistic}, Bonn~\\cite{palazzolo2019refusion}, and KITTI~\\cite{geiger2013vision} datasets.\nWe adopt per-frame median scaling following DUSt3R~\\cite{wang2024dust3r}, and the evaluation metrics we used include absolute relative error (Abs Rel) and percentage of inlier points \\(\\delta<1.25\\)\\delta<1.25.\nAs shown in Table~\\ref{tab:monodepth}, our method achieves state-of-the-art or competitive performance in both static and dynamic, indoor and outdoor scenes.\n\n\\begin{table}[t]\n\\centering\n\\caption{\\textbf{Monocular Depth Evaluation} on NYU-v2 (static), Sintel, Bonn, and KITTI datasets.}\n\\small\n\\renewcommand{\\arraystretch}{0.95}\n\\renewcommand{\\tabcolsep}{2.pt}\n\\begin{tabular}{ccc|cc|cc|cc}\n\\toprule\n  & \\multicolumn{2}{c}{\\textbf{NYU-v2 (Static)}}&\\multicolumn{2}{c}{\\textbf{Sintel}} & \\multicolumn{2}{c}{\\textbf{Bonn}} & \\multicolumn{2}{c}{\\textbf{KITTI}}\\\\ \n\\cmidrule(lr){2-3} \\cmidrule(lr){4-5} \\cmidrule(lr){6-7} \\cmidrule(lr){8-9}\n {\\textbf{Method}} & {Abs Rel $\\downarrow$} & {$\\delta$\\textless{}$1.25\\uparrow$} & {Abs Rel $\\downarrow$} & { $\\delta$\\textless{}$1.25\\uparrow$} & {Abs Rel $\\downarrow$} & {$\\delta$\\textless{}$1.25\\uparrow$} & {Abs Rel $\\downarrow$} & {$\\delta$\\textless{}$1.25\\uparrow$} \\\\ \n\\midrule\nDUSt3R~\\cite{wang2024dust3r} & \\underline{0.080} & \\underline{90.7} & 0.424 & \\underline{58.7} & 0.141 & 82.5 & 0.112 & 86.3 \\\\ \nMASt3R~\\cite{mast3r} & {0.129} & 84.9 & \\textbf{0.340} & \\textbf{60.4} & {0.142} & {82.0} & \\textbf{0.079} & \\textbf{94.7}\\\\ \nMonST3R~\\cite{zhang2024monst3r} & 0.102 & 88.0 & \\underline{0.358} & 54.8 & 0.076 & 93.9 & {0.100} & {89.3} \\\\ \nSpann3R~\\cite{wang20243d}  & 0.122 & 84.9 & {0.470} & 53.9 & {0.118} & {85.9} & {0.128} & {84.6} \\\\ \nCUT3R~\\cite{cut3r} & 0.086 & 90.9 & {0.428} & {55.4} & \\underline{0.063} & \\bf{96.2} & 0.092 & 91.3 \\\\ \n\\textbf{Ours} & \\bf{0.079} & \\textbf{92.0} & {0.395} & {56.8} & \\bf{0.061} & \\underline{95.4} & \\underline{0.087} & \\underline{93.7} \\\\ \n\\bottomrule\n\\end{tabular}\n\\label{tab:monodepth}\n    \\vspace{-3mm}\n\\end{table}\n\\centering\n\\caption{\\textbf{Monocular Depth Evaluation} on NYU-v2 (static), Sintel, Bonn, and KITTI datasets.}\n\\small\n\\renewcommand{\\arraystretch}{0.95}\n\\renewcommand{\\tabcolsep}{2.pt}\n\n\\toprule\n  & \\multicolumn{2}2{c}c{\\textbf{NYU-v2 (Static)}}\\textbf{NYU-v2 (Static)}&\\multicolumn{2}2{c}c{\\textbf{Sintel}}\\textbf{Sintel} & \\multicolumn{2}2{c}c{\\textbf{Bonn}}\\textbf{Bonn} & \\multicolumn{2}2{c}c{\\textbf{KITTI}}\\textbf{KITTI}\\\\ \n\\cmidrule(lr){2-3}2-3 \\cmidrule(lr){4-5}4-5 \\cmidrule(lr){6-7}6-7 \\cmidrule(lr){8-9}8-9\n {\\textbf{Method}}\\textbf{Method} & {Abs Rel $\\downarrow$}Abs Rel $\\downarrow$\\downarrow & {$\\delta$\\textless{}$1.25\\uparrow$}$\\delta$\\delta\\textless{}$1.25\\uparrow$1.25\\uparrow & {Abs Rel $\\downarrow$}Abs Rel $\\downarrow$\\downarrow & { $\\delta$\\textless{}$1.25\\uparrow$} $\\delta$\\delta\\textless{}$1.25\\uparrow$1.25\\uparrow & {Abs Rel $\\downarrow$}Abs Rel $\\downarrow$\\downarrow & {$\\delta$\\textless{}$1.25\\uparrow$}$\\delta$\\delta\\textless{}$1.25\\uparrow$1.25\\uparrow & {Abs Rel $\\downarrow$}Abs Rel $\\downarrow$\\downarrow & {$\\delta$\\textless{}$1.25\\uparrow$}$\\delta$\\delta\\textless{}$1.25\\uparrow$1.25\\uparrow \\\\ \n\\midrule\nDUSt3R~\\cite{wang2024dust3r} & \\underline{0.080} & \\underline{90.7} & 0.424 & \\underline{58.7} & 0.141 & 82.5 & 0.112 & 86.3 \\\\ \nMASt3R~\\cite{mast3r} & {0.129}0.129 & 84.9 & \\textbf{0.340} & \\textbf{60.4} & {0.142}0.142 & {82.0}82.0 & \\textbf{0.079} & \\textbf{94.7}\\\\ \nMonST3R~\\cite{zhang2024monst3r} & 0.102 & 88.0 & \\underline{0.358} & 54.8 & 0.076 & 93.9 & {0.100}0.100 & {89.3}89.3 \\\\ \nSpann3R~\\cite{wang20243d}  & 0.122 & 84.9 & {0.470}0.470 & 53.9 & {0.118}0.118 & {85.9}85.9 & {0.128}0.128 & {84.6}84.6 \\\\ \nCUT3R~\\cite{cut3r} & 0.086 & 90.9 & {0.428}0.428 & {55.4}55.4 & \\underline{0.063} & \\bf{96.2}96.2 & 0.092 & 91.3 \\\\ \n\\textbf{Ours} & \\bf{0.079}0.079 & \\textbf{92.0} & {0.395}0.395 & {56.8}56.8 & \\bf{0.061}0.061 & \\underline{95.4} & \\underline{0.087} & \\underline{93.7} \\\\ \n\\bottomrule\n\n\\label{tab:monodepth}\n    \\vspace{-3mm}\n\n\n\\textbf{Video Depth Estimation.}\nWe align predicted depth maps to ground truth using a per-sequence scale (Per-sequence alignment) to evaluate per-frame quality and inter-frame consistency.\nWe also compare results without alignment with other metric pointmap methods like MASt3R~\\cite{mast3r} and CUT3R~\\cite{cut3r} (Metric-scale alignment).\nAs shown in Table~\\ref{tab:videodepth}, with the per-sequence scale alignment, our method outperforms DUSt3R~\\cite{wang2024dust3r}, MASt3R~\\cite{mast3r}, and Spann3R~\\cite{wang20243d} by a large margin. \nThese methods have a static scene assumption and are trained only on static datasets.\nOur spatial pointer memory directly associates spatial features with their real-world positions and imposes no assumptions or dependencies on whether the scene is static or dynamic.\nOur method performs comparably, or even better than MonST3R-GA~\\cite{zhang2024monst3r} and CUT3R~\\cite{cut3r} (methods trained on dynamic datasets).\nBesides, in the metric-scale setting, our method outperforms MASt3R-GA~\\cite{mast3r} and performs comparably with CUT3R~\\cite{cut3r}, leading on Sintel and Bonn.\n\n\\begin{table}[t]\n\\centering\n\\caption{\\textbf{Video Depth Evaluation.} \nWe compare scale-invariant depth (per-sequence alignment) and metric depth (no alignment) results on Sintel, Bonn, and KITTI datasets.}\n\\small\n\\renewcommand{\\arraystretch}{1.0}\n\\renewcommand{\\tabcolsep}{1.4pt}\n\\begin{tabular}\n{@{}cccccccccc@{}}\n\\toprule\n &  &  &  & \\multicolumn{2}{c}{\\textbf{Sintel}} & \\multicolumn{2}{c}{\\textbf{BONN}} & \\multicolumn{2}{c}{\\textbf{KITTI}}\\\\ \n\\cmidrule(lr){5-6} \\cmidrule(lr){7-8} \\cmidrule(lr){9-10}\n\\textbf{Alignment} & \\textbf{Method} & \\textbf{Optim.} & \\textbf{Onl.\\ } & {Abs Rel $\\downarrow$} & {$\\delta$\\textless $1.25\\uparrow$} & {Abs Rel $\\downarrow$} & {$\\delta$\\textless $1.25\\uparrow$} & {Abs Rel $\\downarrow$} & {$\\delta$ \\textless $1.25\\uparrow$} \\\\ \n\\midrule\n\\multirow{6}{*}{\\begin{minipage}[c]{1.5cm}Per-sequence\\end{minipage}} \n& DUSt3R-GA~\\cite{wang2024dust3r} &  \\checkmark & & 0.656 & {45.2} & {0.155} & {83.3} & 0.144 & 81.3 \\\\\n& MASt3R-GA~\\cite{mast3r} &   \\checkmark & & 0.641 & {43.9} & {0.252} & {70.1} & {0.183} & {74.5} \\\\\n& MonST3R-GA~\\cite{zhang2024monst3r} &  \\checkmark &  & \\textbf{0.378} & \\textbf{55.8} & \\underline{0.067} & \\textbf{96.3} & {0.168} & {74.4} \\\\\n& Spann3R~\\cite{wang20243d}&  & \\checkmark & 0.622 & {42.6} & {0.144} & {81.3} & {0.198} & {73.7} \\\\\n& CUT3R~\\cite{cut3r} &  & \\checkmark & \\underline{0.421}  & 47.9 & 0.078 & 93.7 & \\textbf{0.118} & \\textbf{88.1} \\\\\n& \\textbf{Ours} &  & \\checkmark & 0.452  & \\underline{48.9} & \\bf{0.060} & \\underline{96.0} & \\underline{0.136} & \\underline{84.2} \\\\\n\\midrule\n\\multirow{3}{*}{\\begin{minipage}[c]{1.5cm}Metric-scale\\end{minipage}} \n& MASt3R-GA~\\cite{mast3r} & \\checkmark &  & 1.022  & 14.3 & 0.272 & 70.6 & 0.467 & 15.2 \\\\  \n& CUT3R~\\cite{cut3r} &  & \\checkmark & 1.029 & \\textbf{23.8} & \\textbf{0.103} & 88.5& \\textbf{0.122} & \\textbf{85.5} \\\\\n& \\textbf{Ours} &  & \\checkmark & \\textbf{0.777} & 17.1 & 0.137 & \\textbf{94.7}& 0.191 & 73.8 \\\\\n\\bottomrule\n\\end{tabular}\n\\label{tab:videodepth}\n    \\vspace{-5mm}\n\\end{table}\n\\centering\n\\caption{\\textbf{Video Depth Evaluation.} \nWe compare scale-invariant depth (per-sequence alignment) and metric depth (no alignment) results on Sintel, Bonn, and KITTI datasets.}\n\\small\n\\renewcommand{\\arraystretch}{1.0}\n\\renewcommand{\\tabcolsep}{1.4pt}\n\n\\toprule\n &  &  &  & \\multicolumn{2}2{c}c{\\textbf{Sintel}}\\textbf{Sintel} & \\multicolumn{2}2{c}c{\\textbf{BONN}}\\textbf{BONN} & \\multicolumn{2}2{c}c{\\textbf{KITTI}}\\textbf{KITTI}\\\\ \n\\cmidrule(lr){5-6}5-6 \\cmidrule(lr){7-8}7-8 \\cmidrule(lr){9-10}9-10\n\\textbf{Alignment} & \\textbf{Method} & \\textbf{Optim.} & \\textbf{Onl.\\ } & {Abs Rel $\\downarrow$}Abs Rel $\\downarrow$\\downarrow & {$\\delta$\\textless $1.25\\uparrow$}$\\delta$\\delta\\textless $1.25\\uparrow$1.25\\uparrow & {Abs Rel $\\downarrow$}Abs Rel $\\downarrow$\\downarrow & {$\\delta$\\textless $1.25\\uparrow$}$\\delta$\\delta\\textless $1.25\\uparrow$1.25\\uparrow & {Abs Rel $\\downarrow$}Abs Rel $\\downarrow$\\downarrow & {$\\delta$ \\textless $1.25\\uparrow$}$\\delta$\\delta \\textless $1.25\\uparrow$1.25\\uparrow \\\\ \n\\midrule\n\\multirow{6}6{*}*{\\begin{minipage}[c]{1.5cm}Per-sequence\\end{minipage}}[c]{1.5cm}1.5cmPer-sequence \n& DUSt3R-GA~\\cite{wang2024dust3r} &  \\checkmark & & 0.656 & {45.2}45.2 & {0.155}0.155 & {83.3}83.3 & 0.144 & 81.3 \\\\\n& MASt3R-GA~\\cite{mast3r} &   \\checkmark & & 0.641 & {43.9}43.9 & {0.252}0.252 & {70.1}70.1 & {0.183}0.183 & {74.5}74.5 \\\\\n& MonST3R-GA~\\cite{zhang2024monst3r} &  \\checkmark &  & \\textbf{0.378} & \\textbf{55.8} & \\underline{0.067} & \\textbf{96.3} & {0.168}0.168 & {74.4}74.4 \\\\\n& Spann3R~\\cite{wang20243d}&  & \\checkmark & 0.622 & {42.6}42.6 & {0.144}0.144 & {81.3}81.3 & {0.198}0.198 & {73.7}73.7 \\\\\n& CUT3R~\\cite{cut3r} &  & \\checkmark & \\underline{0.421}  & 47.9 & 0.078 & 93.7 & \\textbf{0.118} & \\textbf{88.1} \\\\\n& \\textbf{Ours} &  & \\checkmark & 0.452  & \\underline{48.9} & \\bf{0.060}0.060 & \\underline{96.0} & \\underline{0.136} & \\underline{84.2} \\\\\n\\midrule\n\\multirow{3}3{*}*{\\begin{minipage}[c]{1.5cm}Metric-scale\\end{minipage}}[c]{1.5cm}1.5cmMetric-scale \n& MASt3R-GA~\\cite{mast3r} & \\checkmark &  & 1.022  & 14.3 & 0.272 & 70.6 & 0.467 & 15.2 \\\\  \n& CUT3R~\\cite{cut3r} &  & \\checkmark & 1.029 & \\textbf{23.8} & \\textbf{0.103} & 88.5& \\textbf{0.122} & \\textbf{85.5} \\\\\n& \\textbf{Ours} &  & \\checkmark & \\textbf{0.777} & 17.1 & 0.137 & \\textbf{94.7}& 0.191 & 73.8 \\\\\n\\bottomrule\n\n\\label{tab:videodepth}\n    \\vspace{-5mm}\n\n\n\\subsection{Camera Pose Estimation}\nWe evaluate the camera pose estimation performance on ScanNet~\\cite{dai2017scannet} (static), Sintel~\\cite{butler2012naturalistic}, and TUM-dynamics~\\cite{sturm2012benchmark} datasets following MonST3R~\\cite{zhang2024monst3r} and CUT3R~\\cite{cut3r}.\nWe report Absolute Translation Error (ATE), Relative Translation Error (RPE trans), and Relative Rotation Error (RPE rot) after applying a Sim(3) Umeyama alignment with the ground truth~\\cite{zhang2024monst3r}.\nIt is worth noting that prior approaches conduct additional optimization while our method does not require any post-processing.\nResults in Table~\\ref{tab:pose} show that our method performs comparably with other online methods, but there persists a performance gap between those optimization-based baselines and our method.\n\n\\begin{table}[t]\n\\centering\n\\caption{\\textbf{Camera Pose Estimation Evaluation} on ScanNet, Sintel, and TUM-dynamics datasets.} \n    \\vspace{-1mm}\n\\small\n\\renewcommand{\\arraystretch}{1.}\n\\renewcommand{\\tabcolsep}{1pt}\n \\resizebox{1\\textwidth}{!}{\n\\begin{tabular}{cccccc|ccc|ccc}\n\\toprule\n& & & \\multicolumn{3}{c}{\\textbf{ScanNet (Static)}} & \\multicolumn{3}{c}{\\textbf{Sintel}} & \\multicolumn{3}{c}{\\textbf{TUM-dynamics}} \\\\ \n\\cmidrule(lr){4-6} \\cmidrule(lr){7-9} \\cmidrule(lr){10-12}\n{\\textbf{Method}}  &  \\textbf{Optim.} & \\textbf{Onl.} & {ATE $\\downarrow$} & {RPE trans $\\downarrow$} & {RPE rot $\\downarrow$} & {ATE $\\downarrow$} & {RPE trans $\\downarrow$} & {RPE rot $\\downarrow$} & {ATE $\\downarrow$} & {RPE trans $\\downarrow$} & {RPE rot $\\downarrow$} \\\\ \n\\midrule\nRobust-CVD~\\cite{kopf2021robust} & \\checkmark & & 0.227 & 0.064 & 7.374 & 0.360 & 0.154 & 3.443 & 0.153 & 0.026 & 3.528\\\\\nCasualSAM~\\cite{zhang2022structure} & \\checkmark & & 0.158 & 0.034 & 1.618 & 0.141 & \\bf 0.035 & \\bf 0.615 & 0.071 & \\bf 0.010 & 1.712\\\\\nDUSt3R-GA~\\cite{wang2024dust3r} & \\checkmark & & {0.081} & 0.028 & 0.784 & 0.417 & 0.250 & 5.796 & 0.083 & 0.017 & 3.567 \\\\  \nMASt3R-GA~\\cite{mast3r} & \\checkmark & & {\\underline{0.078}} & {\\underline{0.020}} & {\\bf {0.475}} & \\underline{0.185} & {0.060} & {1.496} & {\\bf{0.038}} & {\\underline{0.012}} & {\\bf{0.448}} \\\\ \nMonST3R-GA~\\cite{zhang2024monst3r}  & \\checkmark & & {\\bf{0.077}} & {\\bf{0.018}} & {\\underline{0.529}}& \\bf {{0.111}} & \\underline{0.044} & {0.869} & {{0.098}} & {{0.019}} & 0.935 \\\\\nSpann3R~\\cite{wang20243d}& &\\checkmark & 0.096 & 0.023 & 0.661 & {{0.329}} & 0.110 & 4.471 & 0.056 & 0.021 & 0.591 \\\\ \nCUT3R~\\cite{cut3r} &  & \\checkmark & 0.099 & 0.022 & 0.600 & 0.213 & 0.066 & \\underline{0.621} & \\underline{0.046} & 0.015 & \\underline{0.473}\\\\ \n\\textbf{Ours} &  & \\checkmark & 0.106 & 0.035 & 1.946 & 0.351 & 0.128 & 1.822 & 0.075 & 0.029 & 0.642\\\\ \n\\bottomrule\n\\end{tabular}\n}\n\\label{tab:pose}\n    \\vspace{-3mm}\n\\end{table}\n\\centering\n\\caption{\\textbf{Camera Pose Estimation Evaluation} on ScanNet, Sintel, and TUM-dynamics datasets.} \n    \\vspace{-1mm}\n\\small\n\\renewcommand{\\arraystretch}{1.}\n\\renewcommand{\\tabcolsep}{1pt}\n \\resizebox{1\\textwidth}1\\textwidth{!}!{\n\\begin{tabular}{cccccc|ccc|ccc}\n\\toprule\n& & & \\multicolumn{3}{c}{\\textbf{ScanNet (Static)}} & \\multicolumn{3}{c}{\\textbf{Sintel}} & \\multicolumn{3}{c}{\\textbf{TUM-dynamics}} \\\\ \n\\cmidrule(lr){4-6} \\cmidrule(lr){7-9} \\cmidrule(lr){10-12}\n{\\textbf{Method}}  &  \\textbf{Optim.} & \\textbf{Onl.} & {ATE $\\downarrow$} & {RPE trans $\\downarrow$} & {RPE rot $\\downarrow$} & {ATE $\\downarrow$} & {RPE trans $\\downarrow$} & {RPE rot $\\downarrow$} & {ATE $\\downarrow$} & {RPE trans $\\downarrow$} & {RPE rot $\\downarrow$} \\\\ \n\\midrule\nRobust-CVD~\\cite{kopf2021robust} & \\checkmark & & 0.227 & 0.064 & 7.374 & 0.360 & 0.154 & 3.443 & 0.153 & 0.026 & 3.528\\\\\nCasualSAM~\\cite{zhang2022structure} & \\checkmark & & 0.158 & 0.034 & 1.618 & 0.141 & \\bf 0.035 & \\bf 0.615 & 0.071 & \\bf 0.010 & 1.712\\\\\nDUSt3R-GA~\\cite{wang2024dust3r} & \\checkmark & & {0.081} & 0.028 & 0.784 & 0.417 & 0.250 & 5.796 & 0.083 & 0.017 & 3.567 \\\\  \nMASt3R-GA~\\cite{mast3r} & \\checkmark & & {\\underline{0.078}} & {\\underline{0.020}} & {\\bf {0.475}} & \\underline{0.185} & {0.060} & {1.496} & {\\bf{0.038}} & {\\underline{0.012}} & {\\bf{0.448}} \\\\ \nMonST3R-GA~\\cite{zhang2024monst3r}  & \\checkmark & & {\\bf{0.077}} & {\\bf{0.018}} & {\\underline{0.529}}& \\bf {{0.111}} & \\underline{0.044} & {0.869} & {{0.098}} & {{0.019}} & 0.935 \\\\\nSpann3R~\\cite{wang20243d}& &\\checkmark & 0.096 & 0.023 & 0.661 & {{0.329}} & 0.110 & 4.471 & 0.056 & 0.021 & 0.591 \\\\ \nCUT3R~\\cite{cut3r} &  & \\checkmark & 0.099 & 0.022 & 0.600 & 0.213 & 0.066 & \\underline{0.621} & \\underline{0.046} & 0.015 & \\underline{0.473}\\\\ \n\\textbf{Ours} &  & \\checkmark & 0.106 & 0.035 & 1.946 & 0.351 & 0.128 & 1.822 & 0.075 & 0.029 & 0.642\\\\ \n\\bottomrule\n\\end{tabular}\n}\n\n\\toprule\n& & & \\multicolumn{3}3{c}c{\\textbf{ScanNet (Static)}}\\textbf{ScanNet (Static)} & \\multicolumn{3}3{c}c{\\textbf{Sintel}}\\textbf{Sintel} & \\multicolumn{3}3{c}c{\\textbf{TUM-dynamics}}\\textbf{TUM-dynamics} \\\\ \n\\cmidrule(lr){4-6}4-6 \\cmidrule(lr){7-9}7-9 \\cmidrule(lr){10-12}10-12\n{\\textbf{Method}}\\textbf{Method}  &  \\textbf{Optim.} & \\textbf{Onl.} & {ATE $\\downarrow$}ATE $\\downarrow$\\downarrow & {RPE trans $\\downarrow$}RPE trans $\\downarrow$\\downarrow & {RPE rot $\\downarrow$}RPE rot $\\downarrow$\\downarrow & {ATE $\\downarrow$}ATE $\\downarrow$\\downarrow & {RPE trans $\\downarrow$}RPE trans $\\downarrow$\\downarrow & {RPE rot $\\downarrow$}RPE rot $\\downarrow$\\downarrow & {ATE $\\downarrow$}ATE $\\downarrow$\\downarrow & {RPE trans $\\downarrow$}RPE trans $\\downarrow$\\downarrow & {RPE rot $\\downarrow$}RPE rot $\\downarrow$\\downarrow \\\\ \n\\midrule\nRobust-CVD~\\cite{kopf2021robust} & \\checkmark & & 0.227 & 0.064 & 7.374 & 0.360 & 0.154 & 3.443 & 0.153 & 0.026 & 3.528\\\\\nCasualSAM~\\cite{zhang2022structure} & \\checkmark & & 0.158 & 0.034 & 1.618 & 0.141 & \\bf 0.035 & \\bf 0.615 & 0.071 & \\bf 0.010 & 1.712\\\\\nDUSt3R-GA~\\cite{wang2024dust3r} & \\checkmark & & {0.081}0.081 & 0.028 & 0.784 & 0.417 & 0.250 & 5.796 & 0.083 & 0.017 & 3.567 \\\\  \nMASt3R-GA~\\cite{mast3r} & \\checkmark & & {\\underline{0.078}}\\underline{0.078} & {\\underline{0.020}}\\underline{0.020} & {\\bf {0.475}}\\bf {0.475}0.475 & \\underline{0.185} & {0.060}0.060 & {1.496}1.496 & {\\bf{0.038}}\\bf{0.038}0.038 & {\\underline{0.012}}\\underline{0.012} & {\\bf{0.448}}\\bf{0.448}0.448 \\\\ \nMonST3R-GA~\\cite{zhang2024monst3r}  & \\checkmark & & {\\bf{0.077}}\\bf{0.077}0.077 & {\\bf{0.018}}\\bf{0.018}0.018 & {\\underline{0.529}}\\underline{0.529}& \\bf {{0.111}}{0.111}0.111 & \\underline{0.044} & {0.869}0.869 & {{0.098}}{0.098}0.098 & {{0.019}}{0.019}0.019 & 0.935 \\\\\nSpann3R~\\cite{wang20243d}& &\\checkmark & 0.096 & 0.023 & 0.661 & {{0.329}}{0.329}0.329 & 0.110 & 4.471 & 0.056 & 0.021 & 0.591 \\\\ \nCUT3R~\\cite{cut3r} &  & \\checkmark & 0.099 & 0.022 & 0.600 & 0.213 & 0.066 & \\underline{0.621} & \\underline{0.046} & 0.015 & \\underline{0.473}\\\\ \n\\textbf{Ours} &  & \\checkmark & 0.106 & 0.035 & 1.946 & 0.351 & 0.128 & 1.822 & 0.075 & 0.029 & 0.642\\\\ \n\\bottomrule\n\n\n\\label{tab:pose}\n    \\vspace{-3mm}\n\n\n\n\\subsection{Analysis and Discussion}\n\n\n\\begin{wrapfigure}{r}{0.5\\textwidth}\n\\vspace{-12mm}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/Ablation.pdf}\n    \\vspace{-7mm}\n    \\caption{Changes on \\textbf{the total number of pointers} and \\textbf{per-frame runtime} with memory fusion.}\n    \\label{fig:ablation_fig}\n\n\\end{wrapfigure}{r}r{0.5\\textwidth}0.5\\textwidth\n\\vspace{-12mm}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/Ablation.pdf}\n    \\vspace{-7mm}\n    \\caption{Changes on \\textbf{the total number of pointers} and \\textbf{per-frame runtime} with memory fusion.}\n    \\label{fig:ablation_fig}\n\n\n\n\\textbf{Effect of the memory fusion mechanism and runtime analysis.}\nWe design the memory fusion mechanism to get a balance between efficiency and performance.\nFigure~\\ref{fig:ablation_fig} shows the changes in the number of pointers \\(\\mathrm{N}\\)\\mathrm{N} in the memory (line graph) and per-frame runtime \\(\\mathrm{t}\\)\\mathrm{t} (histogram) with the increasing number of frames \\(\\mathrm{K}\\)\\mathrm{K} when processing Scene \\(\\mathit{WhiteRoom}\\)\\mathit{WhiteRoom} (from NRGBD dataset).\nWe can see that this memory fusion mechanism can control the total number of pointers and per-frame runtime within a reasonable range.\nWe also report the 3D reconstruction results without the memory fusion mechanism on the 7-scenes and NRGBD datasets in Table~\\ref{tab:ablation}.\nAlthough this fusion mechanism may result in a slight decrease in some metrics, we believe that the sacrifice made for efficiency improvement is worthwhile.\n\n\\textbf{Effect of the 3D hierarchical position embedding.}\nWe report the 3D reconstruction results without the 3D hierarchical position embedding in Table~\\ref{tab:ablation}, which shows the effectiveness of the elaborate position embedding we proposed.\n\n\n\n\\begin{table}[t]\n  \\centering\n  \\caption{\\textbf{Effects of the 3D hierarchical position embedding and the memory fusion mechanism.}} \n    \\vspace{-1mm}\n  \\small\n    \\begin{tabularx}{\\textwidth}{c >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X >{\\centering\\arraybackslash}X}\n      \\toprule\n\n          & \\multicolumn{3}{c}{\\textbf{7-scenes }} & \\multicolumn{3}{c}{\\textbf{NRGBD}}\\\\\n\n      \\cmidrule(lr){2-4} \\cmidrule(lr){5-7}\n         & \\multicolumn{1}{c}{{Acc}$\\downarrow$} & \\multicolumn{1}{c}{{Comp}$\\downarrow$} & \\multicolumn{1}{c}{{NC}$\\uparrow$} & \\multicolumn{1}{c}{{Acc}$\\downarrow$} & \\multicolumn{1}{c}{{Comp}$\\downarrow$} & \\multicolumn{1}{c}{{NC}$\\uparrow$}\\\\\n      \\midrule\n      Ours (w/o 3DHPE) & 0.180 & 0.180 & 0.683 & 0.145 &  0.123 & 0.770 \\\\\n      Ours (w/o Mem-Fusion) & \\bf 0.118 & 0.148 & 0.721 & \\bf 0.079 &  0.074 & \\bf 0.824 \\\\\n        Ours & 0.124 & \\bf{0.139} & \\bf 0.725 & \\bf 0.079 &  \\bf 0.073 & \\bf 0.824 \\\\\n      \\bottomrule\n    \\end{tabularx}\n    \\label{tab:ablation}\n    \\vspace{-5mm}\n\\end{table}\n  \\centering\n  \\caption{\\textbf{Effects of the 3D hierarchical position embedding and the memory fusion mechanism.}} \n    \\vspace{-1mm}\n  \\small\n    \n      \\toprule\n\n          & \\multicolumn{3}3{c}c{\\textbf{7-scenes }}\\textbf{7-scenes } & \\multicolumn{3}3{c}c{\\textbf{NRGBD}}\\textbf{NRGBD}\\\\\n\n      \\cmidrule(lr){2-4}2-4 \\cmidrule(lr){5-7}5-7\n         & \\multicolumn{1}1{c}c{{Acc}$\\downarrow$}{Acc}Acc$\\downarrow$\\downarrow & \\multicolumn{1}1{c}c{{Comp}$\\downarrow$}{Comp}Comp$\\downarrow$\\downarrow & \\multicolumn{1}1{c}c{{NC}$\\uparrow$}{NC}NC$\\uparrow$\\uparrow & \\multicolumn{1}1{c}c{{Acc}$\\downarrow$}{Acc}Acc$\\downarrow$\\downarrow & \\multicolumn{1}1{c}c{{Comp}$\\downarrow$}{Comp}Comp$\\downarrow$\\downarrow & \\multicolumn{1}1{c}c{{NC}$\\uparrow$}{NC}NC$\\uparrow$\\uparrow\\\\\n      \\midrule\n      Ours (w/o 3DHPE) & 0.180 & 0.180 & 0.683 & 0.145 &  0.123 & 0.770 \\\\\n      Ours (w/o Mem-Fusion) & \\bf 0.118 & 0.148 & 0.721 & \\bf 0.079 &  0.074 & \\bf 0.824 \\\\\n        Ours & 0.124 & \\bf{0.139}0.139 & \\bf 0.725 & \\bf 0.079 &  \\bf 0.073 & \\bf 0.824 \\\\\n      \\bottomrule\n    \n    \\label{tab:ablation}\n    \\vspace{-5mm}\n\n\n\n\n\n", "appendix": false}, "Conclusion and Discussions": {"content": "\n\\label{sec:con}\n\nIn this paper, we have presented an online streaming 3D reconstruction framework, Point3R, with a spatial pointer memory.\nWhen processing streaming inputs, our method maintains a growing spatial pointer memory in which each pointer is assigned a specific 3D position and aggregates scene information nearby with a changing spatial feature.\nEquipped with a 3D hierarchical position embedding and a simple yet effective memory fusion mechanism, our method imposes minimal constraints on the input, handling both static and dynamic scenes as well as ordered or unordered image collections.\nWith a low training cost, our method achieves competitive or state-of-the-art performance on various 3D/4D tasks, which verifies the effectiveness of our method.\n\n\\textbf{Limitations.} \nAs the explored area expands, the positions where pointers are stored also grow progressively, which may introduce additional interference to camera pose estimation in subsequent frames.\nOne of our future works is improving the pointer-image interaction to mitigate this issue.\n\n\\textbf{Broader Impacts.}\nOur method facilitates scalable and efficient dense streaming 3D scene reconstruction, benefiting a wide range of applications. \nThe explicit and interpretable design of our pointer memory makes our method more transparent and adaptable to different real-world scenarios.\n\n\n\n", "appendix": true}, "More Method Details": {"content": "\n\n\\subsection{Memory Fusion Mechanism}\nWe use a changing threshold \\(\\delta\\)\\delta to determine whether a new pointer and its nearest neighbor are sufficiently close.\nAt time \\(t\\)t, we have:\n\\begin{equation}\n    \\delta = \\sqrt{(\\frac{\\max P_{t-1}^{x}-\\min P_{t-1}^{x}}{l_{x}} )^{2} + (\\frac{\\max P_{t-1}^{y}-\\min P_{t-1}^{y}}{l_{y}} )^{2}+(\\frac{\\max P_{t-1}^{z}-\\min P_{t-1}^{z}}{l_{z}} )^{2}},\n\\end{equation}\\begin{equation}\n    \\delta = \\sqrt{(\\frac{\\max P_{t-1}^{x}-\\min P_{t-1}^{x}}{l_{x}} )^{2} + (\\frac{\\max P_{t-1}^{y}-\\min P_{t-1}^{y}}{l_{y}} )^{2}+(\\frac{\\max P_{t-1}^{z}-\\min P_{t-1}^{z}}{l_{z}} )^{2}},\n\\end{equation}\n    \\delta = \\sqrt{(\\frac{\\max P_{t-1}^{x}-\\min P_{t-1}^{x}}{l_{x}} )^{2} + (\\frac{\\max P_{t-1}^{y}-\\min P_{t-1}^{y}}{l_{y}} )^{2}+(\\frac{\\max P_{t-1}^{z}-\\min P_{t-1}^{z}}{l_{z}} )^{2}},\n\nwhere \\(P_{t-1}^{x}, P_{t-1}^{y}, P_{t-1}^{z}\\)P_{t-1}t-1^{x}x, P_{t-1}t-1^{y}y, P_{t-1}t-1^{z}z are the set of X, Y, Z components of the coordinates of all memory pointers from \\(\\mathcal{M}_{t-1}\\)\\mathcal{M}_{t-1}t-1, \\(l_{x}, l_{y}, l_{z}\\)l_{x}x, l_{y}y, l_{z}z are constants we use to control the distribution of memory pointers.\nIn practical implementation, we set \\(l_{x}, l_{y}, l_{z}\\)l_{x}x, l_{y}y, l_{z}z to \\(20\\)20.\n\n\\subsection{3D Hierarchical Position Embedding}\nFor \\(n\\)n, \\(m\\)m-th query and key \\(\\mathbf{q}_{n}, \\mathbf{k}_{m}\\in \\mathbb{R}^{1\\times d_{head}}\\)\\mathbf{q}_{n}n, \\mathbf{k}_{m}m\\in \\mathbb{R}^{1\\times d_{head}}1\\times d_{head}head,\nRoPE converts them to complex vector \\(\\bar{\\mathbf{q}}_{n}, \\bar{\\mathbf{k}}_{m} \\in \\mathbb{R}^{1\\times (d_{head}/2)}\\)\\bar{\\mathbf{q}}_{n}n, \\bar{\\mathbf{k}}_{m}m \\in \\mathbb{R}^{1\\times (d_{head}/2)}1\\times (d_{head}head/2) by considering (\\(2t\\)2t)-th dim as real part and (\\(2t+1\\)2t+1)-th dim as imaginary part.\nWe follow this so \\(\\theta_{t}\\)\\theta_{t}t in Eq. \\textcolor{red}{14} in the main text is \n\\begin{equation}\n    \\theta_{t}=b^{-t/(d_{head}/6)}, \\mathrm{where}\\ t\\in \\{0,1,...,d_{head}/6\\}, b \\in \\{10,100,1000,10000\\}.\n\\end{equation}\\begin{equation}\n    \\theta_{t}=b^{-t/(d_{head}/6)}, \\mathrm{where}\\ t\\in \\{0,1,...,d_{head}/6\\}, b \\in \\{10,100,1000,10000\\}.\n\\end{equation}\n    \\theta_{t}t=b^{-t/(d_{head}/6)}-t/(d_{head}head/6), \\mathrm{where}\\ t\\in \\{0,1,...,d_{head}head/6\\}, b \\in \\{10,100,1000,10000\\}.\n\nWe use different frequency bases \\(b\\)b to accommodate spatial inputs of varying scales, and thus derive four (\\(h\\)h in Eq. \\textcolor{red}{15} in the main text) different rotation matrices \\(\\mathbf{R}_{i} \\ (i=0,1,2,3)\\)\\mathbf{R}_{i}i \\ (i=0,1,2,3).\nThen we can obtain the embedded query and key, and the corresponding attention matrix as follows:\n\\begin{equation}\n    \\bar{\\mathbf{q}}' = \\frac{1}{4}\\sum_{i=1}^{4}(\\bar{\\mathbf{q}} \\circ\\mathbf{R}_{i}), \\ \\bar{\\mathbf{k}}' =\\frac{1}{4}\\sum_{i=1}^{4}(\\bar{\\mathbf{k}} \\circ\\mathbf{R}_{i}),\\ \\mathbf{A}'=\\mathrm{Re}[\\bar{\\mathbf{q}}'\\bar{\\mathbf{k}}'^{*}].\n\\end{equation}\\begin{equation}\n    \\bar{\\mathbf{q}}' = \\frac{1}{4}\\sum_{i=1}^{4}(\\bar{\\mathbf{q}} \\circ\\mathbf{R}_{i}), \\ \\bar{\\mathbf{k}}' =\\frac{1}{4}\\sum_{i=1}^{4}(\\bar{\\mathbf{k}} \\circ\\mathbf{R}_{i}),\\ \\mathbf{A}'=\\mathrm{Re}[\\bar{\\mathbf{q}}'\\bar{\\mathbf{k}}'^{*}].\n\\end{equation}\n    \\bar{\\mathbf{q}}' = \\frac{1}{4}\\sum_{i=1}i=1^{4}4(\\bar{\\mathbf{q}} \\circ\\mathbf{R}_{i}i), \\ \\bar{\\mathbf{k}}' =\\frac{1}{4}\\sum_{i=1}i=1^{4}4(\\bar{\\mathbf{k}} \\circ\\mathbf{R}_{i}i),\\ \\mathbf{A}'=\\mathrm{Re}[\\bar{\\mathbf{q}}'\\bar{\\mathbf{k}}'^{*}*].\n\nThe rotation matrix (with our 3D hierarchical position embedding) \\(\\mathbf{A}'\\)\\mathbf{A}' implies relative position in rotation form, and thus boosts the performance.\n\n\\begin{table}[h]\n\\centering\n\\tiny\n\\vspace{1mm}\n\\setlength{\\tabcolsep}{10pt}\n\\resizebox{\\textwidth}{!}{%\n\\begin{tabular}{l c c c c}\n\\toprule\n\\textbf{Dataset} & \\textbf{Scene Type} & \\textbf{Dynamic} & \\textbf{Real} & \\textbf{Metric} \\\\\n\\midrule\nARKitScenes~\\cite{baruch2021arkitscenes} & Indoor & Static & Real & Yes\\\\\nBlendedMVS~\\cite{yao2020blendedmvs} & Mixed & Static & Synthetic & No\\\\\nCO3Dv2~\\cite{reizenstein2021common} & Object-Centric & Static & Real & No\\\\\nHyperSim~\\cite{hypersim} & Indoor & Static & Synthetic & Yes\\\\\nMegaDepth~\\cite{li2018megadepth} & Outdoor & Static & Real & No\\\\\nOmniObject3D~\\cite{wu2023omniobject3d} & Object-Centric & Static & Synthetic & Yes\\\\\nScanNet~\\cite{dai2017scannet} & Indoor & Static & Real & Yes\\\\\nScanNet++~\\cite{yeshwanth2023scannet++} & Indoor & Static & Real & Yes\\\\\nWildRGBD~\\cite{xia2024rgbd} & Object-Centric & Static & Real & Yes\\\\\nMVS-Synth~\\cite{mvssynth} & Outdoor & Dynamic & Synthetic & Yes\\\\\nPointOdyssey~\\cite{zheng2023pointodyssey} & Mixed & Dynamic & Synthetic & Yes\\\\\nSpring~\\cite{mehl2023spring} & Mixed & Dynamic & Synthetic & Yes\\\\\nVirtualKITTI2~\\cite{cabon2020virtual} & Outdoor & Dynamic & Synthetic & Yes\\\\\nWaymo~\\cite{sun2020scalability} & Outdoor & Dynamic & Real & Yes\\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\vspace{2mm}\n\\caption{\\textbf{Training Datasets.}}\n\\label{tab:dataset_tab}\n\\end{table}\n\\centering\n\\tiny\n\\vspace{1mm}\n\\setlength{\\tabcolsep}{10pt}\n\\resizebox{\\textwidth}\\textwidth{!}!{%\n\\begin{tabular}{l c c c c}\n\\toprule\n\\textbf{Dataset} & \\textbf{Scene Type} & \\textbf{Dynamic} & \\textbf{Real} & \\textbf{Metric} \\\\\n\\midrule\nARKitScenes~\\cite{baruch2021arkitscenes} & Indoor & Static & Real & Yes\\\\\nBlendedMVS~\\cite{yao2020blendedmvs} & Mixed & Static & Synthetic & No\\\\\nCO3Dv2~\\cite{reizenstein2021common} & Object-Centric & Static & Real & No\\\\\nHyperSim~\\cite{hypersim} & Indoor & Static & Synthetic & Yes\\\\\nMegaDepth~\\cite{li2018megadepth} & Outdoor & Static & Real & No\\\\\nOmniObject3D~\\cite{wu2023omniobject3d} & Object-Centric & Static & Synthetic & Yes\\\\\nScanNet~\\cite{dai2017scannet} & Indoor & Static & Real & Yes\\\\\nScanNet++~\\cite{yeshwanth2023scannet++} & Indoor & Static & Real & Yes\\\\\nWildRGBD~\\cite{xia2024rgbd} & Object-Centric & Static & Real & Yes\\\\\nMVS-Synth~\\cite{mvssynth} & Outdoor & Dynamic & Synthetic & Yes\\\\\nPointOdyssey~\\cite{zheng2023pointodyssey} & Mixed & Dynamic & Synthetic & Yes\\\\\nSpring~\\cite{mehl2023spring} & Mixed & Dynamic & Synthetic & Yes\\\\\nVirtualKITTI2~\\cite{cabon2020virtual} & Outdoor & Dynamic & Synthetic & Yes\\\\\nWaymo~\\cite{sun2020scalability} & Outdoor & Dynamic & Real & Yes\\\\\n\\bottomrule\n\\end{tabular}%\n}\n\\toprule\n\\textbf{Dataset} & \\textbf{Scene Type} & \\textbf{Dynamic} & \\textbf{Real} & \\textbf{Metric} \\\\\n\\midrule\nARKitScenes~\\cite{baruch2021arkitscenes} & Indoor & Static & Real & Yes\\\\\nBlendedMVS~\\cite{yao2020blendedmvs} & Mixed & Static & Synthetic & No\\\\\nCO3Dv2~\\cite{reizenstein2021common} & Object-Centric & Static & Real & No\\\\\nHyperSim~\\cite{hypersim} & Indoor & Static & Synthetic & Yes\\\\\nMegaDepth~\\cite{li2018megadepth} & Outdoor & Static & Real & No\\\\\nOmniObject3D~\\cite{wu2023omniobject3d} & Object-Centric & Static & Synthetic & Yes\\\\\nScanNet~\\cite{dai2017scannet} & Indoor & Static & Real & Yes\\\\\nScanNet++~\\cite{yeshwanth2023scannet++} & Indoor & Static & Real & Yes\\\\\nWildRGBD~\\cite{xia2024rgbd} & Object-Centric & Static & Real & Yes\\\\\nMVS-Synth~\\cite{mvssynth} & Outdoor & Dynamic & Synthetic & Yes\\\\\nPointOdyssey~\\cite{zheng2023pointodyssey} & Mixed & Dynamic & Synthetic & Yes\\\\\nSpring~\\cite{mehl2023spring} & Mixed & Dynamic & Synthetic & Yes\\\\\nVirtualKITTI2~\\cite{cabon2020virtual} & Outdoor & Dynamic & Synthetic & Yes\\\\\nWaymo~\\cite{sun2020scalability} & Outdoor & Dynamic & Real & Yes\\\\\n\\bottomrule\n\n\\vspace{2mm}\n\\caption{\\textbf{Training Datasets.}}\n\\label{tab:dataset_tab}\n\n\n", "appendix": true}, "More Implementation and Training Details": {"content": "\n\nIn the forward pass of \\(\\mathrm{Head}_{global}\\)\\mathrm{Head}_{global}global, we first generate the pose-modulated tokens using an additional modulation function in CUT3R~\\cite{cut3r},\nand then feed them to the DPT architecture to generate the output \\({\\hat{X}_{t}}^{global}\\){\\hat{X}_{t}}\\hat{X}_{t}t^{global}global.\nThe modulation function uses two self-attention blocks and modulates the input tokens within the Layer Normalization layers using the pose token.\n\nTable~\\ref{tab:dataset_tab} provides more details about the datasets we use to train our model.\nTo be specific, we use ARKitScenes, BlendedMVS, CO3Dv2, HyperSim, MegaDepth, ScanNet, ScanNet++, WildRGBD, VirtualKITTI2 and Waymo datasets in the first stage which takes almost 2 days.\nWe add other datasets and increase the input resolution in the second stage which takes almost 2 days.\nThe final stage (longer input sequences) takes about 3 days.\nBesides, we do not use the memory fusion mechanism in the first stage to improve the stability of training.\nWe dynamically adjust the usage ratio of each dataset at different stages during our training to improve the overall model performance.\n\n", "appendix": true}, "More Visualizations": {"content": "\n\nWe show qualitative results on sparse inputs in Figure \\textcolor{red}{3} in the main text.\nIn this section, we show more qualitative results on dense inputs from\nstatic (Figure~\\ref{fig:staticvis}) and dynamic (Figure~\\ref{fig:dynamicvis}) scenes.\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/staticvis.pdf}\n    \\caption{\\textbf{Qualitative results on dense inputs from static scenes.}}\n    \\label{fig:staticvis}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/staticvis.pdf}\n    \\caption{\\textbf{Qualitative results on dense inputs from static scenes.}}\n    \\label{fig:staticvis}\n\n\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/dynamicvis.pdf}\n    \\caption{\\textbf{Qualitative results on dense inputs from dynamic scenes.}}\n    \\label{fig:dynamicvis}\n\\end{figure}\n    \\centering\n    \\includegraphics[width=\\linewidth]{figures/dynamicvis.pdf}\n    \\caption{\\textbf{Qualitative results on dense inputs from dynamic scenes.}}\n    \\label{fig:dynamicvis}\n\n\n\n\n\n\n\n\n\n\n", "appendix": false}}, "categories": ["cs.CV", "cs.AI", "cs.LG"], "published": "2025-07-03 17:59:56+00:00", "primary_category": "cs.CV", "summary": "Dense 3D scene reconstruction from an ordered sequence or unordered image\ncollections is a critical step when bringing research in computer vision into\npractical scenarios. Following the paradigm introduced by DUSt3R, which unifies\nan image pair densely into a shared coordinate system, subsequent methods\nmaintain an implicit memory to achieve dense 3D reconstruction from more\nimages. However, such implicit memory is limited in capacity and may suffer\nfrom information loss of earlier frames. We propose Point3R, an online\nframework targeting dense streaming 3D reconstruction. To be specific, we\nmaintain an explicit spatial pointer memory directly associated with the 3D\nstructure of the current scene. Each pointer in this memory is assigned a\nspecific 3D position and aggregates scene information nearby in the global\ncoordinate system into a changing spatial feature. Information extracted from\nthe latest frame interacts explicitly with this pointer memory, enabling dense\nintegration of the current observation into the global coordinate system. We\ndesign a 3D hierarchical position embedding to promote this interaction and\ndesign a simple yet effective fusion mechanism to ensure that our pointer\nmemory is uniform and efficient. Our method achieves competitive or\nstate-of-the-art performance on various tasks with low training costs. Code is\navailable at: https://github.com/YkiWu/Point3R."}