INFO 08-31 20:17:03 [__init__.py:241] Automatically detected platform cuda.
CLIP image features: (1, 512)
Top CLIP tags: ['contains scatter plot']
Image tag list: [['contains scatter plot']]
[WARN] Path not found: download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Path not found: download/output/figures/2310.07894v1/tex_figures_5d.png
Image tag list: [None, None]
[WARN] Path not found: download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
Image tag list: [None]
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_demog
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_time
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_model
[WARN] Path not found: download/output/figures/2508.00717/results_ai_pays_dem
Image tag list: [None, None, None, None]
CLIP image features: (1, 512)
Top CLIP tags: ['a figure showing network']
CLIP image features: (1, 512)
Top CLIP tags: ['a figure showing network']
Image tag list: [['a figure showing network'], ['a figure showing network']]
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about bar chart']
Image tag list: [['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about bar chart']]
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
Prompt list: ['You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Manifold Preserving Guided Diffusion\nAbstract: Despite the recent advancements, conditional image generation still faces challenges of cost, generalizability, and the need for task-specific training. In this paper, we propose Manifold Preserving Guided Diffusion (MPGD), a training-free conditional generation framework that leverages pretrained diffusion models and off-the-shelf neural networks with minimal additional inference cost for a broad range of tasks. Specifically, we leverage the manifold hypothesis to refine the guided diffusion steps and introduce a shortcut algorithm in the process. We then propose two methods for on-manifold training-free guidance using pre-trained autoencoders and demonstrate that our shortcut inherently preserves the manifolds when applied to latent diffusion models. Our experiments show that MPGD is efficient and effective for solving a variety of conditional generation applications in low-compute settings, and can consistently offer up to 3.8x speed-ups with the same number of diffusion steps while maintaining high sample quality compared to the baselines.\nSection name: Experiments\n\nFigure block (optional):\n- label: \\label{fig:qualitative_faceid}; caption: \\caption{Examples of the FaceID guidance generation with pre-trained CelebA-HQ models. }\nTable block (optional):\n- label: \\label{tab:faceid}; text: \\begin{tabular}{@{}l|ccc@{}} \\toprule \\textbf{Method} & \\textbf{KID}$\\downarrow$        & \\textbf{FaceID}$\\downarrow$ & \\textbf{Time}$\\downarrow$ \\\\ \\midrule DDIM            & 0.0442          & 1.3914               & 3.41s             \\\\ \\m\nCited paper titles/abstracts (optional):\n- [lgd] Loss-guided diffusion models for plug-and-play controllable generation.: \n- [freedom] FreeDoM: Training-free energy-guided conditional diffusion model.: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. \\vspace{-5pt}\n\\subsection{Pixel space diffusion models}\nIn this section, we evaluate the performance of our proposed pixel domain methods (i.e. \\textbf{MPGD w/o Proj.}, \\textbf{MPGD-AE} and \\textbf{MPGD-Z}) with two different sets of conditional image generation tasks: solving liner inverse problems and human face generation guided by face recognition loss, which we refer to as FaceID guidance generation. For MPGD-AE and MPGD-Z, we use the pre-trained VQGAN models provided by~\\citet{rombach2021highresolution}.\n\\subsubsection{Noisy Linear Inverse Problem}\n2. For linear tasks, we use noisy super-resolution and noisy Gaussian deblurring as the test bed. We choose DPS~\\citep{dps}, LGD-MC~\\citep{lgd}, and MCG~\\citep{mcg} as the basleines. We test each method with two pre-trained diffusion models provided by~\\citet{dps}: one trained on FFHQ dataset~\\citep{karras2019style} and another on ImageNet~\\citep{deng2009imagenet}, both with $256 \\times 256$256 \\times 256 resolution. \nFor super-resolution, we down-sample ground truth images to $64 \\times 64$64 \\times 64. In the Gaussian deblurring task, we apply a $61\\times 61$61\\times 61 sized Gaussian blur with kernel intensity $3.0$3.0. to original images, Measurements in both tasks have a random noise with a variance of $\\sigma^{2}=0.05^{2}$\\sigma^{2}2=0.05^{2}2. We evaluate each task on a set of 1000~samples. We use the Kernel Inception distance (KID)~\\citep{binkowski2018demystifying} to assess the fidelity, Learned Perceptual Image Patch Similarity (LPIPS)~\\citep{zhang2018unreasonable} to evaluate the guidance quality, and the inference time to test the efficiency of the methods. All experiments are conducted on a single NVIDIA GeForce RTX 2080 Ti GPU . Figure~\\ref{fig:linear_main} shows the generated examples for qualitative comparison, and Figure~\\ref{fig:ffhq_sr} presents the quantitative results for the super-resolution task on FFHQ. All three of our methods significantly outperform the baselines with all metrics tested across a variety of different numbers of DDIM steps, and we can observe manifold projection improves the sample fidelity by a large margin.\nNext:\n1. \\hfill\n    [t]{0.53\\textwidth}0.53\\textwidth\n        \\centering\n        \\caption{Quantitative results for style guidance with Stable Diffusion experiment. Our method finds the sweet spot between following the prompt and following the style guidance.}\n        \\label{tab:style}\n2. \\vspace{-7pt}\n\\subsection{Latent diffusion models}\nTo evaluate \\textbf{MPGD-LDM}, we test our methods against the same baselines as the pixel-space FaceID experiments with text-to-image style guided generation task. The goal of this task is to generate images that fit both the text input prompts and the style of the reference images.\nWe use Stable Diffusion~\\citep{rombach2021highresolution} as the pre-trained text-to-image model and deploy the guided sampling methods to incorporate a style loss, which is calculated by the Frobenius norm between the Gram matrices of the reference images and the generated images.\nFor reference style images and text prompts, we randomly created 1000 conditioning pairs, using images from WikiArt~\\citep{saleh2015large} and prompts from PartiPrompts~\\citep{yu2022scaling} dataset. \nFigure~\\ref{fig:style_main} and Table~\\ref{tab:style} show qualitative and quantitative results for this experiment respectively. All the samples are generated on a single NVIDIA A100 GPU with 100 DDIM steps.\nOur method finds the sweet spot between following the text prompts, which usually instruct the generation to generate photo realistic images that do not suit the given style, and following the style guidance, which deviate from the prompts. Notably, because MPGD does not require propagation through the diffusion model, our method can provide significant speedup and can be fitted into a 16GB GPU while all the other methods cannot.\n3. \\vspace{-7pt}\n\nTarget length (characters): ~1212 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:qualitative_faceid\']\ntable_labels = [\'tab:faceid\']\nbib_keys = [\'freedom\', \'lgd\', \'freedom\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Efficient Integrators for Diffusion Generative Models\nAbstract: Diffusion models suffer from slow sample generation at inference time. Therefore, developing a principled framework for fast deterministic/stochastic sampling for a broader class of diffusion models is a promising direction. We propose two complementary frameworks for accelerating sample generation in pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate integrators generalize DDIM, mapping the reverse diffusion dynamics to a more amenable space for sampling. In contrast, splitting-based integrators, commonly used in molecular dynamics, reduce the numerical simulation error by cleverly alternating between numerical updates involving the data and auxiliary variables. After extensively studying these methods empirically and theoretically, we present a hybrid method that leads to the best-reported performance for diffusion models in augmented spaces. Applied to Phase Space Langevin Diffusion [Pandey & Mandt, 2023] on CIFAR-10, our deterministic and stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing baselines, respectively. Our code and model checkpoints wil\nSection name: Additional Experimental Results\n\nFigure block (optional):\n- label: \\label{fig:precond_a}; caption: \\caption{}\n- label: \\label{fig:precond_b}; caption: \\caption{}\nTable block (optional):\n- label: \\label{table:main}; text: \\begin{tabular}{@{}clcccc@{}} \\toprule                                         &               &                                                             &           & \\multicolumn{2}{c}{NFE (FID@50k $\\downarrow$)} \\\\ \\midrule           \nCited paper titles/abstracts (optional):\n- [songdenoising] Denoising diffusion implicit models.: \n- [lu2022dpm] Dpm-solver: A fast ode solver for diffusion probabilistic model   sampling in around 10 steps.: \n- [zhang2023fast] Fast sampling of diffusion models with exponential integrator.: \n- [karraselucidating] Elucidating the design space of diffusion-based generative models.: \n- [liupseudo] Pseudo numerical methods for diffusion models on manifolds.: \n- [bao2022analyticdpm] Analytic-DPM: an analytic estimate of the optimal reverse variance   in diffusion probabilistic models.: \n- [xue2023sasolver] Sa-solver: Stochastic adams solver for fast sampling of diffusion   models, 2023: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. \\textbf{Notation.} For simplicity, we denote our best-performing Reduced Splitting and Conjugate Splitting integrators as \\textit{\\gls{SPS}} and \\textit{\\gls{CSPS}}, respectively. Consequently, we refer to the \\underline{D}eterministic \\gls{RVV}RVV and \\underline{S}tochastic \\gls{ROBA}ROBA samplers as \\gls{SPS}SPS-D and \\gls{SPS}SPS-S, and their conjugate variants as \\gls{CSPS}CSPS-D and \\gls{CSPS}CSPS-S, respectively.\n2. \\textbf{Datasets and Evaluation Metrics.} We use the CIFAR-10, CelebA-64 \\citep{liu2015faceattributes} and the AFHQ-v2 \\citep{choi2020stargan} datasets for comparisons. Unless specified otherwise, we report FID for 50k generated samples for all datasets and quantify sampling efficiency using \\gls{NFE}NFE. We include full experimental details in Appendix \\ref{app:exp_setup}.\nNext:\n1. \\textbf{Empirical Observations:} For CIFAR-10, our ODE sampler performs comparably or outperforms all other baselines for \\gls{NFE}NFE $\\geq$\\geq 50 (Fig. \\ref{fig:sota_all}, Top Left). Similarly, our SDE sampler outperforms all other baselines for \\gls{NFE}NFE $\\geq$\\geq 40 (Fig. \\ref{fig:sota_all}, Top Right). We make similar observations for the CelebA-64 and AFHQv2-64 datasets, where our proposed samplers can obtain significant gains over prior methods for \\gls{NFE}NFE $\\geq$\\geq 70 (See Fig. \\ref{fig:sota_all}, Bottom Left). Therefore, our proposed samplers for \\gls{PSLD}PSLD are competitive with recent work. Moreover, for all datasets, our stochastic sampler achieves better sample quality for low sampling budgets (\\gls{NFE}NFE $<$< 50) as compared to our deterministic sampler. \nLastly, in contrast to CIFAR-10, we find that the \\gls{CSPS}CSPS-S sampler works better than the \\gls{SPS}SPS-S sampler for the CelebA-64 and AFHQv2-64 datasets, indicating its effectiveness for higher-resolution sampling.\n\nTarget length (characters): ~1323 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:precond_a\', \'fig:precond_b\']\ntable_labels = [\'table:main\']\nbib_keys = [\'songdenoising\', \'zhang2023fast\', \'lu2022dpm\', \'liupseudo\', \'karraselucidating\', \'xue2023sasolver\', \'bao2022analyticdpm\', \'karraselucidating\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average\nAbstract: Presently the most successful approaches to semi-supervised learning are based on consistency regularization, whereby a model is trained to be robust to small perturbations of its inputs and parameters. To understand consistency regularization, we conceptually explore how loss geometry interacts with training procedures. The consistency loss dramatically improves generalization performance over supervised-only training; however, we show that SGD struggles to converge on the consistency loss and continues to make large steps that lead to changes in predictions on the test data. Motivated by these observations, we propose to train consistency-based methods with Stochastic Weight Averaging (SWA), a recent approach which averages weights along the trajectory of SGD with a modified learning rate schedule. We also propose fast-SWA, which further accelerates convergence by averaging multiple points within each cycle of a cyclical learning rate schedule. With weight averaging, we achieve the best known semi-supervised results on CIFAR-10 and CIFAR-100, over many different quantities of labeled training data. For example, we achieve 5.0% error on CIFAR-10 with only 4000 labels, compared to \nSection name: Experiments\n\nFigure block (optional):\n- label: \\label{fig:cnn_c10_c100_trend}; caption: \\caption{Prediction errors of base models and their weight averages (\\fastswa and \\swa) for CNN    on \\textbf{(left)} CIFAR-10 with $4k$ labels,   \\textbf{(middle)} CIFAR-100 with $10k$ labels,   and \\textbf{(right)} CIFAR-100 $50k$ labels \nTable block (optional):\n- label: \\label{table:cifar100_shakeshake}; text: \\begin{tabular}{l c c c c c l l l l l } Number of labels\t\t\t& \t10k \t\t& \t50k \t\t& \t50k + 500k \t\t& 50k +   237k$^*$ \t\\\\  \\midrule TE (CNN) \\citep{te}  & \t38.65 \\tpm 0.51\t\t&\t26.30 \\tpm 0.15\t\t&\t23.62 \\tpm 0.17  \t&\t 23.79 \\tpm 0.17  \t\\\\  \\midrule \nCited paper titles/abstracts (optional):\n- [te] Temporal Ensembling for Semi-Supervised Learning.: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. In Figure \\ref{fig:cnn_c10_c100_trend} (left), we visualize the test\nerror as a function of iteration using the CNN.\nWe observe that when the cyclical learning rate starts after epoch $\\ell=180$\\ell=180, \nthe base models drop in performance due to the sudden increase in learning rate (see Figure \\ref{fig:swa_model} left).\nHowever, \\fastswa continues to improve while collecting the weights corresponding to high learning rates for averaging.\nIn general, we also find that the cyclical learning rate improves the base models beyond the usual cosine annealing \nschedule and increases the performance of \\fastswa as training progresses. \nCompared to \\swa, we also observe that \\fastswa converges substantially faster, \nfor instance, reducing the error to $10.5\\%$10.5\\%  at epoch $200$200 while \\swa attains \nsimilar error at epoch $350$350 for CIFAR-10 $4k$4k labels (Figure \n\\ref{fig:cnn_c10_c100_trend} left). We provide additional plots in Section \\ref{sec:add_results} \nshowing the convergence of the \\pi and MT models in all label \nsettings, where we observe similar trends that \\fastswa results in faster error reduction.\n2. We also find that the performance gains of \\fastswa over base models are higher for the \\pi model compared to the MT model, which is consistent with the convexity observation in Section \\ref{sec:empirical_convexity} \nand Figure \\ref{fig:convexity}.\nIn the previous evaluations \\citep[see e.g.][]{ssl-eval, mt}, the \\pi model was shown to be inferior to the MT model.\nHowever, with weight averaging, \\fastswa reduces the gap between \\pi and MT performance. \nSurprisingly, we find that the \\pi model can outperform MT after applying \\fastswa  \nwith moderate to large numbers of labeled points. \nIn particular, the $\\Pi$\\Pi+fast-SWA model outperforms MT+\\fastswa on CIFAR-10 with $4k$4k, $10k$10k, and $50k$50k labeled\ndata points for the Shake-Shake  architecture.\n3. \\vspace{-0.8\\baselineskip}\n4. \\subsection{CIFAR-100 and Extra Unlabeled Data} \\label{sec:cifar100_cnn} \\label{sec:cifar100}\n5. We evaluate the \\pi and MT models with \\fastswa on \nCIFAR-100.\nWe train our models using $50000$50000 images with $10k$10k and $50k$50k labels using the $13$13-layer CNN. \nWe also analyze the effect of using the Tiny Images dataset \\citep{tiny_images} as an additional source of unlabeled data.\nNext:\n1. \\subsection{Advancing State-of-the-Art} \\label{sec:soa}\n2. We have shown that \\fastswa can significantly improve the performance of both the \\pi and \nMT models. We provide a summary comparing our results with the previous best results in the literature in Table  \\ref{table:soa}, using the $13$13-layer CNN and\nthe Shake-Shake architecture that had been applied previously. We also provide detailed results the Appendix \\ref{sec:add_results}.\n3. \\subsection{Preliminary Results on Domain Adaptation}\n\\label{sec:domain_adaptation}\nDomain adaptation problems involve learning using a source domain $X_s$X_s \nequipped with labels $Y_s$Y_s and performing classification on the target \ndomain $X_t$X_t while having no access to the target labels at training time. \nA recent model by \\citet{da_mt} applies the consistency enforcing principle for \ndomain adaptation and achieves state-of-the-art results on many datasets. \nApplying \\fastswa to this model on domain adaptation from CIFAR-10 to STL we \nwere able to improve the best results reported in the literature from\n$19.9\\%$19.9\\% to $16.8\\%$16.8\\%. \nSee Section \\ref{sec:exp_da} for more details on the domain adaptation experiments.\n\nTarget length (characters): ~1438 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:cnn_c10_c100_trend\']\ntable_labels = [\'table:cifar100_shakeshake\']\nbib_keys = [\'te\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Generative AI in Higher Education: Evidence from an Elite College\nAbstract: Generative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPT\'s release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AI\'s potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AI\'s educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.\nSection name: Generative AI Usage Patterns Among Students\n\nFigure block (optional):\n- label: \\label{fig:ai_use}; caption: \\caption{The Adoption of Generative AI among Middlebury College Students}\n- label: \\label{fig:ai_use_time}; caption: \\caption{The Evolution of Generative AI Adoption among Middlebury College Students}\n- label: \\label{fig:ai_models}; caption: \\caption{Adoption of Generative AI Models Among College Students}\n- label: \\label{fig:ai_pays_dem}; caption: \\caption{Percent of Students Who Pay for Generative AI Tools}\nTable block (optional):\n- label: \\label{tab:ai_usage_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{8}{c}{Outcome: Uses AI during the semester with frequency of at least...}  \\\\\\cmidrule{3-10}  \t\t\t\t\t%\t\t\n- label: \\label{tab:ai_adopt_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{10}{c}{Outcome: Started using generative AI...}  \\\\\\cmidrule{3-12}  \t\t\t\t\t&& Before\n- label: \\label{tab:ai_freq_het}; text: \\begin{tabular}{lccccc} \t\t\t\t\t\t\\toprule \t\t\t\t\t\t&& \\multicolumn{4}{c}{By Usage Frequency} \\\\ \\cmidrule{3-6} \t\t\t\t\t\t& Any use & Rarely & Occasionally & Frequently & Very Frequently \\\\ \t\t\t\t\t\t& (1) & (2) & (3) & (4) & (5) \\\\ \\midrule\t\t\t\t\t\t \t\t\t\t\t\t\\\n- label: \\label{tab:ai_model_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{10}{c}{Outcome: =1 if student uses}  \\\\\\cmidrule{3-12}  \t\t\t\t\t&& OpenAI\'s && Google\nCited paper titles/abstracts (optional):\n- [bick.etal2025] The rapid adoption of generative AI.: \n- [mcclain2024] Americans\' use of ChatGPT is ticking up, but few trust its   election information: \n- [nam2023ai] 56\\% of college students have used ai on assignments or exams.: \n- [stohr.etal2024] \\"o: \n- [ravselj.etal2025] s: \n- [otis2024global] Global Evidence on Gender Gaps and Generative AI.: \n- [hartley.etal2025] The Labor Market Effects of Generative Artificial   Intelligence: \n- [humlum.vestergaard2025] Large Language Models, Small Labor Market Effects: \n- [carvajal2024] Will artificial intelligence get in the way of achieving gender   equality?: \n- [david1990dynamo] The dynamo and the computer: an historical perspective on the modern   productivity paradox.: \n- [gallup2024] AI in the Workplace: Answering 3 Big Questions: \n- [hall2003adoption] Adoption of new technology: \n- [stokey2021technology] Technology diffusion.: \n- [world2016world] World Bank: \n- [bick.etal2025] The rapid adoption of generative AI.: \n- [mcclain2024] Americans\' use of ChatGPT is ticking up, but few trust its   election information: \n- [nam2023ai] 56\\% of college students have used ai on assignments or exams.: \n- [stohr.etal2024] \\"o: \n- [ravselj.etal2025] s: \n- [otis2024global] Global Evidence on Gender Gaps and Generative AI.: \n- [hartley.etal2025] The Labor Market Effects of Generative Artificial   Intelligence: \n- [humlum.vestergaard2025] Large Language Models, Small Labor Market Effects: \n- [carvajal2024] Will artificial intelligence get in the way of achieving gender   equality?: \n- [david1990dynamo] The dynamo and the computer: an historical perspective on the modern   productivity paradox.: \n- [gallup2024] AI in the Workplace: Answering 3 Big Questions: \n- [hall2003adoption] Adoption of new technology: \n- [stokey2021technology] Technology diffusion.: \n- [world2016world] World Bank: \n\nk-most adjacent paragraphs (context):\n(none)\n\nTarget length (characters): ~13653 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:ai_use\', \'fig:ai_use_time\', \'fig:ai_models\', \'fig:ai_pays_dem\']\ntable_labels = [\'tab:ai_usage_correlates\', \'tab:ai_adopt_correlates\', \'tab:ai_freq_het\', \'tab:ai_model_correlates\']\nbib_keys = [\'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'nam2023ai\', \'stohr.etal2024\', \'carvajal2024\', \'ravselj.etal2025\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'david1990dynamo\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'hall2003adoption\', \'otis2024global\', \'nam2023ai\', \'stokey2021technology\', \'carvajal2024\', \'stohr.etal2024\', \'world2016world\', \'ravselj.etal2025\', \'carvajal2024\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Neural-Symbolic Message Passing with Dynamic Pruning\nAbstract: Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs) is a challenging task. Recently, a line of message-passing-based research has been proposed to solve CQA. However, they perform unsatisfactorily on negative queries and fail to address the noisy messages between variable nodes in the query graph. Moreover, they offer little interpretability and require complex query data and resource-intensive training. In this paper, we propose a Neural-Symbolic Message Passing (NSMP) framework based on pre-trained neural link predictors. By introducing symbolic reasoning and fuzzy logic, NSMP can generalize to arbitrary existential first order logic queries without requiring training while providing interpretable answers. Furthermore, we introduce a dynamic pruning strategy to filter out noisy messages between variable nodes. Experimental results show that NSMP achieves a strong performance. Additionally, through complexity analysis and empirical verification, we demonstrate the superiority of NSMP in inference time over the current state-of-the-art neural-symbolic method. Compared to this approach, NSMP demonstrates faster inference times across all query types on benchmark dat\nSection name: More Details about the Datasets\n\nFigure block (optional):\n- label: \\label{figure4}; caption: \\caption{Graphical representation of the query types of the BetaE dataset considered in our experiment, where $p$, $i$, $u$, and $n$ represent projection, intersection, union, and negation, respectively. }\n- label: \\label{figure5}; caption: \\caption{Graphical representation of the query types of the FIT dataset considered in our experiment, where $l$, $m$, and $c$ represent existential leaf, multi graph, and circle, respectively. }\nTable block (optional):\n- label: \\label{KG stat}; text: \\begin{tabular}{ccccccc}     \\toprule     \\textbf{Knowledge Graph} & \\textbf{Entities} & \\textbf{Relations} & \\textbf{Training Edges} & \\textbf{Val Edges} & \\textbf{Test Edges} & \\textbf{Total Edges} \\\\     \\midrule     FB15k-237 & 14,505 &\n- label: \\label{BetaE stat}; text: \\begin{tabular}{ccccccc}     \\toprule     \\multirow{2}{*}{\\textbf{Knowledge Graph}} & \\multicolumn{2}{c}{\\textbf{Training Queries}} & \\multicolumn{2}{c}{\\textbf{Validation Queries}} & \\multicolumn{2}{c}{\\textbf{Test Queries}} \\\\     \\cmidru\n- label: \\label{FIT stat}; text: \\begin{tabular}{ccccccccccc}     \\toprule     \\textbf{Knowledge Graph} & \\textbf{pni} & \\textbf{2il} & \\textbf{3il} & \\textbf{2m} & \\textbf{2nm} & \\textbf{3mp} & \\textbf{3pm} & \\textbf{im} & \\textbf{3c} & \\textbf{3cm} \\\\     \\midrule     FB\nCited paper titles/abstracts (optional):\n- [ren2020beta] Beta embeddings for multi-hop logical reasoning in knowledge graphs: \n- [yin2024rethinking] Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors: \n\nk-most adjacent paragraphs (context):\n(none)\n\nTarget length (characters): ~973 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'figure4\', \'figure5\']\ntable_labels = [\'KG stat\', \'BetaE stat\', \'FIT stat\']\nbib_keys = [\'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Progressive Prompts: Continual Learning for Language Models\nAbstract: We introduce Progressive Prompts - a simple and efficient approach for continual learning in language models. Our method allows forward transfer and resists catastrophic forgetting, without relying on data replay or a large number of task-specific parameters. Progressive Prompts learns a new soft prompt for each task and sequentially concatenates it with the previously learned prompts, while keeping the base model frozen. Experiments on standard continual learning benchmarks show that our approach outperforms state-of-the-art methods, with an improvement >20% in average test accuracy over the previous best-preforming method on T5 model. We also explore a more challenging continual learning setup with longer sequences of tasks and show that Progressive Prompts significantly outperforms prior methods.\nSection name: Further experimental results\n\nFigure block (optional):\n- label: \\label{fig:fwt_order8}; caption: \\caption{Forward transfer score of different approaches on order 8. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:fwt_order9}; caption: \\caption{Forward transfer score of different approaches on order 9. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:fwt_order10}; caption: \\caption{Forward transfer score of different approaches on order 10. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order8}; caption: \\caption{Backward transfer score of different approaches on order 8. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order9}; caption: \\caption{Backward transfer score of different approaches on order 9. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order10}; caption: \\caption{Backward transfer score of different approaches on order 10. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:evolution}; caption: \\caption{Evolution of average accuracy after learning new tasks.}\n- label: \\label{fig:fig_long_bars}; caption: \\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improve\nTable block (optional):\n- label: \\label{wrap-tab:init}; text: \\begin{tabular}{lcc}\\\\\\toprule   Method & Few-shot & Full-shot \\\\\\midrule PT + Prev. Init. & 48.2 & 50.0 \\\\  ProgPrompt & \\textbf{53.5} & \\textbf{69.3} \\\\ \\bottomrule \\end{tabular}\n- label: \\label{tab:table_appendix_long}; text: \\begin{tabular}{lccc|ccc|ccc|ccc} % <-- Alignments: 1st column left & 2nd middle and 3rd right & with vertical lines in between     \\toprule       \\textbf{Method} $\\downarrow$ & \\multicolumn{3}{c}{\\textbf{Order 8}}  & \\multicolumn{3}{c}{\\te\nCited paper titles/abstracts (optional):\n- [lopez2017gradient] Gradient episodic memory for continual learning: \n\nk-most adjacent paragraphs (context):\nNext:\n1. \\includegraphics[width=0.9\\textwidth]{images/long_exp_bars.png}\n\\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improvement of Progressive Prompts test score compared to per-task prompt on the corresponding task.}\n\\label{fig:fig_long_bars}\n\\end{figure}\n2. \\includegraphics[width=0.9\\textwidth]{images/long_exp_bars.png}\n\\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improvement of Progressive Prompts test score compared to per-task prompt on the corresponding task.}\n\\label{fig:fig_long_bars}\n3. \\includegraphics[width=0.7\\textwidth]{images/superglue_illustration.png}\n4. \\caption{Original prompt tuning versus Progressive Prompts on SuperGLUE datasets. For illustration, we show how SuperGLUE task WiC is leaned (we have similar scheme for other tasks). Prompt tuning trains a single prompt of 100 tokens for WiC task. Progressive Prompts method learns a prompt of 40 tokens, which is progressively appended to the six frozen prompts of 10 tokens learned on GLUE benchmark (with random task order). Total prompt length is equal in both approaches. }\n\\label{fig:fig_super_illustration}\n\\end{figure}\n5. \\includegraphics[width=0.7\\textwidth]{images/superglue_illustration.png}\n\nTarget length (characters): ~1627 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:fwt_order8\', \'fig:fwt_order9\', \'fig:fwt_order10\', \'fig:bwt_order8\', \'fig:bwt_order9\', \'fig:bwt_order10\', \'fig:evolution\', \'fig:fig_long_bars\']\ntable_labels = [\'wrap-tab:init\', \'tab:table_appendix_long\']\nbib_keys = [\'lopez2017gradient\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).']
Number of prompts: 6
Image tag lists: [[['contains scatter plot']], [None, None], [None], [None, None, None, None], [['a figure showing network'], ['a figure showing network']], [['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about bar chart']]]
Number of Image tag lists: 6
Number of Image projection lists: 6
INFO 08-31 20:17:45 [utils.py:326] non-default args: {'model': 'Qwen/Qwen3-8B', 'trust_remote_code': True, 'dtype': 'float16', 'gpu_memory_utilization': 0.6, 'disable_log_stats': True, 'enforce_eager': True}
INFO 08-31 20:17:52 [__init__.py:711] Resolved architecture: Qwen3ForCausalLM
WARNING 08-31 20:17:52 [__init__.py:2819] Casting torch.bfloat16 to torch.float16.
INFO 08-31 20:17:52 [__init__.py:1750] Using max model len 40960
INFO 08-31 20:17:53 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 08-31 20:17:53 [__init__.py:3565] Cudagraph is disabled under eager mode
WARNING 08-31 20:17:53 [__init__.py:2921] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized
INFO 08-31 20:18:00 [__init__.py:241] Automatically detected platform cuda.
[1;36m(EngineCore_0 pid=307134)[0;0m INFO 08-31 20:18:02 [core.py:636] Waiting for init message from front-end.
[1;36m(EngineCore_0 pid=307134)[0;0m INFO 08-31 20:18:02 [core.py:74] Initializing a V1 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen3-8B', speculative_config=None, tokenizer='Qwen/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{},"max_capture_size":0,"local_cache_dir":null}
[1;36m(EngineCore_0 pid=307134)[0;0m INFO 08-31 20:18:03 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_0 pid=307134)[0;0m WARNING 08-31 20:18:03 [topk_topp_sampler.py:61] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_0 pid=307134)[0;0m INFO 08-31 20:18:04 [gpu_model_runner.py:1953] Starting to load model Qwen/Qwen3-8B...
[1;36m(EngineCore_0 pid=307134)[0;0m INFO 08-31 20:18:04 [gpu_model_runner.py:1985] Loading model from scratch...
[1;36m(EngineCore_0 pid=307134)[0;0m INFO 08-31 20:18:04 [cuda.py:328] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_0 pid=307134)[0;0m INFO 08-31 20:18:04 [weight_utils.py:296] Using model weights format ['*.safetensors']
[1;36m(EngineCore_0 pid=307134)[0;0m INFO 08-31 20:18:08 [default_loader.py:262] Loading weights took 4.13 seconds
[1;36m(EngineCore_0 pid=307134)[0;0m INFO 08-31 20:18:09 [gpu_model_runner.py:2007] Model loading took 15.2683 GiB and 4.548690 seconds
[1;36m(EngineCore_0 pid=307134)[0;0m INFO 08-31 20:18:11 [gpu_worker.py:276] Available KV cache memory: 11.78 GiB
[1;36m(EngineCore_0 pid=307134)[0;0m INFO 08-31 20:18:11 [kv_cache_utils.py:849] GPU KV cache size: 85,792 tokens
[1;36m(EngineCore_0 pid=307134)[0;0m INFO 08-31 20:18:11 [kv_cache_utils.py:853] Maximum concurrency for 40,960 tokens per request: 2.09x
[1;36m(EngineCore_0 pid=307134)[0;0m INFO 08-31 20:18:11 [core.py:214] init engine (profile, create kv cache, warmup model) took 2.23 seconds
[1;36m(EngineCore_0 pid=307134)[0;0m ERROR 08-31 20:18:11 [config.py:130] Error retrieving file list: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/Qwen/Qwen3-8B/tree/main?recursive=True&expand=False, retrying 1 of 2
[1;36m(EngineCore_0 pid=307134)[0;0m ERROR 08-31 20:18:13 [config.py:128] Error retrieving file list: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/models/Qwen/Qwen3-8B/tree/main?recursive=True&expand=False
[1;36m(EngineCore_0 pid=307134)[0;0m INFO 08-31 20:18:14 [__init__.py:3565] Cudagraph is disabled under eager mode
INFO 08-31 20:18:14 [llm.py:298] Supported_tasks: ['generate']
ERROR 08-31 20:18:15 [core_client.py:562] Engine core proc EngineCore_0 died unexpectedly, shutting down client.
INFO 08-31 20:25:31 [__init__.py:241] Automatically detected platform cuda.
CLIP image features: (1, 512)
Top CLIP tags: ['contains scatter plot']
Image tag list: [['contains scatter plot']]
[WARN] Path not found: download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Path not found: download/output/figures/2310.07894v1/tex_figures_5d.png
Image tag list: [None, None]
[WARN] Path not found: download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
Image tag list: [None]
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_demog
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_time
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_model
[WARN] Path not found: download/output/figures/2508.00717/results_ai_pays_dem
Image tag list: [None, None, None, None]
CLIP image features: (1, 512)
Top CLIP tags: ['a figure showing network']
CLIP image features: (1, 512)
Top CLIP tags: ['a figure showing network']
Image tag list: [['a figure showing network'], ['a figure showing network']]
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about bar chart']
Image tag list: [['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about bar chart']]
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
Prompt list: ['You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Manifold Preserving Guided Diffusion\nAbstract: Despite the recent advancements, conditional image generation still faces challenges of cost, generalizability, and the need for task-specific training. In this paper, we propose Manifold Preserving Guided Diffusion (MPGD), a training-free conditional generation framework that leverages pretrained diffusion models and off-the-shelf neural networks with minimal additional inference cost for a broad range of tasks. Specifically, we leverage the manifold hypothesis to refine the guided diffusion steps and introduce a shortcut algorithm in the process. We then propose two methods for on-manifold training-free guidance using pre-trained autoencoders and demonstrate that our shortcut inherently preserves the manifolds when applied to latent diffusion models. Our experiments show that MPGD is efficient and effective for solving a variety of conditional generation applications in low-compute settings, and can consistently offer up to 3.8x speed-ups with the same number of diffusion steps while maintaining high sample quality compared to the baselines.\nSection name: Experiments\n\nFigure block (optional):\n- label: \\label{fig:qualitative_faceid}; caption: \\caption{Examples of the FaceID guidance generation with pre-trained CelebA-HQ models. }\nTable block (optional):\n- label: \\label{tab:faceid}; text: \\begin{tabular}{@{}l|ccc@{}} \\toprule \\textbf{Method} & \\textbf{KID}$\\downarrow$        & \\textbf{FaceID}$\\downarrow$ & \\textbf{Time}$\\downarrow$ \\\\ \\midrule DDIM            & 0.0442          & 1.3914               & 3.41s             \\\\ \\m\nCited paper titles/abstracts (optional):\n- [lgd] Loss-guided diffusion models for plug-and-play controllable generation.: \n- [freedom] FreeDoM: Training-free energy-guided conditional diffusion model.: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. \\vspace{-5pt}\n\\subsection{Pixel space diffusion models}\nIn this section, we evaluate the performance of our proposed pixel domain methods (i.e. \\textbf{MPGD w/o Proj.}, \\textbf{MPGD-AE} and \\textbf{MPGD-Z}) with two different sets of conditional image generation tasks: solving liner inverse problems and human face generation guided by face recognition loss, which we refer to as FaceID guidance generation. For MPGD-AE and MPGD-Z, we use the pre-trained VQGAN models provided by~\\citet{rombach2021highresolution}.\n\\subsubsection{Noisy Linear Inverse Problem}\n2. For linear tasks, we use noisy super-resolution and noisy Gaussian deblurring as the test bed. We choose DPS~\\citep{dps}, LGD-MC~\\citep{lgd}, and MCG~\\citep{mcg} as the basleines. We test each method with two pre-trained diffusion models provided by~\\citet{dps}: one trained on FFHQ dataset~\\citep{karras2019style} and another on ImageNet~\\citep{deng2009imagenet}, both with $256 \\times 256$256 \\times 256 resolution. \nFor super-resolution, we down-sample ground truth images to $64 \\times 64$64 \\times 64. In the Gaussian deblurring task, we apply a $61\\times 61$61\\times 61 sized Gaussian blur with kernel intensity $3.0$3.0. to original images, Measurements in both tasks have a random noise with a variance of $\\sigma^{2}=0.05^{2}$\\sigma^{2}2=0.05^{2}2. We evaluate each task on a set of 1000~samples. We use the Kernel Inception distance (KID)~\\citep{binkowski2018demystifying} to assess the fidelity, Learned Perceptual Image Patch Similarity (LPIPS)~\\citep{zhang2018unreasonable} to evaluate the guidance quality, and the inference time to test the efficiency of the methods. All experiments are conducted on a single NVIDIA GeForce RTX 2080 Ti GPU . Figure~\\ref{fig:linear_main} shows the generated examples for qualitative comparison, and Figure~\\ref{fig:ffhq_sr} presents the quantitative results for the super-resolution task on FFHQ. All three of our methods significantly outperform the baselines with all metrics tested across a variety of different numbers of DDIM steps, and we can observe manifold projection improves the sample fidelity by a large margin.\nNext:\n1. \\hfill\n    [t]{0.53\\textwidth}0.53\\textwidth\n        \\centering\n        \\caption{Quantitative results for style guidance with Stable Diffusion experiment. Our method finds the sweet spot between following the prompt and following the style guidance.}\n        \\label{tab:style}\n2. \\vspace{-7pt}\n\\subsection{Latent diffusion models}\nTo evaluate \\textbf{MPGD-LDM}, we test our methods against the same baselines as the pixel-space FaceID experiments with text-to-image style guided generation task. The goal of this task is to generate images that fit both the text input prompts and the style of the reference images.\nWe use Stable Diffusion~\\citep{rombach2021highresolution} as the pre-trained text-to-image model and deploy the guided sampling methods to incorporate a style loss, which is calculated by the Frobenius norm between the Gram matrices of the reference images and the generated images.\nFor reference style images and text prompts, we randomly created 1000 conditioning pairs, using images from WikiArt~\\citep{saleh2015large} and prompts from PartiPrompts~\\citep{yu2022scaling} dataset. \nFigure~\\ref{fig:style_main} and Table~\\ref{tab:style} show qualitative and quantitative results for this experiment respectively. All the samples are generated on a single NVIDIA A100 GPU with 100 DDIM steps.\nOur method finds the sweet spot between following the text prompts, which usually instruct the generation to generate photo realistic images that do not suit the given style, and following the style guidance, which deviate from the prompts. Notably, because MPGD does not require propagation through the diffusion model, our method can provide significant speedup and can be fitted into a 16GB GPU while all the other methods cannot.\n3. \\vspace{-7pt}\n\nTarget length (characters): ~1212 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:qualitative_faceid\']\ntable_labels = [\'tab:faceid\']\nbib_keys = [\'freedom\', \'lgd\', \'freedom\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Efficient Integrators for Diffusion Generative Models\nAbstract: Diffusion models suffer from slow sample generation at inference time. Therefore, developing a principled framework for fast deterministic/stochastic sampling for a broader class of diffusion models is a promising direction. We propose two complementary frameworks for accelerating sample generation in pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate integrators generalize DDIM, mapping the reverse diffusion dynamics to a more amenable space for sampling. In contrast, splitting-based integrators, commonly used in molecular dynamics, reduce the numerical simulation error by cleverly alternating between numerical updates involving the data and auxiliary variables. After extensively studying these methods empirically and theoretically, we present a hybrid method that leads to the best-reported performance for diffusion models in augmented spaces. Applied to Phase Space Langevin Diffusion [Pandey & Mandt, 2023] on CIFAR-10, our deterministic and stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing baselines, respectively. Our code and model checkpoints wil\nSection name: Additional Experimental Results\n\nFigure block (optional):\n- label: \\label{fig:precond_a}; caption: \\caption{}\n- label: \\label{fig:precond_b}; caption: \\caption{}\nTable block (optional):\n- label: \\label{table:main}; text: \\begin{tabular}{@{}clcccc@{}} \\toprule                                         &               &                                                             &           & \\multicolumn{2}{c}{NFE (FID@50k $\\downarrow$)} \\\\ \\midrule           \nCited paper titles/abstracts (optional):\n- [songdenoising] Denoising diffusion implicit models.: \n- [lu2022dpm] Dpm-solver: A fast ode solver for diffusion probabilistic model   sampling in around 10 steps.: \n- [zhang2023fast] Fast sampling of diffusion models with exponential integrator.: \n- [karraselucidating] Elucidating the design space of diffusion-based generative models.: \n- [liupseudo] Pseudo numerical methods for diffusion models on manifolds.: \n- [bao2022analyticdpm] Analytic-DPM: an analytic estimate of the optimal reverse variance   in diffusion probabilistic models.: \n- [xue2023sasolver] Sa-solver: Stochastic adams solver for fast sampling of diffusion   models, 2023: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. \\textbf{Notation.} For simplicity, we denote our best-performing Reduced Splitting and Conjugate Splitting integrators as \\textit{\\gls{SPS}} and \\textit{\\gls{CSPS}}, respectively. Consequently, we refer to the \\underline{D}eterministic \\gls{RVV}RVV and \\underline{S}tochastic \\gls{ROBA}ROBA samplers as \\gls{SPS}SPS-D and \\gls{SPS}SPS-S, and their conjugate variants as \\gls{CSPS}CSPS-D and \\gls{CSPS}CSPS-S, respectively.\n2. \\textbf{Datasets and Evaluation Metrics.} We use the CIFAR-10, CelebA-64 \\citep{liu2015faceattributes} and the AFHQ-v2 \\citep{choi2020stargan} datasets for comparisons. Unless specified otherwise, we report FID for 50k generated samples for all datasets and quantify sampling efficiency using \\gls{NFE}NFE. We include full experimental details in Appendix \\ref{app:exp_setup}.\nNext:\n1. \\textbf{Empirical Observations:} For CIFAR-10, our ODE sampler performs comparably or outperforms all other baselines for \\gls{NFE}NFE $\\geq$\\geq 50 (Fig. \\ref{fig:sota_all}, Top Left). Similarly, our SDE sampler outperforms all other baselines for \\gls{NFE}NFE $\\geq$\\geq 40 (Fig. \\ref{fig:sota_all}, Top Right). We make similar observations for the CelebA-64 and AFHQv2-64 datasets, where our proposed samplers can obtain significant gains over prior methods for \\gls{NFE}NFE $\\geq$\\geq 70 (See Fig. \\ref{fig:sota_all}, Bottom Left). Therefore, our proposed samplers for \\gls{PSLD}PSLD are competitive with recent work. Moreover, for all datasets, our stochastic sampler achieves better sample quality for low sampling budgets (\\gls{NFE}NFE $<$< 50) as compared to our deterministic sampler. \nLastly, in contrast to CIFAR-10, we find that the \\gls{CSPS}CSPS-S sampler works better than the \\gls{SPS}SPS-S sampler for the CelebA-64 and AFHQv2-64 datasets, indicating its effectiveness for higher-resolution sampling.\n\nTarget length (characters): ~1323 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:precond_a\', \'fig:precond_b\']\ntable_labels = [\'table:main\']\nbib_keys = [\'songdenoising\', \'zhang2023fast\', \'lu2022dpm\', \'liupseudo\', \'karraselucidating\', \'xue2023sasolver\', \'bao2022analyticdpm\', \'karraselucidating\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average\nAbstract: Presently the most successful approaches to semi-supervised learning are based on consistency regularization, whereby a model is trained to be robust to small perturbations of its inputs and parameters. To understand consistency regularization, we conceptually explore how loss geometry interacts with training procedures. The consistency loss dramatically improves generalization performance over supervised-only training; however, we show that SGD struggles to converge on the consistency loss and continues to make large steps that lead to changes in predictions on the test data. Motivated by these observations, we propose to train consistency-based methods with Stochastic Weight Averaging (SWA), a recent approach which averages weights along the trajectory of SGD with a modified learning rate schedule. We also propose fast-SWA, which further accelerates convergence by averaging multiple points within each cycle of a cyclical learning rate schedule. With weight averaging, we achieve the best known semi-supervised results on CIFAR-10 and CIFAR-100, over many different quantities of labeled training data. For example, we achieve 5.0% error on CIFAR-10 with only 4000 labels, compared to \nSection name: Experiments\n\nFigure block (optional):\n- label: \\label{fig:cnn_c10_c100_trend}; caption: \\caption{Prediction errors of base models and their weight averages (\\fastswa and \\swa) for CNN    on \\textbf{(left)} CIFAR-10 with $4k$ labels,   \\textbf{(middle)} CIFAR-100 with $10k$ labels,   and \\textbf{(right)} CIFAR-100 $50k$ labels \nTable block (optional):\n- label: \\label{table:cifar100_shakeshake}; text: \\begin{tabular}{l c c c c c l l l l l } Number of labels\t\t\t& \t10k \t\t& \t50k \t\t& \t50k + 500k \t\t& 50k +   237k$^*$ \t\\\\  \\midrule TE (CNN) \\citep{te}  & \t38.65 \\tpm 0.51\t\t&\t26.30 \\tpm 0.15\t\t&\t23.62 \\tpm 0.17  \t&\t 23.79 \\tpm 0.17  \t\\\\  \\midrule \nCited paper titles/abstracts (optional):\n- [te] Temporal Ensembling for Semi-Supervised Learning.: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. In Figure \\ref{fig:cnn_c10_c100_trend} (left), we visualize the test\nerror as a function of iteration using the CNN.\nWe observe that when the cyclical learning rate starts after epoch $\\ell=180$\\ell=180, \nthe base models drop in performance due to the sudden increase in learning rate (see Figure \\ref{fig:swa_model} left).\nHowever, \\fastswa continues to improve while collecting the weights corresponding to high learning rates for averaging.\nIn general, we also find that the cyclical learning rate improves the base models beyond the usual cosine annealing \nschedule and increases the performance of \\fastswa as training progresses. \nCompared to \\swa, we also observe that \\fastswa converges substantially faster, \nfor instance, reducing the error to $10.5\\%$10.5\\%  at epoch $200$200 while \\swa attains \nsimilar error at epoch $350$350 for CIFAR-10 $4k$4k labels (Figure \n\\ref{fig:cnn_c10_c100_trend} left). We provide additional plots in Section \\ref{sec:add_results} \nshowing the convergence of the \\pi and MT models in all label \nsettings, where we observe similar trends that \\fastswa results in faster error reduction.\n2. We also find that the performance gains of \\fastswa over base models are higher for the \\pi model compared to the MT model, which is consistent with the convexity observation in Section \\ref{sec:empirical_convexity} \nand Figure \\ref{fig:convexity}.\nIn the previous evaluations \\citep[see e.g.][]{ssl-eval, mt}, the \\pi model was shown to be inferior to the MT model.\nHowever, with weight averaging, \\fastswa reduces the gap between \\pi and MT performance. \nSurprisingly, we find that the \\pi model can outperform MT after applying \\fastswa  \nwith moderate to large numbers of labeled points. \nIn particular, the $\\Pi$\\Pi+fast-SWA model outperforms MT+\\fastswa on CIFAR-10 with $4k$4k, $10k$10k, and $50k$50k labeled\ndata points for the Shake-Shake  architecture.\n3. \\vspace{-0.8\\baselineskip}\n4. \\subsection{CIFAR-100 and Extra Unlabeled Data} \\label{sec:cifar100_cnn} \\label{sec:cifar100}\n5. We evaluate the \\pi and MT models with \\fastswa on \nCIFAR-100.\nWe train our models using $50000$50000 images with $10k$10k and $50k$50k labels using the $13$13-layer CNN. \nWe also analyze the effect of using the Tiny Images dataset \\citep{tiny_images} as an additional source of unlabeled data.\nNext:\n1. \\subsection{Advancing State-of-the-Art} \\label{sec:soa}\n2. We have shown that \\fastswa can significantly improve the performance of both the \\pi and \nMT models. We provide a summary comparing our results with the previous best results in the literature in Table  \\ref{table:soa}, using the $13$13-layer CNN and\nthe Shake-Shake architecture that had been applied previously. We also provide detailed results the Appendix \\ref{sec:add_results}.\n3. \\subsection{Preliminary Results on Domain Adaptation}\n\\label{sec:domain_adaptation}\nDomain adaptation problems involve learning using a source domain $X_s$X_s \nequipped with labels $Y_s$Y_s and performing classification on the target \ndomain $X_t$X_t while having no access to the target labels at training time. \nA recent model by \\citet{da_mt} applies the consistency enforcing principle for \ndomain adaptation and achieves state-of-the-art results on many datasets. \nApplying \\fastswa to this model on domain adaptation from CIFAR-10 to STL we \nwere able to improve the best results reported in the literature from\n$19.9\\%$19.9\\% to $16.8\\%$16.8\\%. \nSee Section \\ref{sec:exp_da} for more details on the domain adaptation experiments.\n\nTarget length (characters): ~1438 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:cnn_c10_c100_trend\']\ntable_labels = [\'table:cifar100_shakeshake\']\nbib_keys = [\'te\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Generative AI in Higher Education: Evidence from an Elite College\nAbstract: Generative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPT\'s release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AI\'s potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AI\'s educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.\nSection name: Generative AI Usage Patterns Among Students\n\nFigure block (optional):\n- label: \\label{fig:ai_use}; caption: \\caption{The Adoption of Generative AI among Middlebury College Students}\n- label: \\label{fig:ai_use_time}; caption: \\caption{The Evolution of Generative AI Adoption among Middlebury College Students}\n- label: \\label{fig:ai_models}; caption: \\caption{Adoption of Generative AI Models Among College Students}\n- label: \\label{fig:ai_pays_dem}; caption: \\caption{Percent of Students Who Pay for Generative AI Tools}\nTable block (optional):\n- label: \\label{tab:ai_usage_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{8}{c}{Outcome: Uses AI during the semester with frequency of at least...}  \\\\\\cmidrule{3-10}  \t\t\t\t\t%\t\t\n- label: \\label{tab:ai_adopt_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{10}{c}{Outcome: Started using generative AI...}  \\\\\\cmidrule{3-12}  \t\t\t\t\t&& Before\n- label: \\label{tab:ai_freq_het}; text: \\begin{tabular}{lccccc} \t\t\t\t\t\t\\toprule \t\t\t\t\t\t&& \\multicolumn{4}{c}{By Usage Frequency} \\\\ \\cmidrule{3-6} \t\t\t\t\t\t& Any use & Rarely & Occasionally & Frequently & Very Frequently \\\\ \t\t\t\t\t\t& (1) & (2) & (3) & (4) & (5) \\\\ \\midrule\t\t\t\t\t\t \t\t\t\t\t\t\\\n- label: \\label{tab:ai_model_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{10}{c}{Outcome: =1 if student uses}  \\\\\\cmidrule{3-12}  \t\t\t\t\t&& OpenAI\'s && Google\nCited paper titles/abstracts (optional):\n- [bick.etal2025] The rapid adoption of generative AI.: \n- [mcclain2024] Americans\' use of ChatGPT is ticking up, but few trust its   election information: \n- [nam2023ai] 56\\% of college students have used ai on assignments or exams.: \n- [stohr.etal2024] \\"o: \n- [ravselj.etal2025] s: \n- [otis2024global] Global Evidence on Gender Gaps and Generative AI.: \n- [hartley.etal2025] The Labor Market Effects of Generative Artificial   Intelligence: \n- [humlum.vestergaard2025] Large Language Models, Small Labor Market Effects: \n- [carvajal2024] Will artificial intelligence get in the way of achieving gender   equality?: \n- [david1990dynamo] The dynamo and the computer: an historical perspective on the modern   productivity paradox.: \n- [gallup2024] AI in the Workplace: Answering 3 Big Questions: \n- [hall2003adoption] Adoption of new technology: \n- [stokey2021technology] Technology diffusion.: \n- [world2016world] World Bank: \n- [bick.etal2025] The rapid adoption of generative AI.: \n- [mcclain2024] Americans\' use of ChatGPT is ticking up, but few trust its   election information: \n- [nam2023ai] 56\\% of college students have used ai on assignments or exams.: \n- [stohr.etal2024] \\"o: \n- [ravselj.etal2025] s: \n- [otis2024global] Global Evidence on Gender Gaps and Generative AI.: \n- [hartley.etal2025] The Labor Market Effects of Generative Artificial   Intelligence: \n- [humlum.vestergaard2025] Large Language Models, Small Labor Market Effects: \n- [carvajal2024] Will artificial intelligence get in the way of achieving gender   equality?: \n- [david1990dynamo] The dynamo and the computer: an historical perspective on the modern   productivity paradox.: \n- [gallup2024] AI in the Workplace: Answering 3 Big Questions: \n- [hall2003adoption] Adoption of new technology: \n- [stokey2021technology] Technology diffusion.: \n- [world2016world] World Bank: \n\nk-most adjacent paragraphs (context):\n(none)\n\nTarget length (characters): ~13653 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:ai_use\', \'fig:ai_use_time\', \'fig:ai_models\', \'fig:ai_pays_dem\']\ntable_labels = [\'tab:ai_usage_correlates\', \'tab:ai_adopt_correlates\', \'tab:ai_freq_het\', \'tab:ai_model_correlates\']\nbib_keys = [\'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'nam2023ai\', \'stohr.etal2024\', \'carvajal2024\', \'ravselj.etal2025\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'david1990dynamo\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'hall2003adoption\', \'otis2024global\', \'nam2023ai\', \'stokey2021technology\', \'carvajal2024\', \'stohr.etal2024\', \'world2016world\', \'ravselj.etal2025\', \'carvajal2024\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Neural-Symbolic Message Passing with Dynamic Pruning\nAbstract: Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs) is a challenging task. Recently, a line of message-passing-based research has been proposed to solve CQA. However, they perform unsatisfactorily on negative queries and fail to address the noisy messages between variable nodes in the query graph. Moreover, they offer little interpretability and require complex query data and resource-intensive training. In this paper, we propose a Neural-Symbolic Message Passing (NSMP) framework based on pre-trained neural link predictors. By introducing symbolic reasoning and fuzzy logic, NSMP can generalize to arbitrary existential first order logic queries without requiring training while providing interpretable answers. Furthermore, we introduce a dynamic pruning strategy to filter out noisy messages between variable nodes. Experimental results show that NSMP achieves a strong performance. Additionally, through complexity analysis and empirical verification, we demonstrate the superiority of NSMP in inference time over the current state-of-the-art neural-symbolic method. Compared to this approach, NSMP demonstrates faster inference times across all query types on benchmark dat\nSection name: More Details about the Datasets\n\nFigure block (optional):\n- label: \\label{figure4}; caption: \\caption{Graphical representation of the query types of the BetaE dataset considered in our experiment, where $p$, $i$, $u$, and $n$ represent projection, intersection, union, and negation, respectively. }\n- label: \\label{figure5}; caption: \\caption{Graphical representation of the query types of the FIT dataset considered in our experiment, where $l$, $m$, and $c$ represent existential leaf, multi graph, and circle, respectively. }\nTable block (optional):\n- label: \\label{KG stat}; text: \\begin{tabular}{ccccccc}     \\toprule     \\textbf{Knowledge Graph} & \\textbf{Entities} & \\textbf{Relations} & \\textbf{Training Edges} & \\textbf{Val Edges} & \\textbf{Test Edges} & \\textbf{Total Edges} \\\\     \\midrule     FB15k-237 & 14,505 &\n- label: \\label{BetaE stat}; text: \\begin{tabular}{ccccccc}     \\toprule     \\multirow{2}{*}{\\textbf{Knowledge Graph}} & \\multicolumn{2}{c}{\\textbf{Training Queries}} & \\multicolumn{2}{c}{\\textbf{Validation Queries}} & \\multicolumn{2}{c}{\\textbf{Test Queries}} \\\\     \\cmidru\n- label: \\label{FIT stat}; text: \\begin{tabular}{ccccccccccc}     \\toprule     \\textbf{Knowledge Graph} & \\textbf{pni} & \\textbf{2il} & \\textbf{3il} & \\textbf{2m} & \\textbf{2nm} & \\textbf{3mp} & \\textbf{3pm} & \\textbf{im} & \\textbf{3c} & \\textbf{3cm} \\\\     \\midrule     FB\nCited paper titles/abstracts (optional):\n- [ren2020beta] Beta embeddings for multi-hop logical reasoning in knowledge graphs: \n- [yin2024rethinking] Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors: \n\nk-most adjacent paragraphs (context):\n(none)\n\nTarget length (characters): ~973 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'figure4\', \'figure5\']\ntable_labels = [\'KG stat\', \'BetaE stat\', \'FIT stat\']\nbib_keys = [\'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Progressive Prompts: Continual Learning for Language Models\nAbstract: We introduce Progressive Prompts - a simple and efficient approach for continual learning in language models. Our method allows forward transfer and resists catastrophic forgetting, without relying on data replay or a large number of task-specific parameters. Progressive Prompts learns a new soft prompt for each task and sequentially concatenates it with the previously learned prompts, while keeping the base model frozen. Experiments on standard continual learning benchmarks show that our approach outperforms state-of-the-art methods, with an improvement >20% in average test accuracy over the previous best-preforming method on T5 model. We also explore a more challenging continual learning setup with longer sequences of tasks and show that Progressive Prompts significantly outperforms prior methods.\nSection name: Further experimental results\n\nFigure block (optional):\n- label: \\label{fig:fwt_order8}; caption: \\caption{Forward transfer score of different approaches on order 8. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:fwt_order9}; caption: \\caption{Forward transfer score of different approaches on order 9. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:fwt_order10}; caption: \\caption{Forward transfer score of different approaches on order 10. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order8}; caption: \\caption{Backward transfer score of different approaches on order 8. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order9}; caption: \\caption{Backward transfer score of different approaches on order 9. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order10}; caption: \\caption{Backward transfer score of different approaches on order 10. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:evolution}; caption: \\caption{Evolution of average accuracy after learning new tasks.}\n- label: \\label{fig:fig_long_bars}; caption: \\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improve\nTable block (optional):\n- label: \\label{wrap-tab:init}; text: \\begin{tabular}{lcc}\\\\\\toprule   Method & Few-shot & Full-shot \\\\\\midrule PT + Prev. Init. & 48.2 & 50.0 \\\\  ProgPrompt & \\textbf{53.5} & \\textbf{69.3} \\\\ \\bottomrule \\end{tabular}\n- label: \\label{tab:table_appendix_long}; text: \\begin{tabular}{lccc|ccc|ccc|ccc} % <-- Alignments: 1st column left & 2nd middle and 3rd right & with vertical lines in between     \\toprule       \\textbf{Method} $\\downarrow$ & \\multicolumn{3}{c}{\\textbf{Order 8}}  & \\multicolumn{3}{c}{\\te\nCited paper titles/abstracts (optional):\n- [lopez2017gradient] Gradient episodic memory for continual learning: \n\nk-most adjacent paragraphs (context):\nNext:\n1. \\includegraphics[width=0.9\\textwidth]{images/long_exp_bars.png}\n\\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improvement of Progressive Prompts test score compared to per-task prompt on the corresponding task.}\n\\label{fig:fig_long_bars}\n\\end{figure}\n2. \\includegraphics[width=0.9\\textwidth]{images/long_exp_bars.png}\n\\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improvement of Progressive Prompts test score compared to per-task prompt on the corresponding task.}\n\\label{fig:fig_long_bars}\n3. \\includegraphics[width=0.7\\textwidth]{images/superglue_illustration.png}\n4. \\caption{Original prompt tuning versus Progressive Prompts on SuperGLUE datasets. For illustration, we show how SuperGLUE task WiC is leaned (we have similar scheme for other tasks). Prompt tuning trains a single prompt of 100 tokens for WiC task. Progressive Prompts method learns a prompt of 40 tokens, which is progressively appended to the six frozen prompts of 10 tokens learned on GLUE benchmark (with random task order). Total prompt length is equal in both approaches. }\n\\label{fig:fig_super_illustration}\n\\end{figure}\n5. \\includegraphics[width=0.7\\textwidth]{images/superglue_illustration.png}\n\nTarget length (characters): ~1627 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:fwt_order8\', \'fig:fwt_order9\', \'fig:fwt_order10\', \'fig:bwt_order8\', \'fig:bwt_order9\', \'fig:bwt_order10\', \'fig:evolution\', \'fig:fig_long_bars\']\ntable_labels = [\'wrap-tab:init\', \'tab:table_appendix_long\']\nbib_keys = [\'lopez2017gradient\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).']
Number of prompts: 6
Image tag lists: [[['contains scatter plot']], [None, None], [None], [None, None, None, None], [['a figure showing network'], ['a figure showing network']], [['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about bar chart']]]
Number of Image tag lists: 6
Number of Image projection lists: 6
INFO 08-31 20:26:20 [utils.py:326] non-default args: {'model': 'Qwen/Qwen3-8B', 'enable_prompt_embeds': True, 'trust_remote_code': True, 'dtype': 'float16', 'gpu_memory_utilization': 0.6, 'disable_log_stats': True, 'enforce_eager': True}
INFO 08-31 20:26:28 [__init__.py:711] Resolved architecture: Qwen3ForCausalLM
WARNING 08-31 20:26:28 [__init__.py:2819] Casting torch.bfloat16 to torch.float16.
INFO 08-31 20:26:28 [__init__.py:1750] Using max model len 40960
WARNING 08-31 20:26:28 [arg_utils.py:1770] --enable-prompt-embeds is not supported by the V1 Engine. Falling back to V0. 
WARNING 08-31 20:26:28 [arg_utils.py:1544] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.
INFO 08-31 20:26:28 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 08-31 20:26:28 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 08-31 20:26:28 [__init__.py:3565] Cudagraph is disabled under eager mode
INFO 08-31 20:26:28 [llm_engine.py:222] Initializing a V0 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen3-8B', speculative_config=None, tokenizer='Qwen/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{"enable_fusion":false,"enable_noop":false},"max_capture_size":0,"local_cache_dir":null}, use_cached_outputs=False, 
INFO 08-31 20:26:31 [cuda.py:436] Using Flash Attention backend.
INFO 08-31 20:26:31 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 08-31 20:26:31 [model_runner.py:1080] Starting to load model Qwen/Qwen3-8B...
INFO 08-31 20:26:32 [weight_utils.py:296] Using model weights format ['*.safetensors']
INFO 08-31 20:26:37 [default_loader.py:262] Loading weights took 5.17 seconds
INFO 08-31 20:26:38 [model_runner.py:1112] Model loading took 15.2683 GiB and 5.730333 seconds
INFO 08-31 20:26:39 [worker.py:295] Memory profiling takes 1.19 seconds
INFO 08-31 20:26:39 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.53GiB) x gpu_memory_utilization (0.60) = 28.52GiB
INFO 08-31 20:26:39 [worker.py:295] model weights take 15.27GiB; non_torch_memory takes 0.02GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 11.83GiB.
INFO 08-31 20:26:40 [executor_base.py:114] # cuda blocks: 5385, # CPU blocks: 1820
INFO 08-31 20:26:40 [executor_base.py:119] Maximum concurrency for 40960 tokens per request: 2.10x
INFO 08-31 20:26:42 [llm_engine.py:417] init engine (profile, create kv cache, warmup model) took 4.02 seconds
INFO 08-31 20:26:42 [llm.py:298] Supported_tasks: ['generate']
INFO 08-31 20:29:52 [__init__.py:241] Automatically detected platform cuda.
CLIP image features: (1, 512)
Top CLIP tags: ['contains scatter plot']
Image tag list: [['contains scatter plot']]
[WARN] Path not found: download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Path not found: download/output/figures/2310.07894v1/tex_figures_5d.png
Image tag list: [None, None]
[WARN] Path not found: download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
Image tag list: [None]
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_demog
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_time
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_model
[WARN] Path not found: download/output/figures/2508.00717/results_ai_pays_dem
Image tag list: [None, None, None, None]
CLIP image features: (1, 512)
Top CLIP tags: ['a figure showing network']
CLIP image features: (1, 512)
Top CLIP tags: ['a figure showing network']
Image tag list: [['a figure showing network'], ['a figure showing network']]
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about bar chart']
Image tag list: [['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about bar chart']]
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
Prompt list: ['You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Manifold Preserving Guided Diffusion\nAbstract: Despite the recent advancements, conditional image generation still faces challenges of cost, generalizability, and the need for task-specific training. In this paper, we propose Manifold Preserving Guided Diffusion (MPGD), a training-free conditional generation framework that leverages pretrained diffusion models and off-the-shelf neural networks with minimal additional inference cost for a broad range of tasks. Specifically, we leverage the manifold hypothesis to refine the guided diffusion steps and introduce a shortcut algorithm in the process. We then propose two methods for on-manifold training-free guidance using pre-trained autoencoders and demonstrate that our shortcut inherently preserves the manifolds when applied to latent diffusion models. Our experiments show that MPGD is efficient and effective for solving a variety of conditional generation applications in low-compute settings, and can consistently offer up to 3.8x speed-ups with the same number of diffusion steps while maintaining high sample quality compared to the baselines.\nSection name: Experiments\n\nFigure block (optional):\n- label: \\label{fig:qualitative_faceid}; caption: \\caption{Examples of the FaceID guidance generation with pre-trained CelebA-HQ models. }\nTable block (optional):\n- label: \\label{tab:faceid}; text: \\begin{tabular}{@{}l|ccc@{}} \\toprule \\textbf{Method} & \\textbf{KID}$\\downarrow$        & \\textbf{FaceID}$\\downarrow$ & \\textbf{Time}$\\downarrow$ \\\\ \\midrule DDIM            & 0.0442          & 1.3914               & 3.41s             \\\\ \\m\nCited paper titles/abstracts (optional):\n- [lgd] Loss-guided diffusion models for plug-and-play controllable generation.: \n- [freedom] FreeDoM: Training-free energy-guided conditional diffusion model.: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. \\vspace{-5pt}\n\\subsection{Pixel space diffusion models}\nIn this section, we evaluate the performance of our proposed pixel domain methods (i.e. \\textbf{MPGD w/o Proj.}, \\textbf{MPGD-AE} and \\textbf{MPGD-Z}) with two different sets of conditional image generation tasks: solving liner inverse problems and human face generation guided by face recognition loss, which we refer to as FaceID guidance generation. For MPGD-AE and MPGD-Z, we use the pre-trained VQGAN models provided by~\\citet{rombach2021highresolution}.\n\\subsubsection{Noisy Linear Inverse Problem}\n2. For linear tasks, we use noisy super-resolution and noisy Gaussian deblurring as the test bed. We choose DPS~\\citep{dps}, LGD-MC~\\citep{lgd}, and MCG~\\citep{mcg} as the basleines. We test each method with two pre-trained diffusion models provided by~\\citet{dps}: one trained on FFHQ dataset~\\citep{karras2019style} and another on ImageNet~\\citep{deng2009imagenet}, both with $256 \\times 256$256 \\times 256 resolution. \nFor super-resolution, we down-sample ground truth images to $64 \\times 64$64 \\times 64. In the Gaussian deblurring task, we apply a $61\\times 61$61\\times 61 sized Gaussian blur with kernel intensity $3.0$3.0. to original images, Measurements in both tasks have a random noise with a variance of $\\sigma^{2}=0.05^{2}$\\sigma^{2}2=0.05^{2}2. We evaluate each task on a set of 1000~samples. We use the Kernel Inception distance (KID)~\\citep{binkowski2018demystifying} to assess the fidelity, Learned Perceptual Image Patch Similarity (LPIPS)~\\citep{zhang2018unreasonable} to evaluate the guidance quality, and the inference time to test the efficiency of the methods. All experiments are conducted on a single NVIDIA GeForce RTX 2080 Ti GPU . Figure~\\ref{fig:linear_main} shows the generated examples for qualitative comparison, and Figure~\\ref{fig:ffhq_sr} presents the quantitative results for the super-resolution task on FFHQ. All three of our methods significantly outperform the baselines with all metrics tested across a variety of different numbers of DDIM steps, and we can observe manifold projection improves the sample fidelity by a large margin.\nNext:\n1. \\hfill\n    [t]{0.53\\textwidth}0.53\\textwidth\n        \\centering\n        \\caption{Quantitative results for style guidance with Stable Diffusion experiment. Our method finds the sweet spot between following the prompt and following the style guidance.}\n        \\label{tab:style}\n2. \\vspace{-7pt}\n\\subsection{Latent diffusion models}\nTo evaluate \\textbf{MPGD-LDM}, we test our methods against the same baselines as the pixel-space FaceID experiments with text-to-image style guided generation task. The goal of this task is to generate images that fit both the text input prompts and the style of the reference images.\nWe use Stable Diffusion~\\citep{rombach2021highresolution} as the pre-trained text-to-image model and deploy the guided sampling methods to incorporate a style loss, which is calculated by the Frobenius norm between the Gram matrices of the reference images and the generated images.\nFor reference style images and text prompts, we randomly created 1000 conditioning pairs, using images from WikiArt~\\citep{saleh2015large} and prompts from PartiPrompts~\\citep{yu2022scaling} dataset. \nFigure~\\ref{fig:style_main} and Table~\\ref{tab:style} show qualitative and quantitative results for this experiment respectively. All the samples are generated on a single NVIDIA A100 GPU with 100 DDIM steps.\nOur method finds the sweet spot between following the text prompts, which usually instruct the generation to generate photo realistic images that do not suit the given style, and following the style guidance, which deviate from the prompts. Notably, because MPGD does not require propagation through the diffusion model, our method can provide significant speedup and can be fitted into a 16GB GPU while all the other methods cannot.\n3. \\vspace{-7pt}\n\nTarget length (characters): ~1212 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:qualitative_faceid\']\ntable_labels = [\'tab:faceid\']\nbib_keys = [\'freedom\', \'lgd\', \'freedom\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Efficient Integrators for Diffusion Generative Models\nAbstract: Diffusion models suffer from slow sample generation at inference time. Therefore, developing a principled framework for fast deterministic/stochastic sampling for a broader class of diffusion models is a promising direction. We propose two complementary frameworks for accelerating sample generation in pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate integrators generalize DDIM, mapping the reverse diffusion dynamics to a more amenable space for sampling. In contrast, splitting-based integrators, commonly used in molecular dynamics, reduce the numerical simulation error by cleverly alternating between numerical updates involving the data and auxiliary variables. After extensively studying these methods empirically and theoretically, we present a hybrid method that leads to the best-reported performance for diffusion models in augmented spaces. Applied to Phase Space Langevin Diffusion [Pandey & Mandt, 2023] on CIFAR-10, our deterministic and stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing baselines, respectively. Our code and model checkpoints wil\nSection name: Additional Experimental Results\n\nFigure block (optional):\n- label: \\label{fig:precond_a}; caption: \\caption{}\n- label: \\label{fig:precond_b}; caption: \\caption{}\nTable block (optional):\n- label: \\label{table:main}; text: \\begin{tabular}{@{}clcccc@{}} \\toprule                                         &               &                                                             &           & \\multicolumn{2}{c}{NFE (FID@50k $\\downarrow$)} \\\\ \\midrule           \nCited paper titles/abstracts (optional):\n- [songdenoising] Denoising diffusion implicit models.: \n- [lu2022dpm] Dpm-solver: A fast ode solver for diffusion probabilistic model   sampling in around 10 steps.: \n- [zhang2023fast] Fast sampling of diffusion models with exponential integrator.: \n- [karraselucidating] Elucidating the design space of diffusion-based generative models.: \n- [liupseudo] Pseudo numerical methods for diffusion models on manifolds.: \n- [bao2022analyticdpm] Analytic-DPM: an analytic estimate of the optimal reverse variance   in diffusion probabilistic models.: \n- [xue2023sasolver] Sa-solver: Stochastic adams solver for fast sampling of diffusion   models, 2023: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. \\textbf{Notation.} For simplicity, we denote our best-performing Reduced Splitting and Conjugate Splitting integrators as \\textit{\\gls{SPS}} and \\textit{\\gls{CSPS}}, respectively. Consequently, we refer to the \\underline{D}eterministic \\gls{RVV}RVV and \\underline{S}tochastic \\gls{ROBA}ROBA samplers as \\gls{SPS}SPS-D and \\gls{SPS}SPS-S, and their conjugate variants as \\gls{CSPS}CSPS-D and \\gls{CSPS}CSPS-S, respectively.\n2. \\textbf{Datasets and Evaluation Metrics.} We use the CIFAR-10, CelebA-64 \\citep{liu2015faceattributes} and the AFHQ-v2 \\citep{choi2020stargan} datasets for comparisons. Unless specified otherwise, we report FID for 50k generated samples for all datasets and quantify sampling efficiency using \\gls{NFE}NFE. We include full experimental details in Appendix \\ref{app:exp_setup}.\nNext:\n1. \\textbf{Empirical Observations:} For CIFAR-10, our ODE sampler performs comparably or outperforms all other baselines for \\gls{NFE}NFE $\\geq$\\geq 50 (Fig. \\ref{fig:sota_all}, Top Left). Similarly, our SDE sampler outperforms all other baselines for \\gls{NFE}NFE $\\geq$\\geq 40 (Fig. \\ref{fig:sota_all}, Top Right). We make similar observations for the CelebA-64 and AFHQv2-64 datasets, where our proposed samplers can obtain significant gains over prior methods for \\gls{NFE}NFE $\\geq$\\geq 70 (See Fig. \\ref{fig:sota_all}, Bottom Left). Therefore, our proposed samplers for \\gls{PSLD}PSLD are competitive with recent work. Moreover, for all datasets, our stochastic sampler achieves better sample quality for low sampling budgets (\\gls{NFE}NFE $<$< 50) as compared to our deterministic sampler. \nLastly, in contrast to CIFAR-10, we find that the \\gls{CSPS}CSPS-S sampler works better than the \\gls{SPS}SPS-S sampler for the CelebA-64 and AFHQv2-64 datasets, indicating its effectiveness for higher-resolution sampling.\n\nTarget length (characters): ~1323 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:precond_a\', \'fig:precond_b\']\ntable_labels = [\'table:main\']\nbib_keys = [\'songdenoising\', \'zhang2023fast\', \'lu2022dpm\', \'liupseudo\', \'karraselucidating\', \'xue2023sasolver\', \'bao2022analyticdpm\', \'karraselucidating\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average\nAbstract: Presently the most successful approaches to semi-supervised learning are based on consistency regularization, whereby a model is trained to be robust to small perturbations of its inputs and parameters. To understand consistency regularization, we conceptually explore how loss geometry interacts with training procedures. The consistency loss dramatically improves generalization performance over supervised-only training; however, we show that SGD struggles to converge on the consistency loss and continues to make large steps that lead to changes in predictions on the test data. Motivated by these observations, we propose to train consistency-based methods with Stochastic Weight Averaging (SWA), a recent approach which averages weights along the trajectory of SGD with a modified learning rate schedule. We also propose fast-SWA, which further accelerates convergence by averaging multiple points within each cycle of a cyclical learning rate schedule. With weight averaging, we achieve the best known semi-supervised results on CIFAR-10 and CIFAR-100, over many different quantities of labeled training data. For example, we achieve 5.0% error on CIFAR-10 with only 4000 labels, compared to \nSection name: Experiments\n\nFigure block (optional):\n- label: \\label{fig:cnn_c10_c100_trend}; caption: \\caption{Prediction errors of base models and their weight averages (\\fastswa and \\swa) for CNN    on \\textbf{(left)} CIFAR-10 with $4k$ labels,   \\textbf{(middle)} CIFAR-100 with $10k$ labels,   and \\textbf{(right)} CIFAR-100 $50k$ labels \nTable block (optional):\n- label: \\label{table:cifar100_shakeshake}; text: \\begin{tabular}{l c c c c c l l l l l } Number of labels\t\t\t& \t10k \t\t& \t50k \t\t& \t50k + 500k \t\t& 50k +   237k$^*$ \t\\\\  \\midrule TE (CNN) \\citep{te}  & \t38.65 \\tpm 0.51\t\t&\t26.30 \\tpm 0.15\t\t&\t23.62 \\tpm 0.17  \t&\t 23.79 \\tpm 0.17  \t\\\\  \\midrule \nCited paper titles/abstracts (optional):\n- [te] Temporal Ensembling for Semi-Supervised Learning.: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. In Figure \\ref{fig:cnn_c10_c100_trend} (left), we visualize the test\nerror as a function of iteration using the CNN.\nWe observe that when the cyclical learning rate starts after epoch $\\ell=180$\\ell=180, \nthe base models drop in performance due to the sudden increase in learning rate (see Figure \\ref{fig:swa_model} left).\nHowever, \\fastswa continues to improve while collecting the weights corresponding to high learning rates for averaging.\nIn general, we also find that the cyclical learning rate improves the base models beyond the usual cosine annealing \nschedule and increases the performance of \\fastswa as training progresses. \nCompared to \\swa, we also observe that \\fastswa converges substantially faster, \nfor instance, reducing the error to $10.5\\%$10.5\\%  at epoch $200$200 while \\swa attains \nsimilar error at epoch $350$350 for CIFAR-10 $4k$4k labels (Figure \n\\ref{fig:cnn_c10_c100_trend} left). We provide additional plots in Section \\ref{sec:add_results} \nshowing the convergence of the \\pi and MT models in all label \nsettings, where we observe similar trends that \\fastswa results in faster error reduction.\n2. We also find that the performance gains of \\fastswa over base models are higher for the \\pi model compared to the MT model, which is consistent with the convexity observation in Section \\ref{sec:empirical_convexity} \nand Figure \\ref{fig:convexity}.\nIn the previous evaluations \\citep[see e.g.][]{ssl-eval, mt}, the \\pi model was shown to be inferior to the MT model.\nHowever, with weight averaging, \\fastswa reduces the gap between \\pi and MT performance. \nSurprisingly, we find that the \\pi model can outperform MT after applying \\fastswa  \nwith moderate to large numbers of labeled points. \nIn particular, the $\\Pi$\\Pi+fast-SWA model outperforms MT+\\fastswa on CIFAR-10 with $4k$4k, $10k$10k, and $50k$50k labeled\ndata points for the Shake-Shake  architecture.\n3. \\vspace{-0.8\\baselineskip}\n4. \\subsection{CIFAR-100 and Extra Unlabeled Data} \\label{sec:cifar100_cnn} \\label{sec:cifar100}\n5. We evaluate the \\pi and MT models with \\fastswa on \nCIFAR-100.\nWe train our models using $50000$50000 images with $10k$10k and $50k$50k labels using the $13$13-layer CNN. \nWe also analyze the effect of using the Tiny Images dataset \\citep{tiny_images} as an additional source of unlabeled data.\nNext:\n1. \\subsection{Advancing State-of-the-Art} \\label{sec:soa}\n2. We have shown that \\fastswa can significantly improve the performance of both the \\pi and \nMT models. We provide a summary comparing our results with the previous best results in the literature in Table  \\ref{table:soa}, using the $13$13-layer CNN and\nthe Shake-Shake architecture that had been applied previously. We also provide detailed results the Appendix \\ref{sec:add_results}.\n3. \\subsection{Preliminary Results on Domain Adaptation}\n\\label{sec:domain_adaptation}\nDomain adaptation problems involve learning using a source domain $X_s$X_s \nequipped with labels $Y_s$Y_s and performing classification on the target \ndomain $X_t$X_t while having no access to the target labels at training time. \nA recent model by \\citet{da_mt} applies the consistency enforcing principle for \ndomain adaptation and achieves state-of-the-art results on many datasets. \nApplying \\fastswa to this model on domain adaptation from CIFAR-10 to STL we \nwere able to improve the best results reported in the literature from\n$19.9\\%$19.9\\% to $16.8\\%$16.8\\%. \nSee Section \\ref{sec:exp_da} for more details on the domain adaptation experiments.\n\nTarget length (characters): ~1438 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:cnn_c10_c100_trend\']\ntable_labels = [\'table:cifar100_shakeshake\']\nbib_keys = [\'te\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Generative AI in Higher Education: Evidence from an Elite College\nAbstract: Generative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPT\'s release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AI\'s potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AI\'s educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.\nSection name: Generative AI Usage Patterns Among Students\n\nFigure block (optional):\n- label: \\label{fig:ai_use}; caption: \\caption{The Adoption of Generative AI among Middlebury College Students}\n- label: \\label{fig:ai_use_time}; caption: \\caption{The Evolution of Generative AI Adoption among Middlebury College Students}\n- label: \\label{fig:ai_models}; caption: \\caption{Adoption of Generative AI Models Among College Students}\n- label: \\label{fig:ai_pays_dem}; caption: \\caption{Percent of Students Who Pay for Generative AI Tools}\nTable block (optional):\n- label: \\label{tab:ai_usage_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{8}{c}{Outcome: Uses AI during the semester with frequency of at least...}  \\\\\\cmidrule{3-10}  \t\t\t\t\t%\t\t\n- label: \\label{tab:ai_adopt_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{10}{c}{Outcome: Started using generative AI...}  \\\\\\cmidrule{3-12}  \t\t\t\t\t&& Before\n- label: \\label{tab:ai_freq_het}; text: \\begin{tabular}{lccccc} \t\t\t\t\t\t\\toprule \t\t\t\t\t\t&& \\multicolumn{4}{c}{By Usage Frequency} \\\\ \\cmidrule{3-6} \t\t\t\t\t\t& Any use & Rarely & Occasionally & Frequently & Very Frequently \\\\ \t\t\t\t\t\t& (1) & (2) & (3) & (4) & (5) \\\\ \\midrule\t\t\t\t\t\t \t\t\t\t\t\t\\\n- label: \\label{tab:ai_model_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{10}{c}{Outcome: =1 if student uses}  \\\\\\cmidrule{3-12}  \t\t\t\t\t&& OpenAI\'s && Google\nCited paper titles/abstracts (optional):\n- [bick.etal2025] The rapid adoption of generative AI.: \n- [mcclain2024] Americans\' use of ChatGPT is ticking up, but few trust its   election information: \n- [nam2023ai] 56\\% of college students have used ai on assignments or exams.: \n- [stohr.etal2024] \\"o: \n- [ravselj.etal2025] s: \n- [otis2024global] Global Evidence on Gender Gaps and Generative AI.: \n- [hartley.etal2025] The Labor Market Effects of Generative Artificial   Intelligence: \n- [humlum.vestergaard2025] Large Language Models, Small Labor Market Effects: \n- [carvajal2024] Will artificial intelligence get in the way of achieving gender   equality?: \n- [david1990dynamo] The dynamo and the computer: an historical perspective on the modern   productivity paradox.: \n- [gallup2024] AI in the Workplace: Answering 3 Big Questions: \n- [hall2003adoption] Adoption of new technology: \n- [stokey2021technology] Technology diffusion.: \n- [world2016world] World Bank: \n- [bick.etal2025] The rapid adoption of generative AI.: \n- [mcclain2024] Americans\' use of ChatGPT is ticking up, but few trust its   election information: \n- [nam2023ai] 56\\% of college students have used ai on assignments or exams.: \n- [stohr.etal2024] \\"o: \n- [ravselj.etal2025] s: \n- [otis2024global] Global Evidence on Gender Gaps and Generative AI.: \n- [hartley.etal2025] The Labor Market Effects of Generative Artificial   Intelligence: \n- [humlum.vestergaard2025] Large Language Models, Small Labor Market Effects: \n- [carvajal2024] Will artificial intelligence get in the way of achieving gender   equality?: \n- [david1990dynamo] The dynamo and the computer: an historical perspective on the modern   productivity paradox.: \n- [gallup2024] AI in the Workplace: Answering 3 Big Questions: \n- [hall2003adoption] Adoption of new technology: \n- [stokey2021technology] Technology diffusion.: \n- [world2016world] World Bank: \n\nk-most adjacent paragraphs (context):\n(none)\n\nTarget length (characters): ~13653 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:ai_use\', \'fig:ai_use_time\', \'fig:ai_models\', \'fig:ai_pays_dem\']\ntable_labels = [\'tab:ai_usage_correlates\', \'tab:ai_adopt_correlates\', \'tab:ai_freq_het\', \'tab:ai_model_correlates\']\nbib_keys = [\'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'nam2023ai\', \'stohr.etal2024\', \'carvajal2024\', \'ravselj.etal2025\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'david1990dynamo\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'hall2003adoption\', \'otis2024global\', \'nam2023ai\', \'stokey2021technology\', \'carvajal2024\', \'stohr.etal2024\', \'world2016world\', \'ravselj.etal2025\', \'carvajal2024\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Neural-Symbolic Message Passing with Dynamic Pruning\nAbstract: Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs) is a challenging task. Recently, a line of message-passing-based research has been proposed to solve CQA. However, they perform unsatisfactorily on negative queries and fail to address the noisy messages between variable nodes in the query graph. Moreover, they offer little interpretability and require complex query data and resource-intensive training. In this paper, we propose a Neural-Symbolic Message Passing (NSMP) framework based on pre-trained neural link predictors. By introducing symbolic reasoning and fuzzy logic, NSMP can generalize to arbitrary existential first order logic queries without requiring training while providing interpretable answers. Furthermore, we introduce a dynamic pruning strategy to filter out noisy messages between variable nodes. Experimental results show that NSMP achieves a strong performance. Additionally, through complexity analysis and empirical verification, we demonstrate the superiority of NSMP in inference time over the current state-of-the-art neural-symbolic method. Compared to this approach, NSMP demonstrates faster inference times across all query types on benchmark dat\nSection name: More Details about the Datasets\n\nFigure block (optional):\n- label: \\label{figure4}; caption: \\caption{Graphical representation of the query types of the BetaE dataset considered in our experiment, where $p$, $i$, $u$, and $n$ represent projection, intersection, union, and negation, respectively. }\n- label: \\label{figure5}; caption: \\caption{Graphical representation of the query types of the FIT dataset considered in our experiment, where $l$, $m$, and $c$ represent existential leaf, multi graph, and circle, respectively. }\nTable block (optional):\n- label: \\label{KG stat}; text: \\begin{tabular}{ccccccc}     \\toprule     \\textbf{Knowledge Graph} & \\textbf{Entities} & \\textbf{Relations} & \\textbf{Training Edges} & \\textbf{Val Edges} & \\textbf{Test Edges} & \\textbf{Total Edges} \\\\     \\midrule     FB15k-237 & 14,505 &\n- label: \\label{BetaE stat}; text: \\begin{tabular}{ccccccc}     \\toprule     \\multirow{2}{*}{\\textbf{Knowledge Graph}} & \\multicolumn{2}{c}{\\textbf{Training Queries}} & \\multicolumn{2}{c}{\\textbf{Validation Queries}} & \\multicolumn{2}{c}{\\textbf{Test Queries}} \\\\     \\cmidru\n- label: \\label{FIT stat}; text: \\begin{tabular}{ccccccccccc}     \\toprule     \\textbf{Knowledge Graph} & \\textbf{pni} & \\textbf{2il} & \\textbf{3il} & \\textbf{2m} & \\textbf{2nm} & \\textbf{3mp} & \\textbf{3pm} & \\textbf{im} & \\textbf{3c} & \\textbf{3cm} \\\\     \\midrule     FB\nCited paper titles/abstracts (optional):\n- [ren2020beta] Beta embeddings for multi-hop logical reasoning in knowledge graphs: \n- [yin2024rethinking] Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors: \n\nk-most adjacent paragraphs (context):\n(none)\n\nTarget length (characters): ~973 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'figure4\', \'figure5\']\ntable_labels = [\'KG stat\', \'BetaE stat\', \'FIT stat\']\nbib_keys = [\'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Progressive Prompts: Continual Learning for Language Models\nAbstract: We introduce Progressive Prompts - a simple and efficient approach for continual learning in language models. Our method allows forward transfer and resists catastrophic forgetting, without relying on data replay or a large number of task-specific parameters. Progressive Prompts learns a new soft prompt for each task and sequentially concatenates it with the previously learned prompts, while keeping the base model frozen. Experiments on standard continual learning benchmarks show that our approach outperforms state-of-the-art methods, with an improvement >20% in average test accuracy over the previous best-preforming method on T5 model. We also explore a more challenging continual learning setup with longer sequences of tasks and show that Progressive Prompts significantly outperforms prior methods.\nSection name: Further experimental results\n\nFigure block (optional):\n- label: \\label{fig:fwt_order8}; caption: \\caption{Forward transfer score of different approaches on order 8. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:fwt_order9}; caption: \\caption{Forward transfer score of different approaches on order 9. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:fwt_order10}; caption: \\caption{Forward transfer score of different approaches on order 10. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order8}; caption: \\caption{Backward transfer score of different approaches on order 8. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order9}; caption: \\caption{Backward transfer score of different approaches on order 9. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order10}; caption: \\caption{Backward transfer score of different approaches on order 10. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:evolution}; caption: \\caption{Evolution of average accuracy after learning new tasks.}\n- label: \\label{fig:fig_long_bars}; caption: \\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improve\nTable block (optional):\n- label: \\label{wrap-tab:init}; text: \\begin{tabular}{lcc}\\\\\\toprule   Method & Few-shot & Full-shot \\\\\\midrule PT + Prev. Init. & 48.2 & 50.0 \\\\  ProgPrompt & \\textbf{53.5} & \\textbf{69.3} \\\\ \\bottomrule \\end{tabular}\n- label: \\label{tab:table_appendix_long}; text: \\begin{tabular}{lccc|ccc|ccc|ccc} % <-- Alignments: 1st column left & 2nd middle and 3rd right & with vertical lines in between     \\toprule       \\textbf{Method} $\\downarrow$ & \\multicolumn{3}{c}{\\textbf{Order 8}}  & \\multicolumn{3}{c}{\\te\nCited paper titles/abstracts (optional):\n- [lopez2017gradient] Gradient episodic memory for continual learning: \n\nk-most adjacent paragraphs (context):\nNext:\n1. \\includegraphics[width=0.9\\textwidth]{images/long_exp_bars.png}\n\\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improvement of Progressive Prompts test score compared to per-task prompt on the corresponding task.}\n\\label{fig:fig_long_bars}\n\\end{figure}\n2. \\includegraphics[width=0.9\\textwidth]{images/long_exp_bars.png}\n\\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improvement of Progressive Prompts test score compared to per-task prompt on the corresponding task.}\n\\label{fig:fig_long_bars}\n3. \\includegraphics[width=0.7\\textwidth]{images/superglue_illustration.png}\n4. \\caption{Original prompt tuning versus Progressive Prompts on SuperGLUE datasets. For illustration, we show how SuperGLUE task WiC is leaned (we have similar scheme for other tasks). Prompt tuning trains a single prompt of 100 tokens for WiC task. Progressive Prompts method learns a prompt of 40 tokens, which is progressively appended to the six frozen prompts of 10 tokens learned on GLUE benchmark (with random task order). Total prompt length is equal in both approaches. }\n\\label{fig:fig_super_illustration}\n\\end{figure}\n5. \\includegraphics[width=0.7\\textwidth]{images/superglue_illustration.png}\n\nTarget length (characters): ~1627 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:fwt_order8\', \'fig:fwt_order9\', \'fig:fwt_order10\', \'fig:bwt_order8\', \'fig:bwt_order9\', \'fig:bwt_order10\', \'fig:evolution\', \'fig:fig_long_bars\']\ntable_labels = [\'wrap-tab:init\', \'tab:table_appendix_long\']\nbib_keys = [\'lopez2017gradient\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).']
Number of prompts: 6
Image tag lists: [[['contains scatter plot']], [None, None], [None], [None, None, None, None], [['a figure showing network'], ['a figure showing network']], [['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about bar chart']]]
Number of Image tag lists: 6
Number of Image projection lists: 6
INFO 08-31 20:30:36 [utils.py:326] non-default args: {'model': 'Qwen/Qwen3-8B', 'enable_prompt_embeds': True, 'trust_remote_code': True, 'dtype': 'float16', 'gpu_memory_utilization': 0.6, 'disable_log_stats': True, 'enforce_eager': True}
INFO 08-31 20:30:43 [__init__.py:711] Resolved architecture: Qwen3ForCausalLM
WARNING 08-31 20:30:43 [__init__.py:2819] Casting torch.bfloat16 to torch.float16.
INFO 08-31 20:30:43 [__init__.py:1750] Using max model len 40960
WARNING 08-31 20:30:43 [arg_utils.py:1770] --enable-prompt-embeds is not supported by the V1 Engine. Falling back to V0. 
WARNING 08-31 20:30:43 [arg_utils.py:1544] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.
INFO 08-31 20:30:43 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 08-31 20:30:43 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 08-31 20:30:43 [__init__.py:3565] Cudagraph is disabled under eager mode
INFO 08-31 20:30:43 [llm_engine.py:222] Initializing a V0 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen3-8B', speculative_config=None, tokenizer='Qwen/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{"enable_fusion":false,"enable_noop":false},"max_capture_size":0,"local_cache_dir":null}, use_cached_outputs=False, 
INFO 08-31 20:30:45 [cuda.py:436] Using Flash Attention backend.
INFO 08-31 20:30:46 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 08-31 20:30:46 [model_runner.py:1080] Starting to load model Qwen/Qwen3-8B...
INFO 08-31 20:30:46 [weight_utils.py:296] Using model weights format ['*.safetensors']
INFO 08-31 20:30:52 [default_loader.py:262] Loading weights took 5.12 seconds
INFO 08-31 20:30:52 [model_runner.py:1112] Model loading took 15.2683 GiB and 5.549776 seconds
INFO 08-31 20:30:54 [worker.py:295] Memory profiling takes 1.14 seconds
INFO 08-31 20:30:54 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.53GiB) x gpu_memory_utilization (0.60) = 28.52GiB
INFO 08-31 20:30:54 [worker.py:295] model weights take 15.27GiB; non_torch_memory takes 0.02GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 11.83GiB.
INFO 08-31 20:30:54 [executor_base.py:114] # cuda blocks: 5385, # CPU blocks: 1820
INFO 08-31 20:30:54 [executor_base.py:119] Maximum concurrency for 40960 tokens per request: 2.10x
INFO 08-31 20:30:56 [llm_engine.py:417] init engine (profile, create kv cache, warmup model) took 3.56 seconds
INFO 08-31 20:30:56 [llm.py:298] Supported_tasks: ['generate']
INFO 08-31 20:36:14 [__init__.py:241] Automatically detected platform cuda.
CLIP image features: (1, 512)
Top CLIP tags: ['contains scatter plot']
Image tag list: [['contains scatter plot']]
[WARN] Path not found: download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Path not found: download/output/figures/2310.07894v1/tex_figures_5d.png
Image tag list: [None, None]
[WARN] Path not found: download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
Image tag list: [None]
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_demog
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_time
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_model
[WARN] Path not found: download/output/figures/2508.00717/results_ai_pays_dem
Image tag list: [None, None, None, None]
CLIP image features: (1, 512)
Top CLIP tags: ['a figure showing network']
CLIP image features: (1, 512)
Top CLIP tags: ['a figure showing network']
Image tag list: [['a figure showing network'], ['a figure showing network']]
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about bar chart']
Image tag list: [['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about bar chart']]
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
Prompt list: ['You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Manifold Preserving Guided Diffusion\nAbstract: Despite the recent advancements, conditional image generation still faces challenges of cost, generalizability, and the need for task-specific training. In this paper, we propose Manifold Preserving Guided Diffusion (MPGD), a training-free conditional generation framework that leverages pretrained diffusion models and off-the-shelf neural networks with minimal additional inference cost for a broad range of tasks. Specifically, we leverage the manifold hypothesis to refine the guided diffusion steps and introduce a shortcut algorithm in the process. We then propose two methods for on-manifold training-free guidance using pre-trained autoencoders and demonstrate that our shortcut inherently preserves the manifolds when applied to latent diffusion models. Our experiments show that MPGD is efficient and effective for solving a variety of conditional generation applications in low-compute settings, and can consistently offer up to 3.8x speed-ups with the same number of diffusion steps while maintaining high sample quality compared to the baselines.\nSection name: Experiments\n\nFigure block (optional):\n- label: \\label{fig:qualitative_faceid}; caption: \\caption{Examples of the FaceID guidance generation with pre-trained CelebA-HQ models. }\nTable block (optional):\n- label: \\label{tab:faceid}; text: \\begin{tabular}{@{}l|ccc@{}} \\toprule \\textbf{Method} & \\textbf{KID}$\\downarrow$        & \\textbf{FaceID}$\\downarrow$ & \\textbf{Time}$\\downarrow$ \\\\ \\midrule DDIM            & 0.0442          & 1.3914               & 3.41s             \\\\ \\m\nCited paper titles/abstracts (optional):\n- [lgd] Loss-guided diffusion models for plug-and-play controllable generation.: \n- [freedom] FreeDoM: Training-free energy-guided conditional diffusion model.: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. \\vspace{-5pt}\n\\subsection{Pixel space diffusion models}\nIn this section, we evaluate the performance of our proposed pixel domain methods (i.e. \\textbf{MPGD w/o Proj.}, \\textbf{MPGD-AE} and \\textbf{MPGD-Z}) with two different sets of conditional image generation tasks: solving liner inverse problems and human face generation guided by face recognition loss, which we refer to as FaceID guidance generation. For MPGD-AE and MPGD-Z, we use the pre-trained VQGAN models provided by~\\citet{rombach2021highresolution}.\n\\subsubsection{Noisy Linear Inverse Problem}\n2. For linear tasks, we use noisy super-resolution and noisy Gaussian deblurring as the test bed. We choose DPS~\\citep{dps}, LGD-MC~\\citep{lgd}, and MCG~\\citep{mcg} as the basleines. We test each method with two pre-trained diffusion models provided by~\\citet{dps}: one trained on FFHQ dataset~\\citep{karras2019style} and another on ImageNet~\\citep{deng2009imagenet}, both with $256 \\times 256$256 \\times 256 resolution. \nFor super-resolution, we down-sample ground truth images to $64 \\times 64$64 \\times 64. In the Gaussian deblurring task, we apply a $61\\times 61$61\\times 61 sized Gaussian blur with kernel intensity $3.0$3.0. to original images, Measurements in both tasks have a random noise with a variance of $\\sigma^{2}=0.05^{2}$\\sigma^{2}2=0.05^{2}2. We evaluate each task on a set of 1000~samples. We use the Kernel Inception distance (KID)~\\citep{binkowski2018demystifying} to assess the fidelity, Learned Perceptual Image Patch Similarity (LPIPS)~\\citep{zhang2018unreasonable} to evaluate the guidance quality, and the inference time to test the efficiency of the methods. All experiments are conducted on a single NVIDIA GeForce RTX 2080 Ti GPU . Figure~\\ref{fig:linear_main} shows the generated examples for qualitative comparison, and Figure~\\ref{fig:ffhq_sr} presents the quantitative results for the super-resolution task on FFHQ. All three of our methods significantly outperform the baselines with all metrics tested across a variety of different numbers of DDIM steps, and we can observe manifold projection improves the sample fidelity by a large margin.\nNext:\n1. \\hfill\n    [t]{0.53\\textwidth}0.53\\textwidth\n        \\centering\n        \\caption{Quantitative results for style guidance with Stable Diffusion experiment. Our method finds the sweet spot between following the prompt and following the style guidance.}\n        \\label{tab:style}\n2. \\vspace{-7pt}\n\\subsection{Latent diffusion models}\nTo evaluate \\textbf{MPGD-LDM}, we test our methods against the same baselines as the pixel-space FaceID experiments with text-to-image style guided generation task. The goal of this task is to generate images that fit both the text input prompts and the style of the reference images.\nWe use Stable Diffusion~\\citep{rombach2021highresolution} as the pre-trained text-to-image model and deploy the guided sampling methods to incorporate a style loss, which is calculated by the Frobenius norm between the Gram matrices of the reference images and the generated images.\nFor reference style images and text prompts, we randomly created 1000 conditioning pairs, using images from WikiArt~\\citep{saleh2015large} and prompts from PartiPrompts~\\citep{yu2022scaling} dataset. \nFigure~\\ref{fig:style_main} and Table~\\ref{tab:style} show qualitative and quantitative results for this experiment respectively. All the samples are generated on a single NVIDIA A100 GPU with 100 DDIM steps.\nOur method finds the sweet spot between following the text prompts, which usually instruct the generation to generate photo realistic images that do not suit the given style, and following the style guidance, which deviate from the prompts. Notably, because MPGD does not require propagation through the diffusion model, our method can provide significant speedup and can be fitted into a 16GB GPU while all the other methods cannot.\n3. \\vspace{-7pt}\n\nTarget length (characters): ~1212 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:qualitative_faceid\']\ntable_labels = [\'tab:faceid\']\nbib_keys = [\'freedom\', \'lgd\', \'freedom\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Efficient Integrators for Diffusion Generative Models\nAbstract: Diffusion models suffer from slow sample generation at inference time. Therefore, developing a principled framework for fast deterministic/stochastic sampling for a broader class of diffusion models is a promising direction. We propose two complementary frameworks for accelerating sample generation in pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate integrators generalize DDIM, mapping the reverse diffusion dynamics to a more amenable space for sampling. In contrast, splitting-based integrators, commonly used in molecular dynamics, reduce the numerical simulation error by cleverly alternating between numerical updates involving the data and auxiliary variables. After extensively studying these methods empirically and theoretically, we present a hybrid method that leads to the best-reported performance for diffusion models in augmented spaces. Applied to Phase Space Langevin Diffusion [Pandey & Mandt, 2023] on CIFAR-10, our deterministic and stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing baselines, respectively. Our code and model checkpoints wil\nSection name: Additional Experimental Results\n\nFigure block (optional):\n- label: \\label{fig:precond_a}; caption: \\caption{}\n- label: \\label{fig:precond_b}; caption: \\caption{}\nTable block (optional):\n- label: \\label{table:main}; text: \\begin{tabular}{@{}clcccc@{}} \\toprule                                         &               &                                                             &           & \\multicolumn{2}{c}{NFE (FID@50k $\\downarrow$)} \\\\ \\midrule           \nCited paper titles/abstracts (optional):\n- [songdenoising] Denoising diffusion implicit models.: \n- [lu2022dpm] Dpm-solver: A fast ode solver for diffusion probabilistic model   sampling in around 10 steps.: \n- [zhang2023fast] Fast sampling of diffusion models with exponential integrator.: \n- [karraselucidating] Elucidating the design space of diffusion-based generative models.: \n- [liupseudo] Pseudo numerical methods for diffusion models on manifolds.: \n- [bao2022analyticdpm] Analytic-DPM: an analytic estimate of the optimal reverse variance   in diffusion probabilistic models.: \n- [xue2023sasolver] Sa-solver: Stochastic adams solver for fast sampling of diffusion   models, 2023: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. \\textbf{Notation.} For simplicity, we denote our best-performing Reduced Splitting and Conjugate Splitting integrators as \\textit{\\gls{SPS}} and \\textit{\\gls{CSPS}}, respectively. Consequently, we refer to the \\underline{D}eterministic \\gls{RVV}RVV and \\underline{S}tochastic \\gls{ROBA}ROBA samplers as \\gls{SPS}SPS-D and \\gls{SPS}SPS-S, and their conjugate variants as \\gls{CSPS}CSPS-D and \\gls{CSPS}CSPS-S, respectively.\n2. \\textbf{Datasets and Evaluation Metrics.} We use the CIFAR-10, CelebA-64 \\citep{liu2015faceattributes} and the AFHQ-v2 \\citep{choi2020stargan} datasets for comparisons. Unless specified otherwise, we report FID for 50k generated samples for all datasets and quantify sampling efficiency using \\gls{NFE}NFE. We include full experimental details in Appendix \\ref{app:exp_setup}.\nNext:\n1. \\textbf{Empirical Observations:} For CIFAR-10, our ODE sampler performs comparably or outperforms all other baselines for \\gls{NFE}NFE $\\geq$\\geq 50 (Fig. \\ref{fig:sota_all}, Top Left). Similarly, our SDE sampler outperforms all other baselines for \\gls{NFE}NFE $\\geq$\\geq 40 (Fig. \\ref{fig:sota_all}, Top Right). We make similar observations for the CelebA-64 and AFHQv2-64 datasets, where our proposed samplers can obtain significant gains over prior methods for \\gls{NFE}NFE $\\geq$\\geq 70 (See Fig. \\ref{fig:sota_all}, Bottom Left). Therefore, our proposed samplers for \\gls{PSLD}PSLD are competitive with recent work. Moreover, for all datasets, our stochastic sampler achieves better sample quality for low sampling budgets (\\gls{NFE}NFE $<$< 50) as compared to our deterministic sampler. \nLastly, in contrast to CIFAR-10, we find that the \\gls{CSPS}CSPS-S sampler works better than the \\gls{SPS}SPS-S sampler for the CelebA-64 and AFHQv2-64 datasets, indicating its effectiveness for higher-resolution sampling.\n\nTarget length (characters): ~1323 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:precond_a\', \'fig:precond_b\']\ntable_labels = [\'table:main\']\nbib_keys = [\'songdenoising\', \'zhang2023fast\', \'lu2022dpm\', \'liupseudo\', \'karraselucidating\', \'xue2023sasolver\', \'bao2022analyticdpm\', \'karraselucidating\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average\nAbstract: Presently the most successful approaches to semi-supervised learning are based on consistency regularization, whereby a model is trained to be robust to small perturbations of its inputs and parameters. To understand consistency regularization, we conceptually explore how loss geometry interacts with training procedures. The consistency loss dramatically improves generalization performance over supervised-only training; however, we show that SGD struggles to converge on the consistency loss and continues to make large steps that lead to changes in predictions on the test data. Motivated by these observations, we propose to train consistency-based methods with Stochastic Weight Averaging (SWA), a recent approach which averages weights along the trajectory of SGD with a modified learning rate schedule. We also propose fast-SWA, which further accelerates convergence by averaging multiple points within each cycle of a cyclical learning rate schedule. With weight averaging, we achieve the best known semi-supervised results on CIFAR-10 and CIFAR-100, over many different quantities of labeled training data. For example, we achieve 5.0% error on CIFAR-10 with only 4000 labels, compared to \nSection name: Experiments\n\nFigure block (optional):\n- label: \\label{fig:cnn_c10_c100_trend}; caption: \\caption{Prediction errors of base models and their weight averages (\\fastswa and \\swa) for CNN    on \\textbf{(left)} CIFAR-10 with $4k$ labels,   \\textbf{(middle)} CIFAR-100 with $10k$ labels,   and \\textbf{(right)} CIFAR-100 $50k$ labels \nTable block (optional):\n- label: \\label{table:cifar100_shakeshake}; text: \\begin{tabular}{l c c c c c l l l l l } Number of labels\t\t\t& \t10k \t\t& \t50k \t\t& \t50k + 500k \t\t& 50k +   237k$^*$ \t\\\\  \\midrule TE (CNN) \\citep{te}  & \t38.65 \\tpm 0.51\t\t&\t26.30 \\tpm 0.15\t\t&\t23.62 \\tpm 0.17  \t&\t 23.79 \\tpm 0.17  \t\\\\  \\midrule \nCited paper titles/abstracts (optional):\n- [te] Temporal Ensembling for Semi-Supervised Learning.: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. In Figure \\ref{fig:cnn_c10_c100_trend} (left), we visualize the test\nerror as a function of iteration using the CNN.\nWe observe that when the cyclical learning rate starts after epoch $\\ell=180$\\ell=180, \nthe base models drop in performance due to the sudden increase in learning rate (see Figure \\ref{fig:swa_model} left).\nHowever, \\fastswa continues to improve while collecting the weights corresponding to high learning rates for averaging.\nIn general, we also find that the cyclical learning rate improves the base models beyond the usual cosine annealing \nschedule and increases the performance of \\fastswa as training progresses. \nCompared to \\swa, we also observe that \\fastswa converges substantially faster, \nfor instance, reducing the error to $10.5\\%$10.5\\%  at epoch $200$200 while \\swa attains \nsimilar error at epoch $350$350 for CIFAR-10 $4k$4k labels (Figure \n\\ref{fig:cnn_c10_c100_trend} left). We provide additional plots in Section \\ref{sec:add_results} \nshowing the convergence of the \\pi and MT models in all label \nsettings, where we observe similar trends that \\fastswa results in faster error reduction.\n2. We also find that the performance gains of \\fastswa over base models are higher for the \\pi model compared to the MT model, which is consistent with the convexity observation in Section \\ref{sec:empirical_convexity} \nand Figure \\ref{fig:convexity}.\nIn the previous evaluations \\citep[see e.g.][]{ssl-eval, mt}, the \\pi model was shown to be inferior to the MT model.\nHowever, with weight averaging, \\fastswa reduces the gap between \\pi and MT performance. \nSurprisingly, we find that the \\pi model can outperform MT after applying \\fastswa  \nwith moderate to large numbers of labeled points. \nIn particular, the $\\Pi$\\Pi+fast-SWA model outperforms MT+\\fastswa on CIFAR-10 with $4k$4k, $10k$10k, and $50k$50k labeled\ndata points for the Shake-Shake  architecture.\n3. \\vspace{-0.8\\baselineskip}\n4. \\subsection{CIFAR-100 and Extra Unlabeled Data} \\label{sec:cifar100_cnn} \\label{sec:cifar100}\n5. We evaluate the \\pi and MT models with \\fastswa on \nCIFAR-100.\nWe train our models using $50000$50000 images with $10k$10k and $50k$50k labels using the $13$13-layer CNN. \nWe also analyze the effect of using the Tiny Images dataset \\citep{tiny_images} as an additional source of unlabeled data.\nNext:\n1. \\subsection{Advancing State-of-the-Art} \\label{sec:soa}\n2. We have shown that \\fastswa can significantly improve the performance of both the \\pi and \nMT models. We provide a summary comparing our results with the previous best results in the literature in Table  \\ref{table:soa}, using the $13$13-layer CNN and\nthe Shake-Shake architecture that had been applied previously. We also provide detailed results the Appendix \\ref{sec:add_results}.\n3. \\subsection{Preliminary Results on Domain Adaptation}\n\\label{sec:domain_adaptation}\nDomain adaptation problems involve learning using a source domain $X_s$X_s \nequipped with labels $Y_s$Y_s and performing classification on the target \ndomain $X_t$X_t while having no access to the target labels at training time. \nA recent model by \\citet{da_mt} applies the consistency enforcing principle for \ndomain adaptation and achieves state-of-the-art results on many datasets. \nApplying \\fastswa to this model on domain adaptation from CIFAR-10 to STL we \nwere able to improve the best results reported in the literature from\n$19.9\\%$19.9\\% to $16.8\\%$16.8\\%. \nSee Section \\ref{sec:exp_da} for more details on the domain adaptation experiments.\n\nTarget length (characters): ~1438 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:cnn_c10_c100_trend\']\ntable_labels = [\'table:cifar100_shakeshake\']\nbib_keys = [\'te\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Generative AI in Higher Education: Evidence from an Elite College\nAbstract: Generative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPT\'s release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AI\'s potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AI\'s educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.\nSection name: Generative AI Usage Patterns Among Students\n\nFigure block (optional):\n- label: \\label{fig:ai_use}; caption: \\caption{The Adoption of Generative AI among Middlebury College Students}\n- label: \\label{fig:ai_use_time}; caption: \\caption{The Evolution of Generative AI Adoption among Middlebury College Students}\n- label: \\label{fig:ai_models}; caption: \\caption{Adoption of Generative AI Models Among College Students}\n- label: \\label{fig:ai_pays_dem}; caption: \\caption{Percent of Students Who Pay for Generative AI Tools}\nTable block (optional):\n- label: \\label{tab:ai_usage_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{8}{c}{Outcome: Uses AI during the semester with frequency of at least...}  \\\\\\cmidrule{3-10}  \t\t\t\t\t%\t\t\n- label: \\label{tab:ai_adopt_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{10}{c}{Outcome: Started using generative AI...}  \\\\\\cmidrule{3-12}  \t\t\t\t\t&& Before\n- label: \\label{tab:ai_freq_het}; text: \\begin{tabular}{lccccc} \t\t\t\t\t\t\\toprule \t\t\t\t\t\t&& \\multicolumn{4}{c}{By Usage Frequency} \\\\ \\cmidrule{3-6} \t\t\t\t\t\t& Any use & Rarely & Occasionally & Frequently & Very Frequently \\\\ \t\t\t\t\t\t& (1) & (2) & (3) & (4) & (5) \\\\ \\midrule\t\t\t\t\t\t \t\t\t\t\t\t\\\n- label: \\label{tab:ai_model_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{10}{c}{Outcome: =1 if student uses}  \\\\\\cmidrule{3-12}  \t\t\t\t\t&& OpenAI\'s && Google\nCited paper titles/abstracts (optional):\n- [bick.etal2025] The rapid adoption of generative AI.: \n- [mcclain2024] Americans\' use of ChatGPT is ticking up, but few trust its   election information: \n- [nam2023ai] 56\\% of college students have used ai on assignments or exams.: \n- [stohr.etal2024] \\"o: \n- [ravselj.etal2025] s: \n- [otis2024global] Global Evidence on Gender Gaps and Generative AI.: \n- [hartley.etal2025] The Labor Market Effects of Generative Artificial   Intelligence: \n- [humlum.vestergaard2025] Large Language Models, Small Labor Market Effects: \n- [carvajal2024] Will artificial intelligence get in the way of achieving gender   equality?: \n- [david1990dynamo] The dynamo and the computer: an historical perspective on the modern   productivity paradox.: \n- [gallup2024] AI in the Workplace: Answering 3 Big Questions: \n- [hall2003adoption] Adoption of new technology: \n- [stokey2021technology] Technology diffusion.: \n- [world2016world] World Bank: \n- [bick.etal2025] The rapid adoption of generative AI.: \n- [mcclain2024] Americans\' use of ChatGPT is ticking up, but few trust its   election information: \n- [nam2023ai] 56\\% of college students have used ai on assignments or exams.: \n- [stohr.etal2024] \\"o: \n- [ravselj.etal2025] s: \n- [otis2024global] Global Evidence on Gender Gaps and Generative AI.: \n- [hartley.etal2025] The Labor Market Effects of Generative Artificial   Intelligence: \n- [humlum.vestergaard2025] Large Language Models, Small Labor Market Effects: \n- [carvajal2024] Will artificial intelligence get in the way of achieving gender   equality?: \n- [david1990dynamo] The dynamo and the computer: an historical perspective on the modern   productivity paradox.: \n- [gallup2024] AI in the Workplace: Answering 3 Big Questions: \n- [hall2003adoption] Adoption of new technology: \n- [stokey2021technology] Technology diffusion.: \n- [world2016world] World Bank: \n\nk-most adjacent paragraphs (context):\n(none)\n\nTarget length (characters): ~13653 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:ai_use\', \'fig:ai_use_time\', \'fig:ai_models\', \'fig:ai_pays_dem\']\ntable_labels = [\'tab:ai_usage_correlates\', \'tab:ai_adopt_correlates\', \'tab:ai_freq_het\', \'tab:ai_model_correlates\']\nbib_keys = [\'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'nam2023ai\', \'stohr.etal2024\', \'carvajal2024\', \'ravselj.etal2025\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'david1990dynamo\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'hall2003adoption\', \'otis2024global\', \'nam2023ai\', \'stokey2021technology\', \'carvajal2024\', \'stohr.etal2024\', \'world2016world\', \'ravselj.etal2025\', \'carvajal2024\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Neural-Symbolic Message Passing with Dynamic Pruning\nAbstract: Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs) is a challenging task. Recently, a line of message-passing-based research has been proposed to solve CQA. However, they perform unsatisfactorily on negative queries and fail to address the noisy messages between variable nodes in the query graph. Moreover, they offer little interpretability and require complex query data and resource-intensive training. In this paper, we propose a Neural-Symbolic Message Passing (NSMP) framework based on pre-trained neural link predictors. By introducing symbolic reasoning and fuzzy logic, NSMP can generalize to arbitrary existential first order logic queries without requiring training while providing interpretable answers. Furthermore, we introduce a dynamic pruning strategy to filter out noisy messages between variable nodes. Experimental results show that NSMP achieves a strong performance. Additionally, through complexity analysis and empirical verification, we demonstrate the superiority of NSMP in inference time over the current state-of-the-art neural-symbolic method. Compared to this approach, NSMP demonstrates faster inference times across all query types on benchmark dat\nSection name: More Details about the Datasets\n\nFigure block (optional):\n- label: \\label{figure4}; caption: \\caption{Graphical representation of the query types of the BetaE dataset considered in our experiment, where $p$, $i$, $u$, and $n$ represent projection, intersection, union, and negation, respectively. }\n- label: \\label{figure5}; caption: \\caption{Graphical representation of the query types of the FIT dataset considered in our experiment, where $l$, $m$, and $c$ represent existential leaf, multi graph, and circle, respectively. }\nTable block (optional):\n- label: \\label{KG stat}; text: \\begin{tabular}{ccccccc}     \\toprule     \\textbf{Knowledge Graph} & \\textbf{Entities} & \\textbf{Relations} & \\textbf{Training Edges} & \\textbf{Val Edges} & \\textbf{Test Edges} & \\textbf{Total Edges} \\\\     \\midrule     FB15k-237 & 14,505 &\n- label: \\label{BetaE stat}; text: \\begin{tabular}{ccccccc}     \\toprule     \\multirow{2}{*}{\\textbf{Knowledge Graph}} & \\multicolumn{2}{c}{\\textbf{Training Queries}} & \\multicolumn{2}{c}{\\textbf{Validation Queries}} & \\multicolumn{2}{c}{\\textbf{Test Queries}} \\\\     \\cmidru\n- label: \\label{FIT stat}; text: \\begin{tabular}{ccccccccccc}     \\toprule     \\textbf{Knowledge Graph} & \\textbf{pni} & \\textbf{2il} & \\textbf{3il} & \\textbf{2m} & \\textbf{2nm} & \\textbf{3mp} & \\textbf{3pm} & \\textbf{im} & \\textbf{3c} & \\textbf{3cm} \\\\     \\midrule     FB\nCited paper titles/abstracts (optional):\n- [ren2020beta] Beta embeddings for multi-hop logical reasoning in knowledge graphs: \n- [yin2024rethinking] Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors: \n\nk-most adjacent paragraphs (context):\n(none)\n\nTarget length (characters): ~973 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'figure4\', \'figure5\']\ntable_labels = [\'KG stat\', \'BetaE stat\', \'FIT stat\']\nbib_keys = [\'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Progressive Prompts: Continual Learning for Language Models\nAbstract: We introduce Progressive Prompts - a simple and efficient approach for continual learning in language models. Our method allows forward transfer and resists catastrophic forgetting, without relying on data replay or a large number of task-specific parameters. Progressive Prompts learns a new soft prompt for each task and sequentially concatenates it with the previously learned prompts, while keeping the base model frozen. Experiments on standard continual learning benchmarks show that our approach outperforms state-of-the-art methods, with an improvement >20% in average test accuracy over the previous best-preforming method on T5 model. We also explore a more challenging continual learning setup with longer sequences of tasks and show that Progressive Prompts significantly outperforms prior methods.\nSection name: Further experimental results\n\nFigure block (optional):\n- label: \\label{fig:fwt_order8}; caption: \\caption{Forward transfer score of different approaches on order 8. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:fwt_order9}; caption: \\caption{Forward transfer score of different approaches on order 9. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:fwt_order10}; caption: \\caption{Forward transfer score of different approaches on order 10. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order8}; caption: \\caption{Backward transfer score of different approaches on order 8. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order9}; caption: \\caption{Backward transfer score of different approaches on order 9. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order10}; caption: \\caption{Backward transfer score of different approaches on order 10. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:evolution}; caption: \\caption{Evolution of average accuracy after learning new tasks.}\n- label: \\label{fig:fig_long_bars}; caption: \\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improve\nTable block (optional):\n- label: \\label{wrap-tab:init}; text: \\begin{tabular}{lcc}\\\\\\toprule   Method & Few-shot & Full-shot \\\\\\midrule PT + Prev. Init. & 48.2 & 50.0 \\\\  ProgPrompt & \\textbf{53.5} & \\textbf{69.3} \\\\ \\bottomrule \\end{tabular}\n- label: \\label{tab:table_appendix_long}; text: \\begin{tabular}{lccc|ccc|ccc|ccc} % <-- Alignments: 1st column left & 2nd middle and 3rd right & with vertical lines in between     \\toprule       \\textbf{Method} $\\downarrow$ & \\multicolumn{3}{c}{\\textbf{Order 8}}  & \\multicolumn{3}{c}{\\te\nCited paper titles/abstracts (optional):\n- [lopez2017gradient] Gradient episodic memory for continual learning: \n\nk-most adjacent paragraphs (context):\nNext:\n1. \\includegraphics[width=0.9\\textwidth]{images/long_exp_bars.png}\n\\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improvement of Progressive Prompts test score compared to per-task prompt on the corresponding task.}\n\\label{fig:fig_long_bars}\n\\end{figure}\n2. \\includegraphics[width=0.9\\textwidth]{images/long_exp_bars.png}\n\\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improvement of Progressive Prompts test score compared to per-task prompt on the corresponding task.}\n\\label{fig:fig_long_bars}\n3. \\includegraphics[width=0.7\\textwidth]{images/superglue_illustration.png}\n4. \\caption{Original prompt tuning versus Progressive Prompts on SuperGLUE datasets. For illustration, we show how SuperGLUE task WiC is leaned (we have similar scheme for other tasks). Prompt tuning trains a single prompt of 100 tokens for WiC task. Progressive Prompts method learns a prompt of 40 tokens, which is progressively appended to the six frozen prompts of 10 tokens learned on GLUE benchmark (with random task order). Total prompt length is equal in both approaches. }\n\\label{fig:fig_super_illustration}\n\\end{figure}\n5. \\includegraphics[width=0.7\\textwidth]{images/superglue_illustration.png}\n\nTarget length (characters): ~1627 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:fwt_order8\', \'fig:fwt_order9\', \'fig:fwt_order10\', \'fig:bwt_order8\', \'fig:bwt_order9\', \'fig:bwt_order10\', \'fig:evolution\', \'fig:fig_long_bars\']\ntable_labels = [\'wrap-tab:init\', \'tab:table_appendix_long\']\nbib_keys = [\'lopez2017gradient\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).']
Number of prompts: 6
Image tag lists: [[['contains scatter plot']], [None, None], [None], [None, None, None, None], [['a figure showing network'], ['a figure showing network']], [['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about bar chart']]]
Number of Image tag lists: 6
Number of Image projection lists: 6
INFO 08-31 20:37:03 [utils.py:326] non-default args: {'model': 'Qwen/Qwen3-8B', 'enable_prompt_embeds': True, 'trust_remote_code': True, 'dtype': 'float16', 'gpu_memory_utilization': 0.6, 'disable_log_stats': True, 'enforce_eager': True}
INFO 08-31 20:37:12 [__init__.py:711] Resolved architecture: Qwen3ForCausalLM
WARNING 08-31 20:37:12 [__init__.py:2819] Casting torch.bfloat16 to torch.float16.
INFO 08-31 20:37:12 [__init__.py:1750] Using max model len 40960
WARNING 08-31 20:37:12 [arg_utils.py:1770] --enable-prompt-embeds is not supported by the V1 Engine. Falling back to V0. 
WARNING 08-31 20:37:12 [arg_utils.py:1544] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.
INFO 08-31 20:37:13 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 08-31 20:37:13 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 08-31 20:37:13 [__init__.py:3565] Cudagraph is disabled under eager mode
INFO 08-31 20:37:13 [llm_engine.py:222] Initializing a V0 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen3-8B', speculative_config=None, tokenizer='Qwen/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{"enable_fusion":false,"enable_noop":false},"max_capture_size":0,"local_cache_dir":null}, use_cached_outputs=False, 
INFO 08-31 20:37:15 [cuda.py:436] Using Flash Attention backend.
INFO 08-31 20:37:15 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 08-31 20:37:15 [model_runner.py:1080] Starting to load model Qwen/Qwen3-8B...
INFO 08-31 20:37:16 [weight_utils.py:296] Using model weights format ['*.safetensors']
INFO 08-31 20:37:20 [default_loader.py:262] Loading weights took 4.70 seconds
INFO 08-31 20:37:21 [model_runner.py:1112] Model loading took 15.2683 GiB and 5.064988 seconds
INFO 08-31 20:37:22 [worker.py:295] Memory profiling takes 0.93 seconds
INFO 08-31 20:37:22 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.53GiB) x gpu_memory_utilization (0.60) = 28.52GiB
INFO 08-31 20:37:22 [worker.py:295] model weights take 15.27GiB; non_torch_memory takes 0.02GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 11.83GiB.
INFO 08-31 20:37:22 [executor_base.py:114] # cuda blocks: 5385, # CPU blocks: 1820
INFO 08-31 20:37:22 [executor_base.py:119] Maximum concurrency for 40960 tokens per request: 2.10x
INFO 08-31 20:37:24 [llm_engine.py:417] init engine (profile, create kv cache, warmup model) took 3.34 seconds
INFO 08-31 20:37:24 [llm.py:298] Supported_tasks: ['generate']
INFO 08-31 20:41:00 [__init__.py:241] Automatically detected platform cuda.
CLIP image features: (1, 512)
Top CLIP tags: ['contains scatter plot']
Image tag list: [['contains scatter plot']]
[WARN] Path not found: download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Path not found: download/output/figures/2310.07894v1/tex_figures_5d.png
Image tag list: [None, None]
[WARN] Path not found: download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
Image tag list: [None]
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_demog
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_time
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_model
[WARN] Path not found: download/output/figures/2508.00717/results_ai_pays_dem
Image tag list: [None, None, None, None]
CLIP image features: (1, 512)
Top CLIP tags: ['a figure showing network']
CLIP image features: (1, 512)
Top CLIP tags: ['a figure showing network']
Image tag list: [['a figure showing network'], ['a figure showing network']]
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about bar chart']
Image tag list: [['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about bar chart']]
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
Prompt list: ['You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Manifold Preserving Guided Diffusion\nAbstract: Despite the recent advancements, conditional image generation still faces challenges of cost, generalizability, and the need for task-specific training. In this paper, we propose Manifold Preserving Guided Diffusion (MPGD), a training-free conditional generation framework that leverages pretrained diffusion models and off-the-shelf neural networks with minimal additional inference cost for a broad range of tasks. Specifically, we leverage the manifold hypothesis to refine the guided diffusion steps and introduce a shortcut algorithm in the process. We then propose two methods for on-manifold training-free guidance using pre-trained autoencoders and demonstrate that our shortcut inherently preserves the manifolds when applied to latent diffusion models. Our experiments show that MPGD is efficient and effective for solving a variety of conditional generation applications in low-compute settings, and can consistently offer up to 3.8x speed-ups with the same number of diffusion steps while maintaining high sample quality compared to the baselines.\nSection name: Experiments\n\nFigure block (optional):\n- label: \\label{fig:qualitative_faceid}; caption: \\caption{Examples of the FaceID guidance generation with pre-trained CelebA-HQ models. }\nTable block (optional):\n- label: \\label{tab:faceid}; text: \\begin{tabular}{@{}l|ccc@{}} \\toprule \\textbf{Method} & \\textbf{KID}$\\downarrow$        & \\textbf{FaceID}$\\downarrow$ & \\textbf{Time}$\\downarrow$ \\\\ \\midrule DDIM            & 0.0442          & 1.3914               & 3.41s             \\\\ \\m\nCited paper titles/abstracts (optional):\n- [lgd] Loss-guided diffusion models for plug-and-play controllable generation.: \n- [freedom] FreeDoM: Training-free energy-guided conditional diffusion model.: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. \\vspace{-5pt}\n\\subsection{Pixel space diffusion models}\nIn this section, we evaluate the performance of our proposed pixel domain methods (i.e. \\textbf{MPGD w/o Proj.}, \\textbf{MPGD-AE} and \\textbf{MPGD-Z}) with two different sets of conditional image generation tasks: solving liner inverse problems and human face generation guided by face recognition loss, which we refer to as FaceID guidance generation. For MPGD-AE and MPGD-Z, we use the pre-trained VQGAN models provided by~\\citet{rombach2021highresolution}.\n\\subsubsection{Noisy Linear Inverse Problem}\n2. For linear tasks, we use noisy super-resolution and noisy Gaussian deblurring as the test bed. We choose DPS~\\citep{dps}, LGD-MC~\\citep{lgd}, and MCG~\\citep{mcg} as the basleines. We test each method with two pre-trained diffusion models provided by~\\citet{dps}: one trained on FFHQ dataset~\\citep{karras2019style} and another on ImageNet~\\citep{deng2009imagenet}, both with $256 \\times 256$256 \\times 256 resolution. \nFor super-resolution, we down-sample ground truth images to $64 \\times 64$64 \\times 64. In the Gaussian deblurring task, we apply a $61\\times 61$61\\times 61 sized Gaussian blur with kernel intensity $3.0$3.0. to original images, Measurements in both tasks have a random noise with a variance of $\\sigma^{2}=0.05^{2}$\\sigma^{2}2=0.05^{2}2. We evaluate each task on a set of 1000~samples. We use the Kernel Inception distance (KID)~\\citep{binkowski2018demystifying} to assess the fidelity, Learned Perceptual Image Patch Similarity (LPIPS)~\\citep{zhang2018unreasonable} to evaluate the guidance quality, and the inference time to test the efficiency of the methods. All experiments are conducted on a single NVIDIA GeForce RTX 2080 Ti GPU . Figure~\\ref{fig:linear_main} shows the generated examples for qualitative comparison, and Figure~\\ref{fig:ffhq_sr} presents the quantitative results for the super-resolution task on FFHQ. All three of our methods significantly outperform the baselines with all metrics tested across a variety of different numbers of DDIM steps, and we can observe manifold projection improves the sample fidelity by a large margin.\nNext:\n1. \\hfill\n    [t]{0.53\\textwidth}0.53\\textwidth\n        \\centering\n        \\caption{Quantitative results for style guidance with Stable Diffusion experiment. Our method finds the sweet spot between following the prompt and following the style guidance.}\n        \\label{tab:style}\n2. \\vspace{-7pt}\n\\subsection{Latent diffusion models}\nTo evaluate \\textbf{MPGD-LDM}, we test our methods against the same baselines as the pixel-space FaceID experiments with text-to-image style guided generation task. The goal of this task is to generate images that fit both the text input prompts and the style of the reference images.\nWe use Stable Diffusion~\\citep{rombach2021highresolution} as the pre-trained text-to-image model and deploy the guided sampling methods to incorporate a style loss, which is calculated by the Frobenius norm between the Gram matrices of the reference images and the generated images.\nFor reference style images and text prompts, we randomly created 1000 conditioning pairs, using images from WikiArt~\\citep{saleh2015large} and prompts from PartiPrompts~\\citep{yu2022scaling} dataset. \nFigure~\\ref{fig:style_main} and Table~\\ref{tab:style} show qualitative and quantitative results for this experiment respectively. All the samples are generated on a single NVIDIA A100 GPU with 100 DDIM steps.\nOur method finds the sweet spot between following the text prompts, which usually instruct the generation to generate photo realistic images that do not suit the given style, and following the style guidance, which deviate from the prompts. Notably, because MPGD does not require propagation through the diffusion model, our method can provide significant speedup and can be fitted into a 16GB GPU while all the other methods cannot.\n3. \\vspace{-7pt}\n\nTarget length (characters): ~1212 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:qualitative_faceid\']\ntable_labels = [\'tab:faceid\']\nbib_keys = [\'freedom\', \'lgd\', \'freedom\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Efficient Integrators for Diffusion Generative Models\nAbstract: Diffusion models suffer from slow sample generation at inference time. Therefore, developing a principled framework for fast deterministic/stochastic sampling for a broader class of diffusion models is a promising direction. We propose two complementary frameworks for accelerating sample generation in pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate integrators generalize DDIM, mapping the reverse diffusion dynamics to a more amenable space for sampling. In contrast, splitting-based integrators, commonly used in molecular dynamics, reduce the numerical simulation error by cleverly alternating between numerical updates involving the data and auxiliary variables. After extensively studying these methods empirically and theoretically, we present a hybrid method that leads to the best-reported performance for diffusion models in augmented spaces. Applied to Phase Space Langevin Diffusion [Pandey & Mandt, 2023] on CIFAR-10, our deterministic and stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing baselines, respectively. Our code and model checkpoints wil\nSection name: Additional Experimental Results\n\nFigure block (optional):\n- label: \\label{fig:precond_a}; caption: \\caption{}\n- label: \\label{fig:precond_b}; caption: \\caption{}\nTable block (optional):\n- label: \\label{table:main}; text: \\begin{tabular}{@{}clcccc@{}} \\toprule                                         &               &                                                             &           & \\multicolumn{2}{c}{NFE (FID@50k $\\downarrow$)} \\\\ \\midrule           \nCited paper titles/abstracts (optional):\n- [songdenoising] Denoising diffusion implicit models.: \n- [lu2022dpm] Dpm-solver: A fast ode solver for diffusion probabilistic model   sampling in around 10 steps.: \n- [zhang2023fast] Fast sampling of diffusion models with exponential integrator.: \n- [karraselucidating] Elucidating the design space of diffusion-based generative models.: \n- [liupseudo] Pseudo numerical methods for diffusion models on manifolds.: \n- [bao2022analyticdpm] Analytic-DPM: an analytic estimate of the optimal reverse variance   in diffusion probabilistic models.: \n- [xue2023sasolver] Sa-solver: Stochastic adams solver for fast sampling of diffusion   models, 2023: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. \\textbf{Notation.} For simplicity, we denote our best-performing Reduced Splitting and Conjugate Splitting integrators as \\textit{\\gls{SPS}} and \\textit{\\gls{CSPS}}, respectively. Consequently, we refer to the \\underline{D}eterministic \\gls{RVV}RVV and \\underline{S}tochastic \\gls{ROBA}ROBA samplers as \\gls{SPS}SPS-D and \\gls{SPS}SPS-S, and their conjugate variants as \\gls{CSPS}CSPS-D and \\gls{CSPS}CSPS-S, respectively.\n2. \\textbf{Datasets and Evaluation Metrics.} We use the CIFAR-10, CelebA-64 \\citep{liu2015faceattributes} and the AFHQ-v2 \\citep{choi2020stargan} datasets for comparisons. Unless specified otherwise, we report FID for 50k generated samples for all datasets and quantify sampling efficiency using \\gls{NFE}NFE. We include full experimental details in Appendix \\ref{app:exp_setup}.\nNext:\n1. \\textbf{Empirical Observations:} For CIFAR-10, our ODE sampler performs comparably or outperforms all other baselines for \\gls{NFE}NFE $\\geq$\\geq 50 (Fig. \\ref{fig:sota_all}, Top Left). Similarly, our SDE sampler outperforms all other baselines for \\gls{NFE}NFE $\\geq$\\geq 40 (Fig. \\ref{fig:sota_all}, Top Right). We make similar observations for the CelebA-64 and AFHQv2-64 datasets, where our proposed samplers can obtain significant gains over prior methods for \\gls{NFE}NFE $\\geq$\\geq 70 (See Fig. \\ref{fig:sota_all}, Bottom Left). Therefore, our proposed samplers for \\gls{PSLD}PSLD are competitive with recent work. Moreover, for all datasets, our stochastic sampler achieves better sample quality for low sampling budgets (\\gls{NFE}NFE $<$< 50) as compared to our deterministic sampler. \nLastly, in contrast to CIFAR-10, we find that the \\gls{CSPS}CSPS-S sampler works better than the \\gls{SPS}SPS-S sampler for the CelebA-64 and AFHQv2-64 datasets, indicating its effectiveness for higher-resolution sampling.\n\nTarget length (characters): ~1323 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:precond_a\', \'fig:precond_b\']\ntable_labels = [\'table:main\']\nbib_keys = [\'songdenoising\', \'zhang2023fast\', \'lu2022dpm\', \'liupseudo\', \'karraselucidating\', \'xue2023sasolver\', \'bao2022analyticdpm\', \'karraselucidating\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average\nAbstract: Presently the most successful approaches to semi-supervised learning are based on consistency regularization, whereby a model is trained to be robust to small perturbations of its inputs and parameters. To understand consistency regularization, we conceptually explore how loss geometry interacts with training procedures. The consistency loss dramatically improves generalization performance over supervised-only training; however, we show that SGD struggles to converge on the consistency loss and continues to make large steps that lead to changes in predictions on the test data. Motivated by these observations, we propose to train consistency-based methods with Stochastic Weight Averaging (SWA), a recent approach which averages weights along the trajectory of SGD with a modified learning rate schedule. We also propose fast-SWA, which further accelerates convergence by averaging multiple points within each cycle of a cyclical learning rate schedule. With weight averaging, we achieve the best known semi-supervised results on CIFAR-10 and CIFAR-100, over many different quantities of labeled training data. For example, we achieve 5.0% error on CIFAR-10 with only 4000 labels, compared to \nSection name: Experiments\n\nFigure block (optional):\n- label: \\label{fig:cnn_c10_c100_trend}; caption: \\caption{Prediction errors of base models and their weight averages (\\fastswa and \\swa) for CNN    on \\textbf{(left)} CIFAR-10 with $4k$ labels,   \\textbf{(middle)} CIFAR-100 with $10k$ labels,   and \\textbf{(right)} CIFAR-100 $50k$ labels \nTable block (optional):\n- label: \\label{table:cifar100_shakeshake}; text: \\begin{tabular}{l c c c c c l l l l l } Number of labels\t\t\t& \t10k \t\t& \t50k \t\t& \t50k + 500k \t\t& 50k +   237k$^*$ \t\\\\  \\midrule TE (CNN) \\citep{te}  & \t38.65 \\tpm 0.51\t\t&\t26.30 \\tpm 0.15\t\t&\t23.62 \\tpm 0.17  \t&\t 23.79 \\tpm 0.17  \t\\\\  \\midrule \nCited paper titles/abstracts (optional):\n- [te] Temporal Ensembling for Semi-Supervised Learning.: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. In Figure \\ref{fig:cnn_c10_c100_trend} (left), we visualize the test\nerror as a function of iteration using the CNN.\nWe observe that when the cyclical learning rate starts after epoch $\\ell=180$\\ell=180, \nthe base models drop in performance due to the sudden increase in learning rate (see Figure \\ref{fig:swa_model} left).\nHowever, \\fastswa continues to improve while collecting the weights corresponding to high learning rates for averaging.\nIn general, we also find that the cyclical learning rate improves the base models beyond the usual cosine annealing \nschedule and increases the performance of \\fastswa as training progresses. \nCompared to \\swa, we also observe that \\fastswa converges substantially faster, \nfor instance, reducing the error to $10.5\\%$10.5\\%  at epoch $200$200 while \\swa attains \nsimilar error at epoch $350$350 for CIFAR-10 $4k$4k labels (Figure \n\\ref{fig:cnn_c10_c100_trend} left). We provide additional plots in Section \\ref{sec:add_results} \nshowing the convergence of the \\pi and MT models in all label \nsettings, where we observe similar trends that \\fastswa results in faster error reduction.\n2. We also find that the performance gains of \\fastswa over base models are higher for the \\pi model compared to the MT model, which is consistent with the convexity observation in Section \\ref{sec:empirical_convexity} \nand Figure \\ref{fig:convexity}.\nIn the previous evaluations \\citep[see e.g.][]{ssl-eval, mt}, the \\pi model was shown to be inferior to the MT model.\nHowever, with weight averaging, \\fastswa reduces the gap between \\pi and MT performance. \nSurprisingly, we find that the \\pi model can outperform MT after applying \\fastswa  \nwith moderate to large numbers of labeled points. \nIn particular, the $\\Pi$\\Pi+fast-SWA model outperforms MT+\\fastswa on CIFAR-10 with $4k$4k, $10k$10k, and $50k$50k labeled\ndata points for the Shake-Shake  architecture.\n3. \\vspace{-0.8\\baselineskip}\n4. \\subsection{CIFAR-100 and Extra Unlabeled Data} \\label{sec:cifar100_cnn} \\label{sec:cifar100}\n5. We evaluate the \\pi and MT models with \\fastswa on \nCIFAR-100.\nWe train our models using $50000$50000 images with $10k$10k and $50k$50k labels using the $13$13-layer CNN. \nWe also analyze the effect of using the Tiny Images dataset \\citep{tiny_images} as an additional source of unlabeled data.\nNext:\n1. \\subsection{Advancing State-of-the-Art} \\label{sec:soa}\n2. We have shown that \\fastswa can significantly improve the performance of both the \\pi and \nMT models. We provide a summary comparing our results with the previous best results in the literature in Table  \\ref{table:soa}, using the $13$13-layer CNN and\nthe Shake-Shake architecture that had been applied previously. We also provide detailed results the Appendix \\ref{sec:add_results}.\n3. \\subsection{Preliminary Results on Domain Adaptation}\n\\label{sec:domain_adaptation}\nDomain adaptation problems involve learning using a source domain $X_s$X_s \nequipped with labels $Y_s$Y_s and performing classification on the target \ndomain $X_t$X_t while having no access to the target labels at training time. \nA recent model by \\citet{da_mt} applies the consistency enforcing principle for \ndomain adaptation and achieves state-of-the-art results on many datasets. \nApplying \\fastswa to this model on domain adaptation from CIFAR-10 to STL we \nwere able to improve the best results reported in the literature from\n$19.9\\%$19.9\\% to $16.8\\%$16.8\\%. \nSee Section \\ref{sec:exp_da} for more details on the domain adaptation experiments.\n\nTarget length (characters): ~1438 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:cnn_c10_c100_trend\']\ntable_labels = [\'table:cifar100_shakeshake\']\nbib_keys = [\'te\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Generative AI in Higher Education: Evidence from an Elite College\nAbstract: Generative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPT\'s release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AI\'s potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AI\'s educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.\nSection name: Generative AI Usage Patterns Among Students\n\nFigure block (optional):\n- label: \\label{fig:ai_use}; caption: \\caption{The Adoption of Generative AI among Middlebury College Students}\n- label: \\label{fig:ai_use_time}; caption: \\caption{The Evolution of Generative AI Adoption among Middlebury College Students}\n- label: \\label{fig:ai_models}; caption: \\caption{Adoption of Generative AI Models Among College Students}\n- label: \\label{fig:ai_pays_dem}; caption: \\caption{Percent of Students Who Pay for Generative AI Tools}\nTable block (optional):\n- label: \\label{tab:ai_usage_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{8}{c}{Outcome: Uses AI during the semester with frequency of at least...}  \\\\\\cmidrule{3-10}  \t\t\t\t\t%\t\t\n- label: \\label{tab:ai_adopt_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{10}{c}{Outcome: Started using generative AI...}  \\\\\\cmidrule{3-12}  \t\t\t\t\t&& Before\n- label: \\label{tab:ai_freq_het}; text: \\begin{tabular}{lccccc} \t\t\t\t\t\t\\toprule \t\t\t\t\t\t&& \\multicolumn{4}{c}{By Usage Frequency} \\\\ \\cmidrule{3-6} \t\t\t\t\t\t& Any use & Rarely & Occasionally & Frequently & Very Frequently \\\\ \t\t\t\t\t\t& (1) & (2) & (3) & (4) & (5) \\\\ \\midrule\t\t\t\t\t\t \t\t\t\t\t\t\\\n- label: \\label{tab:ai_model_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{10}{c}{Outcome: =1 if student uses}  \\\\\\cmidrule{3-12}  \t\t\t\t\t&& OpenAI\'s && Google\nCited paper titles/abstracts (optional):\n- [bick.etal2025] The rapid adoption of generative AI.: \n- [mcclain2024] Americans\' use of ChatGPT is ticking up, but few trust its   election information: \n- [nam2023ai] 56\\% of college students have used ai on assignments or exams.: \n- [stohr.etal2024] \\"o: \n- [ravselj.etal2025] s: \n- [otis2024global] Global Evidence on Gender Gaps and Generative AI.: \n- [hartley.etal2025] The Labor Market Effects of Generative Artificial   Intelligence: \n- [humlum.vestergaard2025] Large Language Models, Small Labor Market Effects: \n- [carvajal2024] Will artificial intelligence get in the way of achieving gender   equality?: \n- [david1990dynamo] The dynamo and the computer: an historical perspective on the modern   productivity paradox.: \n- [gallup2024] AI in the Workplace: Answering 3 Big Questions: \n- [hall2003adoption] Adoption of new technology: \n- [stokey2021technology] Technology diffusion.: \n- [world2016world] World Bank: \n- [bick.etal2025] The rapid adoption of generative AI.: \n- [mcclain2024] Americans\' use of ChatGPT is ticking up, but few trust its   election information: \n- [nam2023ai] 56\\% of college students have used ai on assignments or exams.: \n- [stohr.etal2024] \\"o: \n- [ravselj.etal2025] s: \n- [otis2024global] Global Evidence on Gender Gaps and Generative AI.: \n- [hartley.etal2025] The Labor Market Effects of Generative Artificial   Intelligence: \n- [humlum.vestergaard2025] Large Language Models, Small Labor Market Effects: \n- [carvajal2024] Will artificial intelligence get in the way of achieving gender   equality?: \n- [david1990dynamo] The dynamo and the computer: an historical perspective on the modern   productivity paradox.: \n- [gallup2024] AI in the Workplace: Answering 3 Big Questions: \n- [hall2003adoption] Adoption of new technology: \n- [stokey2021technology] Technology diffusion.: \n- [world2016world] World Bank: \n\nk-most adjacent paragraphs (context):\n(none)\n\nTarget length (characters): ~13653 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:ai_use\', \'fig:ai_use_time\', \'fig:ai_models\', \'fig:ai_pays_dem\']\ntable_labels = [\'tab:ai_usage_correlates\', \'tab:ai_adopt_correlates\', \'tab:ai_freq_het\', \'tab:ai_model_correlates\']\nbib_keys = [\'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'nam2023ai\', \'stohr.etal2024\', \'carvajal2024\', \'ravselj.etal2025\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'david1990dynamo\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'hall2003adoption\', \'otis2024global\', \'nam2023ai\', \'stokey2021technology\', \'carvajal2024\', \'stohr.etal2024\', \'world2016world\', \'ravselj.etal2025\', \'carvajal2024\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Neural-Symbolic Message Passing with Dynamic Pruning\nAbstract: Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs) is a challenging task. Recently, a line of message-passing-based research has been proposed to solve CQA. However, they perform unsatisfactorily on negative queries and fail to address the noisy messages between variable nodes in the query graph. Moreover, they offer little interpretability and require complex query data and resource-intensive training. In this paper, we propose a Neural-Symbolic Message Passing (NSMP) framework based on pre-trained neural link predictors. By introducing symbolic reasoning and fuzzy logic, NSMP can generalize to arbitrary existential first order logic queries without requiring training while providing interpretable answers. Furthermore, we introduce a dynamic pruning strategy to filter out noisy messages between variable nodes. Experimental results show that NSMP achieves a strong performance. Additionally, through complexity analysis and empirical verification, we demonstrate the superiority of NSMP in inference time over the current state-of-the-art neural-symbolic method. Compared to this approach, NSMP demonstrates faster inference times across all query types on benchmark dat\nSection name: More Details about the Datasets\n\nFigure block (optional):\n- label: \\label{figure4}; caption: \\caption{Graphical representation of the query types of the BetaE dataset considered in our experiment, where $p$, $i$, $u$, and $n$ represent projection, intersection, union, and negation, respectively. }\n- label: \\label{figure5}; caption: \\caption{Graphical representation of the query types of the FIT dataset considered in our experiment, where $l$, $m$, and $c$ represent existential leaf, multi graph, and circle, respectively. }\nTable block (optional):\n- label: \\label{KG stat}; text: \\begin{tabular}{ccccccc}     \\toprule     \\textbf{Knowledge Graph} & \\textbf{Entities} & \\textbf{Relations} & \\textbf{Training Edges} & \\textbf{Val Edges} & \\textbf{Test Edges} & \\textbf{Total Edges} \\\\     \\midrule     FB15k-237 & 14,505 &\n- label: \\label{BetaE stat}; text: \\begin{tabular}{ccccccc}     \\toprule     \\multirow{2}{*}{\\textbf{Knowledge Graph}} & \\multicolumn{2}{c}{\\textbf{Training Queries}} & \\multicolumn{2}{c}{\\textbf{Validation Queries}} & \\multicolumn{2}{c}{\\textbf{Test Queries}} \\\\     \\cmidru\n- label: \\label{FIT stat}; text: \\begin{tabular}{ccccccccccc}     \\toprule     \\textbf{Knowledge Graph} & \\textbf{pni} & \\textbf{2il} & \\textbf{3il} & \\textbf{2m} & \\textbf{2nm} & \\textbf{3mp} & \\textbf{3pm} & \\textbf{im} & \\textbf{3c} & \\textbf{3cm} \\\\     \\midrule     FB\nCited paper titles/abstracts (optional):\n- [ren2020beta] Beta embeddings for multi-hop logical reasoning in knowledge graphs: \n- [yin2024rethinking] Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors: \n\nk-most adjacent paragraphs (context):\n(none)\n\nTarget length (characters): ~973 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'figure4\', \'figure5\']\ntable_labels = [\'KG stat\', \'BetaE stat\', \'FIT stat\']\nbib_keys = [\'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Progressive Prompts: Continual Learning for Language Models\nAbstract: We introduce Progressive Prompts - a simple and efficient approach for continual learning in language models. Our method allows forward transfer and resists catastrophic forgetting, without relying on data replay or a large number of task-specific parameters. Progressive Prompts learns a new soft prompt for each task and sequentially concatenates it with the previously learned prompts, while keeping the base model frozen. Experiments on standard continual learning benchmarks show that our approach outperforms state-of-the-art methods, with an improvement >20% in average test accuracy over the previous best-preforming method on T5 model. We also explore a more challenging continual learning setup with longer sequences of tasks and show that Progressive Prompts significantly outperforms prior methods.\nSection name: Further experimental results\n\nFigure block (optional):\n- label: \\label{fig:fwt_order8}; caption: \\caption{Forward transfer score of different approaches on order 8. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:fwt_order9}; caption: \\caption{Forward transfer score of different approaches on order 9. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:fwt_order10}; caption: \\caption{Forward transfer score of different approaches on order 10. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order8}; caption: \\caption{Backward transfer score of different approaches on order 8. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order9}; caption: \\caption{Backward transfer score of different approaches on order 9. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order10}; caption: \\caption{Backward transfer score of different approaches on order 10. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:evolution}; caption: \\caption{Evolution of average accuracy after learning new tasks.}\n- label: \\label{fig:fig_long_bars}; caption: \\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improve\nTable block (optional):\n- label: \\label{wrap-tab:init}; text: \\begin{tabular}{lcc}\\\\\\toprule   Method & Few-shot & Full-shot \\\\\\midrule PT + Prev. Init. & 48.2 & 50.0 \\\\  ProgPrompt & \\textbf{53.5} & \\textbf{69.3} \\\\ \\bottomrule \\end{tabular}\n- label: \\label{tab:table_appendix_long}; text: \\begin{tabular}{lccc|ccc|ccc|ccc} % <-- Alignments: 1st column left & 2nd middle and 3rd right & with vertical lines in between     \\toprule       \\textbf{Method} $\\downarrow$ & \\multicolumn{3}{c}{\\textbf{Order 8}}  & \\multicolumn{3}{c}{\\te\nCited paper titles/abstracts (optional):\n- [lopez2017gradient] Gradient episodic memory for continual learning: \n\nk-most adjacent paragraphs (context):\nNext:\n1. \\includegraphics[width=0.9\\textwidth]{images/long_exp_bars.png}\n\\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improvement of Progressive Prompts test score compared to per-task prompt on the corresponding task.}\n\\label{fig:fig_long_bars}\n\\end{figure}\n2. \\includegraphics[width=0.9\\textwidth]{images/long_exp_bars.png}\n\\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improvement of Progressive Prompts test score compared to per-task prompt on the corresponding task.}\n\\label{fig:fig_long_bars}\n3. \\includegraphics[width=0.7\\textwidth]{images/superglue_illustration.png}\n4. \\caption{Original prompt tuning versus Progressive Prompts on SuperGLUE datasets. For illustration, we show how SuperGLUE task WiC is leaned (we have similar scheme for other tasks). Prompt tuning trains a single prompt of 100 tokens for WiC task. Progressive Prompts method learns a prompt of 40 tokens, which is progressively appended to the six frozen prompts of 10 tokens learned on GLUE benchmark (with random task order). Total prompt length is equal in both approaches. }\n\\label{fig:fig_super_illustration}\n\\end{figure}\n5. \\includegraphics[width=0.7\\textwidth]{images/superglue_illustration.png}\n\nTarget length (characters): ~1627 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:fwt_order8\', \'fig:fwt_order9\', \'fig:fwt_order10\', \'fig:bwt_order8\', \'fig:bwt_order9\', \'fig:bwt_order10\', \'fig:evolution\', \'fig:fig_long_bars\']\ntable_labels = [\'wrap-tab:init\', \'tab:table_appendix_long\']\nbib_keys = [\'lopez2017gradient\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).']
Number of prompts: 6
Image tag lists: [[['contains scatter plot']], [None, None], [None], [None, None, None, None], [['a figure showing network'], ['a figure showing network']], [['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about bar chart']]]
Number of Image tag lists: 6
Number of Image projection lists: 6
INFO 08-31 20:41:44 [utils.py:326] non-default args: {'model': 'Qwen/Qwen3-8B', 'enable_prompt_embeds': True, 'trust_remote_code': True, 'dtype': 'float16', 'gpu_memory_utilization': 0.6, 'disable_log_stats': True, 'enforce_eager': True}
INFO 08-31 20:41:53 [__init__.py:711] Resolved architecture: Qwen3ForCausalLM
WARNING 08-31 20:41:53 [__init__.py:2819] Casting torch.bfloat16 to torch.float16.
INFO 08-31 20:41:53 [__init__.py:1750] Using max model len 40960
WARNING 08-31 20:41:53 [arg_utils.py:1770] --enable-prompt-embeds is not supported by the V1 Engine. Falling back to V0. 
WARNING 08-31 20:41:53 [arg_utils.py:1544] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.
INFO 08-31 20:41:53 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 08-31 20:41:53 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 08-31 20:41:53 [__init__.py:3565] Cudagraph is disabled under eager mode
INFO 08-31 20:41:53 [llm_engine.py:222] Initializing a V0 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen3-8B', speculative_config=None, tokenizer='Qwen/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{"enable_fusion":false,"enable_noop":false},"max_capture_size":0,"local_cache_dir":null}, use_cached_outputs=False, 
INFO 08-31 20:41:55 [cuda.py:436] Using Flash Attention backend.
INFO 08-31 20:41:56 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 08-31 20:41:56 [model_runner.py:1080] Starting to load model Qwen/Qwen3-8B...
INFO 08-31 20:41:56 [weight_utils.py:296] Using model weights format ['*.safetensors']
INFO 08-31 20:42:01 [default_loader.py:262] Loading weights took 4.43 seconds
INFO 08-31 20:42:01 [model_runner.py:1112] Model loading took 15.2683 GiB and 4.872734 seconds
INFO 08-31 20:42:02 [worker.py:295] Memory profiling takes 0.81 seconds
INFO 08-31 20:42:02 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.53GiB) x gpu_memory_utilization (0.60) = 28.52GiB
INFO 08-31 20:42:02 [worker.py:295] model weights take 15.27GiB; non_torch_memory takes 0.02GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 11.83GiB.
INFO 08-31 20:42:02 [executor_base.py:114] # cuda blocks: 5385, # CPU blocks: 1820
INFO 08-31 20:42:02 [executor_base.py:119] Maximum concurrency for 40960 tokens per request: 2.10x
INFO 08-31 20:42:04 [llm_engine.py:417] init engine (profile, create kv cache, warmup model) took 2.88 seconds
INFO 08-31 20:42:04 [llm.py:298] Supported_tasks: ['generate']
INFO 08-31 20:43:03 [__init__.py:241] Automatically detected platform cuda.
CLIP image features: (1, 512)
Top CLIP tags: ['contains scatter plot']
Image tag list: [['contains scatter plot']]
[WARN] Path not found: download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Path not found: download/output/figures/2310.07894v1/tex_figures_5d.png
Image tag list: [None, None]
[WARN] Path not found: download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
Image tag list: [None]
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_demog
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_time
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_model
[WARN] Path not found: download/output/figures/2508.00717/results_ai_pays_dem
Image tag list: [None, None, None, None]
CLIP image features: (1, 512)
Top CLIP tags: ['a figure showing network']
CLIP image features: (1, 512)
Top CLIP tags: ['a figure showing network']
Image tag list: [['a figure showing network'], ['a figure showing network']]
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about bar chart']
Image tag list: [['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about bar chart']]
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
Prompt list: ['You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Manifold Preserving Guided Diffusion\nAbstract: Despite the recent advancements, conditional image generation still faces challenges of cost, generalizability, and the need for task-specific training. In this paper, we propose Manifold Preserving Guided Diffusion (MPGD), a training-free conditional generation framework that leverages pretrained diffusion models and off-the-shelf neural networks with minimal additional inference cost for a broad range of tasks. Specifically, we leverage the manifold hypothesis to refine the guided diffusion steps and introduce a shortcut algorithm in the process. We then propose two methods for on-manifold training-free guidance using pre-trained autoencoders and demonstrate that our shortcut inherently preserves the manifolds when applied to latent diffusion models. Our experiments show that MPGD is efficient and effective for solving a variety of conditional generation applications in low-compute settings, and can consistently offer up to 3.8x speed-ups with the same number of diffusion steps while maintaining high sample quality compared to the baselines.\nSection name: Experiments\n\nFigure block (optional):\n- label: \\label{fig:qualitative_faceid}; caption: \\caption{Examples of the FaceID guidance generation with pre-trained CelebA-HQ models. }\nTable block (optional):\n- label: \\label{tab:faceid}; text: \\begin{tabular}{@{}l|ccc@{}} \\toprule \\textbf{Method} & \\textbf{KID}$\\downarrow$        & \\textbf{FaceID}$\\downarrow$ & \\textbf{Time}$\\downarrow$ \\\\ \\midrule DDIM            & 0.0442          & 1.3914               & 3.41s             \\\\ \\m\nCited paper titles/abstracts (optional):\n- [lgd] Loss-guided diffusion models for plug-and-play controllable generation.: \n- [freedom] FreeDoM: Training-free energy-guided conditional diffusion model.: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. \\vspace{-5pt}\n\\subsection{Pixel space diffusion models}\nIn this section, we evaluate the performance of our proposed pixel domain methods (i.e. \\textbf{MPGD w/o Proj.}, \\textbf{MPGD-AE} and \\textbf{MPGD-Z}) with two different sets of conditional image generation tasks: solving liner inverse problems and human face generation guided by face recognition loss, which we refer to as FaceID guidance generation. For MPGD-AE and MPGD-Z, we use the pre-trained VQGAN models provided by~\\citet{rombach2021highresolution}.\n\\subsubsection{Noisy Linear Inverse Problem}\n2. For linear tasks, we use noisy super-resolution and noisy Gaussian deblurring as the test bed. We choose DPS~\\citep{dps}, LGD-MC~\\citep{lgd}, and MCG~\\citep{mcg} as the basleines. We test each method with two pre-trained diffusion models provided by~\\citet{dps}: one trained on FFHQ dataset~\\citep{karras2019style} and another on ImageNet~\\citep{deng2009imagenet}, both with $256 \\times 256$256 \\times 256 resolution. \nFor super-resolution, we down-sample ground truth images to $64 \\times 64$64 \\times 64. In the Gaussian deblurring task, we apply a $61\\times 61$61\\times 61 sized Gaussian blur with kernel intensity $3.0$3.0. to original images, Measurements in both tasks have a random noise with a variance of $\\sigma^{2}=0.05^{2}$\\sigma^{2}2=0.05^{2}2. We evaluate each task on a set of 1000~samples. We use the Kernel Inception distance (KID)~\\citep{binkowski2018demystifying} to assess the fidelity, Learned Perceptual Image Patch Similarity (LPIPS)~\\citep{zhang2018unreasonable} to evaluate the guidance quality, and the inference time to test the efficiency of the methods. All experiments are conducted on a single NVIDIA GeForce RTX 2080 Ti GPU . Figure~\\ref{fig:linear_main} shows the generated examples for qualitative comparison, and Figure~\\ref{fig:ffhq_sr} presents the quantitative results for the super-resolution task on FFHQ. All three of our methods significantly outperform the baselines with all metrics tested across a variety of different numbers of DDIM steps, and we can observe manifold projection improves the sample fidelity by a large margin.\nNext:\n1. \\hfill\n    [t]{0.53\\textwidth}0.53\\textwidth\n        \\centering\n        \\caption{Quantitative results for style guidance with Stable Diffusion experiment. Our method finds the sweet spot between following the prompt and following the style guidance.}\n        \\label{tab:style}\n2. \\vspace{-7pt}\n\\subsection{Latent diffusion models}\nTo evaluate \\textbf{MPGD-LDM}, we test our methods against the same baselines as the pixel-space FaceID experiments with text-to-image style guided generation task. The goal of this task is to generate images that fit both the text input prompts and the style of the reference images.\nWe use Stable Diffusion~\\citep{rombach2021highresolution} as the pre-trained text-to-image model and deploy the guided sampling methods to incorporate a style loss, which is calculated by the Frobenius norm between the Gram matrices of the reference images and the generated images.\nFor reference style images and text prompts, we randomly created 1000 conditioning pairs, using images from WikiArt~\\citep{saleh2015large} and prompts from PartiPrompts~\\citep{yu2022scaling} dataset. \nFigure~\\ref{fig:style_main} and Table~\\ref{tab:style} show qualitative and quantitative results for this experiment respectively. All the samples are generated on a single NVIDIA A100 GPU with 100 DDIM steps.\nOur method finds the sweet spot between following the text prompts, which usually instruct the generation to generate photo realistic images that do not suit the given style, and following the style guidance, which deviate from the prompts. Notably, because MPGD does not require propagation through the diffusion model, our method can provide significant speedup and can be fitted into a 16GB GPU while all the other methods cannot.\n3. \\vspace{-7pt}\n\nTarget length (characters): ~1212 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:qualitative_faceid\']\ntable_labels = [\'tab:faceid\']\nbib_keys = [\'freedom\', \'lgd\', \'freedom\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Efficient Integrators for Diffusion Generative Models\nAbstract: Diffusion models suffer from slow sample generation at inference time. Therefore, developing a principled framework for fast deterministic/stochastic sampling for a broader class of diffusion models is a promising direction. We propose two complementary frameworks for accelerating sample generation in pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate integrators generalize DDIM, mapping the reverse diffusion dynamics to a more amenable space for sampling. In contrast, splitting-based integrators, commonly used in molecular dynamics, reduce the numerical simulation error by cleverly alternating between numerical updates involving the data and auxiliary variables. After extensively studying these methods empirically and theoretically, we present a hybrid method that leads to the best-reported performance for diffusion models in augmented spaces. Applied to Phase Space Langevin Diffusion [Pandey & Mandt, 2023] on CIFAR-10, our deterministic and stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing baselines, respectively. Our code and model checkpoints wil\nSection name: Additional Experimental Results\n\nFigure block (optional):\n- label: \\label{fig:precond_a}; caption: \\caption{}\n- label: \\label{fig:precond_b}; caption: \\caption{}\nTable block (optional):\n- label: \\label{table:main}; text: \\begin{tabular}{@{}clcccc@{}} \\toprule                                         &               &                                                             &           & \\multicolumn{2}{c}{NFE (FID@50k $\\downarrow$)} \\\\ \\midrule           \nCited paper titles/abstracts (optional):\n- [songdenoising] Denoising diffusion implicit models.: \n- [lu2022dpm] Dpm-solver: A fast ode solver for diffusion probabilistic model   sampling in around 10 steps.: \n- [zhang2023fast] Fast sampling of diffusion models with exponential integrator.: \n- [karraselucidating] Elucidating the design space of diffusion-based generative models.: \n- [liupseudo] Pseudo numerical methods for diffusion models on manifolds.: \n- [bao2022analyticdpm] Analytic-DPM: an analytic estimate of the optimal reverse variance   in diffusion probabilistic models.: \n- [xue2023sasolver] Sa-solver: Stochastic adams solver for fast sampling of diffusion   models, 2023: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. \\textbf{Notation.} For simplicity, we denote our best-performing Reduced Splitting and Conjugate Splitting integrators as \\textit{\\gls{SPS}} and \\textit{\\gls{CSPS}}, respectively. Consequently, we refer to the \\underline{D}eterministic \\gls{RVV}RVV and \\underline{S}tochastic \\gls{ROBA}ROBA samplers as \\gls{SPS}SPS-D and \\gls{SPS}SPS-S, and their conjugate variants as \\gls{CSPS}CSPS-D and \\gls{CSPS}CSPS-S, respectively.\n2. \\textbf{Datasets and Evaluation Metrics.} We use the CIFAR-10, CelebA-64 \\citep{liu2015faceattributes} and the AFHQ-v2 \\citep{choi2020stargan} datasets for comparisons. Unless specified otherwise, we report FID for 50k generated samples for all datasets and quantify sampling efficiency using \\gls{NFE}NFE. We include full experimental details in Appendix \\ref{app:exp_setup}.\nNext:\n1. \\textbf{Empirical Observations:} For CIFAR-10, our ODE sampler performs comparably or outperforms all other baselines for \\gls{NFE}NFE $\\geq$\\geq 50 (Fig. \\ref{fig:sota_all}, Top Left). Similarly, our SDE sampler outperforms all other baselines for \\gls{NFE}NFE $\\geq$\\geq 40 (Fig. \\ref{fig:sota_all}, Top Right). We make similar observations for the CelebA-64 and AFHQv2-64 datasets, where our proposed samplers can obtain significant gains over prior methods for \\gls{NFE}NFE $\\geq$\\geq 70 (See Fig. \\ref{fig:sota_all}, Bottom Left). Therefore, our proposed samplers for \\gls{PSLD}PSLD are competitive with recent work. Moreover, for all datasets, our stochastic sampler achieves better sample quality for low sampling budgets (\\gls{NFE}NFE $<$< 50) as compared to our deterministic sampler. \nLastly, in contrast to CIFAR-10, we find that the \\gls{CSPS}CSPS-S sampler works better than the \\gls{SPS}SPS-S sampler for the CelebA-64 and AFHQv2-64 datasets, indicating its effectiveness for higher-resolution sampling.\n\nTarget length (characters): ~1323 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:precond_a\', \'fig:precond_b\']\ntable_labels = [\'table:main\']\nbib_keys = [\'songdenoising\', \'zhang2023fast\', \'lu2022dpm\', \'liupseudo\', \'karraselucidating\', \'xue2023sasolver\', \'bao2022analyticdpm\', \'karraselucidating\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average\nAbstract: Presently the most successful approaches to semi-supervised learning are based on consistency regularization, whereby a model is trained to be robust to small perturbations of its inputs and parameters. To understand consistency regularization, we conceptually explore how loss geometry interacts with training procedures. The consistency loss dramatically improves generalization performance over supervised-only training; however, we show that SGD struggles to converge on the consistency loss and continues to make large steps that lead to changes in predictions on the test data. Motivated by these observations, we propose to train consistency-based methods with Stochastic Weight Averaging (SWA), a recent approach which averages weights along the trajectory of SGD with a modified learning rate schedule. We also propose fast-SWA, which further accelerates convergence by averaging multiple points within each cycle of a cyclical learning rate schedule. With weight averaging, we achieve the best known semi-supervised results on CIFAR-10 and CIFAR-100, over many different quantities of labeled training data. For example, we achieve 5.0% error on CIFAR-10 with only 4000 labels, compared to \nSection name: Experiments\n\nFigure block (optional):\n- label: \\label{fig:cnn_c10_c100_trend}; caption: \\caption{Prediction errors of base models and their weight averages (\\fastswa and \\swa) for CNN    on \\textbf{(left)} CIFAR-10 with $4k$ labels,   \\textbf{(middle)} CIFAR-100 with $10k$ labels,   and \\textbf{(right)} CIFAR-100 $50k$ labels \nTable block (optional):\n- label: \\label{table:cifar100_shakeshake}; text: \\begin{tabular}{l c c c c c l l l l l } Number of labels\t\t\t& \t10k \t\t& \t50k \t\t& \t50k + 500k \t\t& 50k +   237k$^*$ \t\\\\  \\midrule TE (CNN) \\citep{te}  & \t38.65 \\tpm 0.51\t\t&\t26.30 \\tpm 0.15\t\t&\t23.62 \\tpm 0.17  \t&\t 23.79 \\tpm 0.17  \t\\\\  \\midrule \nCited paper titles/abstracts (optional):\n- [te] Temporal Ensembling for Semi-Supervised Learning.: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. In Figure \\ref{fig:cnn_c10_c100_trend} (left), we visualize the test\nerror as a function of iteration using the CNN.\nWe observe that when the cyclical learning rate starts after epoch $\\ell=180$\\ell=180, \nthe base models drop in performance due to the sudden increase in learning rate (see Figure \\ref{fig:swa_model} left).\nHowever, \\fastswa continues to improve while collecting the weights corresponding to high learning rates for averaging.\nIn general, we also find that the cyclical learning rate improves the base models beyond the usual cosine annealing \nschedule and increases the performance of \\fastswa as training progresses. \nCompared to \\swa, we also observe that \\fastswa converges substantially faster, \nfor instance, reducing the error to $10.5\\%$10.5\\%  at epoch $200$200 while \\swa attains \nsimilar error at epoch $350$350 for CIFAR-10 $4k$4k labels (Figure \n\\ref{fig:cnn_c10_c100_trend} left). We provide additional plots in Section \\ref{sec:add_results} \nshowing the convergence of the \\pi and MT models in all label \nsettings, where we observe similar trends that \\fastswa results in faster error reduction.\n2. We also find that the performance gains of \\fastswa over base models are higher for the \\pi model compared to the MT model, which is consistent with the convexity observation in Section \\ref{sec:empirical_convexity} \nand Figure \\ref{fig:convexity}.\nIn the previous evaluations \\citep[see e.g.][]{ssl-eval, mt}, the \\pi model was shown to be inferior to the MT model.\nHowever, with weight averaging, \\fastswa reduces the gap between \\pi and MT performance. \nSurprisingly, we find that the \\pi model can outperform MT after applying \\fastswa  \nwith moderate to large numbers of labeled points. \nIn particular, the $\\Pi$\\Pi+fast-SWA model outperforms MT+\\fastswa on CIFAR-10 with $4k$4k, $10k$10k, and $50k$50k labeled\ndata points for the Shake-Shake  architecture.\n3. \\vspace{-0.8\\baselineskip}\n4. \\subsection{CIFAR-100 and Extra Unlabeled Data} \\label{sec:cifar100_cnn} \\label{sec:cifar100}\n5. We evaluate the \\pi and MT models with \\fastswa on \nCIFAR-100.\nWe train our models using $50000$50000 images with $10k$10k and $50k$50k labels using the $13$13-layer CNN. \nWe also analyze the effect of using the Tiny Images dataset \\citep{tiny_images} as an additional source of unlabeled data.\nNext:\n1. \\subsection{Advancing State-of-the-Art} \\label{sec:soa}\n2. We have shown that \\fastswa can significantly improve the performance of both the \\pi and \nMT models. We provide a summary comparing our results with the previous best results in the literature in Table  \\ref{table:soa}, using the $13$13-layer CNN and\nthe Shake-Shake architecture that had been applied previously. We also provide detailed results the Appendix \\ref{sec:add_results}.\n3. \\subsection{Preliminary Results on Domain Adaptation}\n\\label{sec:domain_adaptation}\nDomain adaptation problems involve learning using a source domain $X_s$X_s \nequipped with labels $Y_s$Y_s and performing classification on the target \ndomain $X_t$X_t while having no access to the target labels at training time. \nA recent model by \\citet{da_mt} applies the consistency enforcing principle for \ndomain adaptation and achieves state-of-the-art results on many datasets. \nApplying \\fastswa to this model on domain adaptation from CIFAR-10 to STL we \nwere able to improve the best results reported in the literature from\n$19.9\\%$19.9\\% to $16.8\\%$16.8\\%. \nSee Section \\ref{sec:exp_da} for more details on the domain adaptation experiments.\n\nTarget length (characters): ~1438 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:cnn_c10_c100_trend\']\ntable_labels = [\'table:cifar100_shakeshake\']\nbib_keys = [\'te\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Generative AI in Higher Education: Evidence from an Elite College\nAbstract: Generative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPT\'s release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AI\'s potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AI\'s educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.\nSection name: Generative AI Usage Patterns Among Students\n\nFigure block (optional):\n- label: \\label{fig:ai_use}; caption: \\caption{The Adoption of Generative AI among Middlebury College Students}\n- label: \\label{fig:ai_use_time}; caption: \\caption{The Evolution of Generative AI Adoption among Middlebury College Students}\n- label: \\label{fig:ai_models}; caption: \\caption{Adoption of Generative AI Models Among College Students}\n- label: \\label{fig:ai_pays_dem}; caption: \\caption{Percent of Students Who Pay for Generative AI Tools}\nTable block (optional):\n- label: \\label{tab:ai_usage_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{8}{c}{Outcome: Uses AI during the semester with frequency of at least...}  \\\\\\cmidrule{3-10}  \t\t\t\t\t%\t\t\n- label: \\label{tab:ai_adopt_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{10}{c}{Outcome: Started using generative AI...}  \\\\\\cmidrule{3-12}  \t\t\t\t\t&& Before\n- label: \\label{tab:ai_freq_het}; text: \\begin{tabular}{lccccc} \t\t\t\t\t\t\\toprule \t\t\t\t\t\t&& \\multicolumn{4}{c}{By Usage Frequency} \\\\ \\cmidrule{3-6} \t\t\t\t\t\t& Any use & Rarely & Occasionally & Frequently & Very Frequently \\\\ \t\t\t\t\t\t& (1) & (2) & (3) & (4) & (5) \\\\ \\midrule\t\t\t\t\t\t \t\t\t\t\t\t\\\n- label: \\label{tab:ai_model_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{10}{c}{Outcome: =1 if student uses}  \\\\\\cmidrule{3-12}  \t\t\t\t\t&& OpenAI\'s && Google\nCited paper titles/abstracts (optional):\n- [bick.etal2025] The rapid adoption of generative AI.: \n- [mcclain2024] Americans\' use of ChatGPT is ticking up, but few trust its   election information: \n- [nam2023ai] 56\\% of college students have used ai on assignments or exams.: \n- [stohr.etal2024] \\"o: \n- [ravselj.etal2025] s: \n- [otis2024global] Global Evidence on Gender Gaps and Generative AI.: \n- [hartley.etal2025] The Labor Market Effects of Generative Artificial   Intelligence: \n- [humlum.vestergaard2025] Large Language Models, Small Labor Market Effects: \n- [carvajal2024] Will artificial intelligence get in the way of achieving gender   equality?: \n- [david1990dynamo] The dynamo and the computer: an historical perspective on the modern   productivity paradox.: \n- [gallup2024] AI in the Workplace: Answering 3 Big Questions: \n- [hall2003adoption] Adoption of new technology: \n- [stokey2021technology] Technology diffusion.: \n- [world2016world] World Bank: \n- [bick.etal2025] The rapid adoption of generative AI.: \n- [mcclain2024] Americans\' use of ChatGPT is ticking up, but few trust its   election information: \n- [nam2023ai] 56\\% of college students have used ai on assignments or exams.: \n- [stohr.etal2024] \\"o: \n- [ravselj.etal2025] s: \n- [otis2024global] Global Evidence on Gender Gaps and Generative AI.: \n- [hartley.etal2025] The Labor Market Effects of Generative Artificial   Intelligence: \n- [humlum.vestergaard2025] Large Language Models, Small Labor Market Effects: \n- [carvajal2024] Will artificial intelligence get in the way of achieving gender   equality?: \n- [david1990dynamo] The dynamo and the computer: an historical perspective on the modern   productivity paradox.: \n- [gallup2024] AI in the Workplace: Answering 3 Big Questions: \n- [hall2003adoption] Adoption of new technology: \n- [stokey2021technology] Technology diffusion.: \n- [world2016world] World Bank: \n\nk-most adjacent paragraphs (context):\n(none)\n\nTarget length (characters): ~13653 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:ai_use\', \'fig:ai_use_time\', \'fig:ai_models\', \'fig:ai_pays_dem\']\ntable_labels = [\'tab:ai_usage_correlates\', \'tab:ai_adopt_correlates\', \'tab:ai_freq_het\', \'tab:ai_model_correlates\']\nbib_keys = [\'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'nam2023ai\', \'stohr.etal2024\', \'carvajal2024\', \'ravselj.etal2025\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'david1990dynamo\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'hall2003adoption\', \'otis2024global\', \'nam2023ai\', \'stokey2021technology\', \'carvajal2024\', \'stohr.etal2024\', \'world2016world\', \'ravselj.etal2025\', \'carvajal2024\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Neural-Symbolic Message Passing with Dynamic Pruning\nAbstract: Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs) is a challenging task. Recently, a line of message-passing-based research has been proposed to solve CQA. However, they perform unsatisfactorily on negative queries and fail to address the noisy messages between variable nodes in the query graph. Moreover, they offer little interpretability and require complex query data and resource-intensive training. In this paper, we propose a Neural-Symbolic Message Passing (NSMP) framework based on pre-trained neural link predictors. By introducing symbolic reasoning and fuzzy logic, NSMP can generalize to arbitrary existential first order logic queries without requiring training while providing interpretable answers. Furthermore, we introduce a dynamic pruning strategy to filter out noisy messages between variable nodes. Experimental results show that NSMP achieves a strong performance. Additionally, through complexity analysis and empirical verification, we demonstrate the superiority of NSMP in inference time over the current state-of-the-art neural-symbolic method. Compared to this approach, NSMP demonstrates faster inference times across all query types on benchmark dat\nSection name: More Details about the Datasets\n\nFigure block (optional):\n- label: \\label{figure4}; caption: \\caption{Graphical representation of the query types of the BetaE dataset considered in our experiment, where $p$, $i$, $u$, and $n$ represent projection, intersection, union, and negation, respectively. }\n- label: \\label{figure5}; caption: \\caption{Graphical representation of the query types of the FIT dataset considered in our experiment, where $l$, $m$, and $c$ represent existential leaf, multi graph, and circle, respectively. }\nTable block (optional):\n- label: \\label{KG stat}; text: \\begin{tabular}{ccccccc}     \\toprule     \\textbf{Knowledge Graph} & \\textbf{Entities} & \\textbf{Relations} & \\textbf{Training Edges} & \\textbf{Val Edges} & \\textbf{Test Edges} & \\textbf{Total Edges} \\\\     \\midrule     FB15k-237 & 14,505 &\n- label: \\label{BetaE stat}; text: \\begin{tabular}{ccccccc}     \\toprule     \\multirow{2}{*}{\\textbf{Knowledge Graph}} & \\multicolumn{2}{c}{\\textbf{Training Queries}} & \\multicolumn{2}{c}{\\textbf{Validation Queries}} & \\multicolumn{2}{c}{\\textbf{Test Queries}} \\\\     \\cmidru\n- label: \\label{FIT stat}; text: \\begin{tabular}{ccccccccccc}     \\toprule     \\textbf{Knowledge Graph} & \\textbf{pni} & \\textbf{2il} & \\textbf{3il} & \\textbf{2m} & \\textbf{2nm} & \\textbf{3mp} & \\textbf{3pm} & \\textbf{im} & \\textbf{3c} & \\textbf{3cm} \\\\     \\midrule     FB\nCited paper titles/abstracts (optional):\n- [ren2020beta] Beta embeddings for multi-hop logical reasoning in knowledge graphs: \n- [yin2024rethinking] Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors: \n\nk-most adjacent paragraphs (context):\n(none)\n\nTarget length (characters): ~973 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'figure4\', \'figure5\']\ntable_labels = [\'KG stat\', \'BetaE stat\', \'FIT stat\']\nbib_keys = [\'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Progressive Prompts: Continual Learning for Language Models\nAbstract: We introduce Progressive Prompts - a simple and efficient approach for continual learning in language models. Our method allows forward transfer and resists catastrophic forgetting, without relying on data replay or a large number of task-specific parameters. Progressive Prompts learns a new soft prompt for each task and sequentially concatenates it with the previously learned prompts, while keeping the base model frozen. Experiments on standard continual learning benchmarks show that our approach outperforms state-of-the-art methods, with an improvement >20% in average test accuracy over the previous best-preforming method on T5 model. We also explore a more challenging continual learning setup with longer sequences of tasks and show that Progressive Prompts significantly outperforms prior methods.\nSection name: Further experimental results\n\nFigure block (optional):\n- label: \\label{fig:fwt_order8}; caption: \\caption{Forward transfer score of different approaches on order 8. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:fwt_order9}; caption: \\caption{Forward transfer score of different approaches on order 9. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:fwt_order10}; caption: \\caption{Forward transfer score of different approaches on order 10. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order8}; caption: \\caption{Backward transfer score of different approaches on order 8. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order9}; caption: \\caption{Backward transfer score of different approaches on order 9. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order10}; caption: \\caption{Backward transfer score of different approaches on order 10. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:evolution}; caption: \\caption{Evolution of average accuracy after learning new tasks.}\n- label: \\label{fig:fig_long_bars}; caption: \\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improve\nTable block (optional):\n- label: \\label{wrap-tab:init}; text: \\begin{tabular}{lcc}\\\\\\toprule   Method & Few-shot & Full-shot \\\\\\midrule PT + Prev. Init. & 48.2 & 50.0 \\\\  ProgPrompt & \\textbf{53.5} & \\textbf{69.3} \\\\ \\bottomrule \\end{tabular}\n- label: \\label{tab:table_appendix_long}; text: \\begin{tabular}{lccc|ccc|ccc|ccc} % <-- Alignments: 1st column left & 2nd middle and 3rd right & with vertical lines in between     \\toprule       \\textbf{Method} $\\downarrow$ & \\multicolumn{3}{c}{\\textbf{Order 8}}  & \\multicolumn{3}{c}{\\te\nCited paper titles/abstracts (optional):\n- [lopez2017gradient] Gradient episodic memory for continual learning: \n\nk-most adjacent paragraphs (context):\nNext:\n1. \\includegraphics[width=0.9\\textwidth]{images/long_exp_bars.png}\n\\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improvement of Progressive Prompts test score compared to per-task prompt on the corresponding task.}\n\\label{fig:fig_long_bars}\n\\end{figure}\n2. \\includegraphics[width=0.9\\textwidth]{images/long_exp_bars.png}\n\\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improvement of Progressive Prompts test score compared to per-task prompt on the corresponding task.}\n\\label{fig:fig_long_bars}\n3. \\includegraphics[width=0.7\\textwidth]{images/superglue_illustration.png}\n4. \\caption{Original prompt tuning versus Progressive Prompts on SuperGLUE datasets. For illustration, we show how SuperGLUE task WiC is leaned (we have similar scheme for other tasks). Prompt tuning trains a single prompt of 100 tokens for WiC task. Progressive Prompts method learns a prompt of 40 tokens, which is progressively appended to the six frozen prompts of 10 tokens learned on GLUE benchmark (with random task order). Total prompt length is equal in both approaches. }\n\\label{fig:fig_super_illustration}\n\\end{figure}\n5. \\includegraphics[width=0.7\\textwidth]{images/superglue_illustration.png}\n\nTarget length (characters): ~1627 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:fwt_order8\', \'fig:fwt_order9\', \'fig:fwt_order10\', \'fig:bwt_order8\', \'fig:bwt_order9\', \'fig:bwt_order10\', \'fig:evolution\', \'fig:fig_long_bars\']\ntable_labels = [\'wrap-tab:init\', \'tab:table_appendix_long\']\nbib_keys = [\'lopez2017gradient\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).']
Number of prompts: 6
Image tag lists: [[['contains scatter plot']], [None, None], [None], [None, None, None, None], [['a figure showing network'], ['a figure showing network']], [['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about bar chart']]]
Number of Image tag lists: 6
Number of Image projection lists: 6
INFO 08-31 20:44:04 [utils.py:326] non-default args: {'model': 'Qwen/Qwen3-8B', 'enable_prompt_embeds': True, 'trust_remote_code': True, 'dtype': 'float16', 'gpu_memory_utilization': 0.6, 'disable_log_stats': True, 'enforce_eager': True}
INFO 08-31 20:44:14 [__init__.py:711] Resolved architecture: Qwen3ForCausalLM
WARNING 08-31 20:44:14 [__init__.py:2819] Casting torch.bfloat16 to torch.float16.
INFO 08-31 20:44:14 [__init__.py:1750] Using max model len 40960
WARNING 08-31 20:44:14 [arg_utils.py:1770] --enable-prompt-embeds is not supported by the V1 Engine. Falling back to V0. 
WARNING 08-31 20:44:14 [arg_utils.py:1544] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.
INFO 08-31 20:44:14 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 08-31 20:44:14 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 08-31 20:44:14 [__init__.py:3565] Cudagraph is disabled under eager mode
INFO 08-31 20:44:14 [llm_engine.py:222] Initializing a V0 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen3-8B', speculative_config=None, tokenizer='Qwen/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{"enable_fusion":false,"enable_noop":false},"max_capture_size":0,"local_cache_dir":null}, use_cached_outputs=False, 
INFO 08-31 20:44:16 [cuda.py:436] Using Flash Attention backend.
INFO 08-31 20:44:17 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 08-31 20:44:17 [model_runner.py:1080] Starting to load model Qwen/Qwen3-8B...
INFO 08-31 20:44:17 [weight_utils.py:296] Using model weights format ['*.safetensors']
INFO 08-31 20:44:23 [default_loader.py:262] Loading weights took 5.24 seconds
INFO 08-31 20:44:23 [model_runner.py:1112] Model loading took 15.2683 GiB and 5.616267 seconds
INFO 08-31 20:44:25 [worker.py:295] Memory profiling takes 0.90 seconds
INFO 08-31 20:44:25 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.53GiB) x gpu_memory_utilization (0.60) = 28.52GiB
INFO 08-31 20:44:25 [worker.py:295] model weights take 15.27GiB; non_torch_memory takes 0.02GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 11.83GiB.
INFO 08-31 20:44:25 [executor_base.py:114] # cuda blocks: 5385, # CPU blocks: 1820
INFO 08-31 20:44:25 [executor_base.py:119] Maximum concurrency for 40960 tokens per request: 2.10x
INFO 08-31 20:44:26 [llm_engine.py:417] init engine (profile, create kv cache, warmup model) took 3.18 seconds
INFO 08-31 20:44:27 [llm.py:298] Supported_tasks: ['generate']
INFO 08-31 20:49:07 [__init__.py:241] Automatically detected platform cuda.
CLIP image features: (1, 512)
Top CLIP tags: ['contains scatter plot']
Image tag list: [['contains scatter plot']]
[WARN] Path not found: download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Path not found: download/output/figures/2310.07894v1/tex_figures_5d.png
Image tag list: [None, None]
[WARN] Path not found: download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
Image tag list: [None]
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_demog
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_time
[WARN] Path not found: download/output/figures/2508.00717/results_ai_use_model
[WARN] Path not found: download/output/figures/2508.00717/results_ai_pays_dem
Image tag list: [None, None, None, None]
CLIP image features: (1, 512)
Top CLIP tags: ['a figure showing network']
CLIP image features: (1, 512)
Top CLIP tags: ['a figure showing network']
Image tag list: [['a figure showing network'], ['a figure showing network']]
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about heatmap']
CLIP image features: (1, 512)
Top CLIP tags: ['a chart about bar chart']
Image tag list: [['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about bar chart']]
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5c.png
[WARN] Not a file: ./download/output/figures/2310.07894v1/tex_figures_5d.png
[WARN] Not a file: ./download/output/figures/1806.05594v3/figs_exps_results_epochs_c10_cnn_MT_4k_wmodelTrue_nolegend
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_demog
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_time
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_use_model
[WARN] Not a file: ./download/output/figures/2508.00717/results_ai_pays_dem
Prompt list: ['You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Manifold Preserving Guided Diffusion\nAbstract: Despite the recent advancements, conditional image generation still faces challenges of cost, generalizability, and the need for task-specific training. In this paper, we propose Manifold Preserving Guided Diffusion (MPGD), a training-free conditional generation framework that leverages pretrained diffusion models and off-the-shelf neural networks with minimal additional inference cost for a broad range of tasks. Specifically, we leverage the manifold hypothesis to refine the guided diffusion steps and introduce a shortcut algorithm in the process. We then propose two methods for on-manifold training-free guidance using pre-trained autoencoders and demonstrate that our shortcut inherently preserves the manifolds when applied to latent diffusion models. Our experiments show that MPGD is efficient and effective for solving a variety of conditional generation applications in low-compute settings, and can consistently offer up to 3.8x speed-ups with the same number of diffusion steps while maintaining high sample quality compared to the baselines.\nSection name: Experiments\n\nFigure block (optional):\n- label: \\label{fig:qualitative_faceid}; caption: \\caption{Examples of the FaceID guidance generation with pre-trained CelebA-HQ models. }\nTable block (optional):\n- label: \\label{tab:faceid}; text: \\begin{tabular}{@{}l|ccc@{}} \\toprule \\textbf{Method} & \\textbf{KID}$\\downarrow$        & \\textbf{FaceID}$\\downarrow$ & \\textbf{Time}$\\downarrow$ \\\\ \\midrule DDIM            & 0.0442          & 1.3914               & 3.41s             \\\\ \\m\nCited paper titles/abstracts (optional):\n- [lgd] Loss-guided diffusion models for plug-and-play controllable generation.: \n- [freedom] FreeDoM: Training-free energy-guided conditional diffusion model.: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. \\vspace{-5pt}\n\\subsection{Pixel space diffusion models}\nIn this section, we evaluate the performance of our proposed pixel domain methods (i.e. \\textbf{MPGD w/o Proj.}, \\textbf{MPGD-AE} and \\textbf{MPGD-Z}) with two different sets of conditional image generation tasks: solving liner inverse problems and human face generation guided by face recognition loss, which we refer to as FaceID guidance generation. For MPGD-AE and MPGD-Z, we use the pre-trained VQGAN models provided by~\\citet{rombach2021highresolution}.\n\\subsubsection{Noisy Linear Inverse Problem}\n2. For linear tasks, we use noisy super-resolution and noisy Gaussian deblurring as the test bed. We choose DPS~\\citep{dps}, LGD-MC~\\citep{lgd}, and MCG~\\citep{mcg} as the basleines. We test each method with two pre-trained diffusion models provided by~\\citet{dps}: one trained on FFHQ dataset~\\citep{karras2019style} and another on ImageNet~\\citep{deng2009imagenet}, both with $256 \\times 256$256 \\times 256 resolution. \nFor super-resolution, we down-sample ground truth images to $64 \\times 64$64 \\times 64. In the Gaussian deblurring task, we apply a $61\\times 61$61\\times 61 sized Gaussian blur with kernel intensity $3.0$3.0. to original images, Measurements in both tasks have a random noise with a variance of $\\sigma^{2}=0.05^{2}$\\sigma^{2}2=0.05^{2}2. We evaluate each task on a set of 1000~samples. We use the Kernel Inception distance (KID)~\\citep{binkowski2018demystifying} to assess the fidelity, Learned Perceptual Image Patch Similarity (LPIPS)~\\citep{zhang2018unreasonable} to evaluate the guidance quality, and the inference time to test the efficiency of the methods. All experiments are conducted on a single NVIDIA GeForce RTX 2080 Ti GPU . Figure~\\ref{fig:linear_main} shows the generated examples for qualitative comparison, and Figure~\\ref{fig:ffhq_sr} presents the quantitative results for the super-resolution task on FFHQ. All three of our methods significantly outperform the baselines with all metrics tested across a variety of different numbers of DDIM steps, and we can observe manifold projection improves the sample fidelity by a large margin.\nNext:\n1. \\hfill\n    [t]{0.53\\textwidth}0.53\\textwidth\n        \\centering\n        \\caption{Quantitative results for style guidance with Stable Diffusion experiment. Our method finds the sweet spot between following the prompt and following the style guidance.}\n        \\label{tab:style}\n2. \\vspace{-7pt}\n\\subsection{Latent diffusion models}\nTo evaluate \\textbf{MPGD-LDM}, we test our methods against the same baselines as the pixel-space FaceID experiments with text-to-image style guided generation task. The goal of this task is to generate images that fit both the text input prompts and the style of the reference images.\nWe use Stable Diffusion~\\citep{rombach2021highresolution} as the pre-trained text-to-image model and deploy the guided sampling methods to incorporate a style loss, which is calculated by the Frobenius norm between the Gram matrices of the reference images and the generated images.\nFor reference style images and text prompts, we randomly created 1000 conditioning pairs, using images from WikiArt~\\citep{saleh2015large} and prompts from PartiPrompts~\\citep{yu2022scaling} dataset. \nFigure~\\ref{fig:style_main} and Table~\\ref{tab:style} show qualitative and quantitative results for this experiment respectively. All the samples are generated on a single NVIDIA A100 GPU with 100 DDIM steps.\nOur method finds the sweet spot between following the text prompts, which usually instruct the generation to generate photo realistic images that do not suit the given style, and following the style guidance, which deviate from the prompts. Notably, because MPGD does not require propagation through the diffusion model, our method can provide significant speedup and can be fitted into a 16GB GPU while all the other methods cannot.\n3. \\vspace{-7pt}\n\nTarget length (characters): ~1212 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:qualitative_faceid\']\ntable_labels = [\'tab:faceid\']\nbib_keys = [\'freedom\', \'lgd\', \'freedom\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Efficient Integrators for Diffusion Generative Models\nAbstract: Diffusion models suffer from slow sample generation at inference time. Therefore, developing a principled framework for fast deterministic/stochastic sampling for a broader class of diffusion models is a promising direction. We propose two complementary frameworks for accelerating sample generation in pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate integrators generalize DDIM, mapping the reverse diffusion dynamics to a more amenable space for sampling. In contrast, splitting-based integrators, commonly used in molecular dynamics, reduce the numerical simulation error by cleverly alternating between numerical updates involving the data and auxiliary variables. After extensively studying these methods empirically and theoretically, we present a hybrid method that leads to the best-reported performance for diffusion models in augmented spaces. Applied to Phase Space Langevin Diffusion [Pandey & Mandt, 2023] on CIFAR-10, our deterministic and stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing baselines, respectively. Our code and model checkpoints wil\nSection name: Additional Experimental Results\n\nFigure block (optional):\n- label: \\label{fig:precond_a}; caption: \\caption{}\n- label: \\label{fig:precond_b}; caption: \\caption{}\nTable block (optional):\n- label: \\label{table:main}; text: \\begin{tabular}{@{}clcccc@{}} \\toprule                                         &               &                                                             &           & \\multicolumn{2}{c}{NFE (FID@50k $\\downarrow$)} \\\\ \\midrule           \nCited paper titles/abstracts (optional):\n- [songdenoising] Denoising diffusion implicit models.: \n- [lu2022dpm] Dpm-solver: A fast ode solver for diffusion probabilistic model   sampling in around 10 steps.: \n- [zhang2023fast] Fast sampling of diffusion models with exponential integrator.: \n- [karraselucidating] Elucidating the design space of diffusion-based generative models.: \n- [liupseudo] Pseudo numerical methods for diffusion models on manifolds.: \n- [bao2022analyticdpm] Analytic-DPM: an analytic estimate of the optimal reverse variance   in diffusion probabilistic models.: \n- [xue2023sasolver] Sa-solver: Stochastic adams solver for fast sampling of diffusion   models, 2023: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. \\textbf{Notation.} For simplicity, we denote our best-performing Reduced Splitting and Conjugate Splitting integrators as \\textit{\\gls{SPS}} and \\textit{\\gls{CSPS}}, respectively. Consequently, we refer to the \\underline{D}eterministic \\gls{RVV}RVV and \\underline{S}tochastic \\gls{ROBA}ROBA samplers as \\gls{SPS}SPS-D and \\gls{SPS}SPS-S, and their conjugate variants as \\gls{CSPS}CSPS-D and \\gls{CSPS}CSPS-S, respectively.\n2. \\textbf{Datasets and Evaluation Metrics.} We use the CIFAR-10, CelebA-64 \\citep{liu2015faceattributes} and the AFHQ-v2 \\citep{choi2020stargan} datasets for comparisons. Unless specified otherwise, we report FID for 50k generated samples for all datasets and quantify sampling efficiency using \\gls{NFE}NFE. We include full experimental details in Appendix \\ref{app:exp_setup}.\nNext:\n1. \\textbf{Empirical Observations:} For CIFAR-10, our ODE sampler performs comparably or outperforms all other baselines for \\gls{NFE}NFE $\\geq$\\geq 50 (Fig. \\ref{fig:sota_all}, Top Left). Similarly, our SDE sampler outperforms all other baselines for \\gls{NFE}NFE $\\geq$\\geq 40 (Fig. \\ref{fig:sota_all}, Top Right). We make similar observations for the CelebA-64 and AFHQv2-64 datasets, where our proposed samplers can obtain significant gains over prior methods for \\gls{NFE}NFE $\\geq$\\geq 70 (See Fig. \\ref{fig:sota_all}, Bottom Left). Therefore, our proposed samplers for \\gls{PSLD}PSLD are competitive with recent work. Moreover, for all datasets, our stochastic sampler achieves better sample quality for low sampling budgets (\\gls{NFE}NFE $<$< 50) as compared to our deterministic sampler. \nLastly, in contrast to CIFAR-10, we find that the \\gls{CSPS}CSPS-S sampler works better than the \\gls{SPS}SPS-S sampler for the CelebA-64 and AFHQv2-64 datasets, indicating its effectiveness for higher-resolution sampling.\n\nTarget length (characters): ~1323 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:precond_a\', \'fig:precond_b\']\ntable_labels = [\'table:main\']\nbib_keys = [\'songdenoising\', \'zhang2023fast\', \'lu2022dpm\', \'liupseudo\', \'karraselucidating\', \'xue2023sasolver\', \'bao2022analyticdpm\', \'karraselucidating\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average\nAbstract: Presently the most successful approaches to semi-supervised learning are based on consistency regularization, whereby a model is trained to be robust to small perturbations of its inputs and parameters. To understand consistency regularization, we conceptually explore how loss geometry interacts with training procedures. The consistency loss dramatically improves generalization performance over supervised-only training; however, we show that SGD struggles to converge on the consistency loss and continues to make large steps that lead to changes in predictions on the test data. Motivated by these observations, we propose to train consistency-based methods with Stochastic Weight Averaging (SWA), a recent approach which averages weights along the trajectory of SGD with a modified learning rate schedule. We also propose fast-SWA, which further accelerates convergence by averaging multiple points within each cycle of a cyclical learning rate schedule. With weight averaging, we achieve the best known semi-supervised results on CIFAR-10 and CIFAR-100, over many different quantities of labeled training data. For example, we achieve 5.0% error on CIFAR-10 with only 4000 labels, compared to \nSection name: Experiments\n\nFigure block (optional):\n- label: \\label{fig:cnn_c10_c100_trend}; caption: \\caption{Prediction errors of base models and their weight averages (\\fastswa and \\swa) for CNN    on \\textbf{(left)} CIFAR-10 with $4k$ labels,   \\textbf{(middle)} CIFAR-100 with $10k$ labels,   and \\textbf{(right)} CIFAR-100 $50k$ labels \nTable block (optional):\n- label: \\label{table:cifar100_shakeshake}; text: \\begin{tabular}{l c c c c c l l l l l } Number of labels\t\t\t& \t10k \t\t& \t50k \t\t& \t50k + 500k \t\t& 50k +   237k$^*$ \t\\\\  \\midrule TE (CNN) \\citep{te}  & \t38.65 \\tpm 0.51\t\t&\t26.30 \\tpm 0.15\t\t&\t23.62 \\tpm 0.17  \t&\t 23.79 \\tpm 0.17  \t\\\\  \\midrule \nCited paper titles/abstracts (optional):\n- [te] Temporal Ensembling for Semi-Supervised Learning.: \n\nk-most adjacent paragraphs (context):\nPrevious:\n1. In Figure \\ref{fig:cnn_c10_c100_trend} (left), we visualize the test\nerror as a function of iteration using the CNN.\nWe observe that when the cyclical learning rate starts after epoch $\\ell=180$\\ell=180, \nthe base models drop in performance due to the sudden increase in learning rate (see Figure \\ref{fig:swa_model} left).\nHowever, \\fastswa continues to improve while collecting the weights corresponding to high learning rates for averaging.\nIn general, we also find that the cyclical learning rate improves the base models beyond the usual cosine annealing \nschedule and increases the performance of \\fastswa as training progresses. \nCompared to \\swa, we also observe that \\fastswa converges substantially faster, \nfor instance, reducing the error to $10.5\\%$10.5\\%  at epoch $200$200 while \\swa attains \nsimilar error at epoch $350$350 for CIFAR-10 $4k$4k labels (Figure \n\\ref{fig:cnn_c10_c100_trend} left). We provide additional plots in Section \\ref{sec:add_results} \nshowing the convergence of the \\pi and MT models in all label \nsettings, where we observe similar trends that \\fastswa results in faster error reduction.\n2. We also find that the performance gains of \\fastswa over base models are higher for the \\pi model compared to the MT model, which is consistent with the convexity observation in Section \\ref{sec:empirical_convexity} \nand Figure \\ref{fig:convexity}.\nIn the previous evaluations \\citep[see e.g.][]{ssl-eval, mt}, the \\pi model was shown to be inferior to the MT model.\nHowever, with weight averaging, \\fastswa reduces the gap between \\pi and MT performance. \nSurprisingly, we find that the \\pi model can outperform MT after applying \\fastswa  \nwith moderate to large numbers of labeled points. \nIn particular, the $\\Pi$\\Pi+fast-SWA model outperforms MT+\\fastswa on CIFAR-10 with $4k$4k, $10k$10k, and $50k$50k labeled\ndata points for the Shake-Shake  architecture.\n3. \\vspace{-0.8\\baselineskip}\n4. \\subsection{CIFAR-100 and Extra Unlabeled Data} \\label{sec:cifar100_cnn} \\label{sec:cifar100}\n5. We evaluate the \\pi and MT models with \\fastswa on \nCIFAR-100.\nWe train our models using $50000$50000 images with $10k$10k and $50k$50k labels using the $13$13-layer CNN. \nWe also analyze the effect of using the Tiny Images dataset \\citep{tiny_images} as an additional source of unlabeled data.\nNext:\n1. \\subsection{Advancing State-of-the-Art} \\label{sec:soa}\n2. We have shown that \\fastswa can significantly improve the performance of both the \\pi and \nMT models. We provide a summary comparing our results with the previous best results in the literature in Table  \\ref{table:soa}, using the $13$13-layer CNN and\nthe Shake-Shake architecture that had been applied previously. We also provide detailed results the Appendix \\ref{sec:add_results}.\n3. \\subsection{Preliminary Results on Domain Adaptation}\n\\label{sec:domain_adaptation}\nDomain adaptation problems involve learning using a source domain $X_s$X_s \nequipped with labels $Y_s$Y_s and performing classification on the target \ndomain $X_t$X_t while having no access to the target labels at training time. \nA recent model by \\citet{da_mt} applies the consistency enforcing principle for \ndomain adaptation and achieves state-of-the-art results on many datasets. \nApplying \\fastswa to this model on domain adaptation from CIFAR-10 to STL we \nwere able to improve the best results reported in the literature from\n$19.9\\%$19.9\\% to $16.8\\%$16.8\\%. \nSee Section \\ref{sec:exp_da} for more details on the domain adaptation experiments.\n\nTarget length (characters): ~1438 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:cnn_c10_c100_trend\']\ntable_labels = [\'table:cifar100_shakeshake\']\nbib_keys = [\'te\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Generative AI in Higher Education: Evidence from an Elite College\nAbstract: Generative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPT\'s release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AI\'s potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AI\'s educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.\nSection name: Generative AI Usage Patterns Among Students\n\nFigure block (optional):\n- label: \\label{fig:ai_use}; caption: \\caption{The Adoption of Generative AI among Middlebury College Students}\n- label: \\label{fig:ai_use_time}; caption: \\caption{The Evolution of Generative AI Adoption among Middlebury College Students}\n- label: \\label{fig:ai_models}; caption: \\caption{Adoption of Generative AI Models Among College Students}\n- label: \\label{fig:ai_pays_dem}; caption: \\caption{Percent of Students Who Pay for Generative AI Tools}\nTable block (optional):\n- label: \\label{tab:ai_usage_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{8}{c}{Outcome: Uses AI during the semester with frequency of at least...}  \\\\\\cmidrule{3-10}  \t\t\t\t\t%\t\t\n- label: \\label{tab:ai_adopt_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{10}{c}{Outcome: Started using generative AI...}  \\\\\\cmidrule{3-12}  \t\t\t\t\t&& Before\n- label: \\label{tab:ai_freq_het}; text: \\begin{tabular}{lccccc} \t\t\t\t\t\t\\toprule \t\t\t\t\t\t&& \\multicolumn{4}{c}{By Usage Frequency} \\\\ \\cmidrule{3-6} \t\t\t\t\t\t& Any use & Rarely & Occasionally & Frequently & Very Frequently \\\\ \t\t\t\t\t\t& (1) & (2) & (3) & (4) & (5) \\\\ \\midrule\t\t\t\t\t\t \t\t\t\t\t\t\\\n- label: \\label{tab:ai_model_correlates}; text: \\begin{tabular}{l@{}lR{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}R{\\w cm}@{}L{0.45cm}} \t\t\t\t\t\\midrule \t\t\t\t\t&& \\multicolumn{10}{c}{Outcome: =1 if student uses}  \\\\\\cmidrule{3-12}  \t\t\t\t\t&& OpenAI\'s && Google\nCited paper titles/abstracts (optional):\n- [bick.etal2025] The rapid adoption of generative AI.: \n- [mcclain2024] Americans\' use of ChatGPT is ticking up, but few trust its   election information: \n- [nam2023ai] 56\\% of college students have used ai on assignments or exams.: \n- [stohr.etal2024] \\"o: \n- [ravselj.etal2025] s: \n- [otis2024global] Global Evidence on Gender Gaps and Generative AI.: \n- [hartley.etal2025] The Labor Market Effects of Generative Artificial   Intelligence: \n- [humlum.vestergaard2025] Large Language Models, Small Labor Market Effects: \n- [carvajal2024] Will artificial intelligence get in the way of achieving gender   equality?: \n- [david1990dynamo] The dynamo and the computer: an historical perspective on the modern   productivity paradox.: \n- [gallup2024] AI in the Workplace: Answering 3 Big Questions: \n- [hall2003adoption] Adoption of new technology: \n- [stokey2021technology] Technology diffusion.: \n- [world2016world] World Bank: \n- [bick.etal2025] The rapid adoption of generative AI.: \n- [mcclain2024] Americans\' use of ChatGPT is ticking up, but few trust its   election information: \n- [nam2023ai] 56\\% of college students have used ai on assignments or exams.: \n- [stohr.etal2024] \\"o: \n- [ravselj.etal2025] s: \n- [otis2024global] Global Evidence on Gender Gaps and Generative AI.: \n- [hartley.etal2025] The Labor Market Effects of Generative Artificial   Intelligence: \n- [humlum.vestergaard2025] Large Language Models, Small Labor Market Effects: \n- [carvajal2024] Will artificial intelligence get in the way of achieving gender   equality?: \n- [david1990dynamo] The dynamo and the computer: an historical perspective on the modern   productivity paradox.: \n- [gallup2024] AI in the Workplace: Answering 3 Big Questions: \n- [hall2003adoption] Adoption of new technology: \n- [stokey2021technology] Technology diffusion.: \n- [world2016world] World Bank: \n\nk-most adjacent paragraphs (context):\n(none)\n\nTarget length (characters): ~13653 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:ai_use\', \'fig:ai_use_time\', \'fig:ai_models\', \'fig:ai_pays_dem\']\ntable_labels = [\'tab:ai_usage_correlates\', \'tab:ai_adopt_correlates\', \'tab:ai_freq_het\', \'tab:ai_model_correlates\']\nbib_keys = [\'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'nam2023ai\', \'stohr.etal2024\', \'carvajal2024\', \'ravselj.etal2025\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'david1990dynamo\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'hall2003adoption\', \'otis2024global\', \'nam2023ai\', \'stokey2021technology\', \'carvajal2024\', \'stohr.etal2024\', \'world2016world\', \'ravselj.etal2025\', \'carvajal2024\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\', \'nam2023ai\', \'ravselj.etal2025\', \'stohr.etal2024\', \'carvajal2024\', \'mcclain2024\', \'gallup2024\', \'bick.etal2025\', \'hartley.etal2025\', \'humlum.vestergaard2025\', \'mcclain2024\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'otis2024global\', \'nam2023ai\', \'carvajal2024\', \'stohr.etal2024\', \'ravselj.etal2025\', \'carvajal2024\', \'stohr.etal2024\', \'nam2023ai\', \'ravselj.etal2025\', \'bick.etal2025\', \'humlum.vestergaard2025\', \'david1990dynamo\', \'hall2003adoption\', \'stokey2021technology\', \'world2016world\', \'mcclain2024\', \'stohr.etal2024\', \'bick.etal2025\', \'bick.etal2025\', \'ravselj.etal2025\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Neural-Symbolic Message Passing with Dynamic Pruning\nAbstract: Complex Query Answering (CQA) over incomplete Knowledge Graphs (KGs) is a challenging task. Recently, a line of message-passing-based research has been proposed to solve CQA. However, they perform unsatisfactorily on negative queries and fail to address the noisy messages between variable nodes in the query graph. Moreover, they offer little interpretability and require complex query data and resource-intensive training. In this paper, we propose a Neural-Symbolic Message Passing (NSMP) framework based on pre-trained neural link predictors. By introducing symbolic reasoning and fuzzy logic, NSMP can generalize to arbitrary existential first order logic queries without requiring training while providing interpretable answers. Furthermore, we introduce a dynamic pruning strategy to filter out noisy messages between variable nodes. Experimental results show that NSMP achieves a strong performance. Additionally, through complexity analysis and empirical verification, we demonstrate the superiority of NSMP in inference time over the current state-of-the-art neural-symbolic method. Compared to this approach, NSMP demonstrates faster inference times across all query types on benchmark dat\nSection name: More Details about the Datasets\n\nFigure block (optional):\n- label: \\label{figure4}; caption: \\caption{Graphical representation of the query types of the BetaE dataset considered in our experiment, where $p$, $i$, $u$, and $n$ represent projection, intersection, union, and negation, respectively. }\n- label: \\label{figure5}; caption: \\caption{Graphical representation of the query types of the FIT dataset considered in our experiment, where $l$, $m$, and $c$ represent existential leaf, multi graph, and circle, respectively. }\nTable block (optional):\n- label: \\label{KG stat}; text: \\begin{tabular}{ccccccc}     \\toprule     \\textbf{Knowledge Graph} & \\textbf{Entities} & \\textbf{Relations} & \\textbf{Training Edges} & \\textbf{Val Edges} & \\textbf{Test Edges} & \\textbf{Total Edges} \\\\     \\midrule     FB15k-237 & 14,505 &\n- label: \\label{BetaE stat}; text: \\begin{tabular}{ccccccc}     \\toprule     \\multirow{2}{*}{\\textbf{Knowledge Graph}} & \\multicolumn{2}{c}{\\textbf{Training Queries}} & \\multicolumn{2}{c}{\\textbf{Validation Queries}} & \\multicolumn{2}{c}{\\textbf{Test Queries}} \\\\     \\cmidru\n- label: \\label{FIT stat}; text: \\begin{tabular}{ccccccccccc}     \\toprule     \\textbf{Knowledge Graph} & \\textbf{pni} & \\textbf{2il} & \\textbf{3il} & \\textbf{2m} & \\textbf{2nm} & \\textbf{3mp} & \\textbf{3pm} & \\textbf{im} & \\textbf{3c} & \\textbf{3cm} \\\\     \\midrule     FB\nCited paper titles/abstracts (optional):\n- [ren2020beta] Beta embeddings for multi-hop logical reasoning in knowledge graphs: \n- [yin2024rethinking] Rethinking Complex Queries on Knowledge Graphs with Neural Link Predictors: \n\nk-most adjacent paragraphs (context):\n(none)\n\nTarget length (characters): ~973 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'figure4\', \'figure5\']\ntable_labels = [\'KG stat\', \'BetaE stat\', \'FIT stat\']\nbib_keys = [\'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\', \'ren2020beta\', \'yin2024rethinking\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).', 'You are reconstructing one missing LaTeX paragraph in a research paper.\n\nTitle: Progressive Prompts: Continual Learning for Language Models\nAbstract: We introduce Progressive Prompts - a simple and efficient approach for continual learning in language models. Our method allows forward transfer and resists catastrophic forgetting, without relying on data replay or a large number of task-specific parameters. Progressive Prompts learns a new soft prompt for each task and sequentially concatenates it with the previously learned prompts, while keeping the base model frozen. Experiments on standard continual learning benchmarks show that our approach outperforms state-of-the-art methods, with an improvement >20% in average test accuracy over the previous best-preforming method on T5 model. We also explore a more challenging continual learning setup with longer sequences of tasks and show that Progressive Prompts significantly outperforms prior methods.\nSection name: Further experimental results\n\nFigure block (optional):\n- label: \\label{fig:fwt_order8}; caption: \\caption{Forward transfer score of different approaches on order 8. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:fwt_order9}; caption: \\caption{Forward transfer score of different approaches on order 9. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:fwt_order10}; caption: \\caption{Forward transfer score of different approaches on order 10. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order8}; caption: \\caption{Backward transfer score of different approaches on order 8. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order9}; caption: \\caption{Backward transfer score of different approaches on order 9. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:bwt_order10}; caption: \\caption{Backward transfer score of different approaches on order 10. Different data limits are shown (20, 200 and 1000 samples per class).}\n- label: \\label{fig:evolution}; caption: \\caption{Evolution of average accuracy after learning new tasks.}\n- label: \\label{fig:fig_long_bars}; caption: \\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improve\nTable block (optional):\n- label: \\label{wrap-tab:init}; text: \\begin{tabular}{lcc}\\\\\\toprule   Method & Few-shot & Full-shot \\\\\\midrule PT + Prev. Init. & 48.2 & 50.0 \\\\  ProgPrompt & \\textbf{53.5} & \\textbf{69.3} \\\\ \\bottomrule \\end{tabular}\n- label: \\label{tab:table_appendix_long}; text: \\begin{tabular}{lccc|ccc|ccc|ccc} % <-- Alignments: 1st column left & 2nd middle and 3rd right & with vertical lines in between     \\toprule       \\textbf{Method} $\\downarrow$ & \\multicolumn{3}{c}{\\textbf{Order 8}}  & \\multicolumn{3}{c}{\\te\nCited paper titles/abstracts (optional):\n- [lopez2017gradient] Gradient episodic memory for continual learning: \n\nk-most adjacent paragraphs (context):\nNext:\n1. \\includegraphics[width=0.9\\textwidth]{images/long_exp_bars.png}\n\\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improvement of Progressive Prompts test score compared to per-task prompt on the corresponding task.}\n\\label{fig:fig_long_bars}\n\\end{figure}\n2. \\includegraphics[width=0.9\\textwidth]{images/long_exp_bars.png}\n\\caption{Per-task improvement of Progressive Prompts verus per-task prompts in CL experiment with order 8 across different data limits (20, 200 and 1000 samples per class). X-axis shows the sequence of tasks, Y-axis shows percentage improvement of Progressive Prompts test score compared to per-task prompt on the corresponding task.}\n\\label{fig:fig_long_bars}\n3. \\includegraphics[width=0.7\\textwidth]{images/superglue_illustration.png}\n4. \\caption{Original prompt tuning versus Progressive Prompts on SuperGLUE datasets. For illustration, we show how SuperGLUE task WiC is leaned (we have similar scheme for other tasks). Prompt tuning trains a single prompt of 100 tokens for WiC task. Progressive Prompts method learns a prompt of 40 tokens, which is progressively appended to the six frozen prompts of 10 tokens learned on GLUE benchmark (with random task order). Total prompt length is equal in both approaches. }\n\\label{fig:fig_super_illustration}\n\\end{figure}\n5. \\includegraphics[width=0.7\\textwidth]{images/superglue_illustration.png}\n\nTarget length (characters): ~1627 (±15%)\n\n# Canonicalized metadata\nfig_labels = [\'fig:fwt_order8\', \'fig:fwt_order9\', \'fig:fwt_order10\', \'fig:bwt_order8\', \'fig:bwt_order9\', \'fig:bwt_order10\', \'fig:evolution\', \'fig:fig_long_bars\']\ntable_labels = [\'wrap-tab:init\', \'tab:table_appendix_long\']\nbib_keys = [\'lopez2017gradient\']\n\n# Task\nWrite exactly one LaTeX-formatted paragraph that naturally fits between the adjacent paragraphs.\n\n# HARD REQUIREMENTS\n- Maintain objective, concise academic tone; ensure logical continuity with context.\n- Output exactly one paragraph (no lists/headers/environments, no blank lines).\n- Include each figure label exactly once via \\ref{<label>}.\n- Include each table label exactly once via \\ref{<label>}.\n- Cite all bib_keys using a single \\cite{...} or multiple as needed.\n\n\n# ORDERING\n- Follow the order in fig_labels when referring to figures.\n- Follow the order in table_labels when referring to tables.\n\n# SILENT SELF-CHECK (do not print this checklist)\n- If any fig_labels/table_labels exist: verify each appears exactly once as "\\ref{<label>}".\n- If any bib_keys exist: verify they appear in "\\cite{...}".\n- If none exist: verify the paragraph contains NO "\\ref{...}" and NO "\\cite{...}".\n- Verify single paragraph and length within tolerance.\n\n# Output\nReturn only the LaTeX paragraph text (no extra lines, no explanations).']
Number of prompts: 6
Image tag lists: [[['contains scatter plot']], [None, None], [None], [None, None, None, None], [['a figure showing network'], ['a figure showing network']], [['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about heatmap'], ['a chart about bar chart']]]
Number of Image tag lists: 6
Number of Image projection lists: 6
INFO 08-31 20:50:03 [utils.py:326] non-default args: {'model': 'Qwen/Qwen3-8B', 'enable_prompt_embeds': True, 'trust_remote_code': True, 'dtype': 'float16', 'gpu_memory_utilization': 0.6, 'disable_log_stats': True, 'enforce_eager': True}
INFO 08-31 20:50:11 [__init__.py:711] Resolved architecture: Qwen3ForCausalLM
WARNING 08-31 20:50:11 [__init__.py:2819] Casting torch.bfloat16 to torch.float16.
INFO 08-31 20:50:11 [__init__.py:1750] Using max model len 40960
WARNING 08-31 20:50:11 [arg_utils.py:1770] --enable-prompt-embeds is not supported by the V1 Engine. Falling back to V0. 
WARNING 08-31 20:50:11 [arg_utils.py:1544] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.
INFO 08-31 20:50:11 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 08-31 20:50:11 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
INFO 08-31 20:50:11 [__init__.py:3565] Cudagraph is disabled under eager mode
INFO 08-31 20:50:11 [llm_engine.py:222] Initializing a V0 LLM engine (v0.10.1.1) with config: model='Qwen/Qwen3-8B', speculative_config=None, tokenizer='Qwen/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":null,"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"cudagraph_mode":0,"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"pass_config":{"enable_fusion":false,"enable_noop":false},"max_capture_size":0,"local_cache_dir":null}, use_cached_outputs=False, 
INFO 08-31 20:50:13 [cuda.py:436] Using Flash Attention backend.
INFO 08-31 20:50:13 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
INFO 08-31 20:50:13 [model_runner.py:1080] Starting to load model Qwen/Qwen3-8B...
INFO 08-31 20:50:13 [weight_utils.py:296] Using model weights format ['*.safetensors']
INFO 08-31 20:50:18 [default_loader.py:262] Loading weights took 4.51 seconds
INFO 08-31 20:50:18 [model_runner.py:1112] Model loading took 15.2683 GiB and 4.838446 seconds
INFO 08-31 20:50:19 [worker.py:295] Memory profiling takes 0.84 seconds
INFO 08-31 20:50:19 [worker.py:295] the current vLLM instance can use total_gpu_memory (47.53GiB) x gpu_memory_utilization (0.60) = 28.52GiB
INFO 08-31 20:50:19 [worker.py:295] model weights take 15.27GiB; non_torch_memory takes 0.02GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 11.83GiB.
INFO 08-31 20:50:20 [executor_base.py:114] # cuda blocks: 5385, # CPU blocks: 1820
INFO 08-31 20:50:20 [executor_base.py:119] Maximum concurrency for 40960 tokens per request: 2.10x
INFO 08-31 20:50:21 [llm_engine.py:417] init engine (profile, create kv cache, warmup model) took 2.84 seconds
INFO 08-31 20:50:21 [llm.py:298] Supported_tasks: ['generate']
